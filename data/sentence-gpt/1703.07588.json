{"id": "1703.07588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries", "abstract": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 22 Mar 2017 10:08:51 GMT  (2823kb,D)", "https://arxiv.org/abs/1703.07588v1", "5 pages"], ["v2", "Thu, 31 Aug 2017 12:01:36 GMT  (2823kb,D)", "http://arxiv.org/abs/1703.07588v2", "5 pages, The code is available atthis https URL"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["yu-hsuan wang", "cheng-tao chung", "hung-yi lee"], "accepted": false, "id": "1703.07588"}, "pdf": {"name": "1703.07588.pdf", "metadata": {"source": "CRF", "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and Its Correlation with Phoneme Boundaries", "authors": ["Yu-Hsuan Wang", "Cheng-Tao Chung", "Hung-yi Lee"], "emails": ["r04922167@ntu.edu.tw,", "f01921031@ntu.edu.tw,", "hungyilee@ntu.edu.tw"], "sections": [{"heading": null, "text": "Index Terms: autoencoder, recurrent neural network"}, {"heading": "1. Introduction", "text": "Deep learning has achieved great success in many areas [1][2][3]. For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7]. In addition to taking the neural network outputs to be used in the target applications, internal signals within the neural networks were also found useful. A good example is the bottleneck features [8][9].\nOn the other hand, in the era of big data, huge quantities of unlabeled speech data are available but difficult to annotate, and unsupervised approaches to effectively extract useful information out of such unlabeled data are highly attractive [10][11]. Autoencoder structures have been used for extracting bottleneck features [12], while GRNN with various structures can be learned very well without labeled data. As one example, the outputs of GRNN learned in an unsupervised fashion have been shown to carry phoneme boundary information and used in phoneme segmentation [13][14].\nIn this paper, we try to analyze the gate activation signals (GAS) in GRNN, which are internal signals within such networks. We found such signals have temporal structures highly related to the phoneme boundaries, which was further verified by phoneme segmentation experiments."}, {"heading": "2. Approaches", "text": ""}, {"heading": "2.1. Gate Activation Signals (GAS) for LSTM and GRU", "text": "Recurrent neural networks (RNN) are neural networks whose hidden neurons form a directed cycle. Given a sequence x = (x1, x2, ..., xT ), RNN updates its hidden state ht at time index t according to the current input xt and the previous hidden state ht\u22121. Gated recurrent neural networks (GRNN) achieved better performance than RNN by introducing gates in the units to control the information flow. Two popularly used gated units are LSTM and GRU [15][16].\nThe signals within an LSTM recurrent unit are formulated as:\nft = \u03c3(Wfxt + Ufht\u22121 + bf ) (1)\nit = \u03c3(Wixt + Uiht\u22121 + bi) (2)\nc\u0303t = tanh(Wcxt + Ucht\u22121 + bc) (3)\nct = ftct\u22121 + itc\u0303t (4)\not = \u03c3(Woxt + Uoht\u22121 + bo) (5)\nht = ottanh(ct) (6)\nwhere ft, it, ot, ct, c\u0303t and ht are the signals over the forget gate, input gate, output gate, cell state, candidate cell state and hidden state at time t, respectively; \u03c3(\u00b7) and tanh(\u00b7) are the sigmoid and hyperbolic tangent activation functions respectively; W? and U? are the weight matrices and b? are the biases.\nThe GRU modulates the information flow inside the unit without a memory cell,\nht = (1\u2212 zt)ht\u22121 + zth\u0303t (7)\nzt = \u03c3(Wzxt + Uzht\u22121 + bz) (8)\nh\u0303t = tanh(Whxt + Uh(rt ht\u22121) + bh) (9)\nrt = \u03c3(Wrxt + Urht\u22121 + br) (10)\nwhere zt, rt, ht and h\u0303t are the signals over the update gate, reset gate, hidden state and candidate hidden state at time t, respectively; means element-wise product [17].\nHere we wish to analyze the gate activations computed in equations (1), (2), (5), (8), (10) [18] and consider their temporal structures. For a GRNN layer consisting of J gated units, we view the activations for a specific gate at time step t as a vector gt with dimensionality J , called gate activation signals (GAS). Here gt can be ht, it, ot, zt or rt above. Figure 1 is the schematic plot showing how GAS may imply for a gate in an gated unit.\nar X\niv :1\n70 3.\n07 58\n8v 2\n[ cs\n.S D\n] 3\n1 A\nug 2\n01 7"}, {"heading": "2.2. Autoencoder GRNN", "text": "Autoencoders can be trained in an unsupervised way, and have been shown to be very useful for many applications [19]. We can have an autoencoder with GRNN as in Figure 2 called AEGRNN here. Given an input utterance represented by its acoustic feature sequence {x1, x2, ..., xT }, at each time step t, AEGRNN takes the input vector xt, and produces the output x\u0302t, the reconstruction of xt. Due to the recurrent structure, to generate x\u0302t, AE-GRNN actually considers all information up to xt in the sequence, x1, x2, ..., xt, or x\u0302t = AE-GRNN(x1, x2, ..., xt). The loss function L of AE-GRNN in (11) is the averaged squared `-2 norm for the reconstruction error of xt.\nL = N\u2211 n Tn\u2211 t 1 d \u2016xnt \u2212 AE-GRNN(xn1 , xn2 , ..., xnt )\u20162 (11)\nwhere the superscript n indicates the n-th training utterance with length Tn, andN is the number of utterances used in training. d indicates the number of dimensions of xnt ."}, {"heading": "3. Initial Experiments and Analysis", "text": ""}, {"heading": "3.1. Experimental Setup", "text": "We conducted our initial experiments on TIMIT, including 4620 training utterances and 1680 testing utterances. The ground truth phoneme boundaries provided in TIMIT are used here. We train models on the training utterances, and perform analysis on the testing ones.\nIn the AE-GRNN tested, both the encoder and the decoder consisted of a recurrent layer and a feed-forward layer. The recurrent layers consisted of 32 recurrent units, while the feedforward layers consisted of 64 neurons. We used ReLU as the activation function for the feed-forward layers [20]. The dropout rate was set to be 0.3 [21]. The networks were trained with Adam [22]. The acoustic features used were the MFCCs of 39-dim with utterance-wise cepstral mean and variance normalization (CMVN) applied."}, {"heading": "3.2. Initial Results and Observations", "text": "Figure 3 shows the means of the gate activation signals over all gated units in the encoder of AE-GRNN with respect to the frame index, taken from an example utterance. The upper three subfigures (a)(b)(c) are for LSTM gates, while the lower two (d)(e) for GRU gates. The temporal variations of GRU gates are similar to the forget gate of LSTM to some degree, and looks like the negative of the input gate of LSTM except for a level shift. More importantly, when compared with the phoneme boundaries shown as the blue dashed lines, a very strong correlation can be found. In other words, whenever the signal switches from a phoneme to the next across the phoneme boundary, the changes in signal characteristics are reflected in the gate activation signals. This is consistent to the previous finding that the sudden bursts of gate activations indicated that there were boundaries of phonemes in a speech synthesis task [23]."}, {"heading": "3.3. Difference GAS", "text": "With the above observations, we define difference GAS as follows. For a GAS vector at time index t, gt, we compute its mean over all units to get a real value g\u0304t. We can then compute the difference GAS as the following:\n\u2206g\u0304t = g\u0304t+1 \u2212 g\u0304t (12)\nThe difference GAS can also be evaluated for each individual gated unit for each dimension of the vector gt,\n\u2206g\u0304jt = g\u0304 j t+1 \u2212 g\u0304 j t (13)\nwhere gjt is the j-th component of the vector gt. We plotted the difference GAS and the individual difference GAS for the first 8 units in a GRNN over the frame index for an example utterance as in Figure 4. We see those differences bear even stronger correlation with phoneme boundaries shown by vertical dashed\nlines. All these results are consistent with the finding that the gate activations of forget gate over recurrent LSTM units in the same layer have close correlation with phoneme boundaries in speech synthesis [23], although here the experiments were performed with AE-GRNN."}, {"heading": "4. Phoneme Segmentation", "text": "Because the GAS was found to be closely related to phoneme boundaries, we tried to use these signals to perform phoneme segmentation. The segmentation accuracy can be a good indicator to show the degree of the correlation between GAS and phoneme boundaries. In this section, we will first describe recurrent predictor model (RPM), an unsupervised phoneme segmentation approach, which servers as the baseline. Then we will describe how to use GAS in phoneme segmentation."}, {"heading": "4.1. Baseline: Recurrent Predictor Model", "text": "RPM was proposed earlier to train GRNN without labeled data, and it was found the discontinuity on model outputs have to do with phoneme boundaries [13][14]. An RPM has only the lower half of Figure 2. The model output at time t, x\u0302t = RPM(x1, x2, ..., xt), is to predict the next input xt+1. The loss function L used in training RPM is the averaged squared `-2 norm of prediction error,\nL = N\u2211 n Tn\u22121\u2211 t 1 d \u2016xnt+1 \u2212 RPM(xn1 , xn2 , ..., xnt )\u20162 (14)\nwhich is actually parallel to (11). The superscript n indicates the n-th training utterance and d indicates the number of dimensions of xnt . Because frames which are difficult to predict or with significantly larger errors are likely to be phoneme boundaries, the error signals Et of RPM,\nEt = 1\nd \u2016xnt+1 \u2212 RPM(xn1 , xn2 , ..., xnt )\u20162 (15)\ncan be used to predict the phoneme boundary similar to GAS here. A time index is taken as a phoneme boundary if Et is a local maximum, that is Et > Et\u22121 and Et > Et+1, and Et exceeds a selected threshold."}, {"heading": "4.2. GAS for Phoneme Segmentation", "text": "From Figure 4, a direct approach to use GAS for phoneme segmentation is to take a time index as a phoneme boundary if \u2206g\u0304t\nis a local maximum, that is \u2206g\u0304t > \u2206g\u0304t\u22121 and \u2206g\u0304t > \u2206g\u0304t+1, and \u2206g\u0304t exceeds a selected threshold.\nGAS can also be integrated with RPM. Since RPM also includes GRNN within its structure, GAS can also be obtained and interpolated with the error signals obtained in (15) as in (16), wherew is the weight. A time index is taken as a phoneme boundary if It is a local maximum and exceeds a selected threshold.\nIt = (1\u2212 w)Et + w\u2206g\u0304t (16)"}, {"heading": "5. Experiments Results for Phoneme Segmentation", "text": "Here we take phoneme segmentation accuracy as an indicator to show the correlation between GAS and phoneme boundaries. The setup is the same as in Section 3.1. In the segmentation experiments, a 20-ms tolerance window is used for evaluation. All GAS were obtained from the first recurrent layer. Different segmentation results were obtained according to different thresholds, we report the best results in the following tables."}, {"heading": "5.1. R-value Evaluation", "text": "It is well-known that the F1-score is not suitable for segmentation, because over segmentation may give very high recall leading to high F1-score, even with a relatively low precision[14]. In our preliminary experiments, a periodic predictor which predicted a boundary for every 40 ms gave F1-score 71.07 with precision 55.13 and recall 99.99, which didn\u2019t look reasonable. It has been shown that a better evaluation metric is the R-value [24], which properly penalized the over segmentation phenomenon. The approach proposed in a previous work [25] achieved an r-value 76.0, while the 40-ms periodic predictor only achieved 30.53. Therefore, we chose to use R-value on the performance measure."}, {"heading": "5.2. Comparison among different gates", "text": "The R-values using different gates of LSTM and GRU are shown in Table 1. The results for LSTM gates are consistent with the findings in the previous works [23][26]. In LSTM, the forget gate clearly captures the temporal structure most related to phoneme boundaries. GRU outperformed LSTM which is also consistent with earlier results [17][26]. The highest Rvalue is obtained with the update gate of GRU. The update gate in GRU is similar to the forget gate in LSTM. Both of them control whether the memory units should be overwritten. Interestingly, the reset gate of GRU achieved an R-value significantly higher than the corresponding input gate in LSTM. The reason is probably the location of reset gate. In GRU, the reset gate does not control the amount of the candidate hidden state independently, but shares some information of the update gate, thus has better access to more temporal information for phoneme segmentation [17]. The update gate in GRU was used for extracting GAS in the following experiments."}, {"heading": "5.3. Comparison among different approaches", "text": "In Table 2, we compared the R-value obtained from the temporal information provided by RPM, without or with its GAS (rows (a)(b)(c)(d)). The best result in Table 1 is in row(e), which was obtained with update gates of GRU in AE-GRNN. We considered two structures of RPM: the same as AE-GRNN (4 layers) or only use the encoder part (2 layers, a feed-forward layer plus a recurrent layer). The latter used the same number\nof parameters as AE-GRNN. We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in row (f) [27][28]. We further added a white noise with 6dB SNR to the original TIMIT corpus (the right column). The last row (g) is for the periodic predictor predicted a boundary every 80 ms, serving as a naive baseline. Precision-Recall curves in Figure 6 illustrate the overall performance on clean TIMIT corpus with different thresholds.\nWe found that the RPM performance was improved by the interpolation with GAS (RPM+GAS v.s. RPM). Larger improvements were gained when data became noisy. We further analyzed the segmentation results on clean corpus with the highest R-values of 2-layered RPM and AE-GRNN. We analyzed the results of AE-GRNN by observing \u2206g\u0304t and \u2206g\u0304jt . Likewise, we analyzed the results of RPM by observing Et and Ejt , where E j t indicates the squared prediction error in the jth dimension computed in (15). We showed their relations in Figure 5. We see that the curve of Et is smooth and significantly different from the sharp curve of \u2206g\u0304t. The smoothness led RPM to suffer from over segmentation. The smoothness was caused by the fact that there were always a subset of Ejt which were significantly large. On the other hand, the curves of \u2206g\u0304jt are more consistent. The consistency enables curve of \u2206g\u0304t to be sharp and thus AE-GRNN would not suffer from over segmentation. This explains why GAS are helpful here.\nAlso, we can see RPM alone didn\u2019t benefit much from adding more layers (rows (c) v.s. (a)). Providing if RPM is powerful enough to predict next frames better, there will be no\nerror signals and thus no temporal information. This side effect balanced the advantage of increased model size. Interestingly, not only GAS offered improvements (rows (b) v.s. (a) and rows (d) v.s. (c)), but with more layers, the interpolation with GAS achieved larger improvements (rows (d) v,s, (b)). The best performances in both clean and noisy corpora are achieved by 4-layered RPM with GAS. Last but not least, performance of approaches using GAS and HAC are more robust to noise."}, {"heading": "6. Conclusions", "text": "We show that the gate activation signals (GAS) obtained in an unsupervised fashion have temporal structures highly correlated with the phoneme changes in the signals, and this correlation was verified in the experiments for phoneme segmentation. Also, our experiments also showed that GAS bring improvements to RPM without additional parameters. Like bottleneck features, GAS are obtained from the element of neural networks, instead of networks\u2019 outputs, and both of them are shown to bring improvements. With the promising results of GAS shown in the paper, we hope GAS can brought the same improvements as the ones brought by bottleneck features."}, {"heading": "7. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] H. Hermansky, D. P. Ellis, and S. Sharma, \u201cTandem connectionist feature extraction for conventional hmm systems,\u201d in Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on, vol. 3. IEEE, 2000, pp. 1635\u20131638.\n[3] B.-P. Network, \u201cHandwritten digit recognition with,\u201d 1989.\n[4] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, \u201cEndto-end learning of action detection from frame glimpses in videos,\u201d CoRR, vol. abs/1511.06984, 2015. [Online]. Available: http://arxiv.org/abs/1511.06984\n[5] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, \u201cNeural architectures for named entity recognition,\u201d CoRR, vol. abs/1603.01360, 2016. [Online]. Available: http: //arxiv.org/abs/1603.01360\n[6] Y.-A. Chung, C.-C. Wu, C.-H. Shen, H.-Y. Lee, and L.-S. Lee, \u201cAudio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder,\u201d arXiv preprint arXiv:1603.00982, 2016.\n[7] Y. Adi, J. Keshet, E. Cibelli, and M. Goldrick, \u201cSequence segmentation using joint rnn and structured prediction models,\u201d arXiv preprint arXiv:1610.07918, 2016.\n[8] F. Gre\u0301zl, M. Karafia\u0301t, S. Konta\u0301r, and J. Cernocky, \u201cProbabilistic and bottle-neck features for lvcsr of meetings,\u201d in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013757.\n[9] D. Yu and M. L. Seltzer, \u201cImproved bottleneck features using pretrained deep neural networks.\u201d in Interspeech, vol. 237, 2011, p. 240.\n[10] C.-y. Lee and J. Glass, \u201cA nonparametric bayesian approach to acoustic model discovery,\u201d in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.\n[11] C.-T. Chung, C.-a. Chan, and L.-s. Lee, \u201cUnsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.\n[12] J. Gehring, Y. Miao, F. Metze, and A. Waibel, \u201cExtracting deep bottleneck features using stacked auto-encoders,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3377\u20133381.\n[13] J. F. Drexler, \u201cDeep unsupervised learning from speech,\u201d Master\u2019s thesis, Massachusetts Institute of Technology, 2016.\n[14] P. Michel, O. Ra\u0308sa\u0308nen, R. Thiollie\u0300re, and E. Dupoux, \u201cImproving phoneme segmentation with recurrent neural networks,\u201d CoRR, vol. abs/1608.00508, 2016. [Online]. Available: http://arxiv.org/ abs/1608.00508\n[15] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[16] K. Cho, B. Van Merrie\u0308nboer, D. Bahdanau, and Y. Bengio, \u201cOn the properties of neural machine translation: Encoder-decoder approaches,\u201d arXiv preprint arXiv:1409.1259, 2014.\n[17] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d arXiv preprint arXiv:1412.3555, 2014.\n[18] A. Karpathy, J. Johnson, and L. Fei-Fei, \u201cVisualizing and understanding recurrent networks,\u201d arXiv preprint arXiv:1506.02078, 2015.\n[19] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and G. E. Hinton, \u201cBinary coding of speech spectrograms using a deep auto-encoder.\u201d in Interspeech. Citeseer, 2010, pp. 1692\u20131695.\n[20] V. Nair and G. E. Hinton, \u201cRectified linear units improve restricted boltzmann machines,\u201d in Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807\u2013814.\n[21] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting.\u201d Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\n[22] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[23] Z. Wu and S. King, \u201cInvestigating gated recurrent networks for speech synthesis,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5140\u20135144.\n[24] O. J. Ra\u0308sa\u0308nen, U. K. Laine, and T. Altosaar, \u201cAn improved speech segmentation quality measure: the r-value.\u201d in Interspeech, 2009, pp. 1851\u20131854.\n[25] O. Ra\u0308sa\u0308nen, \u201cBasic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a prelinguistic level.\u201d in CogSci, 2014.\n[26] W. Zaremba, \u201cAn empirical exploration of recurrent network architectures,\u201d 2015.\n[27] Y. Qiao, N. Shimomura, and N. Minematsu, \u201cUnsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons,\u201d in Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 3989\u2013 3992.\n[28] C.-a. Chan, \u201cUnsupervised spoken term detection with spoken queries,\u201d Ph.D. dissertation, National Taiwan University, 2012."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Tandem connectionist feature extraction for conventional hmm systems", "author": ["H. Hermansky", "D.P. Ellis", "S. Sharma"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on, vol. 3. IEEE, 2000, pp. 1635\u20131638.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Handwritten digit recognition with", "author": ["B.-P. Network"], "venue": "1989.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Endto-end learning of action detection from frame glimpses in videos", "author": ["S. Yeung", "O. Russakovsky", "G. Mori", "L. Fei-Fei"], "venue": "CoRR, vol. abs/1511.06984, 2015. [Online]. Available: http://arxiv.org/abs/1511.06984", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural architectures for named entity recognition", "author": ["G. Lample", "M. Ballesteros", "S. Subramanian", "K. Kawakami", "C. Dyer"], "venue": "CoRR, vol. abs/1603.01360, 2016. [Online]. Available: http: //arxiv.org/abs/1603.01360", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder", "author": ["Y.-A. Chung", "C.-C. Wu", "C.-H. Shen", "H.-Y. Lee", "L.-S. Lee"], "venue": "arXiv preprint arXiv:1603.00982, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence segmentation using joint rnn and structured prediction models", "author": ["Y. Adi", "J. Keshet", "E. Cibelli", "M. Goldrick"], "venue": "arXiv preprint arXiv:1610.07918, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic and bottle-neck features for lvcsr of meetings", "author": ["F. Gr\u00e9zl", "M. Karafi\u00e1t", "S. Kont\u00e1r", "J. Cernocky"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013757.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Improved bottleneck features using pretrained deep neural networks.", "author": ["D. Yu", "M.L. Seltzer"], "venue": "in Interspeech,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A nonparametric bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J. Glass"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 40\u201349.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["C.-T. Chung", "C.-a. Chan", "L.-s. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3377\u20133381.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep unsupervised learning from speech", "author": ["J.F. Drexler"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving phoneme segmentation with recurrent neural networks", "author": ["P. Michel", "O. R\u00e4s\u00e4nen", "R. Thiolli\u00e8re", "E. Dupoux"], "venue": "CoRR, vol. abs/1608.00508, 2016. [Online]. Available: http://arxiv.org/ abs/1608.00508", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Binary coding of speech spectrograms using a deep auto-encoder.", "author": ["L. Deng", "M.L. Seltzer", "D. Yu", "A. Acero", "A.-r. Mohamed", "G.E. Hinton"], "venue": "in Interspeech. Citeseer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigating gated recurrent networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5140\u20135144.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Basic cuts revisited: Temporal segmentation of speech into phone-like units with statistical learning at a prelinguistic level.", "author": ["O. R\u00e4s\u00e4nen"], "venue": "CogSci,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["W. Zaremba"], "venue": "2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised optimal phoneme segmentation: Objectives, algorithm and comparisons", "author": ["Y. Qiao", "N. Shimomura", "N. Minematsu"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 3989\u2013 3992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised spoken term detection with spoken queries", "author": ["C.-a. Chan"], "venue": "Ph.D. dissertation, National Taiwan University, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Deep learning has achieved great success in many areas [1][2][3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 248, "endOffset": 251}, {"referenceID": 4, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 251, "endOffset": 254}, {"referenceID": 5, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 254, "endOffset": 257}, {"referenceID": 6, "context": "For problems related to sequential data such as audio, video and text, significant improvements have been achieved by Gated Recurrent Neural Networks (GRNN), in which the hidden neurons form a directed cycle suitable for processing sequential data [4][5][6][7].", "startOffset": 257, "endOffset": 260}, {"referenceID": 7, "context": "A good example is the bottleneck features [8][9].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "A good example is the bottleneck features [8][9].", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "On the other hand, in the era of big data, huge quantities of unlabeled speech data are available but difficult to annotate, and unsupervised approaches to effectively extract useful information out of such unlabeled data are highly attractive [10][11].", "startOffset": 244, "endOffset": 248}, {"referenceID": 10, "context": "On the other hand, in the era of big data, huge quantities of unlabeled speech data are available but difficult to annotate, and unsupervised approaches to effectively extract useful information out of such unlabeled data are highly attractive [10][11].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Autoencoder structures have been used for extracting bottleneck features [12], while GRNN with various structures can be learned very well without labeled data.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "As one example, the outputs of GRNN learned in an unsupervised fashion have been shown to carry phoneme boundary information and used in phoneme segmentation [13][14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "As one example, the outputs of GRNN learned in an unsupervised fashion have been shown to carry phoneme boundary information and used in phoneme segmentation [13][14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Two popularly used gated units are LSTM and GRU [15][16].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Two popularly used gated units are LSTM and GRU [15][16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "where zt, rt, ht and h\u0303t are the signals over the update gate, reset gate, hidden state and candidate hidden state at time t, respectively; means element-wise product [17].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "Here we wish to analyze the gate activations computed in equations (1), (2), (5), (8), (10) [18] and consider their temporal structures.", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Autoencoders can be trained in an unsupervised way, and have been shown to be very useful for many applications [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "We used ReLU as the activation function for the feed-forward layers [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "3 [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "The networks were trained with Adam [22].", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "This is consistent to the previous finding that the sudden bursts of gate activations indicated that there were boundaries of phonemes in a speech synthesis task [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 22, "context": "All these results are consistent with the finding that the gate activations of forget gate over recurrent LSTM units in the same layer have close correlation with phoneme boundaries in speech synthesis [23], although here the experiments were performed with AE-GRNN.", "startOffset": 202, "endOffset": 206}, {"referenceID": 12, "context": "RPM was proposed earlier to train GRNN without labeled data, and it was found the discontinuity on model outputs have to do with phoneme boundaries [13][14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "RPM was proposed earlier to train GRNN without labeled data, and it was found the discontinuity on model outputs have to do with phoneme boundaries [13][14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "It is well-known that the F1-score is not suitable for segmentation, because over segmentation may give very high recall leading to high F1-score, even with a relatively low precision[14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "The approach proposed in a previous work [25] achieved an r-value 76.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "The results for LSTM gates are consistent with the findings in the previous works [23][26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "The results for LSTM gates are consistent with the findings in the previous works [23][26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "GRU outperformed LSTM which is also consistent with earlier results [17][26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "GRU outperformed LSTM which is also consistent with earlier results [17][26].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "In GRU, the reset gate does not control the amount of the candidate hidden state independently, but shares some information of the update gate, thus has better access to more temporal information for phoneme segmentation [17].", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": "We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in row (f) [27][28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "We also tested the conventional approach of using hierarchical agglomerative clustering (HAC) as shown in row (f) [27][28].", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.", "creator": "LaTeX with hyperref package"}}}