{"id": "1703.07638", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Machine Learning Based Source Code Classification Using Syntax Oriented Features", "abstract": "As of today the programming language of the vast majority of the published source code is manually specified or programmatically assigned based on the sole file extension. In this paper we show that the source code programming language identification task can be fully automated using machine learning techniques. We first define the criteria that a production-level automatic programming language identification solution should meet. Our criteria include accuracy, programming language coverage, extensibility and performance. We then describe our approach: How training files are preprocessed for extracting features that mimic grammar productions, and then how these extracted grammar productions are used for the training and testing of our classifier. We achieve a 99 percent accuracy rate while classifying 29 of the most popular programming languages with a Maximum Entropy classifier. It is important to note that most programs use this classification task. If you don't know the classification process, you may not understand the information on how to identify it. We are not making an exact definition of the classification task. All of the above criteria require specific information on the types of language they are using to classify software that is run on.\n\nThe Classification of Software\nThese criteria are developed to be useful in the production and testing of our classifier. However, in order to properly determine the classification task, we are interested in defining a machine learning algorithm based on the actual algorithm used. We refer to the machine learning algorithm described below.\nMachine Learning\nMachine learning algorithms that can be trained to classify software in the production environment are able to perform a relatively simple task. The following algorithms can be trained to classify software by looking at their original source code in the source.\nMachine learning algorithms that can be trained to classify software in the production environment are able to perform a relatively simple task. The following algorithms can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original source code in the source. Machine learning algorithms that can be trained to classify software by looking at their original", "histories": [["v1", "Sat, 4 Mar 2017 10:44:40 GMT  (577kb)", "http://arxiv.org/abs/1703.07638v1", "13 pages, 4 tables, 4 examples"]], "COMMENTS": "13 pages, 4 tables, 4 examples", "reviews": [], "SUBJECTS": "cs.LG cs.PL", "authors": ["shaul zevin", "catherine holzem"], "accepted": false, "id": "1703.07638"}, "pdf": {"name": "1703.07638.pdf", "metadata": {"source": "CRF", "title": "Machine Learning Based Source Code Classification Using Syntax Oriented Features", "authors": ["Shaul Zevin", "Catherine Holzem"], "emails": ["shaul.zevin@gmail.com", "catherine.holzem@gmx.net"], "sections": [{"heading": null, "text": "As of today the programming language of the vast majority of the published source code is manually specified or programmatically assigned based on the sole file extension. In this paper we show that the source code programming language identification task can be fully automated using machine learning techniques. We first define the criteria that a production-level automatic programming language identification solution should meet. Our criteria include accuracy, programming language coverage, extensibility and performance. We then describe our approach: How training files are preprocessed for extracting features that mimic grammar productions, and then how these extracted \u2018grammar productions\u2019 are used for the training and testing of our classifier. We achieve a 99% accuracy rate while classifying 29 of the most popular programming languages with a Maximum Entropy classifier.\nIndex Terms Classification algorithms, Computer languages, Information entropy"}, {"heading": "1. INTRODUCTION", "text": "The need for source-code-to-programming-language classification arises in use cases such as syntax highlighting, source code repositories indexing and estimation of trends in programming language popularity.\nPopular syntax highlighters take different approaches: SyntaxHighlighter[1] and many other such tools ask the user to\nannotate the source code with the name of the matching programming language, which effectively boils down to manual\nprogramming language identification. Highlight.js[2] on the other hand uses the success-rate of the highlighting process to\nidentify the language: Rare language constructs are given more weight while ubiquitous constructs such as common numbers\nhave no weight at all; And there are also constructs defined as illegal that cause the parser to drop the highlighting attempt for\na given language. Google Code Prettify[3] finally bypasses the programming identification hurdle altogether by generalizing\nthe highlighting so it will work independently of the language.\nThe approach for the labelling of source code varies also when it comes to source code repositories: SourceForge[4] does not maintain the programming language on the source code file level, it merely stores the programming language of the project as a whole as manually specified by the project submitter. GitHub[5]\u2019s approach is more sophisticated: It uses the Linguist[6] library, which applies following cascade of strategies to identify the programming language of the submitted source code:\n1. Detect Emacs/Vim modelines\n2. Looksfor a shebang \u201c#!/\u2026\u201d\n3. Check if file extension is unique for the language\n4. Heuristics - are there any tell-tale syntax sequences that we can use to identify the language\n5. Na\u00efve Bayesian classification\nLinguist stops when a single programming language candidate is left. Note that identification by unique file extension (strategy 3) has precedence over machine-learning-based classification (strategy 5).\nThe ideal of automatic computation goes back to Charles Babbage\u2019s wish to eliminate the risk of error in the production of printed mathematical tables. To make the automatic computation ideal practically applicable, one has to consider aspects such as use cases coverage, performance, maintenance and testing. We argue that the automatic solution for programming language identification should be evaluated based on following criteria:\n1. Accuracy\nThe accuracy of the results must be evaluated on a data set collected from as many different sources as possible. As stated by Brian W. Kernighan[7] \u201cComputer programs can be written many different ways and still achieve the same effect \u2026 We have come to learn, however, that functionally equivalent programs can have extremely important stylistic differences\u201d. Note that absolute accuracy is unattainable because of the existence of polyglots \u2013 source code valid for more than a single programming language.\n2. Popular programming languages support [8][9]\nNote that there are at least 20 programming languages with popularity rates exceeding 1%. And over 50 languages that pass the 0.1% mark.\n3. Extensibility in a reasonable amount of effort and time\nProgramming language popularity is dynamic and may exhibit fluctuations of a few percent in any one-year period. Moreover new programming languages emerge all the time e.g. Swift 2014.\n4. Independence of file extension\nA file extension will not be available for snippets, short portions of code as posted on question and answer sites such as Stack Overflow [10] Moreover, even when available, file extensions can be ambiguous (.h .m .pl) or just plainly wrong.\n5. Performance\nWe define performance as the time required for the identification of the programming language of a single source file. Programming language identification will be embedded into real-time human-computer interactions and should not increase the response time of the whole process significantly.\nDespite its many practical applications automated programming language identification has received moderate attention from research. We have summed up the various attempts known to us based on our evaluation criteria as described above in Table 1.\nKennedy van Dam et al. [14] Modified Kneser-Ney\ndiscounting classifier\nUnigrams, bigrams and trigrams of sequences of characters separated by white spaces 20 4000 96.9\nThis paper Maximum\nEntropy classifier\nUnigrams, bigrams and trigrams of sequences of lexicalized tokens 29 147843 99.0"}, {"heading": "2. METHOD", "text": "One can hypothesize the best approach for identifying the programming language of a piece of source code to be the selection of the language whose grammar parses the code successfully. Such a method cannot be implemented in practice for a number of reasons: First, to obtain a grammar for each language we try to identify is hardly possible. For example \u201cC++ is pretty much impossible to parse by merely writing up its grammar\u201d \u2013 Terence Parr, the author of ANTLR [15]. Second, the maintenance cost of grammars will be extremely high as languages evolve and new languages emerge over time. Finally the need to run the source code against every available parser would inevitably negatively impact performance.\nOur chosen approach is to automatically derive a single grammar from the training data that will later in the process be used for parsing every training and test file. The grammar includes the most representative productions for every language in the training set. The ability to build such a common grammar automatically relies on the fact that programming languages share a common syntax structure, with building blocks such as keywords, identifiers, constants and statements. As Aho [17] p. 112 et al. states \u201cIn many programming languages, the following classes cover most or all of the tokens:\n One token for each keyword. The pattern for a keyword is the same as the keyword itself.\n Tokens for the operators, either individually or in classes such as the token comparison.\n One token representing all identifiers.\n One or more tokens representing constants, such as numbers and literal strings.\n Tokens for each punctuation symbol, such as left and right parentheses, comma, and semicolon.\u201d\nOnce built the grammar is run against the source code files from the training set: The most representative grammar productions for each language are at this stage of the process selected to be used as features by the language classifier.\nOnce the feature set has been defined the classifier can be trained. Classifier training is a process in which a weight is assigned to each (feature, language) pair. Features (grammar productions) found more frequently in the training set for a particular language than in the training files for other languages will be assigned a higher weight for that language.\nOnce the classifier has been trained we run it on our test set to measure its accuracy: Each test file is parsed and matched grammar productions are extracted and fed to the trained classifier, which outputs probabilities for each language represented in the training set. To estimate the classifier accuracy, we compare the language for which the classifier returned the highest probability to the actual language of the test file.\nNote that the distribution of grammar productions is assumed to be very similar in the training and test sets. This is a key assumption for obtaining a high classification accuracy rate on the test set.\nIn the next subsections we provide a detailed description of the grammar construction, classifier training and classifier testing processes."}, {"heading": "2.1 File Preprocessing", "text": "Any source code file, whether from the training or from the test set, first goes through some preliminary file preprocessing as detailed below.\nDiagram 1 File Preprocessing\nA conversion of all letters to lowercase is required since some programming languages (as e.g. Ada, Visual BASIC, GW Basic, Quite Basic, FORTRAN, SQL and Pascal) are case-insensitive.\nClassical lexical analysis (see Appendix A) converts parsed source code into a stream of tokens. The lexer splits the source code into tokens by using white space characters as separators. Before applying the lexer we insert whitespaces between alpha, numeric and punctuation sequences as part of the file preprocessing since each such sequence represents a different type of token.\nAll numeric sequences are considered as constants and we replace each numeric sequence with a generic \u2018number\u2019 token __d__.\n__BOF__ and __EOF__ (beginning and end of file) tokens are inserted at the beginning and at the end of file. The rationale behind this addition is that some statements like java package declaration can appear only at the beginning or the end of the file.\nConsecutive new line characters ae replaced with a single __NL__ token since statements in languages like Basic, FORTRAN and Matlab are separated by a new line.\nExample 1. File Preprocessing\nsource code fragment \u2018FUNCTION(\u201c123\u201d)\u2019 will be split into \u2018function (\u201c __d__ \u201c)\u2019."}, {"heading": "2.2 Grammar Construction", "text": "Diagram 2 Grammar Construction\nFile Convert to\nlowercase\nSplit between\nalpha,\nnumeric and punctuation\ncharacters\nReplace\nevery\nnumeric sequence\nwith generic __d__ token\nAdd __BOF__ and __EOF__\ntokens\nReplace new\nline\nsequences\nwith\n__NL__\ntoken\nPreprocessed\nfile\nPreprocessed training files (per language)\nCompute alpha and punctuation words frequency > 0.01\nKeyword\ntoken with\nsame pattern\nIdentifier\ntoken __a__\nor __s__\nYes\nNo\nExtract\nUnigrams\nBigrams Trigrams\nIn a first step we process the set of training files available for each language in turn: We convert each word in the preprocessed training files (see section 2.1) to either the matching specific keyword token or to a generic \u2018identifier\u2019 token. To decide whether the word is an actual keyword or an identifier, we calculate the word frequency in the training files for the language being processed. The word frequency is expressed as the number of training files in which the word was found divided by total number of training files for the particular language. Words whose frequency exceeds 0.01 are considered as language keywords and replaced with a token of the same pattern i.e. the word \"if\" is replaced with a token \"if\". Words with a frequency below 0.01 are considered as identifiers. Alpha identifiers are replaced with the generic __a__ token and punctuation identifiers are replaced with __s__.\nNote that, when calculating word frequencies, we deliberately skip language comments. Text such as copyright statement is not part of the language and should not be used for the language grammar extraction. The comments syntax is the only actual programming language syntax rule we have hardwired, all other grammar productions are derived from the training set in a next step as described below.\nExample 2. Keywords and identifiers replacement\nPreprocessed training file:\n< ul class =\u201d democrats \u201d> __NL__\n< li > Clinton < / li > __NL__\n</ ul > __NL__\nInfrequent words democrats and Clinton are replaced with identifier token __a__, while frequent words such as class are replaced with a token of the same pattern:\n< ul class =\u201d __a__ \u201d> __NL__\n< li > __a__ </ li > __NL__\n</ ul > __NL__\nIn a second step we extract all unigrams, bigrams and trigrams from the token stream output of the first step. These extracted n-grams are the candidates for our grammar production selection process.\nExample 3. N-grams as extracted from Example 2. Unigrams: <, ul, class, =\u201d, __a__, \">, >, __NL__, li, </ Bigrams: < ul, ul class, class =\u201d, =\" __a__, __a__ \u201d> , \"> __NL__, __NL__ <, < li, li >, > __a__, __a__ </, </ li, > __NL__, __NL__ </, </ ul, ul >\nTrigrams: < ul class, ul class =\u201d, class =\u201d __a__, =\u201d __a__ \u201d>, __a__ \u201d> __NL__, \u201d> __NL__ <, __NL__ < li, < li >, li > __a__, > __a__ </, __a__ </ li, </ li >, li > __NL__, > __NL__ </, __NL__ </ ul, </ ul >, ul > __NL__\nThe third and last step of our grammar construction process is the selection of the most informative grammar productions from the n-gram set generated during the second step. We use the MI (Mutual Information) index to determine which n-grams should be retained as grammar productions. The MI index measures how much information the presence/absence of a feature contributes to making the correct classification decision.\nIf f is a nominal feature with k different values and l is your target class with m classes, the MI of f is given by:\nEquation 1. Mutual Information\n)()(\n),( log),(\n1 1 ji\nji\nj\nk\ni\nm\nj\ni lpfP\nlfP lfPMI \n \n\nCompute mutual information > 0.05 Grammar productions Yes\nIn our case k=2 and 1f =0/ 2f =1 denote absence/presence of a grammar production in a training file. Our m=29, the number of classified programming languages.\nOnly n-grams with an MI index above 0.05 make it into our grammar.\nExample 4. Grammar productions as selected from example 3 (MI above 0.05 threshold) followed by MI value class 0.297, __NL__ < 0.203, __NL__ </ 0.189, > __NL__ </ 0.180, \"> __NL__ 0.167, \"> __NL__ < 0.163, class =\" 0.143, ul 0.109, _NL__ < li 0.093, __NL__ </ ul 0.089, __a__ \"> __NL__ 0.071, ul class 0.0589, ul class =\" 0.0584, < ul class 0.0576, class =\" __a__ 0.0546"}, {"heading": "2.3 Classifier training", "text": "We have opted for a maximum entropy (maxent) classifier. \"The motivating idea behind maximum entropy is that one should prefer the most uniform models that also satisfy any given constraints.\" [16]. A nice property of the maxent classifier, as can be seen from the discussion below, is that no assumption is made on the relationships between features (our \u2018grammar productions\u2019). For example an assumption on feature independency is not required. Since grammar productions are clearly\ndependent, the maxent classifier seemed like a promising classifier choice for our particular use case.\nTo project the programming language identification task onto the maximum entropy model, we define the following notation:\nL the set of all supported languages jl\nS the set of all preprocessed training files l(s) the programming language of the training file s\nG the set of all grammar productions ig we have extracted in 2.2\n),(, lsg ji a grammar production indicator function from S x L to {0,1}. The function returns 1 if the sample s contains\ngrammar production ig and jll  . It returns 0 otherwise. The total number of such indicator functions is LG \nji , the weight of matched grammar production i for language j. The values of these weights are computed during the\nclassifier training process.\n)|( slp a modeled conditional probability of language l given sample s )(~ sp an empirical probability of a sample s\n   Ss\nslpslpsppH )|(log)|()(~)( is an entropy function of a modeled conditional probability\n   Ss sslpSLogLik )|)((log()( is the log likelihood of )|( slp over the training set S\nThe constraints of our maximum entropy model are described in Equation 2. It requires each grammar production indicator\nfunction jig , to result in model-predicted counts matching the empirical ones for the training set.\nEquation 2. Maximum entropy model constraints\n     Ss Ss jijjijji slsglsgslplg ))(,(),()|(, ,,\nIt can be shown [20],[21] that the conditional probability distribution )|( slp that satisfies the constraints as described by\nEquation 2, and which has the form of Equation 3, will maximize the entropy )( pH . Furthermore such a function is unique.\nEquation 3. Probability of sample s to be classified with language jl\n),(exp\n),(exp\n),|( ,,\n,,\nkki\nGg\nki\nLl\njji\nGg\nji\nj lsg\nlsg\nslp\nik\ni\n\n\n\n \n\n\n\nIt can be also shown [20],[21] that the conditional probability distribution )|( slp that meets the maximum entropy\nrequirements (i.e. satisfies constraints of Equation 2 and has the form of Equation 3) will also have the maximum log\nlikelihood over the training set )(SLogLik . The log likelihood )(SLogLik is a convex function with a single maximum.\nTherefore any numerical optimization package can be used to find optimal grammar production weights ji , by exploring the\nlog likelihood function space.\nFor the log likelihood value calculation we need to check if sample s contains grammar production g. The grammar production checker procedure, described next, is used to answer that question."}, {"heading": "2.4 Grammar Production Checker", "text": "The GrammarProductionChecker procedure outputs true if sample s contains grammar production g and false otherwise.\nprocedure GrammarProductionChecker(sample s, grammar production g)\n# s is preprocessed (see 2.1) s' = preprocess s\n# Nwwws 21' N = number of words in s' # loop on every word boundary for x in 1..N\nif g is unigram and g matches xw\nreturn true\nendif\nif g is bigram and g matches 1xxww\nreturn true\nendif\nif g is trigram and g matches 21  xxx www\nreturn true\nendif\nend # words loop return false\nend # procedure"}, {"heading": "2.5 Classifier testing", "text": "Once the classifier has been trained, ie once values have been computed for the ji , weights in Equation 3, we need to\nevaluate the accuracy of the trained classifier on a set of unseen test files.\nEach test file is preprocessed and parsed against the grammar extracted from the training set to obtain the set of features or grammar productions present in the file. The probability of each programming language is calculated by using Equation 3. The classifier outputs the language with the highest probability as the language detected for the file.\nMatching the output of the classifier against the actual labels of the test files we obtain precision, recall and F-measure values for each programming language in the test set.\nlanguagethewithlabeledsamplestestofnumber\nlanguagethewithidentifiedcorrectlysamplestestofnumber languagetheofprecision \nlanguagethewithidentifiedsamplestestofnumber\nlanguagethewithidentifiedcorrectlysamplestestofnumber languagetheofrecall \nrecallprecision\nrecallprecision languagetheofmeasureF\n\n  2\nPrecision and recall measure 2 different aspects of the accuracy of the trained classifier. The precision statistic (one statistic per supported language) gives the probability of the actual language being detected for any sample in the particular language. The recall statistic (also per language) gives the probability of the detection of the particular language to be a true positive, ie for that language to be the actual language of the classified sample. The F-measure combines these 2 complementary measures of a classifier\u2019s accuracy in one unique statistic."}, {"heading": "3. EXPERIMENT DESIGN", "text": ""}, {"heading": "3.1 Training and Test Data Selection", "text": "Similarly to [12],[13] and [14] we sourced our training and test data from GitHub [5]. Each GitHub source code\nrepository typically consists of the files for a single software project.\nWe created a web crawler that randomly selects a subset of all the GitHub repositories annotated with a particular programming language until obtaining more than 5,000 training and 5,000 test files for each programming language to be supported by our classifier. Typically a few hundred repositories were needed to collect enough files for each programming language. Each selected repository was assigned in its entirety to either training or testing. Almost every repository in our data set was created by a different programmer. Duplicate files were removed, and files smaller than 3 bytes or bigger than 240,000 bytes were skipped.\nIn total our entire dataset includes 293,319 source files collected from 16,134 distinct GitHub repositories. 145,476 files were used for training and 147,843 files for testing."}, {"heading": "3.2 File Preprocessing", "text": "File preprocessing as described in section 2.1 was implemented by defining a lexical grammar using the ANTLR [15]\nsoftware library."}, {"heading": "3.3 Grammar Construction", "text": "We use a slightly modified version of the lexical grammar we implemented for the file preprocessing stage to compute the frequency of use of each alpha and punctuation word present in the training set of a particular language (see section 2.2). The lexical grammar is language-aware, so that it knows to skip comments for each language and only include words in the actual code. Then we apply an additional lexical analysis pass, in which each alpha and punctuation token in the language training set is converted to either the specific matching keyword or a generic identifier depending on the frequency of the word in the training set (the threshold for a word to be considered as a keyword for a particular language is 1 %, ie the word has to be present in at least 1% of all the training files selected for the language).\nWe use the WEKA v. 3.7.13 [18] software package for feature extraction and selection: We first apply the StringToWordVector WEKA filter with the NGramTokenizer procedure on the lexer output for each training file. This filter extracts all possible unigrams, bigrams and trigrams from the normalized token stream and adds them to the global grammar production set. This set is further filtered to merely retain the most informative grammar productions. This is achieved by applying the InfoGainAttributeEval WEKA filter, which calculates the Mutual Information index of each feature in the set (see Equation 1) and filters out any feature with a MI index lower than a specified threshold value ( in our case 0.05).\nThe final grammar consists of 47,147 productions, 5,327 of which are unigrams, 18,687 bigrams and 23,133 trigrams.\n3.4 Classifier training and testing"}, {"heading": "3.5 Software implementation", "text": ""}, {"heading": "4. Results", "text": "Precision Recall F\nMaximum Entropy\n0.991 0.99 0.99\nNa\u00efve Bayesian\n0.965 0.967 0.965\nWith a resulting F score of 0.99 our method achieves a 2.1% improvement in accuracy on the best results published so far (see Table 1): Kennedy Van Dam [14] achieved an F-score of 0.969 while using a modified Kneser-Ney discounting classifier.\nOur results (see Table 3) were verified on a very diverse and large test set containing 147843 files and covering the most popular 29 programming languages."}, {"heading": "4.1. Results discussion", "text": "Results vary from programming language to programming language (see Table 3). We speculate the main contributing factors for misclassification (see Table 4) to be:\n Short polyglots.\nSome files are syntactically correct and equally probable in more than one language. Typically such files consist of a few lines of code. C/C++ and Objectice C header files provides a good example for this use case. Another example is given by Tcl commands that also exist in the Unix shell.\n Bidirectional embedding of one programming language into another.\nClassification seems to cope fairly well with unidirectional embedding like SQL embedding into other languages. Languages that can be embedded into each other are more challenging for the classifier. A good example is HTML and PHP. Another example is JavaScript and HTML."}, {"heading": "5. Conclusions", "text": "Our method shows that source code to programming language classification can be done in accordance with the criteria\nwe set out to define for a production-ready implementation in the Introduction section:\n1. Method achieves F=0.99 accuracy measured on 147843 source files collected from diverse sources.\n2. Method supports most popular 29 programming languages.\n3. Method is fully automated and does not require any knowledge of the programming languages it identifies.\n(Except for the grammar construction where we have used comments syntax knowledge)\n4. Method does not rely on programming language file extension or any other file metadata.\n5. Method is implemented with an average 0.1 sec per identification performance on a very modest server\nconfiguration.\nProgramming language grammatical rules have a recursive nature. In the future we would like to explore the possibility of using deep learning Recurrent Neural Networks to improve our results even further."}, {"heading": "Appendix A. Programming Language Grammar Definitions", "text": "Definitions below are quotes from the classical book \u201cCompilers: Principles, Techniques and Tools\u201d 2nd edition written by Aho, A., Lam, M., Sethi, R., Ullman, J., Cooper, K., Torczon, L., & Muchnick, S [17]\nContext Free Grammar [p. 42] - A context-free grammar has four components:\n1. A set of terminal symbols, sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the\nlanguage defined by the grammar.\n2. A set of nonterminals, sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of\nterminals, in a manner we shall describe.\n3. A set of productions, where each production consists of a nonterminal called the head or left side of the\nproduction, an arrow, and a sequence of terminals and/or nonterminals , called the body or right side of the production. The intuitive intent of a production is to specify one of the written forrms of a construct; if the head nonterminal represents a construct, then the body represents a written form of the construct.\n4. A designation of one of the nonterminals as the start symbol.\nLexical Analyzer [p. 43] - In a compiler, the lexical analyzer reads the characters of the source program,\ngroups them into lexically meaningful units called lexemes, and produces as output tokens presenting these lexemes. A token consists of two components, a token name and an attribute value. The token names are abstract symbols that are used by the parser for syntax analysis. Often, we shall call these token names terminals, since they appear as terminal symbols in the grammar for a programming language."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "7. REFERENCES [1] SyntaxHighlighter http://alexgorbatchev.com/SyntaxHighlighter/.\n[2] Highlight.js https://highlightjs.org/.\n[3] Google Code Prettify https://github.com/google/code-prettify.\n[4] SourceForge https://sourceforge.net/.\n[5] GitHub https://github.com/.\n[6] Linguist https://github.com/github/linguist.\n[7] Kernighan, Brian W., and Phillip J. Plauger. \"Programming style: Examples and counterexamples.\" ACM Computing\nSurveys (CSUR) 6.4 (1974): 303-319.\n[8] TIOBE index http://www.tiobe.com/tiobe-index/.\n[9] PYPL index http://pypl.github.io/PYPL.html.\n[10] Stack Overflow http://stackoverflow.com/\n[11] Ugurel, Secil, Robert Krovetz, and C. Lee Giles. \"What's the code?: automatic classification of source code\narchives.\" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.\n[12] Klein, David, Kyle Murray, and Simon Weber. \"Algorithmic programming language identification.\" arXiv preprint\narXiv:1106.4064 (2011).\n[13] Khasnabish, Jyotiska Nath, et al. \"Detecting Programming Language from Source Code Using Bayesian Learning\nTechniques.\" International Workshop on Machine Learning and Data Mining in Pattern Recognition. Springer International Publishing, 2014.\n[14] van Dam, Juriaan Kennedy, and Vadim Zaytsev. \"Software Language Identification with Natural Language\nClassifiers.\" 2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER). Vol. 1. IEEE, 2016.\n[15] Parr, Terence. The definitive ANTLR 4 reference. Pragmatic Bookshelf, 2013.\n[16] Nigam, Kamal, John Lafferty, and Andrew McCallum. \"Using maximum entropy for text classification.\" IJCAI-99\nworkshop on machine learning for information filtering. Vol. 1. 1999.\n[17] Aho, A., Lam, M., Sethi, R., Ullman, J., Cooper, K., Torczon, L., & Muchnick, S. (2007). Compilers: Principles,\nTechniques and Tools.\n[18] Witten, Ian H., and Eibe Frank. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann,\n2005.\n[19] Manning, Christopher D., et al. \"The Stanford CoreNLP Natural Language Processing Toolkit.\" ACL (System\nDemonstrations). 2014.\n[20] Ratnaparkhi, Adwait. \"A simple introduction to maximum entropy models for natural language processing.\" IRCS\nTechnical Reports Series (1997): 81.\n[21] Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty. \"Inducing features of random fields.\" IEEE transactions\non pattern analysis and machine intelligence 19.4 (1997): 380-393.\n[22] Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006."}], "references": [{"title": "Programming style: Examples and counterexamples.", "author": ["Kernighan", "Brian W", "Phillip J. Plauger"], "venue": "ACM Computing Surveys (CSUR)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1974}, {"title": "What's the code?: automatic classification of source code archives.", "author": ["Ugurel", "Secil", "Robert Krovetz", "C. Lee Giles"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Algorithmic programming language identification.", "author": ["Klein", "David", "Kyle Murray", "Simon Weber"], "venue": "arXiv preprint arXiv:1106.4064", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Detecting Programming Language from Source Code Using Bayesian Learning Techniques.\" International Workshop on Machine Learning and Data Mining in Pattern Recognition", "author": ["Khasnabish", "Jyotiska Nath"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Software Language Identification with Natural Language Classifiers.\" 2016", "author": ["van Dam", "Juriaan Kennedy", "Vadim Zaytsev"], "venue": "IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "The definitive ANTLR 4 reference", "author": ["Parr", "Terence"], "venue": "Pragmatic Bookshelf,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Using maximum entropy for text classification.\" IJCAI-99 workshop on machine learning for information", "author": ["Nigam", "Kamal", "John Lafferty", "Andrew McCallum"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Compilers: Principles, Techniques and Tools", "author": ["A. Aho", "M. Lam", "R. Sethi", "J. Ullman", "K. Cooper", "L. Torczon", "S. Muchnick"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["Witten", "Ian H", "Eibe Frank"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit.\" ACL (System Demonstrations)", "author": ["Manning", "Christopher D"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "A simple introduction to maximum entropy models for natural language processing.", "author": ["Ratnaparkhi", "Adwait"], "venue": "IRCS Technical Reports Series", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Inducing features of random fields.\" IEEE transactions on pattern analysis and machine intelligence", "author": ["Della Pietra", "Stephen", "Vincent Della Pietra", "John Lafferty"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Kernighan[7] \u201cComputer programs can be written many different ways and still achieve the same effect .", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "[11] Support Vector Machine Alphabetical sequences of characters separated by nonalphabetical characters 10 300 89.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[12] Statistical analysis Statistical features like percentage of lines with most common language keyword or brackets distribution 25 625 48.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[13] Multinomial Na\u00efve Bayes classifier Keyword counts 10 2392 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[14] Modified Kneser-Ney discounting classifier Unigrams, bigrams and trigrams of sequences of characters separated by white spaces 20 4000 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "For example \u201cC++ is pretty much impossible to parse by merely writing up its grammar\u201d \u2013 Terence Parr, the author of ANTLR [15].", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "As Aho [17] p.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "\" [16].", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "It can be shown [20],[21] that the conditional probability distribution ) | ( s l p that satisfies the constraints as described by Equation 2, and which has the form of Equation 3, will maximize the entropy ) ( p H .", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "It can be shown [20],[21] that the conditional probability distribution ) | ( s l p that satisfies the constraints as described by Equation 2, and which has the form of Equation 3, will maximize the entropy ) ( p H .", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "It can be also shown [20],[21] that the conditional probability distribution ) | ( s l p that meets the maximum entropy requirements (i.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "It can be also shown [20],[21] that the conditional probability distribution ) | ( s l p that meets the maximum entropy requirements (i.", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "1 Training and Test Data Selection Similarly to [12],[13] and [14] we sourced our training and test data from GitHub [5].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "1 Training and Test Data Selection Similarly to [12],[13] and [14] we sourced our training and test data from GitHub [5].", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "1 Training and Test Data Selection Similarly to [12],[13] and [14] we sourced our training and test data from GitHub [5].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "1 was implemented by defining a lexical grammar using the ANTLR [15] software library.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "13 [18] software package for feature extraction and selection: We first apply the StringToWordVector WEKA filter with the NGramTokenizer procedure on the lexer output for each training file.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "We use the Stanford NLP toolkit [19] v.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "1% improvement in accuracy on the best results published so far (see Table 1): Kennedy Van Dam [14] achieved an F-score of 0.", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": ", & Muchnick, S [17] Context Free Grammar [p.", "startOffset": 16, "endOffset": 20}], "year": 2017, "abstractText": "As of today the programming language of the vast majority of the published source code is manually specified or programmatically assigned based on the sole file extension. In this paper we show that the source code programming language identification task can be fully automated using machine learning techniques. We first define the criteria that a production-level automatic programming language identification solution should meet. Our criteria include accuracy, programming language coverage, extensibility and performance. We then describe our approach: How training files are preprocessed for extracting features that mimic grammar productions, and then how these extracted \u2018grammar productions\u2019 are used for the training and testing of our classifier. We achieve a 99% accuracy rate while classifying 29 of the most popular programming languages with a Maximum Entropy classifier.", "creator": "Microsoft\u00ae Word 2013"}}}