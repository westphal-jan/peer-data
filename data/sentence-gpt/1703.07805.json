{"id": "1703.07805", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Supervised Typing of Big Graphs using Semantic Embeddings", "abstract": "We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology. Finally, we illustrate the possibility of using a searchable subset of DBpedia instances in the DBpedia ontology, in addition to aggregating the relevant features and subregions of DBpedia instances by performing a searchable subset of DBpedia instances in our extended ontology.", "histories": [["v1", "Wed, 22 Mar 2017 18:20:07 GMT  (1504kb,D)", "http://arxiv.org/abs/1703.07805v1", "6 pages, to be published in Semantic Big Data Workshop at ACM, SIGMOD 2017; extended version in preparation for Open Journal of Semantic Web (OJSW)"]], "COMMENTS": "6 pages, to be published in Semantic Big Data Workshop at ACM, SIGMOD 2017; extended version in preparation for Open Journal of Semantic Web (OJSW)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mayank kejriwal", "pedro szekely"], "accepted": false, "id": "1703.07805"}, "pdf": {"name": "1703.07805.pdf", "metadata": {"source": "META", "title": "Supervised Typing of Big Graphs using Semantic Embeddings", "authors": ["Mayank Kejriwal", "Marina Del Rey", "Pedro Szekely"], "emails": ["kejriwal@isi.edu", "pszekely@isi.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Information systems\u2192 Data mining; \u2022Computing methodologies\u2192 Arti cial intelligence; Natural language processing;\nKEYWORDS Semantic Embeddings; Type recommendation; DBpedia\nACM Reference format: Mayank Kejriwal and Pedro Szekely. 2017. Supervised Typing of Big Graphs using Semantic Embeddings. In Proceedings of SBD\u201917, Chicago, IL, USA, May 19, 2017, 6 pages. DOI: h p://dx.doi.org/10.1145/3066911.3066918\nIn recent years, the distributional semantics paradigm has been used to great e ect in natural language processing (NLP) for embedding words in vector spaces [20]. e distributional hypothesis (also known as Firth\u2019s axiom) states that the meaning of a word is determined by its context [19]. Algorithms like word2vec use neural networks on large corpora of text to embed words in semantic vector spaces such that contextually similar words are close to each other in the vector space [12]. Simple arithmetic operations on\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SBD\u201917, Chicago, IL, USA \u00a9 2017 ACM. 978-1-4503-4987-1/17/05. . .$15.00 DOI: h p://dx.doi.org/10.1145/3066911.3066918\nsuch embeddings have yielded semantically consistent results (e.g. Kin\u0434 \u2212Man +Woman = Queen).\nRecent work has extended such neural embedding techniques, traditionally introduced only for natural language word sequences, to alternate kinds of data, including entities in large knowledge graphs like DBpedia [16, 17]. e basic approach is to convert an instance-rich knowledge graph into sets of sequences of graph nodes by performing random walks or using graph kernels [8]. NLP algorithms like word2vec are applied on the sequences to embed entities, just like words in natural language sentences [17].\nIn the Semantic Web, the domain of discourse is typically expressed by a manually curated ontology. A basic element of an ontology is a type, also called a class. Type assertion statements relate entities (i.e. instances) in a knowledge base (KB) to the domain ontology, which can then be used to infer more facts about entities.\nGiven the crucial role played by types in mediating between domain ontologies and instance-rich KBs, a natural question is whether types can be embedded in the same semantic vector space as entities, and whether data-driven type embeddings can be used to reason about, and visualize, ontologies. For example, one could use these embeddings to ask whether the data adequately capture ontological semantics (Section 3.3), and to recommend types for new entities in the knowledge base (Section 3.2). Unfortunately, type embeddings are di cult to directly derive from graphs because big knowledge graphs are sparse in type assertion statements compared to the number of unique instances and facts. In DBpedia, for example there are over 17 million (non-type) triples, and almost 4 million unique entities; the number of type assertion statements is also about 4 million, meaning that there is usually only one type\nar X\niv :1\n70 3.\n07 80\n5v 1\n[ cs\n.C L\n] 2\n2 M\nar 2\n01 7\nassertion per entity. In many cases, the type assertions can be trivial (e.g. owl# ing). Another problem is that types are typically asserted as objects, not subjects, in the KB; hence, a random walk cannot be initiated from a type node.\nWe propose a scalable proof-of-concept solution to the problem of deriving type embeddings from entity embeddings in big graphs. A visual intuition behind the method is provided in Figure 1. Given a set of pre-generated entity embeddings, and a sparse collection of type assertion triples, we are able to robustly generate embeddings for a set of types (e.g. Fruit, Building). We use these embeddings for a variety of tasks, most notably probabilistic type recommendation (e.g. recommending types such as Fruit and Plant for a new entity like Grape), intensional semantics visualization, and probabilistic type clustering over large graphs (Section 3). Speci c empirical highlights are noted below. To the best of our knowledge, this is the rst work that presents a feature-agnostic supervised typing of Big Graphs.\nEmpirical Highlights: Preliminary empirical results on a partition of DBpedia show that our algorithm achieved run-time speedups by more than a factor of 15 on the type recommendation task compared to non-parametric nearest-neighbors baselines, with superior recall on two relevance criteria. Visualizations of the type embeddings using the DBpedia ontology as a ground-truth show that they are in good agreement with the intensional semantics expressed by the ontology. e scalability of the model enabled us to probabilistically cluster almost 4 million DBpedia instances into 415 types on a serial machine in under 50 hours."}, {"heading": "1 RELATEDWORK", "text": "Semantic vector space embeddings have witnessed much research in recent years, with neural word embedding algorithms (e.g. word2vec [12] and glove [14]) achieving state-of-the-art performance on a number of NLP tasks (e.g. dependency parsing) [2]. e success of word embedding approaches has led to a renewed interest in graphbased communities for embedding graphs. A famous example is DeepWalk, which applies word embedding techniques on random walk sequences on a graph to embed nodes in the graph to vectors [15]. In the Semantic Web, variants of this strategy were recently applied to DBpedia and Wikidata, and the embedded entities were used in several important problems, include content-based recommendation and node classi cation [17],[18]. Some other in uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5]. An important aspect of this research is automatic knowledge base construction and completion (AKBC), to which this work is related [21], [4]. A major di erence is that, because of an additional layer of semantic abstraction (types vs. entities), we can a ord to infer types without incrementally training the model such as in [6] or any other details of how the entity embeddings were derived. We also do not rely on natural language context of any kind [3].\nIn this paper, we do not seek to develop a new learning algorithm for graph (including knowledge graph) or word embeddings; instead, the goal is to use an existing publicly available set of graph entity embeddings to extensionally model types. To the best of our knowledge, this is the rst paper to derive the embedding of schema-level elements (like types) directly using the embeddings of\ninstance-level elements like entities. Because our method does not make underlying assumptions about the entity embeddings, it is general and can be applied to any set of entity embeddings.\ne type recommendation problem to which we apply the type models is closely connected to the type prediction problem studied in prior work, a good example being Typi er [9]. Unlike Typi er, which is not embedding-based and relies on manually devised features (e.g. data and pseudo-schema features [9]), our approach is feature-agnostic. Other examples of feature-centric type recommenders are the systems in [11], [13]. Due to the di culty in automating feature construction, feature-agnostic systems are still quite rare; for example, in the Semantic Web, only a recent work achieved competitive performance at scale for feature-agnostic node classi cation [17]. Furthermore, because embeddings are a very general framework, we use the embeddings not just for type embeddings but also visualization and online clustering, which cannot be handled by the other (special-purpose) type recommenders."}, {"heading": "2 APPROACH", "text": ""}, {"heading": "2.1 Framework", "text": "We lay the groundwork for this section by formally de ning a typed knowledge base (t-KB) and related terminology below:\nDe nition. Given a set I of Internationalized Resource Identiers (IRIs), a set B of blank nodes and a set L of literals, a typed RDF Knowledge Base T is a set of RDF triples (i.e. \u2286 {I \u222aB}\u00d7I \u00d7 {I \u222a B \u222a L}) such that \u2200(s,p,o) \u2208 T \u2192 \u2203t \u2208 I, (s, : type, t) \u2208 T , where : type \u2208 I is a special type property (e.g. rdf:type).\nWe denote (s, : type, t) as a type assertion statement, an arbitrary element s from the entity set S = {s |(s,p,o) \u2208 T } as an entity, and the set T (s) = {t |(s, : type, t) \u2208 T } as its set of asserted types1. Similar to the entity set S , we denote T = \u22c3 s \u2208S T (s) as the type set of knowledge base T . Finally, we denote a type-only KB (t-KB) T \u2032 as the subset of the typed knowledge base T that contains exactly the type assertions in T . Although each entity s is represented by an IRI per the RDF data model, an alternate representation is as an embedding in a real-valued d-dimensional space:\nDe nition (entity embedding). A d-dimensional entity embedding representation of an entity s is a mapping from s to a real-valued vector \u2212\u2192s that is constrained to lie on the unit-radius hypersphere in d-dimensional space.\ne constraint in the de nition above (\u03a3i\u2212\u2192s [i]2 = 1) ensures that the entity embedding is l2-normalized, which simpli es distance calculations considerably by equating cosine similarity between the two vectors with a dot product.\nConcerning the actual learning of entity embeddings, we noted in Section 1 that recent work has successfully managed to learn embeddings (in spaces with only a few hundred dimensions) on large datasets like DBpedia by applying neural embedding algorithms like word2vec on graph node sequences [17]. Typically, such embeddings do not include types. A practical reason is that, in big knowledge graphs like DBpedia, the graphs T \u2032 and T \u2212 T \u2032 are released as separate les (we provide links in Section 3), and entity embeddings are only trained over the la er. A more serious reason,\n1At present, we take an extensional (or instance-driven) view of a type by identifying it by its referents (the set of explicitly declared instances) of an entity s . We investigate an empirical relaxation of this condition in Section 3.\nAlgorithm 1 Generate Type Embeddings\nInput: Sets S and \u00aeS of entities and entity embeddings, type-only Knowledge Base T \u2032 Output: Type embedding \u2212\u2192t for each type t in T \u2032\n(1) Initialize empty dictionary TS where keys are entities and values are type-sets (2) Initialize type-set T of T \u2032 to the empty set // First pass through T \u2032: collect entity-type statistics\n(3) for all triples (s, : type, t) \u2208 T \u2032 such that \u2212\u2192s \u2208 \u00aeS do Add t to T Add t to TS [s], if it does not already exist (4) end for (5) for all s \u2208 keys(TS ), setTS [s] = |TS [s]| to save memory\nend for //Second pass through T \u2032 to derive type embeddings\n(6) Initialize Mean parameter dictionary M such that keys(M) = T , and each value in M is \u00ae0 (7) for all triples (s, : type, t) \u2208 T \u2032 such that s \u2208 S do Update M[t] using Equation 1, using T (s) = TS [s] (8) end for //Derive type embedding from \u2212\u2192\u00b5t (9) for all types t \u2208 keys(M) do Let type embedding \u2212\u2192t be the projection of M[t] on d-dimensional hypersphere with unit radius (divide throughout by | |M[t]| |2)\n(10) end for (11) return type embeddings derived in last step\npointed out in the introduction, is sparsity of assertions and the observation that T \u2229 S is typically empty.\nTo address problems of sparsity and robustness, we a empt to embed types into the same vector space as entities (thereby leveraging the enormous context of entities). Formally, we de ne a type embedding below.\nDe nition (type embedding). Given an entity set S and a type t , a d-dimensional type embedding representation of t is a mapping from t to a real-valued vector \u2212\u2192t that is constrained to lie on the unit-radius hypersphere in d-dimensional space.\nIntuitively, a \u2018good\u2019 type embedding should have two elements: (1) be close in the vector space to entities that have that type. In Figure 1 (b), for example, Fruit is much closer to Apple than it is to Times Square; (2) be closer to \u2018related\u2019 types than to types that are unrelated. In Figure 1 (b), Building, City and Place are all closer to one another than to either Fruit or Plant.\nClearly, the two elements above are related as they strongly rely on the data and on the context in which types are asserted or co-appear with entities that share context. In the next section, we explore how such robust embeddings can be scalably generated."}, {"heading": "2.2 Generating Type Embeddings", "text": "We propose a type embedding algorithm that is lightweight both in terms of run-time and memory. Algorithm 1 provides the\npseudocode for our solution. Before describing the pseudocode, we describe the intuition as follows.\nAlgorithm 1 relies on two assumptions: rst, a type is ultimately described by its entities. is means that, all things the same, a type should be close to as many entities having that type as possible. Second, a type should give more preference to entities that describe it exclusively. For example, suppose an entity s1 has more than ten (explicitly declared) type assertions {t1, t2 . . . t10} while entity s2 only has two type assertions {t1, t2}. Algorithm 1 is set up so that s2 will contribute more to the derivation of t1 and t2 embeddings than s1.\nTo operationalize these assumptions, while still being simple and scalable, Algorithm 1 computes a weighted average of entity embeddings to derive a type embedding. Speci cally, in a rst pass over a type-only KB T \u2032, the algorithm computes the number of types |T (s)| asserted by entity s . For each new type encountered, the algorithm initializes a zero vector for the type, in the same d-dimensional space as the entity embeddings. Even with millions of entities, this information can be stored in memory at li le cost.\ne second pass of the algorithm is incremental. For each triple (s, : type, t), we update a type vector \u2212\u2192t (initialized to \u2212\u21920 ) using the equation:\n\u2212\u2192 t new = \u2212\u2192 t old + 1 |T (s)| \u2212\u2192s (1)\nIn the notation of the algorithm, TS [s] = T (s). Line 9 in Algorithm 1 shows a simple way of obtaining the nal type embedding \u2212\u2192t by normalizing the \u2018 nal\u2019 mean vector \u2212\u2192t new so that it lies on the unitradius (d-dimensional) hypersphere. Normalizing ensures that the type embedding obeys the same constraints as the original entity embeddings and conforms to the type embedding de nition earlier stated. A second reason for normalizing is that the computation of the cosine similarity between any vectors (whether type or entity) on the d-dimensional hypersphere reduces to the computation of the dot product between the vectors. e next section covers two applications of such computations."}, {"heading": "2.3 Applications", "text": "Type Recommendation. Given the type embeddings generated in the previous section, we can recommend types (with scores) for an untyped entity s by computing the dot product between the embedding \u2212\u2192s of the entity and each of the |T | type embeddings derived in Algorithm 1, and normalizing over the |T | results (if a valid probability distribution is desired). e probability distribution can also be used to rank a set of types with the highest-ranked type being the most relevant suggestion for the entity. Other than \u2018completing\u2019 knowledge bases with many untyped entities, such rankings can also assist in tasks like semantic search.\nOnline Clustering. In contrast with other update-based machine learning algorithms like Stochastic Gradient Descent, we note that data can be discarded once seen, making Algorithm 1 amenable to streaming se ings. For example, we were able to probabilistically cluster the full set of DBpedia instances (yielding a >100 GB results le) in less than 2 days in a single-threaded computing environment."}, {"heading": "3 PRELIMINARY EXPERIMENTS", "text": "e goal of this section is to illustrate the promise of the approach through some preliminary results. In Section 4, we discuss future directions based on the preliminary results."}, {"heading": "3.1 Preliminaries", "text": "Datasets: We construct ve evaluation datasets by performing random strati ed partitioning on the full set of DBpedia triples. We used the publicly available type-only KB2 for our experiments from the October 2015 release of the English-language DBpedia. is le contains all the type assertions obtained only from the mappingbased extractions, without transitive closure using the ontology. Details of the ve ground-truth datasets are provided in Table 1. Across the full ve-dataset partition, there are 3,963,983 unique instances and 415 unique types. e ve datasets are roughly uniform in their representation of the overall dataset, and not subject to spli ing or dataset bias, owing to random strati ed partitioning. In Section 3.3, we also compare our type embeddings to the DBpedia domain ontology3.\nEntity Embeddings: e algorithms presented in this paper assume that a set of entity embeddings has already been generated. Recently, [17] publicly made available 500-dimensional embeddings for DBpedia entities by using the word2vec algorithm on graph node sequences4. e word2vec model was trained using skip-gram, and was found to perform well on a range of node classi cation tasks5. Rather than generate our own entity embeddings (which could potentially cause bias by over ing to the type modeling task), we used those previously generated embeddings for all experiments in this paper.\nImplementation: All experiments in this paper were run on a serial iMac with a 4 GHz Intel core i7 processor and 32 GB RAM. All code was wri en in the Python programming language. We used the gensim package6 for accessing, manipulating and computing similarity on the entity embeddings.\n2Accessed at h p://downloads.dbpedia.org/2015-10/core-i18n/en/instance types en. l.bz2 3Downloaded from h p://downloads.dbpedia.org/2015-10/dbpedia 2015-10.nt 4Accessed at h p://data.dws.informatik.uni-mannheim.de/rdf2vec/models/DBpedia 5 e authors also released Wikidata embeddings, which did not do as well on node classi cation and were noisier (and much larger) than the DBpedia embeddings. For this reason, we do not consider the Wikidata embeddings in this paper. 6h ps://pypi.python.org/pypi/gensim"}, {"heading": "1. (a) plots the averageRecall@k across all ve experimental runs, while (b) illustrates average number of recommended types per entity for each of the datasets and baselines.", "text": ""}, {"heading": "3.2 Probabilistic Type Recommendation", "text": "In this experiment, we evaluate Algorithm 1 on the probabilistic type recommendation task. e input to the recommendation system is an entity, and the output must be a set of scored (or ranked) types that are topically relevant to the entity. e issue of relevance, by its nature, is subjective; we present a methodology for evaluating subjective relevance shortly.\nBaseline: We employ baselines based on weighted k Nearest Neighbors (kNN). e kNN algorithm is a strong baseline with some excellent theoretical properties: for example, even the 1NN algorithm is known to guarantee an error rate that is at most twice the Bayes error rate7 in a given feature space and for a given distance function in the space. Because the entity embeddings are given, the kNN baseline, just like the embedding method, is feature-agnostic; to the best of our knowledge, it is the only baseline that has this property and is not super-linear. We consider three versions with k set to 1, 5 and 10 respectively.\nBaseline Complexity: Compared to the embedding method, kNN has high time and memory complexity since it is non-parametric and requires storing all the (training) entities in memory. In contrast, a er computing the type embeddings, the embedding method has to store |T | vectors, one for each type. In terms of run-time, a full pass is required over the training dataset for each new test entity, regardless of the value of k . We explore the empirical consequence of this in our results.\nExperiment 1: We perform ve experiments, where in each experiment, four partitions (from Table 1) are used as the training set, and 5000 entities are randomly sampled from the remaining partition and used as the test set. We were forced to constrain the test set to 5000 for this initial experiment because of baseline complexity.\nis experiment adopts an extremely strict de nition of relevance: namely, the only relevant types are the ones that are explicitly asserted for the entity in the test partition. us, even a super-type (of the true asserted type) would be marked as \u2018irrelevant\u2019 if not explicitly asserted itself, since we do not consider transitive closures in the type-only KB. Although a strict measure, it provides a reasonable rst benchmark, as it conforms to extensional semantics.\n7 is is the minimum possible error for a particular distribution of data.\nWe evaluate performance using the Recall@k8 measure from Information Retrieval. Recall@k computes, for each rank k in a ranked list of types, the ratio of true positives to the sum of true positives and false negatives. For each one of the ve experimental runs, we compute a single Recall@k measure (for each k) by averaging the Recall@k over all 5000 entities. Finally, we compute the average across the experimental runs, and plot the results in Figure 2 (a). We also plo ed individual plots for all ve experimental runs, which turned out to be qualitatively very similar to Figure 2 (a). We omit those gures and a per-run analysis herein due to space constraints.\nAnalysis: Figure 2 (a) shows that, even with the strict de nition of relevance, although the embedding method starts out with low recall at the highest ranks, it converges with the other methods fairly quickly (between ranks 3-13). Figure 2 (b) shows that the baselines return very few non-zero recommendations per entity (fewer than 1.5) and the returned number is sub-linear in k: in this respect, the baselines perform \u2018hard\u2019 type predictions rather than graded recommendations. In contrast, the embedding method returns a more nuanced probability distribution over the 415 types (per entity), and is more apt for recommendations, as we show in the next experiment.\nExperiment 2: Although Experiment 1 is appropriate for determining the extensional types of an entity, it takes an overly strict re ection of relevance. Considering the number of triples and unique instances in Table 1, there was usually only one extensional type asserted in the knowledge base. For a be er judgment of relevance, we randomly sampled 100 instances from D-1 and pruned the ranked type lists for 10NN (clearly the best performing method in Figure 2 (a)) and the embedding method to 10. Because the number of returned types for 10NN was o en fewer than 10, we randomly \u2018padded\u2019 the rest of the list with DBpedia types, and manually counted the number of topically relevant recommendations in each (10-element) list9. is allows us to compute a single-point Recall@10 score over the 100 sampled entities. Note that the random padding can only help, not hurt, the Recall@10 score of the baseline. We also asked the annotator to do a side-by-side comparison of the two lists (for each of the 100 entities) and mark the list that is more topically useful overall. e annotator was not given the details of the two ranking algorithms; also, all annotations were conducted within a single short time-span.\nAnalysis: e single-point Recall@10 was 0.4712 for the embedding method (averaged across the 100 manually annotated samples) and 0.1423 for the 10NN with standard deviations of 0.2593 and 0.0805 respectively. Some representative results for both 10NN and the embedding method are presented in Table 2. e table presents some intuitive evidence that types recommended by the embedding method are more topically relevant than the types recommended by 10NN. e annotator found the embedding method to be topically more useful for 99 of the 100 entities, with 10NN more useful on only a single entity.\nRun-times: Concerning the empirical run-times, all baseline methods required about 1 hour to process 5000 test instances for the 4-partition training set while the embedding method only required\n8k in Recall@k should not be confused with k in kNN. 9 e annotations were performed externally, not by the authors.\n4 minutes. If the training set is xed, all methods were found to exhibit linear dependence on the size of the test set. is illustrates why we were forced to sample 5000 test instances (per experimental run) for evaluating kNN, since predicting types on only one of the full ve partitions in Table 1 would take about 150 hours, which is untenable, even if only strict type predictions (i.e. assertions) are of interest."}, {"heading": "3.3 Visualizing the Extensional Model", "text": "Note that our methods never relied on the DBpedia ontology when deriving embeddings and generative model parameters. However, an interesting experiment is to use the intensional semantics of types (e.g. sub-class relationships in the ontology) to visualize the embeddings derived from extensional assertions (the mappingbased type assertion extractions). We perform two visualization experiments using the unsupervised t-SNE algorithm [10], a stateof-the-art tool for visualizing high-dimensional points on a 2-D plot.\nVisualization 1: We apply t-SNE on a matrix containing the type embedding vectors of all direct sub-types of ve sampled types from the DBpedia ontology, namely Animal, Politican, MusicalWork, Building and Plant. e t-SNE algorithm takes the matrix as input and returns another matrix with the same number of rows but only 2 columns. We plot these points (rows) in Figure 3 (a), by\nassigning labels (i.e. colors) to points10 based on their super-type. e results show ve well-separated clusters, with each cluster representing a super-type. In this case, the extensional model is in excellent agreement with the intensional model. Moreover, the clusters also demonstrate other interesting aspects not captured intensionally: e.g. Building, Politician and MusicalWork (arti cial constructs) are much closer to each other, than they are to Animal and Plant (natural constructs), which form a separate \u2018super\u2019 cluster.\nVisualization 2: We re-run the experiment but over sub-type embeddings of the types SportsTeam, SportsLeague, Company, Organisation and EducationalInstitution. Note that this set is much more topically coherent than the earlier set. e 2D visualization is illustrated in Figure 3 (b); the topical coherence (there are now two clusters rather than ve) is well-re ected in the gure. e two purple \u2018outliers\u2019 on the le cluster are the embeddings for SportsTeam and SportsLeague, which are themselves sub-types of Organisation."}, {"heading": "3.4 Online Type Clustering of DBpedia", "text": "e results of Experiment 2 in Section 3.2 illustrated that there are many types in the DBpedia ontology that are clearly related to an entity of interest, even if they are not super-types (or even sub-types) of the entity\u2019s type. Given an entity, we ideally want to assign a probability to each type. Such a clustering has much potential, including topic modeling of entities and fuzzy reasoning. To achieve a full fuzzy clustering over DBpedia, we used the union of all ve datasets in Table 1 to compute our type embeddings, and then executed the fuzzy clustering algorithm over all DBpedia entities in the union of all ve datasets. e algorithm scaled nearlinearly and was able to nish executing over almost 4 million entities and 415 clusters (types) in about 50 hours, collecting over 100 GB of data. We will make this data available on a public server in the near future. Overall, the results (using the extensional type assertions as ground-truth) were found to be highly correlated to the results in Figure 2 (a). We are currently in the process of conducting detailed probabilistic analyses on this data, and are also clustering Wikidata embeddings, a much larger dataset than DBpedia."}, {"heading": "4 CONCLUSION", "text": "In this paper, we developed a framework for deriving type embeddings in the same space as a given set of entity embeddings. We devised a scalable data-driven algorithm for inferring the type embeddings, and applied the algorithm to a probabilistic type recommendation task on ve DBpedia partitions. Compared to a kNN baseline, the algorithm yields be er results on various relevance criteria, and is signi cantly faster. Visualizations also show that clusters over the type embeddings are intensionally consistent.\nFuture Work. ere are many avenues for future work that we have already started exploring. First, we are using the methods in this paper to embed entire ontologies (collections of types and properties) into vector spaces, to enable a combination of distributional and ontological semantic reasoning. Second, we are exploring more applications of embedded types, such as an enhanced version of semantic search, and semantically guided information extraction from\n10Because t-SNE is unsupervised, it never accessed the labels during the clustering.\nstructured data. Last, but not least, we are conducting broader empirical studies e.g. on datasets other than DBpedia and using knowledge graph embeddings other than the DeepWalk-based RDF2Vec to test the robustness of our type embedding approach to such variations. We are also testing the hypothesis that deriving type embeddings from entity embeddings yields higher quality typing than treating types as part of a knowledge graph and jointly deriving entity and type embeddings. We are also looking to carry out a broader user study than the preliminary study in Experiment 2."}], "references": [{"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in neural information processing systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Entity Typing using Distributional Semantics and DBpedia", "author": ["Marieke van Erp", "Piek Vossen"], "venue": "In Under Review", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Applying Universal Schemas for Domain Speci\u0080c Ontology Expansion", "author": ["Paul Groth", "Sujit Pal", "Darin McBeath", "Brad Allen", "Ron Daniel"], "venue": "Proceedings of AKBC", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Semantically Smooth Knowledge Graph Embedding", "author": ["Shu Guo", "\u008ban Wang", "Bin Wang", "Lihong Wang", "Li Guo"], "venue": "In ACL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "PSDVec: A toolbox for incremental and scalable word embedding", "author": ["Shaohua Li", "Jun Zhu", "Chunyan Miao"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In AAAI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Graph kernels for RDF data", "author": ["Uta L\u00f6sch", "Stephan Bloehdorn", "Achim Re\u008ainger"], "venue": "In Extended Semantic Web Conference", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Typi\u0080er: Inferring the type semantics of structured data", "author": ["Yongtao Ma", "\u008canh Tran", "Veli Bicer"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geo\u0082rey Hinton"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Type prediction in RDF knowledge bases using hierarchical multilabel classi\u0080cation", "author": ["Andr\u00e9 Melo", "Heiko Paulheim", "Johanna V\u00f6lker"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Je\u0082 Dean"], "venue": "In Advances in neural information processing systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Type inference on noisy rdf data", "author": ["Heiko Paulheim", "Christian Bizer"], "venue": "In International Semantic Web Conference", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Graph classi\u0080cation and clustering based on vector space embedding. World Scienti\u0080c Publishing Co., Inc", "author": ["Kaspar Riesen", "Horst Bunke"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Rdf2vec: Rdf graph embeddings for data mining", "author": ["Petar Ristoski", "Heiko Paulheim"], "venue": "In International Semantic Web Conference", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "RDF graph embeddings for content-based recommender systems", "author": ["Jessica Rosati", "Petar Ristoski", "Tommaso Di Noia", "Renato de Leone", "Heiko Paulheim"], "venue": "In CEUR workshop proceedings,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "\u008ce distributional hypothesis", "author": ["Magnus Sahlgren"], "venue": "Italian Journal of Linguistics 20,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Knowledge Base Completion Using Embeddings and Rules", "author": ["\u008ban Wang", "Bin Wang", "Li Guo"], "venue": "In IJCAI", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "InAAAI. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "In recent years, the distributional semantics paradigm has been used to great e\u0082ect in natural language processing (NLP) for embedding words in vector spaces [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 18, "context": "\u008ce distributional hypothesis (also known as Firth\u2019s axiom) states that the meaning of a word is determined by its context [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "Algorithms like word2vec use neural networks on large corpora of text to embed words in semantic vector spaces such that contextually similar words are close to each other in the vector space [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "Recent work has extended such neural embedding techniques, traditionally introduced only for natural language word sequences, to alternate kinds of data, including entities in large knowledge graphs like DBpedia [16, 17].", "startOffset": 212, "endOffset": 220}, {"referenceID": 16, "context": "Recent work has extended such neural embedding techniques, traditionally introduced only for natural language word sequences, to alternate kinds of data, including entities in large knowledge graphs like DBpedia [16, 17].", "startOffset": 212, "endOffset": 220}, {"referenceID": 7, "context": "\u008ce basic approach is to convert an instance-rich knowledge graph into sets of sequences of graph nodes by performing random walks or using graph kernels [8].", "startOffset": 153, "endOffset": 156}, {"referenceID": 16, "context": "NLP algorithms like word2vec are applied on the sequences to embed entities, just like words in natural language sentences [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "word2vec [12] and glove [14]) achieving state-of-the-art performance on a number of NLP tasks (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "word2vec [12] and glove [14]) achieving state-of-the-art performance on a number of NLP tasks (e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "dependency parsing) [2].", "startOffset": 20, "endOffset": 23}, {"referenceID": 14, "context": "A famous example is DeepWalk, which applies word embedding techniques on random walk sequences on a graph to embed nodes in the graph to vectors [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 16, "context": "In the Semantic Web, variants of this strategy were recently applied to DBpedia and Wikidata, and the embedded entities were used in several important problems, include content-based recommendation and node classi\u0080cation [17],[18].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": "In the Semantic Web, variants of this strategy were recently applied to DBpedia and Wikidata, and the embedded entities were used in several important problems, include content-based recommendation and node classi\u0080cation [17],[18].", "startOffset": 226, "endOffset": 230}, {"referenceID": 6, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 21, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Some other in\u0083uential examples of such knowledge graph embeddings (KGEs), which is an active area of research, include (but are not limited to) [7], [22], [1], [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 20, "context": "An important aspect of this research is automatic knowledge base construction and completion (AKBC), to which this work is related [21], [4].", "startOffset": 131, "endOffset": 135}, {"referenceID": 3, "context": "An important aspect of this research is automatic knowledge base construction and completion (AKBC), to which this work is related [21], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "entities), we can a\u0082ord to infer types without incrementally training the model such as in [6] or any other details of how the entity embeddings were derived.", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "We also do not rely on natural language context of any kind [3].", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "in prior work, a good example being Typi\u0080er [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "data and pseudo-schema features [9]), our approach is feature-agnostic.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "ommenders are the systems in [11], [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "ommenders are the systems in [11], [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "Due to the di\u0081culty in automating feature construction, feature-agnostic systems are still quite rare; for example, in the Semantic Web, only a recent work achieved competitive performance at scale for feature-agnostic node classi\u0080cation [17].", "startOffset": 238, "endOffset": 242}, {"referenceID": 16, "context": "Concerning the actual learning of entity embeddings, we noted in Section 1 that recent work has successfully managed to learn embeddings (in spaces with only a few hundred dimensions) on large datasets like DBpedia by applying neural embedding algorithms like word2vec on graph node sequences [17].", "startOffset": 293, "endOffset": 297}, {"referenceID": 16, "context": "Recently, [17] publicly made available 500-dimensional embeddings for DBpedia entities by using the word2vec algorithm on graph node sequences4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "We perform two visualization experiments using the unsupervised t-SNE algorithm [10], a stateof-the-art tool for visualizing high-dimensional points on a 2-D plot.", "startOffset": 80, "endOffset": 84}], "year": 2017, "abstractText": "We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. \u008ce algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.", "creator": "LaTeX with hyperref package"}}}