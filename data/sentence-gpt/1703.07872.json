{"id": "1703.07872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Random Features for Compositional Kernels", "abstract": "We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 22 Mar 2017 22:05:04 GMT  (92kb,D)", "http://arxiv.org/abs/1703.07872v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "roy frostig", "vineet gupta", "yoram singer"], "accepted": false, "id": "1703.07872"}, "pdf": {"name": "1703.07872.pdf", "metadata": {"source": "CRF", "title": "Random Features for Compositional Kernels", "authors": ["Amit Daniely", "Roy Frostig", "Vineet Gupta", "Yoram Singer"], "emails": ["singer}@google.com"], "sections": [{"heading": null, "text": "\u2217Google Brain, {amitdaniely, frostig, vineet, singer}@google.com\nar X\niv :1\n70 3.\n07 87\n2v 1\n[ cs\n.L G\n] 2\n2 M\nar 2"}, {"heading": "1 Introduction", "text": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks. Learning using kernel representations amounts to convex optimization with provable convergence guarantees. The first generation of kernel functions in machine learning were oblivious to spatial or temporal characteristics of input spaces such as text, speech, and images. A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9]. Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].\nWhile the \u201ckernel trick\u201d unleashes the power of convex optimization, it comes with a large computational cost as it requires storing or repeatedly computing kernel products between pairs of examples. Rahimi and Recht [16] described and analyzed an elegant and computationally effective way that mitigates this problem by generating random features that approximate certain kernels. Their work was extended to various other kernels [10, 15, 2, 1].\nIn this paper we describe and analyze a simple random feature generation scheme from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of neural networks. The kernels\u2019 definition and the connection to networks was developed in [6, 5]. Our feature map construction has several benefits over previous ones. It naturally exploits hierarchical structure in terms of representation power. The random feature generation is computationally efficient. More importantly, computing the feature map is efficient and often can be performed in time linear in the embedding dimension. Last but not least, computing the feature map requires highly sparse access patterns to the input, implying low memory requirements in the process.\nThe course of the paper is as follows. After a brief background, we start the paper by recapping in Sec. 3 the notion of random features schemes (RFS). Informally speaking, a random feature scheme is an embedding from an input space into the real or complex numbers. The scheme is random such that multiple instantiations result in different mappings. Standard inner products in the embedded space emulate a kernel function and converge to the inner product that the kernel defines. We conclude the section with a derivation of concentration bounds for kernel approximation by RFS and a generalization bound for learning with RFS.\nThe subsequent sections provide the algorithmic core of the paper. In Sec. 4 we describe RFS for basic spaces such as {\u22121,+1}, [n], and T. We show that the standard inner product on the sphere in one and two dimensions admits an effective norm-efficient RFS. However, any RFS for Sd\u22121 where d \u2265 3 is norm-deficient. In Sec. 5, we discuss how to build random feature schemes from compositional kernels that are described by a computation skeleton, which is an annotated directed acyclic graph. The base spaces constitute the initial nodes of the skeleton. As the name implies, a compositional kernel consists of a succession of compositions of prior constructed kernels, each of which is by itself a compositional kernel or a base kernel. We conclude the section with run-time and sparsity-level analysis.\nThe end result of our construction is a lightweight yet flexible feature generation procedure. Each random feature can be represented as an algebraic expression over of a small number of (random) paths in a composition tree. Thus, compositional random features\ncan be stored very compactly. The discrete nature of the generation process enables deduplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. The latter property cannot be directly achieved by previously studied random feature schemes. We would like to emphasize that our approach does not stand in contrast to, but rather complements, prior work. Indeed, the base kernels of a compositional kernel can be non-elementary such as the Gaussian kernel, and hence our RFS can be used in conjunction with the well-studied RFS of [16] for Gaussian kernels. One can also envision a hybrid structure where the base kernels are defined, for example, through a PSD matrix obtained by metric learning on the original input space."}, {"heading": "2 Background and notation", "text": "We start with a few notational conventions used throughout the paper. The Hilbert spaces we consider are over the reals. This includes spaces that are usually treated as complex Hilbert spaces. For example, for z = a+ ib, z\u2032 = a\u2032 + ib\u2032 \u2208 C we denote \u3008z, z\u2032\u3009 = Re(zz\u0304\u2032) = aa\u2032 + bb\u2032 (rather than the more standard \u3008z, z\u2032\u3009 = zz\u0304\u2032). Likewise, for z, z\u2032 \u2208 Cq we denote \u3008z, z\u2032\u3009 =\u2211q\ni=1\u3008zi, z\u2032i\u3009. For a measure space (\u2126, \u00b5), L2(\u2126) denotes the space of square integrable functions f : \u2126 \u2192 C. For f, g \u2208 L2(\u2126) we denote \u3008f, g\u3009L2(\u2126) = \u222b \u2126 \u3008f(x), g(x)\u3009d\u00b5(x). For all the measurable spaces we consider, we assume that singletons are measurable. We denote T = {z \u2208 C : |z| = 1}.\nNext, we introduce the notation for kernel spaces and recap some of their properties. A kernel is a function k : X \u00d7 X \u2192 R such that for every x1, . . . ,xm \u2208 X the matrix {k(xi,xj)}i,j is positive semi-definite. We say that k is D-bounded if k(x,x) \u2264 D2 for every x \u2208 X . We call k a normalized kernel if k(x,x) = 1 for every x \u2208 X . We will always assume that kernels are normalized. A kernel space is a Hilbert space H of functions from X to R such that for every x \u2208 X the linear functional f \u2208 H 7\u2192 f(x) is bounded. The following theorem describes a one-to-one correspondence between kernels and kernel spaces.\nTheorem 1. For every kernel k there exists a unique kernel space Hk such that for every x,x\u2032 \u2208 X , k(x,x\u2032) = \u3008k(\u00b7,x), k(\u00b7,x\u2032)\u3009Hk . Likewise, for every kernel space H there is a kernel k for which H = Hk.\nThe following theorem underscores a tight connection between kernels and embeddings of X into Hilbert spaces.\nTheorem 2. A function k : X \u00d7 X \u2192 R is a kernel if and only if there exists a mapping \u03c6 : X \u2192 H to some Hilbert space for which k(x,x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009H. In this case, Hk = {fv | v \u2208 H} where fv(x) = \u3008v, \u03c6(x)\u3009H. Furthermore, \u2016f\u2016Hk = min{\u2016v\u2016H | f = fv} and the minimizer is unique.\nWe say that f : [\u22121, 1]\u2192 R is a normalized positive semi-definite (PSD) function if\nf(\u03c1) = \u221e\u2211 i=0 ai\u03c1 i where \u221e\u2211 i=0 ai = 1, \u2200i : ai \u2265 0 .\nNote that f is PSD if and only if f \u25e6 k is a normalized kernel for any normalized kernel k."}, {"heading": "3 Random feature schemes", "text": "Let X be a measurable space and let k : X \u00d7 X \u2192 R be a normalized kernel. A random features scheme (RFS) for k is a pair (\u03c8, \u00b5) where \u00b5 is a probability measure on a measurable space \u2126, and \u03c8 : \u2126\u00d7X \u2192 C is a measurable function, such that\n\u2200x,x\u2032 \u2208 X , k(x,x\u2032) = E \u03c9\u223c\u00b5\n[ \u03c8(\u03c9,x)\u03c8(\u03c9,x\u2032) ] .\nSince the kernel is real valued, we have in this case that,\nk(x,x\u2032) = Re (k(x,x\u2032))\n= E \u03c9\u223c\u00b5\n[ Re ( \u03c8(\u03c9,x)\u03c8(\u03c9,x\u2032) )] = E\n\u03c9\u223c\u00b5 \u3008\u03c8(\u03c9,x), \u03c8(\u03c9,x\u2032)\u3009 . (1)\nWe often refer to \u03c8 as a random feature scheme. We define the norm of \u03c8 as \u2016\u03c8\u2016 = sup\u03c9,x |\u03c8(\u03c9,x)|. We say that \u03c8 is C-bounded if \u2016\u03c8\u2016 \u2264 C. As the kernels are normalized, (1) implies that \u2016\u03c8\u2016 \u2265 1 always. In light of this, we say that an RFS \u03c8 is norm-efficient if it is 1-bounded. Note that in this case, since the kernel is normalized, it holds that |\u03c8(\u03c9,x)| = 1 for almost every \u03c9 as otherwise we would obtain that k(x,x) < 1. Hence, we can assume w.l.o.g. that the range of norm-efficient RFSs is T.\nComment 1 (From complex to real RFSs). While complex-valued features would simplify the analysis of random feature schemes, it often favorable to work in practice with real-valued features. Let \u03c8(\u03c9,x) := R\u03c9(x)e i\u03b8\u03c9(x) be a C-bounded RFS for k. Consider the RFS\n\u03c8\u2032((\u03c9, b),x) := \u221a\n2R\u03c9(x) cos (\u03b8\u03c9(x) + b) , where \u03c9 \u223c \u00b5, and b \u2208 {\n0, \u03c0 2\n} is distributed uniformly and independently from \u03c9. It is not\ndifficult to verify that \u03c8\u2032 is \u221a 2C-bounded RFS for k.\nA random feature generated from \u03c8 is a random function \u03c8(\u03c9, \u00b7) from X to C where \u03c9 \u223c \u00b5. A random q-embedding generated from \u03c8 is the random mapping\n\u03a8\u03c9(x) def = (\u03c8(\u03c91,x), . . . , \u03c8(\u03c9q,x))\u221a q ,\nwhere \u03c91, . . . , \u03c9q \u223c \u00b5 are i.i.d. The random q-kernel corresponding to \u03a8\u03c9 is k\u03c9(x,x\u2032) = \u3008\u03a8\u03c9(x),\u03a8\u03c9(x\u2032)\u3009. Likewise, the random q-kernel space corresponding to \u03a8\u03c9 is Hk\u03c9 . For the rest of this section, let us fix a C-bounded RFS \u03c8 for a normalized kernel k and a random q embedding \u03a8\u03c9. For every x,x \u2032 \u2208 X\nk\u03c9(x,x \u2032) =\n1\nq q\u2211 i=1 \u3008\u03c8(\u03c9i,x), \u03c8(\u03c9i,x\u2032)\u3009\nis an average of q independent random variables whose expectation is k(x,x\u2032). By Hoeffding\u2019s bound we have the following theorem.\nTheorem 3 (Kernel Approximation). Assume that q \u2265 2C 4 log( 2\u03b4 ) 2\n, then for every x,x\u2032 \u2208 X we have Pr (|k\u03c9(x,x\u2032)\u2212 k(x,x\u2032)| \u2265 ) \u2264 \u03b4.\nWe next discuss approximation of functions in Hk by functions in Hk\u03c9 . It would be useful to consider the embedding\nx 7\u2192 \u03a8x where \u03a8x def= \u03c8(\u00b7,x) \u2208 L2(\u2126) . (2)\nFrom (1) it holds that for any x,x\u2032 \u2208 X , k(x,x\u2032) = \u3008\u03a8x,\u03a8x\u2032\u3009L2(\u2126). In particular, from Theorem 2, for every f \u2208 Hk there is a unique function f\u030c \u2208 L2(\u2126) such that \u2016f\u030c\u2016L2(\u2126) = \u2016f\u2016Hk and for every x \u2208 X ,\nf(x) = \u3008f\u030c ,\u03a8x\u3009L2(\u2126) = E \u03c9\u223c\u00b5 \u3008f\u030c(\u03c9), \u03c8(\u03c9,x)\u3009 . (3)\nLet us denote f\u03c9(x) = 1 q \u2211q i=1\u3008f\u030c(\u03c9i), \u03c8(\u03c9i,x)\u3009. From (3) we have that E\u03c9 [f\u03c9(x)] = f(x). Furthermore, for every x, the variance of f\u03c9(x) is at most\n1 q E \u03c9\u223c\u00b5 \u2223\u2223\u3008f\u030c(\u03c9), \u03c8(\u03c9,x)\u3009\u2223\u22232 \u2264 C2 q E \u03c9\u223c\u00b5 \u2223\u2223f\u030c(\u03c9)\u2223\u22232 =\nC2\u2016f\u20162Hk q .\nAn immediate consequence is the following corollary.\nCorollary 4 (Function Approximation). For all x \u2208 X , E\u03c9 |f(x)\u2212 f\u03c9(x)|2 \u2264 C2\u2016f\u20162Hk\nq .\nAs a result, if \u03c7 is a distribution on X , we have\nE \u03c9 \u2016f \u2212 f\u03c9\u20162,\u03c7 = E \u03c9 \u221a E \u03c7 |f(x)\u2212 f\u03c9(x)|2\n\u2264 \u221a\nE \u03c9 E \u03c7 |f(x)\u2212 f\u03c9(x)|2 = \u221a\nE \u03c7 E \u03c9 |f(x)\u2212 f\u03c9(x)|2\n\u2264 C\u2016f\u2016Hk\u221a q .\nWe next consider supervised learning with RFS. Let Y be a target (output) space and let ` : Rt\u00d7Y \u2192 R+ be a \u03c1-Lipschitz loss function, i.e. for every y \u2208 Y , |`(y1, y)\u2212`(y2, y)| \u2264 \u03c1|y1\u2212y2|. Let D be a distribution on X \u00d7 Y . We define the loss of a (prediction) function f : X \u2192 Rt as LD(f) = E(x,y)\u223cD `(f(x), y). Let S = {(x1, y1), . . . , (xm, ym)} denote m i.i.d. examples sampled from D. We denote by Htk the space of all functions f = (f1, . . . , ft) : X \u2192 Rt where fi \u2208 Hk for every i. Htk is a Hilbert space with the inner product \u3008f ,g\u3009Htk = \u2211t i=1\u3008fi, gi\u3009Hk . Let f\u0302 be the function in Htk that minimizes the regularized empirical loss,\nL\u03bbS(f) = 1\nm m\u2211 i=1 `(f(xi), yi) + \u03bb\u2016f\u20162Htk ,\nover all functions in Htk. It is well established (see e.g. Corollary 13.8 in [20]) that for every f? \u2208 Htk,\nE S LD(f\u0302) \u2264 LD (f?) + \u03bb\u2016f?\u20162Htk +\n2\u03c12 \u03bbm . (4)\nIf we further assume that \u2016f?\u2016Htk \u2264 B, for B > 0, and set \u03bb = \u221a 2\u03c1\u221a mB , we obtain that\nE S LD(f\u0302) \u2264 inf\n\u2016f?\u2016Ht k \u2264B\nLD (f ?) + \u221a 8\u03c1B\u221a m . (5)\nThe additive term in (5) is optimal, up to a constant factor. We would like to obtain similar bounds for an algorithm that minimizes the regularized loss w.r.t. the embedding \u03a8\u03c9. Let f\u0302\u03c9 be the function that minimizes,\nL\u03bbS(f) = 1\nm m\u2211 i=1 `(f(xi), yi) + \u03bb\u2016f\u20162Htk\u03c9 , (6)\nover all functions in Htk\u03c9 . Note that in most settings f\u0302\u03c9 can be found efficiently by defining a matrix V \u2208 Ct\u00d7q whose i\u2019th row is vi, and rewriting f\u0302\u03c9 as,\nf\u0302\u03c9(x) = (\u3008v1,\u03a8\u03c9(x)\u3009, . . . , \u3008vt,\u03a8\u03c9(x)\u3009) def = V\u03a8\u03c9(x) .\nWe now can recast the empirical risk minimization of (6) as,\nL\u03bbS(V ) = 1\nm m\u2211 i=1 `(V\u03a8\u03c9(xi), yi) + \u03bb\u2016V \u20162F .\nTheorem 5 (Learning with RFS). For every f? \u2208 Htk,\nE \u03c9 E S LD(f\u0302\u03c9) \u2264 LD (f?) + \u03bb\u2016f?\u20162Htk +\n2\u03c12C2 \u03bbm + \u03c1\u2016f?\u2016HtkC\u221a q . (7)\nIf we additionally impose the constraint \u2016f?\u2016Htk \u2264 B for B > 0 and set \u03bb = \u221a 2\u03c1C\u221a mB we have,\nE \u03c9 E S LD(f\u0302\u03c9) \u2264 inf\n\u2016f?\u2016Ht k \u2264B\nLD (f ?) + \u221a 8\u03c1BC\u221a m + \u03c1BC \u221a q . (8)\nWe note that for norm-efficient RFS (i.e. when C = 1), if the number of random features is proportional to the number of examples, then the error terms in the bounds (8) and (5) are the same up to a multiplicative factor. Since the error term in (5) is optimal up to a constant factor, we get that the same holds true for (8).\nProof. For simplicity, we analyze the case t = 1. Since k\u03c9 is C-bonded, we have from (4) that,\nE S LD (f) \u2264 LD (f ?\u03c9) + \u03bb\u2016f ?\u03c9\u20162Hk\u03c9 +\n2\u03c12C2\n\u03bbm .\nHence, it is enough to show that E\u03c9 \u2016f ?\u03c9\u20162Hk\u03c9 \u2264 \u2016f ?\u20162Hk and E\u03c9 LD (f ? \u03c9) \u2264 LD (f ?)+ \u03c1\u2016f?\u2016HkC\u221a q . Indeed, since\nf ?\u03c9(x) =\n\u2329 (f\u030c ?(\u03c91), . . . , f\u030c\n?(\u03c9q))\u221a q ,\u03a8\u03c9(x)\n\u232a ,\nwe have, by Theorem 2,\nE \u03c9 \u2016f ?\u03c9\u20162Hk\u03c9 \u2264 E\u03c9\n[\u2211q i=1 \u2223\u2223f\u030c ?(\u03c9i)\u2223\u22232 q ]\n= 1\nq q\u2211 i=1 E \u03c9i \u2223\u2223f\u030c ?(\u03c9i)\u2223\u22232 = \u2016f ?\u20162Hk ,\nand similarly,\nE \u03c9 LD (f ? \u03c9) = E \u03c9 E D l(f ?\u03c9(x), y) = ED E\u03c9 l(f ? \u03c9(x), y) .\nNow, from the \u03c1-Lipschitzness of ` and Theorem (4) we obtain,\nE \u03c9 `(f ?\u03c9(x), y) \u2264 `(f ?(x), y) + \u03c1E \u03c9 |f ?(x)\u2212 f ?\u03c9(x)| \u2264 `(f ?(x), y) + \u03c1 \u221a\nE \u03c9 |f ?(x)\u2212 f ?\u03c9(x)|2\n\u2264 `(f ?(x), y) + \u03c1\u2016f ?\u2016HkC\u221a q ,\nconcluding the proof."}, {"heading": "4 Random feature schemes for basic spaces", "text": "In order to apply Theorem 5, we need to control the boundedness of the generated features. Consider the RFS generation procedure, given in Algorithm 1, which employs multiplications of features generated from basic RFSs. If each basic RFS is C-bounded, then every feature that is a multiplication of t basic features is Ct-bounded. In light of this, we would like have RFSs for the basic spaces whose norm is as small as possible. The best we can hope for is norm-efficient RFSs\u2014namely, RFSs with norm of 1. We first describe such RFSs for several kernels including the Gaussian kernel on Rd, and the standard inner product on S0 and S1. Then, we discuss the standard inner product on Sd\u22121 for d \u2265 3. In this case, we show that the smallest possible norm for an RFS is \u221a d/2. Hence, if the basic spaces are Sd\u22121 for d \u2265 3, one might prefer to use other kernels such as the Gaussian kernel.\nExample 1 (Binary coordinates). Let X = {\u00b11} and k(x, x\u2032) = xx\u2032. In this case the deterministic identity RFS \u03c8(\u03c9, x) = x is norm-efficient.\nExample 2 (One dimensional sphere). Let X = T and k(z, z\u2032) = \u3008z, z\u2032\u3009. Let \u03c8(\u03c9, z) = z\u03c9, where \u03c9 is either \u22121 or +1 with equal probability. Then, \u03c8 is a norm-efficient RFS since\nE \u03c9\u223c\u00b5\n\u03c8(\u03c9, z)\u03c8(\u03c9, z\u2032) = zz\u2032 + zz\u2032\n2 = \u3008z, z\u2032\u3009 .\nExample 3 (Gaussian kernel). Let X = Rd and k(x,x\u2032) = e\u2212 a2\u2016x\u2212x\u2032\u20162\n2 , where a > 0 is a constant. The Gaussian RFS is \u03c8(\u03c9,x) = eia\u3008\u03c9,x\u3009, where \u03c9 \u2208 Rd is the standard normal distribution. Then, \u03c8 is a norm-efficient RFS, as implied by [16].\nExample 4 (Categorical coordinates). Let X = [n] and define k(x, x\u2032) = 1 if x = x\u2032 and 0 otherwise. In this case \u03c8(\u03c9, x) = e i\u03c9x 2\u03c0n , where \u03c9 is distributed uniformly over [n], is a norm-efficient RFS since,\nE \u03c9\u223c\u00b5 \u03c8(\u03c9, x)\u03c8(\u03c9, x\u2032) = E \u03c9 e i\u03c9(x\u2212x\u2032) 2\u03c0n =\n{ 1 x = x\u2032\n0 x 6= x\u2032 .\nExamples 1 and 2 show that the standard inner product on the sphere in one and two dimensions admits a norm-efficient RFS. We next examine the sphere Sd\u22121 for d \u2265 3. In this case, we show a construction of a \u221a d/2-bounded RFS. Furthermore, we show that it is the best attainable bound. Namely, any RFS for Sd\u22121 will necessarily have a norm of at least\u221a d/2. In particular, there does not exist a norm-efficient RFS when d \u2265 3.\nExample 5 (Sd\u22121 for d \u2265 3). Let \u00b5 be the uniform distribution on \u2126 = [d] \u00d7 {\u22121,+1}. Define \u03c8 : \u2126 \u00d7 Sd\u22121 \u2192 C for \u03c9 = (j, b) as \u03c8(\u03c9,x) = \u221a d/2(xj + ibxj+1), where we use the\nconvention xd+1 := x1. Now, \u03c8 is a \u221a d/2-bounded RFS as,\nE (j,b)\u223c\u00b5 \u03c8((j, b),x)\u03c8((j, b),x\u2032)\n= d 2\n\u2211d j=1 [ (xj + ixj+1)(x \u2032 j \u2212 ix\u2032j+1) + (xj \u2212 ixj+1)(x\u2032j + ix\u2032j+1) ] 2d\n=\n\u2211d j=1 [ xjx \u2032 j + xj+1x \u2032 j+1 ] 2\n= \u3008x,x\u2032\u3009 .\nWe find it instructive to compare the RFS above to the following \u221a d-bounded RFSs.\nExample 6. Let \u00b5 be the uniform distribution on \u2126 = Sd\u22121 and define \u03c8 : \u2126 \u00d7 Sd\u22121 \u2192 R as \u03c8(w,x) = \u221a d\u3008w,x\u3009. We get that,\nE w\u223c\u00b5 [\u03c8(w,x)\u03c8(w,x\u2032)] = d E w\u223c\u00b5 \u3008w,x\u3009\u3008w,x\u2032\u3009 = d \u3008x,Wx\u2032\u3009 ,\nwhere Wi,j = Ew\u223c\u00b5 [wiwj]. Since w is distributed uniformly on Sd\u22121, E [wiwj] = 0 for i 6= j and E [w2i ] = 1/d. Thus, we have W = (1/d)I and therefore Ew\u223c\u00b5 [\u03c8(w,x)\u03c8(w,x\u2032)] = \u3008x,x\u2032\u3009. A similar result still holds when \u03c8(\u03c9,x) = \u221a d x\u03c9 where \u03c9 \u2208 [d] is distributed uniformly.\nTo conclude the section, we prove that \u221a d/2-boundedness is optimal for RFS on Sd\u22121.\nTheorem 6. Let d \u2265 1 and > 0. There does not exist a ( \u221a d/2\u2212 )-bounded RFS for the kernel k(x,x\u2032) = \u3008x,x\u2032\u3009 on Sd\u22121.\nBefore proving the theorem, we need the following lemmas.\nLemma 7. Let z \u2208 Cd. There exists a \u2208 Sd\u22121 such that \u2223\u2223\u2223\u2211j ajzj\u2223\u2223\u22232 \u2265 12\u2016z\u20162.\nProof. Let us write z = \u03b1 + i\u03b2 where \u03b1,\u03b2 \u2208 Rd. We thus have \u2016z\u20162 = \u2016\u03b1\u20162 + \u2016\u03b2\u20162. We can further assume that \u2016\u03b1\u20162 \u2265 1\n2 \u2016z\u20162 as otherwise we can replace z with iz. Let us define\na as \u03b1/\u2016\u03b1\u2016. We now obtain that,\u2223\u2223\u2223\u2223\u2223\u2211 j ajzj \u2223\u2223\u2223\u2223\u2223 2 \u2265 \u3008a,\u03b1\u30092 = \u2016\u03b1\u20162 \u2265 1 2 \u2016z\u20162 ,\nwhich concludes the proof.\nLemma 8. Let (\u03c8, \u00b5) be an RFS for the kernel k(x,x\u2032) = \u3008x,x\u2032\u3009 on Sd\u22121 and let a \u2208 Sd\u22121. Then, for almost all \u03c9 we have \u03c8(\u03c9, a) = \u2211d j=1 aj\u03c8(\u03c9, ej).\nProof. Let us examine the difference between \u03a8a def = \u03c8(\u00b7, a) and \u2211 j aj\u03a8 ej def= \u2211\nj aj\u03c8(\u00b7, ej),\u2225\u2225\u2225\u2225\u2225\u03a8a \u2212 d\u2211 i=1 ai\u03a8 ei \u2225\u2225\u2225\u2225\u2225 2\nL2(\u2126) = \u3008\u03a8a,\u03a8a\u3009L2 + d\u2211\ni,j=1 aiaj\u3008\u03a8ei ,\u03a8ej\u3009L2 \u2212 2 d\u2211 i=1 ai\u3008\u03a8a,\u03a8ei\u3009L2\n= \u3008a, a\u3009+ d\u2211\ni,j=1 aiaj\u3008ei, ej\u3009 \u2212 2 d\u2211 i=1 ai\u3008a, ei\u3009\n= \u2225\u2225\u2225\u2225\u2225a\u2212 d\u2211 i=1 aiei \u2225\u2225\u2225\u2225\u2225 2 = 0.\nProof of Theorem 6. Let \u03c8 : Sd\u22121 \u00d7 \u2126 \u2192 C be an RFS for k(x,x\u2032) = \u3008x,x\u2032\u3009 and let > 0. We next show that \u03c8 is not \u221a d/2\u2212 - bounded. Let A \u2282 Sd\u22121 be a dense and countable set. From Lemma (8) and the fact that sets of measure zero are closed under countable union, it follows that for almost every \u03c9 and all a \u2208 A we have \u03c8(\u03c9, a) = \u2211d i=1 ai\u03c8(\u03c9, ei). Using the linearity of expectation we know that,\nE \u03c9\u223c\u00b5 d\u2211 i=1 |\u03c8(\u03c9, ei)|2 = d\u2211 i=1 E \u03c9\u223c\u00b5 |\u03c8(\u03c9, ei)|2 = d\u2211 i=1 \u3008ei, ei\u3009 = d .\nHence, with a non-zero probability we get,\nd\u2211 i=1 |\u03c8(\u03c9, ei)|2 > d\u2212 , (9)\nand,\n\u2200a \u2208 A, \u03c8(\u03c9, a) = d\u2211 i=1 ai\u03c8(\u03c9, ei) . (10)\nLet us now fix \u03c9 for which (9) holds. From Lemma (7) there exists a\u0303 \u2208 Sd\u22121 such that | \u2211 i a\u0303i\u03c8(\u03c9, ei)| 2 \u2265 d\u2212 2 . Since A is dense in Sd\u22121 there is a vector a \u2208 A for which\n| \u2211\ni ai\u03c8(\u03c9, ei)| 2 \u2265 d 2 \u2212 . Finally, from (9) it follows that |\u03c8(\u03c9, a)|2 \u2265 d 2 \u2212 ."}, {"heading": "5 Compositional random feature schemes", "text": "Compositional kernels are obtained by sequentially multiplying and averaging kernels. Hence, it will be useful to have RFSs for multiplications and averages of kernels. The proofs of Lemmas 9 and 10 below are direct consequences of properties of kernel spaces and RFSs and thus omitted.\nLemma 9. Let (\u03c81, \u00b51), (\u03c82, \u00b52), . . . be RFSs for the kernels k1, k2, . . . and let (\u03b1i) \u221e i=1 be a sequence of non-negative numbers that sum to one. Then, the following procedure defines an RFS for the kernel k(x,x\u2032) = \u2211n i=1 \u03b1ik i(x,x\u2032).\n1. Sample i with probability \u03b1i\n2. Choose \u03c9 \u223c \u00b5i\n3. Generate the feature x 7\u2192 \u03c8i\u03c9(x)\nLemma 10. Let (\u03c81, \u00b51), . . . , (\u03c8n, \u00b5n) be RFSs for the kernels k1, . . . , kn. The following scheme is an RFS for the kernel k(x,x\u2032) = \u220fn i=1 k\ni(x,x\u2032). Sample \u03c91, . . . , \u03c9n \u223c \u00b51\u00d7 . . .\u00d7\u00b5n and generate the feature x 7\u2192 \u220fn i=1 \u03c8 i \u03c9i (x).\nRandom feature schemes from computation skeletons. We next describe and analyze the case where the compositional kernel is defined recursively using a concrete computation graph defined below. Let X1, . . . ,Xn be measurable spaces with corresponding normalized kernels k1, . . . , kn and RFSs \u03c81, . . . , \u03c8n. We refer to these spaces, kernels, and RFS as base spaces, kernels and RFSs. We also denote X = X1 \u00d7 . . .\u00d7Xn. The base spaces (and correspondingly kernels, and RFSs) often adhere to a simple form. For example, for real-valued input, feature i is represented as Xi = T, where ki(z, z\u2032) = \u3008z, z\u2032\u3009, \u03c8i\u03c9(z) = z\u03c9, and \u03c9 is distributed uniformly in {\u00b11}.\nWe next discuss the procedure for generating compositional RFSs using a structure termed computation skeleton, or skeleton for short. A skeleton S is a DAG with m := |S| nodes. S has a single node whose out degree is zero, termed the output node and denoted out(S), see Figure 1 for an illustration. The nodes indexed 1 through n are input nodes, each of which is associated with a base space. We refer to non-input nodes as internal nodes. Thus, the indices of internal nodes are in {n + 1, . . . ,m}. An internal node v is associated with a PSD function (called a conjugate activation function [6][Sec. 5]), \u03c3\u0302v(\u03c1) = \u2211\u221e i=0 a v i \u03c1 i. For every node v we denote by Sv the subgraph of S rooted at v. This sub-graph defines a compositional kernel through all the nodes nodes with a directed path to v. By definition it holds that out(Sv) = v and Sout(S) = S. We denote by in(v) the set of nodes with a directed edge into v. Each skeleton defines a kernel kS : X \u00d7 X \u2192 R according to the following recurrence,\nkS(x,x \u2032) =\n{ kv(x,x\n\u2032) v \u2208 [n] \u03c3\u0302v\n(\u2211 u\u2208in(v) kS(u)(x,x \u2032)\n|in(v)|\n) v 6\u2208 [n] for v = out(S) .\nIn Figure 1 we give the pseudocode describing the RFS for the kernel kS . We call the routine RFSS as a shorthand for a Random Feature Scheme for a Skeleton. The correctness of the algorithm is a direct consequence of Lemmas 9 and 10.\nAlgorithm 1 RFSS(S) Let v = out(S) if v \u2208 [n] then\nReturn x 7\u2192 \u03c8v(x) else\nSample l \u2208 {0, 1, 2, . . . , } according to (avi )\u221ei=0 for j = 1, . . . , l do\nChoose u \u2208 in(v) at random Call RFSS(Su) and get x 7\u2192 \u03c8\u03c9j(x)\nend for Return x 7\u2192 \u220fl j=1 \u03c8\u03c9j(x)\nend if\nWe next present a simple running time analysis of Algorithm 1 and the sparsity of the generated random features. Note that a compositional random feature is a multiplication of base random features. Thus the amortized time it takes to generate a compositional random\nfeature and its sparsity amount to the expected number of recursive calls made by RFSS. For a given node v the expected number of recursive calls emanating from v is,\nE l\u223c(avj ) [ l ] = \u221e\u2211 j=0 j aj = \u03c3\u0302 \u2032(1) .\nWe now define the complexity of a skeleton as,\nC(S) = { 1 out(S) \u2208 [n] \u03c3\u0302\u2032v(1) \u2211 u\u2208in(v) C(S(u)) |in(v)| otherwise . (11)\nIt is immediate to verify that C(S) is the expected value of the number of recursive calls and the sparsity of a random feature. When all conjugate activations are the same (11) implies that\nC(S) \u2264 (\u03c3\u0302\u2032(1))depth(S) ,\nwhere equality holds when the skeleton is layered. For activations such as ReLU, \u03c3(x) = max(0, x), and exponential, \u03c3(x) = ex, we have \u03c3\u0302\u2032(1) = 1, and thus C(S) = 1. Hence, it takes constant time in expectation to generate a random feature, which in turn has a constant number of multiplications of base random features. For example, let us assume that the basic spaces are S1 with the standard inner product, and that the skeleton has a single non-input node, equipped with the exponential dual activation \u03c3\u0302(\u03c1) = e\u03c1. In this case, the resulting kernel is the Gaussian kernel and C(S) = 1. Therefore, the computational cost of storing and evaluating each feature is constant. This is in contrast to the Rahimi and Recht scheme [16], in which the cost is linear in n."}, {"heading": "6 Empirical Evaluation", "text": "Accuracy of kernel approximation. We empirically evaluated the kernel approximation of our random feature scheme under kernels of varying structure and depth. Our concern is efficiency of random features: how does the quality of approximation fare in response to increasing the target dimension of the feature map? Theorem 3 already establishes an upper bound for the approximation error (in high probability), but overlooks a few practical advantages of our construction that are illustrated in the experiments that follow. Primarily, when one repeatedly executes Algorithm 1 in order to generate features, duplicates may arise. It is straightforward to merge them and hence afford to generate more under the target feature budget. We use the CIFAR-10 dataset for visual object recognition [11].\nWe considered two kernel structures, one shallow and another deep. Figure 2 caricatures both. For visual clarity, it oversimplifies convolutions and the original image to onedimensional objects, considers only five input pixels, and disregards the true size and stride of convolutions used.\nFollowing Daniely et al. [6], for a scalar function \u03c3 : R \u2192 R termed an activation, let \u03c3\u0302(\u03c1) = E(X,Y )\u223cN\u03c1 [\u03c3(X)\u03c3(Y )] be its conjugate activation (shown in the original to be a PSD function). Our shallow structure is simply a Gaussian (RBF) kernel with scale 0.5.\nshallow deep\nEquivalently, again borrowing the terminology of Daniely et al. [6], it corresponds to a singlelayer fully-connected skeleton, having a single internal node v to which all input nodes point. The node v is labeled with a conjugate activation \u03c3\u0302v(\u03c1) = exp((\u03c1\u2212 1)/4).\nThe deep structure comprises a layer of 5x5 convolutions at stride 2 with a conjugate activation of \u03c3\u03021(\u03c1) = exp((\u03c1 \u2212 1)/4), followed by a layer of 4x4 convolutions at stride 2 with the conjugate activation \u03c3\u03022 corresponding to the ReLU activation \u03c32(t) = max{0, t}, followed by a fully-connected layer again with the ReLU\u2019s conjugate activation.\nIn each setting, we compare to a natural baseline built (in part, where possible) on the scheme of Rahimi and Recht [16] for Gaussian kernels. In the shallow setting, doing so is straightforward, as their scheme applies directly. As their scheme is not defined for compositional kernels, our baseline in the deep setting is a hybrid construction. A single random features is generated according to the recursive procedure of Algorithm 1, until an internal node is reached in the bottom-most convolutional layer. Each such node corresponds to a Gaussian kernel, and so we apply the scheme Rahimi and Recht [16] to approximate the kernel of that node.\nFor each configuration of true compositional kernel and feature budget, we repeat the following ten times: draw a batch of 128 data points at random (each center-cropped to 24x24 image pixels), generate a randomized feature map, and compute 1282 kernel evaluations. The result is 10\u00b71282 corresponding evaluations of the true kernel k and an empirical kernel k\u0302. We compare error measures and correlation between the two kernels using (i) RFS inner products as the empirical kernel and (ii) inner products from the baseline feature map. Figure 3 plots the comparison.\nStructural effects and relation to neural networks. In connection to deep neural networks, we experiment with the effect of compositional kernel architecture on learning. We explore two questions: (i) Is a convolutional structure effective in a classifier built on random features? (ii) What is the relation between learning a compositional kernel and a neural network corresponding to its skeleton? The second question is motivated by the connection that Daniely et al. [6] establish between a compositional kernel and neural networks\ndescribed by its skeleton. In particular, we first explore the difference in classification accuracy between the kernel and the network. Then, since training under the kernel is simply a convex problem, we ask whether its relative accuracy across architectures predicts well the relative performance of analogous fully-trained networks.\nFor the experiment, we considered several structures and trained both a corresponding networks and the compositional kernel through an RFS approximation. We again use the CIFAR-10 dataset, with a standard data augmentation pipeline [12]: random 24x24 pixel crop, random horizontal flip, random brightness, saturation, and contrast delta, per-image whitening, and per-patch PCA. In the test set, no data augmentation is applied, and images are center-cropped to 24x24. We generated 106 random features, and trained for 120 epochs with AdaGrad [7] and a manually tuned initial learning rate among {25, 50, 100, 200}.\nThe convolutional architectures are of the same kind as the deep variety in Sec. 6, i.e. two convolutions with a size between 4x4 and 6x6 and a stride of 2, followed by a fully-connected layer. The fully-connected architecture is the shallow structure described in Section 6. All activations are ReLU. The per-patch PCA preprocessing step projects patches to the top q principal components where q is 10, 12, and 16, respectively, for first-layer convolutions of size 4, 5, and 6, respectively. The neural networks are generated from the kernel\u2019s skeleton by node replication (i.e. producing channels) at a rate of 64 neurons per skeleton node for convolutions and 384 for fully-connected layers. We use the typical random Gaussian initialization [8], 200 epochs of AdaGrad with a manually tuned initial learning rate among {.0002, .0005, .001, .002, .005}.\nTest set accuracies are given in Table 1, annotated with how well approximate RFS learning accuracies rank those of the trained networks. We would like to underscore that, for convolutional kernels, networks consistently outperformed the kernel. Meanwhile, the fullyconnected kernel actually outperforms the corresponding network. RFS learning moreover\nranks the top two networks correctly (as well as the bottom three). This observation is qualitatively in line with earlier findings of Saxe et al. [17], who final-layer training of a randomly initialized network to rank fully-trained networks. Indeed, from Daniely et al. [6], we know that the random network approach is an alternative random feature map for the compositional kernel.\nVisualization of hierarchical random features. In Figure 4, we illustrate how our random feature scheme is able to accommodate local structures that are fundamental to image data. We chose 4 different networks of varying depths, and generated 500,000 random features for each. We then computed for each pixel, the probability that it co-occurs in a random feature with any other pixel, by measuring the correlation between their occurrences. As expected, for the flat kernel, the correlation between any pair of pixels was the same. However for deeper kernels, nearby pixels co-occur more often in the random features, and the degree of this co-occurrence is shown in the figure. As we increase the depth, different tiers of locality appear. The most intense correlations share a common first-layer convolution, while moderate correlation share only a second-layer convolution. Lastly, mild correlation share no convolutions."}], "references": [{"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv:1412.8690,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F.R. Bach"], "venue": "CoRR, abs/1502.06800,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729\u20131736. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "Advances in neural information processing systems, pages 342\u2013350,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Sgd learns the conjugate kernel class of the network", "author": ["Amit Daniely"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Tenth IEEE International Conference on Computer Vision, volume 2, pages 1458\u20131465,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "arXiv:1201.6530,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "author": ["J. Mairal"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F. Yu", "S. Kumar"], "venue": "Advances in Neural Information Processing Systems, pages 1837\u20131845,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089\u20131096,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Advances in Kernel Methods - Support Vector Learning", "author": ["B. Sch\u00f6lkopf", "C. Burges", "A. Smola", "editors"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Prior knowledge in support vector kernels", "author": ["B. Sch\u00f6lkopf", "P. Simard", "A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems 10, pages 640\u2013646. MIT Press,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}], "referenceMentions": [{"referenceID": 21, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 20, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 17, "context": "Before the resurgence of deep architectures, kernel methods [22, 21, 18] achieved state of the art results in various supervised learning tasks.", "startOffset": 60, "endOffset": 72}, {"referenceID": 18, "context": "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].", "startOffset": 144, "endOffset": 151}, {"referenceID": 8, "context": "A natural way to capture local spatial or temporal structure is through hierarchical structures using compositions of kernels, see for instance [19, 9].", "startOffset": 144, "endOffset": 151}, {"referenceID": 3, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 2, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 13, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 12, "context": "Compositional kernels became more prominent among kernel methods following the success of deep networks and, for several tasks, they currently achieve the state of the art among all kernel methods [4, 3, 14, 13].", "startOffset": 197, "endOffset": 211}, {"referenceID": 15, "context": "Rahimi and Recht [16] described and analyzed an elegant and computationally effective way that mitigates this problem by generating random features that approximate certain kernels.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 14, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 1, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 0, "context": "Their work was extended to various other kernels [10, 15, 2, 1].", "startOffset": 49, "endOffset": 63}, {"referenceID": 5, "context": "The kernels\u2019 definition and the connection to networks was developed in [6, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 4, "context": "The kernels\u2019 definition and the connection to networks was developed in [6, 5].", "startOffset": 72, "endOffset": 78}, {"referenceID": 15, "context": "Indeed, the base kernels of a compositional kernel can be non-elementary such as the Gaussian kernel, and hence our RFS can be used in conjunction with the well-studied RFS of [16] for Gaussian kernels.", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "8 in [20]) that for every f \u2208 H k, E S LD(f\u0302) \u2264 LD (f) + \u03bb\u2016f?\u20162Ht k + 2\u03c1 \u03bbm .", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Then, \u03c8 is a norm-efficient RFS, as implied by [16].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "An internal node v is associated with a PSD function (called a conjugate activation function [6][Sec.", "startOffset": 93, "endOffset": 96}, {"referenceID": 15, "context": "This is in contrast to the Rahimi and Recht scheme [16], in which the cost is linear in n.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "We use the CIFAR-10 dataset for visual object recognition [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "[6], for a scalar function \u03c3 : R \u2192 R termed an activation, let \u03c3\u0302(\u03c1) = E(X,Y )\u223cN\u03c1 [\u03c3(X)\u03c3(Y )] be its conjugate activation (shown in the original to be a PSD function).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6], it corresponds to a singlelayer fully-connected skeleton, having a single internal node v to which all input nodes point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In each setting, we compare to a natural baseline built (in part, where possible) on the scheme of Rahimi and Recht [16] for Gaussian kernels.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "Each such node corresponds to a Gaussian kernel, and so we apply the scheme Rahimi and Recht [16] to approximate the kernel of that node.", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "[6] establish between a compositional kernel and neural networks", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In the shallow setting, the baseline (RR) is the scheme of Rahimi and Recht [16] for Gaussian kernels.", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "We again use the CIFAR-10 dataset, with a standard data augmentation pipeline [12]: random 24x24 pixel crop, random horizontal flip, random brightness, saturation, and contrast delta, per-image whitening, and per-patch PCA.", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "We generated 10 random features, and trained for 120 epochs with AdaGrad [7] and a manually tuned initial learning rate among {25, 50, 100, 200}.", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "We use the typical random Gaussian initialization [8], 200 epochs of AdaGrad with a manually tuned initial learning rate among {.", "startOffset": 50, "endOffset": 53}, {"referenceID": 16, "context": "[17], who final-layer training of a randomly initialized network to rank fully-trained networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], we know that the random network approach is an alternative random feature map for the compositional kernel.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes. \u2217Google Brain, {amitdaniely, frostig, vineet, singer}@google.com ar X iv :1 70 3. 07 87 2v 1 [ cs .L G ] 2 2 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}