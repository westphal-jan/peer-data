{"id": "1703.08136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Visually grounded learning of keyword prediction from untranscribed speech", "abstract": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \"man\" and \"person\", making it even more effective as a semantic keyword spotter. Thus, we find that we can use the neural network to identify semantic semantic groups from speech and text. The human brain's decision-making process may be dependent on the ability to predict what words will occur in such a context---or the capacity to identify semantic groups as they are. Our model predicts that words may also have semantic content at a different time of the speech and text. Using our neural network we also analyze the cognitive process by which words are processed and used to recognize a spoken sentence by a given sentence. Finally, we analyze the use of word recognition in speech. To determine whether the model captures words that are spoken in a specific language, we first search the dictionary of words. To understand the effect that words will appear in speech, we use an open-source language called \"learning.\" The word recognition algorithm is used by many cognitive researchers, to identify and analyze word recognition for semantic information. The results of the system are presented in the first part of this post.", "histories": [["v1", "Thu, 23 Mar 2017 16:46:00 GMT  (569kb,D)", "http://arxiv.org/abs/1703.08136v1", "5 pages, 3 figures, 5 tables"], ["v2", "Thu, 25 May 2017 20:49:15 GMT  (343kb,D)", "http://arxiv.org/abs/1703.08136v2", "5 pages, 3 figures, 5 tables; small updates, added link to code; accepted to Interspeech 2017"]], "COMMENTS": "5 pages, 3 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["herman kamper", "shane settle", "gregory shakhnarovich", "karen livescu"], "accepted": false, "id": "1703.08136"}, "pdf": {"name": "1703.08136.pdf", "metadata": {"source": "CRF", "title": "Visually grounded learning of keyword prediction from untranscribed speech", "authors": ["Herman Kamper", "Shane Settle", "Gregory Shakhnarovich", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "1. Introduction Current automatic speech recognition (ASR) systems use supervised models trained on huge amounts of annotated resources. In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135]. Here we consider the problem of grounding unlabelled speech when paired with images. Annotating speech is expensive and sometimes impossible, e.g. for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7]. This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].\nSpecifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled. Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other. This approach allows images to be retrieved using speech and vice versa. The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18]. Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.\nHere we consider the possibility of using externally trained computer vision systems, which do have access to textual labels, to provide (noisy) supervision for untranscribed speech. Concretely, we use an external image-to-words multi-label visual classifier, predicting for an image a set of words that refer to aspects of the scene. Using soft labels (probabilities) from\nthis vision system, we train a convolutional neural network to map spoken captions to these soft unordered word targets. The result is a speech model that can predict which words (from a fixed vocabulary defined by the vision system) occur in a spoken utterance\u2014acting as a spoken bag-of-words (BoW) classifier.\nThe previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models. Our approach can be seen as a further way to exploit vision systems, by also using their textual classification output. We are thus transferring labels from a high-resourced modality (vision, for which extensive resources are available) to a low-resource modality (speech, where resources are scarce for many languages).\nWe first apply our word prediction model to two tasks: BoW prediction, where the aim is to predict an unordered set of words that occur in a given utterance, and keyword spotting, where the task is to retrieve all utterances in a collection that contain a given textual keyword. Promising results are achieved on both tasks. Analysis shows that many of the model errors are semantically related to the correct labels, e.g. the model retrieves the speech utterance \u201ca dog runs in the grass\u201d for the textual keyword \u201cfield\u201d. These \u201cerrors\u201d may be desirable in certain settings. So in a final task, we evaluate our model as a semantic keyword spotter, where it achieves performance much closer to that of an oracle model trained using ground-truth transcriptions."}, {"heading": "2. Related work", "text": "Our work intersects with several other research directions. Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21]. These systems still rely on labelled speech data, while our aim is to use vision to ground untranscribed speech. There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326]. The study of [26] particularly influenced our approach, since they build a speech system using textual BoW labels (\u00a73.1). In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330]. In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews. Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36]."}, {"heading": "3. Word prediction from images and speech", "text": "Given a corpus of parallel images and spoken captions, neither with textual labels, we propose a method to train a spoken word prediction model using labels obtained from the visual modality."}, {"heading": "3.1. Model overview", "text": "Every training image I is paired with a spoken caption of frames X = x1,x2, . . . ,xT (e.g. MFCCs). We use a vision system to\nar X\niv :1\n70 3.\n08 13\n6v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 7\ntag I with soft textual labels, giving targets to train the speech network f(X) to predict which words are present in X . The network f(X) therefore acts as a spoken bag-of-words (BoW) classifier (disregarding the order and quantity of words). No transcriptions are used during training. When applying the trained f(X), only speech input is used (and no image). The approach is illustrated in Figure 1, and below we give complete details.\nIf we knew which words occur in training utterance X , we could construct a multi-hot vector ybow \u2208 {0, 1}W , with W the vocabulary size, and each dimension ybow,w a binary indicator for whether word w occurs in X . In [26], transcriptions were used to obtain this type of ideal BoW supervision. Instead of a transcription for X , we only have access to the paired image I . We use a multi-label visual classifier (with parameters \u03b3) which, instead of binary indicators, produces soft targets yvis \u2208 [0, 1]W , with yvis,w = P (w|I,\u03b3) the probability of wordw being present given image I . In Figure 1, yvis would ideally be close to 1 for w corresponding to words such as \u201chat\u201d, \u201cman\u201d and \u201cshirt\u201d, and close to 0 for irrelevant dimensions. This vision system is fixed: during training (below), vision parameters \u03b3 are never updated.\nGiven yvis as target, we train the word prediction model f(X). This model (with parameters \u03b8) consists of a convolutional neural network (CNN) over the speech X , as shown on the right in Figure 1. We interpret each dimension of the output as fw(X) = P (w|X,\u03b8). Note that f(X) is not a distribution over the vocabulary, since any number of terms in the vocabulary can be present in an utterance; rather, each dimension fw(X) can have any value in [0, 1]. We train this speech network using the cross-entropy loss, which (for a single training example) is:\nL(f(X),yvis) = \u2212 W\u2211\nw=1\n{yvis,w log fw(X) +\n(1\u2212 yvis,w) log [1\u2212 fw(X)]} (1)\nIf we had yvis,w \u2208 {0, 1}, as in ybow, this could be described as the summed log loss of W binary classifiers. The size-W vocabulary of our system is implicitly specified by the vision system."}, {"heading": "3.2. Two convolutional architectures over speech", "text": "We consider two different convolutional architectures for f(X). Both deal with the variable number of frames in X by pooling over the entire output of their final convolution layer. As input layer, both use a one-dimensional convolution only over time, covering a number of frames and the entire frequency axis.\nThe first architecture is shown schematically in Figure 1. It is a CNN based on [16, 37], consisting of several convolution\nand max pooling layers (final pooling covering the entire output), followed by fully connected layers. A sigmoid activation is used for the final output f(X), and ReLUs in intermediate layers.\nThe second architecture is the one from Palaz, Synnaeve and Collobert [26], referred to as PSC. It was originally developed for ideal BoW supervision (\u00a73.1), with the aim of not only doing spoken BoW classification, but also locating where words occur in the speech. PSC aims to do this by explicitly building the vocabulary into its final convolutional layer, as illustrated in Figure 2. The final convolution is linear with W output filters, matching the system vocabulary. The outputs of these final filters are h1,h2, . . . ,hT \u2032 , with hi \u2208 RW . The idea is that hi,w gives a score for word w occurring in the time span corresponding to output i, thus giving an estimate of where w would occur in X . To obtain the network output s(X) \u2208 RW , PSC does not use mean or max pooling, but rather an intermediate option:\nsw(X) = 1\nr log  1 T \u2032 T \u2032\u2211 t=1 exp (r ht,w(X))  (2) with sw(X) giving an overall unnormalized score for word w being present in X . This logsumexp pooling is equivalent to mean pooling when r \u2192 0 and max pooling for r \u2192\u221e; Palaz et al. note that this intermediate method improves PSC\u2019s location prediction capability (refer to [26]). The final output of the network is f(X) = \u03c3(s(X)), with \u03c3 the sigmoid function. We use r = 1, ReLU activations, and no intermediate pooling."}, {"heading": "3.3. The vision system", "text": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330]. In contrast, rather than a fluent sentence, here we want a vision tagging system [38\u201340] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left). This is a multi-label binary classification task, where for each word we must predict whether it is appropriate for the image.\nWe train our vision tagging system on the Flickr30k data set [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stop words. Given a limited set of task-specific training data, such as Flickr30k, a common approach is to start with a visual representation learned as a part of end-to-end training on a larger data set (possibly for a different task), and then adapt it to the task at hand. We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].1 Specifically, we use VGG-16 [43], but replace the final\n1The ImageNet output itself is not well-suited to our setting, since it performs a single multi-way classification among a set of image classes.\nclassification layer with four 1024-unit ReLU layers, followed by a binary classifier for word occurrence. We train this multilabel visual classifier (with parameters \u03b3) on Flickr30k, with the output layer limited to the W = 1000 most common word types in the image captions. The VGG-16 parameters are fixed during training; only the final layers that we add on top are trained.\nNote that Flickr30k is distinct from the parallel imagespeech data used in our experiments (\u00a74). The vision system is also trained and then fixed (parameters \u03b3 is not updated in \u00a74).\n4. Experiments"}, {"heading": "4.1. Experimental setup", "text": "We train our word prediction model on the data set of parallel images and spoken captions of [44], containing 8000 images with 5 spoken captions each. The audio comprises around 37 hours of active speech. The data comes with train, development and test splits containing 30 000, 5000 and 5000 utterances, respectively. Speech is parametrized as MFCCs with first and second order derivatives, giving 39-dimensional input.2 Utterances longer than 8 s are truncated (99.5% of utterances are shorter than 8 s).\nTraining images are passed through the vision system (\u00a73.3), producing soft targets yvis for training the word prediction model f(X) on the unlabelled speech. We consider two architectures for f(X), referred to as VisionSpeechCNN and VisionSpeechPSC, respectively (see \u00a73.2). VisionSpeechCNN is structured as follows: 1-D ReLU convolution with 64 filters over 9 frames; max pooling over 3 units; 1-D ReLU convolution with 256 filters over 10 units; max pooling over 3 units; 1-D ReLU convolution with 1024 filters over 11 units; max pooling over all units; 2048- unit fully-connected ReLU; and the 1000-unit sigmoid output. VisionSpeechPSC is structured as follows: 1-D ReLU convolution with 64 filters over 9 frames; four 1-D ReLU convolutions, each with 64 filters over 10 units; 1-D linear convolution with W = 1000 filters over 10 units; and logsumexp pooling followed by the final sigmoid activation. We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.\nWe also obtain upper and lower bounds on performance. As an upper bound, we train two oracle models, OracleSpeechCNN and OracleSpeechPSC, with the same structures as the two VisionSpeech models above. These models are trained on ideal BoW supervision (\u00a73.1): we obtain ybow targets for the 1000 most common words in the transcriptions of the 30 000 speech training utterances, after removing stop words. Next, as a lowerbound baseline, we use a unigram language model prior that gives the unigram probability of each keyword as estimated from the transcriptions. This baseline gives an indication of how much better our models do than simply hypothesizing common words. Note that the textual transcriptions are used only for the baseline and oracle models and for evaluation: neither of the VisionSpeech models ever see any parallel speech and text.\nAll models were implemented in TensorFlow [45]. Based on development tuning, they are trained using Adam [46] for 25 epochs with a minibatch of 128 and a learning rate of 0.0001 for all models, except those based on PSC, which uses 0.001."}, {"heading": "4.2. Spoken bag-of-words prediction", "text": "We first consider the task of predicting which words are present in a given test utterance. Given input X , our model gives a score fw(X) \u2208 [0, 1] for every word w in its vocabulary, and\n2We also tried filterbanks; MFCCs always worked similarly or better.\na child in a black shirt running boy, child, young a small dog runs through a field dog, field, grass, running, runs a snowboarder jumping in the air with a person riding a ski lift in the background air, snow, snowboarder\na white dog with black spots jumps in midair dog, jumping, jumps, white\nthe man in a blue sweater is climbing down the rocks climber, climbing, rock\nthese can be used for spoken BoW prediction. To make a hard prediction, we set a threshold \u03b1 and output labels for allw where fw(X) > \u03b1. We compare the predicted BoW labels to the true set of words in the transcriptions, and calculate precision, recall and F -score across all word types in the reference transcriptions (not only the 1000 words in the system vocabulary). To compare performance independently of \u03b1, we report average precision (AP), the area under the precision-recall curve as \u03b1 is varied.\nTable 1 presents BoW prediction performance for the different models at two operating points for \u03b1, to show the trade-off between precision and recall. The unigram baseline achieves nontrivial performance, indicating that some words are commonly used across the utterances in the data set. Both VisionSpeech models substantially outperform this baseline at both \u03b1\u2019s, and in AP. Although the VisionSpeech models still lag far behind the two oracle models, the VisionSpeech models are trained without seeing any parallel speech and text. The precision of 61.3% of VisionSpeechCNN at \u03b1 = 0.7 is therefore noteworthy, since it shows that (although we miss many words in terms of recall), a relatively high-precision textual labelling system can be obtained using only images and unlabelled speech. For the oracle models, the PSC architecture is beneficial, outperforming its CNN counterpart by all measures; but for the VisionSpeech models, the PSC model falls slightly behind. We discuss this below.\nTable 2 gives examples of the type of output produced by the VisionSpeechCNN model. To better analyze the model\u2019s behavior, we examine a selection of words that the model predicts that do not occur in the corresponding reference transcriptions. Figure 3 shows some of these \u201cfalse alarm words\u201d, along with the most common words that do occur in the corresponding utterances. In many cases, the predicted words are variants of the correct words: e.g. for an incorrect prediction of \u201csnow\u201d, most of the reference transcriptions contain the word \u201csnowy\u201d. Other confusions are semantic in nature, e.g. \u201cyoung\u201d is predicted when \u201cgirl\u201d is present, and \u201ctrick\u201d when \u201cramp\u201d is present."}, {"heading": "4.3. Keyword spotting", "text": "Our model can also be naturally used as a keyword spotter: given a text query, the goal is to retrieve all of the utterances in the test set containing spoken instances of that query. We randomly select 20 textual keywords from the VisionSpeech output vocabulary as queries. For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.\nTable 3 shows keyword spotting results. The trend in relative performance is similar to that of Table 1: the unigram baseline performs worst, the VisionSpeech models give reasonable scores, and the oracle models perform best. The very high oracle performance indicates that the constrained nature of the data used here (narrow domain, relatively small vocabulary) makes the task fairly easy when true transcriptions are available. Nevertheless, it is again noteworthy that both VisionSpeech models obtain a P@10 of around 50% at an EER of 23%, without using any text.\nTo give a qualitative view of VisionSpeechCNN\u2019s errors, Table 4 shows examples of incorrectly matched utterances for some keywords. As before, many of these erroneous utterances contain either variants of the keyword (e.g. \u201cplay\u201d and \u201cplaying\u201d) or are semantically related (e.g. \u201cyoung\u201d and \u201clittle girl\u201d). Although these matches would seem reasonable and even desirable in some settings, they are penalized under the metrics in Table 3."}, {"heading": "4.4. Semantic keyword spotting", "text": "To investigate this issue quantitatively, we considered the top 10 proposed utterances for each keyword for each model, and relabelled as correct those utterances that either contained keyword variants or were semantically related. This allows us to report P@10 for the task of semantic keyword spotting, as shown in Table 5 (the other metrics would require us to semantically label all test utterances). Compared to Table 3, the semantic keyword spotting performance is better than exact keyword spotting scores for all models. However, the VisionSpeech models im-\nTable 4: Examples of incorrectly retrieved utterances when VisionSpeechCNN is used for keyword spotting.\nKeyword Example of incorrectly matched utterance Type\nModel P@10\nUnigram baseline 10.0\nVisionSpeechCNN 82.5 VisionSpeechPSC 73.0\nOracleSpeechCNN 99.5 OracleSpeechPSC 100.0\nprove most, with VisionSpeechCNN improving by almost 30% absolute. Moreover, while the oracle models improved mainly due to variant matches, the VisionSpeech models had about equal numbers of relabelled variant and semantic matches.\nIn Tables 3 and 5 we again see that while PSC is superior to CNN for the oracle models, this is not the case for the VisionSpeech models. As mentioned in \u00a73.2, PSC is intended to also estimate word locations. Our results suggest that when trained on transcriptions (oracle models), there is a benefit in attempting to capture aspects of word order. However, when trained through visual grounding, the output of VisionSpeechPSC produces high probabilities for several semantically related words, and there is far less structure in the order of these words."}, {"heading": "5. Conclusion", "text": "We have introduced a new way of using images to learn from untranscribed speech. By using a visual image-to-word classifier to provide soft labels for the speech, we are able to learn a neural speech-to-keyword prediction system. Our best model achieves a spoken bag-of-words precision of more than 60%, and a keyword spotting P@10 of more than 50% with an equal error rate of 23%. The model achieves this performance without access to any parallel speech and text. Further analysis shows that the model\u2019s mistakes are often semantic in nature, e.g. confusing \u201cboys\u201d and \u201cchildren\u201d. To quantify this, we evaluated our model as a semantic keyword spotter, where the task is to find all utterances in a corpus that are semantically related to the textual keyword query. In this setting, our model achieves a semantic P@10 of more than 80%. Future work will consider how semantic search in speech can be formalized, and how the visual component of our approach can be explicitly tailored to obtain an improved visual grounding signal for unlabelled speech.\nAcknowledgements: We thank Gabriel Synnaeve and David Harwath for assistance with data and models, as well as Shubham Toshniwal and Hao Tang for helpful feedback. This research was funded by NSF grant IIS-1433485. The opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency.\n6. References [1] A. S. Park and J. R. Glass, \u201cUnsupervised pattern discovery in\nspeech,\u201d IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.\n[2] A. Jansen et al., \u201cA summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition,\u201d in Proc. ICASSP, 2013.\n[3] C.-y. Lee, T. O\u2019Donnell, and J. R. Glass, \u201cUnsupervised lexicon discovery from acoustic input,\u201d Trans. ACL, vol. 3, pp. 389\u2013403, 2015.\n[4] M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, \u201cThe Zero Resource Speech Challenge 2015: Proposed approaches and results,\u201d in Proc. SLTU, 2016.\n[5] H. Kamper, A. Jansen, and S. J. Goldwater, \u201cUnsupervised word segmentation and lexicon discovery using acoustic word embeddings,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.\n[6] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \u201cAutomatic speech recognition for under-resourced languages: A survey,\u201d Speech Commun., vol. 56, pp. 85\u2013100, 2014.\n[7] G. Chrupa\u0142a, L. Gelderloos, and A. Alishahi, \u201cRepresentations of language in a model of visually grounded speech signal,\u201d arXiv preprint arXiv:1702.01991, 2017.\n[8] J. Luo, B. Caputo, A. Zweig, J.-H. Bach, and J. Anemu\u0308ller, \u201cObject category detection using audio-visual cues,\u201d in Proc. ICVS, 2008.\n[9] M. Sun and H. Van hamme, \u201cJoint training of non-negative Tucker decomposition and discrete density hidden Markov models,\u201d Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.\n[10] T. Taniguchi, T. Nagai, T. Nakamura, N. Iwahashi, T. Ogata, and H. Asoh, \u201cSymbol emergence in robotics: A survey,\u201d Adv. Robotics, vol. 30, no. 11-12, pp. 706\u2013728, 2016.\n[11] L. Smith and C. Yu, \u201cInfants rapidly learn word-referent mappings via cross-situational statistics,\u201d Cognition, vol. 106, no. 3, pp. 1558\u2013 1568, 2008.\n[12] E. D. Thiessen, \u201cEffects of visual information on adults and infants auditory statistical learning,\u201d Cognitive Sci., vol. 34, no. 6, pp. 1093\u20131106, 2010.\n[13] O. J. Ra\u0308sa\u0308nen, \u201cComputational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions,\u201d Speech Commun., vol. 54, pp. 975\u2013997, 2012.\n[14] S. Frank, N. H. Feldman, and S. J. Goldwater, \u201cWeak semantic context helps phonetic learning in a model of infant language acquisition,\u201d in Proc. ACL, 2014.\n[15] G. Synnaeve, M. Versteegh, and E. Dupoux, \u201cLearning words from images and speech,\u201d in NIPS Workshop Learn. Semantics, 2014.\n[16] D. Harwath, A. Torralba, and J. R. Glass, \u201cUnsupervised learning of spoken language with visual context,\u201d in Proc. NIPS, 2016.\n[17] T. J. Hazen, B. Sherry, and M. Adler, \u201cSpeech-based annotation and retrieval of digital photographs,\u201d in Proc. Interspeech, 2007.\n[18] X. Anguera, J. Xu, and N. Oliver, \u201cMultimodal photo annotation and retrieval on a mobile phone,\u201d in Proc. ICMIR, 2008.\n[19] D. Harwath and J. R. Glass, \u201cLearning word-like units from joint audio-visual analysis,\u201d arXiv preprint arXiv:1701.07481, 2017.\n[20] F. Sun, D. Harwath, and J. R. Glass, \u201cLook, listen, and decode: Multimodal speech recognition with images,\u201d in Proc. SLT, 2016.\n[21] A. Gupta, Y. Miao, L. Neves, and F. Metze, \u201cVisual features for context-aware speech recognition,\u201d in Proc. ICASSP, 2017.\n[22] G. Aimetti, R. K. Moore, and L. ten Bosch, \u201cDiscovering an optimal set of minimally contrasting acoustic speech units: A point of focus for whole-word pattern matching,\u201d in Proc. Interspeech, 2010.\n[23] V. Renkens and H. Van hamme, \u201cMutually exclusive grounding for weakly supervised non-negative matrix factorisation,\u201d in Proc. Interspeech, 2015.\n[24] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, \u201cAn attentional model for speech translation without transcription,\u201d in Proc. NAACL, 2016, pp. 949\u2013959.\n[25] S. Bansal, H. Kamper, A. Lopez, and S. J. Goldwater, \u201cTowards speech-to-text translation without speech recognition,\u201d in Proc. EACL, 2017.\n[26] D. Palaz, G. Synnaeve, and R. Collobert, \u201cJointly learning to locate and classify words using convolutional networks,\u201d in Proc. Interspeech, 2016.\n[27] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, \u201cShow and tell: A neural image caption generator,\u201d in Proc. CVPR, 2015.\n[28] A. Karpathy and L. Fei-Fei, \u201cDeep visual-semantic alignments for generating image descriptions,\u201d in Proc. CVPR, 2015.\n[29] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dolla\u0301r, J. Gao, X. He, M. Mitchell, J. C. Platt et al., \u201cFrom captions to visual concepts and back,\u201d in Proc. CVPR, 2015.\n[30] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, \u201cLong-term recurrent convolutional networks for visual recognition and description,\u201d in Proc. CVPR, 2015.\n[31] C. H. Silberer, \u201cLearning visually grounded meaning representations,\u201d Ph.D. dissertation, The University of Edinburgh, 2015.\n[32] R. Bernardi, R. Cakici, D. Elliott, A. Erdem, E. Erdem, N. IkizlerCinbis, F. Keller, A. Muscat, and B. Plank, \u201cAutomatic description generation from images: A survey of models, datasets, and evaluation measures,\u201d J. Artif. Intell. Res., vol. 55, pp. 409\u2013442, 2016.\n[33] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba, \u201cAmbient sound provides supervision for visual learning,\u201d in Proc. ECCV, 2016.\n[34] Y. Aytar, C. Vondrick, and A. Torralba, \u201cSoundnet: Learning sound representations from unlabeled video,\u201d in Proc. NIPS, 2016, pp. 892\u2013900.\n[35] A. K. Vijayakumar, R. Vedantam, and D. Parikh, \u201cSoundWord2Vec: Learning word representations grounded in sounds,\u201d arXiv preprint arXiv:1703.01720, 2017.\n[36] L. Gelderloos and G. Chrupa\u0142a, \u201cFrom phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning,\u201d Proc. COLING, 2016.\n[37] H. Kamper, W. Wang, and K. Livescu, \u201cDeep convolutional acoustic word embeddings using word-pair side information,\u201d in Proc. ICASSP, 2016.\n[38] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. M. Blei, and M. I. Jordan, \u201cMatching words and pictures,\u201d J. Mach. Learn. Res., vol. 3, pp. 1107\u20131135, 2003.\n[39] M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid, \u201cTagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation,\u201d in Proc. ICCV, 2009.\n[40] M. Chen, A. X. Zheng, and K. Q. Weinberger, \u201cFast image tagging.\u201d in Proc. ICML, 2013.\n[41] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \u201cFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,\u201d Trans. ACL, vol. 2, pp. 67\u201378, 2014.\n[42] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proc. CVPR, 2009.\n[43] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.\n[44] D. Harwath and J. Glass, \u201cDeep multimodal semantic embeddings for speech and images,\u201d in Proc. ASRU, 2015.\n[45] M. Abadi et al., \u201cTensorFlow: Large-scale machine learning on heterogeneous systems,\u201d 2015. [Online]. Available: http: //tensorflow.org/\n[46] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[47] T. J. Hazen, W. Shen, and C. White, \u201cQuery-by-example spoken term detection using phonetic posteriorgram templates,\u201d in Proc. ASRU, 2009.\n[48] Y. Zhang and J. R. Glass, \u201cUnsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams,\u201d in Proc. ASRU, 2009."}], "references": [{"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition", "author": ["A. Jansen"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The Zero Resource Speech Challenge 2015: Proposed approaches and results", "author": ["M. Versteegh", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. SLTU, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Commun., vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Representations of language in a model of visually grounded speech signal", "author": ["G. Chrupa\u0142a", "L. Gelderloos", "A. Alishahi"], "venue": "arXiv preprint arXiv:1702.01991, 2017.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Object category detection using audio-visual cues", "author": ["J. Luo", "B. Caputo", "A. Zweig", "J.-H. Bach", "J. Anem\u00fcller"], "venue": "Proc. ICVS, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Joint training of non-negative Tucker decomposition and discrete density hidden Markov models", "author": ["M. Sun", "H. Van hamme"], "venue": "Comput. Speech Lang., vol. 27, no. 4, pp. 969\u2013988, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Symbol emergence in robotics: A survey", "author": ["T. Taniguchi", "T. Nagai", "T. Nakamura", "N. Iwahashi", "T. Ogata", "H. Asoh"], "venue": "Adv. Robotics, vol. 30, no. 11-12, pp. 706\u2013728, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Infants rapidly learn word-referent mappings via cross-situational statistics", "author": ["L. Smith", "C. Yu"], "venue": "Cognition, vol. 106, no. 3, pp. 1558\u2013 1568, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Effects of visual information on adults and infants auditory statistical learning", "author": ["E.D. Thiessen"], "venue": "Cognitive Sci., vol. 34, no. 6, pp. 1093\u20131106, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions", "author": ["O.J. R\u00e4s\u00e4nen"], "venue": "Speech Commun., vol. 54, pp. 975\u2013997, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Weak semantic context helps phonetic learning in a model of infant language acquisition", "author": ["S. Frank", "N.H. Feldman", "S.J. Goldwater"], "venue": "Proc. ACL, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning words from images and speech", "author": ["G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "NIPS Workshop Learn. Semantics, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J.R. Glass"], "venue": "Proc. NIPS, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech-based annotation and retrieval of digital photographs", "author": ["T.J. Hazen", "B. Sherry", "M. Adler"], "venue": "Proc. Interspeech, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Multimodal photo annotation and retrieval on a mobile phone", "author": ["X. Anguera", "J. Xu", "N. Oliver"], "venue": "Proc. ICMIR, 2008.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning word-like units from joint audio-visual analysis", "author": ["D. Harwath", "J.R. Glass"], "venue": "arXiv preprint arXiv:1701.07481, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Look, listen, and decode: Multimodal speech recognition with images", "author": ["F. Sun", "D. Harwath", "J.R. Glass"], "venue": "Proc. SLT, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual features for context-aware speech recognition", "author": ["A. Gupta", "Y. Miao", "L. Neves", "F. Metze"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Discovering an optimal set of minimally contrasting acoustic speech units: A point of focus for whole-word pattern matching", "author": ["G. Aimetti", "R.K. Moore", "L. ten Bosch"], "venue": "Proc. Interspeech, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Mutually exclusive grounding for weakly supervised non-negative matrix factorisation", "author": ["V. Renkens", "H. Van hamme"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "An attentional model for speech translation without transcription", "author": ["L. Duong", "A. Anastasopoulos", "D. Chiang", "S. Bird", "T. Cohn"], "venue": "Proc. NAACL, 2016, pp. 949\u2013959.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards speech-to-text translation without speech recognition", "author": ["S. Bansal", "H. Kamper", "A. Lopez", "S.J. Goldwater"], "venue": "Proc. EACL, 2017.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Jointly learning to locate and classify words using convolutional networks", "author": ["D. Palaz", "G. Synnaeve", "R. Collobert"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning visually grounded meaning representations", "author": ["C.H. Silberer"], "venue": "Ph.D. dissertation, The University of Edinburgh, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler- Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "J. Artif. Intell. Res., vol. 55, pp. 409\u2013442, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "Proc. ECCV, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Soundnet: Learning sound representations from unlabeled video", "author": ["Y. Aytar", "C. Vondrick", "A. Torralba"], "venue": "Proc. NIPS, 2016, pp. 892\u2013900.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound- Word2Vec: Learning word representations grounded in sounds", "author": ["A.K. Vijayakumar", "R. Vedantam", "D. Parikh"], "venue": "arXiv preprint arXiv:1703.01720, 2017.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "author": ["L. Gelderloos", "G. Chrupa\u0142a"], "venue": "Proc. COLING, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1107\u20131135, 2003.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "Proc. ICCV, 2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Trans. ACL, vol. 2, pp. 67\u201378, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. CVPR, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal semantic embeddings for speech and images", "author": ["D. Harwath", "J. Glass"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi"], "venue": "2015. [Online]. Available: http: //tensorflow.org/", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example spoken term detection using phonetic posteriorgram templates", "author": ["T.J. Hazen", "W. Shen", "C. White"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised spoken keyword spotting via segmental DTW on Gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 1, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 2, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 3, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 4, "context": "In an effort to alleviate this dependence on labelled data, there is growing interest in methods that can learn from untranscribed speech [1\u20135].", "startOffset": 138, "endOffset": 143}, {"referenceID": 5, "context": "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "for endangered or unwritten languages [6]; grounding speech using co-occurring visual contexts could be a way to train systems in such lowresource scenarios [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 8, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 9, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 116, "endOffset": 122}, {"referenceID": 10, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 11, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 12, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 13, "context": "This setting is also relevant in robotics, where audio and visual signals can be combined for learning new commands [8\u201310], and for understanding language acquisition in humans, who have access to visual cues for grounding [11\u201314].", "startOffset": 223, "endOffset": 230}, {"referenceID": 14, "context": "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.", "startOffset": 61, "endOffset": 69}, {"referenceID": 15, "context": "Specifically, we are interested in the setting considered in [15, 16], where natural images of scenes are paired with spoken descriptions, and neither the images nor speech are labelled.", "startOffset": 61, "endOffset": 69}, {"referenceID": 14, "context": "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Both [15] and [16] used paired neural networks to map images and speech into a common semantic space where matched images and spoken captions are close to each other.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].", "startOffset": 106, "endOffset": 114}, {"referenceID": 17, "context": "The same task was also considered in earlier work on tagging mobile phone images with spoken descriptions [17, 18].", "startOffset": 106, "endOffset": 114}, {"referenceID": 6, "context": "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.", "startOffset": 78, "endOffset": 85}, {"referenceID": 18, "context": "Despite the practical relevance, and interesting extensions in follow-on work [7, 19], this joint mapping approach does not give an explicit grounding of speech in terms of textual labels.", "startOffset": 78, "endOffset": 85}, {"referenceID": 6, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 14, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 15, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 18, "context": "The previous work in this setting [7, 15, 16, 19] also makes use of intermediate features from pretrained vision models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 19, "context": "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 20, "context": "Recent studies have shown that using extra visual features from the scene in which the speech occurs can improve conventional ASR [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 21, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 22, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 23, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 24, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 25, "context": "There has also been much interest in developing speech models that, instead of exact transcriptions, can learn from very noisy labels [22\u201326].", "startOffset": 134, "endOffset": 141}, {"referenceID": 25, "context": "The study of [26] particularly influenced our approach, since they build a speech system using textual BoW labels (\u00a73.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 27, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 28, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 29, "context": "In the vision community, image captioning has received much recent attention, where the goal is to produce a fluent and informative natural language description for a visual scene [27\u201330].", "startOffset": 180, "endOffset": 187}, {"referenceID": 30, "context": "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.", "startOffset": 126, "endOffset": 134}, {"referenceID": 31, "context": "In natural language processing, images have also been used to capture aspects of meaning (semantics) of written language; see [31, 32] for reviews.", "startOffset": 126, "endOffset": 134}, {"referenceID": 32, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 33, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 34, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 95, "endOffset": 102}, {"referenceID": 35, "context": "Other studies have considered multimodal modelling of sounds (not speech) with text and images [33\u201335], and phonemes with images [36].", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "In [26], transcriptions were used to obtain this type of ideal BoW supervision.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "We use a multi-label visual classifier (with parameters \u03b3) which, instead of binary indicators, produces soft targets yvis \u2208 [0, 1] , with yvis,w = P (w|I,\u03b3) the probability of wordw being present given image I .", "startOffset": 125, "endOffset": 131}, {"referenceID": 0, "context": "Note that f(X) is not a distribution over the vocabulary, since any number of terms in the vocabulary can be present in an utterance; rather, each dimension fw(X) can have any value in [0, 1].", "startOffset": 185, "endOffset": 191}, {"referenceID": 15, "context": "It is a CNN based on [16, 37], consisting of several convolution X logsumexp", "startOffset": 21, "endOffset": 29}, {"referenceID": 36, "context": "It is a CNN based on [16, 37], consisting of several convolution X logsumexp", "startOffset": 21, "endOffset": 29}, {"referenceID": 25, "context": "Figure 2: A two-layer Palaz, Synnaeve and Collobert (PSC) network [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "The second architecture is the one from Palaz, Synnaeve and Collobert [26], referred to as PSC.", "startOffset": 70, "endOffset": 74}, {"referenceID": 25, "context": "note that this intermediate method improves PSC\u2019s location prediction capability (refer to [26]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 27, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 28, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 29, "context": "In image captioning, the goal is to produce a natural language description of a scene [27\u201330].", "startOffset": 86, "endOffset": 93}, {"referenceID": 37, "context": "In contrast, rather than a fluent sentence, here we want a vision tagging system [38\u201340] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).", "startOffset": 81, "endOffset": 88}, {"referenceID": 38, "context": "In contrast, rather than a fluent sentence, here we want a vision tagging system [38\u201340] that predicts an unordered set of words (nouns, adjectives, verbs) that accurately describe aspects of the scene (Figure 1, left).", "startOffset": 81, "endOffset": 88}, {"referenceID": 39, "context": "We train our vision tagging system on the Flickr30k data set [41], which contains 30k images, each with a set of 5 captions, which we convert into a BoW after removing stop words.", "startOffset": 61, "endOffset": 65}, {"referenceID": 40, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "We follow the established practice of using a representation trained for the ImageNet classification task [42], as also in prior work [7, 16].", "startOffset": 134, "endOffset": 141}, {"referenceID": 41, "context": "Specifically, we use VGG-16 [43], but replace the final", "startOffset": 28, "endOffset": 32}, {"referenceID": 42, "context": "We train our word prediction model on the data set of parallel images and spoken captions of [44], containing 8000 images with 5 spoken captions each.", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "We arrived at these two structures starting from those in [16] and [26], respectively, and then tuned them on our development data.", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "All models were implemented in TensorFlow [45].", "startOffset": 42, "endOffset": 46}, {"referenceID": 44, "context": "Based on development tuning, they are trained using Adam [46] for 25 epochs with a minibatch of 128 and a learning rate of 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Given input X , our model gives a score fw(X) \u2208 [0, 1] for every word w in its vocabulary, and", "startOffset": 48, "endOffset": 54}, {"referenceID": 45, "context": "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.", "startOffset": 37, "endOffset": 45}, {"referenceID": 46, "context": "For evaluation, we use three metrics [47, 48]: P@10 is the average precision (across keywords, in %) of the 10 highest-scoring proposals; P@N is the average precision of the top N proposals, with N the number of true occurrences of the keyword; and equal error rate (EER) is the average error rate at which the false acceptance and false rejection rates are equal.", "startOffset": 37, "endOffset": 45}], "year": 2017, "abstractText": "During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance\u2014acting as a spoken bag-of-words classifier\u2014without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. \u201cman\u201d and \u201cperson\u201d, making it even more effective as a semantic keyword spotter.", "creator": "LaTeX with hyperref package"}}}