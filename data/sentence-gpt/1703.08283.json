{"id": "1703.08283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Experimental Identification of Hard Data Sets for Classification and Feature Selection Methods with Insights on Method Selection", "abstract": "The paper first reports an experimentally identified list of benchmark data sets that are hard for representative classification and feature selection methods. This was done after systematically evaluating a total of 54 combinations of methods, involving nine state-of-the-art classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository (some data sets with known high classification accuracy were excluded). In this paper, a data set for classification is called hard if none of the 54 combinations can achieve an AUC over 0.001 per match, with the remaining combinations representing less than 2,000 of the selected combinations. In our analysis, we performed an error correction based on the inclusion of all the 56 combinations used in the UCI repository, and the error correction is corrected by adding the \u20325-n/\u22121\u2033 test parameter. The \u20321-n/\u22121\u2033 test parameter was then calculated as the total number of states for each state, by computing the weighted state as its own.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 24 Mar 2017 04:26:22 GMT  (300kb)", "http://arxiv.org/abs/1703.08283v1", "19 pages, 3 figures, 12 tables"]], "COMMENTS": "19 pages, 3 figures, 12 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cuiju luan", "guozhu dong"], "accepted": false, "id": "1703.08283"}, "pdf": {"name": "1703.08283.pdf", "metadata": {"source": "CRF", "title": "Experimental Identification of Hard Data Sets for Classification and Feature Selection Methods with Insights on Method Selection", "authors": ["Cuiju Luan", "Guozhu Dong"], "emails": ["cjluan@shmtu.edu.cn", "guozhu.dong@wright.edu"], "sections": [{"heading": null, "text": "for representative classification and feature selection methods. This was done after\nsystematically evaluating a total of 54 combinations of methods, involving nine\nstate-of-the-art classification algorithms and six commonly used feature selection methods, on\n129 data sets from the UCI repository (some data sets with known high classification accuracy\nwere excluded). In this paper, a data set for classification is called hard if none of the 54\ncombinations can achieve an AUC over 0.8 and none of them can achieve an F-Measure value\nover 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be\nhard in that sense. This paper also compares the performance of different methods, and it\nproduces rankings of classification methods, separately on the hard data sets and on the easy\ndata sets. This paper is the first to rank methods separately for hard data sets and for easy data\nsets. It turns out that the classifier rankings resulting from our experiments are somehow\ndifferent from those in the literature and hence they offer new insights on method selection.\nKeywords: classification methods, feature selection methods, hard data sets, method ranking, performance comparison Running Title: Hard Data Sets for Major Classification Methods"}, {"heading": "1 Introduction", "text": "When faced with a classification job, an analyst will often want to select the best methods for the application; this can be a daunting task since there are a large number of methods available. Users will need insights, such as rankings of the methods, to guide them to make the best selection, and to go through the selection process in an easy-to-handle manner. Several studies on the experimental evaluation of various methods for classification have been reported recently (e.g., Fern\u00e1ndez-Delgado et al., 2014; Van Hulse, Khoshgoftaar and Napolitano, 2007). Reference (Fern\u00e1ndez-Delgado et al., 2014) is a main representative of such studies, which used 121 data sets to evaluate 179 classifiers.\nclassification methods by considering all data sets in one pool \u2013 they did not distinguish the data sets based on their hardness. Moreover, there was no systematic study to identify which classification benchmark data sets are hard for traditional classification methods, and there were no rankings of methods based on their performance on hard data sets only. Filling these gaps is important, as the identified hard data sets can help future studies to develop new classification algorithms to complement existing classification algorithms, and the ranking of methods on hard data sets can help users select the best method when they are working with potentially hard data sets. We plan to fill this gap in this study.\nThis study will evaluate both classification algorithms and feature selection methods in\ncombination. Specifically, it will identify hard data sets for which no combinations of representative classification algorithms and feature selection methods can produce accurate classification models. Moreover, the study will use the area under the ROC (AUC) and F-Measure, instead of the accuracy measure (as was done in Fern\u00e1ndez-Delgado et al., 2014), to evaluate the performance of classification models. These measures were chosen based on the recent consensus that the accuracy measure has significant shortcomings when compared with the above two measures, especially AUC.\nTo identify a list of benchmark data sets that are hard for representative classification and\nfeature selection methods, we perform a systematical evaluation of 54 combinations, involving nine representative classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository. We note that some data sets with known high classification accuracy based on results of Fern\u00e1ndez-Delgado et al. (2014) were excluded in our experiments.\nFor ease of discussion, a data set for classification will be called hard if none of the 54\ncombinations can achieve an AUC over 0.8 and none of the 54 combinations can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be hard in our experiments.\nThis paper also compares the performance of different methods separately on the hard\ndata sets and on the easy data sets. This was done based on their performance on data sets for which complete results were obtained for all of the 54 combinations. It turns out that the method rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection.\nThe rest of the paper is organized as follows. Section 2 describes the classification\nalgorithms and feature selection methods used in this study. Section 3 describes the data sets included in this study. Section 4 gives the experiment settings and the evaluation measures used. Section 5 presents the experimental results and the associated analysis. Section 6 concludes the paper."}, {"heading": "2 Algorithms Used in the Study", "text": "In the experiments, we used multiple commonly-used representative classification algorithms and feature selection methods. The classification algorithms we used are Boosting, Decision Tree, Random Forest, Nearest Neighbor, Logistic Regression, and Support Vector Machine (SVM). The feature selection methods we used are correlation based method, information gain based method, and the relief-f method (all of which are filter based methods). During the\nto its computational expensiveness (see Table X in Appendix) (it is seldom used in practice (Li et al., 2016) due to the same reason).\nAll the classification algorithms and feature selection methods we used are as\nimplemented in Weka 3.8.0 (Hall et al., 2009). More details are given in the next two subsections."}, {"heading": "2.1 Classification Algorithms and Parameter Settings", "text": "We selected representative classification algorithms, partly based on several papers that reported systematic evaluation of classification algorithms and partly based on common knowledge. In particular, reference (Fern\u00e1ndez-Delgado et al., 2014) showed Random Forest and SVM are often better than the others, and reference (Wyner et al. 2015) gave a list of common-used successful classification algorithms. We selected Boosting, Decision Tree, Random Forest, Nearest Neighbor, Logistic Regression and SVM, as the representatives of existing classification algorithms. Table 1 shows the correspondence of classification algorithms and their implementations in Weka that we used in experiments. Some of the classification algorithms given in Table 1 have multiple versions due to different parameter settings, yielding a total of nine classification algorithms (discussed below).\nTo better evaluate the six algorithms, two J48 and three SVM classifiers were examined.\nFor J48, we examined the classifier using the default parameter values, and the other using no pruning and using Laplace smoothing (J48(-U-A)). The three LibSVM classifiers are SVM using linear (SVM-LN), polynomial (SVM-PN) and radial basis function (SVM-RBF) kernels respectively. For IBk, we used K=10 and crossValidate=True, with which the system will find the optional K value between 1 and 10. More details are given below.\n1) AdaBoost uses the M1 method (Freund and Schapire, 1996) with DecisionStump as\nbase classifiers.\n2) J48 implements a pruned C4.5 (Quinlan, 1993) decision tree algorithm, with\nparameters confidenceFactor=0.25 and minNumObj=2.\n3) J48(-U-A) implements a unpruned C4.5 decision tree algorithm with Laplace-based\nsmoothening.\n4) RF builds a forest of random trees (Breiman, 2001), with parameters\nbagSizePercent=100 and unlimited depth.\nselecting the optimal value of K between 1 and 10 based on cross-validation.\n6) LOG builds and uses a multinomial logistic regression model with a ridge estimator\n(Cessie and Houwelingen, 1992), with parameter ridge=1.0E-8.\n7) SVM-LN uses the LibSVM library (Chang and Lin, 2011) with linear kernel, with\nparameters SVMType=C-SVC and cost=1.0.\n8) SVM-PN uses the LibSVM library with polynomial kernel, with parameters\nSVMType=C-SVC, cost =1.0, degree=3, gamma=0.0 and coef0=0.\n9) SVM-RBF uses the LibSVM library with radial basis function kernel, with\nparameters SVMType=C-SVC, cost=1.0 and gamma=0.0."}, {"heading": "2.2 Feature Selection Methods", "text": "Feature selection methods are used to remove irrelevant, redundant, or noisy attributes, with the aim of speeding up the computation and improving the accuracy (Li et al., 2009; Liu and Yu, 2005; Dash and Liu, 1997). Our experiments examine the effectiveness of feature selection methods when used with various classification algorithms.\nMany feature selection methods have been proposed, each with its own pros and cons.\nWe selected the following filter methods because they are commonly used (Liu and Yu, 2005; Robnik-\u0160ikonja and Kononenko, 2003): the correlation based method, the information gain based method and the relief-f method. We used Weka\u2019s implementation of the three feature selection methods.\nIn order to better evaluate these methods, we used two different parameter settings, which\ndictate how many features are selected, as described below.\n(1) Let IG1 (similarly RLF1) denote the version of the Information Gain method\n(similarly Relief-F) that works to select 80 percent of the features if the total number of features of a given data set is no more than 50, and to select 40 features if the total number of features is more than 50. (2) Let IG2 (similarly RLF2) denote the version of the method that works to select 60\npercent of the features if the total number of features of a given data set is no more than 50, and to select 25 features if the total number of features is more than 50. For ease of discussion, we consider \u201cno attribute selection\u201d as a feature selection method.\nTherefore, a total of 6 feature selection methods are considered.\nWe now give some more details on the feature selection methods.\n1) CFS uses CfsSubsetEval as the attribute evaluator to evaluate the worth of a subset of\nattributes by considering the individual predictive ability of each feature along with the degree of redundancy between them, and uses BestFirst as the search method to searches the space of attribute subsets by greedy hill-climbing augmented with a backtracking facility.\n2) IG1 uses InfoGainAttributeEval as the attribute evaluator to evaluate the worth of\nattributes by measuring the information gain with respect to the class, and uses Ranker as the search method. If the total number of features is no more than 50, IG1 selects 80 percent of the features, and IG1 selects 40 features otherwise.\n3) IG2 differs from IG1 as follows. It selects 60 percent of the features if the total number\nof features is no more than 50, and it selects 25 features otherwise.\nattributes by repeatedly sampling an instance and considering the value of the given attribute for the nearest instance belonging to the same and different classes. It uses Ranker as the search method. It selects 60 percent of the features if the total number of features is no more than 50, and it selects 25 features otherwise.\n5) RLF2 differs from RLF1 as follows. It selects 60 percent of the features if the total\nnumber of features is no more than 50, and it selects 25 features otherwise.\n6) NO means \u201cno attribute selection is performed\u201d."}, {"heading": "3 Data Sets Included in the Study", "text": "Our experiments used 129 data sets, all from the UCI repository (Lichman, 2013). Table 2 lists the 98 data sets for which complete results for all of the 54 combinations were obtained; the remaining data sets are given Appendix.\nData set #Instance #Attribute Data set #Instance #Attribute\nabalone 4177 9 heart 270 14\nAnneal 798 39 heart-cleveland 303 14\nArrhythmia 452 263 heart-switzerland 123 13\naustralian 690 15 heart-va 200 13\nballoons_a 20 5 hepatitis 155 20\nballoons_b 20 5 hill-valley 1212 101\nballoons_c 20 5 leaf 340 16\nballoons_d 16 5 led-display 1000 8\nbankrupt_qualitative 250 7 letter-recognition 13339 17\nBiodeg 1055 42 lung-cancer 32 57\nBlogger 100 6 mfeat 2000 650\nbreast-cancer 286 10 monks-1 556 7\nbreast-cancer-wisc 699 10 monks-2 601 7\nbreast-cancer-wisc-diag 569 31 monks-3 554 7\nbreast-cancer-wisc-prog 198 34 occupancy 20560 7\nbreast-tissue 106 10 phishing 11055 31\nchronic_kidney_disease 400 25 pima 768 9\nclimate 540 21 pittsburg-bridges-material 106 8\ncongressional-voting 435 17 pittsburg-bridges-rel-l 103 8\ncontrac 1473 10 pittsburg-bridges-span 92 8\ncortex_nuclear 1080 82 pittsburg-bridges-t-or-d 102 8\ncredit_card 30000 24 pittsburg-bridges-type 105 8\ncrx 690 16 planning 182 13\ndata_banknote_authentication 1372 5 post-operative 90 9\ndr 1151 20 primary-tumor 330 18\ndresses_attribute_sales 500 14 seismic-bumps 2584 19\neeg_data 14980 15 shuttle 58000 10\nelectricity-board 45781 5 spect 265 23\nBelow we discuss where the data sets are from and how we selected them. (a) From the data sets studied in (Fern\u00e1ndez-Delgado et al., 2014), we first selected 34\ndata sets such that the maximum reported accuracy of (Fern\u00e1ndez-Delgado et al., 2014) is below 0.8. (This saved our effort by eliminating the data sets having high known accuracy.) Then we added another 11 data sets that are variations (sharing the same data set name at UCI) of some of the 34 data sets. As a result, this group has a total of 45 data sets.\n(b) Because (Fern\u00e1ndez-Delgado et al., 2014) only dealt with UCI data sets dated before\nMarch 2013, we examined the 91 UCI data sets whose dates are between January 2013 and June 2016. From these we selected 32 and excluded the other 59 for reasons such as \u201ctoo many instances\u201d, \u201chaving no classification attribute\u201d, \u201chaving complex data structure requiring preprocessing\u201d, \u201chaving no data\u201d, and \u201cinaccessible\u201d. Some data sources provide multiple versions (e.g. 15 for actrecog, 16 for gesture, 10 for mhealth, 2 for student); we took each version as a different data set (e.g. 15 data sets from the 15 versions of actrecog). This group has a total of 71 data sets.\n(c) We also examined data sets having no dates marked at UCI, from which 10 data sets\nare included (the others are excluded due to complex data). Among the 10 data sets, ballon has 4 versions, yielding extra data sets. So this group has a total of 13 data sets.\nAmong the 129 data sets from the three groups, there were 31 (listed in Table XI in\nTable 2. One of the 31 came from the (a) group and the other 30 from the (b) group."}, {"heading": "4 Experimental Settings and Evaluation Measures", "text": "We used 10-fold cross validation to evaluate classification performance. For each fold of each data set, a classification model is built from the other 9 folds, using each of the 54 combinations involving nine classifiers and six feature selection methods.\nAs widely noted in the literature, the simple accuracy measure may be not adequate for\nimbalanced data sets. As some data sets used in our experiments are not balanced, we did not use the accuracy measure; we used the AUC and F-Measure measures instead.\nAUC is equivalent to the probability that the underlying classifier will rank a randomly\nchosen positive instance higher than a randomly chose negative instance (Fawcett, 2006). It is also called ROC Area in Weka. The F-Measure is the harmonic mean of Precision and Recall. AUC has several desirable properties as a classification performance measure, such as being decision threshold independent and invariant to a priori class probabilities. AUC is widely accepted as one of the best ways to evaluate a classifier\u2019s performance (Bradley, 1997) and it has been widely used to measure the performance in classification. We chose to include the F-Measure, in order to complement the AUC, and also to indicate the strength of the classifier in terms of Precision and Recall. (Reference (Hand, 2009) pointed out that for some situations, namely when the ROC curves cross, AUC has some weakness and it may give potentially misleading information.)"}, {"heading": "5 Experimental Results and Discussion", "text": "This section first presents the 17 identified hard data sets. It then analyzes the performance of classification and feature selection methods under several different conditions. It presents, for each hard data set, the combinations that are the best or worst for the data set. Based on that, it identifies the most frequent best classification algorithm, the most frequent best feature selection method, and so on. Similarly, it presents the worst combinations and identifies the most frequent worst methods. Finally, it also gives rankings of classification algorithms based on the number of data sets where they are the best and base on the average AUC; the rankings are given separately for the hard data sets and for the easy data sets. It should be noted that the rankings are based solely on results on the 98 data sets for which complete results were obtained for all of the 54 combinations.\nFor ease of discussion, we introduce a few terms and notations. For each data set, let\nmax54AUC denote the highest AUC achieved by the 54 classification-algorithm and feature-selection-method combinations, and similarly let max54FMeasure denote the highest F-Measure. We say a data set is a hard data set if both max54AUC and max54FMeasure are no higher than 0.8, and we call the other data sets as easy data sets.\nAs the detailed experiment results require too much space, they are not listed here; they\ncan be found as supplementary materials at http://cecs.wright.edu/~gdong/harddata/.\n5.1 The 17 Hard Data Sets We Identified\nthe data sets for which max54FMeasure is less than or equal to 0.8. In both tables, the data sets are listed in increasing measure value order.\nData set Max54FMeasure\nheart-va 0.399\nprimary-tumor 0.423\nheart-switzerland 0.434\nstudent-por 0.443\nstudent-mat 0.455\ncontrac 0.555\ncongressional-voting 0.558\nheart-cleveland 0.586\ndresses_attribute_sales 0.6\npittsburg-bridges-type 0.601\nyeast 0.608\nplanning 0.623\npost-operative 0.626\nplant-shape 0.64\nTable 5 reports the hard data sets having both max54AUC and max54FMeasure less than\nor equal to 0.8 --- they are precisely those that appear in both Tables 3 and 4.\nData set Max54AUC Max54FMeasure\npost-operative 0.523 0.626\nplanning 0.562 0.623\ndresses_attribute_sales 0.59 0.6\nstatlog-australian-credit 0.61 0.668"}, {"heading": "5.2 Best and Worst Method Combinations for the Hard Data Sets", "text": "For each hard data set, we identified the best combination of classification and feature selection methods obtaining the highest AUC; the result is reported in Table 6. Summarizing the table we have the following:\nAt the individual algorithm level, RF and IG1 make the most-frequent best combination (being the best for 4 hard data sets). The best combinations for ten (10) of the 17 hard data sets involve the use of feature selection methods (58.8%); the best combinations for three (3) do not use feature selection methods (17.6%); for the remaining four (4) data sets, multiple combinations achieved the best AUC (23.5%), some of which involve feature selection methods and some do not. Focusing on the feature selection methods, we see that IG1 and NO (no attribute selection) are used in the best combinations for 7 hard data sets (41.2%), IG2 is used for 4 hard data sets (23.5%), RLF1 is used for 3 hard data sets (17.6%), CFS is used for 2 hard data sets (11.8%) and RLF2 is used for 1 hard data sets (5.9%). Focusing on classification algorithms, we see that RF appears in the best combinations of 8 hard data sets (47.1%), LOG appears in 5 (29.4%), AdaBoost appears in 2 (11.8%), and SVM and IBk each appears in 1 (5.9%).\nSo, for the 17 hard data sets, RF is the most-frequent best classification algorithm, IG1\nand NO (no attribute selection) is the most-frequent best feature selection method, and the combination of RF and IG1 is the most-frequent best combination.\nData set #Instance #Attribute #SelAttr Classifier FS Method Max54AUC\npost-operative 90 9 9 SVM-PN NO 0.523\npost-operative 90 9 8 SVM-PN IG1 0.523\nplanning 182 13 2 IBK CFS 0.562\ndresses_attribute_sales 500 14 9 AdaBoost IG2 0.59\nWe now turn to the worst combination of classification and feature selection methods\ngetting lowest AUC for the hard data sets; the result is given in Table 7. Summarizing the table we have the following:\nAt the individual algorithm level, AdaBoost & IG2 and AdaBoost & NO (no attribute selection) are the most-frequent worst combinations, each being the worst for 4 hard data sets. The worst combinations for ten (10) of the 17 hard data sets involve the use of feature selection methods (58.8%); the worst combinations for three (3) do not involve the use of feature selection methods (17.6%); for the remaining four (4), multiple combinations obtained the worst AUC (23.5%), some of which involve feature selection methods and some do not. Focusing on the feature selection methods, we see that RLF2 and NO (no attribute selection) are used in the worst combinations for 7 hard data sets (41.2%), CFS and IG2 are used for 6 hard data sets (35.3%), RLF1 is used for 4 hard data sets (23.5%), and IG1 is used for 3 hard data sets (17.6%). Focusing on classification algorithms, we see that SVM-PN and AdaBoost appears in the worst combinations of 5 hard data sets (29.4%), SVM-RBF and J48 appears in 2 (11.8 %), and RF, IBk and LOG each appears in 1 (5.9 %).\nSo, for the 17 hard data sets, SVM-PN and AdaBoost are the most-frequent worst\nclassification algorithms, RLF2 and NO (no attribute selection) are the most-frequent worst\nare the most-frequent worst combinations."}, {"heading": "5.3 Maximum and Minimum AUC by All Combinations for the Hard/Easy Data Sets", "text": "For each data set, let min54AUC be defined similarly to max54AUC, and let span54AUC\nFigure 1 presents max54AUC and min54AUC for the hard data sets. We note that\nmaximum span54AUC, minimum span54AUC, and average span54AUC are 0.277, 0.083 and 0.182 respectively. So the choice of classification and feature selection methods often has big impact on the classification accuracy for the hard data sets.\nFigure 2 presents max54AUC and min54AUC for all easy data sets. We note that\nmaximum span54AUC, minimum span54AUC, and average span54AUC are 0.603, 0.056 and 0.351 respectively. Moreover, span54AUC is greater than 0.3 for 66.7% of the easy data sets. While several easy data sets can be classified well by all of the 54 combinations, for more than half of the easy data sets the difference in classification performance by different combinations is large."}, {"heading": "5.4 Average AUC and Average F-Measure for the Hard/Easy Data Sets", "text": "Figure 3 shows the average AUC and average F-Measure for the nine classification algorithms on (1) the 17 hard data sets (upper panel) and (2) all of the easy data sets (lower panel). We observe that, based on average AUC, LOG is the best, followed by RF, for the hard data sets, and RF is the best for the easy data sets.\nThe slope of curve in the upper panel is gentle (almost flat): the maximum average AUC\nis 0.64, the minimum average AUC is 0.56, and their difference is 0.08; the maximum average F-Measure is 0.594, the minimum average F-Measure is 0.563, and their difference is 0.031. The slope of the curve in the lower panel is fairly steep: the maximum average AUC is 0.91, the minimum average AUC is 0.673, and their difference is 0.237; the maximum average F-Measure is 0.817, the minimum average F-Measure is 0.579, and their difference is 0.238. In summary, the difference among the performance of the classification algorithms for the 17 hard data sets is fairly small; in contrast, the difference for the easy data sets is fairly large.\nWe note that RF is the best or the second best for both easy and hard data sets, which is\nin strong agreement with the ranking of algorithms provided by (Fern\u00e1ndez-Delgado et al., 2014). However, Figure 3 shows that SVM-PN is the last one in the rank for both easy and hard data sets, which is very different from the ranking give by (Fern\u00e1ndez-Delgado et al., 2014) (which found SVM to be the second best classification algorithm). There are at least three potential reasons for the disagreement: We used AUC and F-Measure whereas (Fern\u00e1ndez-Delgado et al., 2014) used accuracy. (2) In our study we excluded a number of\n(by any of the classification algorithms) and we included some data sets not studied in (Fern\u00e1ndez-Delgado et al., 2014). (3) In our ranking, we separated data sets into a hard pool and an easy pool, whereas (Fern\u00e1ndez-Delgado et al., 2014) considered all data sets in one pool.\nTable 8 shows the average AUC and average F-Measure for the six feature selection\nmethods on the hard data sets (left table) and the easy data sets (right table). For the hard data sets, the NO (no attribute selection) and IG1 methods are the best; for the easy data sets, RLF1 is the best, followed by NO (no attribute selection). We note that the average AUC and average F-Measure of NO are all just 0.002 below those of RLF1.\nMore specifically, for the hard data sets (Table 8, left), the maximum average AUC is\n0.607 and the minimum average AUC is 0.593, and the difference is just 0.014; the maximum average F-Measure is 0.589, the minimum average F-Measure is 0.575, and the difference is just 0.014. For the easy data sets (Table 8, right), the maximum average AUC is 0.8, the minimum average AUC is 0.783, and the difference is 0.017; the maximum average F-Measure is 0.701, the minimum average F-Measure is 0.688, and the difference is 0.013. The above suggests that there is little difference on the classification performance whether feature selection methods are used, or which feature selection methods are used, based on average performance. We must note that the above statement is based on average performance over a large number of data sets, and the experiments used default parameter settings without data set-specific parameter tuning. As noted above, the exclusion of a fairly large number of data sets with known high classification accuracy may also contribute to the above findings."}, {"heading": "5.5 Summary of Classifier Rankings on Hard Data Sets and on Easy Data Sets", "text": "Tables 9 and 10 summarize the rankings of the 9 classification algorithms for the hard and easy data sets respectively. Each gives two classifier rankings, one based on the number of data sets for which the classifier is the best, and the other based on average AUC. #DatasetsBest is the number of data sets for which a given algorithm obtained the maximum AUC. There are several classifiers obtaining the maximum AUC for some easy data sets, so the sum of #DatasetsBest in Table 10 (left panel) is larger than the number of easy data sets.\nClassifier #DatasetsBest Classifier AVG_AUC"}, {"heading": "J48 0 SVM-LN 0.580", "text": ""}, {"heading": "6 Conclusions", "text": "This paper reported a systematic evaluation of classification performance by representative state-of-the-art classification algorithms and feature selection methods on 129 data sets from UCI. It identified a list of benchmark data sets that are hard for representative classification and feature selection methods. It ranked the classification algorithms based on their performance on the hard data sets, and on their performance on the easy data sets. It also compared the effectiveness of feature selection methods. To the best of our knowledge, this study is the first to give a list of hard benchmark data sets in the machine learning literature, and to rank classification algorithms by considering their performance on hard data sets. This list of hard benchmark data sets can be useful for motivating the development of new classification and feature selection algorithms, and for use in the evaluation of such algorithms."}, {"heading": "Acknowledgements", "text": "This work was partly supported by the Shanghai Natural Science Foundation (Grant No.\nThe authors wish to thank Dr Huan Liu and Dr Lei Yu for encouraging us to carry out this study. Part of the first author\u2019s work was done while visiting the Data Mining Research Lab at Wright State University.\nAppendices\nBelow, Table X gives the run time for the wrapper method on several data sets, showing that the method is very time consuming.\nTable XI lists the 31 data sets for which our experiments with the 54 classification\nalgorithm and feature selection method combinations were not completed due to reasons such as \u201clack of memory\u201d, \u201ctaking too much time\u201d (that is, for some of the combinations, the 10 folds cross validation took more than 120 hours), \u201cabnormal program termination\u201d etc. For each of the 31 data sets computation for at least one of the 54 combinations was completed. From the partial results of the finished experiments, we get the maximum AUC and maximum F-Measure for these data sets. Based on the partial results we are quite certain that the 31 data sets all belong to the easy data set category except the first one. We are not sure whether the first data set is a hard data set or not, because its maximum AUC and maximum F-Measure are all less than 0.8 based on the partial results.\nData set Instance Attribute Classifier Runtime(seconds)\npima 768 9 SVM-PN 193623\ncrx 690 16 SVM-PN 105864\nflags 194 29 SVM-LN 96216\nanneal 798 39 RF 19652"}], "references": [{"title": "Do we need hundreds of classifiers to solve real world classification problems", "author": ["Manuel Fern\u00e1ndez-Delgado", "Eva Cernadas", "Sen\u00e9n Barro", "Dinani Amorim"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fern\u00e1ndez.Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fern\u00e1ndez.Delgado et al\\.", "year": 2014}, {"title": "Experimental perspectives on learning from imbalanced data", "author": ["Jason Van Hulse", "Taghi M. Khoshgoftaar", "Amri Napolitano"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning,", "citeRegEx": "Hulse et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hulse et al\\.", "year": 2007}, {"title": "Feature selection: A data perspective", "author": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Fred Morstatter", "Robert P Trevino", "Jiliang Tang", "Huan Liu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The WEKA Data Mining Software: An Update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers", "author": ["Abraham J. Wyner", "Matthew Olson", "Justin Bleich", "David Mease"], "venue": null, "citeRegEx": "Wyner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wyner et al\\.", "year": 2015}, {"title": "Toward Integrating Feature Selection Algorithms for Classification and Clustering", "author": ["Huan Liu", "Lei Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Liu and Yu.,? \\Q2005\\E", "shortCiteRegEx": "Liu and Yu.", "year": 2005}, {"title": "Feature Selection for Classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis,", "citeRegEx": "Dash and Liu.,? \\Q1997\\E", "shortCiteRegEx": "Dash and Liu.", "year": 1997}, {"title": "Theoretical and empirical analysis of ReliefF and RReliefF", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Machine Learning,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2003\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2003}, {"title": "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "Lichman.,? \\Q2013\\E", "shortCiteRegEx": "Lichman.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Reference (Fern\u00e1ndez-Delgado et al., 2014) is a main representative of such studies, which used 121 data sets to evaluate 179 classifiers.", "startOffset": 10, "endOffset": 42}, {"referenceID": 0, "context": "Moreover, the study will use the area under the ROC (AUC) and F-Measure, instead of the accuracy measure (as was done in Fern\u00e1ndez-Delgado et al., 2014), to evaluate the performance of classification models. These measures were chosen based on the recent consensus that the accuracy measure has significant shortcomings when compared with the above two measures, especially AUC. To identify a list of benchmark data sets that are hard for representative classification and feature selection methods, we perform a systematical evaluation of 54 combinations, involving nine representative classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository. We note that some data sets with known high classification accuracy based on results of Fern\u00e1ndez-Delgado et al. (2014) were excluded in our experiments.", "startOffset": 121, "endOffset": 824}, {"referenceID": 2, "context": "3 experiments, we also considered the wrapper based method, but we decided to exclude it due to its computational expensiveness (see Table X in Appendix) (it is seldom used in practice (Li et al., 2016) due to the same reason).", "startOffset": 185, "endOffset": 202}, {"referenceID": 3, "context": "0 (Hall et al., 2009).", "startOffset": 2, "endOffset": 21}, {"referenceID": 0, "context": "In particular, reference (Fern\u00e1ndez-Delgado et al., 2014) showed Random Forest and SVM are often better than the others, and reference (Wyner et al.", "startOffset": 25, "endOffset": 57}, {"referenceID": 4, "context": ", 2014) showed Random Forest and SVM are often better than the others, and reference (Wyner et al. 2015) gave a list of common-used successful classification algorithms.", "startOffset": 85, "endOffset": 104}, {"referenceID": 5, "context": "Feature selection methods are used to remove irrelevant, redundant, or noisy attributes, with the aim of speeding up the computation and improving the accuracy (Li et al., 2009; Liu and Yu, 2005; Dash and Liu, 1997).", "startOffset": 160, "endOffset": 215}, {"referenceID": 6, "context": "Feature selection methods are used to remove irrelevant, redundant, or noisy attributes, with the aim of speeding up the computation and improving the accuracy (Li et al., 2009; Liu and Yu, 2005; Dash and Liu, 1997).", "startOffset": 160, "endOffset": 215}, {"referenceID": 5, "context": "We selected the following filter methods because they are commonly used (Liu and Yu, 2005; Robnik-\u0160ikonja and Kononenko, 2003): the correlation based method, the information gain based method and the relief-f method.", "startOffset": 72, "endOffset": 126}, {"referenceID": 7, "context": "We selected the following filter methods because they are commonly used (Liu and Yu, 2005; Robnik-\u0160ikonja and Kononenko, 2003): the correlation based method, the information gain based method and the relief-f method.", "startOffset": 72, "endOffset": 126}, {"referenceID": 8, "context": "Our experiments used 129 data sets, all from the UCI repository (Lichman, 2013).", "startOffset": 64, "endOffset": 79}, {"referenceID": 0, "context": "(a) From the data sets studied in (Fern\u00e1ndez-Delgado et al., 2014), we first selected 34 data sets such that the maximum reported accuracy of (Fern\u00e1ndez-Delgado et al.", "startOffset": 34, "endOffset": 66}, {"referenceID": 0, "context": ", 2014), we first selected 34 data sets such that the maximum reported accuracy of (Fern\u00e1ndez-Delgado et al., 2014) is below 0.", "startOffset": 83, "endOffset": 115}, {"referenceID": 0, "context": "(b) Because (Fern\u00e1ndez-Delgado et al., 2014) only dealt with UCI data sets dated before March 2013, we examined the 91 UCI data sets whose dates are between January 2013 and June 2016.", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "We note that RF is the best or the second best for both easy and hard data sets, which is in strong agreement with the ranking of algorithms provided by (Fern\u00e1ndez-Delgado et al., 2014).", "startOffset": 153, "endOffset": 185}, {"referenceID": 0, "context": "However, Figure 3 shows that SVM-PN is the last one in the rank for both easy and hard data sets, which is very different from the ranking give by (Fern\u00e1ndez-Delgado et al., 2014) (which found SVM to be the second best classification algorithm).", "startOffset": 147, "endOffset": 179}, {"referenceID": 0, "context": "There are at least three potential reasons for the disagreement: We used AUC and F-Measure whereas (Fern\u00e1ndez-Delgado et al., 2014) used accuracy.", "startOffset": 99, "endOffset": 131}, {"referenceID": 0, "context": "15 data sets for which (Fern\u00e1ndez-Delgado et al., 2014) reported high classification accuracies (by any of the classification algorithms) and we included some data sets not studied in (Fern\u00e1ndez-Delgado et al.", "startOffset": 23, "endOffset": 55}, {"referenceID": 0, "context": ", 2014) reported high classification accuracies (by any of the classification algorithms) and we included some data sets not studied in (Fern\u00e1ndez-Delgado et al., 2014).", "startOffset": 136, "endOffset": 168}, {"referenceID": 0, "context": "(3) In our ranking, we separated data sets into a hard pool and an easy pool, whereas (Fern\u00e1ndez-Delgado et al., 2014) considered all data sets in one pool.", "startOffset": 86, "endOffset": 118}], "year": 2017, "abstractText": "The paper first reports an experimentally identified list of benchmark data sets that are hard for representative classification and feature selection methods. This was done after systematically evaluating a total of 54 combinations of methods, involving nine state-of-the-art classification algorithms and six commonly used feature selection methods, on 129 data sets from the UCI repository (some data sets with known high classification accuracy were excluded). In this paper, a data set for classification is called hard if none of the 54 combinations can achieve an AUC over 0.8 and none of them can achieve an F-Measure value over 0.8; it is called easy otherwise. A total of 17 out of the 129 data sets were found to be hard in that sense. This paper also compares the performance of different methods, and it produces rankings of classification methods, separately on the hard data sets and on the easy data sets. This paper is the first to rank methods separately for hard data sets and for easy data sets. It turns out that the classifier rankings resulting from our experiments are somehow different from those in the literature and hence they offer new insights on method selection.", "creator": "PDFCreator 2.5.1.5"}}}