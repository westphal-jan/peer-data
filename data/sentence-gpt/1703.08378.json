{"id": "1703.08378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Feature Fusion using Extended Jaccard Graph and Stochastic Gradient Descent for Robot", "abstract": "Robot vision is a fundamental device for human-robot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. This feature represents the most flexible approach to design robotics that utilizes artificial intelligence in a wide range of different types of human and robot interaction (e.g., robot-human interface and robotics automation). The FGF algorithm is a combination of several possible approaches for computer vision (e.g., artificial intelligence) to be developed by artificial intelligence. These include a method called the deep learning algorithm. FGF is a combination of different algorithms of different algorithms and the underlying hardware to produce the system.\n\n\n\n\n\n\n\nThe first FGF algorithm is the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep learning algorithm\" (the \"deep", "histories": [["v1", "Fri, 24 Mar 2017 11:58:14 GMT  (2353kb,D)", "http://arxiv.org/abs/1703.08378v1", "Assembly Automation"]], "COMMENTS": "Assembly Automation", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["shenglan liu", "muxin sun", "wei wang", "feilong wang"], "accepted": false, "id": "1703.08378"}, "pdf": {"name": "1703.08378.pdf", "metadata": {"source": "CRF", "title": "Feature Fusion using Extended Jaccard Graph and Stochastic Gradient Descent for Robot", "authors": ["Shenglan Liu", "Muxin Sun", "Wei Wang", "Feilong Wang"], "emails": ["liusl@dlut.edu.cn)."], "sections": [{"heading": null, "text": "Index Terms\u2014Jaccard graph, word embedding, Feature fusion.\nI. INTRODUCTION The object recognition is one of the important problems in machine vision and essential capabilities for social robot in real word environments. Object recognition in real word is a challenged problem because of environment noisy, complex viewpoint, illumination change and shadows. 2D camera always cannot deal with such hard task. Kinect [13] released promising a new approach to help compliant hand designing [1], recognize objects and human (emotions) for robot. The characteristic of the new Kinect 2.0 release are list as follows (see figure 1):\nRGB CameraTake the color image/video in the scope of view.\nIR Emitters: When actively projected Near Infrared Spectrum (NIS) irradiates to rough object or through a frosted glass, spectrum will distort and form random spots (called speckle) that can be read by an infrared camera.\nDepth CameraAnalyze infrared spectrum, and create RGBDepth (RGB-D) images of human body and objects in the visual field.\nMicrophone ArrayEquip built-in components (e.g. Digital Signal Processor (DSP)) to collect voice and filter background noise simultaneously. The equipment can locate the sound source direction.\nKinect (RGB-D sensor) can capture a color image and corresponding depth information of each pixel for real word objects and scenes synchronously. The color and depth images are complementary information for real-word tasks. A\nShenglan Liu and Wei Wang are with Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, Liaoning, 116024 China. Feilong Wang is with the School of Innovation and Entrepreneurship, Dalian University of Technology, Dalian, Liaoning, 116024 China. Muxin Sun is with the State Key Laboratory of Software Architecture (Neusoft Corporation). e-mail: ( liusl@dlut.edu.cn).\nsignificant number of applications are exploited by RGB-D sensor. It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.. In this paper, we only discuss recognition problem by using RGB-D sensor. The recent feature-based object recognition methods mainly fall into three categories: converting 3D point clouds, jointing 2D and depth image features and designing RGB-D feature.\nTo extract 3D object feature, Bo et al.[16] utilizes depth information and maps to generate 3D point clouds. Motivated by local feature in RGB image, depth kernel feature can be extracted for 3D point clouds. However, feature of 3D point clouds will suffer from noisy and the limited views while only one view/similar views is available and noisy involved. Jointing 2D and depth image features is a flexible approach for RGB-D vision learning. This relies on many excellent 2D image descriptors which are proposed in computer vision.\nLocal Binary Patterns (LBP) [17], [18] and Histogram of Gradient (HOG) [19], which according to the texture and edge of image respectively. Recently, motivated by the visual perception mechanisms for image retrieval, perceptual uniform descriptor (PUD)[20] achieves high performance by involving human perception. PUD defined the perceptual-structures by the similarity of edge orientation and the colors, and introduced structure element correlation statistics to capture the spatial correlation among them. On the contrary, local image descriptors focus on describing local information which includes edge and gradient information etc.. Lowe et al. [21] introduced a classical local descriptor called scale-invariant feature transform (SIFT), which aims to detect and describe local neighborhoods closing to key points in scale space. SIFT\nar X\niv :1\n70 3.\n08 37\n8v 1\n[ cs\n.C V\n] 2\n4 M\nar 2\n01 7\nand HOG are both can be included in Bag of Words (BOW) framework as image descriptor. Gabor wavelets [22] have been also applied to image understand, which due to vision similarity of human and Gabor wavelets. HMAX-based image descriptor [23] according to the hierarchical visual processing in the primary visual cortex (V1) can get promising results in vision tasks. The above image descriptors can be selected to process the RGB and depth image respectively. However, most jointing features are incompatible, which means difficult to choose a suitable weight for jointing.\nTo full use of the 2D image descriptor, we intend to utilize fusion approach to get 3D object feature for further RGB-D vision learning. Few feature fusion methods are proposed in RGB-D recognition to our best knowledge. The most popular approach is multi-view graph learning [34]. This approach can get good performance while only classifies a small quantity of objects.\nIn this paper, a new feature graph fusion (FGF) method is proposed for RGB and depth images. We first utilize Jaccard similarity to construct a graph of RGB and depth images, which indicates the similarity of pair-wise images. Then, fusion feature of RGB and depth images can be computed by our Extended Jaccard Graph (EJG) using word embedding method. Our Feature Graph Fusion can get better performance\nand efficiency in RGB-D sensor for robots. Our simple Kinectbased robot is show in figure 2."}, {"heading": "II. BOW BASED ON SIFT AND CNN-RNN", "text": "In this section, we will introduce the fundamental features which are used in our paper. As describe in introduction section, local and Bio-Inspired features always can achieve better results than other type ones. Subsection 2.1 and 2.2 introduce BOW and CNN-RNN features respectively."}, {"heading": "A. BOW based on SIFT", "text": "Scale Invariant Feature Transform (SIFT) was first introduced to extract distinctive local features for image matching. This feature has great discriminative performance and robustness and has been widely applied for various vision tasks. It is significantly invariant to translation, rotation and rescaling of images, and also has certain robustness to change in 3D viewpoint and illumination. There are two major stages in SIFT for maintaining the superior properties: detector and descriptor.\nSIFT detector aims at find out the key points or regions in the Gaussian scale space. Since natural images from camera or other devices tend to be sampled from different views, it is necessary to construct scale space pyramid to simulate all the possible scales for identifying accurately the locations and scales of key points. And then the locations can be determined using local extreme detection in the difference-of-Gaussian scale space. Some low contrast or edge responses need to be further removed due to their less discrimination.\nSIFT descriptor is computed using the image gradients in the neighborhood of key point. In order to maintain rotation invariance, the descriptors need to be rotated relative to the key point orientation. And then by computing the gradient information in the neighborhood of key point, the descriptor characterizes the orientation distribution around the key point which is distinctive and partially robust to illumination or 3D viewpoint. So for each key points detected above, a 128-D feature vector is created to extract the local discriminative information.\nEven though SIFT has superior performance in local feature description and matching without high efficiency, it is still not appropriate to be used for analyzing the holistic image feature directly considering the large amount of key points in each image. So by introducing Bag-of-Word model to be combined with SIFT, it takes the main SIFT vectors as the basic words, which preserves the great distinction of SIFT. The main procedure for this strategy is to find out the cluster centers as words by pre-training with k-means, and then create the words vectors by assigning the whole SIFT key points into the nearest word. Normally, these cluster centers contains the discriminative patches among images. So the selected number of words also plays an important part in image representation. The large words might create elaborate features which describe more detailed information, while the small words mainly consider coarse distribution of these SIFT descriptors.\nIn D-SIFT, SIFT descriptor is created in the whole image region without detecting the key points in scale-space. Instead\nof smoothing the images by Gaussian kernel in scale-space, the image is pre-smoothed before feature description. So it is much faster than standard SIFT since the key point detection tends to be very time-consuming in large-scale image understanding. The main process of BOW using D-SIFT is concluded as Fig. 3."}, {"heading": "B. CNN-RNN (CRNN)", "text": "Richard Socher et. al [24] proposed CNN-RNN model which has 2 steps: 1) learning the CNN filters in an unsupervised way by clustering random patches and then feeding these patches into a CNN layer. 2) using the resulting lowlevel, translationally invariant features generated from the first step to compose higher order features that can then be used to classify the images with RNNs.\nFirst, random patches are extracted into two sets: RGB and depth. Then, each set of patches is normalized and whitened. K-means classifier is used to cluster patches for preprocessing.\nSecond, a CNN architecture is chosen for its translational invariance properties to generate features for the RNN layer. The main idea of CNNs is to convolve filters over the input image. The single layer CNN is similar to the one proposed by Jarrett et. al [25] and consists of a convolution, followed by rectification and local contrast normalization (LCN) [25], [26], [27]. Each image of size (height and width) dI is convolved with K square filters of size dP , resulting in K filter responses, and each of which is with dimensionality dI \u2212 dP + 1. After that, the image averagely pools them with square regions of size d` and a stride size of s, to obtain a pooled response with width and height equal to r = (dI \u2212 d`)/s + 1. So the output X of the CNN layer applied to one image is a K \u00d7 r \u00d7 r dimensional 3D matrix. The same procedure is applied to both color and depth images separately.\nThe idea of recursive neural networks [28], [29] is to learn hierarchical feature representations by applying the same neural network recursively in a tree structure. In the case of CNN-RNN model, the leaf nodes of the tree are K-dimensional vectors (the result of the CNN pooling over an image patch repeated for all K filters) and there are r2 of them.\nIt starts with a 3D matrix X \u2208 RK\u00d7r\u00d7r for each image (the columns are K-dimensional), and defines a block to be a list of adjacent column vectors which are merged into a parent vector p \u2208 RK . For convenience, only square blocks with size\nK \u00d7 b \u00d7 b are employed. For instance, if vectors are merged in a block with b = 3, it will output a total size 128\u00d7 3\u00d7 3 and a resulting list of vectors (x1, \u00b7 \u00b7 \u00b7 , x9). In general, their are b2 vectors in each block. The neural network where the parameter matrix W \u2208 R2K , f is a nonlinearity such as tanh. Generally, there will be (r/b)2 parent vectors p, forming a new matrix P1. The vectors in P1 will again be merged in blocks just as those in matrix X with the same tied weights resulting in matrix P2."}, {"heading": "III. FEATURE GRAPH FUSION", "text": "In this section, EJG will propose by Jaccard similarity for robust graph construction which is important to our FGF in this paper."}, {"heading": "A. Extended Jaccard Graph", "text": "We use extended Jaccard graph to construct a fused graph to compute feature fusion. The detail of graph fusion are described in this subsection.\nWe first define H \u2208 RD\u00d7n as the original image set. Let hq denote the query, and Nk (hq) = {hqc} represent the KNNS1 of hq , c = 1, \u00b7 \u00b7 \u00b7 , k. Nk (hq) is the original ranking list which returns top-k images of xq . Similar to hq , we denote Nk1 (hqc) = { hiqc }\nas the KNNS of hqc, i = 1, \u00b7 \u00b7 \u00b7 , k1, Nk2 ( hiqc )\nas the KNNS of hiqc. Jaccard coefficient (J (\u00b7, \u00b7)) is set to measure the similarity of hqc and hq as follows:\nJ (hqc, hq) = |Nk1 (hqc) \u2229Nk (hq)| |Nk1 (hqc) \u222aNk (hq)|\n(1)\nThe information in J (hqc, hq) is more than that in norm measure of hqc and hq . In construction of the graph, the edge weight between hqc and hq is denoted by w (hqc, hq) in Eq.(1), where \u03b1 is a decay coefficient.\nTo avoid outliers in Nk1 (hqc), we consider comparing Nk1 (hqc) with Nk2 ( hiqc )\nby the similar process of hqc and hq as follows:\nJ ( hqc, h i qc ) = \u2223\u2223Nk1 (hqc) \u2229Nk2 (hiqc)\u2223\u2223\u2223\u2223Nk1 (hqc) \u222aNk2 (hiqc)\u2223\u2223 (2) We utilize the results of Eq. (2) to define in Eq.(3). If the\nvalue of w\u2032 ( hqc, h i qc ) is small enough, hqc is the outlier to query.\n1KNNS indicates the k nearest neighborhood of a sample.\n` = \u2211\ni,j\u2208V Wij \u00b7 log\n( exp ( fTi fj )\u2211 j\u2208V exp ( fTi fj )) \u2248 \u2211 i\u2208V \u2211 j\u2208s(i) log ( exp(fTi fj)\u2211 j\u2208V exp(fTi fj) ) M\n(7)\nw\u2032 ( hqc, h i qc ) =\n{ 1 ( J ( hqc, h i qc ) > 0 ) \u2227 ( hiqc \u2208 Nk1 (hqc) ) 0 else\n(3) Then, the weight of hqc and hq can be computed by Eq.\n(4) as follows\nw (hqc, hq) = \u2211\nhiqc\u2208Nk1 (hqc)\nw\u2032 ( hqc, h i qc ) (4)\nIn order to obtain the complementary information of RGB and depth image features to improve the accuracy of machine/robot recognition, we need to fuse multi-feature of images. We denote V as node, E as edge and w as weight in image graph. Assuming RGB and depth features have been extracted from an object. Then RGB and depth graphs can be constructed by Extended Jaccard Graph in reference [30]. In graph fusion methods, the RGB feature graph defines as Grgb = ( V rgb, Ergb, wrgb ) , and depth feature graph can be\ndenoted by Gdepth = ( V depth, Edepth, wdepth ) . Multi-feature graph can be expressed by G = (V,E,w) which satisfies three constrains as follows: 1) V = V rgb \u22c3 V depth; 2) E =\nErgb \u22c3 Edepth; 3) w (x\u0302, xq) = wrgb (x\u0302, xq) \u22c3 wdepth (x\u0302, xq). The fusion graph G can be treat as the relationships between images in dataset. We can also get the final fusion feature on G in the next subsection."}, {"heading": "B. Fusion Feature by Word Embedding", "text": "We fuse RGB weight affinity matrix wrgb and depth weight affinity matrix wdepth as affinity matrix W = [Wij ], where i, j \u2208 V are denotes in subsection IIIA. Then, we can get the normalized neighborhood affinity matrixW = [Wij ], where i, j \u2208 V . Wij can be expressed using a Gaussian kernel as follows.\nWij =  exp ( \u2212 Wij 2\u03c32 i ) \u2211 j\u2032\u2208Nk(i) exp ( \u2212 W ij\u2032 2\u03c32 i ) , j \u2208 Nk (i) 0, else\n(5)\nwhere \u03c3i is the bandwidth parameter of Gaussian kernel, we denote \u03c3i by variance of the i-th row.\nThe fused features are implicit expression in the normalized neighborhood affinity matrix W = [Wij ]. We use the following optimization models to get the fused features.\nmax F \u220f i,j\u2208V\n( exp ( fTi fj )\u2211 j\u2208V exp ( fTi fj ))Wij (6) , where {f1, f2, \u00b7 \u00b7 \u00b7 , fn} \u2208 Rd\u00d7n, fi is the fused feature of the i-th RGB-D image pair. We change the likelihood function into log function as follows.\n, where s(i) indicates sampling M times according to the distribution function which generates by the i-th row of W . The optimization function can get F by using word embedding model[31], [32]."}, {"heading": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "text": "In this section, we use two datasets to evaluate our feature fusion method. We first introduce the parameters and details of the two dataset. Then, the results and its analysis of the experiments are list in subsection 4.2."}, {"heading": "A. Details of Datasets", "text": "The dataset 1 and dataset 2 are collected by Kinect V1 and V2 respectively. The difference between V1 and V2 are listed in Table 1. Dataset 1 is recorded by Kinect V2 and Dataset 2 is given by Kinect V1. The two datasets are described as follows.\nDataset 1 (DUT RGB-D face dataset): This dataset utilizes Microsoft Kinect for Xbox one V2.0 camera to acquire images, which acquires RGB images as well as depth images. This dataset contains 1620 RGB-D (RGB and depth) photos recorded with 6480 files of 54 people. Each class includes 30 faces. Expressions of happiness, anger, sorrow, scare and surprise are acquired from five different angles (up, down, left, right and middle) for each person. Color photos are recorded with 8 bits, and each color image is decomposed into three files (R, G, and B). Depths photos are recorded using 16-bit data to guarantee the depth of facial small changes are accurately recorded. All people in these photos do not wear glasses to ensure the precision of expression acquisition.\nDataset 2: The RGB-D Household Object Dataset contains 300 household objects. The dataset was captured by a Kinect V1 camera. Each image pair are RGB and depth images (RGB size: 640\u00d7480 and depth image at 30 Hz). The objects fall into 51 categories. The objects are obtained by RGB-D video from different angles of each object. More details can be referred to [16]."}, {"heading": "B. Experimental results and analysis", "text": "In social robot tasks, recognition and grasp are both important to applications. The experimental results of DUT RGB-D face dataset are listed in table 2 and figure 4 as follows:\nWe use dense SIFT method to extract feature of RGB-D face dataset, and utilize one Vs. rest SVM classifer to complete the face recognition. In face recognition, we extract 3 training faces in each class and the rest as the testing set. As can be seen in table 2, depth information is more effective than RGB representation. The RGB recognition rate is 82.30% which is 22.29% higher than the depth faces. This is because that face recognition is high related to RGB. An important result is that RGB+depth feature deduced 0.27% than single RGB feature. This phenomenon illustrates joint feature may suffer from data distribution changed. Our method achieve 84.50% which is higher than any single feature (or joint feature) and can deal with the joint shortcoming by fused graph feature extraction.\nFig. 4 shows parameter influence of our fusion model. We can see that FGF is not sensitive to the change of parameter.\nIn object experiment, we use CNN-RNN features extracting from RGB and depth images and split 10 times. Each split of testing set selects all images of one instance and the rest as training set. Table 3 shows the results of object RGB-D recognition. Different from face experiments, object recognition using depth information can get 92.98% higher precision than that using RGB images. This result illustrates\nobject recognition more relies on \u201cdepth feeling\u201d. Our fused feature is more effective and efficiency than the joint one (reach 93.92% only using 200 dimension feature), though the joint feature enhances the precision in object dataset. Table 4 and Fig. 52 illustrate that our method can achieve more higher results than other state-of-the-art methods."}, {"heading": "V. CONCLUSION", "text": "In this paper, we built a vision robot with RGB-D camera and gave a DUT RGB-D face dataset. We mainly proposed a RGB-D recognition method FGF and evaluated FGF in two RGB-D datasets. FGF can get better performance than previous approach and can help robot to execute complex tasks, such as SLAM, compliant hand designing, human-robot interaction etc.. We will consider designing a more effective sensor and robust supervised dimensionality reduction method (such as reference [35]) as robot vision in our future work."}], "references": [{"title": "The compliance of robotic hands\u2013from functionality to mechanism[J", "author": ["R Li", "W Wu", "H. Qiao"], "venue": "Assembly Automation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fear Detection with Background Subtraction from RGB-D data[J", "author": ["A Veenendaal", "E Daly", "E Jones"], "venue": "Computer Science and Emerging Research Journal,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "A survey on human motion analysis from depth data[M]//Time-of-flight and depth imaging. sensors, algorithms, and applications", "author": ["M Ye", "Q Zhang", "L Wang"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "On RGB-D face recognition using Kinect[C]//Biometrics: Theory, Applications and Systems (BTAS)", "author": ["G Goswami", "S Bharadwaj", "M Vatsa"], "venue": "IEEE Sixth International Conference on", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "An efficient LBP-based descriptor for facial depth images applied to gender recognition using RGB-D face data[C]//Asian Conference on Computer Vision", "author": ["T Huynh", "R Min", "L. Dugelay J"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On facial expressions and emotions RGB-D database[C]//International Conference: Beyond Databases, Architectures and Structures", "author": ["M. Szwoch"], "venue": "Springer International Publishing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Face recognition robust to head pose changes based on the RGB-D sensor[C]//Biometrics: Theory, Applications and Systems (BTAS)", "author": ["C Ciaccio", "L Wen", "G. Guo"], "venue": "IEEE Sixth International Conference on. IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Kinectfacedb: A kinect database for face recognition[J", "author": ["R Min", "N Kose", "L. Dugelay J"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "A head pose tracking system using RGB- D camera[C]//International Conference on Computer Vision Systems", "author": ["S Li", "N Ngan K", "L. Sheng"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Learning rich features from RGB- D images for object detection and segmentation[C]//European", "author": ["S Gupta", "R Girshick", "P Arbel\u00e1ez"], "venue": "Conference on Computer Vision. Springer International Publishing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Histogram of 3D facets: A depth descriptor for human action and hand gesture recognition[J", "author": ["C Zhang", "Y. Tian"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Human detection using depth information by kinect[C]//CVPR 2011 WORKSHOPS", "author": ["L Xia", "C Chen C", "K. Aggarwal J"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Kinect and rgbd images: Challenges and applications[C]//Graphics, Patterns and Images", "author": ["L Cruz", "D Lucio", "L. Velho"], "venue": "Tutorials (SIBGRAPI-T),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "RGBD object recognition and visual texture classification for indoor semantic mapping[C]//2012", "author": ["D Filliat", "E Battesti", "S Bazeille"], "venue": "IEEE International Conference on Technologies for Practical Robot Applications (TePRA). IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Hand gesture recognition using Kinect[C]//2012", "author": ["Y. Li"], "venue": "IEEE International Conference on Computer Science and Automation Engineering", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Depth kernel descriptors for object recognition[C]//2011", "author": ["L Bo", "X Ren", "D. Fox"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns[J", "author": ["T Ojala", "M Pietik\u00e4inen", "T. M\u00e4enp\u00e4\u00e4"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Gray scale and rotation invariant texture classification with local binary patterns[M]//Computer Vision-ECCV", "author": ["T Ojala", "M Pietik\u00e4inen", "T M\u00e4enp\u00e4\u00e4"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Histograms of oriented gradients for human detection[C]//IEEE", "author": ["N Dalal", "B Triggs"], "venue": "Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Perceptual uniform descriptor and Ranking on manifold: A bridge between image representation and ranking for image retrieval[J", "author": ["S Liu", "J Wu", "L Feng"], "venue": "arXiv preprint arXiv:1609.07615", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Distinctive image features from scale-invariant keypoints[J", "author": ["G Lowe D"], "venue": "International journal of computer vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex[J", "author": ["P Jones J", "A Palmer L"], "venue": "Journal of neurophysiology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1987}, {"title": "Convolutional-recursive deep learning for 3d object classification.", "author": ["Socher", "Richard"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "What is the Best Multi-Stage Architecture for Object Recognition? In ICCV", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D.D. Cox", "J.J. DiCarlo"], "venue": "PLoS Comput Biol,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M.A. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "ICML,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Recursive distributed representations", "author": ["J.B. Pollack"], "venue": "Artificial Intelligence, 46,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1990}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "ICML,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Three Tiers Neighborhood Graph and Multi-graph Fusion Ranking for Multi-feature Image Retrieval: A Manifold Aspect[J", "author": ["S Liu", "M Sun", "L Feng"], "venue": "arXiv preprint arXiv:1609.07599", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality[C]//Advances in neural information processing", "author": ["T Mikolov", "I Sutskever", "K Chen"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Efficient estimation of word representations in vector space[J", "author": ["T Mikolov", "K Chen", "G Corrado"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Robust Visual Knowledge Transfer via Extreme Learning Machine based Domain Adaptation", "author": ["L Zhang", "D Zhang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Visual Understanding via Multi-Feature Shared Learning with Global Consistency", "author": ["L Zhang", "D. Zhang"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Scatter Balance: An Angle-Based Supervised Dimensionality Reduction[J", "author": ["S Liu", "F Lin", "Q Hong"], "venue": "IEEE Transactions on Neural Networks & Learning Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Kinect [13] released promising a new approach to help compliant hand designing [1], recognize objects and human (emotions) for robot.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "Kinect [13] released promising a new approach to help compliant hand designing [1], recognize objects and human (emotions) for robot.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 4, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 5, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 6, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 7, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 13, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 14, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 31, "context": "It can be referred to object detection [2], [10], [12], human motion analysis [3], [11], object tracking [9], and object/human recognition [4-8, 14, 15, 33] etc.", "startOffset": 139, "endOffset": 156}, {"referenceID": 15, "context": "[16] utilizes depth information and maps to generate 3D point clouds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Local Binary Patterns (LBP) [17], [18] and Histogram of Gradient (HOG) [19], which according to the texture and edge of image respectively.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "Local Binary Patterns (LBP) [17], [18] and Histogram of Gradient (HOG) [19], which according to the texture and edge of image respectively.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Local Binary Patterns (LBP) [17], [18] and Histogram of Gradient (HOG) [19], which according to the texture and edge of image respectively.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "Recently, motivated by the visual perception mechanisms for image retrieval, perceptual uniform descriptor (PUD)[20] achieves high performance by involving human perception.", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "[21] introduced a classical local descriptor called scale-invariant feature transform (SIFT), which aims to detect and describe local neighborhoods closing to key points in scale space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Gabor wavelets [22] have been also applied to image understand, which due to vision similarity of human and Gabor wavelets.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "The most popular approach is multi-view graph learning [34].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "al [24] proposed CNN-RNN model which has 2 steps: 1) learning the CNN filters in an unsupervised way by clustering random patches and then feeding these patches into a CNN layer.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "al [25] and consists of a convolution, followed by rectification and local contrast normalization (LCN) [25], [26], [27].", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "al [25] and consists of a convolution, followed by rectification and local contrast normalization (LCN) [25], [26], [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "al [25] and consists of a convolution, followed by rectification and local contrast normalization (LCN) [25], [26], [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "al [25] and consists of a convolution, followed by rectification and local contrast normalization (LCN) [25], [26], [27].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "The idea of recursive neural networks [28], [29] is to", "startOffset": 38, "endOffset": 42}, {"referenceID": 27, "context": "The idea of recursive neural networks [28], [29] is to", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "Then RGB and depth graphs can be constructed by Extended Jaccard Graph in reference [30].", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "The optimization function can get F by using word embedding model[31], [32].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "The optimization function can get F by using word embedding model[31], [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "More details can be referred to [16].", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "We will consider designing a more effective sensor and robust supervised dimensionality reduction method (such as reference [35]) as robot vision in our future work.", "startOffset": 124, "endOffset": 128}], "year": 2017, "abstractText": "Robot vision is a fundamental device for humanrobot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. FGF involves multi-Jaccard similarity to compute a robust graph and utilize word embedding method to enhance the recognition results. We also collect DUT RGB-D face dataset and a benchmark datset to evaluate the effectiveness and efficiency of our method. The experimental results illustrate FGF is robust and effective to face and object datasets in robot applications.", "creator": "LaTeX with hyperref package"}}}