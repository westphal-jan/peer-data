{"id": "1703.08769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Open Vocabulary Scene Parsing", "abstract": "Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations, such as the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 26 Mar 2017 05:44:56 GMT  (5445kb,D)", "https://arxiv.org/abs/1703.08769v1", null], ["v2", "Tue, 4 Apr 2017 18:28:20 GMT  (8759kb,D)", "http://arxiv.org/abs/1703.08769v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hang zhao", "xavier puig", "bolei zhou", "sanja fidler", "antonio torralba"], "accepted": false, "id": "1703.08769"}, "pdf": {"name": "1703.08769.pdf", "metadata": {"source": "CRF", "title": "Open Vocabulary Scene Parsing", "authors": ["Hang Zhao", "Xavier Puig", "Bolei Zhou", "Sanja Fidler", "Antonio Torralba"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "One of the grand goals in computer vision is to recognize and segment arbitrary objects in the wild. Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12]. However, nowadays most recognition models are still not capable of classifying objects at the level of a human, in particular, taking into account the taxonomy of object categories. Ordinary people or laymen classify things on the entry-levels, and experts give more specific labels: there is no object with a single correct label, so the prediction vocabulary is inherently open-ended. Furthermore, there is no widely-accepted way to evaluate open-ended recognition tasks, which is also a main reason this direction is not pursued more often.\nIn this work, we are pushing towards open vocabulary scene parsing: model predictions are not limited to a fixed set of categories, but also words in a larger dictionary, or even a knowledge graph. Considering existing image parsing datasets only contain a small number of categories (~100 classes), there is much more a model can learn from those images given extra semantic knowledge, like WordNet dictionary (~100,000 synsets) or Word2Vec from external corpus.\nTo solve this new problem, we propose a framework that is able to segment all objects in an image using open vocabulary labels. In particular, while the method strives to label each pixel with the same word as the one used by the human annotator, it resorts to a taxonomy when it is not sure about its prediction. As a result, our model aims to make plausible predictions even for categories that have not been shown during training, e.g. if the model has never seen tricycle, it may still give a confident guess on vehicle, performing more like a human.\nOur framework incorporates hypernym/hyponym relations from WordNet [19] to help parsing. More concretely, word concepts and image pixel features are embedded into a joint high-dimentional vector space so that (1) hypernym/hyponym relations are preserved for the concepts, (2) image pixel embeddings are close to concepts related to their annotations according to some distance measures. This framework offers three major advantages: (1) predictions are made in a structured way, i.e., they can be intermediate nodes in WordNet, and thus yielding more reasonable mistakes; (2) it is an end-to-end trainable system, its vocab-\n1\nar X\niv :1\n70 3.\n08 76\n9v 2\n[ cs\n.C V\n] 4\nA pr\n2 01\n7\nulary can be huge and is easily extensible; (3) the framework leaves more freedom to the annotations: inconsistent annotations from workers with different domain knowledge have less of an affect on the performance of the model.\nWe additionally explored several evaluation metrics, which are useful measures not only for our open vocabulary parsing tasks, but also for any large-scale recognition tasks where confusions often exist.\nThe open vocabulary parsing ability of the proposed framework is evaluated on the recent ADE20K dataset [34]. We further explore the properties of the embedding space by concept retrieval, classification boundary loosing and concept synthesis with arithmetics."}, {"heading": "1.1. Related work", "text": "Our work is related to different topics in literature which we briefly review below.\nSemantic segmentation and scene parsing. Due to astonishing performance of deep learning, in particular CNNs [14], pixel-wise dense labeling has received significant amount of attention. Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.\nThese networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects. However, these models are not easily adaptable to new objects. In this paper we aim at going beyond this limit and to make predictions in the wild.\nZero-shot learning. Zero-shot learning addresses knowledge transfer and generalization [25, 10]. Models are often evaluated on unseen categories, and predictions are made based on the knowledge extracted from the training categories. Rohrbach [26] introduced the idea to transfer large-scale linguistic knowledge into vision tasks. Socher et al. [28] and Frome et al. [9] directly embedded visual features into the word vector space so that visual similarities are connected to semantic similarities. Norouzi et al. [21] used a convex combination of visual features of training classes to represent new categories. Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].\nHierarchical classifications. Hierarchical classification addresses the common circumstances that candidate categories share hierarchical semantic relations. Zweig et al. [35] combined classifiers on different levels to help improve classification. Deng et al. [7] achieved hierarchical imagelevel classification by trading off accuracy and gain as an optimization problem. Ordonez et al. [22], on the other hand, proposed to make entry-level predictions when deal-\ning with a large number of categories. More recently, Deng et al. [5] formulated a label relation graph that could be directly integrated with deep neural networks.\nOur approach on hierarchical parsing is inspired by the order-embeddings work [29], we attempt to construct an asymmetric embedding space, so that both image features and hierarchical information in the knowledge graph are effectively and implicitly encoded by the deep neural networks. While most previous approaches combine deep neural networks with optimization methods like conditional random fields so that the semantic relatedness is incorporated into the framework, the advantage of our approach is that it makes an end-to-end trainable network, which is easily scalable when dealing with larger datasets in practical applications."}, {"heading": "2. Learning joint embeddings for pixel features", "text": "and word concepts\nWe treat open-ended scene parsing as a retrieval problem for each pixel, following the ideas of image-caption retrieval work [29]. Our goal is to embed image pixel features and word concepts into a joint high-dimensional positive vector space RN+ , as illustrated in Figure 2. The guiding principle while constructing the joint embedding space is that image features should be close to their concept labels, and word concepts should preserve their semantic hypernym/hyponym relations. In this embedding space, (1) vectors close to origin are general concepts, and vectors with larger norms represent higher specificity; (2) hypernym/hyponym relation is defined by whether one vector is smaller/greater than another vector in all theN dimensions. A hypernym scoring function is crucial in building this embedding space, which will be detailed in Section 2.1.\nFigure 3 gives an overview of our proposed framework. It is composed of two streams: a concept stream and an\nimage stream. The concept stream tries to encode the predefined semantics: it learns an embedding function f(\u00b7) that maps the words into RN+ so that they preserve the hypernym/hyponym relationship between word concepts. The image stream g(\u00b7) embeds image pixels into the same space by pushing them close to their labels (word concepts). We describe these two streams in more details in Section 2.2 and 2.3."}, {"heading": "2.1. Scoring functions", "text": "In this embedding problem, training is performed on pairs: image-label pairs and concept-concept pairs. For either of the streams, the goal is to maximize scores of matching pairs and minimize scores of non-matching pairs. So the choice of scoring functions S(x, y) becomes important. There are symmetric scoring functions like Lp distance and cosine similarity widely used in the embedding tasks,\nSLp(x, y) = \u2212\u2016x\u2212 y\u2016p, Scos(x, y) = x \u00b7 y. (1)\nIn order to reveal the asymmetric hypernym/hyponym relations between word concepts, hypernym scoring function [29] is indispensable,\nShyper(x, y) = \u2212\u2016max(0, x\u2212 y)\u2016p. (2)\nIf x is hypernym of y (x y), then ideally all the coordinates of x are smaller than y ( \u2227 i(xi \u2264 yi)), so Shyper(x, y) = Shyper,max = 0. Note that due to asymmetry, swapping x and y will result in total different scores."}, {"heading": "2.2. Concept stream", "text": "The objective of the concept stream is to build up semantic relations in the embedding space. In our case, the semantic structure is obtained from WordNet hypernym/hyponym\nrelations. Consider all the vocabulary concepts form a directed acyclic graph (DAG) H = (V,E), sharing a common root v\u0302 \u2208 V \u201centity\u201d, each node in the graph v \u2208 V can be an abstract concept as the unions of its children nodes, or a specific class as a leaf. A visualization of part of the DAG we built based on WordNet and ADE20K labels can be found in Supplementary Materials.\nInternally, the concept stream include parallel layers of a shared trainable lookup table, mapping the word concepts u, v to f(u), f(v). And then they are evaluated on hypernym scores Sconcept(f(u), f(v)) = Shyper(f(u), f(v)), which tells how confident u is a hypernym of v. A maxmargin loss is used to learn the embedding function f(\u00b7),\nLconcept(u, v) ={ \u2212Sconcept(f(u), f(v)) if u v, max{0, \u03b1+ Sconcept(f(u), f(v))} otherwise\nNote that positive samples u v are the cases where u is an ancestor of v in the graph, so all the coordinates of f(v) are pushed towards values larger than f(u); negative samples can be inverted pairs or random pairs, the loss function pushes them apart in the embedding space. In our training, we fix the root of DAG \u201centity\u201d as anchor at origin, so the embedding space stays in RN+ ."}, {"heading": "2.3. Image stream", "text": "The image stream is composed of a fully convolutional network which is commonly used in image segmentation tasks, and a lookup layer shared with the word concept stream. Consider an image pixel at position (i, j) with label xi,j , its feature yi,j is the top layer output of the convolutional network. Our mapping function g(yi,j) embeds the pixel features into the same space as their label f(xi,j), and then evaluate them with a scoring function Simage(f(xi,j), g(yi,j)).\nAs label retrieval is inherently a ranking problem, negative labels x\u2032i,j are introduced in training. A max-margin ranking loss is commonly used [9] to encourage the scores of true labels be larger than negative labels by a margin,\nLimage(yi,j) =\u2211 x\u2032i,j\nmax{0, \u03b2 \u2212 Simage(f(xi,j), g(yi,j)) + Simage(f(x\u2032i,j), g(yi,j))}. (3)\nIn the experiment, we use a softmax loss for all our models and empirically find better performance,\nLimage(yi,j) =\n\u2212 log e Simage(f(xi,j),g(yi,j)) eSimage(f(xi,j),g(yi,j)) + \u2211\nx\u2032i,j eSimage(f(x\n\u2032 i,j),g(yi,j))\n.\n(4) This loss function is a variation of triplet ranking loss proposed in [11].\nThe choice of scoring function here is flexible, we can either (1) simply make image pixel features \u201cclose\u201d to the embedding of their labels by using symmetric scores SLp(f(xi,j), g(yi,j)), Scos(f(xi,j), g(yi,j)); (2) or use asymmetric hypernym score Shyper(f(xi,j), g(yi,j)). In the latter case, we treat images as specific instances or specializations of their label concepts, and labels as general abstraction of the images."}, {"heading": "2.4. Joint model", "text": "Our joint model combines the two streams via a joint loss function to preserve concept hierarchy as well as visual feature similarities. In particular, we simply weighted sum the losses of two streams L = Limage + \u03bbLconcept(\u03bb = 5) during training. We set the embedding space dimension to N = 300, which is commonly used in word embeddings. Training and model details are described in Section 4.2."}, {"heading": "3. Evaluation Criteria", "text": "To better evaluate our models, metrics for different parsing tasks are explored in this section."}, {"heading": "3.1. Baseline flat metrics", "text": "While working on a limited number of classes, four traditional criteria are good measures of the scene parsing model performance: (1) pixel-wise accuracy: the proportion of correctly classified pixels; (2) mean accuracy: the proportion of correctly classified pixels averaged over all the classes; (3) mean IoU: the intersection-over-union between the predictions and ground-truth, averaged over all the classes; (4) weighted IoU: the IoU weighted by pixel ratio of each class."}, {"heading": "3.2. Open vocabulary metrics", "text": "Given the nature of open vocabulary recognition, selecting a good evaluation criteria is non-trivial. Firstly, it should leverage the graph structure of the concepts to tell the distance of the predicted class from the ground truth. Secondly, the evaluation should correctly represent the highly unbalanced distribution of the dataset classes, which are also common in the objects seen in nature.\nTo do so, for each sample/pixel, a score s(l, p) is used to measure the similarity between the ground truth label s and the prediction p. The total accuracy is the mean score over all the samples/pixels."}, {"heading": "3.2.1 Hierarchical precision, recall and F-score", "text": "Hierarchical precision, recall and F-score were also known as Wu-Palmer similarity, which was originally used for lexical selection [30].\nFor two given concepts l and p, we define the lowest common ancestor LCA as the most specific concept (i.e. furthest from the root Entity) that is an hypernym of both. Then hierarchical precision and recall are defined by the number of common hypernyms that prediction and label have over the vocabulary hierarchy H , formally:\nsHP (l, p) = dLCA dp , sHR(l, p) = dLCA dl . (5)\nwhere depth of the lowest common ancestor node dLCA is the number of hypernyms in common.\nCombining hierarchical precision and hierarchical recall, we get hierarchical F-score sHF (l, p) which defined as the depth of LCA node over the sum of depth of label and prediction nodes:\nsHF (l, p) = 2sHP (l, p) \u00b7 sHR(l, p) sHP (l, p) + sHR(l, p) = 2 \u00b7 dLCA dl + dp . (6)\nOne prominent advantage of these hierarchical metrics is they penalize predictions when they go too specific. For example, \u201cguitar\u201d (dl=10) and \u201cpiano\u201d (dp=10) are all \u201cmusical instrument\u201d (dLCA=8). When \u201cguitar\u201d is predicted as \u201cpiano\u201d, sHF = 2\u00b7810+10 = 0.8; when \u201cguitar\u201d is predicted as \u201cmusical instrument\u201d, sHF = 2\u00b7810+8 = 0.89. It agrees with human judgment that the prediction \u201cmusical instrument\u201d is more accurate than \u201cpiano\u201d."}, {"heading": "3.2.2 Information content ratio", "text": "As mentioned before, unbalanced distribution of data points could make performance dominated by frequent classes. Information content ratio, which was also used in lexical search, addresses these problems effectively.\nAccording to information theory and statistics, the information content of a message is the inverse logarithm of its\nfrequency I(c) = \u2212 logP (c). We inherit this idea and preprocess our image data to get the pixel frequency of each concept v \u2208 H . Specifically, the frequency of a concept is the sum of its own frequency and all its descendents\u2019 frequencies in the image dataset. It is expected that the root \u201centity\u201d has frequency 1.0 and information content 0.\nDuring evaluations, we measure, for each testing sample, how much information our prediction gets out of the total amount of information in the label. So the final score is determined by the information of the LCA and that of the ground truth and predicted concepts.\nsI(l, p) = 2 \u00b7 ILCA Il + Ip = 2 \u00b7 logP (LCA) logP (l) + logP (p) (7)\nAs information content ratio requires the statistics of the image dataset and the semantic hierarchy, it rewards both inference difficulty and hierarchical accuracy."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Image label and concept association", "text": "To learn the joint embedding, we associate each class in ADE20K dataset with a Synset in WordNet, representing a unique concept. The data association process requires semantic understanding, so we resort to Amazon Mechanical Turk (AMT). We develop a rigorous annotation protocol, which is detailed in Supplementary Materials.\nAfter association, we end up with 3019 classes in the dataset having synset matches. Out of these there are 2019 unique synsets forming a DAG. All the matched synsets have entity.n.01 as the top hypernym and there are in average 8.2 synsets in between. The depths of the ADE20K dataset annotations range from 4 to 19."}, {"heading": "4.2. Network implementations", "text": ""}, {"heading": "4.2.1 Concept stream", "text": "The data layer of concept stream feeds the network with positive and negative vocabulary concept pairs. The positive training pairs are found by traversing the graph H and find all the transitive closure hypernym pairs, e.g. \u201cneckwear\u201d and \u201ctie\u201d, \u201cclothing\u201d and \u201ctie\u201d, \u201centity\u201d and \u201ctie\u201d; negative samples are randomly generated before each training epoch by excluding these positive samples. Using transitive closure pair greatly improves the performance of embedding by providing us with more training data."}, {"heading": "4.2.2 Image stream", "text": "Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 32]. Considering the features of an image pixel from the last layer of the fully convolutional network fc7 to be yi,j with dimension 4096, we add a 1 \u00d7 1 convolution layer g(\u00b7) with weight dimension of 4096\u00d7300 to embed the pixel feature. To ensure positivity, we further add a ReLU layer.\nA technique we use to improve the training is to fix the norms of the embeddings of image pixelsto be 30, where a wide range of values will work. This technique stabilizes training numerically and speeds up the convergence. Intuitively, fixing image to have a large norm makes sense in the hierarchical embedding space: image pixels are most specific descriptions of concepts, while words are more general, and closer to the origin."}, {"heading": "4.2.3 Training and inference", "text": "In all the experiments, we first train the concept stream to get the word embeddings, and then use them as initializations in the joint training. Pre-trained weights from\nVGG-ImageNet [27] are used as initializations for the image stream.\nAdam optimizer [13] with learning rate 1e-3 is used to update weights across the model. The margin of loss functions is default to \u03b1 = 1.0.\nIn the inference stage, there are two cases: (1) While testing on the 150 training classes, the pixel embeddings are compared with the embeddings of all the 150 candidate labels based on the scoring function, the class with the highest score is taken as the prediction; (2) While doing zeroshot predictions, on the other hand, we use a threshold on the scores to decide the cutoff score, concepts with scores above the cutoff are taken as predictions. This best threshold is found before testing on a set of 100 validation images."}, {"heading": "4.3. Results on SceneParse150", "text": "In this section, we report the performance of our model on scene parsing task. The training is performed on the most frequent 150 classes of stuffs and objects in the ADE20K dataset, SceneParse150, where each of the class has at least 0.02% of total pixels in the dataset.\nWe have trained some models in the references and several variants of our proposed model, all of which share the same architecture of convolutional networks to make fair comparisons. Softmax is the baseline model that does classical multi-class classification.\nConditional Softmax is a hierarchical classification model proposed in [24]. It builds a tree based on the label relations, and softmax is performed only between nodes of a common parent, so only conditional probabilities for each node are computed. To get absolute probabilities during testing, the conditional probabilities are multiplied following the paths to root.\nWord2Vec is a model that simply regresses the image pixel features to pre-trained word embeddings, where we use the GoogleNews vectors. Since the dimensionality of GoogleNews vectors is 300, the weight dimension of the last convolution layer is a 1\u00d7 1\u00d7 4096\u00d7 300. Cosine similarity and max-margin ranking loss with negative samples are used during training. This model is a direct counterpart of DeViSe[9] in our scene parsing settings.\nWord2Vec+ is our improved version of Word2Vec model. There are two major modifications: (1) We replace the max\nmargin loss with a softmax loss as mentioned in Section 2.3; (2) We augment the GoogleNews vectors by finetuning them on domain specific corpus. Concretely, from AMT we collect 3 to 5 scene descriptive sentences for each image in the ADE20K training set (20,210 images). Then we finetune the pre-trained word vectors with skip-gram model [18] for 5 epochs, and these word vectors are finally fixed for regression like Word2Vec.\nThere are 6 variants of our proposed model. Model names with Image-* refer to the cases where only image stream is trained, by fixing the concept embeddings. In models Joint-* we train two streams together to learn a joint embedding space. Three aforementioned scoring functions are used for the image stream, their corresponding models are marked as *-L2, *-Cosine and *-Hyper."}, {"heading": "4.3.1 On the training classes", "text": "Evaluating on the 150 training classes, our proposed models offer competitive results. Baseline flat metrics are used to compare the performance, as shown in Table 1. Without surprise, the best performance is achieved by the Softmax baseline, which agrees with the observation from [9], classification formulations usually achieves higher accuracy than regression formulations. At the same time, our proposed models Joint-Cosine and Word2Vec+ fall short of Softmax by only around 1%, which is an affordable sacrifice given the zero-shot prediction capability and interpretability that will be discussed later. Visual results of the best proposed model Joint-Cosine are shown in Figure 4."}, {"heading": "4.3.2 Zero-shot predictions", "text": "We then move to the zero-shot prediction tasks to fully leverage the hierarchical prediction ability of our models. The models are evaluated on 500 less frequent object classes in the ADE20K dataset. Predictions can be in the 500 classes, or their hypernyms, which could be compared based on our open vocabulary metrics.\nSoftmax and Conditional Softmax models are not able to make inferences outside the training classes, so we take their predictions within the 150 classes for evaluation.\nConvex Combination [21] is another baseline model for comparison: we take the probability output from Softmax within the 150 classes, to form new embeddings in the word\nTable 2. Zero-shot parsing performance, evaluated with hierarchical metrics.\nNetworks Hierarchical Precision Hierarchical Recall Hierarchical F-score Information content ratio Softmax [34] 0.5620 0.5168 0.5325 0.1632 Conditional Softmax [24] 0.5701 0.5146 0.5340 0.1657 Word2Vec [9] 0.5782 0.5265 0.5507 0.1794 Convex Combination [21] 0.5777 0.5384 0.5492 0.1745 Word2Vec+ 0.6138 0.5248 0.5671 0.2002 Image-L2 0.5741 0.5032 0.5375 0.1650 Image-Hyper 0.6318 0.5346 0.5937 0.2136 Joint-L2 0.5956 0.5385 0.5655 0.1945 Joint-Hyper 0.6567 0.5838 0.6174 0.2226\nFigure 6. Diversity test, evaluated with hierarchical metrics.\nvector space, and then find the nearest neighbors in vector space. This approach does not require re-training, but still offers reasonable performance.\nMost of our proposed models can retrieve the hypernym of the testing classes, except *-Cosine as they throw away the norm information during scoring, which is important for hypernym predictions.\nTable 2 shows results on zero-shot predictions. In terms of the hierarchical metrics, Joint-Hyper gives the best performance. And our proposed models in general win by a large margin over baseline methods. It confirms us that modeling the asymmetric relations of data pairs better represents the hierarchy. Figure 5 shows some prediction sam-\nples of our best model Joint-Hyper (see Supplementary Materials for full predictions of our model). In each image, we only show one ground truth category to make things clear, different colors represent different predictions. Though the model does not always get the ground truth labels exactly correct, it gives reasonable predictions. Another observation is that predictions are sometimes noisy, we get 2-3 predictions on a single objects. Some of the inconsistencies are plausible though, e.g. in the first row, the upper part of the \u201crocking chair\u201d is predicted as \u201cchair\u201d while the lower part is predicted as \u201cfurniture\u201d. As the pixels in the upper segment are closer to ordinary chairs while the lower segment does not, so in the latter case the model gives a more general prediction."}, {"heading": "4.4. Diversity test", "text": "The open vocabulary recognition problem naturally raises a question: how many training classes do we need to generalize well on zero-shot tasks? To answer this question, we do a diversity test in this section.\nDifferent from the previous experiments, we do not take the most frequent classes for training, instead uniformly sample training and testing classes from the histogram of pixel numbers. For better comparison, we fix the number of zero-shot test set classes to be 500, and the training classes range from 50 to 1500. In the training process, we offset\nthe unbalance in pixel numbers by weighting the training class loss with their corresponding information content, so the less frequent classes contribute higher loss.\nWe only experiment with our best model Joint-Hyper for this diversity test. Results in Figure 6 suggest that performance could saturate after training with more than 500 classes. We conjecture that training with many classes with few instances could introduce sample noise. So to further improve performance, more high quality data is required."}, {"heading": "5. Interpreting the embedding space", "text": "The joint embedding space we trained earlier features different properties from known spaces like Word2Vec. In this section, we conduct three tests to explore them. Concept search. In our framework, the joint training does not require all the concepts to have corresponding image data, the semantics can be propagated. This enables us to train with all the WordNet synsets and search with concepts that are not trained with images. In Figure 7, we show some pixel-level concept search results. The heatmaps are the\nQuery Image Score Map\nmax(\u201cgame equipment\u201d, \u201ctable\u201d)\nmin(\u201cbicycle\u201d, \u201ccanopy\u201d)\n\u201ctable\u201d\n\u201cbicycle\u201d\nTest Image\nTest Image\nscores in corresponding embedding spaces. As the search concepts become increasingly abstract, our model far outperforms Word2Vec+, showing the effective encoding of hierarchical information in our embedding space. Implicit attributes encoding. One intriguing property of feature embeddings is that it is a continuous space, and classification boundaries are flexible. So we explore the vicinity of some concepts. In Figure 8, we show score maps when searching for the concept \u201cchair\u201d. Interestingly, it is a common phenomenon that objects like \u201cbench\u201d and \u201cottoman\u201d, which are not hyponyms of \u201cchair\u201d in WordNet, get reasonable response. We conjecture that the embedding space implicitly encodes some abstract attributes by clustering them, e.g. sittable is an affordance attribute. So by simply loosing classification threshold of \u201cchair\u201d, one can detect regions where one can sit on. Synthesized concepts with arithmetics. Similar to Word2Vec, in our joint embedding space, new concepts or image detectors can be synthesized with arithmetics. As shown in Figure 9, we take elementwise min and max operations on the word concepts, and then search for these synthesized concepts in the images. It can be found that max operation takes the intersection of the concepts, e.g. the pool table is the common hyponym of \u201ctable\u201d and \u201cgame equipment\u201d; and min takes the union, e.g. the cart is composed of attributes of \u201cbicycle\u201d and \u201ccanopy\u201d."}, {"heading": "6. Conclusion", "text": "We introduced a new challenging task: open vocabulary scene parsing, which aims at parsing images in the wild. And we proposed a framework to solve it by embedding concepts from a knowledge graph and image pixel features into a joint vector space, where the semantic hierarchy is preserved. We showed our model performs well on open vocabulary parsing, and further explored the semantics learned in the embedding space.\nAcknowledgement: This work was supported by Samsung and NSF grant No.1524817 to AT. SF acknowledges the support from NSERC. BZ is supported by Facebook Fellowship."}, {"heading": "1. Data association protocol", "text": "To learn the joint embeddings of images and word concepts, we need to augment ADE20K dataset by adding information about how the label classes (> 3000) are semantically related. We associate each class in ADE20K dataset with a synset in WordNet, representing a unique concept. The data association process requires semantic understanding, so we resort to Amazon Mechanical Turk (AMT). The annotation protocol is detailed as follows, and screen shots of our AMT interface are shown in Figure 10.\nWe search for each class in the dataset, for all the synsets having the same name. We find 3 different cases: (1) a single synset is found for the given class; (2) multiple synsets are found due to polysemy; (3) no sysnets are found, either because the correct synset has a different name or because that concept is not in WordNet.\nIn the first case, we automatically match classes in the dataset with the obtained synsets, and then ask workers on AMT to verify by looking at the image labels and the definitions of synsets in the WordNet.\nIn the second case where multiple synsets were found, we show an image displaying such concept and ask workers to select the synset whose definition matches the given class.\nIn the last case where no synset candidate was found, we show an image with the concept and ask workers to find the best matching synset by looking over WordNet online API. They also have the option to indicate when no synset can match."}, {"heading": "2. Concept graph", "text": "After data association, we end up getting 3019 classes in the dataset having synset matches. Out of these there are 2019 unique synsets forming a DAG. All the matched synsets have entity.n.01 as the top hypernym and there are in average 8.2 synsets in between. The depths of the ADE20K dataset annotations range from 4 to 19.\nA detailed visualization of the concept graph built is shown in Figure 11. The node radii indicate the class frequencies in the ADE20K dataset. The figure only shows part of the full graph, nodes with 5 descendents or less have been hidden."}, {"heading": "3. Full zero-shot prediction lists", "text": "Our model gives each sample a list of predictions in hierarchical order. Due to the page limitations, full prediction lists are not shown in the main paper. In Figure 12, we give details of zero-shot predictions, both ground truth and prediction lists are shown in the texts beneath the images. Correct predictions are marked in green, inconsistent items are marked in orange. It can be seen that for hard examples, e.g. \u201cdome\u201d (row1, column3), a general and conservative prediction is made; when the test sample is easy and similar to training samples, e.g. \u201cwagon\u201d (row1, column1), our model gives specific and aggressive predictions."}], "references": [{"title": "Labelembedding for attribute-based classification", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv:1606.00915,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Scharw\u00e4chter", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR Workshop on The Future of Datasets in Vision,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale object classification using label relation graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "European Conference on Computer Vision, pages 48\u201364. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition", "author": ["J. Deng", "J. Krause", "A.C. Berg", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3450\u20133457. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "Int\u2019l Journal of Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Large-scale knowledge transfer for object localization in imagenet", "author": ["M. Guillaumin", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3202\u20133209. IEEE,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep metric learning using triplet network", "author": ["E. Hoffer", "N. Ailon"], "venue": "International Workshop on Similarity-Based Pattern Recognition, pages 84\u201392. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u2013 3137,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(3):453\u2013465,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u201341,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "From large scale image categorization to entry-level categories", "author": ["V. Ordonez", "J. Deng", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2768\u20132775,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pages 503\u2013 510. IEEE,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Yolo9000: Better, faster, stronger", "author": ["J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1612.08242,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1641\u20131648. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "What helps where\u2013and why? semantic relatedness for knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 910\u2013917. IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "Advances in neural information processing systems, pages 935\u2013943,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Order-embeddings of images and language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133\u2013138. Association for Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1994}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485\u20133492. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems, pages 487\u2013495,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic understanding of scenes through the ade20k dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "arXiv preprint arXiv:1608.05442,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting object hierarchy: Combining models from different category levels", "author": ["A. Zweig", "D. Weinshall"], "venue": "2007 IEEE 11th International Conference on Computer Vision, pages 1\u20138. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 30, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 32, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 145, "endOffset": 156}, {"referenceID": 11, "context": "Recent efforts in image classification/detection/segmentation have shown this trend: emerging image datasets enable recognition on a large-scale [6, 31, 33], while image captioning can be seen as a special instance of this task [12].", "startOffset": 228, "endOffset": 232}, {"referenceID": 18, "context": "Our framework incorporates hypernym/hyponym relations from WordNet [19] to help parsing.", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "The open vocabulary parsing ability of the proposed framework is evaluated on the recent ADE20K dataset [34].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Due to astonishing performance of deep learning, in particular CNNs [14], pixel-wise dense labeling has received significant amount of attention.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 157, "endOffset": 164}, {"referenceID": 31, "context": "Existing work include fully convolutional neural network (FCN) [17], deconvolutional neural network [20], encoder-decoder SegNet [2], dilated neural network [3, 32], etc.", "startOffset": 157, "endOffset": 164}, {"referenceID": 7, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 98, "endOffset": 101}, {"referenceID": 33, "context": "These networks perform well on datasets like PASCAL VOC [8] with 20 object categories, Cityscapes [4] with 30 classes, and a recently released benchmark SceneParse150 [34] covering 150 most frequent daily objects.", "startOffset": 167, "endOffset": 171}, {"referenceID": 24, "context": "Zero-shot learning addresses knowledge transfer and generalization [25, 10].", "startOffset": 67, "endOffset": 75}, {"referenceID": 9, "context": "Zero-shot learning addresses knowledge transfer and generalization [25, 10].", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "Rohrbach [26] introduced the idea to transfer large-scale linguistic knowledge into vision tasks.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "[28] and Frome et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] directly embedded visual features into the word vector space so that visual similarities are connected to semantic similarities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[21] used a convex combination of visual features of training classes to represent new categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 0, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 14, "context": "Attribute-based methods are another major direction in zero-shot learning that maps object attribute labels or language descriptions to visual classifiers [23, 1, 16, 15].", "startOffset": 155, "endOffset": 170}, {"referenceID": 34, "context": "[35] combined classifiers on different levels to help improve classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] achieved hierarchical imagelevel classification by trading off accuracy and gain as an optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22], on the other hand, proposed to make entry-level predictions when dealEntity Physical entity Surface", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] formulated a label relation graph that could be directly integrated with deep neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Our approach on hierarchical parsing is inspired by the order-embeddings work [29], we attempt to construct an asymmetric embedding space, so that both image features and hierarchical information in the knowledge graph are effectively and implicitly encoded by the deep neural networks.", "startOffset": 78, "endOffset": 82}, {"referenceID": 28, "context": "We treat open-ended scene parsing as a retrieval problem for each pixel, following the ideas of image-caption retrieval work [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "In order to reveal the asymmetric hypernym/hyponym relations between word concepts, hypernym scoring function [29] is indispensable,", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "A max-margin ranking loss is commonly used [9] to encourage the scores of true labels be larger than negative labels by a margin,", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "(4) This loss function is a variation of triplet ranking loss proposed in [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "Hierarchical precision, recall and F-score were also known as Wu-Palmer similarity, which was originally used for lexical selection [30].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 31, "context": "Our core CNN in the image stream is adapted from VGG-16 by taking away pool4 and pool5 and then making all the following convolution layers dilated (or Atrous) [3, 32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 33, "context": "Softmax [34] 73.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "6014 Conditional Softmax [24] 72.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "5942 Word2Vec [9] 71.", "startOffset": 14, "endOffset": 17}, {"referenceID": 26, "context": "VGG-ImageNet [27] are used as initializations for the image stream.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "Adam optimizer [13] with learning rate 1e-3 is used to update weights across the model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "Conditional Softmax is a hierarchical classification model proposed in [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "This model is a direct counterpart of DeViSe[9] in our scene parsing settings.", "startOffset": 44, "endOffset": 47}, {"referenceID": 17, "context": "Then we finetune the pre-trained word vectors with skip-gram model [18] for 5 epochs, and these word vectors are finally fixed for regression like Word2Vec.", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "Without surprise, the best performance is achieved by the Softmax baseline, which agrees with the observation from [9], classification formulations usually achieves higher accuracy than regression formulations.", "startOffset": 115, "endOffset": 118}, {"referenceID": 20, "context": "Convex Combination [21] is another baseline model for comparison: we take the probability output from Softmax within the 150 classes, to form new embeddings in the word", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "Softmax [34] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "1632 Conditional Softmax [24] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "1657 Word2Vec [9] 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "1794 Convex Combination [21] 0.", "startOffset": 24, "endOffset": 28}], "year": 2017, "abstractText": "Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.", "creator": "LaTeX with hyperref package"}}}