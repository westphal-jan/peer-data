{"id": "1703.08774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Who Said What: Modeling Individual Labelers Improves Classification", "abstract": "Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution of the data and to be applied to the other expert on the same issue.\n\n\n\n\n\nThe above methods can lead to incorrect conclusions. To avoid erroneous assumptions, consider examining the data in the previous article.\n\nHere are three definitions of the data for both expert and not expert:\n1) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n2) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n3) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n4) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n5) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n6) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n7) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n8) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n9) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n10) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n11) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n12) the expert opinion, by default, is presented with two sources of data and a non-standard interpretation that is based on the same assumptions:\n13) the", "histories": [["v1", "Sun, 26 Mar 2017 06:34:45 GMT  (3896kb,D)", "http://arxiv.org/abs/1703.08774v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["melody y guan", "varun gulshan", "rew m dai", "geoffrey e hinton"], "accepted": false, "id": "1703.08774"}, "pdf": {"name": "1703.08774.pdf", "metadata": {"source": "META", "title": "Who Said What: Modeling Individual Labelers Improves Classification", "authors": ["Melody Y. Guan", "Varun Gulshan", "Andrew M. Dai", "Geo\u0082rey E. Hinton"], "emails": ["melodyguan@google.com", "varungulshan@google.com", "adai@google.com", "geointon@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Over the last few years, deep convolutional neural networks have led to rapid improvements in the ability of computers to classify objects in images and they are now comparable with human performance in several domains. As computers get faster and researchers develop even be er techniques, neural networks will continue to improve, especially for tasks where it is possible to get a very large number of accurately labeled training examples. In the near future, we can expect neural networks to start serving as alternatives to human experts. We would, in fact, like the neural networks to perform much be er than the human experts used to provide the training labels because these training labels are o en unreliable as indicated by the poor agreement between di erent experts (55.4% for the datasets we consider) or even between an expert and the same expert looking at the same image some time later (70.7%).1 Intuitively, we would expect the quality of the training labels to\n\u2217Work done as a member of the Google Brain Residency program (g.co/brainresidency). 1Inter-grader variability is a well-known issue in many se ings in which human interpretation is used as a proxy for ground truth, such as radiology [7] or pathology [6].\nprovide an upper bound on the performance of the trained net. In the rst part of the paper we show that this intuition is incorrect.\ne main contribution of the paper is to show that there are signi cantly be er ways to use the opinions of multiple experts than simply treating the consensus of the experts as the correct label or using the experts to de ne a probability distribution over labels."}, {"heading": "2 MOTIVATION", "text": ""}, {"heading": "2.1 Beating the teacher", "text": "To demonstrate that a trained neural net can perform far be er than its teacher we use the well-known MNIST benchmark for which the true labels are known and we create unreliable training labels by corrupting the true labels. is corruption is performed just once per experiment, before training starts, so the noise introduced by the corruption cannot be averaged away by training on the same example several times. MNIST has 60,000 training images and 10,000 test images of isolated, normalized, hand-wri en digits and the task is to classify the image into one of ten classes. Each image has 28\u00d728 pixels. For the purposes of this demonstration, we used a very simple neural net containing two hidden convolutional layers each with 1,024 recti ed linear units and 64 patches followed by a fully connected hidden layer of 32 recti ed linear units followed by a 10-way so max layer. We trained the net on 50,000 examples using stochastic gradient descent on mini-batches of size 200 with the Adam optimizer [14] and we used the remaining 10,000 training images as a validation set to select good values for the learning rate and the magnitude of the initial random weights. On the test data, the net that performed best on the validation set had an test error rate of 1.01% when the training labels were all correct.2 If the labels are corrupted by changing each label to one of the other nine classes with a probability of 0.5, the test error rate only rises to 2.29%. Even if each training label is changed to an incorrect label with probability 0.8 so that the teacher is wrong 80% of the time, the trained net only gets 8.23% test error. If the teacher is even less reliable there comes a point at which the neural net fails to \u201cget the point\u201d and its error rate rises catastrophically, but this does not happen until the teacher is extremely unreliable as is shown in Figure 1.\n2We did hyperparameter tuning for data where each training label was changed to an incorrect label with probability 0.8. If we had tuned for data where all labels were correct, the corresponding error rate would have been lower.\nar X\niv :1\n70 3.\n08 77\n4v 1\n[ cs\n.L G\n] 2\n6 M\nar 2\n01 7\nis demonstration shows that the performance of a neural net is not limited by the accuracy of its teacher, provided the teacher\u2019s errors are random. One obvious question is how many noisily labeled training examples are worth a correctly labeled training example. In Appendix B we show that this question can be answered, at least approximately, by computing the mutual information between the label and the truth."}, {"heading": "2.2 Making better use of noisy labels", "text": "We are interested in datasets of medical images where many different doctors have provided labels but each image has only been labeled by a few doctors and most of the doctors have only labeled a fairly small fraction of the images. We expect that some doctors will be more reliable than others and we would like to give more weight to their opinions. We also expect that the doctors will have received di erent training and may have experienced di erent distributions of images so that the relative reliability of two doctors may depend on both the class of the image and on properties of the image such as the type of camera it was taken with. In this paper we focus on datasets of images used for screening diabetic retinopathy because neural networks have recently achieved human-level performance on such images [10] and if we can produce even a relatively small improvement in the state-of-the-art system it will be of great value.\nere is more information in the particular labels produced by particular doctors than is captured by simply averaging the opinions of all the doctors who have labeled a particular image and treating this distribution as the correct answer. e amount of constraint that a training case imposes on the weights of a neural network depends on the amount of information required to specify the desired output. So if we force the network to predict what each particular doctor would say for each particular training case we should be able to get be er generalization to test data, provided this does not introduce too many extra parameters. For a K-way classi cation task, we can replace the single so max [9] that is\nnormally used by as many di erent K-way so maxes as we have doctors. Of course, there will be many doctors who have not labeled a particular training image, but this is easily handled by simply not backpropagating any error from the so maxes that are used to model those doctors. At test time we can compute the predictions of all of the modeled doctors and average them. Our belief is that forcing a neural network to model the individual doctors and then averaging at test time should give be er generalization than simply training a neural network to model the average of the doctors.\nWe should also be able to do be er than just averaging the opinions of the modeled doctors. A er we have nished learning how to model all of the individual doctors we can learn how much to weight each modeled doctor\u2019s opinion in the averaging. is allows us to downweight the unreliable doctor models."}, {"heading": "2.3 Diabetic retinopathy classi cation", "text": "Diabetic retinopathy (DR) is the fastest growing cause of blindness worldwide, with nearly 415 million diabetics at risk [8]. Early detection and treatment of DR can reduce the risk of blindness by 95% [12]. One of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye called fundus images and rate them on the International Clinical Diabetic Retinopathy scale [16], de ned based on the type and extent of lesions (e.g. microaneurysms, hemorrhages, hard exudates) present in the image. e image is classi ed into one of 5 categories consisting of (1) No DR, (2) Mild NPDR (non-proliferative DR), (3) Moderate NPDR, (4) Severe NPDR, and (5) Proliferative DR (Figure 2). Another important clinical diagnosis that can be made from the fundus image is the presence of diabetic macular edema (DME). While this work focuses only on the 5 point grading of DR, the ndings should be applicable to DME diagnosis as well.\nMost of the prior work on diabetic retinopathy classi cation focuses on obtaining a single ground truth diagnosis for each image, and then using that for training and evaluation. Deep learning has recently been used within this se ing by Gulshan et al. [10] who show a high sensitivity (97.5%) and speci city (93.4%) in the detection of referable DR (moderate or more severe DR).3\nIn this work we explore whether, in the context of data in which every example is labeled by multiple experts, a be er model can be trained by predicting the opinions of the individual experts as opposed to collapsing the many opinions into a single one. is allows us to keep the information contained in the assignment of experts to opinions, which should be valuable because experts labelling data di er from each other in skill and area of expertise (as is the case with our opthalmologists, see Figure 3). Note that we still need a single opinion on the test set to be able to evaluate the models. To that end, we use a rigorous adjudicated reference standard for evaluation, where a commi ee of three retinal specialists resolved disagreements by discussion until a single consensus is achieved.\n3In a recent Kaggle machine-learning competition [11] for DR, all the winning models also used deep learning. ere have been non-deep-learning a empts as well. On the Messidor-2 dataset [4], Abra\u0300mo et al. [2] report a sensitivity of 96.8% at a speci city of 59.4% for detecting referable DR while Solanki et al. [22] report a sensitivity of 93.8% at a speci city of 72.2%."}, {"heading": "3 METHODS", "text": ""}, {"heading": "3.1 Model Architecture", "text": "We considered a sequence of models of increasing complexity for training the diabetic retinopathy classi er (Figure 5). e neural network base used in this work is the Inception-v3 architecture proposed by Szegedy et al. [23].\n\u2022 Baseline Net (BN): Inception-v3 trained on average opinions of doctors; a TensorFlow reimplementation of the model used in Gulshan et al. [10] (see Section 4.3 for differences). \u2022 Doctor Net (DN): BN extended to model the opinions of each of the doctors (31 in total, see Section 4.2). \u2022 Weighted Doctor Net (WDN): Fixed DN with averaging weights for combining the predictions of the doctor models learned on top, one weight per doctor model. \u2022 Image-speci c WDN (IWDN): WDN with averaging weights that are learned as a function of the image. \u2022 Bo lenecked IWDN (BIWDN): IWDN with a small bo leneck layer for learning the averaging weights.\nFor BN, the outputs of the last hidden layer of Inception were used to compute the logits used in the 5-way so max output layer. For DN, the opinions of each doctor were modeled using a separate so max for each doctor, while Inception weights were shared. For evaluation, the predictions from the so max \u201cdoctor models\u201d were arithmetically averaged to give a single 5-class prediction. For subsequent nets, the parameters and predictions of the DN model were frozen and only the averaging weights for the doctor models were learned. For WDN, one averaging weight per doctor was trained, used across all images. For IWDN, these averaging weights were made image-dependent by le ing them be a function of the last hidden layer of Inception. For BIWDN, a linear bo leneck layer of size 3 was added between the last hidden layer of Inception (which has dimension 2048) and the 31-way so max of IWDN as a precautionary measure against model under ing; a bo leneck layer of this size reduced the number of trainable parameters about 10 times.\nRather than directly learning the averaging weight for each doctor model (B)(I)WDN, we learned averaging logits for each model that we could then pass through a so max to produce averaging weights that are guaranteed to be positive. To train the averaging logits, we used the opinions of the doctors who actually labeled a training image to de ne the target output distribution for that image (Appendix C.2 discusses an alternative target). We then combined the predictions of the models of all the other doctors using the weights de ned by their current averaging logits. Finally we updated our parameters by backpropagating with the cross entropy loss between the target distribution and the weighted average prediction. is way all of the training cases that a doctor did not label can be used to learn the averaging logit for that doctor, and no extra data were needed beyond those used to learn the weights of DN. Moreover, if a doctor model has similar performance to other doctor models but makes very di erent errors it will tend to be upweighted because it will be more useful in the averaging. is upweighting of diverse doctor models would not occur if we had computed the reliabilities of the doctors separately.\nFor a single image, let I be the set of indices of the doctors who actually graded that image. Let the label of doctor i \u2208 I be li . For every doctor j \u2208 {1, 2, . . ., 31}, denote the prediction of its model pj . Let p\u2205 be the prediction of the model of the average doctor in BN. For WDN, IWDN, and BIWDN, let w j be the averaging weight for the jth modeled doctor, where \u2211 j w j = 1. Note that pj is a 5-dimensional vector and w j is a scalar. e explicit inputs of the cross entropy loss being minimized during training of each model are shown in Table 1 and post-Inception computations are shown schematically in Figure 5. In the case of DN, the cross entropy losses of the individual doctor models were summed to get the total loss for each training example."}, {"heading": "3.2 Estimating doctor reliability with EM", "text": "Since the foundational work of Dawid and Skene [3], who model annotator accuracies with expectation-maximization (EM), and Smyth et al. [21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25]. Representatively, Welinder and Perona [24] use an online EM algorithm to estimate abilities of multiple noisy annotators and to determine the most likely value of the labels. We calculated updated labels by executing Welinder and Perona [24]\u2019s method on our human doctors and used these updated labels to train BN, as a competing algorithm for our DN method. Welinder and Perona [24] also actively select which images to label and how many labels to request based on the uncertainty of their estimated ground truth values and the desired level of con dence, and they select and prioritize which annotators to use when requesting labels. We do not use these other aspects of their algorithm because labels for all images in our dataset have already been collected."}, {"heading": "3.3 Modeling label noise", "text": "Mnih and Hinton [15] describe a deep neural network that learns to label road pixels in aerial images. e target labels are derived from road maps that represent roads using vectors. ese vectors are converted to road pixels by using knowledge of the approximate width of the roads so the target labels are unreliable. To handle this label noise, Mnih and Hinton [15] propose a robust loss function that models asymmetric omission noise.\ney assume that a true, unobserved label m is rst generated from a wm \u00d7 wm image patch s according to some distribution p(m|s), and the corrupted, observed label m\u0303 is then generated from m according to a noise distribution p(m\u0303|m). e authors assume an asymmetric binary noise distribution p(m\u0303i |mi ) that is the same for all pixels i . ey assume that conditioned on m, all components of m\u0303 are independent and that each m\u0303i is independent of allmj,i . e observed label distribution is then modeled as:\np(m\u0303|s) = w2m\u220f i=1 \u2211 mi p(m\u0303i |mi )p(mi |s)\nWe used a multi-class extension of their method on DN, modeling the noise distribution prior for all doctors d with the parameters:\n\u03b8l l \u2032 = p(m\u0303d = l \u2032 |md = l) where l , l \u2032 \u2208 {1, 2, 3, 4, 5}. We estimated \u03b8l l \u2032 using the 5\u00d75 confusion matrix between individual and average doctor opinions on training images. Treating the average doctor opinion as the true label, we converted each doctor\u2019s individual count matrix into proportions and then averaged these proportions across all doctors. We trained this model by minimizing the negative log posterior, \u2212lo\u0434(p(m\u0303|s)). Our variant of Mnih and Hinton [15] is an alternative way to improve upon DN to our proposed approach of learning averaging weights."}, {"heading": "4 EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1 Neural network training", "text": "e optimization algorithm used to train the network weights was distributed stochastic gradient descent (SGD) [1] with the Adam optimizer on mini-batches of size 8. We trained using TensorFlow with 32 replicas and 17 parameter servers, with 1 Tesla K80 GPU per replica. To speed up the training, we used batch normalization [13], pre-initialization of our Inception network using weights from the network trained to classify objects in the ImageNet dataset [20], and the following trick: we set the learning rate on the weight matrix producing prediction logits to one-tenth of the learning rate for\nthe other weights. We prevented over ing using a combination of L1 and L2 penalties, dropout, and a con dence penalty [17], which penalizes a model for having an output distribution with low entropy. At the end of training, we used an exponentially decaying average of the recent parameters in the nal model.\nWe tuned hyperparameters and picked model checkpoints for early stopping on the validation dataset, using 5-class classi cation error rate as the evaluation metric. e optimal values for these hyperparameters are displayed in Table 2 and the hyperparameter search spaces are listed in Appendix D. When evaluating on the test set we averaged the predictions for the horizontally and vertically ipped versions (4 in total) of every image.\nWe also trained a version of BN where the output prediction is binary instead of multi-class, as was done in Gulshan et al. [10]. e binary output was obtained by thresholding the 5-class output at the Moderate NPDR or above level, a commonly used threshold in clinics to de ne a referable eye condition. For this BN-binary network, the area under the ROC curve was used as the evaluation metric on the validation set.\nTo deal with di erences in class distribution between the datasets (Table 3), we used log prior correction during evaluation. is entails adding to the prediction logits, for each class, the log of the ratio of the proportion of labels in that class in the evaluation dataset to the proportion of labels in that class in the training set. Our assumed test class distribution for computing the log prior correction was the mean distribution of all known images (those of the training and validation sets).4 So for each image under evaluation we update the prediction logit for class c by adding:\nlo\u0434 ( qvalid (c) qtrain (c) ) for the validation dataset, and\nlo\u0434\n( qvalid\u222atrain (c)\nqtrain (c)\n) for the test dataset,\nwhere q(c) is the proportion of labels in that class. We saw improvement from the application of log prior correction and all our reported results use it. See Appendix C.1 for another way we attempted to correct for di erences in class distribution."}, {"heading": "4.2 Datasets", "text": "e training dataset consists of 126, 522 images sourced from patients presenting for diabetic retinopathy screening at sites managed by 4 di erent clinical partners: EyePACS, Aravind Eye Care, Sankara Nethralaya, and Narayana Nethralaya. e validation dataset consists of 7,804 images obtained from EyePACS clinics. Our test dataset consists of 3,547 images from the EyePACS-1 and Messidor-2 datasets. More details on image sourcing can be found in Appendix E.\nEach of the images in the training and validation datasets was graded by at least one of 54 US-licensed ophthalmologist or ophthalmology trainee in their last year of residency (postgraduate year 4). For training the doctor models, we used the 30 ophthalmologists who graded at least 1,000 images, and we lumped the remaining doctors as a single composite doctor to avoid introducing doctor-speci c parameters that are constrained by fewer than 1,000\n4We did not follow the standard machine learning paradigm of assuming that the test dataset has the same distribution as the validation dataset because our test data are sourced di erently from the validation data.\ntraining cases. Meanwhile, the labels for the test set were obtained through an adjudication process: three retina specialists graded all images in the test dataset, and discussed any disagreements as a commi ee until a consensus label was obtained.\nWe scale normalized our images by detecting the circular fundus disk and removing the black borders around them. We used images at a resolution of 587\u00d7587 pixels and we augmented our training data with random perturbations to image brightness, saturation, hue, and contrast."}, {"heading": "4.3 Our baseline vs published baseline", "text": "is section describes multiple ways in which our baseline di ers from that of Gulshan et al. [10]. For these reasons, results from this paper\u2019s own BN should be used for model comparisons with DN, WN, IWDN, and BIWDN rather than numbers from Gulshan et al. [10].\n\u2022 Unlike in Gulshan et al. [10], we remove grades of doctors who graded test set images from training and validation sets to reduce the chance that the model is over ing on certain experts. is removal handicaps our performance vis-a\u0300-vis their paper, especially because we exclude the most expert doctors (the retinal specialists) during model development, but ensures generalizability of our results. \u2022 We use di erent datasets, and in particular our adjudicated test set has gold standard labels that are meant to represent the ground truth. \u2022 We train with 5-class loss instead of binary loss (see sections 4.1 and 5.1). \u2022 If a doctor grades a single image multiple times, as o en occurs, Gulshan et al. [10] treats these as independent diagnoses while we collapse these multiple diagnoses into a single diagnosis which may be a distribution over classes. \u2022 We employ higher resolution images (587\u00d7587 pixels versus 299\u00d7299) and image preprocessing and theoretical techniques unused in Gulshan et al. [10] (section 4.2).\nMore di erences are discussed in Appendix F."}, {"heading": "5 SUMMARY OF RESULTS", "text": "We ran 10 replicates of each model and averaged the resulting metrics, which are reported in Table 5. For full comparability of models we used the same 10 replicates reported for DN to serve as the xed part of the model for training the WDN, IWDN, and BIWDN replicates."}, {"heading": "5.1 Training with ve-class loss beats training with binary loss even on binary metrics", "text": "We found that training BN with a 5-class loss improves test binary AUC compared to training with a binary loss, as did Gulshan et al. [10], even when validating the former on 5-class training error instead of binary AUC (Table 4). Test binary AUC was raised by 1.53% from 95.58% from using the multi-class loss. Intuitively this ts with our thesis that generalization is improved by increasing the amount of information in the desired outputs. All results reported in Table 5 and subsequent sections were obtained from training with 5-class loss."}, {"heading": "2 24.75 17.62", "text": ""}, {"heading": "1 51.03 72.69", "text": ""}, {"heading": "5.2 Averaging modeled doctors beats modeling the average doctor", "text": "We saw a reduction in 5-class classi cation test error of 1.97% from 23.83% (8.27% relative decrease) due to averaging modeled doctors (DN) instead of modeling the averaged doctor (BN). In comparison, using labels calculated with EM to train BN only reduced 5-class test classi cation error by 0.09% (0.38% relative decrease). Over BN, DN also increased binary AUC by 0.17% from 97.11%, decreased binary classi cation error by 0.17% from 9.92%, and increased speci city at 97% sensitivity (spec@97%sens) by 2.21% from 79.60%. Meanwhile, using labels calculated with EM on BN merely increased spec@97%sens by 0.37% compared to vanilla BN and actually led to slightly worse performance on binary AUC (-0.11%) and binary error (+0.20%). Note that the binary AUC, binary error, and spec@97%sens metrics could have been improved for all models had we done hyperparameter tuning and early stopping for them speci cally, but we decided to do all our model selection on one metric (5-class error) both for simplicity and to simulate the metric decision required in real-life automated diagnosis systems. We see that DN was signi cantly be er on all test metrics compared to BN trained using the labels obtained with EM."}, {"heading": "5.3 Learning averaging weights helps", "text": "We saw a further 1.28% decrease in 5-class test error relative to BN from using WDN as opposed to DN (5.37% additional relative decrease). Binary AUC increased an additional 0.17%, binary classi - cation error decreased another 0.68%, and spec@97%sens increased an extra 0.88%, all on test data. Results from IWDN and BIWDN were slightly worse than those from WDN. We would expect a bigger improvement from WDN and potentially further improvements from training averaging logits in an image-speci c way if we had doctors with more varied abilities and greater environmental di erences, but for the dataset we used image-speci c averaging logits did not help. Our extension of Mnih and Hinton [15]\u2019s competing algorithm actually caused DN to perform worse by 0.90% on 5-class test error (3.78% less relative reduction), and was also more computationally costly than (B)(I)WDN. A di erent noise model we considered did not help either (Appendix C.3)."}, {"heading": "6 CONCLUSIONS", "text": "We introduce a method to make more e ective use of noisy labels when every example is labeled by a subset of a larger pool of experts. Our method learns from the identity of multiple noisy annotators by modeling them individually with a shared neural net that has separate sets of outputs for each expert, and then learning averaging weights for combining their modeled predictions. We evaluate our method on the diagnosis of diabetic retinopathy severity on the 5-point scale from images of the retina. Compared to our baseline model of training on the average doctor opinion, a strategy that yielded state-of-the-art results on automated diagnosis of DR, our method can lower 5-class classi cation test error from 23.83% to 20.58%, a relative reduction of 13.6%. We also found that, on binary metrics, training with a 5-class loss signi cantly beats training with a binary loss, as was done in the published baseline. We compared our method to competing algorithms by Welinder and Perona and by Mnih and Hinton and we showed that corresponding parts of our method give superior performance to both. Our methodology is generally applicable to supervised training systems using datasets with labels from multiple annotators."}, {"heading": "A CODE", "text": "e TensorFlow code used in this paper will soon be made publicly available."}, {"heading": "B MUTUAL INFORMATION FOR NOISY LABELS", "text": "Here we compute the mutual information between a noisy MNIST label and the truth, assuming random noise, in order to estimate the number of noisily labeled training cases equivalent to one case that is known to be correctly labeled.\nEmpirically, N perfectly labeled training cases give about the same test error as NIperfect/Inoisy training cases with noisy labels, where Inoisy is the mutual information per case between a noisy label and the truth and Iperfect is the corresponding mutual information for perfect labels. For ten classes, the mutual information (in nats) is Iperfect = 2.3 = \u2212lo\u0434(0.1), but when the noisy label is\n20% correct on average, the mutual information is: Inoisy = 0.044 = \u2212 lo\u0434(0.1) \u2212 10 \u00d7 0.02 \u00d7 lo\u0434 (\n0.1 0.02 ) \u2212 90 \u00d7 0.1 \u00d7 0.89 lo\u0434 ( 0.1 0.1 \u00d7 0.8/9 ) .\nSo if the learning is making good use of the mutual information in the noisy labels we can predict that 60,000 noisy labels are worth 60, 000\u00d7 0.044/2.3 \u2248 1, 148 clean labels. In reality we needed about 1,000 clean labels to get similar results."}, {"heading": "C OTHER IDEAS TESTED C.1 Mean Class Balancing", "text": "In addition to log prior correction of class distributions, we also a empted mean class balancing wherein examples from less frequent classes are upweighted and more frequent classes are downweighted in the cross entropy loss, in inverse proportion to their prevalence relative to the uniform distribution across classes. Explicitly, we weight each example of class c by:\n\u03b1c = q\u0304 q(c) = 1 |c |q(c) ,\nEigen and Fergus. [5] employ a similar method for computer vision tasks although they use medians instead of means. In our case, using mean class balancing lowered performance, possibly because it made too many assumptions on the hidden test distribution, and was not employed.\nC.2 Alternative target distribution for training averaging logits\nTo train the averaging logits, we took each training case and use the opinions of the doctors who actually labeled that case to de ne the target output distribution. Alternatively, the target distribution can be de ned as the equally weighted average of the predictions of the doctor models corresponding to the doctors who labeled that case. In the notation used in Table 1, this would be 1|I | \u2211 i \u2208I pi . We experimented with using this alternative target distribution in calculating cross entropy loss but saw inferior results.\nC.3 A alternative noise model Because the multi-class extension of Mnih and Hinton [15] we tried showed poor results, which we postulated may have been because it was sensitive to di erences in class distributions between datasets, we considered a di erent noise model that made less assumptions on the class distribution of the data. We assumed a symmetric noise distribution that is determined by a single prior parameter. is assumes that if a label is wrong, it has equal probability of belonging to any of the other classes. However we allowed this parameter to vary by doctor. For each doctor d we estimated this parameter: \u03b8d = p(m\u0303d = l |md = l) with the real doctor reliability score calculated from the Welinder and Perona [24] algorithm. is method performed slightly worse than the 5-class variant of Mnih and Hinton [15]. Note that a number of other noise models of varying complexity can be considered as well."}, {"heading": "D HYPERPARAMETER TUNING", "text": "For the MNIST experiment we used default Adam optimizer hyperparameters \u03b21 = 0.9, \u03b22 = 0.999, and \u03f5 = 1\u00d710\u22128. We did a grid search on learning rates in the set {0.000003, 0.00001, 0.00003, . . ., 0.003} and standard deviations of the initial random normal weights in the set {0.0001, 0.0003, 0.001, . . ., 0.01} and found optimal values of 0.00003 for the former and 0.001 for the la er.\nFor computer-aided diagnosis of DR we did a grid search on the following hyperparameter spaces: dropout for Inception backbone \u2208 {0.5, 0.55, 0.6, . . ., 1.0}, dropout for doctor models \u2208 {0.5, 0.55, 0.6, . . ., 1.0}, learning rate \u2208 {1\u00d710\u22127, 3\u00d710\u22127, 1\u00d710\u22126, . . ., 0.03}, entropy weight \u2208 {0.0, 0.0025, 0.005, . . ., 0.03} \u222a {0.1}, weight decay for Inception \u2208 {0.000004, 0.00001, 0.00004, . . ., 0.1}, L1 weight decay for doctor models \u2208 {0.000004, 0.00001, 0.00004, . . ., 0.04}, L2 weight decay for doctor models \u2208 {0.00001, 0.00004, . . ., 0.04}, L1 weight decay for averaging logits \u2208 {0.001, 0.01, 0.02, 0.03, . . ., 0.1, 0.2, 0.3, . . ., 1, 2, 3, . . ., 10, 100, 1000}, L2 weight decay for averaging logits \u2208 {0.001, 0.01, 0.1, 0.2, 0.3, . . .,1, 5, 10, 15, 20, 30, . . ., 150, 200, 300, 400, 500, 1000}, and bo leneck size (for BIWDN) \u2208 {2, 3, 4, 5, 6, 7}. We used a learning rate decay factor of 0.99 optimized for BN. e magnitudes of the image preprocessing perturbations were also tuned for BN."}, {"heading": "E DATASET DETAILS", "text": "119,589 of our training set images are the same as those used in the training set of Gulshan et al. [10] (which consists of 128,175 images). e images removed from the training dataset used by Gulshan et al. [10] are detailed here: (i) 4,204 out of the 128,175 were removed to create a separate validation dataset for experiments within the research group. (ii) 4,265 out of the 128,175 images were excluded since they were deemed ungradable by every ophthalmologist that graded them. Unlike Gulshan et al. [10], we do not predict image gradeability in this work and hence exclude those images. (iii) 117 out of the 128, 175 fail our image scale normalization preprocessing step and were also excluded. We also acquired 6,933 more labeled images since the creation of the training dataset in Gulshan et al. [10] and added them to this training set.\ne validation dataset consists of 7,963 images obtained from EyePACS clinics. ese images are a random subset of the 9,963 images of the EyePACS-1 test set used in Gulshan et al. [10]. e remaining 2,000 images were included as part of the test set in this work. In practice, only 7,805 of the 7,963 validation images have at least one label, since the remaining 158 images were of poor quality and considered ungradable by all ophthalmologists that labeled them.\ne test set consists of 1,748 images of the Messidor-2 dataset [4] and the remaining 2,000 out of the 9,963 images of the EyePACS-1 test dataset used in Gulshan et al. [10]. 1,803 of the 2,000 images from the EyePACS-1 test set, and 1,744 of the 1,748 images of the Messidor-2 were considered gradable a er adjudication and were assigned labels."}, {"heading": "F MORE DIFFERENCES FROM PUBLISHED BASELINE", "text": "Here we list distinctions between BN and the model in Gulshan et al. [10] that are not mentioned in Section 4.3. Gulshan et al. [10]\nde ned referable diabetic retinopathy as the presence of moderate and worse diabetic retinopathy or referable diabetic macular edema, while we ignore information on the la er. ey also only reported binary (referable/non-referable) classi cation metrics while we reported both binary and 5-class classi cation metrics. Finally, we did not ensemble replicates as Gulshan et al. [10] did because we focused on comparing di erent methods of using the labels rather than squeezing the last drop of performance from one method."}, {"heading": "ACKNOWLEDGMENTS", "text": "anks to Dale Webster, Lily Peng, Jonathan Krause, Arunachalam Narayanaswamy, oc Le, Alexey Kurakin, Anelia Angelova, Nathan Silberman, George Dahl, Brian Cheung, Anna Goldie, David Ha, Ma Ho man, Olga Wichrowska, Justin Gilmer, Denny Britz, Mohammad Norouzi, and Luke Metz for helpful discussions and feedback. is work was supported by the Google Brain Residency program, for whom we give particular thanks to Leslie Phillips, Samy Bengio, and Je Dean."}], "references": [{"title": "TensorFlow: A System for Large-Scale Machine Learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard", "M. Kudlur", "J. Levenberg", "R. Monga", "S. Moore", "D.G. Murray", "B. Steiner", "P. Tucker", "V. Vasudevan", "P. Warden", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Automated analysis of retinal images for detection of referable diabetic retinopathy", "author": ["M.D. Abr\u00e0mo", "J.C. Folk", "D.P. Han", "J.D. Walker", "D.F. Williams", "S.R. Russell", "P. Massin", "B. Cochener", "L. Tang P. Gain", "M. Lamard", "D.C. Moga", "G. \u008bellec", "M. Niemeijer"], "venue": "JAMA Ophthalmol. 131,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied Statistics 28,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "Feedback on a Publicly Distributed Image Database: \u008ce", "author": ["E. Decenci\u00e8re", "Z. Xiwei", "G. Cazuguel", "B. Lay", "B. Cochener", "C. Trone", "P. Gain", "J.-R. Ord\u00f3 nez Varela", "P. Massin", "A. Erginay", "B. Charton", "J.-C"], "venue": "Kleain", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "ICCV 11,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Onega, A .N", "author": ["J.G. Elmore", "G.M. Longton", "P.A. Carney", "T.B.M. Geller"], "venue": "JAMA 313,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Variability in radiologists\u2019 interpretations of mammograms", "author": ["J.G. Elmore", "C.K. Wells", "C.H. Lee", "D.H. Howard", "A.R. Feinstein"], "venue": "NEJM 331,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs", "author": ["V. Gulshan", "L. Peng", "M. Coram", "M.C. Stumpe", "A. Narayanaswamy D. Wu", "S. Venugopalan", "T K. Widner", "Madams", "J. Cuadros", "R. Kim", "R. Raman", "P.C. Nelson", "J.L. Mega", "D.R. Webster"], "venue": "JAMA 316,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi\u0089", "author": ["S. Io\u0082e", "C. Szegedy"], "venue": "ICML 37", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "ADAM: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "ICLR (July", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning to Label Aerial Images from Noisy Data", "author": ["V. Mnih", "G.E. Hinton"], "venue": "ICML (July", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Regularizing Neural Networks by Penalizing Con\u0080dent Output Distributions", "author": ["G. Pereyra", "G. Tucker", "L. Kaiser", "G.E. Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Supervised Learning from Multiple Experts: Whom to trust when everyone lies a bit", "author": ["V. Raykar", "S. Yu", "L. Zhao", "A. Jerebko", "C. Florin", "G. Valadez", "L. Bogoni", "L. Moy"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Eliminating spammers and ranking annotators for crowdsourced labeling", "author": ["V.C. Raykar", "S. Yu"], "venue": "tasks. JMLR", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Inferring ground truth from subjective labelling of Venus", "author": ["P. Smyth", "U. Fayyad", "M. Burl", "P. Perona", "P. Baldi"], "venue": "images. NIPS", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "EyeArt: Automated, High-throughput, Image Analysis for Diabetic Retinopathy Screening", "author": ["K. Solanki", "C. Ramachandra", "S. Bhat", "M. Bhaskaranand", "M.G. Ni\u008aala", "S.R. Sadda"], "venue": "Invest Ophthalmol Vis Sci 56,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Re-thinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Io\u0082e", "J. Shlens", "Z. Wojna"], "venue": "CVPR (June", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Online crowdsourcing: rating annotators and obtaining cost-e\u0082ective labels", "author": ["P. Welinder", "P. Perona"], "venue": "CVPR Workshop (June", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise", "author": ["J. Whitehill", "P. Ruvolo", "T. Wu", "J. Bergsma", "J. Movellan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "1Inter-grader variability is a well-known issue in many se\u008aings in which human interpretation is used as a proxy for ground truth, such as radiology [7] or pathology [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "1Inter-grader variability is a well-known issue in many se\u008aings in which human interpretation is used as a proxy for ground truth, such as radiology [7] or pathology [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 10, "context": "using stochastic gradient descent on mini-batches of size 200 with the Adam optimizer [14] and we used the remaining 10,000 training images as a validation set to select good values for the learning rate and the magnitude of the initial random weights.", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "In this paper we focus on datasets of images used for screening diabetic retinopathy because neural networks have recently achieved human-level performance on such images [10] and if we can produce even a relatively small improvement in the state-of-the-art system it will be of great value.", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "For a K-way classi\u0080cation task, we can replace the single so\u0089max [9] that is normally used by as many di\u0082erent K-way so\u0089maxes as we have doctors.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "[10] who show a high sensitivity (97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "On the Messidor-2 dataset [4], Abr\u00e0mo\u0082 et al.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "[2] report a sensitivity of 96.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[22] report a sensitivity of 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "\u008cese were calculated from Welinder and Perona [24]\u2019s expectation-maximization algorithm on our training data.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] (see Section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Since the foundational work of Dawid and Skene [3], who model annotator accuracies with expectation-maximization (EM), and Smyth et al.", "startOffset": 47, "endOffset": 50}, {"referenceID": 15, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 14, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 19, "context": "[21], who integrate the opinions of many experts to infer ground truth, there has a large body of work using EM approaches to estimate accurate labels for datasets annotated by multiple experts [18, 19, 25].", "startOffset": 194, "endOffset": 206}, {"referenceID": 18, "context": "Representatively, Welinder and Perona [24] use an online EM algorithm to estimate abilities of multiple noisy annotators and to determine the most likely value of the labels.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "We calculated updated labels by executing Welinder and Perona [24]\u2019s method on our human doctors and used these updated labels to train BN, as a competing algorithm for our DN method.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Welinder and Perona [24] also actively select which images to label and how many labels to request based on the uncertainty of their estimated ground truth values and the desired level of con\u0080dence, and they select and prioritize which annotators to use when requesting labels.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Mnih and Hinton [15] describe a deep neural network that learns to label road pixels in aerial images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "To handle this label noise, Mnih and Hinton [15] propose a robust loss function that models asymmetric omission noise.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Our variant of Mnih and Hinton [15] is an alternative way to improve upon DN to our proposed approach of learning averaging weights.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "\u008ce optimization algorithm used to train the network weights was distributed stochastic gradient descent (SGD) [1] with the Adam optimizer on mini-batches of size 8.", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "To speed up the training, we used batch normalization [13], pre-initialization of our Inception network using weights from the network trained to classify objects in the ImageNet dataset [20], and the following trick: we set the learning rate on the weight matrix producing prediction logits to one-tenth of the learning rate for", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "We prevented over\u0080\u008aing using a combination of L1 and L2 penalties, dropout, and a con\u0080dence penalty [17], which penalizes a model for having an output distribution with low entropy.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], we remove grades of doctors who graded test set images from training and validation sets to reduce the chance that the model is over\u0080\u008aing on certain experts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] treats these as independent diagnoses while we collapse these multiple diagnoses into a single diagnosis which may be a distribution over classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] (section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], even when validating the former on 5-class training error instead of binary AUC (Table 4).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Our extension of Mnih and Hinton [15]\u2019s competing algorithm actually caused DN to perform worse by 0.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "[5] employ a similar method for computer vision tasks although they use medians instead of means.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Because the multi-class extension of Mnih and Hinton [15] we tried showed poor results, which we postulated may have been because it was sensitive to di\u0082erences in class distributions between datasets, we considered a di\u0082erent noise model that made less assumptions on the class distribution of the data.", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "with the real doctor reliability score calculated from the Welinder and Perona [24] algorithm.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "\u008cis method performed slightly worse than the 5-class variant of Mnih and Hinton [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "[10] (which consists of 128,175 images).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] are detailed here: (i) 4,204 out of the 128,175 were removed to create a separate validation dataset for experiments within the research group.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10], we do not predict image gradeability in this work and hence exclude those images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] and added them to this training set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u008ce test set consists of 1,748 images of the Messidor-2 dataset [4] and the remaining 2,000 out of the 9,963 images of the EyePACS-1 test dataset used in Gulshan et al.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] that are not mentioned in Section 4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] did because we focused on comparing di\u0082erent methods of using the labels rather than squeezing the last drop of performance from one method.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Data are o\u0089en labeled by many di\u0082erent experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. \u008cis reduces the workload on individual experts and also gives a be\u008aer estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. \u008cese approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-speci\u0080c ways. \u008cis allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs be\u008aer than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work o\u0082ers an innovative approach for dealing with the myriad real-world se\u008aings that use expert opinions to de\u0080ne labels for training.", "creator": "LaTeX with hyperref package"}}}