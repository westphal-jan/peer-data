{"id": "1703.08885", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Question Answering from Unstructured Text by Retrieval and Comprehension", "abstract": "Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. If you are interested in reading a document and need to find a book on the subject (e.g., Answering) then please do so through a QA.", "histories": [["v1", "Sun, 26 Mar 2017 23:48:06 GMT  (249kb,D)", "http://arxiv.org/abs/1703.08885v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yusuke watanabe", "bhuwan dhingra", "ruslan salakhutdinov"], "accepted": false, "id": "1703.08885"}, "pdf": {"name": "1703.08885.pdf", "metadata": {"source": "CRF", "title": "Question Answering from Unstructured Text by Retrieval and Comprehension", "authors": ["Yusuke Watanabe", "Bhuwan Dhingra", "Ruslan Salakhutdinov"], "emails": ["rsalakhu}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Natural language based consumer products, such as Apple Siri and Amazon Alexa, have found wide spread use in the last few years. A key requirement for these conversational systems is the ability to answer factual questions from the users, such as those about movies, music, and artists.\nMost of the current approaches for Question Answering (QA) are based on structured Knowledge Bases (KB) such as Freebase (Bollacker et al., 2008) and Wikidata (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014). In this setting the question is converted to a logical form using semantic parsing, which is queried against the KB to obtain the answer (Fader et al., 2014; Berant et al., 2013).\nHowever, recent studies have shown that even large curated KBs, such as Freebase, are incomplete (West et al., 2014). Further, KBs support only certain types of answer schemas, and constructing and maintaining them is expensive.\nOn the other hand, there is a vast amount of unstructured knowledge available in textual form from web pages such as Wikipedia, and hence an alternative is to directly answer questions from these documents. In this approach, shown in Figure 1, articles relevant to the question are first selected (retrieval step). Then, the retrieved articles and question are jointly processed to extract the answer (comprehension step). This retrieval based approach has a longer history than the KB based approach (Voorhees and Tice, 2000). It can potentially provide a much wider coverage over questions, and is not limited to specific answer schemas. However, there are still gaps in its performance compared to the KB-based approach (Miller et al., 2016). The comprehension step, which requires parsing information from natural language, is the main bottleneck, though suboptimal retrieval can also lead to lower performance.\nSeveral large-scale datasets introduced recently (Rajpurkar et al., 2016; Hermann et al., 2015) have\nar X\niv :1\n70 3.\n08 88\n5v 1\n[ cs\n.C L\n] 2\n6 M\nar 2\n01 7\nA Funny Man is a 2011 Danish drama film directed by Martin Zandvliet about the Danish actor and comedian Dirch Passer. Q. Martin Zandvliet directed which movies? A. A Funny Man\nfacilitated the development of powerful neural models for reading comprehension. These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al., 2016; Miller et al., 2016) (Figure 2 bottom). Here we argue that depending on the type of question, either (1) or (2) may be more appropriate, and introduce a latent variable mixture model to combine the two in a single end-to-end framework.\nWe incorporate the above mixture model in a simple Recurrent Neural Network (RNN) architecture with an attention mechanism (Bahdanau et al., 2015) for comprehension. In the second part of the paper we focus on the retrieval step for the QA system, and introduce a neural network based ranking model to select the articles to feed the comprehension model. We evaluate our model on WIKIMOVIES dataset, which consists of 200K questions about movies, along with 18K Wikipedia articles for extracting the answers. Miller et al. (2016) applied Key-Value Memory Neural Networks (KV-MemNN) to the dataset, achieving 76.2% accuracy. Adding the mixture model for answer selection improves the performance to 85.4%. Further, the ranking model improves both precision and recall of the retrieved articles, and leads to an overall performance of 85.8%."}, {"heading": "2 WIKIMOVIES Dataset", "text": "We focus on the WIKIMOVIES1 dataset, proposed by (Miller et al., 2016). The dataset consists of pairs of questions and answers about movies. Some examples are shown in Table 1.\nAs a knowledge source approximately 18K articles from Wikipedia are also provided, where each article is about a movie. Since movie articles can be very long, we only use the first paragraph of the article, which typically provides a summary of the movie. Formally, the dataset consists of question-answer pairs {(qj , Aj)}Jj=1 and movie articles {dk}Kk=1. Additionally, the dataset includes a list of entities: movie titles, actor names, genres etc. Answers to all the questions are in the entity list. The questions are created by human annotators using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset, and the annotated answers come from facts in two structured KBs: OMDb2 and MovieLens3.\nThere are two splits of the dataset. The \u201cFull\u201d dataset consists of 200K pairs of questions and answers. In this dataset, some questions are difficult to answer from Wikipedia articles alone. A second version of the dataset, \u201cWiki Entity\u201d is constructed by removing those QA pairs where the entities in QAs are not found in corresponding Wikipedia articles. We call these splits WIKIMOVIES-FL and WIKIMOVIES-WE, respectively. The questions are divided into train, dev and test such that the same question template does not appear in different splits. Further, they can be categorized into 13 categories, including movie to actors, director to movies, etc.4 The basic statistics of the dataset are summarized in Table 2.\nWe also note that more than 50% of the entities appear less than 5 times in the training set. This makes it very difficult to learn the global statistics of each entity, necessitating the need to use an external knowledge source."}, {"heading": "3 Comprehension Model", "text": "Our QA system answers questions in two steps, as shown in Figure 1. The first step is retrieval,\n1 http://fb.ai/babi 2 http://beforethecode.com/projects/omdb/download.\naspx 3 http://grouplens.org/datasets/movielens/\n4Category labels are only available for dev/test dataset\nwhere articles relevant to the question are retrieved. The second step is comprehension, where the question and retrieved articles are processed to derive answers.\nIn this section we focus on the comprehension model, assuming that relevant articles have already been retrieved and merged into a context document. In the next section, we will discuss approaches for retrieving the articles.\nMiller et al. (2016), who introduced WIKIMOVIES dataset, used an improved variant of Memory Networks called Key-Value Memory Networks. Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).\nWIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016). First, with imperfect retrieval, the answer may not be present in the context. We handle this case by using the proposed mixture model. Second, there may be multiple answers to a question, such as a list of actors. We handle this by optimizing a sum of the cross-entropy loss over all possible answers.\nWe also use attention sum architecture proposed by Kadlec et al. (2016), which has been shown to give high performance for comprehension tasks. In this approach, attention scores over the context entities are used as the output. We term this the attention distribution patt, defined over the entities in the context. The mixture model combines this distribution with another output probability distribution pvocab over all the entities in the vocabulary. The intuition behind this is that named entities (such as actors and directors) can be better handled by the attention part, since there are few global statistics available for these, and other entities (such as languages and genres) can be captured by vocabulary part, for which global statistics can be leveraged."}, {"heading": "3.1 Comprehension model detail", "text": "Let V be the vocabulary consisting of all tokens in the corpus, and E be the set of entities in the corpus The question is converted to a sequence of lower cased word ids, (wi) \u2208 V and a sequence of 0-1 flags for word capitalization, (ci) \u2208 {0, 1}. For each word position i, we also associate an entity id if the i-th word is part of an entity, ei \u2208 E (see Figure 3). Then, the combined embedding of the i-th position is given by\nxi = Ww(wi)+Wc(ci)\u2016We(ei), (i = 1, . . . , Lq), (1) where \u2016 is the concatenation of two vectors, Lq is the number of words in a question q, and Ww,Wc and We are embedding matrices. Note that if there are no entities at i-th position, We(ei) is set to zero. The context is composed of up to M movie articles concatenated with a special separation symbol. The contexts are embedded in exactly the same way as questions, sharing the embedding matrices.\nTo avoid overfitting, we use another technique called anonymization. We limit the number of columns of We to a relatively small number, ne, and entity ids are mapped to one of ne columns randomly (without collision). The map is common for each question/context pair but randomized across pairs. The method is similar to the anonymization method used in CNN / Daily Mail datasets (Hermann et al., 2015). Wang et al. (2016) showed that such a procedure actually helps readers since it adds coreference information to the system.\nNext, the question embedding sequence (xi) is fed into a bidirectional GRU (BiGRU) (Cho et al., 2014) to obtain a fixed length vector v\nv = \u2212\u2192 h q(Lq)\u2016 \u2190\u2212 h q(0), (2)\nwhere \u2212\u2192 h q and \u2190\u2212 h q are the final hidden states of forward and backward GRUs respectively. The context embedding sequence is fed into another BiGRU, to produce the output Hc = [hc,1, hc,2, . . . hc,Lc ], where Lc is the length of the context. An attention score for each word position i is given by\nsi \u221d exp(vThc,i). (3)\nThe probability over the entities in the context is then given by\npatt(e) \u221d \u2211\ni\u2208I(e,c)\nsi, (4)\nwhere I(e, c) is the set of word positions in the entity e within the context c.\nWe next define the probability pvocab to be the probability over the complete set of entities in the corpus, given by\npvocab(e) = Softmax(V u), (5)\nwhere the vector u is given by u = \u2211\ni sihc,i. Each row of the matrix V is the coefficient vector for an entity in the vocabulary. It is computed similar to Eq. (1).\nV (e) = \u2211 w\u2208e Ww(w) + \u2211 c\u2208e Wc(c)\u2016We(e). (6)\nThe embedding matrices are shared between question and context.\nThe final probability that an entity e answers the question is given by the mixture p(e) = (1 \u2212 g)patt(e)+gpvocab(e), with the mixture coefficient g defined as\ng = \u03c3(Wgg0), g0 = v Tu\u2016maxV u. (7)\nThe two components of g0 correspond to the attention part and vocabulary part respectively. Depending on the strength of each, the value of g may be high or low.\nSince there may be multiple answers for a question, we optimize the sum of the probabilities:\nloss = \u2212 log ( \u2211\na\u2208Aj\np(a|qj , cj) )\n(8)\nOur overall model is displayed in Figure 4. We note that KV-MemNN (Miller et al., 2016) employs \u201cTitle encoding\u201d technique, which uses the prior knowledge that movie titles are often in\nanswers. Miller et al. (2016) showed that this technique substantially improves model performance by over 7% for WIKIMOVIES-WE dataset. In our work, on the other hand, we do not use any data specific feature engineering."}, {"heading": "4 Retrieval Model", "text": "Our QA system answers questions by two steps as in Figure 1. Accurate retrieval of relevant articles is essential for good performance of the comprehension model, and in this section we discuss three approaches for it. We use up toM articles as context. A baseline approach for retrieval is to select articles which contain at least one entity also present in the question. We identify maximal intervals of words that match entities in questions and articles. Capitalization of words is ignored in this step because some words in the questions are not properly capitalized. Out of these (say N ) articles we can randomly select M . We call this approach (r0). For some movie titles, however, this method retrieves too many articles that are actually not related to questions. For example, there is a movie titled \u201cLove Story\u201d which accidentally picks up the words \u201clove story\u201d. This degrades the performance of the comprehension step. Hence, we describe two more retrieval models \u2013 (1) a dataset specific hand-crafted approach, and (2) a general learning based approach."}, {"heading": "4.1 Hand-Crafted Model (r1)", "text": "In this approach, the N articles retrieved using entity matching are assigned scores based on certain heuristics. If the movie title matches an entity in the question, the article is given a high score, since it is very likely to be relevant. A similar heuristic was also employed in (Miller et al., 2016). In addi-\ntion, the number of matching entities is also used to score each article. The top M articles based on these scores are selected for comprehension. This hand-crafted approach already gives strong performance for the WIKIMOVIES dataset, however the heuristic for matching article titles may not be appropriate for other QA tasks. Hence we also study a general learning based approach for retrieval."}, {"heading": "4.2 Learning Model (R2)", "text": "The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question. For example, for x to movie question type, the answer movie articles are the correct articles to be retrieved. On the other hand, for questions in movie to x type, the movie in the question should be retrieved. Having collected the labels, we train a retrieval model for classifying a question and article pair as relevant or not relevant.\nFigure 5 gives an overview of the model, which uses a Word Level Attention (WLA) mechanism. First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs v (for the question) and Hc (for the document) similar to the previous section.\nTo classify the article as relevant or not, we introduce a novel attention mechanism to compute the score,\ns = \u2211 i ((wv\u0303 + b)T h\u0303c,i) 4 (9)\nEach term in the sum above corresponds to the\nmatch between the query representation and a token in the context. This is passed through a 4-th order non-linearity so that relevant tokens are emphasized more5. Next, we compute the probability that the article is relevant using a sigmoid:\no = \u03c3(w\u2032s+ b\u2032) (10)\nIn the above, x\u0303 is the normalized version (by L2norm) of vector x, w, b, w\u2032, b\u2032 are scalar learnable parameters to control scales."}, {"heading": "5 Experiments", "text": "We evaluate the comprehension model on both WIKIMOVIES-FL and WIKIMOVIES-WE datasets. The performance is evaluated using the accuracy of the top hit (single answer) over all possible answers (all entities). This is called hits@1 metric.\nFor the comprehension model, we use embedding dimension 100, and GRU dimension 128. We use up to M = 10 retrieved articles as context. The order of the articles are randomly shuffled for each training instance to prevent over-fitting. The size of the anonymized entity set ne is 600, since in most of the cases, number of entities in a question and context pair is less than 600.\nFor training the comprehension model, the Adam (Kingma and Ba, 2015) optimization rule is used with batch size 32. We stop the optimization based on dev-set performance, and training takes around 10 epochs. For WIKIMOVIES-FL (resp. WIKIMOVIES-WE) dataset, each epoch took approximately 4 (resp. 2) hours on an Nvidia GTX1080 GPU.\nFor training the retrieval model R2, we use a binary cross entropy objective. Since most articles are not relevant to a question, the ration of positive and negative samples is tuned to 1 : 10. Each\n5 We use exponent d = 4 here. Higher d tend to have better performance. Empirically, this approach works better than exponential and softmax non-linearities.\nepoch for training the retrieval model takes about 40 minutes on an Nvidia GTX1080 GPU."}, {"heading": "5.1 Performance of Retrieval Models", "text": "We evaluate the retrieval models based on precision and recall of the oracle articles. The evaluation is done on the test set. R@k is the ratio of cases where the highest ranked oracle article is in the top k retrieved articles. P@k is the ratio of oracle articles which are in the top k retrieved results. These numbers are summarized in Table 3. We can see that both (r1) and (R2) significantly outperform (r0), with (R2) doing slightly better. We emphasize that (R2) uses no domain specific knowledge, and can be readily applied to other datasets where articles may not be about specific types of entities.\nWe have also tested simpler models based on inner product of question and article vectors. In these models, a question qj and article dk are converted to vectors \u03a6(qj),\u03a8(dk), and the relevance score is given by their inner product:\nscore(j, k) = \u03a6(qj) T\u03a8(dk). (11)\nIn the view of computation, those models are attractive because we can compute the article vectors offline, and do not need to compute the attention over words in the article. Maximum Inner Product Search algorithms may also be utilized here (Chandar et al., 2016; Auvolat et al., 2015). However, as shown in upper block of\nTable 4, those models perform much worse in terms of scoring. The \u201cSum of Hidden State\u201d and \u201cQuery Free Attention\u201d models are similar to WLA model, using BiGRUs for question and article. In both of those models, \u03a6(q) is defined the same way as WLA model, Eq (2). For the \u201cSum of Hidden States\u201d model, \u03a8(d) is given by the sum of BiGRU hidden states. This is the same as the proposed model by replacing the fourth order of WLA to one. For the \u201cQuery Free Attention\u201d model, \u03a8(d) is given by the sum of BiGRU hidden states.\nWe compare our model and several ablations with the KV-MemNN model. Table 5 shows the average performance across three evaluations. The (V) \u201cVocabulary Model\u201d and (A) \u201cAttention Model\u201d are simplified versions of the full (AV) \u201cAttention and Vocabulary Model\u201d, using only pvocab and patt, respectively. Using a mixture of patt and pvocab gives the best performance.\nInterestingly, for WE dataset the Attention model works better. For FL dataset, on the other hand, it is often impossible to select answer from the context, and hence the Vocab model works better.\nThe number of entities in the full vocabulary is 71K, and some of these are rare. Our intuition to use the Vocab model was to only use it for common entities, and hence we next constructed a smaller vocabulary consisting of all entities which appear at least 10 times in the corpus. This results in a subset vocabulary VS of 2400 entities. Using\nthis vocabulary in the mixture model (AsV) further improves the performance.\nTable 5 also shows a comparison between (r0), (r1), and (R2) in terms of the overall task performance. We can see that improving the quality of retrieved articles benefits the downstream comprehension performance. In line with the results of the previous section, (r1) and (R2) significantly outperform (r0). Among (r1) and (R2), (R2) performs slightly better."}, {"heading": "5.2 Benefit of training methods", "text": "Table 6 shows the impact of anonymization of entities and shuffling of training articles before the comprehension step, described in Section 3.\nShuffling the context article before concatenating them, works as a data augmentation technique. Entity anonymization helps because without it each entity has one embedding. Since most of the entities appear only a few times in the articles, these embeddings may not be properly trained. Instead, the anonymous embedding vectors are trained to distinguish different entities. This technique is motivated by a similar procedure used in the construction of CNN / Daily Mail\n(Hermann et al., 2015), and discussed in detail in (Wang et al., 2016)."}, {"heading": "5.3 Visualization", "text": "Figure 6 shows a test example from the WIKIMOVIES-FL test data. In this case, even though the answers \u201cHindi\u201d and \u201cEnglish\u201d are not in the context, they are correctly estimated from pvocab. Note the high value of g in this case. Figure 7 shows another example of how the mixture model works. Here the the answer is successfully selected from the document instead of the vocabulary. Note the low value of g in this case."}, {"heading": "5.4 Performance in each category", "text": "Table 7 shows the comparison for each category of questions between our model and KV-MemNN for the WIKIMOVIES-WE dataset 6. We can see that performance improvements in the movie to x category is relatively large. The KV-MemNN model has a dataset specific \u201cTitle encoding\u201d feature which helps the model x to movie question types. However without this feature performance in other categories is poor."}, {"heading": "5.5 Analysis of the mixture gate", "text": "The benefit of the mixture model comes from the fact that ppointer works well for some question\n6Categories \u201cMovie to IMDb Votes\u201d and \u201cMovie to IMDb Rating\u201d are omitted from this table because there are only 0.5% test data for these categories and most of the answers are \u201cfamous\u201d or \u201cgood\u201d.\ntypes, while pvocab works well for others. Table 8 shows how often for each category pvocab is used (g > 0.5) in AsV model. For question types \u201cMovie to Language\u201d and \u201cMovie to Genre\u201d (the so called \u201cchoice questions\u201d) the number of possible answers is small. For this case, even if the answer can be found in the context, it is easier for the model to select answer from an external vocabulary which encodes global statistics about the entities. For other \u201cfree questions\u201d, depending on the question type, one approach is better than the other. Our model is able to successfully estimate the latent category and switch the model type by controlling the coefficient g."}, {"heading": "6 Related Work", "text": "Choi et al. (2016) solve the QA problem by selecting a sentence in the document. They show\nthat joint training of selection and comprehension slightly improves the performance. In our case, joint training is much harder because of the large number of movie articles. Hence we introduce a two-step retrieval and comprehension approach.\nRecently Zoph and Le (2016) proposed a framework to use the performance on a downstream task (e.g. comprehension) as a signal to guide the learning of neural network which determines the input to the downstream task (e.g. retrieval). This motivates us to introduce neural network based approach for both retrieval and comprehension, since in this case the retrieval step can be directly trained to maximize the downstream performance.\nIn the context of language modeling, the idea of combining of two output probabilities is given in (Merity et al., 2016), however, our equation to compute the mixture coefficient is slightly different. More recently, Ahn et al. (2016) used a mixture model to predict the next word from either the entire vocabulary, or a set of Knowledge Base facts associated with the text. In this work, we present the first application of such a mixture model to reading comprehension."}, {"heading": "7 Conclusion and Future Work", "text": "We have developed QA system using a two-step retrieval and comprehension approach. The comprehension step uses a mixture model to achieve state of the art performance on WIKIMOVIES dataset, improving previous work by a significant margin.\nWe would like to emphasize that our approach has minimal heuristics and does not use dataset specific feature engineering. Efficient retrieval while maintaining representation variation is a challenging problem. While there has been a lot of research on comprehension, little focus has been given to designing neural network based retrieval models. We present a simple such model, and emphasize the importance of this direction of research."}], "references": [{"title": "A neural knowledge language model", "author": ["Sungjin Ahn", "Heeyoul Choi", "Tanel P\u00e4rnamaa", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1608.00318 .", "citeRegEx": "Ahn et al\\.,? 2016", "shortCiteRegEx": "Ahn et al\\.", "year": 2016}, {"title": "Clustering is efficient for approximate maximum inner product searrch", "author": ["Alex Auvolat", "Sarath Chandar", "Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio."], "venue": "arXiv:1507.05910 .", "citeRegEx": "Auvolat et al\\.,? 2015", "shortCiteRegEx": "Auvolat et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1533\u20131544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "ACM SIGMOD international conference on Management of data. ACM, pages", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Hierarchical memory networks", "author": ["Sarath Chandar", "Sungjin Ahn", "Hugo Larochelle", "Pascal Vincent", "Gerald Tesauro", "Yoshua Bengio."], "venue": "arXiv:1605.07427 .", "citeRegEx": "Chandar et al\\.,? 2016", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP. pages", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Hierarchical question answering for long documents", "author": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste", "Illia Polosukhin", "Jakob Uszkoreit", "Jonathan Berant."], "venue": "arXiv: 1611.01839 .", "citeRegEx": "Choi et al\\.,? 2016", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv: 1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 1156\u20131165.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems 28 (NIPS),", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Xiong Caiming", "Bradbury James", "Richard Socher."], "venue": "arXiv: 1609.07843 .", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 200\u2013207.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Communications of the ACM 57(10):78\u201385.", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}, {"title": "Emergent logical structure in vector representations of neural readers", "author": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester."], "venue": "arXiv: 1611.07954 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Knowledge base completion via search-based question answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin."], "venue": "Proceedings of the 23rd international conference on World wide web. ACM, pages", "citeRegEx": "West et al\\.,? 2014", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V. Le."], "venue": "arXiv: 1611.01578 .", "citeRegEx": "Zoph and Le.,? 2016", "shortCiteRegEx": "Zoph and Le.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Most of the current approaches for Question Answering (QA) are based on structured Knowledge Bases (KB) such as Freebase (Bollacker et al., 2008) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014).", "startOffset": 121, "endOffset": 145}, {"referenceID": 19, "context": ", 2008) and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014).", "startOffset": 21, "endOffset": 51}, {"referenceID": 11, "context": "In this setting the question is converted to a logical form using semantic parsing, which is queried against the KB to obtain the answer (Fader et al., 2014; Berant et al., 2013).", "startOffset": 137, "endOffset": 178}, {"referenceID": 3, "context": "In this setting the question is converted to a logical form using semantic parsing, which is queried against the KB to obtain the answer (Fader et al., 2014; Berant et al., 2013).", "startOffset": 137, "endOffset": 178}, {"referenceID": 21, "context": "However, recent studies have shown that even large curated KBs, such as Freebase, are incomplete (West et al., 2014).", "startOffset": 97, "endOffset": 116}, {"referenceID": 18, "context": "This retrieval based approach has a longer history than the KB based approach (Voorhees and Tice, 2000).", "startOffset": 78, "endOffset": 103}, {"referenceID": 16, "context": "However, there are still gaps in its performance compared to the KB-based approach (Miller et al., 2016).", "startOffset": 83, "endOffset": 104}, {"referenceID": 17, "context": "Several large-scale datasets introduced recently (Rajpurkar et al., 2016; Hermann et al., 2015) have ar X iv :1 70 3.", "startOffset": 49, "endOffset": 95}, {"referenceID": 12, "context": "Several large-scale datasets introduced recently (Rajpurkar et al., 2016; Hermann et al., 2015) have ar X iv :1 70 3.", "startOffset": 49, "endOffset": 95}, {"referenceID": 10, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 13, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 22, "context": "These models fall into one of two categories: (1) those which extract answers as a span of text from the document (Dhingra et al., 2016; Kadlec et al., 2016; Xiong et al., 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al.", "startOffset": 114, "endOffset": 177}, {"referenceID": 7, "context": ", 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al., 2016; Miller et al., 2016) (Figure 2 bottom).", "startOffset": 82, "endOffset": 122}, {"referenceID": 16, "context": ", 2016) (Figure 2 top); (2) those which select the answer from a fixed vocabulary (Chen et al., 2016; Miller et al., 2016) (Figure 2 bottom).", "startOffset": 82, "endOffset": 122}, {"referenceID": 2, "context": "We incorporate the above mixture model in a simple Recurrent Neural Network (RNN) architecture with an attention mechanism (Bahdanau et al., 2015) for comprehension.", "startOffset": 123, "endOffset": 146}, {"referenceID": 2, "context": "We incorporate the above mixture model in a simple Recurrent Neural Network (RNN) architecture with an attention mechanism (Bahdanau et al., 2015) for comprehension. In the second part of the paper we focus on the retrieval step for the QA system, and introduce a neural network based ranking model to select the articles to feed the comprehension model. We evaluate our model on WIKIMOVIES dataset, which consists of 200K questions about movies, along with 18K Wikipedia articles for extracting the answers. Miller et al. (2016) applied Key-Value Memory Neural Networks (KV-MemNN) to the dataset, achieving 76.", "startOffset": 124, "endOffset": 530}, {"referenceID": 16, "context": "We focus on the WIKIMOVIES1 dataset, proposed by (Miller et al., 2016).", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "The questions are created by human annotators using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset, and the annotated answers come from facts in two structured KBs: OMDb2 and MovieLens3.", "startOffset": 68, "endOffset": 89}, {"referenceID": 13, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 10, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 7, "context": "Instead, we use RNN based network, which has been successfully used in many reading comprehension tasks (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 104, "endOffset": 166}, {"referenceID": 13, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 10, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 7, "context": "WIKIMOVIES dataset has two notable differences from many of the existing comprehension datasets, such as CNN and SQuAD (Kadlec et al., 2016; Dhingra et al., 2016; Chen et al., 2016).", "startOffset": 119, "endOffset": 181}, {"referenceID": 13, "context": "We also use attention sum architecture proposed by Kadlec et al. (2016), which has been shown to give high performance for comprehension tasks.", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "The method is similar to the anonymization method used in CNN / Daily Mail datasets (Hermann et al., 2015).", "startOffset": 84, "endOffset": 106}, {"referenceID": 8, "context": "Next, the question embedding sequence (xi) is fed into a bidirectional GRU (BiGRU) (Cho et al., 2014) to obtain a fixed length vector v", "startOffset": 83, "endOffset": 101}, {"referenceID": 11, "context": "The method is similar to the anonymization method used in CNN / Daily Mail datasets (Hermann et al., 2015). Wang et al. (2016) showed that such a procedure actually helps readers since it adds coreference information to the system.", "startOffset": 85, "endOffset": 127}, {"referenceID": 16, "context": "We note that KV-MemNN (Miller et al., 2016) employs \u201cTitle encoding\u201d technique, which uses the prior knowledge that movie titles are often in answers.", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": "We note that KV-MemNN (Miller et al., 2016) employs \u201cTitle encoding\u201d technique, which uses the prior knowledge that movie titles are often in answers. Miller et al. (2016) showed that this technique substantially improves model performance by over 7% for WIKIMOVIES-WE dataset.", "startOffset": 23, "endOffset": 172}, {"referenceID": 16, "context": "A similar heuristic was also employed in (Miller et al., 2016).", "startOffset": 41, "endOffset": 62}, {"referenceID": 14, "context": "For training the comprehension model, the Adam (Kingma and Ba, 2015) optimization rule is used with batch size 32.", "startOffset": 47, "endOffset": 68}, {"referenceID": 6, "context": "Maximum Inner Product Search algorithms may also be utilized here (Chandar et al., 2016; Auvolat et al., 2015).", "startOffset": 66, "endOffset": 110}, {"referenceID": 1, "context": "Maximum Inner Product Search algorithms may also be utilized here (Chandar et al., 2016; Auvolat et al., 2015).", "startOffset": 66, "endOffset": 110}, {"referenceID": 12, "context": "This technique is motivated by a similar procedure used in the construction of CNN / Daily Mail (Hermann et al., 2015), and discussed in detail in (Wang et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 20, "context": ", 2015), and discussed in detail in (Wang et al., 2016).", "startOffset": 36, "endOffset": 55}, {"referenceID": 23, "context": "Recently Zoph and Le (2016) proposed a framework to use the performance on a downstream task (e.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": "In the context of language modeling, the idea of combining of two output probabilities is given in (Merity et al., 2016), however, our equation to compute the mixture coefficient is slightly different.", "startOffset": 99, "endOffset": 120}, {"referenceID": 0, "context": "More recently, Ahn et al. (2016) used a mixture model to predict the next word from either the entire vocabulary, or a set of Knowledge Base facts associated with the text.", "startOffset": 15, "endOffset": 33}], "year": 2017, "abstractText": "Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. For comprehension, we present an RNN based attention model with a novel mixture mechanism for selecting answers from either retrieved articles or a fixed vocabulary. For retrieval we introduce a hand-crafted model and a neural model for ranking relevant articles. We achieve state-of-the-art performance on WIKIMOVIES dataset, reducing the error by 40%. Our experimental results further demonstrate the importance of each of the introduced components.", "creator": "LaTeX with hyperref package"}}}