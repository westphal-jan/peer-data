{"id": "1703.08970", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Multimodal deep learning approach for joint EEG-EMG data compression and classification", "abstract": "In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer, and then applied into the neural networks.\n\n\n\n\nThe above two sections illustrate the approach using the deep autoencoder architecture. The first section highlights how it can be used to extract and extract all signals. The second section highlights how it can be used to obtain information from a discriminant representation.\nTo obtain discriminant features in the data representation, we first use the neural networks, which are designed to store all the neural neural networks of the neural network. After extracting the discriminant features in the neural networks of the neural network, we extract the data by extracting and reconstructing the neural networks. The neural networks can be used to extract and store all the neural networks of the neural network. Using the deep autoencoder architecture we first construct an algorithm based on the deep autoencoder architecture which, in our present work, has been implemented as an integral feature. The algorithm will be described in the section on how the neural networks can be used to extract and extract data from the neural networks of the neural network.\nFinally, in the previous section, we have shown how to extract and extract data from the neural networks. The neural networks are written in a simple way which allows us to extract the discriminant features of the neural networks. The neural networks are composed of a single vector with a single vector containing the values of the discriminant components. We then extract the signal in the neural networks of the neural networks of the neural networks of the neural network. The neural networks are written in a simple way which allows us to extract the discriminant features of the neural networks of the neural networks. The neural networks are written in a simple way which allows us to extract and extract data from the neural networks of the neural networks of the neural networks. The neural networks are written in a simple way which allows us to extract and extract the discriminant features of the neural networks of the neural networks. The neural networks are written in a simple way which allows us to extract and extract", "histories": [["v1", "Mon, 27 Mar 2017 08:37:35 GMT  (586kb,D)", "http://arxiv.org/abs/1703.08970v1", "IEEE Wireless Communications and Networking Conference (WCNC), 2017"]], "COMMENTS": "IEEE Wireless Communications and Networking Conference (WCNC), 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed ben said", "amr mohamed", "tarek elfouly", "khaled harras", "z jane wang"], "accepted": false, "id": "1703.08970"}, "pdf": {"name": "1703.08970.pdf", "metadata": {"source": "CRF", "title": "Multimodal deep learning approach for joint EEG-EMG data compression and classification", "authors": ["Ahmed Ben Said", "Amr Mohamed", "Tarek Elfouly", "Khaled Harras", "Z. Jane Wang"], "emails": ["tarekfouly}@qu.edu.qa", "kharras@qatar.cmu.edu", "zjanew@ece.ubc.ca"], "sections": [{"heading": null, "text": "Index Terms\u2014mHealth, deep learning, compression, classification\nI. INTRODUCTION\nHealthcare has always been considered as a strategic priority worldwide. The increasing number of elderly and chronic disease patients has made the physical contact between the caregiver and patients more and more difficult. Following the fast development of wireless technologies, the interoperability between healthcare entities and mobile has grown. The development of complex devices has stimulated the creation of many mobile health or \u2018mHealth\u2019 applications and wearable devices for fitness tracker, sleep monitoring [1]. . . The mHealth industry is predicted to grow $12 billions by 2018 [2]. Motivated by the myriad of biomedical sensors, mobile phones and applications, the scientific communities have standardized the system that focuses on the acquisition of vital signs such as electroencephalogram (EEG) and Electromyogram (EMG) by body area sensor networks (BASN) under IEEE 802.15.6 [3]. Thus, a typical mHealth BASN system consists of sensors collecting the data, a Personal Data Aggregator (PDA) and remote server. However, due to network limitation, data delivery through the network can be hindered. Consequently, we need to optimize every bit of data being sent. One of the possible pre-processing stages is to encode the data in the PDA i.e. mapping xi to compressed data zi. At the server level, the received data zi is decoded i.e. mapped to x\u0302i which approximates the original data xi. Many successful algorithms have been proposed for time series compression. Srinvasan et al. [4] designed a 2-D lossless EEG compression where the signal is arranged in 2-D matrix as a preprocessing. Compression is achieved through a two-stage coder composed of Set Partitioning In Hierarchical Trees (SPIHT) [5] layer and Arithmetic Coding (RC) layer. Hussein et al. [6] proposed a scalable and energy efficient EEG compression scheme based on Discret Wavelet Transform (DWT) and Compressive Sensing (CS) in wireless sensors. Several parameters have been considered to control the total energy consumption of the encoder and transmitter. The optimal configuration of these parameters is chosen based on optimization scheme where the total power consumption should not exceed a certain threshold. In [7], authors applied CS technique for EEG signal compression. Since the multichannel EEG signals have common sparse support in the transform domain, they stack the sparse transform coefficients as columns of matrix. Thus, the recovery problem becomes row-sparse and solved through Bregman algorithm [8]. Majumdar et al. [9] argued that CS is not efficient for EEG compression because there is no sparsifying basis that fulfills the requirements of incoherence and sparsity. Instead, authors formulated the problem as a rank deficiency problem solved by a Bregman-derived algorithm. Following the development of wireless BASN, vital signs data have become abundant. mHealth systems are now capable of collecting data from different modalities (EEG, EMG, etc). Although, they may seem totally different, these data can describe the same phenomena. For example, in case of schizophrenic person, when a stimulus is presented, a peak in the EEG registration is witnessed while the functional Magnetic Resonance Imaging (fMRI) data shows activations in the temporal lobe and the middle anterior cingulate region [10]. Thus, both modalities are very likely to be correlated. Each modality has its advantages and limitations but analyzing multiple modalities offers better understanding of the investigated phenomena. The aforementioned methods, although exhibit good performance, do not exploit the correlation among multiple modalities. Deep learning approach has emerged as one of the possible techniques to exploit the correlation of the data from multiple modalities. Ngiam et al. [11] proposed a multimodal deep ar X iv :1 70 3. 08 97 0v 1 [ cs .L G ] 2 7 M ar 2\n01 7\nlearning approach for cross modality feature learning from video and speech data. Srivastava et al. [12] built a multimodal deep belief network [13] to learn multimodal representation from image and text data for image annotation and retrieval tasks. In [14], authors designed a deep Boltzmann machine [15] based architecture to extract a meaningful representation from multimodal data for classification and information retrieval task. Liu et al. [16] proposed a multimodal autoencoder [17] approach for video classification based on audio, image and text data where the intra-modality semantic for each data is separately learning by a stacked autoencoder. Next, the learned features are concatenated and fed to another deep autoencoder with a softmax layer for classification. Few research attempts have addressed the possible application of autoencoder for biomedical and mHealth applications. In [18], Yann Ollivier proved that there is a strong relationship between minimizing the code length of the data and minimizing reconstruction error that an autoencoder seeks. Tan et al. [19] used a stacked autoencoder for mammogram image compression. Training is conducted on image patches instead of the whole images. In [20], authors applied the autoencoder for Electrocardiogram (ECG) compression. Comparison results with various classic compression methods showed that this special type network is reliable for signal compression. However, the problem of multimodal data compression in context of mHealth is still not well-investigated. We propose in this paper a multimodal approach for data compression and feature learning. The encoding-decoding scheme can be achieved through a stack of autoencoders. Our approach exploits the intracorrelation as well as the intercorrelation among multiple modalities to achieve efficient compression and classification. The rest of the paper is organized as follows: in section II, we present the autoencoder architecture. Section III is dedicated for presenting our multimodal approach for joint EEG and EMG data compression and classification. Experimental results are illustrated and discussed in section IV and we conclude in the last section."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Autoencoder", "text": "An autoencoder, illustrated in Fig. 1, is a special type of neural network consisting of three layers. The data are first fed into the input layer, propagated to a second layer called the hidden or bottleneck layer and then reconstructed at a third layer called the reconstruction layer. The encoder transforms the set of data vectors x \u2208 RX into hidden representation h \u2208 RH via an activation function f :\nh = f(Wx+ b) (1)\nThe decoder transforms back the hidden representation h to reconstruction data r \u2208 RX via an activation function g:\nr = g(W \u2032 h+ b \u2032 ) (2)\nThe Parameters W \u2208 RX\u00d7H and W \u2032 \u2208 RH\u00d7X are called weight matrices. b \u2208 RH and b\u2032 \u2208 RX are called\nthe bias vectors. f and g are typically hyperbolic function tanh(x) = ( ex \u2212 e\u2212x ) / ( ex + e\u2212x ) or sigmoid function\nsigmoid(x) = 1/ ( 1 + e\u2212x ) . In practice, we use tight weight configuration i.e. W \u2032\n= WT . Autoencoder seeks the optimal set of parameters \u0398 = {W, b, b\u2032} that minimizes the reconstruction error J\u0398(x, r). This error is generally the Squared Euclidean distance L(x, r) = ||x \u2212 r||2 or cross-entropy loss L(x, r) = \u2212 \u2211X i=1 xilog(ri)+(1\u2212xi)log(1\u2212ri). When using affine activation function and squared error loss, autoencoder essentially performs Principal Component Analysis (PCA) 1. Minimization is generally carried out via gradient descent algorithm. In other words, the purpose of this minimization is to obtain r \u2248 x i.e. an approximation to the identity function. But, by constraining the system by limiting the number of hidden units at the hidden layer, we are forcing the system to learn a compressed version of the data. Furthermore, to prevent overfitting, i.e. just learning the identity function, another constraint is often added: a weight decay term that regularizes J\u0398(x, r). Then, we have:\nJ\u0398(x, r) = L(x, r) + \u03bb||W ||22 (3)\nWhere \u03bb is the decay parameter that controls the amount of regularization."}, {"heading": "B. Stacked autoencoder", "text": "Stacked autoencoder (SAE), illustrated in Fig 2, is a neural network which consists of multiple layers of autoencoders. The output of each layer is fed to the next layer. SAE is trained via a greedy layer-wise training [21]. Specifically, it is done one layer at a time. At each layer, we consider the autoencoder composed of the current layer and its previous one which is the output of the previous layer. Once N\u22121 layers are trained, we can compute the output of th N th layer wired to it. This unsupervised stage is followed by a supervised fine-tuning of the parameters where a softmax layer is added on top of the SAE.\n1It will find the same subspace as PCA but the projection direction does not essentially correspond to the principal components directions"}, {"heading": "III. MULTIMODAL AUTOENCODER FOR EEG-EMG COMPRESSION AND CLASSIFICATION", "text": "Fig. 3 exhibits the multimodal autoencoder architecture. It consists of two pathways for EEG and EMG. Each pathway represents a unimodal stacked autoencoder dedicated to learn the intra-modality correlation of the data while the joint layer merge the higher level features."}, {"heading": "A. Unimodal data pre-training", "text": "SAE is applied separately for each modality, we use the sigmoid activation function and the Squared Euclidean distance as loss function regularized by a weight decay term. We apply also tied weight configuration. The output of the ith layer is obtained as follows:\nz1 = sigmoid(W1x1 + b1) i = 1 zi = sigmoid(Wixi + bi) i = 2..N\n(4)\nThe SAE is trained using the greedy-layer wise training approach where we feed the latent representation of the autoencoder found below to the current layer. This deep architecture makes the system more scalable and efficient while progressively extracting higher level features from the high dimensional data."}, {"heading": "B. Deep multimodal learning", "text": "The single modal pre-training does not involve intermodality correlation which can contribute in better representation of the higher level features. It especially allows encoding the multiple modalities in a single shared representation obtained by the joint layer. The output of this layer encompasses the contribution of each modality in the code which represents the compressed data. The joint representation is obtained as follows:\nz = \u2211\ni\u2208{e,m}\nsigmoid ( W iN+1z i N+1 + b i N+1 ) (5)\nWhere e and m refer to EEG and EMG respectively. Furthermore, we train the multimodal autoencoder with an augmented noisy data where additional examples are added leading to samples with only one single modality. In practice, we add zeros values examples for one modality while keeping the original values for the other modality and vice-versa. Thus, one third of the training data is EEG only, another one third is EMG only and the rest has both EEG and EMG data. This strategy, inspired from Nigiam et al [11], follows the denoising autoencoder paradigm [22] and is justified by twofold: \u2022 Correlation among multiple modalities is very likely to\nbe non-linear. \u2022 This non-linearity often leads to hidden units being\nactivated by one single modality. Therefore, the original and corrupted inputs are propagated independently to the higher layers which are then trained progressively to reconstruct the clean presentation from both inputs."}, {"heading": "C. Fine-tuning", "text": "The compressed data can be used for classification task, that is, to fine-tune the layers with respect to a supervised criterion by plugging the bottleneck layer to a softmax classifier [23]:\np\u0302 = exp(Wy + b)\u2211L\nl=1 exp(W ly + bl)\n(6)\nWhere p\u0302 is the predicted object label, y represents the compressed data and L is the number of classification labels. Therefore, the overall objective function to minimize is:\n=(x, r, p, p\u0302) = J\u0398(x, r) + L(p, p\u0302) (7)\nWhere p is the true label and L(p, p\u0302) can be an entropic loss function."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "mHealth systems acquire, process, store, secure and transport the medical data. Data delivery should be as efficient and optimized as possible in terms of energy consumption and bandwidth usage. A typical system consists of mHealth wearable device that senses vital signs. These data are collected by a PDA and should be transmitted to a remote server handled by a medical entity [24]. At the server level, a multimodal autoencoder is already trained and the optimal configuration is already found. This configuration is also known by the PDA which should apply it on the collected data for compression. We present in this section several experimental results where we compare our compression scheme with some state of the art compression methods. Furthermore, we compare our multimodal strategy with the unimodal one to highlight the importance of exploiting the intermodality correlation."}, {"heading": "A. Dataset", "text": "We conduct our experiments on the DEAP dataset [25]. It consists of EEG, EMG and multiple physiological signals recorded from 32 participants during 63 seconds at 128 Hz. During experiments, volunteers watched 40 music videos and\nrate them on a scale from 1 to 9 with respect to four criteria: likeness (dislike, like), valence (ranges from unpleasant to pleasant), arousal (ranges from uninterested or bored to excited) and dominance (ranges from helpless and weak feelings to empowered feeling). Signals are segmented into 6 seconds segments, whitened and normalized between 0 and 1. For both EEG and EMG data, we have 23040 samples of 896 dimensionality. These data should then be divided into training and testing sets."}, {"heading": "B. Compression tasks", "text": "We compare our compression method with the Discrete Wavelet Transform (DWT) [26], Compressed Sensing (CS) [27] and the 2D compression approach which is based on SPIHT and FastICA [28]. For the latter algorithm, we use two configurations of 3 and 6 independent components denoted 2D-SPIHT-3-ICs and 2D-SPIHT-6-ICs. We evaluate performance using compression ratio (CR) and residual distortion (D).\nCR = ( 1\u2212 m\nn\n) \u2217 100 (8)\nD = ||r \u2212 x|| ||x|| \u2217 100 (9)\nWhere m and n are the length of the compressed and original signals (number of samples). D is the percentage root-meansquare difference between the compressed and original signals. For each data pathway, we use a two-layer SAE. Table I presents the numbers of hidden units for each layer of the multimodal autoencoder as well as the DWT thresholds and their corresponding CRs. We divide data to 50% training and testing. Fig. 4 and 5 exhibit distortion variation with respect to different CR values for EEG and EMG. The findings show that for higher compression ratios, the multimodal approach\nperforms better than DWT and CS. For example, for CR=80%, our approach is able to reconstruct EEG and EMG with distortions of 12% and 13.85% respectively while CS distorts EEG by 22% and EMG by 17.21%. With 2D-SPIHT-3-ICs, EEG and EMG distortions are 33.7% and 35.7% respectively while with 2D-SPIHT-3-ICs, EEG and EMG are distorted by 33% and 33.5%. DWT exhibits low performance with 68% and 73.12% for EEG and EMG respectively. Although DWT, CS and the 2D approach perform better for low compression levels, the proposed method presents stable performance for different compression levels. This can be explained by the capacity of the underlying architecture to exploit the statistics of the data to achieve better compression. We further examine the effect of training/testing data partition on the compression results. Fig. 6 and Fig. 7 illustrate the whisker diagrams for EEG and EMG signals respectively. We can clearly deduce that more training data result in less distortion. This confirms a known deep learning rule of thumb stating the more training data we have, the better the results are."}, {"heading": "C. Classification task", "text": "The objective of this experiment is to demonstrate the importance of the multimodal approach. We conduct binary classification of the EEG and EMG with respect to two of the four labeling possibilities: dominance and arousal. We follow the same approach as in [25]: video ratings are thresholded into two classes. On the scale of 1 to 9, we simply place the threshold in the middle. We compare our approach with twolayer SAE and Deep Boltzmann Machine (DBM) architectures [15] with the softmax classifier on top of them. For SAE, we use the sigmoid activation function. We choose 75% trainingtesting partition. Figures 8 and 9 illustrate the classification results with respect to the dominance and arousal respectively. By exploiting the inter-modality correlation, the proposed approach achieves the best results with 78.1%. The singlemodality approaches are less accurate. These findings confirm that, when available, multiple modalities can offer better understanding of the underlying phenomena even if the data exhibit different characteristics."}, {"heading": "D. Discussion", "text": "In a typical mHealth system, a client-server architecture is the common choice where the system relies on the available networks to deliver the data. In general, the healthcare giver generally relies on multiple vital signs for an accurate diagnosis. The proposed approach is flexible in the sense that, if an additional modality is collected by the PDA via a wearable device, can be easily incorporated in the architecture presented in Fig. 3, compressed and classified. The deep neural network can be trained offline. Once it achieves good performance, the optimal configuration (weights and biases) is applied at the client side for efficient data delivery. However, it is worthnoting that our approach it less efficient for low compression ratio."}, {"heading": "V. CONCLUSION", "text": "We have presented a deep learning approach for multimodal data compression and classification. Our strategy focuses on exploiting the inter and intra correlation among multiple modalities to enhance the compression and classification of data in context of mHealth application. The core of the proposed method is based on the classic autoencoder which has been originally designed for encodingdecoding data. For each modality presented, we dedicate a stacked autoencoder to extract high level abstraction of the data by modeling the intra-correlation. A joint layer is added on top of each encoding part of the stacked autoencoders to model data intercorrelation. We have conducted compression and classification experiments. Comparison with DWT and CS have shown that our approach performs better with high compression ratio. We have also demonstrated the effectiveness of the multimodal approach for classification of EEG and EMG. Comparison with some unimodal algorithms e.g. Deep Botzmann Machines and stacked autoencoders shows that the multimodal autoencoder leads to better classification accuracy. In future work, we will investigate the possible application of Convolutional Neural Network. Furthermore, we intend to\nmake the autoencoder-based compression scheme adaptive by including the network resource in the choice of the neural network architecture."}, {"heading": "ACKNOWLEDGMENT", "text": "This publication was made possible by NPRP grant #7-684- 1-127 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors."}], "references": [{"title": "A two-dimensional approach for lossless eeg compression", "author": ["K. Srinivasan", "J. Dauwels", "M.R. Reddy"], "venue": "Biomedical Signal Processing and Control, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A new, fast, and efficient image codec based on set partitioning in hierarchical trees", "author": ["A. Said", "W.A. Pearlman"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Scalable real-time energy-efficient eeg compression scheme for wireless body area sensor network", "author": ["R. Hussein", "A. Mohamed", "M. Alghoniemy"], "venue": "Biomedical Signal Processing and Control, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Row-sparse blind compressed sensing for reconstructing multi-channel eeg signals", "author": ["A. Shukla", "A. Majumdar"], "venue": "Biomedical Signal Processing and Control, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Bregman iterative algorithms for L1-minimization with applications to compressed sensing", "author": ["W. Yin", "S. Osher", "D. Goldfarb", "J. Darbon"], "venue": "SIAM Journal on Imaging Sciences, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "A low-rank matrix recovery approach for energy efficient eeg acquisition for a wireless body area network", "author": ["A. Majumdar", "A. Gogna", "R. Ward"], "venue": "Sensors, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Examining associations between fmri and eeg data using correlation analysis", "author": ["N. Correa", "Y.-O. Li", "T. Adali", "V.D. Calhoun"], "venue": "In proceeding of 5th IEEE international symposium on biomedical imaging: From nano to macro,, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning representations for multimodal data with deep belief nets", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep belief networks. [Online]. Available: www. scholarpedia.org/article/Deep belief networks", "author": ["G.E. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep boltzman machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Multimodal video classification with stacked contractive autoencoders", "author": ["Y. Liu", "X. Feng", "Z. Zhou"], "venue": "Signal Processing, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality ofdata with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Auto-encoders: reconstruction versus compression", "author": ["Y. Ollivier"], "venue": "2014, working paper or preprint. [Online]. Available: https://hal. archives-ouvertes.fr/hal-01104268", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Using autoencoders for mammogram compression", "author": ["C.C. Tan", "C. Eswaran"], "venue": "Journal of Medical Systems, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Lightweight lossy compression of biometric patterns via denoising autoencoders", "author": ["D.D. Testa", "M. Rossi"], "venue": "IEEE Signal Processing Letters, vol. 22, pp. 2304\u20132308, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Inproceedings of Advances in Neural Information Processing Systems, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "International Conference on Machine Learning, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning, vol. 2, pp. 1\u2013127, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "User-centric network selection in multi-rat systems", "author": ["A. Awad", "A. Mohamed", "C.-F. Chiasserini"], "venue": "2016 IEEE Wireless Communications and Networking Conference Workshops (WCNCW), April 2016, pp. 97\u2013 102.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Deap: A database for emotion analysis ;using physiological signals", "author": ["S. Koelstra", "C. Muhl", "M. Soleymani", "J.-S. Lee", "A. Yazdani", "T. Ebrahimi", "T. Pun", "A. Nijholt", "I. Patras"], "venue": "IEEE Transactions on Affective Computing, vol. 3, pp. 18\u201331, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyvarinen", "E. Oja"], "venue": "Neural Networks, vol. 13, no. 45, pp. 411\u2013430, 2000.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "[4] designed a 2-D lossless EEG compression where the signal is arranged in 2-D matrix as a preprocessing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Compression is achieved through a two-stage coder composed of Set Partitioning In Hierarchical Trees (SPIHT) [5] layer and Arithmetic Coding (RC) layer.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "[6] proposed a scalable and energy efficient EEG compression scheme based on Discret Wavelet Transform (DWT) and Compressive Sensing (CS) in wireless sensors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "In [7], authors applied CS technique for EEG signal compression.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Thus, the recovery problem becomes row-sparse and solved through Bregman algorithm [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "[9] argued that CS is not efficient for EEG compression because there is no sparsifying basis that fulfills the requirements of incoherence and sparsity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[11] proposed a multimodal deep ar X iv :1 70 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[12] built a multimodal deep belief network [13] to learn multimodal representation from image and text data for image annotation and retrieval tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] built a multimodal deep belief network [13] to learn multimodal representation from image and text data for image annotation and retrieval tasks.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "In [14], authors designed a deep Boltzmann machine [15] based architecture to extract a meaningful representation from multimodal data for classification and information re-", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "In [14], authors designed a deep Boltzmann machine [15] based architecture to extract a meaningful representation from multimodal data for classification and information re-", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "[16] proposed a multimodal autoencoder [17] approach for video classification based on audio, image and text data where the intra-modality semantic for each data is separately learning by a stacked autoencoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] proposed a multimodal autoencoder [17] approach for video classification based on audio, image and text data where the intra-modality semantic for each data is separately learning by a stacked autoencoder.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "In [18], Yann Ollivier proved that there is a strong relationship between minimizing the code length of the data and minimizing reconstruction error that an autoencoder seeks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[19] used a stacked autoencoder for mammogram image compression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [20], authors applied the autoencoder for Electrocardiogram (ECG) compression.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "SAE is trained via a greedy layer-wise training [21].", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "This strategy, inspired from Nigiam et al [11], follows the denoising autoencoder paradigm [22] and is justified by twofold:", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "This strategy, inspired from Nigiam et al [11], follows the denoising autoencoder paradigm [22] and is justified by twofold:", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "The compressed data can be used for classification task, that is, to fine-tune the layers with respect to a supervised criterion by plugging the bottleneck layer to a softmax classifier [23]:", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "These data are collected by a PDA and should be transmitted to a remote server handled by a medical entity [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "We conduct our experiments on the DEAP dataset [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "We compare our compression method with the Discrete Wavelet Transform (DWT) [26], Compressed Sensing (CS) [27] and the 2D compression approach which is based on SPIHT and FastICA [28].", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "We compare our compression method with the Discrete Wavelet Transform (DWT) [26], Compressed Sensing (CS) [27] and the 2D compression approach which is based on SPIHT and FastICA [28].", "startOffset": 179, "endOffset": 183}, {"referenceID": 21, "context": "We follow the same approach as in [25]: video ratings are thresholded", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "We compare our approach with twolayer SAE and Deep Boltzmann Machine (DBM) architectures [15] with the softmax classifier on top of them.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.", "creator": "LaTeX with hyperref package"}}}