{"id": "1703.09783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Two-Stream RNN/CNN for Action Recognition in 3D Videos", "abstract": "The recognition of actions from video sequences has many applications in health monitoring, assisted living, surveillance, and smart homes. Despite advances in sensing, in particular related to 3D video, the methodologies to process the data are still subject to research. We demonstrate superior results by a system which combines recurrent neural networks with convolutional neural networks in a voting approach. The gated-recurrent-unit-based neural networks are particularly well-suited to distinguish actions based on long-term information from optical tracking data; the 3D-CNNs focus more on detailed, recent information from video data. The resulting features are merged in an SVM which then classifies the movement. In this architecture, our method improves recognition rates of state-of-the-art methods by 14% on standard data sets. We show that this allows for real-time, real-time neural networks to be able to detect the movements of people while still providing complete accurate results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 22 Mar 2017 22:29:56 GMT  (4926kb)", "http://arxiv.org/abs/1703.09783v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rui zhao", "haider ali", "patrick van der smagt"], "accepted": false, "id": "1703.09783"}, "pdf": {"name": "1703.09783.pdf", "metadata": {"source": "CRF", "title": "Two-Stream RNN/CNN for Action Recognition in 3D Videos", "authors": ["Rui Zhao", "Haider Ali", "Patrick van der Smagt"], "emails": ["Haider.Ali}@dlr.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n09 78\n3v 1\n[ cs\n.C V\n] 2\n2 M\nar 2\n01 7\nI. INTRODUCTION\nRecognition of human activity in 3D videos has received increasing attention since 2010 [1]\u2013[7]. Compared to 2D videos, 3D videos provide more spatial information and could be more informative. Action recognition with 3D videos is applied in different fields, such as health monitoring for patients, assisted living for disabled people, and robot perception and cognition.\nFollowing this line of research, this paper proposes and applies novel deep-learning methods on what is currently the largest 3D action recognition dataset. Our results are compared with existing best approaches and are shown to be superior. Our proposed deep-learning methods consist mainly of three parts: a novel skeleton-based recurrent neural network structure, using a 3D-convolutional [8] neural network for RGB videos, and sketching a new two-stream fusion method to combine RNN and CNN. All methods are evaluated on the NTU RGB+D Dataset [2]. The dataset was published in 2016 and contains more than 56k action samples in four different modalities: RGB videos, depth map sequences, 3D skeletal data, and infrared videos. The dataset consists of 60 different action classes including daily, healthrelated, and mutual actions. In this paper, we use both the 3D skeletal data and RGB videos.\nTraditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data. Focussing on the latter, for skeleton-based activity analysis, [2]\u2013[5] used different kinds of recurrent neural networks to acquire state-of-the-art performances on various of 3D action datasets. Du et al. [3] propose an hierarchical RNN, which is\nRZ and HA are with the Robotics and Mechatronics Center (RMC), German Aerospace Center (DLR) {Rui.Zhao, Haider.Ali}@dlr.de RZ and PvdS are with the Faculty for Informatics, Technische Universita\u0308t Mu\u0308nchen. PvdS is also with fortiss, TUM Associate Institute.\nfed with manually divided five groups of the human skeleton, such as two hands, two legs, and one torso. Inspired from this, Shahroudy et al. [2] present a novel long short-term memory (LSTM) [55] cell, called part-aware LSTM, which is also fed with separated five parts of skeleton. Evolved from these two ideas, Zhu et al. [5] provide a novel deep RNN structure, which can automatically learn the co-occurrence, similar to grouping data into five human body parts, from skeleton data. Most recently, Liu et al. [4] propose a skeleton tree traversal algorithm and a new gating mechanism to improve robustness against noise and occlusion.\nHowever, our proposed RNN structure makes a different contribution. Our method is inspired by recent normalization technologies [56] and a novel recurrent neuron mechanism [57]. With these advanced deep learning technologies embedded into our RNN structure, it can be trained with 13 times fewer iterations and for each iteration consumes 20% less computational time, compared to a normal RNN model with LSTM cells. Our contribution focuses more on making the network much easier to train, less inclined to overfitting, and deep enough to represent the data. More importantly, our proposed RNN structure outperforms all other skeletonbased methods on the largest 3D action recognition dataset.\nTo process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60]. We use a 3D-CNN [8] model on the RGB videos of the NTU RGB+D dataset. We compare the results with proposed RNN models, and fuse their output.\nTo combine the RNN and CNN models, we propose two fusion structures: decision and feature fusion. The first is very simple to use, whereas the second provides a better performance. For decision fusion, we illustrate a voting method based on the confidence of the classifiers. For feature fusion, we propose a novel two-stream RNN/CNN structure, shown in Fig. 4, which combines temporal and spatial features from the RNN and CNN models and boost the performance by a significant margin. Our two-stream RNN/CNN structure outperforms the current state-of-the-art method [4] more than 14% on both cross subject and cross view settings.\nTo summarize, our contributions in this paper are:\n\u2022 A novel RNN structure is proposed, which converges 13 times faster during training and costs 20% less\ncomputational power at each forward pass, compared to a normal LSTM; \u2022 Two fusion methods, i.e. decision fusion and feature fusion, are proposed to combine the proposed RNN\nstructure and a 3D-CNN structure [8]. The decision fusion is easier to use, while the feature fusion has superior performance."}, {"heading": "II. METHODOLOGY", "text": "In this section, we first introduce the concept of recurrent neural networks and batch normalization, and then describe the proposed RNN structure: a deep bidirectional gated recurrent neural network with batch normalization and dropout. Afterwards, the applied 3D-CNN model and twostream RNN/CNN fusion architectures, i.e., decision fusion and feature fusion, are introduced.\nA. Recurrent Neural Network\n1) Vanilla Recurrent Neural Network: Recurrent neural networks can handle sequence information with varied lengths of time steps. This transforms the input X to a internal hidden state ht at each time step. The network passes the state ht along with the next input xt+1 to the neuron, time step after time step. The neuron learns when to remember and forget information with nonlinear activation functions:\nht = \u03c3\n( W ( xt\nht\u22121\n)) (1)\nyt = \u03c3 (Vht) (2)\nwhere t \u2208 {1, . . . , T } represents time steps, and \u03c3 represents a nonlinear activation function such as a standard logistic sigmoid function sigm(x) = 1/(1 + e\u2212x) or a hyperbolic tangent function tanh(x). Multiple layers of RNN can be stacked to increase the complexity:\nh (l) t = \u03c3\n( W(l) ( h (l\u22121) t\nh (l) t\u22121\n)) (3)\nh (0) t := xt (4)\nyt = \u03c3 ( Vh (L) t ) (5)\nwhere l \u2208 {1, ..., L} denotes the layer number. In practice, a vanilla RNN does not remember information over a longer time; a problem which is related to the vanishing gradient problem.\n2) Long Short-Term Memory: This problem can be solved by LSTM [55] which stores information in gated cells at the neurons. This allows errors to be backpropagated through hundreds or thousands of time steps:   i f\no c\u0302t\n  =  \nsigm sigm sigm tanh\n W ( xt\nht\u22121\n) (6)\nct = f \u2299 ct\u22121 + i\u2299 c\u0302t (7)\nht = o\u2299 tanh(ct) (8)\nwhere i, f and o denote input gate, forget gate, and output gate, respectively. c\u0302t represents new candidate values, which could be added to the cell state ct. We use \u2299 for elementwise multiplication.\n3) Gated Recurrent Unit: An improvement to LSTM called gated recurrent unit (GRU) was proposed in [57]. GRU has a simpler structure and can be computed faster. The three gates from LSTM are combined into two gates, respectively updating gate z and resetting gate r in GRU. GRU also combines cell state ct and hidden state ht into one state ht. The mathematical description is as follows: (\nz r\n) = ( sigm sigm )( W ( xt\nht\u22121\n)) (9)\nh\u0302t = tanh\n( W ( xt\nr \u2299 ht\u22121\n)) (10)\nht = z\u2299 ht\u22121 + (1\u2212 z)\u2299 h\u0302t (11)\nwhere h\u0302t denotes new candidate state values.\n4) Bidirectional Recurrent Neural Network: A bidirectional RNN [61] performs a forward pass and a backward pass, which runs input data from t = T to t = 1 and from t = 1 to t = T , respectively. For classification, the output of an RNN yt can be passed to a fully-connected layer with softmax activation functions; this allows us to interpret the output as a probability.\n5) Batch Normalization: To train a deep neural network, the internal covariate shift [56] slows down the training process. The internal covariate shift is the distribution of each layer\u2019s input changes during training, because the parameters in the previous layer are changing. To reduce the internal covariate shift, we could whiten the layer activations, but this takes too much computation power. Batch normalization, a part of the neural network structure, approximates this process by standardizing the activations x using a statistical estimate of the mean E\u0302(x) and standard deviation V\u0302ar(x) for each training mini-batch. It can be shown that\nBN(x; \u03b3, \u03b2) = \u03b3 x\u2212 E\u0302(x)\u221a V\u0302ar(x) + \u01eb + \u03b2 (12)\nwhere \u03b3 \u2208 Rd and \u03b2 \u2208 Rd are scale and shift parameters for the activation x \u2208 Rn\u00d7d. With these, identity transformation for each activation could be presented. \u01eb \u2208 R is a constant added as a regularization parameter for numerical stability. The division in Eq. (12) is performed element-wise. \u03b3 and \u03b2 are learned during training and fixed during inference.\n6) Proposed RNN Structure: For skeleton-based action recognition tasks, the data set consists of the 3D coordinates of a number of body joints. We feed this information, together with action labels, to an RNN. This RNN network has two bidirectional layers, each of which consists of 300 GRU cells. After the recurrent layers follows the batch normalization layer, which standardizes the activations from the RNN layer. Then the normalized activations flow to the next fully-connected layer with 600 rectified linear unit (ReLU) [62] activation functions. During training, in each iteration the network randomly drops out 25% of the neurons between the batch normalization layer and the next fullyconnected layer to reduce overfitting. Lastly, a softmax layer\nmaps the compressed motion information (features) to 60 action classes. Fig. 1 shows the structure of this RNN network.\nTo highlight the improvements of this final proposed model, we compare our approach to simpler models. These models are a standard RNN; an LSTM-RNN; LSTM plus batch normalization (\u201cLSTM-BN\u201d), LSTM-BN with dropout (\u201cLSTM-BN-DP\u201d), GRU-BN-DP, and a bidirectional GRUBN-DP which we call \u201cBI-GRU-BN-DP\u201d. All these models have one recurrent layer. The next complexity is adding an extra layer of hidden units to the last model (\u201c2 layer BI-GRU-BN-DP\u201d). Finally, we add another fully-connected layer on top, before the softmax layer, and call this model \u201c2 layer BI-GRU-BN-DP-H\u201d. Sec. III discusses the results of all models.\nB. Convolutional Neural Network\nTo process RGB videos, we choose to use the 3D-CNN model from [8], as it shows promising performances on 2D video action recognition tasks. We believe that 3D convolution nets are more suitable for learning features from videos than 2D convolution nets.\n2D convolution generates a series of 2D feature maps from images. Inspired by this, a 3D convolution processes frame clips, where the third dimension is time step, which results in a series of 3D feature volumes, as shown in Fig. 2. This compressed representation contains spatiotemporal information from the video clips. To learn a rich amount of features, multiple layers of convolution and max-pooling operations are stacked into one model.\nTo be specific, the 3D-CNN model [8], which we choose,\nhas five convolutional groups, each group has one or two convolutional layers and one max-pooling layer, two fullyconnected layers, and one softmax output layer. The details of this model are presented in Fig. 3.\nWe finetune this model with pretrained parameters [8] on Sports-1M Dataset, which has approximately one million YouTube videos. This reduces overfitting and demands less training time on the current dataset.\nC. Two-stream RNN/CNN\nAs having the proposed RNN structure for the skeleton data and the 3D-CNN model for the RGB videos, we want to combine the strengths of RNN and CNN nets. To improve the performance, we propose two fusion models, decision fusion and feature fusion.\n1) Decision Fusion: In the case of decision fusion, we use a simple but efficient voting method, inspired by majority voting. As a result of having only two classifiers, we cannot apply majority voting. Instead, the fusion method predicts based on voting confidence.\nWe first split the dataset into training, validation and testing. The same training set is used to train the RNN and CNN nets. The validation set is then used to find the best parameters, trust weights wr and wc for the voting method. We initialize the trust weights with equal values, which means wr = 1.00 and wc = 1.00 for both RNN and CNN classifiers. Afterwards, we compare the confidences, which are the highest probabilities of softmax output from both classifiers for each prediction. The more confident one wins:\ny(xi) =\n{ yr(xi), if wr \u00d7 yr(xi) > wc \u00d7 yc(xi)\nyc(xi), otherwise (13)\nwhere y(xi) is the fused prediction for sample xi; yr and yc denote RNN and CNN prediction, respectively.\nBased on this concept, we develop a way to fuse the predictions from RNN and CNN. We evaluate the performance with the validation dataset and search for the best trust weights for decision fusion. Having only two parameters wr and wc, only little tuning is needed.\n2) Feature Fusion: Another way to combine these two\nneural networks is feature fusion.\nWe first train the RNN and CNN models on the training dataset. As in training, neural nets can learn discriminant information from raw data. Thus, we use the trained RNN model to extract temporal features from 3D skeleton data\nand use the trained CNN model to learn spatiotemporal features from RGB videos. Both features come from the first fully-connected layer in each model. The features are concatenated, L2 normalized, and eventually, fed to a linear SVM classifier.\nThe SVM parameter C is found using the validation dataset. Then the model is tested on the test set. The feature fusion structure for two streams of RNN and CNN features is presented in Fig. 4."}, {"heading": "III. EXPERIMENTS", "text": "The models introduced in the previous section are evaluated in the experiments. The dataset is first introduced in this section, then the setups and parameter settings for the experiments are illustrated. We compare the results of the proposed models with the current best methods. In the end, we analyze and discuss the problems related to deep learning methods for 3D action recognition.\nA. NTU RGB+D Dataset [2]\nThe proposed approaches are evaluated on the NTU RGB+D dataset [2], which we know as the current largest publicly available 3D action recognition dataset. The dataset consists of more than 56k action videos and 4 million frames, which were collected by 3 Kinect V2 cameras from 40 distinct subjects, and divided into 60 different action classes including 40 daily (drinking, eating, reading, etc.), 9 healthrelated (sneezing, staggering, falling down, etc.), and 11 mutual (punching, kicking, hugging, etc.) actions. It has four major data modalities provided by the Kinect sensor: 3D coordinates of 25 joints for each person (skeleton), RGB frames, depth maps, and IR sequences. In this paper, we use the first two modalities, since they are the two most informative modalities.\nThe large intra-class and view point variations make this dataset challenging. However, the large amount of action samples makes it highly suitable for data-driven methods.\nThis dataset has two standard evaluation criteria [2]. The first one is a cross-subject test, in which half of the subjects are used for training and the other half are used for testing. The second one is a cross-view test, in which two viewpoints are used for training and one is excluded for evaluation.\nB. Implementation details\nIn our experiments, the implementation consists of RNN, CNN, and Fusion. For all these models we use the same training, validation and testing splits. The validation set is composed of 10% of the subjects in the training set in [2]. The remaining subjects in the training set [2] make up the training set.\n1) RNN Implementation: In the RNN experiments, we have two human skeletons as input, each skeleton has 25 3D coordinates. Since the longest time step is 300, we pad all the action sequences to a length of 300. The dimension of each action sample is 300 (time steps)\u00d7 150 (coordinates). We use TensorFlow [63] with TFlearn [64] and run the experiments on either one NVIDIA GTX 1080 GPU or one NVIDIA GTX TITAN X GPU. We train the network using RMSprop [65] optimizer and set learning rate as 0.001, decay as 0.9, and momentum as 0. We train the network from scratch using mini-batches of 1000 sequences for one-layer models and use mini-batches of 650 sequences for two-layer models. For all RNN nets, we use 300 neurons for each single-directional layer, double the amount of neurons for bidirectional layers, and we use a 75% keep probability for dropout. For batch normalization, we initialize \u03b3 as 1.0, \u03b2 as 0.0, and set \u01eb as 1\u00d710\u22125. The estimated means and variances are fixed during inference.\nAs a comparison, the mentioned parameters are the same\nfor all proposed RNN models, only the structure changes.\n2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset. From RGB videos, we extract the frames, crop and resize them from 1920 \u00d7 1080 pixels to 320 \u00d7 240 pixels [8]. Videos are split into non-overlapped 16-frame clips.\nWe refer to the input of CNN model as a size of c\u00d7 t\u00d7 h\u00d7w, where c is the number of channels, t is the number of time steps, h and w are the height and width of the frame, respectively. The network takes video clips as input and predicts the 60 action labels which belong to the 60 different actions. It further resizes the input frames to 128\u00d7171pixel resolution. The input dimensions are 3\u00d716\u00d7128\u00d7171pixel. During training we use jittering on the input clips by random cropping them into 3\u00d716\u00d7112\u00d7112pixel. We finetune the network with stochastic gradient descent optimizer using mini-batches of 44 clips, with initial learning rate of 0.0001. The learning rate is then reduced by half, when no training progress was observed [65]. The training stopped after around 20 epochs.\nFor video-based prediction, the model averages the predictions over all 16-frame clips split from the same video and provides the final prediction for the input video. A similar\nidea is applied for extracting features from fc-6 layer, which averages the 4096-dimensional feature vectors over all clips in the same video, resulting in one 4096-dimensional vector for each video.\n3) Fusion Implementation: We fuse the best RNN structure, 2 layer BI-GRU-BN-DP-H, with the 3D-CNN model, first using decision fusion, then using feature fusion.\nFor decision fusion, we first extract the softmax output, then search for the fusion parameters, trust weight wr and wc for RNN and CNN from the validation split. The parameters are wr = 1.00 and wc = 2.88 for the cross subject setup, and wr = 1.00 and wc = 3.02 for the cross view setup. For feature fusion, we extract the RNN features (600 dimensions) from the fully-connected layer, and extract CNN features (4096dimensions) from the fc-6 layer [8]. We then concatenate them into one feature array (4,696dimensions) and apply L2 normalization. In the end, we have normalized RNN/CNN features from training, validation, and testing splits. We use training and validation splits to find the optimal value of C for linear SVM [67] model. For both cross-subject and cross-view setups, we find that C = 8.0 gives the best validation accuracy. Among all the models in this paper, feature fusion model shows the best testing results. We refer to this model as a two-stream RNN/CNN structure as shown in Fig. 4.\nC. Experimental Results and Analysis\nThe evaluation results are shown in Tab. I. The first 16 rows are skeleton-based methods. The 3D-CNN model (17th row) uses RGB videos as input. The decision fusion (18th row) and feature fusion (19th row) models use the best RNN structure, which is the 2 Layer BI-GRU-BN-DP-H (16th row), and the 3D-CNN (17th row) model.\nTab. I shows that our RNN structure, the 1 Layer LSTMBN, already outperforms the baseline method part-aware LSTM reported in [2] because batch normalization improves the LSTM model. Adding a dropout procedure reduces overfitting and further improves the results (rows 11, 12). From rows 12 and 13 we can see that the performances of LSTM and GRU cells are similar [68]. GRU is better in the cross-subject test and LSTM is better in the crossview test. On the other hand, GRU is faster than LSTM both in computational speed and converge rate. As presented in Fig. 5 left, for 1k training steps, the same model performs 5.42% more accurately and takes 20% less computational time when using GRU cells than when using LSTM cells.\nThe addition of the extra fully-connected layer brings another significant improvement. This increases the complexity of the neural network, which helps the model capture more inherent features from the 3D skeleton data [69]. The recurrent layers before the fully-connected layer can be seen as a temporal feature extractor, which compact input information (dimension 300 \u00d7 150) into 600 dimensions. The latter part of the RNN structure can be considered as a classifier learning to map these 600-dimensional features to 60 different action categories. Altogether, our novel RNN model, 2 Layer BI-GRU-BN-DP-H, outperforms all the other\nskeleton-based models including ST-LSTM (Tree traversal) + Trust Gate [4].\nThen, we use the RGB video data to train the 3D-CNN model. We use the voting method based on confidence to fuse the 2 Layer BI-GRU-BN-DP-H and 3D-CNN model. In the next step, we utilize a linear SVM [8] to fuse the fc-6 features from the CNN and the fc features from the RNN. This further improves results by over 13% in comparison to our best RNN, and by more than 14% compared to literature models [2], [4]. This boosting is due to the features from RNN and CNN model being highly complementary. The RNN model uses 50 3D coordinates for two human bodies over 300 time steps, and learns to find the long-term motion pattern. Whereas the CNN model has 2D RGB frames, which additionally has spatiotemporal information about objects, such as cups, pens, and books. However, the CNN model can\nonly memorize information for 16 time steps long\u2014longer memorization is prohibited by GPU memory limitations. These facts make the features from RNN and CNN model highly complementary, as the testing results show in row 16, 17, and 19 in Tab. I.\nD. Discussion\nTo better analyze and improve the performance of the model, we take a closer look at actions that are highly confusing to the two-stream RNN/CNN structure. As presented in Fig. 6 and 7, such action pairs include reading vs. writing, putting on a shoe vs. taking off a shoe, and rubbing two hands vs. clapping. These actions are shown in a video at https://www.youtube.com/watch?v=G0PXKCEgIoA. Fig. 8 shows some classified action samples.\nThere could be several reasons for this observation. First, these actions are sometimes inherent confusing. Secondly, there are flaws in the data. Kinect depth information, from which the NTU skeleton data is created, is quite noisy [70], [71]. Correspondingly, the 3D skeleton data used in our RNNs are also quite noisy [4]. RGB videos data are more accurate and stable, but single frames carry no 3D information. Thirdly, the 3D-CNN model [8] is trained with small video clips, which are 16 time steps long. The CNN model is adapted to find only short-term temporal features in these clips. As GPU memory and computing power increase, the model could also be adapted to find long-term temporal features in each whole video. Lastly, although, the RNN model can memorize the whole action sequences and give final predictions, it has no information about the appearances\nand movements of surrounding objects, which could be discriminative information for the classification task."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose a novel RNN structure for 3D skeletons that achieves state-of-the-art performance on the largest 3D action recognition dataset. The proposed RNN model can also be trained 13 times faster and saves 20% computational power on each training step. Additionally, the RGB videos from the same dataset are used to finetune a 3DCNN model. In the end, an efficient fusion structure, twostream RNN/CNN, is introduced to fuse the capabilities of both RNN and CNN models. The results of this method are 13% higher than using the proposed RNN alone, and 14% higher than the best published result in the literature. In the future, we want to consider using the other sensor modalities such as depth maps and IR sequences and see what is the best architecture to fuse all these modalities."}], "references": [{"title": "Action recognition based on a bag of 3D points", "author": ["W. Li", "Z. Zhang", "Z. Liu"], "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops. IEEE, 2010, pp. 9\u201314.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "NTU RGB+D: A large scale dataset for 3D human activity analysis", "author": ["A. Shahroudy", "J. Liu", "T.-T. Ng", "G. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical recurrent neural network for skeleton based action recognition", "author": ["Y. Du", "W. Wang", "L. Wang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1110\u20131118. Fig. 8: Some correctly classified action samples (first four frames with green predictions) and some mis-classified action samples (last two frames with red predictions). These samples are randomly picked from the feature fusion model cross view testing results.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatio-temporal LSTM with trust gates for 3d human action recognition", "author": ["J. Liu", "A. Shahroudy", "D. Xu", "G. Wang"], "venue": "arXiv preprint arXiv:1607.07043, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Cooccurrence feature learning for skeleton based action recognition using regularized deep lstm networks", "author": ["W. Zhu", "C. Lan", "J. Xing", "W. Zeng", "Y. Li", "L. Shen", "X. Xie"], "venue": "arXiv preprint arXiv:1603.07772, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "3d human activity recognition with reconfigurable convolutional neural networks", "author": ["K. Wang", "X. Wang", "L. Lin", "M. Wang", "W. Zuo"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 97\u2013106.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Action recognition from depth maps using deep convolutional neural networks", "author": ["P. Wang", "W. Li", "Z. Gao", "J. Zhang", "C. Tang", "P.O. Ogunbona"], "venue": "2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatiotemporal features with 3D convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 4489\u20134497.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Skeletal quads: Human action recognition using joint quadruples", "author": ["G. Evangelidis", "G. Singh", "R. Horaud"], "venue": "International Conference on Pattern Recognition, 2014, pp. 4513\u20134518.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Human action recognition by representing 3D skeletons as points in a lie group", "author": ["R. Vemulapalli", "F. Arrate", "R. Chellappa"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 588\u2013595.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Jointly learning heterogeneous features for RGB-D activity recognition", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "J. Zhang"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5344\u20135352.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Histogram of oriented principal components for cross-view action recognition", "author": ["H. Rahmani", "A. Mahmood", "D. Huynh", "A. Mian"], "venue": "2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Human activity recognition process using 3-d posture data", "author": ["S. Gaglio", "G.L. Re", "M. Morana"], "venue": "IEEE Transactions on Human- Machine Systems, vol. 45, no. 5, pp. 586\u2013597, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Real-time human action recognition based on depth motion maps", "author": ["C. Chen", "K. Liu", "N. Kehtarnavaz"], "venue": "Journal of real-time image processing, pp. 1\u20139, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Rgbd-hudaact: A color-depth video database for human daily activity recognition", "author": ["B. Ni", "G. Wang", "P. Moulin"], "venue": "Consumer Depth Cameras for Computer Vision. Springer, 2013, pp. 193\u2013208.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Unstructured human activity detection from rgbd images", "author": ["J. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "Robotics and Automation  (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 842\u2013849.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining actionlet ensemble for action recognition with depth cameras", "author": ["J. Wang", "Z. Liu", "Y. Wu", "J. Yuan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 1290\u20131297.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "View invariant human action recognition using histograms of 3d joints", "author": ["L. Xia", "C.-C. Chen", "J. Aggarwal"], "venue": "2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 2012, pp. 20\u201327.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic feature selection for online action recognition", "author": ["V. Bloom", "V. Argyriou", "D. Makris"], "venue": "International Workshop on Human Behavior Understanding. Springer, 2013, pp. 64\u201376.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Human action recognition and retrieval using sole depth information", "author": ["Y.-C. Lin", "M.-C. Hu", "W.-H. Cheng", "Y.-H. Hsieh", "H.-M. Chen"], "venue": "Proceedings of the 20th ACM international conference on Multimedia. ACM, 2012, pp. 1053\u20131056.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Privacy preserving automatic fall detection for elderly using rgbd cameras", "author": ["C. Zhang", "Y. Tian", "E. Capezuti"], "venue": "International Conference on Computers for Handicapped Persons. Springer, 2012, pp. 625\u2013 633.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences", "author": ["O. Oreifej", "Z. Liu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 716\u2013723.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning human activities and object affordances from rgb-d videos", "author": ["H.S. Koppula", "R. Gupta", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 32, no. 8, pp. 951\u2013970, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A decision forest based feature selection framework for action recognition from rgb-depth cameras", "author": ["F. Negin", "F. \u00d6zdemir", "C.B. Akg\u00fcl", "K.A. Y\u00fcksel", "A. Er\u00e7il"], "venue": "International Conference Image Analysis and Recognition. Springer, 2013, pp. 648\u2013657.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Concurrent action detection with structural prediction", "author": ["P. Wei", "N. Zheng", "Y. Zhao", "S.-C. Zhu"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3136\u20133143.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "An evaluation of 3d motion flow and 3d pose estimation for human action recognition", "author": ["M. Munaro", "S. Michieletto", "E. Menegatti"], "venue": "RSS Workshops: RGB-D: Advanced Reasoning with Depth Cameras, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring the trade-off between accuracy and observational latency in action recognition", "author": ["C. Ellis", "S.Z. Masood", "M.F. Tappen", "J.J. Laviola Jr", "R. Sukthankar"], "venue": "International Journal of Computer Vision, vol. 101, no. 3, pp. 420\u2013436, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Inverse dynamics for action recognition", "author": ["A. Mansur", "Y. Makihara", "Y. Yagi"], "venue": "IEEE transactions on cybernetics, vol. 43, no. 4, pp. 1226\u20131236, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Rgb-depth feature for 3d human activity recognition", "author": ["Z. Yang", "L. Zicheng", "C. Hong"], "venue": "China Communications, vol. 10, no. 7, pp. 93\u2013 103, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Recognition of human actions from rgb-d videos using a reject option", "author": ["V. Carletti", "P. Foggia", "G. Percannella", "A. Saggese", "M. Vento"], "venue": "International Conference on Image Analysis and Processing. Springer, 2013, pp. 436\u2013445.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Gait-based gender recognition using pose information for real time applications", "author": ["D. Kastaniotis", "I. Theodorakopoulos", "G. Economou", "S. Fotopoulos"], "venue": "Digital Signal Processing (DSP), 2013 18th International Conference on. IEEE, 2013, pp. 1\u20136.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Coupled hidden conditional random fields for rgb-d human action recognition", "author": ["A.-A. Liu", "W.-Z. Nie", "Y.-T. Su", "L. Ma", "T. Hao", "Z.-X. Yang"], "venue": "Signal Processing, vol. 112, pp. 74\u201382, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequential maxmargin event detectors", "author": ["D. Huang", "S. Yao", "Y. Wang", "F. De La Torre"], "venue": "European conference on computer vision. Springer, 2014, pp. 410\u2013424.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative hierarchical modeling of spatio-temporally composable human activities", "author": ["I. Lillo", "A. Soto", "J. Carlos Niebles"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 812\u2013819.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative orderlet mining for realtime recognition of human-object interaction", "author": ["G. Yu", "Z. Liu", "J. Yuan"], "venue": "Asian Conference on Computer Vision. Springer, 2014, pp. 50\u201365.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Watch-n-patch: Unsupervised understanding of actions and relations", "author": ["C. Wu", "J. Zhang", "S. Savarese", "A. Saxena"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4362\u20134370.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Exemplarbased recognition of human\u2013object interactions", "author": ["J.-F. Hu", "W.-S. Zheng", "J. Lai", "S. Gong", "T. Xiang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 26, no. 4, pp. 647\u2013660, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor", "author": ["C. Chen", "R. Jafari", "N. Kehtarnavaz"], "venue": "Image Processing (ICIP), 2015 IEEE International Conference on. IEEE, 2015, pp. 168\u2013172.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Human daily action analysis with multi-view and color-depth data", "author": ["Z. Cheng", "L. Qin", "Y. Ye", "Q. Huang", "Q. Tian"], "venue": "European Conference on Computer Vision. Springer, 2012, pp. 52\u201361.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "A viewpointindependent statistical method for fall detection", "author": ["Z. Zhang", "W. Liu", "V. Metsis", "V. Athitsos"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3626\u20133630.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Berkeley mhad: A comprehensive multimodal human action database", "author": ["F. Ofli", "R. Chaudhry", "G. Kurillo", "R. Vidal", "R. Bajcsy"], "venue": "Applications of Computer Vision (WACV), 2013 IEEE Workshop on. IEEE, 2013, pp. 53\u201360.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonintrusive human activity monitoring in a smart home environment", "author": ["S.M. Amiri", "M.T. Pourazad", "P. Nasiopoulos", "V.C. Leung"], "venue": "e-Health Networking, Applications & Services (Healthcom), 2013 IEEE 15th International Conference on. IEEE, 2013, pp. 606\u2013610.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling 4d humanobject interactions for event and object recognition", "author": ["P. Wei", "Y. Zhao", "N. Zheng", "S.-C. Zhu"], "venue": "2013 IEEE International Conference on Computer Vision. IEEE, 2013, pp. 3272\u2013 3279.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-view action modeling, learning and recognition", "author": ["J. Wang", "X. Nie", "Y. Xia", "Y. Wu", "S.-C. Zhu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2649\u20132656.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Hopc: Histogram of oriented principal components of 3d pointclouds for action recognition", "author": ["H. Rahmani", "A. Mahmood", "D.Q. Huynh", "A. Mian"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 742\u2013757.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Multipe/single-view human action recognition via part-induced multitask structural learning", "author": ["A.-A. Liu", "Y.-T. Su", "P.-P. Jia", "Z. Gao", "T. Hao", "Z.-X. Yang"], "venue": "IEEE transactions on cybernetics, vol. 45, no. 6, pp. 1194\u20131208, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Body surface context: A new robust feature for action recognition from depth videos", "author": ["Y. Song", "J. Tang", "F. Liu", "S. Yan"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 6, pp. 952\u2013964, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-person interaction detection using body-pose features and multiple instance learning", "author": ["K. Yun", "J. Honorio", "D. Chattopadhyay", "T.L. Berg", "D. Samaras"], "venue": "2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 2012, pp. 28\u201335.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient interaction recognition through positive action representation", "author": ["T. Hu", "X. Zhu", "W. Guo", "K. Su"], "venue": "Mathematical Problems in Engineering, vol. 2013, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation of video activity localizations integrating quality and quantity measurements", "author": ["C. Wolf", "E. Lombardi", "J. Mille", "O. Celiktutan", "M. Jiu", "E. Dogan", "G. Eren", "M. Baccouche", "E. Dellandr\u00e9a", "C.-E. Bichot"], "venue": "Computer Vision and Image Understanding, vol. 127, pp. 14\u201330, 2014.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "G3di: A gaming interaction dataset with a real time detection and evaluation framework", "author": ["V. Bloom", "V. Argyriou", "D. Makris"], "venue": "Workshop at the European Conference on Computer Vision. Springer, 2014, pp. 698\u2013712.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Dyadic interaction detection from pose and flow", "author": ["C. Van Gemeren", "R.T. Tan", "R. Poppe", "R.C. Veltkamp"], "venue": "International Workshop on Human Behavior Understanding. Springer, 2014, pp. 101\u2013115.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "A deep structured model with radius\u2013margin bound for 3d human activity recognition", "author": ["L. Lin", "K. Wang", "W. Zuo", "M. Wang", "J. Luo", "L. Zhang"], "venue": "International Journal of Computer Vision, pp. 1\u201318, 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring", "author": ["P. Wang", "W. Li", "Z. Gao", "C. Tang", "J. Zhang", "P. Ogunbona"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia. ACM, 2015, pp. 1119\u20131122.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep  network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2625\u20132634.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 568\u2013576.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 1, pp. 221\u2013231, 2013.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1997}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensorflow: Largescale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Tflearn", "author": ["A. Damien"], "venue": "https://github.com/tflearn/tflearn, 2016.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2014}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, vol. 2, no. 4, pp. 303\u2013314, 1989.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1989}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["J. Shotton", "T. Sharp", "A. Kipman", "A. Fitzgibbon", "M. Finocchio", "A. Blake", "M. Cook", "R. Moore"], "venue": "Communications of the ACM, vol. 56, no. 1, pp. 116\u2013124, 2013.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2013}, {"title": "Characterizations of noise in kinect depth images: a review", "author": ["T. Mallick", "P.P. Das", "A.K. Majumdar"], "venue": "IEEE Sensors Journal, vol. 14, no. 6, pp. 1731\u20131740, 2014.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recognition of human activity in 3D videos has received increasing attention since 2010 [1]\u2013[7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Recognition of human activity in 3D videos has received increasing attention since 2010 [1]\u2013[7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Our proposed deep-learning methods consist mainly of three parts: a novel skeleton-based recurrent neural network structure, using a 3D-convolutional [8] neural network for RGB videos, and sketching a new two-stream fusion method to combine RNN and CNN.", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "All methods are evaluated on the NTU RGB+D Dataset [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 81, "endOffset": 84}, {"referenceID": 51, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 158, "endOffset": 161}, {"referenceID": 52, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 163, "endOffset": 167}, {"referenceID": 53, "context": "Traditional studies on 3D action recognition use different kinds of methods [1], [9]\u2013[52] to compute handcrafted features, while deep-learning approaches [2]\u2013[7], [53], [54] are end-to-end trainable and can be applied directly on raw data.", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "Focussing on the latter, for skeleton-based activity analysis, [2]\u2013[5] used different kinds of recurrent neural networks to acquire state-of-the-art performances on various of 3D action datasets.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Focussing on the latter, for skeleton-based activity analysis, [2]\u2013[5] used different kinds of recurrent neural networks to acquire state-of-the-art performances on various of 3D action datasets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "[3] propose an hierarchical RNN, which is", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] present a novel long short-term memory (LSTM) [55] cell, called part-aware LSTM, which is also fed with separated five parts of skeleton.", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[2] present a novel long short-term memory (LSTM) [55] cell, called part-aware LSTM, which is also fed with separated five parts of skeleton.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "[5] provide a novel deep RNN structure, which can automatically learn the co-occurrence, similar to grouping data into five human body parts, from skeleton data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] propose a skeleton tree traversal algorithm and a new gating mechanism to improve robustness against noise and occlusion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 55, "context": "Our method is inspired by recent normalization technologies [56] and a novel recurrent neuron mechanism [57].", "startOffset": 60, "endOffset": 64}, {"referenceID": 56, "context": "Our method is inspired by recent normalization technologies [56] and a novel recurrent neuron mechanism [57].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 98, "endOffset": 101}, {"referenceID": 57, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 103, "endOffset": 107}, {"referenceID": 59, "context": "To process RGB videos, our method is inspired by different kinds of convolutional neural networks [8], [58]\u2013[60].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "We use a 3D-CNN [8] model on the RGB videos of the NTU RGB+D dataset.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Our two-stream RNN/CNN structure outperforms the current state-of-the-art method [4] more than 14% on both cross subject and cross view settings.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "decision fusion and feature fusion, are proposed to combine the proposed RNN structure and a 3D-CNN structure [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 54, "context": "2) Long Short-Term Memory: This problem can be solved by LSTM [55] which stores information in gated cells at the neurons.", "startOffset": 62, "endOffset": 66}, {"referenceID": 56, "context": "3) Gated Recurrent Unit: An improvement to LSTM called gated recurrent unit (GRU) was proposed in [57].", "startOffset": 98, "endOffset": 102}, {"referenceID": 60, "context": "4) Bidirectional Recurrent Neural Network: A bidirectional RNN [61] performs a forward pass and a backward pass, which runs input data from t = T to t = 1 and from t = 1 to t = T , respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 55, "context": "5) Batch Normalization: To train a deep neural network, the internal covariate shift [56] slows down the training process.", "startOffset": 85, "endOffset": 89}, {"referenceID": 61, "context": "Then the normalized activations flow to the next fully-connected layer with 600 rectified linear unit (ReLU) [62] activation functions.", "startOffset": 109, "endOffset": 113}, {"referenceID": 7, "context": "To process RGB videos, we choose to use the 3D-CNN model from [8], as it shows promising performances on 2D video action recognition tasks.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "To be specific, the 3D-CNN model [8], which we choose, Conv1 64 Pool 1 Conv2 128 Pool 2 Conv3 2x256 Pool 3 Conv4 2x512 Pool 4 Conv5 2x512 Pool 5 FC6 4096 FC7 4096 Output 60", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "We finetune this model with pretrained parameters [8] on Sports-1M Dataset, which has approximately one million YouTube videos.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "NTU RGB+D Dataset [2]", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "The proposed approaches are evaluated on the NTU RGB+D dataset [2], which we know as the current largest publicly available 3D action recognition dataset.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "This dataset has two standard evaluation criteria [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "The validation set is composed of 10% of the subjects in the training set in [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "The remaining subjects in the training set [2] make up the training set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 62, "context": "We use TensorFlow [63] with TFlearn [64] and run the experiments on either one NVIDIA GTX 1080 GPU or one NVIDIA GTX TITAN X GPU.", "startOffset": 18, "endOffset": 22}, {"referenceID": 63, "context": "We use TensorFlow [63] with TFlearn [64] and run the experiments on either one NVIDIA GTX 1080 GPU or one NVIDIA GTX TITAN X GPU.", "startOffset": 36, "endOffset": 40}, {"referenceID": 64, "context": "We train the network using RMSprop [65] optimizer and set learning rate as 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 47, "endOffset": 50}, {"referenceID": 65, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "2) CNN Implementation: We use the 3D-CNN model [8] in Caffe [66] and train it on RGB frames from the NTU RGB+D dataset, with pretrained parameters [8] from the Sport1M dataset.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "From RGB videos, we extract the frames, crop and resize them from 1920 \u00d7 1080 pixels to 320 \u00d7 240 pixels [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 64, "context": "The learning rate is then reduced by half, when no training progress was observed [65].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "For feature fusion, we extract the RNN features (600 dimensions) from the fully-connected layer, and extract CNN features (4096dimensions) from the fc-6 layer [8].", "startOffset": 159, "endOffset": 162}, {"referenceID": 66, "context": "We use training and validation splits to find the optimal value of C for linear SVM [67] model.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "I shows that our RNN structure, the 1 Layer LSTMBN, already outperforms the baseline method part-aware LSTM reported in [2] because batch normalization improves the LSTM model.", "startOffset": 120, "endOffset": 123}, {"referenceID": 67, "context": "From rows 12 and 13 we can see that the performances of LSTM and GRU cells are similar [68].", "startOffset": 87, "endOffset": 91}, {"referenceID": 68, "context": "This increases the complexity of the neural network, which helps the model capture more inherent features from the 3D skeleton data [69].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "Method cross subject cross view 01 Skeleton Quads [2], [9] 38.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Method cross subject cross view 01 Skeleton Quads [2], [9] 38.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "36% 02 Lie Group [2], [10] 50.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "36% 02 Lie Group [2], [10] 50.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "76% 03 FTP Dynamic Skeletons [2], [11] 60.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "76% 03 FTP Dynamic Skeletons [2], [11] 60.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "22% 04 HBRNN-L [2], [3] 59.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "22% 04 HBRNN-L [2], [3] 59.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "97% 05 Deep RNN [2] 56.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "09% 06 Deep LSTM [2] 60.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "29% 07 Part-aware LSTM [2] 62.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "27% 08 ST-LSTM (Tree) + Trust Gate [4] 69.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "23% 17 3D-CNN [8] 79.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "skeleton-based models including ST-LSTM (Tree traversal) + Trust Gate [4].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "In the next step, we utilize a linear SVM [8] to fuse the fc-6 features from the CNN and the fc features from the RNN.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "This further improves results by over 13% in comparison to our best RNN, and by more than 14% compared to literature models [2], [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "This further improves results by over 13% in comparison to our best RNN, and by more than 14% compared to literature models [2], [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 69, "context": "Kinect depth information, from which the NTU skeleton data is created, is quite noisy [70], [71].", "startOffset": 86, "endOffset": 90}, {"referenceID": 70, "context": "Kinect depth information, from which the NTU skeleton data is created, is quite noisy [70], [71].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Correspondingly, the 3D skeleton data used in our RNNs are also quite noisy [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Thirdly, the 3D-CNN model [8] is trained with small video clips, which are 16 time steps long.", "startOffset": 26, "endOffset": 29}], "year": 2017, "abstractText": "The recognition of actions from video sequences has many applications in health monitoring, assisted living, surveillance, and smart homes. Despite advances in sensing, in particular related to 3D video, the methodologies to process the data are still subject to research. We demonstrate superior results by a system which combines recurrent neural networks with convolutional neural networks in a voting approach. The gated-recurrent-unit-based neural networks are particularly well-suited to distinguish actions based on long-term information from optical tracking data; the 3D-CNNs focus more on detailed, recent information from video data. The resulting features are merged in an SVMwhich then classifies the movement. In this architecture, our method improves recognition rates of state-of-the-art methods by 14% on standard data sets.", "creator": "LaTeX with hyperref package"}}}