{"id": "1703.09784", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Perception Driven Texture Generation", "abstract": "This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 24 Mar 2017 01:25:30 GMT  (2611kb,D)", "http://arxiv.org/abs/1703.09784v1", "7 pages, 4 figures, icme2017"]], "COMMENTS": "7 pages, 4 figures, icme2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["yanhai gan", "huifang chi", "ying gao", "jun liu", "guoqiang zhong", "junyu dong"], "accepted": false, "id": "1703.09784"}, "pdf": {"name": "1703.09784.pdf", "metadata": {"source": "CRF", "title": "PERCEPTION DRIVEN TEXTURE GENERATION", "authors": ["Yanhai Gan", "Huifang Chi", "Ying Gao", "Jun Liu", "Guoqiang Zhong", "Junyu Dong"], "emails": ["gaoying}@stu.ouc.edu.cn,", "liujunqd@163.com,", "dongjunyu}@ouc.edu.cn"], "sections": [{"heading": null, "text": "Index Terms\u2014 Texture Generation, Perceptual Features, Adversarial Training, Regression, Neural Networks\n1. INTRODUCTION\nTextures play important roles in multimedia applications, such as understanding and generation of multimedia content. Texture synthesis and generation have also been extensively investigated in the past years [1]. Before the revival of deep learning, researchers mainly used example-based approaches to synthesize textures. In these methods, new textures with similar appearances to existing samples can be produced. With the development of deep learning, more methods for texture generation have been proposed by learning from the training data. However, there is no visual perceptual information involved in this process, whereas humans commonly use perceptual attributes, such as texture roughness, coarseness\nThanks to XYZ agency for funding.\nand directionality, to describe textures. Moreover, the majority of deep learning based methods can only generate images of low quality. Thus, it is desired to develop a new way for generating high-quality textures based on human perceptual descriptions; for example, the new generation method should be able to produce textures with strong directionality or less regularity as required by the user.\nConvolutional Neural Network (CNN), which was inspired by the mechanism of visual cortex, has shown great superiority in latest studies [2] [3]. With the aid of deep convolutional networks, researchers have made breakthroughs in many classical computer vision tasks. For example, in the ImageNet Large Scale Visual Recognition Challenge, the performance of computer algorithms even surpassed human\u2019s [4]. Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].\nGoodfellow et al. proposed a generative adversarial framework (GAN) [8] and produced excellent results in many image generation tasks. However, the generated samples were still in low resolution and far from being perfect. In order to generate more realistic images, Wang and Gupta factorized the image generation process and proposed a joint model consisting of Style and Structure Generative Adversarial Networks [9]. Experimental results in [9] suggested that a great gain could be obtained through this factoring trick for generating realistic indoor scenes. All these work indicates that it is a promising practice to exploit joint convolutional neural networks and adversarial training schemes for generating high-quality images.\nIn addition to generating natural images, another question is what we can generate from semantics or high-level descriptions. Many efforts have been made regarding this topic. Karpathy et al. proposed a fragment embedding method in 2014 [10], which was essentially a bidirectional retrieval scheme, as the desired image must exist in the image database. Yan et al. modeled images as composite of foreground and background and developed a layered generative model [11]. Their method shows promising results in the tasks of attribute-conditioned image reconstruction and completion. Nevertheless, the quality of generated images is still not good enough for texture perception study.\nThe contribution of this paper is a new joint model\n978-1-5090-6067-2/17/$31.00 c\u00a92017 IEEE\nar X\niv :1\n70 3.\n09 78\n4v 1\n[ cs\n.C V\n] 2\n4 M\nar 2\n01 7\nthat combines perceptual feature regression and adversarial schemes for generating textures based on perceptual descriptions. Unlike existing Conditional Generative Adversarial Networks (CGAN) [12], in which the discriminative model need to estimate the joint distribution of condition vectors with samples and can not always provide enough information for the generator to adjust parameters, in our new model, perceptual feature regression can supervise the generator to produce textures in consistence with human visual system. Thus, the discriminative model is assisted by the perceptual regression model and therefore released from the inaccurate estimation of joint distributions. Furthermore, the perceptual model is able to supply more information to the generator and guide it to produce texture with enough details, which lead to high-quality output texture images.\n2. RELATED WORK\nTextures have attracted widespread attention in the research field of visual perception and computer vision. Rao et al. identified the perceptual features people used to classify the textures and also established the correlation between semantic attributes and textures [13], which showed the importance of perceptual features for understanding texture images. Meanwhile, texture synthesis and texture generation have been active research areas for many years. Shin et al. proposed a pixel-based method for texture synthesis with non-parametric sampling [14], and Wei proposed an efficient algorithm using tree-structured vector quantization for realistic texture synthesis, which required only a sample texture as input [15]. These studies normally concern on example based texture synthesis, whereas our work focuses on generating textures according to user-defined perceptual attributes.\nDeep learning models, particularly deep convolutional neural networks, have achieved great success in texture analysis due to their strong learning capability. Texture synthesis based on CNN is a new research topic [16], which has produced promising results. These results suggest that this topic deserves more research devotion. In [16], Gatys combined the conceptual framework of spatial summary statistics on feature responses with the feature space of a convolutional neural network, and the goal is to generate textures from a given source image. Ulyanov also trained feed-forward generation networks to generate multiple samples of the same texture with arbitrary sizes [17]. In this manner, the representation of the given image can be learned by the convolutional networks, and the new samples can be generated from the networks. Goodfellow [8] proposed a generative adversarial framework that could estimate generative models via an adversarial process, in which a generative model G and a discriminative modelD were simultaneously trained. The generative model is responsible for capturing the data distribution, and the discriminative model is used to estimate the probability that a sample comes from the training data rather than G.\nThe training procedure for G is to maximize the probability of D making a mistake. It has been proven that GAN can be used to generate realistic images from uniformly distributed random noise [8]. Furthermore, GAN was extended as CGAN for conditional image generation by Mirza and Osindero [12], where both models G and D received an additional vector of information as condition. This vector might contain information about the class of the training example. CGAN has been successfully applied in digit and face image generation [18], whereas we are interested in generating textures with given perceptual attributes.\nInspired by previous works, this paper proposes a joint model, which combines the perceptual feature regression and adversarial training scheme for perception driven texture generation. Since the perceptual regression model can provide additional information for the generator in the adversarial scheme, the proposed model is able to generate high-quality textures.\n3. PERCEPTION DRIVEN TEXTURE GENERATION\nIn this section, we first introduce the overall architecture of the proposed joint model for perception driven texture generation. Then we provide details on the network design and initialization."}, {"heading": "3.1. Overall Architecture of the Joint Model", "text": "Human observers essentially use perceptual features for texture description, e.g. regularity and repetitiveness [19]. According to [20], there are 12 prominent perceptual features for human to perceive a texture. In practice, human can not only perceive these features from a texture but also imagine a texture from these perceptual descriptions. For example, textures with weak or strong directionality can be easily depicted in human mind; in contrast, no computer algorithm is able to generate texture from these descriptions. Therefore we designed a joint deep model in order to achieve such a goal. As shown in Fig. 1, the overall architecture includes three parts: a perceptual feature regression model, a conditional generative model, and a discriminative model. The generative model is responsible for conditional texture generation, whereas the discriminative model is used to distinguish whether the generated texture is from the training sample distribution, and the perceptual model can drive the generative model to produce textures possessing certain attributes.\nInspired by the success of the Inception-v3 model [4], which reached 3.46% top-5 error rate and even surpassed human performance in the 2015 ImageNet Large Scale Visual Recognition Challenge(ILSVRC), we use Inception-v3 for our perceptual feature regression. First we change the activation function of the final output layer and auxiliary units to tanh, as our perceptual features are scaled in the range between -0.9 and 0.9. The reason for scaling the range is\nto avoid the saturation of the output neurons. Furthermore, tanh is much easier to be trained than sigmoid [21]. Second, we change the cross entropy loss of softmax to the quadratic loss. Then we train the modified Inception-v3 model using our texture database for perceptual feature prediction. In the following sections, we call the modified Inception-v3 as the perceptual model.\nIn the CGAN framework, the discriminator needs to figure out the union distribution of the condition and samples. The distinguishing task is relatively difficult, and the discriminator cannot supply enough information for the generator to justify its parameters. In our model, we use the perceptual model to impose perceptual constraints on the generator; this can provide additional information for the generator to produce certain perceived textures. We use G, D, and H to represent the generative, discriminative and perceptual model, respectively. Then the loss of D can be defined as:\nD loss = \u2212 1 n n\u2211 i=1 (qi ln(D(xi, yi))+(1\u2212qi) ln(1\u2212D(xi, yi))), (1) where xi represents a training example, yi is the corresponding perceptual feature vector, qi is one or zero, indicating whether (xi, yi) is a real pair, and n is the number of training examples. The quadratic loss for H is defined as:\nH loss = 1\n2n n\u2211 i=1 (H(xi)\u2212 yi)2. (2)\nThe loss of G contains two parts: one from D, and the other from H; the definition is:\nG loss = G loss d+ \u03b1 \u2217G loss h (3)\nG loss d = \u2212 1 n n\u2211 i=1 ln(D(G(yi, zi), yi)) (4)\nG loss h = 1\n2n n\u2211 i=1 (H(G(yi, zi))\u2212 yi)2, (5)\nwhere \u03b1 is a tradeoff parameter, zi is a random noise vector, H is preliminarily trained, and G and D are trained in an adversarial scheme. In this manner, the discriminator makes the generator produce realistic textures, and the perceptual model makes the generated textures possess certain perceptual attributes."}, {"heading": "3.2. Network Design Details", "text": "In this subsection, we first introduce the initialization scheme for our deep networks, and then present strategies for the design of certain part of the network. Inspired by [22], we initialize weights of one layer of the proposed network by formulation V ar[w] = 2/n. In most cases, we only consider the back propagation situation, so n represents the number\nof units that can be reached by one input neuron, and w represents the weight in convolutional or fully connected layer. ReLU is used as the activation function in the network, since it can reduce the gradient vanishing effect and make the model learn fast. However, we would like the output of the generator to be limited in a certain range, because an image always has limited pixel values. The discriminator should yield a probability result, which indicates whether an image comes from the real training samples. Accordingly, we use tanh as the activation function in the output layer of the generative model, and sigmoid in the discriminative model. Thus, we adopt different initialization strategies for the output layer. In order to keep the gradient variance, when the activation function is tanh, we initialize the weights using the truncated normal distribution with the standard deviation \u221a 1/n. In contrast,\nwe use 4 \u221a 1/n as the deviation when the activation function is sigmoid. Here, we assume that the weights are initialized independently, and the bias is initialized with zero. In particular, if the number of units decreases too much in the output layer, we slightly reduce the deviation of weights to avoid the output becoming too saturated in the forward case. We will introduce more details about the network design in section 4.\nIt should be noted that, in the fully connected layer, the initialization strategy can be easily analyzed. However, it becomes complicated in the convolutional layers. We may take the 1-D convolutional operation as an example, and it can be easily extended to the high dimensional case. We use n to represent the number of units, which can propagate its gradient to certain input unit. When the number of input units becomes very large, we can calculate an average value for n. We define a universal formulation:\nbk\u22121d c+ 1 (6) bkdc bk+1d c\n... bk+d\u22122d c,\nwhere bxc represents the maximal integer no larger than x, k represents the kernel size, and d represents the step size. Eq (6) illustrates a period of the convolutional operation. Each line in Eq (6) calculates the number of units that can be reached by certain input unit. The period begins with the kth input unit. The length of the cycle is d. From Eq (6), we can get the average value of n for general situation:\u2211d\u22122\ni=0 b k+i d c+ b k\u22121 d c+ 1\nd . (7)\nWe use the average value of n to calculate the deviation of w for initialization. To extend this to the two dimensional situation, we simply expand k and d to two dimensions. This scheme is used to initialize our networks through all experiments.\nIn order to emphasize the importance of perceptual features for texture generation, we stretch the perceptual feature vector to 800 dimensions via a fully connected layer. The random noise vector is drawn uniformly from a 200 dimensional space ranging from -1 to 1. The reason for using these specific dimensions is explained as follows. A random noise vector with 200 dimensions can be significantly varied to generate diverse textures given certain perceptual features. In theory, if we change each dimension of the random noise vector with step of 0.1, we can obtain 20200 different vectors. This is a large enough space for variant texture appearance. In addition, textures with the same perceptual feature vector have similar appearances. In the above analysis, we demonstrate that the covariance shift can be avoided by certain initialization strategy in the forward and backward view. In the fully connected layers for stretching perceptual features, we simply consider the forward propagation. Thus, we make n represent the number of units in the input layer. Consequently, the stretched perceptual features own similar variance as the original. Let z represent the random variable. Then its variance is V ar[z] = 1/3. Recall that the perceptual features are scaled to the range between -0.9 and 0.9. Let f represent one perceptual feature, and we use the following equation for scaling:\nf\u0302 = min(max((f \u2212 E(f))/\u03c3(f),\u22123), 3)\u00d7 0.3. (8)\nThrough this transformation, the resulted f\u0302 owns variance of 0.09. Since the stretching layer is initialized by using the forward principle, the variance of the stretched features is also approximately 0.09. The result is that the variance of the random noise is three times larger than that of the stretched perceptual features. Hence if we want the perceptual features to play the same role as random noise in the generating task, we should make the number of the output units in the stretching layer three times larger than that of the random noise. In this work we therefore set the number to 800, and we can let the perceptual features dominate the generating procedure.\n4. EXPERIMENTS"}, {"heading": "4.1. The Data Set", "text": "In our experiments, we use the Perceptual Texture Database (PTD), in which there are 450 textures with corresponding 12- D perceptual features [20]. The textures in PTD have a resolution of 512\u00d7 512, and the 12-D perceptual features include contrast, repetitiveness, granularity, randomness, roughness, density, directionality, structural complexity, coarseness, regularity, orientation and uniformity. However, since 450 textures are still too few to train a deep neural network, we expand the examples in the following way. First, we crop each texture into 81 textures of size 448\u00d7 448; the step used for cropping is 8. Second, we resize the resulted textures to 299\u00d7 299. Regarding perceptual features, we let the resulted textures have same values as their original ones. We eventually obtain 36450 examples of size 299 \u00d7 299, and we use 36000 among them to train our models. The remaining textures are left as the validation set. It should be noted that it is reasonable to make the resulted 81 textures have the same perceptual features as their originals. First, the textures in PTD are isotropic; a 448 \u00d7 448 region can cover most area of the original texture and can therefor keep original perceptual characteristics. Second, resizing the 448\u00d7 448 texture to 299\u00d7 299 does not cause obviously blurring effect."}, {"heading": "4.2. Perceptual Feature Regression", "text": "Since our perceptual model was modified from Inceptionv3, we did not need to train it from scratch. The preliminary trained Inception-v3 on ImageNet can be found in [23]. Since our perceptual model only differed from Inception-v3 in the output layer and loss definition, we initialized the output layer with truncated Gaussian noise, and the other layers were reloaded from preliminary trained Inception-v3 model. Then we fine-tuned the perceptual model with initial learning rate 0.001. The RMSProp method was used for gradient descent [24]. We ran the optimization algorithm for 50000\niterations. The process is illustrated in Fig. 2(a). Finally, the Euclidean loss converged to 0.01161, and the final evaluation error was 0.0039. Since the perceptual features have 12 attributes, the standard error deviation for each attribute in average can be calculated:\n\u03c3(e) = \u221a 0.01161\u00d7 2/12 = 0.044. (9)\nThis means that we can accurately predict the perceptual features for one texture with very small deviation. Based on this observation, we can make a basic assumption here: if the generated textures have certain perceptual attributes, it should be correctly perceived by the perceptual model. We use the preliminary trained perceptual model as an accessory of the whole generative framework."}, {"heading": "4.3. Generating Textures from Perceptual Features", "text": "To generate realistic textures, we must design a reasonable network structure. The kernel size is a vital factor for generating high-quality images. In the experiments, we found that if we set the kernel size too small, i.e. 3, the generated textures owned more details but looked too crude. If the kernel size was too large, i.e. 7, the generated textures looked more smooth, but with less details. Eventually, we used 5\u00d7 5 kernels for convolution or inverse convolution in our discriminative and generative models. We also tried to fuse kernels of different size for generating textures with more details and global information. However, it did not produce good results.\nSince one part of the input to the generative model was drawn from random noise (the other part is the perceptual feature vector), there were infinitely many training examples in practice. Thus we used the ADAM [25] method for optimization. We optimized the generative model twice after each optimization for the discriminative model. We made each batch contain 60 training examples. The tradeoff parameter \u03b1 was\nset as 10. In the end, we ran 266000 optimization iterations. The training process is illustrated in Fig. 2(b)(c)(d). Two experiments were designed after the models were trained. First, we fed real perceptual features in our database with different random noise to the generative model. The generated textures are shown in Fig. 3. Second, we manually edited some perceptual features and used them to generate textures. It should be emphasized that the manually edited or handcrafted perceptual features were based on existing perceptual features, i.e. only certain perceptual feature was set to three different values: 0.9, 0, -0.9, whereas the others were kept the same as the existing ones. In Fig. 4, we only provide six results due to the limited space, but more results are provided in the supplementary materials. As an example, we can see from the first column of Fig. 4, when we decrease the perceptual feature value of directionality from 0.9 to -0.9, the textures gradually lose the overall direction. These results indicate that the proposed method is able to generate desired textures by varying certain perceptual attributes.\n5. CONCLUSION\nWe propose a novel deep network model for perception driven texture generation. In the proposed model, a perceptual regression component is integrated with the generative framework, which drives the produced textures possessing certain perceptual attributes. This perceptual regression model partially releases the discriminative model\u2019s workload, and can supply more information for the generator to produce better perceived texture. Experimental results show that the jointed models are able to generate realistic texture from given perceptual attributes. We attribute this success to the fact that if the generated texture is realistic enough, it should have the potentiality to be correctly perceived by the preliminary trained deep network.\nIt should be noted that the perceptual features are not independent from each other. If we change one perceptual attribute arbitrarily, the remaining relevant features might also need to be changed to fit the real distribution. In the future work, we will design an auxiliary model for generating correct perceptual feature vectors; in this way we may simply provide an existing perceptual feature vector and the desired value for certain attribute, and the tool can generate a suitable input perceptual feature vector.\n6. REFERENCES\n[1] Alexey Badalov, Irene Cheng, Claudio Silva, and Anup Basu, \u201cAn in-place texture synthesis technique for memory constrained multimedia applications,\u201d in IEEE International Conference on Multimedia and Expo, 2011, pp. 1\u20134.\n[2] Kevin Jarrett, Koray Kavukcuoglu, Yann Lecun, et al., \u201cWhat is the best multi-stage architecture for object recognition?,\u201d in 2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 2146\u20132153.\n[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d Advances in Neural Information Processing Systems, vol. 25, no. 2, pp. 2012, 2012.\n[4] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, \u201cRethinking the inception architecture for computer vision,\u201d arXiv preprint arXiv:1512.00567, 2015.\n[5] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.\n[6] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra, \u201cDraw: A\nrecurrent neural network for image generation,\u201d arXiv preprint arXiv:1502.04623, 2015.\n[7] Alexey Dosovitskiy, Jost Tobias Springenberg, and Thomas Brox, \u201cLearning to generate chairs with convolutional neural networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1538\u20131546.\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, \u201cGenerative adversarial nets,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.\n[9] Xiaolong Wang and Abhinav Gupta, \u201cGenerative image modeling using style and structure adversarial networks,\u201d arXiv preprint arXiv:1603.05631, 2016.\n[10] Andrej Karpathy, Armand Joulin, and Fei Fei F Li, \u201cDeep fragment embeddings for bidirectional image sentence mapping,\u201d in Advances in neural information processing systems, 2014, pp. 1889\u20131897.\n[11] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee, \u201cAttribute2image: Conditional image generation from visual attributes,\u201d arXiv preprint arXiv:1512.00570, 2015.\n[12] Mehdi Mirza and Simon Osindero, \u201cConditional generative adversarial nets,\u201d arXiv preprint arXiv:1411.1784, 2014.\n[13] Nalini Bhushan, A Ravishankar Rao, and Gerald L Lohse, \u201cThe texture lexicon: Understanding the categorization of visual texture terms and their relationship to texture images,\u201d Cognitive Science, vol. 21, no. 2, pp. 219\u2013246, 1997.\n[14] Seunghyup Shin, Tomoyuki Nishita, and Sung Yong Shin, \u201cOn pixel-based texture synthesis by nonparametric sampling,\u201d Computers & Graphics, vol. 30, no. 5, pp. 767\u2013778, 2006.\n[15] Li-Yi Wei and Marc Levoy, \u201cFast texture synthesis using tree-structured vector quantization,\u201d in Proceedings of the 27th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 2000, pp. 479\u2013488.\n[16] Leon Gatys, Alexander S Ecker, and Matthias Bethge, \u201cTexture synthesis using convolutional neural networks,\u201d in Advances in Neural Information Processing Systems, 2015, pp. 262\u2013270.\n[17] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky, \u201cTexture networks: Feed-forward synthesis of textures and stylized images,\u201d arXiv preprint arXiv:1603.03417, 2016.\n[18] Jon Gauthier, \u201cConditional generative adversarial nets for convolutional face generation,\u201d Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, vol. 2014, 2014.\n[19] A. Ravishankar Rao and Gerald L. Lohse, \u201cTowards a texture naming system: Identifying relevant dimensions of texture,\u201d Vision Research, vol. 36, no. 11, pp. 1649\u2013 1669, 1996.\n[20] Jun Liu, Junyu Dong, Xiaoxu Cai, Lin Qi, and Mike Chantler, \u201cVisual perception of procedural textures: Identifying perceptual dimensions and predicting generation models,\u201d PloS one, vol. 10, no. 6, pp. e0130335, 2015.\n[21] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDelving deep into rectifiers: Surpassing humanlevel performance on imagenet classification,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.\n[23] \u201cInception-v3 model trained on imagenet,\u201d http:// download.tensorflow.org/models/image/ imagenet/inception-v3-2016-03-01. tar.gz.\n[24] Kevin Swersky Geoffrey Hinton, Nitish Srivastava, \u201cOverview of mini-batch gradient descent,\u201d http:// www.cs.toronto.edu/%7Etijmen/csc321/ slides/lecture_slides_lec6.pdf.\n[25] Diederik Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014."}], "references": [{"title": "An in-place texture synthesis technique for memory constrained multimedia applications", "author": ["Alexey Badalov", "Irene Cheng", "Claudio Silva", "Anup Basu"], "venue": "IEEE International Conference on Multimedia and Expo, 2011, pp. 1\u20134.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Yann Lecun"], "venue": "2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 2146\u20132153.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems, vol. 25, no. 2, pp. 2012, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Draw: A  recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1538\u20131546.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "arXiv preprint arXiv:1603.05631, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "Advances in neural information processing systems, 2014, pp. 1889\u20131897.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "arXiv preprint arXiv:1512.00570, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The texture lexicon: Understanding the categorization of visual texture terms and their relationship to texture images", "author": ["Nalini Bhushan", "A Ravishankar Rao", "Gerald L Lohse"], "venue": "Cognitive Science, vol. 21, no. 2, pp. 219\u2013246, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "On pixel-based texture synthesis by nonparametric sampling", "author": ["Seunghyup Shin", "Tomoyuki Nishita", "Sung Yong Shin"], "venue": "Computers & Graphics, vol. 30, no. 5, pp. 767\u2013778, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast texture synthesis using tree-structured vector quantization", "author": ["Li-Yi Wei", "Marc Levoy"], "venue": "Proceedings of the 27th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 2000, pp. 479\u2013488.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Texture synthesis using convolutional neural networks", "author": ["Leon Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 262\u2013270.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["Dmitry Ulyanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1603.03417, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional generative adversarial nets for convolutional face generation", "author": ["Jon Gauthier"], "venue": "Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester, vol. 2014, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a texture naming system: Identifying relevant dimensions of texture", "author": ["A. Ravishankar Rao", "Gerald L. Lohse"], "venue": "Vision Research, vol. 36, no. 11, pp. 1649\u2013 1669, 1996.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Visual perception of procedural textures: Identifying perceptual dimensions and predicting generation models", "author": ["Jun Liu", "Junyu Dong", "Xiaoxu Cai", "Lin Qi", "Mike Chantler"], "venue": "PloS one, vol. 10, no. 6, pp. e0130335, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Overview of mini-batch gradient descent", "author": ["Kevin Swersky Geoffrey Hinton", "Nitish Srivastava"], "venue": "http:// www.cs.toronto.edu/%7Etijmen/csc321/ slides/lecture_slides_lec6.pdf.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 0}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Texture synthesis and generation have also been extensively investigated in the past years [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Convolutional Neural Network (CNN), which was inspired by the mechanism of visual cortex, has shown great superiority in latest studies [2] [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Convolutional Neural Network (CNN), which was inspired by the mechanism of visual cortex, has shown great superiority in latest studies [2] [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "For example, in the ImageNet Large Scale Visual Recognition Challenge, the performance of computer algorithms even surpassed human\u2019s [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "Consequently, researchers have been investigating different approaches based on CNN for image generation [5] [6] [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "proposed a generative adversarial framework (GAN) [8] and produced excellent results in many image generation tasks.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "In order to generate more realistic images, Wang and Gupta factorized the image generation process and proposed a joint model consisting of Style and Structure Generative Adversarial Networks [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "Experimental results in [9] suggested that a great gain could be obtained through this factoring trick for generating realistic indoor scenes.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "proposed a fragment embedding method in 2014 [10], which was essentially a bidirectional retrieval scheme, as the desired image must exist in the image database.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "modeled images as composite of foreground and background and developed a layered generative model [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Unlike existing Conditional Generative Adversarial Networks (CGAN) [12], in which the discriminative model need to estimate the joint distribution of condition vectors with samples and can not always provide enough information for the generator to adjust parameters, in our new model, perceptual feature regression can supervise the generator to produce textures in consistence with human visual system.", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "identified the perceptual features people used to classify the textures and also established the correlation between semantic attributes and textures [13], which showed the importance of perceptual features for understanding texture images.", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "proposed a pixel-based method for texture synthesis with non-parametric sampling [14], and Wei proposed an efficient algorithm using tree-structured vector quantization for realistic texture synthesis, which required only a sample texture as input [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "proposed a pixel-based method for texture synthesis with non-parametric sampling [14], and Wei proposed an efficient algorithm using tree-structured vector quantization for realistic texture synthesis, which required only a sample texture as input [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 15, "context": "Texture synthesis based on CNN is a new research topic [16], which has produced promising results.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "In [16], Gatys combined the conceptual framework of spatial summary statistics on feature responses with the feature space of a convolutional neural network, and the goal is to generate textures from a given source image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Ulyanov also trained feed-forward generation networks to generate multiple samples of the same texture with arbitrary sizes [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "Goodfellow [8] proposed a generative adversarial framework that could estimate generative models via an adversarial process, in which a generative model G and a discriminative modelD were simultaneously trained.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "It has been proven that GAN can be used to generate realistic images from uniformly distributed random noise [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 11, "context": "Furthermore, GAN was extended as CGAN for conditional image generation by Mirza and Osindero [12], where both models G and D received an additional vector of information as condition.", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "CGAN has been successfully applied in digit and face image generation [18], whereas we are interested in generating textures with given perceptual attributes.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "regularity and repetitiveness [19].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "According to [20], there are 12 prominent perceptual features for human to perceive a texture.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "Inspired by the success of the Inception-v3 model [4], which reached 3.", "startOffset": 50, "endOffset": 53}, {"referenceID": 20, "context": "Furthermore, tanh is much easier to be trained than sigmoid [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Inspired by [22], we initialize weights of one layer of the proposed network by formulation V ar[w] = 2/n.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "In our experiments, we use the Perceptual Texture Database (PTD), in which there are 450 textures with corresponding 12D perceptual features [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "The RMSProp method was used for gradient descent [24].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Thus we used the ADAM [25] method for optimization.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. Meanwhile, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as input. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial framework, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual features, the corresponding appearance of the generated textures will also be changed. We design several experiments to validate the effectiveness of the proposed method. The results show that the proposed method can produce high quality texture images with desired perceptual properties.", "creator": "LaTeX with hyperref package"}}}