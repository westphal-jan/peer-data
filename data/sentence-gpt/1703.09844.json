{"id": "1703.09844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Multi-Scale Dense Convolutional Networks for Efficient Prediction", "abstract": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network's prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across 'easier' and 'harder' examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available. The most commonly used convolutional network architecture in high-resolution visual memory has the potential to provide more flexibility in the treatment of complex neural network operations, especially in low-order tasks such as image-processing or deep learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 29 Mar 2017 00:19:20 GMT  (4488kb,D)", "http://arxiv.org/abs/1703.09844v1", null], ["v2", "Tue, 6 Jun 2017 14:17:22 GMT  (3555kb,D)", "http://arxiv.org/abs/1703.09844v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gao huang", "danlu chen", "tianhong li", "felix wu", "laurens van der maaten", "kilian q weinberger"], "accepted": false, "id": "1703.09844"}, "pdf": {"name": "1703.09844.pdf", "metadata": {"source": "META", "title": "Multi-Scale Dense Convolutional Networks for Efficient Prediction", "authors": ["Gao Huang", "Danlu Chen", "Tianhong Li", "Felix Wu", "Laurens van der Maaten", "Kilian Q. Weinberger"], "emails": ["<gh349@cornell.edu>."], "sections": [{"heading": "1. Introduction", "text": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015). This development was in part driven by several public competition datasets, such as ILSVRC (Deng et al., 2009) and COCO (Lin et al., 2014), where state-of-the-art models may have even managed to surpass human-level performance (He et al., 2015; 2016).\nAt the same time there is also a surge in demand for applications of object recognition, for instance, in self-driving cars (Bojarski et al., 2016) or content-based image search\n1Cornell University, Ithaca, NY, USA 2Tsinghua University, Beijing, China 3Fudan University, Shanghai, China 4Facebook AI Research, New York, USA. Correspondence to: Gao Huang <gh349@cornell.edu>.\n(Wan et al., 2014). However, the requirements for real world applications differ from those necessary to win competitions and there is a significant gap between the latest record-breaking models and those that can be used in realworld applications. Competitions tend to motivate very large, resource-hungry models with high computational demands during inference time. For example, the winner of the 2016 COCO competition consisted of an ensemble of several computationally very intensive convolutional networks1. Such a model can likely not be used in applications such as self-driving cars or web services, which have tight resource and time constraints during inference time.\nFor instance, high accuracy in pedestrian detection is of no use if the detection is performed after the pedestrian has already been hit by the car. Similarly, web services for content-based image search are of little use if each query takes minutes to process. Indeed, the most suitable network architecture in many real-world applications is not the architecture that achieves the highest classification accuracy, but the architecture that achieves the best accuracy within some computational budget.\nIn this paper, we focus on exactly this setting in the context of deep convolutional networks. If the test-time budget is known a priori, one can learn a network that fits just within the computational budget. But what if the budget varies or is amortized across test cases? Here, convolutional networks pose an inherent dilemma: Deeper (He et al., 2016) and wider (Zagoruyko & Komodakis, 2016) networks with more parameters are better at prediction (Huang et al., 2017), but only the last layers produce the high-level features required to obtain their high level of accuracy. Partial inference computation can typically not be used effectively to produce accurate predictions.\nTo solve this dilemma, we propose a novel convolutional network architecture which combines multi-scale feature maps (Saxena & Verbeek, 2016) with dense connectivity (Huang et al., 2017). The multi-scale feature maps produce high-level feature representations that are amenable to classification after just a few layers, whilst maintaining high accuracies when the full network is evaluated. The dense connectivity pattern allows the network to add fea-\n1http://image-net.org/challenges/talks/ 2016/GRMI-COCO-slidedeck.pdf\nar X\niv :1\n70 3.\n09 84\n4v 1\n[ cs\n.L G\n] 2\n9 M\nar 2\n01 7\ntures to the existing feature representation at each layer of the network. This combination allows us to introduce accurate early-exit classifiers throughout the network. We refer to our architecture as Multi-Scale DenseNet (MSDNet).\nWe study the performance of MSDNets in two settings with computational constraints at test-time: (1) an anytime classification setting in which the network has to be able to output a prediction at any given point in time; and (2) a batch computational budget setting in which there is a fixed computational budget available for classifying a set of examples. The anytime setting is applicable to, for example, self-driving cars: such a car needs to process a highresolution video stream on-the-fly and may need to output a prediction immediately when an unexpected event occurs. The batch computational budget setting is applicable to, for example, content-based image search: a search engine receives a large number of queries per second and may increase its average accuracy by reducing the amount of computation spent on simple queries and spending the saved computation on classifying more complex queries.\nOur experimental evaluation on three image-classification datasets demonstrates the efficacy of MSDNets. In the anytime classification setting, we show that it is possible to reduce the computation required at test time significantly compared to popular convolutional networks at a negligible loss in classification accuracy, whilst also providing the ability to output a prediction at any time. In the batch computational budget setting, we show that MSDNets can be effectively used to adapt the amount of computation to the difficulty of the example to be classified, which allows us to reduce the computational requirements of our models significantly whilst performing on par with state-of-the-art convolutional networks in terms of classification accuracy."}, {"heading": "2. Related Work", "text": "We briefly review some related prior work on computationefficient networks, memory-efficient networks, and costsensitive machine learning. Our network architecture draws inspiration from several other network architectures.\nComputation-efficient networks. Most prior work on (convolutional) networks that are computationally efficient at test time focuses on reducing model size after training. In particular, many studies prune weights (LeCun et al., 1989; Hassibi et al., 1993) during or after training and finetune the resulting smaller models (Han et al., 2016; Li et al., 2017). These approaches are generally effective because deep networks often have a substantial number redundant weights that can be pruned without sacrificing (and sometimes even improving) performance. Prior work also studies approaches that directly learn compact models with less parameter redundancy. For example, the popular\nknowledge-distillation method (Bucilua et al., 2006; Hinton et al., 2014) trains small student networks to reproduce the output of a much larger teacher network or ensemble. Our work differs from those prior approaches in that we train a single model that can trade off computation for accuracy at prediction time without any re-training or finetuning. Indeed, weight-pruning and knowledge distillation can be used in combination with our approach, and may lead to further computational improvements.\nMemory-efficient networks. A large body of recent prior work explores memory-efficient (convolutional) network architectures. Most of these studies perform some type of quantization of the network\u2019s weights (Gong et al., 2014) or use hashing (Chen et al., 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al., 2016; Rastegari et al., 2016). Rastegari et al. (2016) recently showed that a binary-weight version of AlexNet (Krizhevsky et al., 2012) can match the performance of its 32-bits floating point counterpart on the ImageNet dataset. Whereas memory efficiency is essential for deploying deep networks on mobile platforms, quantization generally often leads to a decrease in computational efficiency because weights quantization requires additional look-up operations before the network can be evaluated. In this study, we do not focus on memory efficiency although we believe that existing weight quantization approaches can be used successfully with our network architecture in case memory efficiency is important.\nCost-sensitive machine learning. Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015). Most of these studies focus on how to incorporate the computational requirements of computing particular features in the training of machine-learning models such as (gradient-boosted) decision trees. Whilst our study is certainly inspired by this prior work, the architecture we explore differs substantially from prior work on cost-sensitive machine learning: most prior work exploits characteristics of machine-learning models (such as decision trees) that do not apply to deep networks. Our work is most closely related to recent work on FractalNets (Larsson et al., 2017), which can perform anytime prediction by progressively evaluating subnetworks of the full network. FractalNets differ from our work in that they are not explicitly optimized for computation efficiency: our experiments show that MSDNets substantially outperform FractalNets. Our dynamic evaluation strategy for reducing batch computational cost is closely related to the the adaptive computation time approach (Graves, 2016; Figurnov et al., 2016), and the recently proposed method of adaptively evaluating neural networks (Bolukbasi et al., 2017). Different from\nthese works, our method adopts a specially designed network with multiple classifiers, which are jointly optimized during training and can directly output confidence scores to control the evaluation process for each test example.\nThe adaptive computation time method (Graves, 2016) and its extension (Figurnov et al., 2016) also perform adaptive evaluation on test examples to save batch computational cost. Our method differs from these works in that we have a specially designed network with multiple classifiers, which can directly output confidence scores to determine the termination of the evaluation process.\nRelated network architectures. Our network architecture is inspired by several existing architectures. In particular, it uses the same multi-scale convolutions that are used in convolutional neural fabrics (Saxena & Verbeek, 2016) to rapidly construct a low-resolution feature map that is amenable to classification, whilst also maintaining feature maps of higher resolution that are essential for obtaining high classification accuracy. We use the same featureconcatenation approach as DenseNets (Huang et al., 2017), which allows us to construct highly compact models. A similar way of reusing features is used in progressive networks (Rusu et al., 2016). Our architecture is related to deeply supervised networks (Lee et al., 2015) in that it incorporates classifiers at multiple layers throughout the network. In contrast to all these prior architectures, our network is designed to operate in settings with computational constraints at test time."}, {"heading": "3. Problem Setup", "text": "We consider two settings that impose computational constraints at prediction time.\nAnytime prediction. In the anytime prediction setting (Grubb & Bagnell, 2012), there is a finite computational budget B > 0 available for each test example. The budget is nondeterministic, and varies per test instance. It is determined by the occurrence of an event that requires the model to output a prediction immediately. We assume that the budget is drawn from some joint distribution P (x, B). In some applications P (B) may be independent of P (x) and can be estimated. For example, if the event is governed by a Poisson process, P (B) is an exponential distribution. We denote the loss of a model f(x) that has to produce a prediction for instance x within budget B by L(f(x), B). The goal of an anytime learner is to minimize the expected loss under the budget distribution:\nL(f) = E [L(f(x), B)]P (x,B) , (1)\nwhere L(\u00b7) denotes a suitable loss function. As is common in the empirical risk minimization framework, the expectation under P (x, B) may be estimated by an average over samples from P (x, B).\nBatch computational budget. In the batch computational budget setting, the model needs to classify a set of examples Dtest = {x1, . . . ,xM} within a finite computational budget B > 0 that is known in advance. The learner aims to minimize the loss across all examples in Dtest within a cumulative cost bounded by B, which we denote by L(f(Dtest), B) for some suitable loss function L(\u00b7). It can potentially do so by spending less than BM computation on classifying an \u201ceasy\u201d example whilst spending more than B M computation on classifying a \u201cdifficult\u201d example."}, {"heading": "4. Network Architecture", "text": "Like deeply supervised networks (Lee et al., 2015) and Inception models (Szegedy et al., 2015), our convolutional network has classifiers operating on feature maps at intermediate layers in the network. Each of these classifiers can be used to output a prediction, which allows for retrieving preliminary predictions before a test image is propagated through all the layers of the network. Such preliminary retrieval is essential in both the batch computational budget setting and the anytime prediction setting.\nChallenges. There are two main challenges in designing network architectures that have classifiers at various layers of the network: (1) obtaining acceptable prediction accuracies at an early stage in the network whilst training a deep architecture and (2) making predictions in such a way that the computation used to evaluate early classifiers is not wasted when later classifiers are evaluated. The first challenge arises because the accuracy of classifiers that operate on low-level features (i.e., on features obtained before several levels of down-sampling) is generally low. The second challenge arises because feature maps in later layers gener-\nally capture different structure than features in early layers, which makes it difficult to construct classifiers that complement each other. Our convolutional networks address these two challenges via two main architectural changes. An illustration of our architecture is shown in Figure 1.\nMultiple scales. The first main change is that we maintain a feature representation at multiple scales in each layer2 of the network, akin to convolutional neural fabrics (Saxena & Verbeek, 2016). The feature maps at a particular layer and scale are computed by concatenating the results of one or two convolutions: (1) the result of a regular convolution applied on the same-scale features from the previous layer (horizontal connections), and if possible (2) the result of a strided convolution applied on the finer-scale feature map from the previous layer (diagonal connections). By maintaining feature representations at various scales in each layer of the network we obtain a low-resolution feature representation that is amenable to classification after just a few convolutional layers, whilst still being able to produce high-quality features (that rely on the high-resolution feature maps) later in the network.\nDense connectivity. The second change we make is that we use dense connections (Huang et al., 2017) that allow each layer to receive inputs directly from all its previous layers. As shown by Huang et al. (2017), such dense connections (1) significantly reduce redundant computations by encouraging feature reuse and (2) make learning easy by eliminating the gradient vanishing problem in a way similar to residual networks (He et al., 2016). Moreover, the use of dense connections is instrumental in addressing the second challenge identified above: dense connections differ from traditional network architectures in that they add additional features at each layer whilst maintaining all features that were computed prior.\nParameter efficiency. Perhaps somewhat counterintuitively, our architectural changes improve the parameter efficiency significantly. In particular, a version of our network with less than 1 million parameters is able get lower than 7.0% test error on CIFAR-10 dataset, which suggests our architecture is on par with or better than existing architectures in terms of the accuracy-model size trade-off (for instance, Saxena & Verbeek (2016) reported a 7.43% test error on the same dataset using a model with >20 million parameters). We present the details of our MSDNet below.\nFirst layer. The first layer (`= 1) is unique as it includes vertical connections in Figure 1. Its main purpose is to \u201cseed\u201d representations on all S scales. One could view its vertical layout as a miniature \u201cS-layers\u201d convolutional network (S=3 in Figure 1). Let us denote the output feature maps at layer ` and scale s as x(s)` and the original input\n2Here, we use the term \u201clayer\u201d to refer to a column in Figure 1.\nimage as x10. Feature maps at coarser scales are obtained via down-sampling; i.e., the output of the first layer at the s-th scale is given by:\nx (s) 1 =\n{ h0(x (1) 0 ) if s = 1,\nh\u03030(x (s\u22121) 1 ) if s > 1.\nHerein, h0(\u00b7) and h\u03030(\u00b7) denote a regular convolutional transformation and a strided convolutional transformation, respectively. The output of the first layer is a collection of feature maps { x (1) 1 , . . . ,x (S) 1 } , one for each scale.\nSubsequent layers. Following the dense connectivity pattern proposed by Huang et al. (2017), the output feature maps x(s)` produced at subsequent layers, `>1, and scales, s, are a concatenation of transformed feature maps from all previous feature maps of scale s and s\u2212 1 (if s > 1). Formally, the `-th layer of our network outputs a set of features at S scales { x (1) ` , . . . ,x (S) ` } with:\nx (s) ` =  h` ([ x (s) 1 , . . . ,x (s) `\u22121 ]) if s = 1,  h` ([x(s)1 , . . . ,x(s)`\u22121]) , h\u0303` ([ x (s\u22121) 1 , . . . ,x (s\u22121) `\u22121\n])  s > 1. Herein, [. . . ] denotes the concatenation operator, h`(\u00b7) a regular convolution transformation, and h\u0303`(\u00b7) a strided convolutional transformation. Note that the outputs of h` and h\u0303` have the same map size; their outputs are concatenated along the channel dimension. In Figure 1, we did not draw connections across more than one layer explicitly: these connections are implicit through recursive concatenations.\nClassifiers. The classifiers in MSDNets also follow the dense connectivity pattern within the coarsest scale, S, i.e., the classifier at layer ` uses all the features[ x (S) 1 , . . . ,x (S) ` ] . Each classifier consists of two convolutional layers, followed by one average pooling layer and one linear layer. In practice we only attach classifiers to some of the intermediate layers, and we let fk(\u00b7) denote the kth classifier. During testing in the anytime setting we propagate the input through the network until the budget is exhausted and output the most recent prediction. In the batch budget setting at testing time, an example traverses the network and exits after classifier fk if its prediction confidence (we use the maximum value of the softmax probability) exceeds a pre-determined threshold \u03b8k. Before training we compute the cost, Ck, required to process the network up to the kth classifier. We denote by 0 < q \u2264 1 a fixed exit probability that a sample that reaches a classifier will obtain a classification with sufficient confidence to exit. If q is constant across all layers we can compute the probability that a sample exits at classifier k as:\nqk = z(1\u2212 q)k\u22121q, (2)\nwhere z is a normalizing constant (as there are only finitely number of layers). This provides a natural way to determine the required depth of the network: as qk is exponentially decreasing, one can set the final layer to be the first layer with qk < , for some small > 0. At test time, we need to ensure that the overall cost of classifying all samples in Dtest does not exceed our budget B, which gives rise to the constraint:\n|Dtest| \u2211\nk qkCk \u2264 B. (3)\nWe can solve (3) for q and assign the thresholds \u03b8k on a hold-out set such that approximately a fraction of qk validation samples exit at the kth classifier.\nLoss functions. During training we use the logistic loss functions L(fk) for each classifier and minimize the weighted cumulative loss:\n1 |D| \u2211 (x,y)\u2208D \u2211 k wkL(fk), (4)\nwhere D denotes the training set and wk \u2265 0 a weight of the classifier k. We can use these weights to incorporate our prior knowledge about the budget B. In the batch computational budget setting, the budget B is known before test time, which allows for the design of proper weights to adapt for the given budget. In the anytime setting, the budget B is sampled from some distribution P (x, B). When the marginal distribution P (B) is known, we can adapt the weights wk accordingly. Empirically, we find that assigning equal weights to all the loss functions, i.e., wk = 1 for all k, works reasonably well, also.\nNetwork reduction and lazy evaluation. There are three straightforward ways to further reduce the computational requirements of MSDNets. First, it is inefficient to maintain all the finer scales until the end of the network. One simple strategy to reduce the network is splitting the network into S blocks along the depth dimension, and only keep the coarsest (S \u2212 i + 1) scales in the ith block. This reduces computational cost for both training and testing. Second, we can add a transition layer between two blocks to merge the concatenated features with 1 \u00d7 1 convolution and reduce the number of channels by half, similar to the DenseNet-BC architecture (Huang et al., 2017). Third, since the classifier is only attached to the coarsest scale, finer feature maps in that layer are not used for prediction. Therefore, we compute the feature set {x(1)`+1, . . . ,x (S) `+S} in parallel, avoiding unnecessary computations when we need to stop at the (`+ S)th scale to retrieve the prediction. We call this strategy lazy evaluation.\nImplementation details. We use MSDNet with three scales on the CIFAR datasets. The convolutional layer functions in the first layer, h1, denote a sequence of 3\u00d73 convolutions (Conv), batch normalization (BN; Ioffe &\nSzegedy (2015)), and rectified linear unit (ReLU) activation. In the computation of h\u03031, down-sampling is performed by performing convolutions using strides that are powers of two. For subsequent feature layers, the transformations h` and h\u0303` are defined following the design in DenseNets (Huang et al., 2017): Conv(1 \u00d7 1)-BN-ReLUConv(3\u00d73)-BN-ReLU. We set the number of output channels of the three scales to 6, 12, and 24, respectively. Each classifier has two down-sampling convolutional layers with 128 dimensional 3\u00d73 filters, followed by a 2\u00d72 average pooling layer and a linear layer. The MSDNet used for ImageNet has four scales, respectively producing 16, 32, 64, and 64 feature maps at each layer. The original images are first transformed by a 7\u00d77 convolution and a 3\u00d73 max pooling (both with stride 2), before entering the first layer of MSDNets. The classifiers have the same structure as those used for the CIFAR datasets, except that the number of output channels of each convolutional layer is set to be equal to the number of its input channels."}, {"heading": "5. Experiments", "text": "We evaluate the effectiveness of our approach in image classification experiments on three benchmark datasets. Code to reproduce all results is available at https:// github.com/gaohuang/MSDNet.\nDatasets. We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images. Images are flipped horizontally with probability 0.5, and normalized by subtracting channel means and dividing by channel standard deviations. The ImageNet dataset comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images. We hold out 50,000 images from the training set in order to estimate the confidence threshold for classifiers in MSDNet. We adopt the data augmentation scheme of He et al. (2016); Huang et al. (2017) at training time; at test time, we classify a 224\u00d7224 center crop of images that were resized to 256\u00d7256 pixels.\nTraining details. We train all models using the framework of Gross & Wilber (2016). On the two CIFAR datasets, all models (including all baselines) are trained using stochastic gradient descent (SGD) with mini-batch size 64. We use Nesterov momentum with a momentum weight of 0.9 without dampening, and a weight decay of 10\u22124. All models are\ntrained for 300 epochs, with an initial learning rate of 0.1, which is divided by a factor 10 after 150 and 225 epochs. We apply the same optimization scheme to the ImageNet dataset, except that we increase the mini-batch size to 256, and all the models are trained for 90 epochs, with two learning rate drops, after 30 and 60 epochs respectively."}, {"heading": "5.1. Anytime Prediction", "text": "We first evaluate the performance of our models in the anytime prediction setting. Here, the model maintains a predictive distribution over classes that is progressively updated. At any time during the inference process, it can be forced to output its most up-to-date prediction.\nBaselines. There exist two convolutional network architectures that are suitable for anytime prediction: namely, FractalNets (Larsson et al., 2017) and deeply supervised networks Lee et al. (2015).\nFractalNets allow for multiple evaluation paths during inference time, which vary in cost. An Anytime setting can be induced if the paths are evaluated in order of increasing cost. Here, we use the result reported in the original paper.\nDeeply supervised networks introduce multiple early-exit classifiers throughout a network, trained on the features of the particular layer they are attached to. Instead of using the original model proposed in Lee et al. (2015), we use the more competitive ResNet and DenseNet3 as the base networks. We refer to these as ResNetMC and DenseNetMC, where MC stands for multiple classifiers. The ResNetMC has 62 layers, with 10 residual blocks at each spatial resolution (for three resolutions): we train early-exit classifiers on the output of the 4th and 8th residual blocks at each resolution, producing a total of 6 intermediate classifiers (plus the final classification layer). The DenseNetMC consists of 52 layers with three dense blocks and each of them has 16 layers. The six intermediate classifiers are attached to the 6th and 12th layer in each block, also with dense connections to all previous layers in that block. Both networks require about 3\u00d7 108 FLOPs when fully evaluated.\nIn addition, we include ensembles of CNNs of identical or varying sizes. At test time, the networks are evaluated sequentially (in ascending order of network size) to obtain predictions for the test data. These predictions are averaged over the evaluated models. We experiment with ensembles of ResNets, DenseNets and a more competitive, and unpublished, variant of DenseNets (denoted as DenseNet*), that is optimized to maximize FLOP efficiency by doubling the growth rate after each transition layer.\nOn ImageNet, we compare MSDNet against the highly\n3In all experiments, we use the state-of-the-art DenseNet with bottleneck and channel reduction layers, which is referred to as DenseNet-BC by Huang et al. (2017).\ncompetitive ensemble of ResNets with varying depths. The ResNets we used in the ensemble range from 10 layers to 50 layers. All the models are the same as those described by He et al. (2016), except that the ResNet-10 and ResNet26 are variants of ResNet-18 that were modified by removing/adding one residual block from/to each of the last four spatial resolutions in the network.\nDetails of architecture. The MSDNet used in our anytimeprediction experiments has 24 layers (each layer corresponds to a column in Fig. 1), using the reduced network with transition layers as described in Section 4. The classifiers operate on the output of the 2\u00d7(i+1)th layers, with i = 1, . . . , 11. On ImageNet, we use an MSDNet with 23 multi-scale layers, and the ith classifier operates on the (4i+3)th layer (with i=1, . . . , 5). For simplicity, the losses of all the classifiers are weighted equally during training.\nResults. The results of our anytime-prediction experiments on the CIFAR-10 and CIFAR-100 datasets are presented in the left and middle panel of Figure 2. The figures present the classification accuracy of our models as a function of the computational budget at test time. We make two main observations.\nFirst, we observe that MSDNet substantially outperforms ResNetsMC and DenseNetsMC, in particular in regimes in which the computational budget is limited. This result is due to the fact that, unlike residual networks, MSDNets produce low-resolution feature maps after just a few layers of computations: these low-resolution feature maps are much more discriminative for classification than the highresolution feature maps in the early layers of ResNets or DenseNets.\nSecond, MSDNet performs strongly for all budget regimes except in the extremely low-budget regime. Here, the ensemble approaches have an advantage over MSDNets, because we evaluate the smallest models in our ensembles first. In contrast to the first classifier(s) in our model, the features learned by these models are completely optimized to give rise immediate classifications, whereas the feature representations learned by MSDNet also need to maintain more complex features that are relevant for predictions produced by later classifiers. As a result, the \u201ceffective capacity\u201d of our initial classifier (at the same number of FLOPs) is lower than that of the smallest models in the ensembles. However, this pays off quickly with increasing test time budget. In contrast, the performance of the ensembles starts to saturate as a function of the number of FLOPs, in particular, when all models in the ensemble are shallow. The rapid saturation of ensemble methods may be reduced by including increasingly deep models, but this has the downside that (unlike MSDNets) computations of similar lowlevel features are repeated multiple times. As a result, the ensembles already perform worse than MSDNet in regimes\nin which slightly more computational resources are available at inference time.\nThe right panel of Figure 2 shows the anytime-prediction results on ImageNet. For all budgets we evaluated, the prediction accuracy of MSDNet is significantly higher than that of the ResNet ensemble. In particular, when the budget ranges from 0.5\u00d7 1010 to 1.0\u00d7 1010 FLOPs, MSDNet achieves\u223c4%\u22128% higher top-1 accuracy. The strong performance of MSDNet on ImageNet shows that the model is able to effectively reuse feature representations to progressively refine its predictions.\nBudget distribution. To investigate how sensitive the performance of MSDNets on the budget distribution P (B) is, we perform experiments with three different budget distributions. The three distributions are illustrated in Figure 3 as dotted lines: a uniform distribution, an exponential distribution and a normal distribution. During training we assign weights wk proportional to the probability that the forced exit\nwill result in an exit after classifier fk. The results in Figure 3 show that the accuracy distributions are somewhat shifted based on the weight assignments, however, no drastic changes occur. This result provides some reassurance that the uniform weighting is probably a good choice in many real-world applications, and that we need not worry to much about errors in our estimates of p(B)."}, {"heading": "5.2. Batch computational budget setting", "text": "In the batch computational budget setting, the predictive model receives a batch ofM instances and a computational budget B for classifying all instances. In this setting, the\nbest classification accuracy may be achieved by performing a type of dynamic evaluation described below.\nDynamic evaluation with early-exits. We early-exit \u201ceasy\u201d examples from a shallow classifier that is likely to produce a correct prediction for such instances with limited computation, and use the computation we saved to evaluate a deeper classifier on \u201chard\u201d examples. Specifically, we associate each classifier with a confidence threshold, and let a test example exit from the first classifier at which it obtains sufficiently confident prediction, i.e., its maximum soft-max value is no less than the threshold at that classifier. The confidence threshold \u03b8k at the kth classifier is determined using the validation set, such that a proportion of qk = z(1 \u2212 q)k\u22121q validation samples exit from this classifier, as defined in eq. (2).\nDetails of architecture. The MSDNets used here for the two CIFAR datasets have depths ranging from 10 to 36 layers, using the reduced network with transition layers as described in Section 4. The kth classifier is attached to the ( \u2211k\ni=1 i) th layer. The network used for ImageNet is the\nsame as that described in the previous subsection.\nBaselines. On the CIFAR datasets, we compare our dynamically evaluated MSDNet with several highly competitive baselines.\nWe include ResNets (He et al., 2016), DenseNets (Huang et al., 2017) and DenseNets* (described in Section 5.1) of varying sizes, Stochastic Depth Networks (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016) and FractalNets (Larsson et al., 2017), with test time costs within our region of interest. For ResNets, DenseNets and DenseNet* we also introduce a cost-accuracy trade-off curve by interpolating between models of varying sizes and test-time costs Ck: Assume a given budgetB falls between the costs of two such networks, such that C1 \u2264 B < C2, and in particularB = \u03b1C1+(1\u2212\u03b1)C2, we randomly select a fraction \u03b1 of all test instances that we classify with the network of cost C1, and a fraction (1\u2212 \u03b1) that we classify with the network of cost C2. We refer to these baselines as DenseNets, ResNets and DenseNet*.\nWe apply the same thresholding approach, as described earlier for MSDNet, to the ResNetMC and DenseNetMC models used in Section 5.1, and perform dynamic evaluation on them. We denote these baselines as ResNetMC / DenseNetMC with early-exits. We do not experiment with deeper models as they are unlikely to be more competitive than other baselines.\nOur final baseline is obtained by applying the dynamic evaluation strategy to an ensemble of independently trained networks with increasing depth (we use the most competitive DenseNet* architecture). If a test input obtains an insufficiently confident prediction at a shallower network, it will be further evaluated by a deeper one, until its prediction is confident enough or it reaches the deepest model. We assign the confidence thresholds as described for MSDNet. It is worth noting that for each test input this baseline re-uses computation by ensembling the predictions of all evaluated networks. We refer to this baseline as Dynamic Ensemble of DenseNets*.\nOn ImageNet, we compare MSDNet with five (interpolated) ResNets models described in Section 5.1, the 121- layer DenseNet (Huang et al., 2017), AlexNet (Krizhevsky et al., 2012) and GoogleLeNet (Szegedy et al., 2015). In addition, we also compare with an ensemble of the five ResNets with dynamic evaluation.\nResults. The results of our experiments with the batch computational budget setting on the CIFAR-10 and CIFAR-100 datasets are shown in the left and middle panel of Figure 4. We trained multiple MSDNets with different depths, each of which meets a certain range of computational budget with varying exit probability q. For any given budget, the best model (shown by darker curves in the figures) is chosen based on the performance on the validation set. We observe that MSDNets consistently outperform all the baselines in terms of accuracy under the same budget. Notably, on both datasets MSDNet yields similar performance as a 110-layer ResNet using only 1/10 of the computational budget. It is also \u223c 5 times more ef-\nficient than DenseNets, Stochastic Depth Networks, Wide ResNets, and FractalNets in terms of FLOPs. The results of ResNet and DenseNet with multiple classification layers are underwhelming, which can probably be explained by the fact that earlier features in traditional convolution networks are not reliable for immediate classification.\nThe dynamically evaluated DenseNets* ensemble (shown by the solid green curve in Figure 4) performs quite competitively on CIFAR-10, suggesting that our early-exit strategy can be potentially applied to an ensemble of convolutional networks of varying sizes. On CIFAR-100 and ImageNet, however, the ensemble only performs on par with its single-model counterpart, potentially because of a lack of consistency in the predictions of the independently trained models. In contrast, MSDNet benefits from the joint training process, leading it to consistently outperform all the baselines on both datasets.\nThe results of MSDNet on ImageNet under the batch computational budget setting are shown in the right panel of Figure 4. The results in the figure reveal a clear trend: MSDNets with dynamic evaluation yield substantially more accurate predictions than ResNets and DenseNets with the same amount of computation. For example, given an average budget of 0.5\u00d71010 FLOPs, MSDNet is able to reach \u223c76% top-1 accuracy, which is \u223c5% higher than that achieved by ResNets with the same number of FLOPs. Compared to the computationally more efficient DenseNets, MSDNet uses 1.6\u00d7 fewer FLOPs to achieve the same classification accuracy.\nTo illustrate the ability of our approach to reduce the computational requirements for classifying \u201ceasy\u201d examples, we show twelve randomly sampled test images from two ImageNet classes in Figure 5. The top row shows \u201ceasy\u201d examples, that were correctly classified and exited after the first classifier. The bottom row shows \u201chard\u201d examples that would have been incorrectly classified by the first classifier but were classified correctly and exited by the final classifier. The figure suggests that early classifiers are used to\nMulti-Scale DenseNets for Efficient Prediction \u201ce as\ny\u201d \u201ch ar d\u201d \u201ce as y\u201d \u201ch ar d\u201d\nFigure 5. Random example images from the ImageNet classes Red wine and Volcano. Top row: images exited from the first classification layer of a MSDNet with correct prediction; Bottom row: images failed to be correctly classified at the first classifier but were correctly predicted and exited at the last layer.\nrapidly classify prototypical examples of a class, whereas the last classifier is used to recognize non-typical images."}, {"heading": "6. Conclusion", "text": "We presented a study on training convolutional networks that are optimized to operate in settings with computational budgets at test time. In particular, we focus on two different computational budget settings, namely, testing under batch computational budgets and anytime prediction. Both settings require individual test samples to take varying amounts of computation time in order to obtain competitive results. We introduce a new convolutional architecture, which incorporates two changes: (1) maintaining multi-scale feature maps in early layers of the convolutional network and (2) using a feature-map concatenation approach that facilitates feature re-use in subsequent layers. The two changes allow us to attach intermediate classifiers throughout the network architecture and avoid wasted computation by re-using feature maps across classifiers. The results of our experiments demonstrate the effectiveness of these changes in settings with computational constraints.\nIn future work, we plan to investigate the effect of these architecture changes in experiments on tasks other than image classification, e.g., image segmentation (Long et al., 2015). We also intend to explore approaches that combine MSDNets, for instance, with model compression (Chen et al., 2015; Han et al., 2015) to further improve computational efficiency."}, {"heading": "Acknowledgements", "text": "The authors are supported in part by the III-1618134, III1526012, IIS-1149882 grants from the National Science Foundation, and the Bill and Melinda Gates Foundation. We also thank Geoff Pleiss, Yu Sun and Wenlin Wang for helpful and interesting discussions."}], "references": [{"title": "End to end learning for self-driving cars", "author": ["Jackel", "Lawrence D", "Monfort", "Mathew", "Muller", "Urs", "Zhang", "Jiakai"], "venue": "arXiv preprint arXiv:1604.07316,", "citeRegEx": "Jackel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jackel et al\\.", "year": 2016}, {"title": "Adaptive neural networks for fast test-time prediction", "author": ["Bolukbasi", "Tolga", "Wang", "Joseph", "Dekel", "Ofer", "Saligrama", "Venkatesh"], "venue": "arXiv preprint arXiv:1702.07811,", "citeRegEx": "Bolukbasi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bolukbasi et al\\.", "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Compressing convolutional neural networks in the frequency domain", "author": ["Chen", "Wenlin", "Wilson", "James", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "In ACM SIGKDD,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In CVPR, pp", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Spatially adaptive computation time for residual networks", "author": ["Figurnov", "Michael", "Collins", "Maxwell D", "Zhu", "Yukun", "Zhang", "Li", "Huang", "Jonathan", "Vetrov", "Dmitry", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1612.02297,", "citeRegEx": "Figurnov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Figurnov et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong", "Yunchao", "Liu", "Yang", "Ming", "Bourdev", "Lubomir"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves and Alex.,? \\Q2016\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2016}, {"title": "Training and investigating residual nets. 2016", "author": ["Gross", "Sam", "Wilber", "Michael"], "venue": "URL http://torch.ch/", "citeRegEx": "Gross et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gross et al\\.", "year": 2016}, {"title": "Speedboost: Anytime prediction with uniform near-optimality", "author": ["Grubb", "Alexander", "Bagnell", "Drew"], "venue": "In AISTATS,", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "coding. CoRR,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "In ICLR,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Optimal brain surgeon and general network pruning", "author": ["Hassibi", "Babak", "Stork", "David G", "Wolff", "Gregory J"], "venue": "In IJCNN,", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In ICCV,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In CVPR,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Deep networks with stochastic depth", "author": ["Huang", "Gao", "Sun", "Yu", "Liu", "Zhuang", "Sedra", "Daniel", "Weinberger", "Kilian Q"], "venue": "In ECCV,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML, pp", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Anytime recognition of objects and scenes", "author": ["Karayev", "Sergey", "Fritz", "Mario", "Darrell", "Trevor"], "venue": "In CVPR, pp", "citeRegEx": "Karayev et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": "Tech Report,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Larsson", "Gustav", "Maire", "Michael", "Shakhnarovich", "Gregory"], "venue": "In ICLR,", "citeRegEx": "Larsson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2017}, {"title": "Optimal brain damage", "author": ["LeCun", "Yann", "Denker", "John S", "Solla", "Sara A", "Howard", "Richard E", "Jackel", "Lawrence D"], "venue": "In NIPS,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Pruning filters for efficient convnets", "author": ["Li", "Hao", "Kadav", "Asim", "Durdanovic", "Igor", "Samet", "Hanan", "Graf", "Hans Peter"], "venue": "In ICLR,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin", "Tsung-Yi", "Maire", "Michael", "Belongie", "Serge", "Hays", "James", "Perona", "Pietro", "Ramanan", "Deva", "Doll\u00e1r", "Piotr", "Zitnick", "C Lawrence"], "venue": "In ECCV,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In CVPR, pp", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Feature-budgeted random forest", "author": ["Nan", "Feng", "Wang", "Joseph", "Saligrama", "Venkatesh"], "venue": "In ICML,", "citeRegEx": "Nan et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Nan et al\\.", "year": 1991}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "In ECCV,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Convolutional neural fabrics", "author": ["Saxena", "Shreyas", "Verbeek", "Jakob"], "venue": "In NIPS,", "citeRegEx": "Saxena et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR, pp", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Supervised sequential classification under budget constraints", "author": ["Trapeznikov", "Kirill", "Saligrama", "Venkatesh"], "venue": "In AISTATS, pp", "citeRegEx": "Trapeznikov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Trapeznikov et al\\.", "year": 2013}, {"title": "Robust real-time object detection", "author": ["Viola", "Paul", "Jones", "Michael"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Viola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2001}, {"title": "Deep learning for content-based image retrieval: A comprehensive study", "author": ["Wan", "Ji", "Wang", "Dayong", "Hoi", "Steven Chu Hong", "Wu", "Pengcheng", "Zhu", "Jianke", "Zhang", "Yongdong", "Li", "Jintao"], "venue": "In ACM Multimedia,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Efficient learning by directed acyclic graph for resource constrained prediction", "author": ["Wang", "Joseph", "Trapeznikov", "Kirill", "Saligrama", "Venkatesh"], "venue": "In NIPS,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Xu", "Zhixiang", "Chapelle", "Olivier", "Weinberger", "Kilian Q"], "venue": "In ICML, pp", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Cost-sensitive tree of classifiers", "author": ["Xu", "Zhixiang", "Kusner", "Matt", "Chen", "Minmin", "Weinberger", "Kilian Q"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "In BMVC,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 30, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 17, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 26, "context": "Recent years have witnessed astonishing progress in accuracy of convolutional neural networks (CNN) on visual object recognition tasks (Krizhevsky et al., 2012; Russakovsky et al., 2015; Szegedy et al., 2015; Huang et al., 2017; Long et al., 2015).", "startOffset": 135, "endOffset": 247}, {"referenceID": 4, "context": "This development was in part driven by several public competition datasets, such as ILSVRC (Deng et al., 2009) and COCO (Lin et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 25, "context": ", 2009) and COCO (Lin et al., 2014), where state-of-the-art models may have even managed to surpass human-level performance (He et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 13, "context": ", 2014), where state-of-the-art models may have even managed to surpass human-level performance (He et al., 2015; 2016).", "startOffset": 96, "endOffset": 119}, {"referenceID": 33, "context": "(Wan et al., 2014).", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "But what if the budget varies or is amortized across test cases? Here, convolutional networks pose an inherent dilemma: Deeper (He et al., 2016) and wider (Zagoruyko & Komodakis, 2016) networks with more parameters are better at prediction (Huang et al.", "startOffset": 127, "endOffset": 144}, {"referenceID": 17, "context": ", 2016) and wider (Zagoruyko & Komodakis, 2016) networks with more parameters are better at prediction (Huang et al., 2017), but only the last layers produce the high-level features required to obtain their high level of accuracy.", "startOffset": 103, "endOffset": 123}, {"referenceID": 17, "context": "To solve this dilemma, we propose a novel convolutional network architecture which combines multi-scale feature maps (Saxena & Verbeek, 2016) with dense connectivity (Huang et al., 2017).", "startOffset": 166, "endOffset": 186}, {"referenceID": 23, "context": "In particular, many studies prune weights (LeCun et al., 1989; Hassibi et al., 1993) during or after training and finetune the resulting smaller models (Han et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 12, "context": "In particular, many studies prune weights (LeCun et al., 1989; Hassibi et al., 1993) during or after training and finetune the resulting smaller models (Han et al.", "startOffset": 42, "endOffset": 84}, {"referenceID": 11, "context": ", 1993) during or after training and finetune the resulting smaller models (Han et al., 2016; Li et al., 2017).", "startOffset": 75, "endOffset": 110}, {"referenceID": 24, "context": ", 1993) during or after training and finetune the resulting smaller models (Han et al., 2016; Li et al., 2017).", "startOffset": 75, "endOffset": 110}, {"referenceID": 15, "context": "For example, the popular knowledge-distillation method (Bucilua et al., 2006; Hinton et al., 2014) trains small student networks to reproduce the output of a much larger teacher network or ensemble.", "startOffset": 55, "endOffset": 98}, {"referenceID": 6, "context": "Most of these studies perform some type of quantization of the network\u2019s weights (Gong et al., 2014) or use hashing (Chen et al.", "startOffset": 81, "endOffset": 100}, {"referenceID": 2, "context": ", 2014) or use hashing (Chen et al., 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 28, "context": ", 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al., 2016; Rastegari et al., 2016).", "startOffset": 83, "endOffset": 128}, {"referenceID": 21, "context": "(2016) recently showed that a binary-weight version of AlexNet (Krizhevsky et al., 2012) can match the performance of its 32-bits floating point counterpart on the ImageNet dataset.", "startOffset": 63, "endOffset": 88}, {"referenceID": 2, "context": ", 2014) or use hashing (Chen et al., 2015; 2016) with some even going as far as to train networks with binary weights (Hubara et al., 2016; Rastegari et al., 2016). Rastegari et al. (2016) recently showed that a binary-weight version of AlexNet (Krizhevsky et al.", "startOffset": 24, "endOffset": 189}, {"referenceID": 19, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 35, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 34, "context": "Various prior studies explore computationally efficient variants of traditional machine-learning models (Viola & Jones, 2001; Grubb & Bagnell, 2012; Karayev et al., 2014; Trapeznikov & Saligrama, 2013; Xu et al., 2012; 2013; Nan et al., 2015; Wang et al., 2015).", "startOffset": 104, "endOffset": 261}, {"referenceID": 22, "context": "Our work is most closely related to recent work on FractalNets (Larsson et al., 2017), which can perform anytime prediction by progressively evaluating subnetworks of the full network.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "Our dynamic evaluation strategy for reducing batch computational cost is closely related to the the adaptive computation time approach (Graves, 2016; Figurnov et al., 2016), and the recently proposed method of adaptively evaluating neural networks (Bolukbasi et al.", "startOffset": 135, "endOffset": 172}, {"referenceID": 1, "context": ", 2016), and the recently proposed method of adaptively evaluating neural networks (Bolukbasi et al., 2017).", "startOffset": 83, "endOffset": 107}, {"referenceID": 5, "context": "The adaptive computation time method (Graves, 2016) and its extension (Figurnov et al., 2016) also perform adaptive evaluation on test examples to save batch computational cost.", "startOffset": 70, "endOffset": 93}, {"referenceID": 17, "context": "We use the same featureconcatenation approach as DenseNets (Huang et al., 2017), which allows us to construct highly compact models.", "startOffset": 59, "endOffset": 79}, {"referenceID": 30, "context": ", 2015) and Inception models (Szegedy et al., 2015), our convolutional network has classifiers operating on feature maps at intermediate layers in the network.", "startOffset": 29, "endOffset": 51}, {"referenceID": 17, "context": "The second change we make is that we use dense connections (Huang et al., 2017) that allow each layer to receive inputs directly from all its previous layers.", "startOffset": 59, "endOffset": 79}, {"referenceID": 14, "context": "(2017), such dense connections (1) significantly reduce redundant computations by encouraging feature reuse and (2) make learning easy by eliminating the gradient vanishing problem in a way similar to residual networks (He et al., 2016).", "startOffset": 219, "endOffset": 236}, {"referenceID": 14, "context": "The second change we make is that we use dense connections (Huang et al., 2017) that allow each layer to receive inputs directly from all its previous layers. As shown by Huang et al. (2017), such dense connections (1) significantly reduce redundant computations by encouraging feature reuse and (2) make learning easy by eliminating the gradient vanishing problem in a way similar to residual networks (He et al.", "startOffset": 60, "endOffset": 191}, {"referenceID": 16, "context": "Following the dense connectivity pattern proposed by Huang et al. (2017), the output feature maps x ` produced at subsequent layers, `>1, and scales, s, are a concatenation of transformed feature maps from all previous feature maps of scale s and s\u2212 1 (if s > 1).", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "Second, we can add a transition layer between two blocks to merge the concatenated features with 1 \u00d7 1 convolution and reduce the number of channels by half, similar to the DenseNet-BC architecture (Huang et al., 2017).", "startOffset": 198, "endOffset": 218}, {"referenceID": 17, "context": "For subsequent feature layers, the transformations h` and h\u0303` are defined following the design in DenseNets (Huang et al., 2017): Conv(1 \u00d7 1)-BN-ReLUConv(3\u00d73)-BN-ReLU.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets.", "startOffset": 155, "endOffset": 174}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images.", "startOffset": 156, "endOffset": 413}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images. Images are flipped horizontally with probability 0.5, and normalized by subtracting channel means and dividing by channel standard deviations. The ImageNet dataset comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images. We hold out 50,000 images from the training set in order to estimate the confidence threshold for classifiers in MSDNet. We adopt the data augmentation scheme of He et al. (2016); Huang et al.", "startOffset": 156, "endOffset": 1029}, {"referenceID": 4, "context": "We perform image classification experiments on the CIFAR-10 (C10), CIFAR-100 (C100) (Krizhevsky & Hinton, 2009), and ILSVRC 2012 (ImageNet) classification (Deng et al., 2009) datasets. The two CIFAR datasets contain 50, 000 training and 10, 000 test images of 32\u00d732 pixels; we hold out 5, 000 training images as a validation set. The datasets comprise 10 and 100 classes, respectively. We follow He et al. (2016) and apply standard dataaugmentation techniques to the training images: images are zero-padded with 4 pixels on each side, and then randomly cropped to produce 32\u00d732 images. Images are flipped horizontally with probability 0.5, and normalized by subtracting channel means and dividing by channel standard deviations. The ImageNet dataset comprises 1, 000 classes, with a total of 1.2 million training images and 50,000 validation images. We hold out 50,000 images from the training set in order to estimate the confidence threshold for classifiers in MSDNet. We adopt the data augmentation scheme of He et al. (2016); Huang et al. (2017) at training time; at test time, we classify a 224\u00d7224 center crop of images that were resized to 256\u00d7256 pixels.", "startOffset": 156, "endOffset": 1050}, {"referenceID": 22, "context": "There exist two convolutional network architectures that are suitable for anytime prediction: namely, FractalNets (Larsson et al., 2017) and deeply supervised networks Lee et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 22, "context": "There exist two convolutional network architectures that are suitable for anytime prediction: namely, FractalNets (Larsson et al., 2017) and deeply supervised networks Lee et al. (2015).", "startOffset": 115, "endOffset": 186}, {"referenceID": 14, "context": "In all experiments, we use the state-of-the-art DenseNet with bottleneck and channel reduction layers, which is referred to as DenseNet-BC by Huang et al. (2017). competitive ensemble of ResNets with varying depths.", "startOffset": 142, "endOffset": 162}, {"referenceID": 13, "context": "All the models are the same as those described by He et al. (2016), except that the ResNet-10 and ResNet26 are variants of ResNet-18 that were modified by removing/adding one residual block from/to each of the last four spatial resolutions in the network.", "startOffset": 50, "endOffset": 67}, {"referenceID": 14, "context": "We include ResNets (He et al., 2016), DenseNets (Huang et al.", "startOffset": 19, "endOffset": 36}, {"referenceID": 17, "context": ", 2016), DenseNets (Huang et al., 2017) and DenseNets* (described in Section 5.", "startOffset": 19, "endOffset": 39}, {"referenceID": 16, "context": "1) of varying sizes, Stochastic Depth Networks (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016) and FractalNets (Larsson et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 22, "context": ", 2016), Wide ResNets (Zagoruyko & Komodakis, 2016) and FractalNets (Larsson et al., 2017), with test time costs within our region of interest.", "startOffset": 68, "endOffset": 90}, {"referenceID": 13, "context": "ResNets (He et al., 2015)", "startOffset": 8, "endOffset": 25}, {"referenceID": 16, "context": "DenseNets (Huang et al., 2016)", "startOffset": 10, "endOffset": 30}, {"referenceID": 30, "context": "GoogLeNet (Szegedy et al., 2015)", "startOffset": 10, "endOffset": 32}, {"referenceID": 21, "context": "AlexNet (Krizhevsky et al., 2012)", "startOffset": 8, "endOffset": 33}, {"referenceID": 17, "context": "1, the 121layer DenseNet (Huang et al., 2017), AlexNet (Krizhevsky et al.", "startOffset": 25, "endOffset": 45}, {"referenceID": 21, "context": ", 2017), AlexNet (Krizhevsky et al., 2012) and GoogleLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 30, "context": ", 2012) and GoogleLeNet (Szegedy et al., 2015).", "startOffset": 24, "endOffset": 46}, {"referenceID": 26, "context": ", image segmentation (Long et al., 2015).", "startOffset": 21, "endOffset": 40}, {"referenceID": 2, "context": "We also intend to explore approaches that combine MSDNets, for instance, with model compression (Chen et al., 2015; Han et al., 2015) to further improve computational efficiency.", "startOffset": 96, "endOffset": 133}, {"referenceID": 10, "context": "We also intend to explore approaches that combine MSDNets, for instance, with model compression (Chen et al., 2015; Han et al., 2015) to further improve computational efficiency.", "startOffset": 96, "endOffset": 133}], "year": 2017, "abstractText": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network\u2019s prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \u201ceasier\u201d and \u201charder\u201d examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available.", "creator": "LaTeX with hyperref package"}}}