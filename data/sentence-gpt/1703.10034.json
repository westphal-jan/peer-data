{"id": "1703.10034", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Probabilistic Line Searches for Stochastic Optimization", "abstract": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization models and using Bayesian optimization.\n\n\n\n\nThe algorithm is an initialization of a single function that has a deterministic function that has the same function as the original function. Its initial order is the order of the number of operations that it takes in each function, each of which is a function, and each function is an inverse function that has a function that has a corresponding function.\nThe deterministic function is a form of the deterministic function that has a deterministic function. As it does not necessarily need to work to produce any given function and cannot be easily controlled, it also has an inverse function that returns the sum of the parameters of those functions.\nIf a deterministic function is an intermediate and a fixed product, a set of parameters can be used to perform the first one. A set of parameters can be used to perform the first one. A set of parameters can be used to perform the second one. A set of parameters can be used to perform the third one.\nTo do this, a simple line search can be conducted on a random string and it is not possible to perform any additional data-driven steps. For example, a search for a string with \"b\" is possible, but it is not possible to perform any additional data-driven steps. The basic function that can be used is the deterministic function that performs the first one, and it is not possible to perform any additional data-driven steps. The functions are also described in the following table.\nThe first one is not necessary. The second one must be performed at a speed that takes in a set of parameters. The third one must be performed at a speed that takes in a set of parameters.\nThe fourth one must be performed at a speed that takes in a set of parameters. The third one must be performed at a speed that takes in a set of parameters. The third one must be performed at a speed that takes in a set of parameters. The third one must be performed at a speed that takes in a set of parameters. The third one must be performed at a speed that takes in a set of parameters. The third one must be", "histories": [["v1", "Wed, 29 Mar 2017 13:43:52 GMT  (3736kb,D)", "https://arxiv.org/abs/1703.10034v1", "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 51 pages, 30 figures"], ["v2", "Fri, 30 Jun 2017 16:18:08 GMT  (4490kb,D)", "http://arxiv.org/abs/1703.10034v2", "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 59 pages, 35 figures"]], "COMMENTS": "Extended version of the NIPS '15 conference paper, includes detailed pseudo-code, 51 pages, 30 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["maren mahsereci", "philipp hennig"], "accepted": true, "id": "1703.10034"}, "pdf": {"name": "1703.10034.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Line Searches for Stochastic Optimization", "authors": ["Maren Mahsereci", "Philipp Hennig"], "emails": ["mmahsereci@tue.mpg.de", "phennig@tue.mpg.de"], "sections": [{"heading": null, "text": "Keywords: stochastic optimization, learning rates, line searches, Gaussian processes, Bayesian optimization"}, {"heading": "1. Introduction", "text": "This work substantially extends the work of Mahsereci and Hennig (2015) published at NIPS 2015. Stochastic gradient descent (sgd, Robbins and Monro, 1951) is currently the standard in machine learning for the optimization of highly multivariate functions if their gradient is corrupted by noise. This includes the online or mini-batch training of neural networks, logistic regression (Zhang, 2004; Bottou, 2010) and variational models (e.g. Hoffman et al., 2013; Hensman et al., 2012; Broderick et al., 2013). In all these cases, noisy gradients arise because an exchangeable loss-function L(x) of the optimization parameters x \u2208 RD, across a large dataset {di}i=1 ...,M , is evaluated only on a subset {dj}j=1,...,m:\nL(x) := 1 M M\u2211 i=1 `(x, di) \u2248 1 m m\u2211 j=1 `(x, dj) =: L\u0302(x) m M. (1)\nIf the indices j are i.i.d. draws from [1,M ], by the Central Limit Theorem, the error L\u0302(x) \u2212 L(x) is unbiased and approximately normal distributed. Despite its popularity and its low cost per step, sgd has well-known deficiencies that can make it inefficient, or at least tedious to use in practice. Two main issues are that, first, the gradient itself, even without noise, is not the optimal search direction; and second, sgd requires a step size (learning rate) that has drastic effect on the algorithm\u2019s efficiency, is often difficult to choose well, and virtually never optimal for each individual descent step. The former issue, adapting the search direction, has been addressed by many authors (see George and Powell, 2006, for an overview). Existing approaches range from lightweight \u2018diagonal\nar X\niv :1\n70 3.\n10 03\n4v 2\n[ cs\n.L G\n] 3\n0 Ju\nn 20\npreconditioning\u2019 approaches like Adam (Kingma and Ba, 2014), AdaGrad (Duchi et al., 2011), and \u2018stochastic meta-descent\u2019 (Schraudolph, 1999), to empirical estimates for the natural gradient (Amari et al., 2000) or the Newton direction (Roux and Fitzgibbon, 2010), to problem-specific algorithms (Rajesh et al., 2013), and more elaborate estimates of the Newton direction (Hennig, 2013). Most of these algorithms also include an auxiliary adaptive effect on the learning rate. Schaul et al. (2013) provided an estimation method to explicitly adapt the learning rate from one gradient descent step to another. Several very recent works have proposed the use of reinforcement learning and \u2018learning-to-learn\u2019 approaches for parameter adaption (Andrychowicz et al., 2016; Hansen, 2016; Li and Malik, 2016). Mostly these methods are designed to work well on a specified subset of optimization problems, which they are also trained on; they thus need to be re-learned for differing objectives. The corresponding algorithms are usually orders of magnitude more expensive than the low-level black box proposed here, and often require a classic optimizer (e.g sgd) to tune their internal hyper-parameters.\nNone of the mentioned algorithms change the size of the current descent step. Accumulating statistics across steps in this fashion requires some conservatism: If the step size is initially too large, or grows too fast, sgd can become unstable and \u2018explode\u2019, because individual steps are not checked for robustness at the time they are taken.\nprevious line search, at t = 0. The upper plot shows function values, the lower plot corresponding gradients. A sequence of extrapolation steps \u00c1,\u00c2 finds a point of positive gradient at \u00c2. It is followed by interpolation steps until an acceptable point \u00b9 is found. Points of insufficient decrease, above the line f(0) + c1tf\n\u2032(0) (white area in upper plot) are excluded by the Armijo condition W-I, while points of steep negative gradient (white area in lower plot) are excluded by the curvature condition W-II (the strong extension of the Wolfe conditions also excludes the light green area in the lower plot). Point \u00b9 is the first to fulfil both conditions, and is thus accepted.\nIn essence, the same problem exists in deterministic (noise-free) optimization problems. There, providing stability is one of several tasks of the line search subroutine. It is a standard constituent of algorithms like the classic nonlinear conjugate gradient (Fletcher and Reeves, 1964) and BFGS (Broyden, 1969; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970)\nmethods (Nocedal and Wright, 1999, \u00a73).1 In the noise-free case, line searches are considered a solved problem (Nocedal and Wright, 1999, \u00a73). But the methods used in deterministic optimization are not stable to noise. They are easily fooled by even small disturbances, either becoming overly conservative or failing altogether. The reason for this brittleness is that existing line searches take a sequence of hard decisions to shrink or shift the search space. This yields efficiency, but breaks hard in the presence of noise. Section 3 constructs a probabilistic line search for noisy objectives, stabilizing optimization methods like the works cited above. As line searches only change the length, not the direction of a step, they could be used in combination with the algorithms adapting sgd\u2019s direction, cited above. In this paper we focus on parameter tuning of the sgd algorithm and leave other search directions to future work."}, {"heading": "2. Connections", "text": ""}, {"heading": "2.1 Deterministic Line Searches", "text": "There is a host of existing line search variants (Nocedal and Wright, 1999, \u00a73). In essence, though, these methods explore a univariate domain \u2018to the right\u2019 of a starting point, until an \u2018acceptable\u2019 point is reached (Figure 1). More precisely, consider the problem of minimizing L(x) : RD _R, with access to \u2207L(x) : RD _RD. At iteration i, some \u2018outer loop\u2019 chooses, at location xi, a search direction si \u2208 RD (e.g. by the BFGS rule, or simply si = \u2212\u2207L(xi) for gradient descent). It will not be assumed that si has unit norm. The line search operates along the univariate domain x(t) = xi + tsi for t \u2208 R+. Along this direction it collects scalar function values and projected gradients that will be denoted f(t) = L(x(t)) and f \u2032(t) = s\u1d40i\u2207L(x(t)) \u2208 R. Most line searches involve an initial extrapolation phase to find a point tr with f\n\u2032(tr) > 0. This is followed by a search in [0, tr], by interval nesting or by interpolation of the collected function and gradient values, e.g. with cubic splines.2"}, {"heading": "2.1.1 The Wolfe Conditions for Termination", "text": "As the line search is only an auxiliary step within a larger iteration, it need not find an exact root of f \u2032; it suffices to find a point \u2018sufficiently\u2019 close to a minimum. The Wolfe conditions (Wolfe, 1969) are a widely accepted formalization of this notion; they consider t acceptable if it fulfills\nf(t) \u2264 f(0) + c1tf \u2032(0) (W-I) and f \u2032(t) \u2265 c2f \u2032(0) (W-II), (2)\nusing two constants 0 \u2264 c1 < c2 \u2264 1 chosen by the designer of the line search, not the user. W-I is the Armijo or sufficient decrease condition (Armijo, 1966). It encodes that acceptable functions values should lie below a linear extrapolation line of slope c1f\n\u2032(0). W-II is the curvature condition, demanding a decrease in slope. The choice c1 = 0 accepts any value below f(0), while c1 = 1 rejects all points for convex functions. For the curvature condition,\n1. In these algorithms, another task of the line search is to guarantee certain properties of the surrounding estimation rule. In BFGS, e.g., it ensures positive definiteness of the estimate. This aspect will not feature here. 2. This is the strategy in minimize.m by C. Rasmussen, which provided a model for our implementation. At the time of writing, it can be found at http://learning.eng.cam.ac.uk/carl/code/minimize/minimize.m\nc2 = 0 only accepts points with f \u2032(t) \u2265 0; while c2 = 1 accepts any point of greater slope than f \u2032(0). W-I and W-II are known as the weak form of the Wolfe conditions. The strong form replaces W-II with |f \u2032(t)| \u2264 c2|f \u2032(0)|. This guards against accepting points of low function value but large positive gradient. Figure 1 shows a conceptual sketch illustrating the typical process of a line search, and the weak and strong Wolfe conditions. The exposition in \u00a73.3 will initially focus on the weak conditions, which can be precisely modeled probabilistically. Section 3.3.1 then adds an approximate treatment of the strong form."}, {"heading": "2.2 Bayesian Optimization", "text": "A recently blossoming sample-efficient approach to global optimization revolves around modeling the objective f with a probability measure p(f); usually a Gaussian process (gp). Searching for extrema, evaluation points are then chosen by a utility functional u[p(f)]. Our line search borrows the idea of a Gaussian process surrogate, and a popular acquisition function, expected improvement (Jones et al., 1998). Bayesian optimization (bo) methods are often computationally expensive, thus ill-suited for a cost-sensitive task like a line search. But since line searches are governors more than information extractors, the kind of sample-efficiency expected of a Bayesian optimizer is not needed. The following sections develop a lightweight algorithm which adds only minor computational overhead to stochastic optimization."}, {"heading": "3. A Probabilistic Line Search", "text": "We now consider minimizing f(t) = L\u0302(x(t)) from Eq. 1. That is, the algorithm can access only noisy function values and gradients yt, y \u2032 t at location t, with Gaussian likelihood\np(yt, y \u2032 t | f) = N ([ yt y\u2032t ] ; [ f(t) f \u2032(t) ] , [ \u03c32f 0 0 \u03c32f \u2032 ]) . (3)\nThe Gaussian form is supported by the Central Limit argument at Eq. 1. The function value yt and the gradient y \u2032 t are assumed independent for simplicity; see \u00a73.4 and Appendix A regarding estimation of the variances \u03c32f , \u03c3 2 f \u2032 , and some further notes on the independence assumption of y and y\u2032. Each evaluation of f(t) uses a newly drawn mini-batch. Our algorithm is modeled after the classic line search routine minimize.m2 and translates each of its ingredients one-by-one to the language of probability. The following table illustrates the four ingredients of the probabilistic line search and their corresponding classic parts.\nbuilding block classic probabilistic\n1) 1D surrogate for objective f(t)\npiecewise cubic splines gp where the mean are piecewise cubic splines\n2) candidate selection one local minimizer of cubic splines xor extrapolation local minimizers of cubic splines and extrapolation\n3) choice of best candidate \u2014\u2014\u2014 bo acquisition function\n4) acceptance criterion classic Wolfe conditions probabilistic Wolfe conditions\nThe table already motivates certain design choices, for example the particular choice of the gp-surrogate for f(t), which strongly resembles the classic design. Probabilistic line searches operate in the same scheme as classic ones: 1) they construct a surrogate for the underlying 1D-function 2) they select candidates for evaluation which can interpolate between datapoints or extrapolate 3) a heuristic chooses among the candidate locations and the function is evaluated there 4) the evaluated points are checked for Wolfe-acceptance. The following sections introduce all of these building blocks with greater detail: A robust yet lightweight Gaussian process surrogate on f(t) facilitating analytic optimization (\u00a7 3.1); a simple Bayesian optimization objective for exploration (\u00a7 3.2); and a probabilistic formulation of the Wolfe conditions as a termination criterion (\u00a7 3.3). Appendix D contains a detailed pseudocode of the probabilistic line search; algorithm 1 very roughly sketches the structure of the probabilistic line search and highlights its essential ingredients."}, {"heading": "3.1 Lightweight Gaussian Process Surrogate", "text": "We model information about the objective in a probability measure p(f). There are two requirements on such a measure: First, it must be robust to irregularity (low and high variability) of the objective. And second, it must allow analytic computation of discrete candidate points for evaluation, because a line search should not call yet another optimization subroutine itself. Both requirements are fulfilled by a once-integrated Wiener process, i.e. a zero-mean Gaussian process prior p(f) = GP(f ; 0, k) with covariance function\nk(t, t\u2032) = \u03b82 [ 1/3 min3(t\u0303, t\u0303\u2032) + 1/2|t\u2212 t\u2032|min2(t\u0303, t\u0303\u2032) ] . (4)\nAlgorithm 1 probLineSearchSketch(f , y0, y \u2032 0, \u03c3f0 , \u03c3f \u20320)\nGP ^initGP(y0, y\u20320, \u03c3f0 , \u03c3f \u20320) T, Y, Y \u2032^initStorage(0, y0, y\u20320) . for observed points t^ 1 . scaled position of initial candidate\nwhile budget not used and no Wolfe-point found do [y, y\u2032] ^ f(t) . evaluate objective T, Y, Y \u2032^updateStorage(t, y, y\u2032) GP ^updateGP(t, y, y\u2032) PWolfe ^probWolfe(T , GP ) . compute Wolfe probability at points in T\nif any PWolfe above Wolfe threshold cW then return Wolfe-point else Tcand ^computeCandidates(GP ) . positions of new candidates EI^expectedImprovement(Tcand, GP ) PW ^probWolfe(Tcand, GP ) t^ where (PW EI) is maximal . find best candidate among Tcand\nend if end while\nreturn observed point in T with lowest gp mean since no Wolfe-point found\nHere t\u0303 := t+ \u03c4 and t\u0303\u2032 := t\u2032 + \u03c4 denote a shift by a constant \u03c4 > 0. This ensures this kernel is positive semi-definite, the precise value \u03c4 is irrelevant as the algorithm only considers positive values of t (our implementation uses \u03c4 = 10). See \u00a73.4 regarding the scale \u03b82. With the likelihood of Eq. 3, this prior gives rise to a gp posterior whose mean function is a cubic spline3 (Wahba, 1990). We note in passing that regression on f and f \u2032 from N observations of pairs (yt, y \u2032 t) can be formulated as a filter (Sa\u0308rkka\u0308, 2013) and thus performed in O(N) time. However, since a line search typically collects < 10 data points, generic gp inference, using a Gram matrix, has virtually the same, low cost.\nBecause Gaussian measures are closed under linear maps (Papoulis, 1991, \u00a710), Eq. 4 implies a Wiener process (linear spline) model on f \u2032:\np(f ; f \u2032) = GP ([\nf f \u2032\n] ; [ 0 0 ] , [ k k\u2202 k\u2202 k\u2202 \u2202 ]) , (5)\n3. Eq. 4 can be generalized to the \u2018natural spline\u2019, removing the need for the constant \u03c4 (Rasmussen and Williams, 2006, \u00a76.3.1). However, this notion is ill-defined in the case of a single observation, which is crucial for the line search.\nnoise (error-bars indicate \u00b1 1 standard deviations); noise free interpolator in dashed gray for comparison. The classic interpolator in dark blue, which exactly matches the observations, becomes unreliable; the gp reacts robustly to noisy observations; the gp-mean still consists of piecewise cubic splines.\nwith (using the indicator function I(x) = 1 if x, else 0)\nk\u2202tt\u2032 := \u2202k(t, t\u2032) \u2202t\u2032\n= \u03b82 [ I(t < t\u2032) t\u03032\n2 + I(t \u2265 t\u2032)\n( t\u0303t\u0303\u2032 \u2212 t\u0303 \u20322\n2 )] k\u2202 tt\u2032 :=\n\u2202k(t, t\u2032) \u2202t\n= \u03b82 [ I(t\u2032 < t) t\u0303\u20322\n2 + I(t\u2032 \u2265 t)\n( t\u0303t\u0303\u2032 \u2212 t\u0303 2\n2\n)] (6)\nk\u2202 \u2202tt\u2032 := \u22022k(t, t\u2032) \u2202t\u2032\u2202t = \u03b82 min(t\u0303, t\u0303\u2032).\nGiven a set of evaluations (t,y,y\u2032) (vectors, with elements ti, yti , y \u2032 ti) with independent likelihood 3, the posterior p(f |y,y\u2032) is a gp with posterior mean function \u00b5 and covariance function k\u0303 as follows: [\n\u00b5(t) \u00b5\u2032(t)\n] = [ ktt k \u2202 tt\nk\u2202 tt k \u2202 \u2202 tt\n] [ ktt + \u03c3 2 fI k \u2202 tt\nk\u2202 tt k \u2202 \u2202 tt + \u03c3 2 f \u2032I ]\u22121 \ufe38 \ufe37\ufe37 \ufe38\n=:g\u1d40(t)\n[ y y\u2032 ] [ k\u0303(t, t\u2032) k\u0303\u2202(t, t\u2032) k\u0303\u2202 (t\u2032, t) k\u0303\u2202 \u2202(t, t\u2032) ] = [ ktt\u2032 k \u2202 tt\u2032 k\u2202 t\u2032t k \u2202 \u2202 tt\u2032 ] \u2212 g\u1d40(t) [ ktt\u2032 k \u2202 tt\u2032 k\u2202 t\u2032t k \u2202 \u2202 tt\u2032 ] (7)\nThe posterior marginal variance will be denoted by V(t) = k\u0303(t, t). To see that \u00b5 is indeed piecewise cubic (i.e. a cubic spline), we note that it has at most three non-vanishing\nderivatives4, because\nk\u2202 2 tt\u2032 := \u22022k(t, t\u2032) \u2202t2 = \u03b82I(t \u2264 t\u2032) k\u22023 tt\u2032 := \u22023k(t, t\u2032) \u2202t3 = \u03b82I(t \u2264 t\u2032)(t\u2032 \u2212 t)\nk\u2202 2 \u2202 tt\u2032 := \u22024k(t, t\u2032) \u2202t2\u2202t\u2032 = \u2212\u03b82I(t \u2264 t\u2032) k\u22023 \u2202tt\u2032 := \u22024k(t, t\u2032) \u2202t3\u2202t\u2032 = 0. (8)\nThis piecewise cubic form of \u00b5 is crucial for our purposes: having collected N values of f and f \u2032, respectively, all local minima of \u00b5 can be found analytically in O(N) time in a single sweep through the \u2018cells\u2019 ti\u22121 < t < ti, i = 1, . . . , N (here t0 = 0 denotes the start location, where (y0, y \u2032 0) are \u2018inherited\u2019 from the preceding line search. For typical line searches N < 10, c.f. \u00a74. In each cell, \u00b5(t) is a cubic polynomial with at most one minimum in the cell, found by an inexpensive quadratic computation from the three scalars \u00b5\u2032(ti), \u00b5\u2032\u2032(ti), \u00b5\u2032\u2032\u2032(ti). This is in contrast to other gp regression models\u2014for example the one arising from a squared exponential kernel\u2014which give more involved posterior means whose local minima can be found only approximately. Another advantage of the cubic spline interpolant is that it does not assume the existence of higher derivatives (in contrast to the Gaussian kernel, for example), and thus reacts robustly to irregularities in the objective. In our algorithm, after each evaluation of (yN , y \u2032 N ), we use this property to compute a short list of candidates for the next evaluation, consisting of the \u2264 N local minimizers of \u00b5(t) and one additional extrapolation node at tmax + \u03b1, where tmax is the currently largest evaluated t, and \u03b1 is an extrapolation step size starting at \u03b1 = 1 and doubled after each extrapolation step. 5\nAnother motivation for using the integrated Wiener process as surrogate for the objective, as well as for the described candidate selection, are classic line searches. There, the 1Dobjective is modeled by piecewise cubic interpolations between neighboring datapoints. In a sense, this is a non-parametric approach, since a new spline is defined, when a datapoint is added. Classic line searches always only deal with one spline at a time, since they are able to collapse all other parts of the search space. For noise free observations, the mean of the posterior gp is identical to the classic cubic interpolations, and thus candidate locations are identical as well; this is illustrated in Figure 3. The non-parametric approach also prevents issues of over-constrained surrogates for more than two datapoints. For example, unless the objective is a perfect cubic function, it is impossible to fit a parametric third order polynomial to it, for more than two noise free observations. All other variability in the objective would need to be explained away by artificially introducing noise on the observations. An integrated Wiener process very naturally extends its complexity with each newly added datapoint without being overly assertive \u2013 the encoded assumption is, that the objective has at least one derivative (which is also observed in this case).\n4. There is no well-defined probabilistic belief over f \u2032\u2032 and higher derivatives\u2014sample paths of the Wiener process are almost surely non-differentiable almost everywhere (Adler, 1981, \u00a72.2). But \u00b5(t) is always a member of the reproducing kernel Hilbert space induced by k, thus piecewise cubic (Rasmussen and Williams, 2006, \u00a76.1). 5. For the integrated Wiener process and heteroscedastic noise, the variance always attains its maximum exactly at the mid-point between two evaluations; including the variance into the candidate selection biases the existing candidates towards the center (additional candidates might occur between evaluations without local minimizer, even for noise free observations/classic line searches). We did not explore this further since the algorithm showed very good sample efficiency already with the adopted scheme.\n2 are computed, compared,\nand the candidate with the higher value of uEI \u00b7 pWolfe is chosen for evaluation. In this example this would be the candidate at tcand1 ."}, {"heading": "3.2 Choosing Among Candidates", "text": "The previous section described the construction of < N + 1 discrete candidate points for the next evaluation. To decide at which of the candidate points to actually call f and f \u2032, we make use of a popular acquisition function from Bayesian optimization. Expected improvement (Jones et al., 1998) is the expected amount, under the gp surrogate, by which the function f(t) might be smaller than a \u2018current best\u2019 value \u03b7 (we set \u03b7 = mini=0,...,N{\u00b5(ti)}, where ti\nare observed locations),\nuEI(t) = Ep(ft |y,y\u2032)[min{0, \u03b7 \u2212 f(t)}]\n= \u03b7 \u2212 \u00b5(t)\n2\n( 1 + erf\n\u03b7 \u2212 \u00b5(t)\u221a 2V(t)\n) + \u221a V(t) 2\u03c0 exp ( \u2212(\u03b7 \u2212 \u00b5(t)) 2 2V(t) ) .\n(9)\nThe next evaluation point is chosen as the candidate maximizing the product of Eq. 9 and Wolfe probability pWolfe, which is derived in the following section. The intuition is that pWolfe precisely encodes properties of desired points, but has poor exploration properties; uEI has better exploration properties, but lacks the information that we are seeking a point with low curvature; uEI thus puts weight on (by W-II) clearly ruled out points. An illustration of the candidate proposal and selection is shown in Figure 4.\nIn principle other acquisition functions (e.g. the upper-confidence bound, gp-ucb (Srinivas et al., 2010)) are possible, which might have a stronger explorative behavior; we opted for uEI since exploration is less crucial for line searches than for general bo and some (e.g. gp-ucb) had one additional parameter to tune. We tracked the sample efficiency of uEI instead and it was very good (low); the experimental Subsection 4.3 contains further comments and experiments on the alternative choices of uEI and p\nWolfe as standalone acquisition functions; they performed equally well (in terms of loss and sample efficiency) to their product."}, {"heading": "3.3 Probabilistic Wolfe Conditions for Termination", "text": "The key observation for a probabilistic extension of the Wolfe conditions W-I and W-II is that they are positivity constraints on two variables at, bt that are both linear projections of the (jointly Gaussian) variables f and f \u2032:\n[ at bt ] = [ 1 c1t \u22121 0 0 \u2212c2 0 1 ] f(0) f \u2032(0) f(t) f \u2032(t)  \u2265 0. (10) The gp of Eq. (5) on f thus implies, at each value of t, a bivariate Gaussian distribution\np(at, bt) = N ([ at bt ] ; [ mat mbt ] , [ Caat C ab t Cbat C bb t ]) , (11)\nwith mat = \u00b5(0)\u2212 \u00b5(t) + c1t\u00b5\u2032(0) mbt = \u00b5\n\u2032(t)\u2212 c2\u00b5\u2032(0) (12) and Caat = k\u030300 + (c1t) 2 k\u0303\u2202 \u220200 + k\u0303tt + 2[c1t(k\u0303 \u2202 00 \u2212 k\u0303\u2202 0t)\u2212 k\u03030t]\nCbbt = c 2 2 k\u0303 \u2202 \u2202 00 \u2212 2c2 k\u0303\u2202 \u22020t + k\u0303\u2202 \u2202tt Cabt = C ba t = \u2212c2(k\u0303\u220200 + c1t k\u0303\u2202 \u220200) + c2 k\u0303\u2202 0t + k\u0303\u2202 t0 + c1t k\u0303\u2202 \u22020t \u2212 k\u0303\u2202tt.\n(13)\nThe quadrant probability pWolfet = p(at > 0 \u2227 bt > 0) for the Wolfe conditions to hold, is an integral over a bivariate normal probability,\npWolfet = \u222b \u221e \u2212 m\na t\u221a Caat\n\u222b \u221e \u2212 m\nb t\u221a Cbbt\nN ([ a b ] ; [ 0 0 ] , [ 1 \u03c1t \u03c1t 1 ]) da db, (14)\nwith correlation coefficient \u03c1t = C ab t / \u221a Caat C bb t . It can be computed efficiently (Drezner and Wesolowsky, 1990), using readily available code.6 The line search computes this probability for all evaluation nodes, after each evaluation. If any of the nodes fulfills the Wolfe conditions with pWolfet > cW , greater than some threshold 0 < cW \u2264 1, it is accepted and returned. If several nodes simultaneously fulfill this requirement, the most recently evaluated node is returned; there are additional safeguards for cases where e.g. no Wolfe-point can be found, which can be deduced from the pseudo-code in Appendix D; they are similar to standard safeguards of classic line search routines (e.g. returning the node of lowest mean). Section 3.4.1 below motivates fixing cW = 0.3. The acceptance procedure is illustrated in Figure 5."}, {"heading": "3.3.1 Approximation for Strong Conditions:", "text": "As noted in Section 2.1.1, deterministic optimizers tend to use the strong Wolfe conditions, which use |f \u2032(0)| and |f \u2032(t)|. A precise extension of these conditions to the probabilistic setting is numerically taxing, because the distribution over |f \u2032| is a non-central \u03c7-distribution, requiring customized computations. However, a straightforward variation to 14 captures the spirit of the strong Wolfe conditions, that large positive derivatives should not be accepted: Assuming f \u2032(0) < 0 (i.e. that the search direction is a descent direction), the strong second Wolfe condition can be written exactly as\n0 \u2264 bt = f \u2032(t)\u2212 c2f \u2032(0) \u2264 \u22122c2f \u2032(0). (15)\nThe value \u22122c2f \u2032(0) is bounded to 95% confidence by\n\u22122c2f \u2032(0) . 2c2(|\u00b5\u2032(0)|+ 2 \u221a V\u2032(0)) =: b\u0304. (16)\nHence, an approximation to the strong Wolfe conditions can be reached by replacing the infinite upper integration limit on b in Eq. 14 with (b\u0304 \u2212 mbt)/ \u221a Cbbt . The effect of this adaptation, which adds no overhead to the computation, is shown in Figure 2 as a dashed line."}, {"heading": "3.4 Eliminating Hyper-parameters", "text": "As a black-box inner loop, the line search should not require any tuning by the user. The preceding section introduced six so-far undefined parameters: c1, c2, cW , \u03b8, \u03c3f , \u03c3f \u2032 . We will now show that c1, c2, cW , can be fixed by hard design decisions: \u03b8 can be eliminated by standardizing the optimization objective within the line search; and the noise levels can be estimated at runtime with low overhead for finite-sum objectives of the form in Eq. 1. The result is a parameter-free algorithm that effectively removes the one most problematic parameter from sgd\u2014the learning rate.\n3.4.1 Design Parameters c1, c2, cW\nOur algorithm inherits the Wolfe thresholds c1 and c2 from its deterministic ancestors. We set c1 = 0.05 and c2 = 0.5. This is a standard setting that yields a \u2018lenient\u2019 line search,\n6. e.g. http://www.math.wsu.edu/faculty/genz/software/matlab/bvn.m\ni.e. one that accepts most descent points. The rationale is that the stochastic aspect of sgd is not always problematic, but can also be helpful through a kind of \u2018annealing\u2019 effect.\nThe acceptance threshold cW is a new design parameter arising only in the probabilistic setting. We fix it to cW = 0.3. To motivate this value, first note that in the noise-free limit, all values 0 < cW < 1 are equivalent, because p\nWolfe then switches discretely between 0 and 1 upon observation of the function. A back-of-the-envelope computation, assuming only two evaluations at t = 0 and t = t1 and the same fixed noise level on f and f\n\u2032 (which then cancels out), shows that function values barely fulfilling the conditions, i.e. at1 = bt1 = 0, can have pWolfe \u223c 0.2 while function values at at1 = bt1 = \u2212 for _ 0 with \u2018unlucky\u2019 evaluations (both function and gradient values one standard-deviation from true value) can achieve pWolfe \u223c 0.4. The choice cW = 0.3 balances the two competing desiderata for precision and recall. Empirically (Fig. 6), we rarely observed values of pWolfe close to this threshold. Even at high evaluation noise, a function evaluation typically either clearly rules out the Wolfe conditions, or lifts pWolfe well above the threshold. A more in-depth analysis of c1, c2, and cW is done in the experimental Section 4.2.1."}, {"heading": "3.4.2 Scale \u03b8", "text": "The parameter \u03b8 of Eq. 4 simply scales the prior variance. It can be eliminated by scaling the optimization objective: We set \u03b8 = 1 and scale yi ^ (yi\u2212y0)/|y\u20320|, y\u2032i ^ y\u2032i/|y\u20320| within the code of the line search. This gives y(0) = 0 and y\u2032(0) = \u22121, and typically ensures the objective ranges in the single digits across 0 < t < 10, where most line searches take place. The division by |y\u20320| causes a non-Gaussian disturbance, but this does not seem to have notable empirical effect."}, {"heading": "3.4.3 Noise Scales \u03c3f , \u03c3f \u2032", "text": "The likelihood 3 requires standard deviations for the noise on both function values (\u03c3f ) and gradients (\u03c3f \u2032). One could attempt to learn these across several line searches. However, in exchangeable models, as captured by Eq. 1, the variance of the loss and its gradient can be estimated directly for the mini-batch, at low computational overhead\u2014an approach already advocated by Schaul et al. (2013). We collect the empirical statistics\nS\u0302(x) := 1\nm m\u2211 j `2(x, yj), and \u2207\u0302S(x) := 1 m m\u2211 j \u2207`(x, yj) 2 (17)\n(where 2 denotes the element-wise square) and estimate, at the beginning of a line search from xk,\n\u03c32f = 1\nm\u2212 1\n( S\u0302(xk)\u2212 L\u0302(xk)2 ) and \u03c32f \u2032 = s 2 i \u1d40 [ 1\nm\u2212 1\n( \u2207\u0302S(xk)\u2212 (\u2207L\u0302(xk)) 2 )] . (18)\nThis amounts to the assumption that noise on the gradient is independent. We finally scale the two empirical estimates as described in Section \u00a73.4.2: \u03c3f ^\u03c3f/|y\u2032(0)|, and ditto for \u03c3f \u2032 . The overhead of this estimation is small if the computation of `(x, yj) itself is more expensive than the summation over j. In the neural network examples N-I and N-II of the experimental Section 4, the additional steps added only \u223c 1% cost overhead to the\nevaluation of the loss. A more general statement about memory and time requirements can be found in Sections 3.6 and 3.7. Of course, this approach requires a mini-batch size m > 1. For single-sample mini-batches, a running averaging could be used instead (single-sample mini-batches are not necessarily a good choice. In our experiments, for example, vanilla sgd with batch size 10 converged faster in wall-clock time than unit-batch sgd). Estimating noise separately for each input dimension captures the often inhomogeneous structure among gradient elements, and its effect on the noise along the projected direction. For example, in deep models, gradient noise is typically higher on weights between the input and first hidden layer, hence line searches along the corresponding directions are noisier than those along directions affecting higher-level weights. A detailed description of the noise estimator can be found in Appendix A."}, {"heading": "3.4.4 Propagating Step Sizes Between Line Searches", "text": "As will be demonstrated in \u00a74, the line search can find good step sizes even if the length of the direction si is mis-scaled. Since such scale issues typically persist over time, it would be wasteful to have the algorithm re-fit a good scale in each line search. Instead, we propagate step lengths from one iteration of the search to another: We set the initial search direction to s0 = \u2212\u03b10\u2207L\u0302(x0) with some initial learning rate \u03b10. Then, after each line search ending at xi = xi\u22121 + t\u2217si, the next search direction is set to si+1 = \u2212\u03b1ext \u00b7 t\u2217\u03b10\u2207L\u0302(xi) (with \u03b1ext = 1.3). Thus, the next line search starts its extrapolation at 1.3 times the step size of its predecessor (Section 4.2.2 for details)."}, {"heading": "3.5 Relation to Bayesian Optimization and Noise-Free Limit", "text": "The probabilistic line search algorithm is closely related to Bayesian optimization (bo) since it approximately minimizes a 1D-objective under potentially noisy function evaluations. It thus uses notions of bo (e.g. a gp-surrogate for the objective, and an acquisition function to discriminate locations for the next evaluation of the loss), but there are some differences concerning the aim, requirements on computational efficiency, and termination condition, which are shortly discussed here: (i) Performance measure: The final performance measure in bo is usually the lowest found value of the objective function. Line searches are subroutines inside of a greedy, iterative optimization machine, which usually performs several thousand steps (and line searches); many, very approximate steps often performs better than taking less, but preciser steps. (ii) Termination: The termination condition of a line search is imposed from the outside in the form of the Wolfe conditions. Stricter Wolfe conditions do not usually improve the performance of the overall optimizer, thus, no matter if a better (lower) minimum could be found, any Wolfe-point is acceptable at all times. (iii) Sample efficiency: Since the last evaluation from the previous line search can be re-used in the current line search, only one additional value and gradient evaluation is enough to terminate the procedure. This \u2018immediate-accept\u2019 is the desired behavior if the learning rate is currently well calibrated. (iv) Locations for evaluation: bo, usually calls an optimizer to maximize some acquisition function, and the preciseness of this optimization is crucial for performance. Line searches just need to find a Wolfe-acceptable point; classic line searches suggest, that it is enough to look at plausible locations, like minimizer of a local interpolator, or some rough extrapolation point; this inexpensive heuristic usually\nworks rather well. (v) Exploration: bo needs to solve an intricate trade-off problem in between exploring enough of the parameters space for possible locations of minima, and exploiting locations around them further. Since line searches are only concerned with finding a Wolfe-point, they do not need to explore the parameter space of possible step sizes to that extend; crucial features are rather the possibility to explore somewhat larger steps than previous ones (which is done by extrapolation-candidates), and likewise to shorted steps (which is done by interpolation-candidates).\nIn the limit of noise free observed gradients and function values (\u03c3f = \u03c3f \u2032 = 0) the probabilistic line search behaves like its classic parent, except for very slight variations in the candidate choice (building block 3): The gp-mean reverts to the classic interpolator; all candidate locations are thus identical, but the probabilistic line search might propose a second option, since (even if there is a local minimizer) it always also proposes an extrapolation candidate. For intuitive purposes, this is illustrated in the following table.\nbuilding block classic probabilistic (noise free)\n1) 1D surrogate for objective f(t)\npiecewise cubic splines gp-mean identical to classic interpolator\n2) candidate selection local minimizer of cubic splines xor extrapolation local minimizer of cubic splines or extrapolation\n3) choice of best candidate \u2014\u2014\u2014 bo acquisition function\n4) acceptance criterion classic Wolfe conditions pWolfe identical to classic Wolfe conditions"}, {"heading": "3.6 Computational Time Overhead", "text": "The line search routine itself has little memory and time overhead; most importantly it is independent of the dimensionality of the optimization problem. After every call of the objective function the gp (\u00a73.1) needs to be updated, which at most is at the cost of inverting a 2N \u00d7 2N -matrix, where N usually is equal to 1, 2, or 3 but never > 10. In addition, the bivariate normal integral pWolfet of Eq. 14 needs to be computed at most N times. On a laptop, one evaluation of pWolfet costs about 100 microseconds. For the choice among proposed candidates (\u00a73.2), again at most N , for each, we need to evaluate pWolfet and uEI(t) (Eq. 9) where the latter comes at the expense of evaluating two error functions. Since all of these computations have a fixed cost (in total some milliseconds on a laptop), the relative overhead becomes less the more expensive the evaluation of \u2207L\u0302(x).\nThe largest overhead actually lies outside of the actual line search routine. In case the noise levels \u03c3f and \u03c3f \u2032 are not known, we need to estimate them. The approach we took is described in Section 3.4.3 where the variance of \u2207L\u0302 is estimated using the sample variance of the mini-batch, each time the objective function is called. Since in this formulation the variance estimation is about half as expensive as one backward pass of the net, the time overhead depends on the relative cost of the feed forward and backward passes (Balles et al., 2016). If forward and backward pass are the same cost, the most straightforward implementation of the variance estimation would make each function call 1.25 times as\nexpensive.7 At the same time though, all exploratory experiments which very considerably increase the time spend when using sgd with a hand tuned learning rate schedule need not be performed anymore. In Section 4.1 we will also see that sgd using the probabilistic line search often needs less function evaluations to converge, which might lead to overall faster convergence in wall clock time than classic sgd in a single run."}, {"heading": "3.7 Memory Requirement", "text": "Vanilla sgd, at all times, keeps around the current optimization parameters x \u2208 RD and the gradient vector \u2207 \u02c6L(x) \u2208 RD. In addition to this, the probabilistic line search needs to store the estimated gradient variances \u03a3\u2032(x) = (1\u2212m)\u22121( \u02c6\u2207S(x)\u2212\u2207L\u0302(x) 2) (Eq. 18) of same size. The memory requirement of sgd+probLS is thus comparable to AdaGrad or Adam. If combined with a search direction other than sgd always one additional vector of size D needs to be stored."}, {"heading": "4. Experiments", "text": "This section reports on an extensive set of experiments to characterise and test the line search. The overall evidence from these tests is that the line search performs well and is relatively insensitive to the choice of its internal hyper-parameters as well the mini-batch size. We performed experiments on two multi-layer perceptrons N-I and N-II; both were trained on two well known datasets MNIST and CIFAR-10.\n\u2022 N-I: fully connected net with 1 hidden layer and 800 hidden units + biases, and 10 output units, sigmoidal activation functions and a cross entropy loss. Structure without biases: 784-800-10. Many authors used similar nets and reported performances.8\n\u2022 N-II: fully connected net with 3 hidden layers and 10 output units, tanh-activation functions and a squared loss. Structure without biases: 784-1000-500-250-10. Similar nets were also used for example in Martens (2010) and Sutskever et al. (2013).\n\u2022 MNIST (LeCun et al., 1998): multi-class classification task with 10 classes: handwritten digits in gray-scale of size 28\u00d7 28 (numbers \u20180\u2019 to \u20199\u2019); training set size 60 000, test set size 10 000.\n\u2022 CIFAR-10 (Krizhevsky and Hinton, 2009): multi-class classification task with 10 classes: color images of natural objects (horse, dog, frog,. . . ) of size 32\u00d7 32; training set size 50 000, test set size 10 000; like other authors, we only used the \u201cbatch 1\u201d sub-set of CIFAR-10 containing 10 000 training examples.\nIn addition we train logistic regressors with sigmoidal output (N-III) on the following binary classification tasks:\n\u2022 Wisconsin Breast Cancer Dataset (WDBC) (Wolberg et al., 2011): binary classification of tumors as either \u2018malignant\u2019 or \u2018benign\u2019. The set consist of 569 examples of which\n7. It is desirable to decrease this value in the future reusing computation results or by approximation but this is beyond this discussion. 8. http://yann.lecun.com/exdb/mnist/\nwe used 169 to monitor generalization performing; thus 400 remain for the training set; 30 features describe for example radius, area, symmetry, et cetera. In comparison to the other datasets and networks, this yields a very low dimensional optimization problem with only 30 (+1 bias) input parameters as well as just a small number of datapoints.\n\u2022 GISETTE (Guyon et al., 2005): binary classification of the handwritten digits \u20184\u2019 and \u20189\u2019. The original 28\u00d7 28 images are taken from the MNIST datset; then the feature set was expanded and consists of the original normalized pixels, plus a randomly selected subset of products of pairs of features, which are slightly biased towards the upper part of the image; in total there are 5000 features, instead of 784 as in the original MNIST. The size of the training set and test set is 6000 and 1000 respectively.\n\u2022 EPSILON: synthetic dataset from the PASCAL Challenge 2008 for binary classification. It consists of 400 000 training set datapoint and 100 000 test set datapoints, each having 2000 features.\nIn the text and figures, sgd using the probabilistic line search will occasionally be denoted as sgd+probLS. Section 4.1 contains experiments on the sensitivity to varying gradient noise levels (mini-batch sizes) performed on both multi-layer perceptrons N-I and N-II, as well as on the logistic regressor N-III. Section 4.2 discusses sensitivity to the hyper-parameters choices introduced in Section 3.4 and Section 4.3 contains additional diagnostics on step size statistics. Each single experiment was performed 10 times with different random seeds that determined the starting weights and the mini-batch selection and seeds were shared across all experiments. We report all results of the 10 instances as well as means and standard deviations."}, {"heading": "4.1 Varying Mini-batch Sizes", "text": "The noise level of the gradient estimate \u2207L\u0302(x) and the loss L\u0302(x) is determined by the mini-batch size m and ultimately there should exist an optimal m that maximizes the optimizer\u2019s performance in wall-clock-time. In practice of course the cost of computing \u2207L\u0302(x) and L\u0302(x) is not necessarily linear in m since it is upper bounded by the memory capacity of the hardware used. We assume here, that the mini-batch size is chosen by the user; thus we test the line search with the default hyper-parameter setting (see Sections 3.4 and 4.2) on four different mini-batch sizes:\n\u2022 m = 10, 100, 200 and 1000 (for MNIST, CIFAR-10, and EPSILON)\n\u2022 m = 10, 50, 100, and 400 (for WDBC and GISETTE)\nwhich correspond to increasing signal-to-noise ratios. Since the training set of WDBC only consists of 400 datapoints, the run with the larges mini-batch size of 400 in fact runs full-batch gradient descent on WDBC; this is not a problem, since\u2014as discussed above\u2014the probabilistic line search can also handle noise free observations.9 We compare to sgd-runs\n9. Since the dataset size M of WDBC is very small, we used the factor (M\u2212m)/(mM) instead of 1/m to scale the sample variances of Eq. 17; for m M both factors are nearly identical. The former measures the noise level relative to the empirical risk, the latter relative to the risk; so both choices are sensible depending on what is the desired objective.\nusing a fixed step size (which is typical for these architectures) and an annealed step size with annealing schedule \u03b1t = \u03b10/t. Because annealed step sizes performed much worse than sgd+fixed step size, we will only report on the latter results in the plots.10 Since classic sgd without the line search needs a hand crafted learning rate we search on exhaustive logarithmic grids of\n\u03b1N-Isgd = [10 \u22125, 5 \u00b7 10\u22125, 10\u22124, 5 \u00b7 10\u22124, 10\u22123, 5 \u00b7 10\u22123, 10\u22122, 5 \u00b7 10\u22122, 10\u22121, 5 \u00b7 10\u22121] \u03b1N-IIsgd = [\u03b1 N-I sgd, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n\u03b1N-IIIsgd = [10 \u22128, 10\u22127, 10\u22126, 10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 100, 101, 102].\nWe run 10 different initialization for each learning rate, each mini-batch size and each net and dataset combination (10 \u00b7 4 \u00b7 (2 \u00b7 10 + 2 \u00b7 17 + 3 \u00b7 11) = 3480 runs in total) for a large enough budget to reach convergence; and report all numbers. Then we perform the same experiments using the same seeds and setups with sgd using the probabilistic line search and compare the results. For sgd+probLS, \u03b1sgd is the initial learning rate which is used in the very first step. After that, the line search automatically adapts the learning rate, and shows no significant sensitivity to its initialization.\nResults of N-I and N-II on both, MNIST and CIFAR-10 are shown in Figures 7, 14, 15, and 16; results of N-III on WDBC, GISETTE and EPSILON are shown in Figures 18, 17, and 19 respectively. All instances (sgd and sgd+probLS) get the same computational budget (number of mini-batch evaluations) and not the same number of optimization steps. The latter would favour the probabilistic line search since, on average, a bit more than one mini-batch is evaluated per step. Likewise, all plots show performance measure versus the number of mini-batch evaluations, which is proportional to the computational cost.\nAll plots show similar results: While classic sgd is sensitive to the learning rate choice, the line search-controlled sgd performs as good, close to, or sometimes even better than the (in practice unknown) optimal classic sgd instance. In Figure 7, for example, sgd+probLS converges much faster to a good test set error than the best classic sgd instance. In all experiments, across a reasonable range of mini-batch sizes m and of initial \u03b1sgd values, the line search quickly identified good step sizes \u03b1t, stabilized the training, and progressed efficiently, reaching test set errors similar to those reported in the literature for tuned versions of these kind of architectures and datasets. The probabilistic line search thus effectively removes the need for exploratory experiments and learning-rate tuning.\nOverfitting and training error curves: The training error of sgd+probLS often plateaus earlier than the one of vanilla sgd, especially for smaller mini-batch sizes. This does not seem to impair the performance of the optimizer on the test set. We did not investigate this further, since it seemed like a nice natural annealing effect; the exact causes are unclear for now. One explanation might be that the line search does indeed improve overfitting, since it tries to measure descent (by Wolfe conditions which rely on the noise-informed gp). This means, that if\u2014close to a minimum\u2014successive acceptance decisions can not identify a descent direction anymore, diffusion might set in."}, {"heading": "4.2 Sensitivity to Design Parameters", "text": "Most, if not all, numerical methods make implicit or explicit choices about their hyperparameters. Most of these are never seen by the user since they are either estimated at run time, or set by design to a fixed, approximately insensitive value. Well known examples are the discount factor in ordinary differential equation solvers (Hairer et al., 1987, \u00a72.4), or the Wolfe parameters c1 and c2 of classic line searches (\u00a73.4.1). The probabilistic line search inherits the Wolfe parameters c1 and c2 from its classical counterpart as well as introducing two more: The Wolfe threshold cW and the extrapolation factor \u03b1ext. cW does not appear in the classical formulation since the objective function can be evaluated exactly and the Wolfe probability is binary (either fulfilled or not). While cW is thus a natural consequence of allowing the line search to model noise explicitly, the extrapolation factor \u03b1ext is the result of the line search favoring shorter steps, which we will discuss below in more detail, but most prominently because of bias in the line search\u2019s first gradient observation.\nIn the following sections we will give an intuition about the task of the most influential design parameters c2, cW , and \u03b1ext, discuss how they affect the probabilistic line search, and validate good design choices through exploring the parameter space and showing insensitivity to most of them. All experiments on hyper-parameter sensitivity were performed training N-II on MNIST with mini-batch size m = 200. For a full search of the parameter space cW -c2-\u03b1ext we performed 4950 runs in total with 495 different parameter combinations. All results are reported."}, {"heading": "4.2.1 Wolfe II Parameter c2 and Wolfe Threshold cW", "text": "As described in Section 3.4, c2 encodes the strictness of the curvature condition W-II. Pictorially speaking, a larger c2 extends the range of acceptable gradients (green shaded are in the lower part of Figure 5) and leads to a lenient line search while a smaller value of c2 shrinks this area, leading to a stricter line search. cW controls how certain we want to be, that the Wolfe conditions are actually fulfilled. In the extreme case of complete uncertainty about the collected gradients and function values (\u03c3f , \u03c3f \u2032 \u2192\u221e) pWolfe will always be < 0.25, if the strong Wolfe conditions are imposed. In the limit of certain observations (\u03c3f , \u03c3f \u2032 \u2192 0)\n10. An example of annealed step size performance can be found in Mahsereci and Hennig (2015).\nextrapolation at all in between successive line searches, the line search becomes unstable. The hyper-parameters adopted in the line search implementation are indicated as vertical dark red line at \u03b1ext = 1.3 and c2 = 0.5.\npWolfe is binary and reverts to the classic Wolfe criteria. An overly strict line search, therefore (e.g. cW = 0.99 and/ or c2 = 0.1), will still be able to optimize the objective function well, but will waste evaluations at the expense of efficiency. Figure 10 explores the c2-cW parameter space (while keeping \u03b1ext fixed at 1.3). The left column shows final test and train set error, the right column the average number of function evaluations per line search, both versus different choices of Wolfe parameter c2. The left column thus shows the overall performance of the optimizer, while the right column is representative for the computational efficiency of the line search. Intuitively, a line search which is minimally invasive (only corrects the learning rate, when it is really necessary) is preferred. Rows in Figure 10 show the same plot for different choices of the Wolfe threshold cW .\nThe effect of strict c2 can be observed clearly in Figure 10 where for smaller values of c2 <\u2248 0.2 the average number of function evaluations spend in one line search goes up slightly in comparison to looser restrictions on c2, while still a very good perfomace is reached in terms of train and test set error. Likewise, the last row of Figure 10 for the extreme value of cW = 0.99 (demanding 99% certainty about the validity if the Wolfe conditions), shows significant loss in computational efficiency having an average number of 7 function evaluations per line search, but still does not break. Lowering this threshold a bit to 90% increases the computational efficiency of the line search to be nearly optimal again.\nIdeally, we want to trade off the desiderata of being strict enough to reject too small and too large steps that prevent the optimizer to converge, but being lenient enough to allow all other reasonable steps, thus increasing computational efficiency. The values cW = 0.3 and c2 = 0.5, which are adopted in our current implementation are marked as dark red vertical lines in Figure 10."}, {"heading": "4.2.2 Extrapolation Factor \u03b1ext", "text": "The extrapolation parameter \u03b1ext, introduced in Section 3.4.4, pushes the line search to try a larger learning rate first, than the one which was accepted in the previous step. Figure 9 is structured like Figure 10, but this time explores the line search sensitivity in the c2-\u03b1ext parameter space (abscissa and rows respectively) while keeping cW fixed at 0.3. Unless we choose \u03b1ext = 1.0 (no step size increase between steps) in combination with a lenient choice of c2 the line search performs well. For now we adopt \u03b1ext = 1.3 as default value which again is shown as dark red vertical line in Figure 9.\nThe introduction of \u03b1ext is a necessity and well-working fix because of a few shortcomings of the current design. First, the curvature condition W-II is the single condition that prevents too small steps and pushes optimization progress. On the other hand both W-I and W-II simultaneously penalize too large steps (see Figure 1 for a sketch). This is not a problem in case of deterministic observation (\u03c3f , \u03c3f \u2032 \u2192 0), where W-II undoubtedly decides if a gradient is still too negative. Unless W-II is chosen very tightly (small c2) or cW unnecessarily large (both choices, as discussed above, are undesirable), in the presence of noise, pWolfe will thus be more reliable in preventing overshooting than pushing progress. The first row of Figure 9 illustrates this behavior, where the performance drops somewhat if no extrapolation is done (\u03b1ext = 1.0) in combination with a looser version of W-II (larger c2).\nAnother factor that contributes towards accepting small rather than larger learning rates is a bias introduced in the first observation of the line search at t = 0. Observations y\u2032(t)\nthat the gp gets to see are projections of the gradient sample \u2207L\u0302(t) onto the search direction s = \u2212\u2207L\u0302(0). Since the first observations y\u2032(0) is computed from the same mini-batch as the search direction (not doing this would double the optimizer\u2019s computational cost) an inevitable bias is introduced of approximate size of cos\u22121(\u03b3) (where \u03b3 is the expected angle between gradient evaluations from two independent mini-batches at t = 0). Since the scale parameter \u03b8 of the Wiener process is implicitly set by y\u2032(0) (\u00a73.4.2), the gp becomes more uncertain at unobserved points than it needs to be; or alternatively expects the 1D-gradient to cross zero at smaller steps, and thus underestimates a potential learning rate. The posterior at observed positions is little affected. The over-estimation of \u03b8 rather pushes the posterior towards the likelihood (since there is less model to trust) and thus still gives a reliable measure for f(t) and f \u2032(t). The effect on the Wolfe conditions is similar. With y\u2032(0) biased towards larger values, the Wolfe conditions, which measure the drop in projected gradient norm, are thus prone to accept larger gradients combined with smaller function values, which again is met by making small steps. Ultimately though, since candidate points at tcand > 0 that are currently queried for acceptance, are always observed and unbiased, this can be controlled by an appropriate design of the Wolfe factor c2 (\u00a73.4.1 and \u00a74.2.1) and of course \u03b1ext."}, {"heading": "4.2.3 Full Hyper-Parameter Search: cW -c2-\u03b1ext", "text": "An exhaustive performance evaluation on the whole cW -c2-\u03b1ext-grid is shown in Appendix C in Figures 20-24 and Figures 25-35. As discussed above, it shows the necessity of introducing the extrapolation parameter \u03b1ext and shows slightly less efficient performance for obviously undesirable parameter combinations. In a large volume of the parameter space, and most importantly in the vicinity of the chosen design parameters, the line search performance is stable and comparable to carefully hand tuned learning rates."}, {"heading": "4.2.4 Safeguarding Mis-scaled gps: \u03b8reset", "text": "For completeness, an additional experiment was performed on the threshold parameter which is denoted by \u03b8reset in the pseudo-code (Appendix D) and safeguards against gp mis-scaling. The introduction of noisy observations necessitates to model the variability of the 1D-function, which is described by the kernel scale parameter \u03b8 (\u00a73.4.2). Setting this hyper-parameter is implicitly done by scaling the observation input, assuming a similar scale than in the previous line search (\u00a73.4.2) . If, for some reason, the previous line search accepted an unexpectedly large or small step (what this means is encoded in \u03b8reset) the gp scale \u03b8 for the next line search is reset to an exponential running average of previous scales (\u03b1stats in the pseudo-code). This occurs very rarely (for the default value \u03b8reset = 100 the reset occurred in 0.02% of all line searches), but is necessary to safeguard against extremely mis-scaled gp\u2019s. \u03b8reset therefore is not part of the probabilistic line search model as such, but prevents mis-scaled gps due to some unlucky observation or sudden extreme change in the learning rate. Figure 8 shows performance of the line search for \u03b8reset = 10, 100, 1000 and 10 000 showing no significant performance change. We adopted \u03b8reset = 100 in our implementation since this is the expected and desired multiplicative (inverse) factor to maximally vary the learning rate in one single step."}, {"heading": "4.3 Candidate Selection and Learning Rate Traces", "text": "In the current implementation of the probabilistic line search, the choice among candidates for evaluation is done by evaluating an acquisition function uEI(t cand i ) \u00b7 pWolfe(tcandi ) at every candidate point tcandi ; then choosing the one with the highest value for evaluation of the objective (\u00a73.2). The Wolfe probability pWolfe actually encodes precisely what kind of point we want to find and incorporates both (W-I and W-II) conditions about the function value and to the gradient (\u00a73.3). However pWolfe does not have very desirable exploration properties. Since the uncertainty of the gp grows to \u2018the right\u2019 of the last observation, the Wolfe probability quickly drops to a low, approximately constant value there (Figure 4). Also\npWolfe is partially allowing for undesirably short steps (\u00a74.2.2). The expected improvement uEI, on the other hand, is a well studied acquisition function of Bayesian optimization trading off exploration and exploitation. It aims to globally find a point with a function value lower than a current best guess. Though this is a desirable property also for the probabilistic line search, it is lacking the information that we are seeking a point that also fulfills the W-II curvature condition. This is evident in Figure 4 where pWolfe significantly drops at points where the objective function is already evaluated but uEI does not. In addition, we do not need to explore the positive t space to an extend, the expected improvement suggests, since the aim of a line search is just to find a good, acceptable point at positive t and not the globally best one. The product of both acquisition function uEI \u00b7 pWolfe is thus a trade-off between exploring enough, but still preventing too much exploitation in obviously undesirable regions. In practice, though, we found that all three choices ((i) uEI \u00b7 pWolfe, (ii) uEI only, (iii) p\nWolfe only) perform comparable. The following experiments were all performed training N-II on MNIST; only the minibatch size might vary as indicated.\nFigure 11 compares all three choices for mini-batch size m = 200 and default design parameters. The top plot shows the evolution of the logarithmic test and train set error (for plot and color description see Figure caption). All test and train set error curves respectively bundle up (only lastly plotted clearly visible). The choice of acquisition function thus does not change the performance here. Rows 2-4 of Figure 11 show learning rate traces of a single seed. All three curves show very similar global behavior. First the learning rate grows, then drops again, and finally settles around the best found constant learning rate. This is intriguing since on average a larger learning rate seems to be better at the beginning of the optimization process, then later dropping again to a smaller one. This might also explain why sgd+probLS in the first part of the optimization progress outperforms vanilla sgd (Figure 7). Runs, that use just slightly larger constant learning rates than the best performing constant one (above the gray horizontal lines in Figure 11) were failing after a few steps. This shows that there is some non-trivial adaptation going on, not just globally, but locally at every step.\nFigure 12 shows traces of accepted learning rates for different mini-batch sizes m = 100, 200, 1000. Again the global behavior is qualitatively similar for all three mini-batch sizes on the given architecture. For the largest mini-batch size m = 1000 (last row of Figure 12) the probabilistic line search accepts a larger learning rate (on average and in absolute value) than for the smaller mini-batch sizes m = 100 and 200, which is in agreement with practical experience and theoretical findings (Hinton (2012, \u00a74 and 7), Goodfellow et al. (2016, \u00a79.1.3), Balles et al. (2016)).\nFigure 13 shows traces of the (scaled) noise levels \u03c3f and \u03c3f \u2032 and the average number of function evaluations per line search for different noise levels (m = 100, 200, 1000); same colors show the same setup but different seeds. The average number of function evaluations rises very slightly to \u2248 1.5\u2212 2 for minibatch size m = 1000 towards the end of the optimization process, in comparison to \u2248 1.5 for m = 100, 200. This seems counter intuitive in a way, but since larger minibatch sizes also observe smaller value and gradients (especially towards the end of the optimization process), the relative noise levels might actually be larger. (Although the curves for varying m are shown versus the same abscissa, the corresponding optimizers might be in different regions of the loss surface, especially m = 1000 probably reaches regions of smaller absolute gradients). At the start of the optimization the average number\nof function evaluations is high, because the initial default learning rate is small (10\u22124) and the line search extends each step multiple times."}, {"heading": "5. Conclusion", "text": "The line search paradigm widely accepted in deterministic optimization can be extended to noisy settings. Our design combines existing principles from the noise-free case with ideas from Bayesian optimization, adapted for efficiency. We arrived at a lightweight \u201cblack-box\u201d algorithm that exposes no parameters to the user. Empirical evaluations so far show compatibility with the sgd search direction and viability for logistic regression and multi-layer perceptrons. The line search effectively frees users from worries about the choice of a learning rate: Any reasonable initial choice will be quickly adapted and lead to close to optimal performance. Our matlab implementation can be found at http: //tinyurl.com/probLineSearch."}, {"heading": "Acknowledgments", "text": "Thanks to Jonas Jaszkowic who prepared the base of the pseudo-code."}, {"heading": "Appendix A. \u2013 Noise Estimation", "text": "Section 3.4.3 introduced the statistical variance estimators\n\u03a3\u2032(x) = (1\u2212m)\u22121(\u2207\u0302S(x)\u2212\u2207L\u0302(x) 2) \u03a3(x) = (1\u2212m)\u22121(S\u0302(x)\u2212 L\u0302(x)2)\n(19)\nof the function and gradient estimate L\u0302(x) and \u2207L\u0302(x) at position x. The underlying assumption is that L\u0302(x) and \u2207L\u0302(x) are distributed according to[\nL\u0302(x) \u2207L\u0302(x)\n] \u223c N ( [ L\u0302(x) \u2207L\u0302(x) ] ; [ L(x) \u2207L(x) ] , [ \u03a3(x) 0D\u00d71 01\u00d7D diag \u03a3\u2032(x) ]) (20)\nwhich implies Eq 3[ L\u0302(x)\ns(x)\u2032 \u00b7 \u2207L\u0302(x)\n] = [ y(x) y\u2032(x) ] \u223c N ( [ y(x) y\u2032(x) ] ; [ f(x) f \u2032(x) ] , [ \u03c3f (x) 0\n0 \u03c3f \u2032(x)\n]) . (21)\nwhere s(x) is the possibly new search direction at x. This is an approximation since the true covariance matrix is in general not diagonal. A better estimator for the projected gradient noise would be (dropping x from the notation)\n\u03b7f \u2032 = s \u1d40\n[ 1\nm\u2212 1 1 m m\u2211 k=1\n(\u2207lk \u2212\u2207L\u0302)(\u2207lk \u2212\u2207L\u0302)\u1d40 ] s\n= D\u2211\ni,j=1\nsisj 1 m\u2212 1 1 m m\u2211 k=1 ( \u2207lki \u2212\u2207L\u0302i )( \u2207lkj \u2212\u2207L\u0302j )\n= 1\nm\u2212 1 D\u2211 i,j=1 sisj\n( 1\nm m\u2211 k=1 \u2207lki\u2207lkj \u2212\u2207L\u0302i\u2207L\u0302j \u2212\u2207L\u0302j\u2207L\u0302i +\u2207L\u0302i\u2207L\u0302j\n)\n= 1\nm\u2212 1  1 m m\u2211 k=1 D\u2211 i,j=1 si\u2207lki sj\u2207lkj \u2212 D\u2211 i,j=1 sj\u2207L\u0302jsi\u2207L\u0302i  = 1\nm\u2212 1\n( 1\nm m\u2211 k=1\n(s\u2032 \u00b7 \u2207lk)2 \u2212 (s\u2032 \u00b7 \u2207L\u0302)2 ) .\n(22)\nComparing to \u03c3f \u2032 yields\n\u03b7f \u2032 = 1\nm\u2212 1 D\u2211 i,j=1 sisj\n( 1\nm m\u2211 k=1 \u2207lki\u2207lkj \u2212\u2207L\u0302j\u2207L\u0302i\n)\n= 1\nm\u2212 1 D\u2211 i=1 s2i\n( 1\nm m\u2211 k=1 (\u2207lki )2 \u2212\u2207L\u03022i\n)\n+ 1\nm\u2212 1 D\u2211 i 6=j=1 sisj\n( 1\nm m\u2211 k=1 \u2207lki\u2207lkj \u2212\u2207L\u0302j\u2207L\u0302i\n)\n\u03b7f \u2032 = \u03c3f \u2032 + 1\nm\u2212 1 D\u2211 i 6=j=1 sisj\n( 1\nm m\u2211 k=1 \u2207lki\u2207lkj \u2212\u2207L\u0302j\u2207L\u0302i\n) .\n(23)\nFrom Eq 22 we see that, in order to effectively compute \u03b7f \u2032 , we need an efficient way of computing the inner product (s\u2032 \u00b7 \u2207lk) for all k. In addition, we need to know the search direction s(x) of the potential next step (if x was accepted) at the time of computing \u03b7f \u2032 . This is possible e.g. for the sgd search direction where s(x) = \u2212 1m \u2211m k=1\u2207lk(x) but potentially not possible or practical for arbitrary search directions. For all experiments in this paper we used the approximate variance estimator \u03c3f \u2032 .\nThe above paragraph analyzed the independence assumption among gradient elements; the following paragraph is concerned with the independence assumption of gradient and function value y and y\u2032: In general y and y\u2032 are not independent since the algorithm draws them from the same minibatch; the likelihood including the correlation factor \u03c1 reads\np(yt, y \u2032 t | f) = N ([ yt y\u2032t ] ; [ f(t) f \u2032(t) ] , [ \u03c32f \u03c1 \u03c1 \u03c32f \u2032 ]) . (24)\nThe noise covariance matrix enters the gp only in the inverse of the sum with the kernel matrix of the observations. We can compute it analytically for one datapoint at position t, since it is only a 2\u00d7 2 matrix. For \u03c1 = 0, define:\ndet\u03c1=0 := [ktt + \u03c3 2 f ][ k \u2202 \u2202 tt + \u03c3 2 f \u2032 ]\u2212 k\u2202tt k\u2202 tt G\u22121\u03c1=0 := [ ktt + \u03c3 2 f k \u2202 tt\nk\u2202 tt k \u2202 \u2202 tt + \u03c3 2 f \u2032\n]\u22121 =\n1\ndet\u03c1=0\n[ k\u2202 \u2202tt + \u03c3 2 f \u2032 \u2212k\u2202tt\n\u2212 k\u2202 tt ktt + \u03c32f\n] .\n(25)\nFor \u03c1 6= 0 we thus get:\ndet\u03c1 6=0 := [ktt + \u03c3 2 f ][ k \u2202 \u2202 tt + \u03c3 2 f \u2032 ]\u2212 [k\u2202tt + \u03c1][ k\u2202 tt + \u03c1]\n= det\u03c1=0 \u2212 \u03c1( k\u2202 tt + k\u2202tt)\u2212 \u03c12 G\u22121\u03c1 6=0 := [ ktt + \u03c3 2 f k \u2202 tt + \u03c1\nk\u2202 tt + \u03c1 k \u2202 \u2202 tt + \u03c3 2 f \u2032\n]\u22121 =\n1\ndet\u03c1 6=0\n[ k\u2202 \u2202tt + \u03c3 2 f \u2032 \u2212(k\u2202tt + \u03c1)\n\u2212( k\u2202 tt + \u03c1) ktt + \u03c32f ] = det\u03c1=0 det\u03c1 6=0 G\u22121\u03c1=0 \u2212 \u03c1 det\u03c16=0 [ 0 1 1 0 ] (26)\nThe fraction det\u03c1=0/det\u03c16=0 in the first term of the last row, is a positive scalar that scales all element of G\u22121\u03c1=0 equally (since G\u03c1=0 and G\u03c16=0 are positive definite matrices, we know that det\u03c1=0 > 0, det\u03c1 6=0 > 0). If |\u03c1| is small in comparison to the determinant det\u03c1=0, then det\u03c1 6=0 \u2248 det\u03c1=0 and the scaling factor is approximately one. The second term corrects off-diagonal elements in G\u03c1 6=0 and is proportional to \u03c1; if |\u03c1| det\u03c1=0 this term is small as well.\nIn might be possible to estimate \u03c1 as well from the minibatch in a similar style to the estimation of \u03c3f and \u03c3f \u2032 ; it is not clear from this analysis, if the additional computational cost would justify the improvements in the gp-inference.\nAppendix B. \u2013 Noise Sensitivity\nAppendix C. \u2013 Parameter Sensitivity\nlog test set error\nlog train set error\n# function evaluations\nper line search averaged over 10 different initializations. Bottom row: corresponding relative standard deviations. In all plots darker colors are better. For extrapolation parameters \u03b1ext > 1 (see Figures 21, 22, 23, and 24) the different parameter combinations all result in similar good performance. Only at extreme choices, for example \u03b1ext = 1.0 (this figure), which amounts to no extrapolation at all in between successive line searches, the line search becomes unstable. At the extreme value of cW = 0.99, which amounts to imposing nearly absolute certainty about the Wolfe conditions, the line search becomes less efficient, though still does not break. In Figure 23 the default values adopted in the line search implementation (cW = 0.3, c2 = 0.5, and \u03b1ext = 1.3) are indicated as red dots.\nlog test set error\nlog train set error\n# function evaluations"}, {"heading": "Appendix D. \u2013 Pseudocode", "text": "Algorithm 1 of Section 3 roughly sketches the structure of the probabilistic line search and its main ingredients. This section provides a detailed pseudocode which can be used for re-implementation. It is based on the code which was used for the experiments in this paper. A matlab implementation including a minimal example can be found at http: //tinyurl.com/probLineSearch. The actual line search routine is called probLineSearch below and is quite short. Most of the pseudocode is occupied with comments, helper function that define the kernel of the gp, the gp-update or Gauss cdf and pdf which we printed here for completeness such that a detailed re-implementation is possible. For better readability of the pseudocode we use the following color coding:\n\u2022 blue: comments\n\u2022 green: variables of the integrated Wiener process.\n\u2022 red: most recently evaluated observation (noisy loss and gradient). If the line search terminates, these will be returned as \u2018accepted\u2019.\n\u2022 orange: inputs from the main solver procedure and unchanged during each line search.\nNotation and operators:\noperator or function definition\nA B elementwise multiplication A B elementwise division A b elementwise power of b A\u2032 transpose of A A \u00b7B scalar-scalar, scalar-matrix or matrix-matrix multiplication A/B right matrix division, the same as A \u00b7B\u22121 A\\B left matrix division, the same as A\u22121 \u00b7B sign(a) sign of scalar a erf(x) error function erf(x) = 2\u221a \u03c0 \u222b x 0 e \u2212t2dt max(A) maximum element in A\nmin(A) minimum element in A |a| absolute value of scalar a A < B elementwise \u2018less\u2019 comparison A \u2264 B elementwise \u2018less-or-equal\u2019 comparison A > B elementwise \u2018greater\u2019 comparison A \u2265 B elementwise \u2018greater-or-equal\u2019 comparison [a, b, c]\u2190 f(x) function f called at x returns the values a, b and c\nFor better readability and to avoid confusion with transposes, we denote derivatives for example as dy and df (instead of y\u2032 and f \u2032 as in the main text).\n1: function SGDSolver(f) 2: I f \u2013 function handle to objective. Usage: [y, dy,\u03a3f ,\u03a3df ] ^ f(x). 3: 4: I initial weights 5: x^initial weights 6: 7: I initial step size (rather small to avoid over-shooting in very first step) 8: \u03b1^ e.g. \u2248 10\u22124 9: \u03b1stats ^\u03b1\n10: 11: I initial function evaluation at x 12: [y, dy,\u03a3f ,\u03a3df ] ^ f(x) 13: 14: I initial search direction 15: d^\u2212dy 16: 17: I loop over line searches 18: while budget not used do 19: 20: I line search finds step size 21: [\u03b1, \u03b1stats, x, y, dy,\u03a3f ,\u03a3df ] ^probLineSearch(x, d, y, dy,\u03a3f ,\u03a3df , \u03b1, \u03b1stats, f) 22: 23: I set new search direction 24: d^\u2212dy 25: end while 26: 27: return x 28: end function\n1: function probLineSearch(x0, d, f0, df0,\u03a3f0 ,\u03a3df0 , \u03b10, \u03b1stats, f) 2: I x0 \u2013 current weights [D \u00d7 1] 3: I f \u2013 function handle to objective. 4: I 5: I d \u2013 search direction [D \u00d7 1] (does not need to be normalized) 6: I f0 \u2013 function value at start, f0 = f(x0) 7: I df0 \u2013 gradient at start, df0 = \u2207f(x0) [D \u00d7 1] 8: I\u03a3f0 \u2013 sample variance of f0 9: I\u03a3df0 \u2013 sample variances of df0, [D \u00d7 1]\n10: I \u03b10 \u2013 initial step size 11: 12: I set maximum # of f evaluations per line search 13: L ^ 6\n14: I scaling and noise level of gp 15: \u03b2^ |d\u2032 \u00b7 \u03a3df0 | . scale factor 16: \u03c3f ^ \u221a \u03a3f0/(\u03b10 \u00b7 \u03b2) . scaled sample variance of f0\n17: \u03c3df ^ \u221a\n((d 2)\u2032 \u00b7 \u03a3df0)/\u03b2 . scaled and projected sample variances of df0 18: 19: I initialize counter and non-fixed parameters 20: N ^ 1 . size of gp= 2 \u00b7N 21: text ^ 1 . scaled step size for extrapolation 22: tt^ 1 . scaled position of first function evaluation 23: 24: I initialize storage for gp. Dynamic arrays of maximum size [L+ 1\u00d7 1] 25: T ^[0] . scaled positions along search direction 26: Y ^[0] . scaled function values at T 27: dY ^[(df0\u2032 \u00b7 d)/\u03b2] . scaled projected gradients at T 28: 29: I initialize gp with observation at start 30: [G,A] ^updateGP(T , Y , dY ,N, \u03c3f , \u03c3df ) 31: 32: I loop until budged is used or acceptable point is found 33: for N from 2 to L + 1 do 34: 35: I evaluate objective function at tt. 36: [y, dy,\u03a3f ,\u03a3df , T , Y , dY ,N ] ^evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f) 37: 38: I update the gp which is now of size 2 \u00b7N . 39: [G,A] ^updateGP(T , Y , dY ,N, \u03c3f , \u03c3df ) 40: 41: I initialize storage for candidates. Dynamic arrays of maximum size [N \u00d7 1]. 42: Tcand ^[ ] . scaled position of candidates 43: Mcand ^[ ] . gp mean of candidates 44: Scand ^[ ] . gp standard deviation of candidates 45: 46: I current point is above the Wolfe threshold? If yes, accept point and return. 47: if probWolfe(tt, T ,A,G) then 48: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 49: return output 50: end if 51: 52: IWolfe conditions not satisfied at this point. 53: I find suitable candidates for next evaluation. 54: 55: I gp mean of function values and corresponding gradients at points in T . 56: M ^ map function m( , T , A) over T 57: dM ^ map function d1m( , T , A) over T\n58: I candidates 1: local minima of gp mean. 59: Tsorted ^ sort T in ascending order 60: TWolfes ^[ ] . prepare list of acceptable points 61: 62: I iterate through all N \u2212 1 cells, compute locations of local minima. 63: for n from 1 to N \u2212 1 do 64: Tn ^ value of Tsorted at n 65: Tn+1 ^ value of Tsorted at n+ 1 66: 67: I add a little offset for numerical stability 68: trep ^Tn + 10\u22126 \u00b7 (Tn+1 \u2212 Tn) 69: 70: I compute location of cubic minimum in nth cell 71: tcubMin ^cubicMinimum(trep, T , A,N) 72: 73: I add point to candidate list if minimum lies in between Tn and Tn+1 74: if tcubMin > Tn and tcubMin < Tn+1 then 75: if (not isnanOrIsinf(tcubMin)) and (tcubMin > 0) then 76: Tcand ^ append tcubMin 77: Mcand ^ append m(tcubMin, T , A) 78: Scand ^ append V(tcubMin, T ,G) 79: end if 80: else 81: 82: I most likely uphill? If yes, break. 83: if n = 1 and d1m(0, T , A) > 0 then 84: r^ 0.01 85: tt^ r \u00b7 (Tn + Tn+1) 86: 87: I evaluate objective function at tt and return. 88: [y, dy,\u03a3f ,\u03a3df , T , Y , dY ,N ] ^evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f) 89: 90: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 91: return output 92: end if 93: end if 94: 95: I check whether there is an acceptable point among the old evaluations 96: if n > 1 and probWolfe(Tn, T , A,G) then 97: TWolfes ^ append Tn 98: end if 99: end for 100: 101: I check if acceptable points exists and return 102: if TWolfes is not empty then\n103: I if last evaluated point is among acceptable ones, return it. 104: if tt in TWolfes then 105: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 106: return output 107: end if 108: 109: I else, choose the one with the lowest gp mean and re-evaluate its gradient. 110: MWolfes ^ map m( , T , A) over TWolfes 111: tt^ value of TWolfes at index of min(MWolfes) 112: 113: I evaluate objective function at tt. 114: [y, dy,\u03a3f ,\u03a3df , T , Y , dY ,N ] ^evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f) 115: 116: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 117: return output 118: end if 119: 120: I candidates 2: one extrapolation step 121: Tcand ^ append max(T ) + text 122: Mcand ^ append m(max(T ) + text, T , A) 123: Scand ^ append V(max(T ) + text, T ,G) 1 2 124: 125: I find minimal mean among M . 126: \u00b5EI ^ minimal value of M 127: 128: I compute expected improvement and Wolfe probabilities at Tcand 129: EIcand ^expectedImprovement(Mcand, Scand, \u00b5EI) 130: PWcand ^ map probWolfe( , T , A,G) over Tcand 131: 132: I choose point among candidates that maximizes EIcand \u2227 PWcand 133: ibestCand ^ index of max(EIcand PWcand) 134: ttbestCand ^ value of Tcand at ibestCand 135: 136: I extend extrapolation step if necessary 137: if ttbestCand is equal to tt+ text then 138: text ^ 2 \u00b7 text 139: end if 140: 141: I set location for next evaluation 142: tt^ ttbestCand 143: end for 144: 145: I limit reached: evaluate a final time and return the point with lowest gp mean 146: [y, dy,\u03a3f ,\u03a3df , T , Y , dY ,N ] ^evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f)\n147: I update the gp which is now of size 2 \u00b7N . 148: [G,A] ^updateGP(T , Y , dY ,N, \u03c3f , \u03c3df ) 149: 150: I check last point for acceptance 151: if probWolfe(tt, T ,A,G) then 152: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 153: return output 154: end if 155: 156: I at the end of budget return point with the lowest gp mean 157: I compute gp means at T 158: M ^ map m( , T , A) over T 159: ilowest ^ index of minimal value in M 160: tlowest ^ value of T at ilowest 161: 162: I if tlowest is the last evaluated point, return 163: if tlowest is equal to tt then 164: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 165: return output 166: end if 167: 168: I else, re-evaluate its gradient and return 169: tt^ value of tlowest 170: 171: I evaluate objective function at tt. 172: [y, dy,\u03a3f ,\u03a3df , T , Y , dY ,N ] ^evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f) 173: 174: output^rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2) 175: return output 176: end function\n1: function rescaleOutput(x0, f0, \u03b10, d, tt, y, dy,\u03a3f ,\u03a3df , \u03b2, \u03b1stats) 2: I design parameters 3: \u03b1ext ^ 1.3 . extrapolation parameter 4: \u03b8reset ^ 100 . reset threshold for gp scale 5: 6: I rescale accepted step size 7: \u03b1acc ^ tt \u00b7 \u03b10 8:\n9: I update weights 10: xacc ^x0 + \u03b1acc \u00b7 d 11: 12: I rescale accepted function value 13: facc ^ y \u00b7 (\u03b10 \u00b7 \u03b2) + f0\n14: I accepted gradient 15: dfacc ^ dy 16: 17: I sample variance of facc 18: \u03a3facc ^ \u03a3f 19: 20: I sample variances of dfacc 21: \u03a3dfacc ^ \u03a3df 22: 23: I update exponential running average of scalings 24: \u03b3^ 0.95 25: \u03b1stats ^ \u03b3 \u00b7 \u03b1stats + (1\u2212 \u03b3) \u00b7 \u03b1acc 26: 27: I next initial step size 28: \u03b1next ^\u03b1acc \u00b7 \u03b1ext 29: 30: I if new gp scaling is drastically different than previous ones reset it. 31: if (\u03b1next < \u03b1stats/\u03b8reset) or (\u03b1next > \u03b1stats \u00b7 \u03b8reset) then 32: \u03b1next ^\u03b1stats 33: end if 34: 35: I compressed output for readability of pseudocode 36: output^[\u03b1next, \u03b1stats, xacc, facc, dfacc,\u03a3facc ,\u03a3dfacc ] 37: 38: return output 39: end function\n1: function evaluateObjective(tt, x0, \u03b10, d, T , Y , dY ,N, \u03b2, f) 2: I evaluate objective function at tt 3: [y, dy,\u03a3f ,\u03a3df ] ^ f(x0 + tt \u00b7 \u03b10 \u00b7 d) 4: 5: I scale output 6: y^(y \u2212 f0)/(\u03b10 \u00b7 \u03b2) 7: dy^(dy\u2032 \u00b7 d)/\u03b2 8:\n9: I storage 10: T ^ append tt 11: Y ^ append y 12: dY ^ append dy 13: N ^N + 1 14: 15: return [y, dy,\u03a3f ,\u03a3df , T, Y, dY,N ] 16: end function\n1: function cubicMinimum(t, T ,A,N) 2: I compute necessary derivatives of gp mean at t 3: d1mt ^d1m(t, T ,A) 4: d2mt ^d2m(t, T ,A) 5: d3mt ^d3m(t, T ,A,N) 6: a^ 0.5 \u00b7 d3mt 7: b^ d2mt \u2212 t \u00b7 d3mt 8: c^ d1mt \u2212 d2mt \u00b7 t+ 0.5 \u00b7 d3mt \u00b7 t2 9:\n10: I third derivative is almost zero \u2192 essentially a quadratic, single extremum 11: if |d3mt| < 1\u22129 then 12: tcubMin ^\u2212(d1mt \u2212 t \u00b7 d2mt)/d2mt 13: return tcubMin 14: end if 15: 16: I roots are complex, no extremum 17: \u03bb^ b2 \u2212 4 \u00b7 a \u00b7 c 18: if \u03bb < 0 then 19: tcubMin ^ +\u221e 20: return tcubMin 21: end if 22: 23: I compute the two possible roots 24: LR^(\u2212b\u2212 sign(a) \u00b7 \u221a \u03bb)/(2 \u00b7 a) . left root 25: RR^(\u2212b+ sign(a) \u00b7 \u221a \u03bb)/(2 \u00b7 a) . right root\n26: 27: I calculate the two values of the cubic at those points (up to a constant) 28: dtL ^LR\u2212 t . distance to left root 29: dtR ^RR\u2212 t . distance to right root 30: CVL ^ d1mt \u00b7 dtL + 0.5 \u00b7 d2mt \u00b7 dt2L + (d3mt \u00b7 dt3L)/6 . left cubic value 31: CVR ^ d1mt \u00b7 dtR + 0.5 \u00b7 d2mt \u00b7 dt2R + (d3mt \u00b7 dt3R)/6 . right cubic value 32: 33: I find the minimum and return it. 34: if CVL < CVR then 35: tcubMin ^LR 36: else 37: tcubMin ^RR 38: end if 39: 40: return tcubMin 41: 42: end function\n1: function updateGP(T , Y , dY ,N, \u03c3f , \u03c3df ) 2: I initialize kernel matrices 3: kTT ^[N \u00d7N ] matrix with zeros . covariance of function values 4: kdTT ^[N \u00d7N ] matrix with zeros . covariance of function values and gradients 5: dkdTT ^[N \u00d7N ] matrix with zeros . covariance of gradients 6: 7: I fill kernel matrices 8: for i = 1 to N do 9: for j = 1 to N do\n10: kTT (i, j) ^k(T (i), T (j)) 11: kdTT (i, j) ^kd(T (i), T (j)) 12: dkdTT (i, j) ^dkd(T (i), T (j)) 13: end for 14: end for 15: 16: I build diagonal covariance matrix of Gaussian likelihood [2N \u00d7 2N ].\n17: \u039b ^ [ diag(\u03c3f\n2)N\u00d7N 0N\u00d7N 0N\u00d7N diag(\u03c3df 2)N\u00d7N ] 18:\n19: G^ (\nkTT kdTT kd\u2032TT dkdTT\n) + \u039b . [2N \u00d7 2N ] matrix\n20: 21: I residual between observed and predicted data 22: \u2206 ^ (\nY dY\n) . [2N \u00d7 1] vector\n23: 24: I compute weighted observations A. 25: A^G\\\u2206 . [2N \u00d7 1] vector 26: 27: return [G,A] 28: 29: end function\n1: function m(t, T ,A) 2: I posterior mean at t 3: return [k(t, T \u2032), kd(t, T \u2032)] \u00b7A 4: end function 5: 6: function d1m(t, T ,A) 7: I first derivative of mean at t 8: return [dk(t, T \u2032), dkd(t, T \u2032)] \u00b7A 9: end function\n10: 11: function d2m(t, T ,A) 12: I second derivative of mean at t 13: return [ddk(t, T \u2032), ddkd(t, T \u2032)] \u00b7A 14: end function 15: 16: function d3m(t, T ,A,N) 17: I third derivative of mean at t 18: return [dddk(t, T \u2032), zeros(1, N)] \u00b7A 19: end function 20: 21: function V(t, T ,G) 22: I posterior variance of function values at t 23: return k(t, t)\u2212[k((t, T \u2032), kd(t, T \u2032)] \u00b7 (G\\[k(t, T \u2032), kd(t, T \u2032)]\u2032) 24: end function 25: 26: function Vd(t, T ,G) 27: I posterior variance of function values and derivatives at t 28: return kd(t, t)\u2212[k(t, T \u2032), kd(t, T \u2032)] \u00b7 (G\\[dk(t, T \u2032), dkd(t, T \u2032)]\u2032) 29: end function 30: 31: function dVd(t, T ,G) 32: I posterior variance of derivatives at t 33: return dkd(t, t)\u2212[dk(t, T \u2032), dkd(t, T \u2032)] \u00b7 (G\\[dk(t, T \u2032), dkd(t, T \u2032)]\u2032) 34: end function 35: 36: function V0f(t, T ,G) 37: I posterior covariances of function values at t = 0 and t 38: return k(0, t)\u2212[k(0, T \u2032), kd(0, T \u2032)] \u00b7 (G\\[k(t, T \u2032), kd(t, T \u2032)]\u2032) 39: end function 40: 41: function Vd0f(t, T ,G) 42: I posterior covariance of gradient and function value at t = 0 and t respectively 43: return dk(0, t)\u2212[dk(0, T \u2032), dkd(0, T \u2032)] \u00b7 (G\\[k(t, T \u2032), kd(t, T \u2032)]\u2032) 44: end function\n45: function V0df(t, T ,G) 46: I posterior covariance of function value and gradient at t = 0 and t respectively 47: return kd(0, t) \u2212[k(0, T \u2032), kd(0, T \u2032)] \u00b7 (G\\[dk(t, T \u2032), dkd(t, T \u2032)]\u2032) 48: end function 49: 50: function Vd0df(t, T ,G) 51: I same as V0f( ) but for gradients 52: return dkd(0, t)\u2212[dk(0, T \u2032), dkd(0, T \u2032)] \u00b7 (G\\[dk(t, T \u2032), dkd(t, T \u2032)]\u2032) 53: end function\n1: \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 2: I all following procedures use the same design parameter: 3: \u03c4 ^ 10 4: \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 5: function k(a, b) 6: IWiener kernel integrated once in each argument 7: return 1/3 min(a+ \u03c4 , b+ \u03c4) 3 + 0.5 |a\u2212 b| min(a+ \u03c4 , b+ \u03c4) 2 8: end function 9:\n10: function kd(a, b) 11: IWiener kernel integrated in first argument 12: return 0.5 (a < b) (a+ \u03c4) 2 + (a \u2265 b) ( (a+ \u03c4) \u00b7 (b+ \u03c4)\u2212 0.5 (b+ \u03c4) 2 ) 13: end function 14: 15: function dk(a, b) 16: IWiener kernel integrated in second argument 17: return 0.5 (a > b) (b+ \u03c4) 2 + (a \u2264 b) ((a+ \u03c4) \u00b7 (b+ \u03c4)\u2212 0.5 (a+ \u03c4) 2) 18: end function 19: 20: function dkd(a, b) 21: IWiener kernel 22: return min(a+ \u03c4 , b+ \u03c4) 23: end function 24: 25: function ddk(a, b) 26: IWiener kernel integrated in second argument and 1x derived in first argument 27: return (a \u2264 b) (b\u2212 a) 28: end function 29: 30: function ddkd(a, b) 31: IWiener kernel 1x derived in first argument 32: return (a \u2264 b) 33: end function\n34: function dddk(a, b) 35: IWiener kernel 2x derived in first argument and integrated in second argument 36: return \u2212(a \u2264 b) 37: end function\n1: function probWolfe(t, T ,A,G) 2: I design parameters 3: c1 ^ 0.05 . constant for Armijo condition 4: c2 ^ 0.5 . constant for curvature condition 5: cW ^ 0.3 . threshold for Wolfe probability 6: 7: I mean and covariance values at start position (t = 0) 8: m0 ^ m(0, T , A) 9: dm0 ^d1m(0, T , A)\n10: V0 ^ V(0, T ,G) 11: V d0 ^ Vd(0, T ,G) 12: dV d0 ^ dVd(0, T ,G) 13: 14: I marginal mean and variance for Armijo condition 15: ma ^m0\u2212m(t, T ,A)+c1 \u00b7 t \u00b7 dm0 16: Vaa ^V0 + (c1 \u00b7 t)2 \u00b7 dV d0+V(t)+2 \u00b7 (c1 \u00b7 t \u00b7 (V d0\u2212Vd0f(t))\u2212V0f(t)) 17: 18: I marginal mean and variance for curvature condition 19: mb ^d1m(t)\u2212c2 \u00b7 dm0 20: Vbb ^ c2 2 \u00b7 dV d0 \u2212 2 \u00b7 c2 \u00b7Vd0df(t)+dVd(t) 21: 22: I covariance between conditions 23: Vab ^\u2212c2 \u00b7 (V d0 + c1 \u00b7 t \u00b7 dV d0) + c2 \u00b7Vd0f(t)+V0df(t)+c1 \u00b7 t\u00b7Vd0df(t)\u2212Vd(t) 24: 25: I extremely small variances \u2192 very certain (deterministic evaluation) 26: if Vaa \u2264 10\u22129 and Vbb \u2264 10\u22129 then 27: pWolfe ^(ma \u2265 0) \u00b7 (mb \u2265 0) 28: 29: I accept? 30: pacc ^ pWolfe > cW 31: return pacc 32: end if 33: 34: I zero or negative variances (maybe something went wrong?) 35: if Vaa \u2264 0 or Vbb \u2264 0 then 36: return 0 37: end if\n38: I noisy case (everything is alright) 39: I correlation 40: \u03c1^Vab/ \u221a Vaa \u00b7 Vbb 41: 42: I lower and upper integral limits for Armijo condition 43: lowa ^\u2212ma/ \u221a Vaa 44: upa ^ +\u221e 45: 46: I lower and upper integral limits for curvature condition 47: lowb ^\u2212mb/ \u221a Vbb\n48: upb ^ ( 2 \u00b7 c2 \u00b7 ( |dm0|+ 2 \u00b7 \u221a dV d0 ) \u2212mb ) / \u221a Vbb 49: 50: I compute Wolfe probability 51: pWolfe ^bvn(lowa, upa, lowb, upb, \u03c1) 52: 53: I accept? 54: pacc ^ pWolfe > cW 55: return pacc 56:\n57:\nThe function bvn(lowa, upa, lowb, upb, \u03c1) evaluates the 2D-integral\u222b upa lowa \u222b upb lowb N ([ a b ] ; [ 0 0 ] , [ 1 \u03c1 \u03c1 1 ]) dadb.\n58: 59: end function\n1: function gaussCDF(z) 2: I Gauss cumulative density function 3: return 0.5 ( 1 + erf(z/ \u221a 2) ) 4: end function 5: 6: function gaussPDF(z) 7: I Gauss probability density function 8: return exp ( \u22120.5 z 2 ) \u221a\n2\u03c0 9: end function\n10: 11: function expectedImprovement(m, s, \u03b7) 12: I Jones et al. (1998) 13: return (\u03b7 \u2212m) gaussCDF((\u03b7 \u2212m) s) +s gaussPDF((\u03b7 \u2212m) s) 14: end function"}], "references": [{"title": "Adaptive method of realizing natural gradient learning", "author": ["R.J. Adler"], "venue": "The Geometry of Random Fields. Wiley,", "citeRegEx": "Adler.,? \\Q1981\\E", "shortCiteRegEx": "Adler.", "year": 1981}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "e-prints,", "citeRegEx": "Bottou.,? \\Q2016\\E", "shortCiteRegEx": "Bottou.", "year": 2016}, {"title": "Result analysis of the nips 2003 feature selection", "author": ["deeplearningbook.org. I. Guyon", "S. Gunn", "A. Ben-Hur", "G.n Dror"], "venue": null, "citeRegEx": "Guyon et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guyon et al\\.", "year": 2003}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Neural Information Processing Systems", "citeRegEx": "Martens.,? \\Q2015\\E", "shortCiteRegEx": "Martens.", "year": 2015}, {"title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Spline models for observational data. Number 59 in CBMS-NSF Regional Conferences series in applied mathematics", "author": ["G. Wahba"], "venue": null, "citeRegEx": "Wahba.,? \\Q1990\\E", "shortCiteRegEx": "Wahba.", "year": 1990}, {"title": "UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set, January 2011. URL http://archive.ics.uci.edu/ ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "author": ["W.H. Wolberg", "W.N Street", "O.L. Mangasarian"], "venue": null, "citeRegEx": "Wolberg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wolberg et al\\.", "year": 2011}, {"title": "Convergence conditions for ascent methods", "author": ["P. Wolfe"], "venue": "SIAM Review, pages 226\u2013235,", "citeRegEx": "Wolfe.,? \\Q1969\\E", "shortCiteRegEx": "Wolfe.", "year": 1969}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In Twenty-first International Conference on Machine Learning", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "This includes the online or mini-batch training of neural networks, logistic regression (Zhang, 2004; Bottou, 2010) and variational models (e.", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "The Wolfe conditions (Wolfe, 1969) are a widely accepted formalization of this notion; they consider t acceptable if it fulfills f(t) \u2264 f(0) + c1tf \u2032(0) (W-I) and f \u2032(t) \u2265 c2f \u2032(0) (W-II), (2) using two constants 0 \u2264 c1 < c2 \u2264 1 chosen by the designer of the line search, not the user.", "startOffset": 21, "endOffset": 34}, {"referenceID": 7, "context": "3, this prior gives rise to a gp posterior whose mean function is a cubic spline3 (Wahba, 1990).", "startOffset": 82, "endOffset": 95}, {"referenceID": 5, "context": "the upper-confidence bound, gp-ucb (Srinivas et al., 2010)) are possible, which might have a stronger explorative behavior; we opted for uEI since exploration is less crucial for line searches than for general bo and some (e.", "startOffset": 35, "endOffset": 58}, {"referenceID": 9, "context": "To motivate this value, first note that in the noise-free limit, all values 0 < cW < 1 are equivalent, because p Wolfe then switches discretely between 0 and 1 upon observation of the function. A back-of-the-envelope computation, assuming only two evaluations at t = 0 and t = t1 and the same fixed noise level on f and f \u2032 (which then cancels out), shows that function values barely fulfilling the conditions, i.e. at1 = bt1 = 0, can have pWolfe \u223c 0.2 while function values at at1 = bt1 = \u2212 for _ 0 with \u2018unlucky\u2019 evaluations (both function and gradient values one standard-deviation from true value) can achieve pWolfe \u223c 0.4. The choice cW = 0.3 balances the two competing desiderata for precision and recall. Empirically (Fig. 6), we rarely observed values of pWolfe close to this threshold. Even at high evaluation noise, a function evaluation typically either clearly rules out the Wolfe conditions, or lifts pWolfe well above the threshold. A more in-depth analysis of c1, c2, and cW is done in the experimental Section 4.2.1. 3.4.2 Scale \u03b8 The parameter \u03b8 of Eq. 4 simply scales the prior variance. It can be eliminated by scaling the optimization objective: We set \u03b8 = 1 and scale yi ^ (yi\u2212y0)/|y\u2032 0|, y\u2032 i ^ y\u2032 i/|y\u2032 0| within the code of the line search. This gives y(0) = 0 and y\u2032(0) = \u22121, and typically ensures the objective ranges in the single digits across 0 < t < 10, where most line searches take place. The division by |y\u2032 0| causes a non-Gaussian disturbance, but this does not seem to have notable empirical effect. 3.4.3 Noise Scales \u03c3f , \u03c3f \u2032 The likelihood 3 requires standard deviations for the noise on both function values (\u03c3f ) and gradients (\u03c3f \u2032). One could attempt to learn these across several line searches. However, in exchangeable models, as captured by Eq. 1, the variance of the loss and its gradient can be estimated directly for the mini-batch, at low computational overhead\u2014an approach already advocated by Schaul et al. (2013). We collect the empirical statistics", "startOffset": 113, "endOffset": 1967}, {"referenceID": 3, "context": "\u2022 CIFAR-10 (Krizhevsky and Hinton, 2009): multi-class classification task with 10 classes: color images of natural objects (horse, dog, frog,.", "startOffset": 11, "endOffset": 40}, {"referenceID": 8, "context": "In addition we train logistic regressors with sigmoidal output (N-III) on the following binary classification tasks: \u2022 Wisconsin Breast Cancer Dataset (WDBC) (Wolberg et al., 2011): binary classification of tumors as either \u2018malignant\u2019 or \u2018benign\u2019.", "startOffset": 158, "endOffset": 180}, {"referenceID": 3, "context": "Similar nets were also used for example in Martens (2010) and Sutskever et al.", "startOffset": 43, "endOffset": 58}, {"referenceID": 3, "context": "Similar nets were also used for example in Martens (2010) and Sutskever et al. (2013). \u2022 MNIST (LeCun et al.", "startOffset": 43, "endOffset": 86}, {"referenceID": 9, "context": "4), or the Wolfe parameters c1 and c2 of classic line searches (\u00a73.4.1). The probabilistic line search inherits the Wolfe parameters c1 and c2 from its classical counterpart as well as introducing two more: The Wolfe threshold cW and the extrapolation factor \u03b1ext. cW does not appear in the classical formulation since the objective function can be evaluated exactly and the Wolfe probability is binary (either fulfilled or not). While cW is thus a natural consequence of allowing the line search to model noise explicitly, the extrapolation factor \u03b1ext is the result of the line search favoring shorter steps, which we will discuss below in more detail, but most prominently because of bias in the line search\u2019s first gradient observation. In the following sections we will give an intuition about the task of the most influential design parameters c2, cW , and \u03b1ext, discuss how they affect the probabilistic line search, and validate good design choices through exploring the parameter space and showing insensitivity to most of them. All experiments on hyper-parameter sensitivity were performed training N-II on MNIST with mini-batch size m = 200. For a full search of the parameter space cW -c2-\u03b1ext we performed 4950 runs in total with 495 different parameter combinations. All results are reported. 4.2.1 Wolfe II Parameter c2 and Wolfe Threshold cW As described in Section 3.4, c2 encodes the strictness of the curvature condition W-II. Pictorially speaking, a larger c2 extends the range of acceptable gradients (green shaded are in the lower part of Figure 5) and leads to a lenient line search while a smaller value of c2 shrinks this area, leading to a stricter line search. cW controls how certain we want to be, that the Wolfe conditions are actually fulfilled. In the extreme case of complete uncertainty about the collected gradients and function values (\u03c3f , \u03c3f \u2032 \u2192\u221e) pWolfe will always be < 0.25, if the strong Wolfe conditions are imposed. In the limit of certain observations (\u03c3f , \u03c3f \u2032 \u2192 0) 10. An example of annealed step size performance can be found in Mahsereci and Hennig (2015).", "startOffset": 11, "endOffset": 2106}, {"referenceID": 9, "context": "pWolfe is partially allowing for undesirably short steps (\u00a74.2.2). The expected improvement uEI, on the other hand, is a well studied acquisition function of Bayesian optimization trading off exploration and exploitation. It aims to globally find a point with a function value lower than a current best guess. Though this is a desirable property also for the probabilistic line search, it is lacking the information that we are seeking a point that also fulfills the W-II curvature condition. This is evident in Figure 4 where pWolfe significantly drops at points where the objective function is already evaluated but uEI does not. In addition, we do not need to explore the positive t space to an extend, the expected improvement suggests, since the aim of a line search is just to find a good, acceptable point at positive t and not the globally best one. The product of both acquisition function uEI \u00b7 pWolfe is thus a trade-off between exploring enough, but still preventing too much exploitation in obviously undesirable regions. In practice, though, we found that all three choices ((i) uEI \u00b7 pWolfe, (ii) uEI only, (iii) p Wolfe only) perform comparable. The following experiments were all performed training N-II on MNIST; only the minibatch size might vary as indicated. Figure 11 compares all three choices for mini-batch size m = 200 and default design parameters. The top plot shows the evolution of the logarithmic test and train set error (for plot and color description see Figure caption). All test and train set error curves respectively bundle up (only lastly plotted clearly visible). The choice of acquisition function thus does not change the performance here. Rows 2-4 of Figure 11 show learning rate traces of a single seed. All three curves show very similar global behavior. First the learning rate grows, then drops again, and finally settles around the best found constant learning rate. This is intriguing since on average a larger learning rate seems to be better at the beginning of the optimization process, then later dropping again to a smaller one. This might also explain why sgd+probLS in the first part of the optimization progress outperforms vanilla sgd (Figure 7). Runs, that use just slightly larger constant learning rates than the best performing constant one (above the gray horizontal lines in Figure 11) were failing after a few steps. This shows that there is some non-trivial adaptation going on, not just globally, but locally at every step. Figure 12 shows traces of accepted learning rates for different mini-batch sizes m = 100, 200, 1000. Again the global behavior is qualitatively similar for all three mini-batch sizes on the given architecture. For the largest mini-batch size m = 1000 (last row of Figure 12) the probabilistic line search accepts a larger learning rate (on average and in absolute value) than for the smaller mini-batch sizes m = 100 and 200, which is in agreement with practical experience and theoretical findings (Hinton (2012, \u00a74 and 7), Goodfellow et al. (2016, \u00a79.1.3), Balles et al. (2016)).", "startOffset": 1, "endOffset": 3071}], "year": 2017, "abstractText": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.", "creator": "LaTeX with hyperref package"}}}