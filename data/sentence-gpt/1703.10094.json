{"id": "1703.10094", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets", "abstract": "Generative Adversarial Net has shown its great ability in generating samples. The inverse mapping of generator also contains a great value. Some works have been developed to construct the inverse function of generator. However, the existing ways of training the inverse model of GANs have many shortcomings. In this paper, we propose a new approach of training the inverse model of generator by regarding a pre-trained generator as the decoder part of an autoencoder network. This model does not directly minimize the difference between original input and inverse output, but try to minimize the difference between the generated data by using original input and inverse output. This strategy overcome the difficulty in training a inverse model of a non one-to-one function. And the inverse mapping we learned can be directly used in image searching and processing.", "histories": [["v1", "Wed, 29 Mar 2017 15:23:40 GMT  (4411kb,D)", "http://arxiv.org/abs/1703.10094v1", "9 pages, 6 figures"], ["v2", "Tue, 12 Sep 2017 08:46:47 GMT  (895kb,D)", "http://arxiv.org/abs/1703.10094v2", "10 pages, 5 figures"]], "COMMENTS": "9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["junyu luo", "yong xu", "chenwei tang", "jiancheng lv"], "accepted": false, "id": "1703.10094"}, "pdf": {"name": "1703.10094.pdf", "metadata": {"source": "CRF", "title": "Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets", "authors": ["Junyu Luo"], "emails": ["2015141462158@stu.scu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The paper of Deep convolutional generative adversarial nets [10] shows the great potential in the mapping between image space X and latent space Z. This relation can be used in the fields of image processing and searching. Finding the inverse mapping of generator can also provide us useful insights to the generator and we may use this to improve the performance of generator. However, the mapping from Z-space to image space X isn\u2019t a bijection. It brings a great challenge for finding the inverse mapping of generator.\nDumoulin [4] and Donahue [3] proposed a way of learning encoder network\n\u2217\nar X\niv :1\n70 3.\n10 09\n4v 1\n[ cs\n.L G\nalongside the generator and discriminator. This approach successfully avoid the problem of directly training the inverse model. However, this approach has a shortcoming that sample images and and corresponding reconstructions don\u2019t have a very good correlation. The main reason can be that the discriminator only focus on the difference between data sets instead of the difference between two images. So the encoder part can\u2019t catch the unique features of one signal image. In addition it needs to train a third network with the generative net, which means that inversion cannot be learned from a pre-trained generative network. Creswell and Bharath [2] proposed a different idea that can get a good correlation between samples and reconstructions. The main idea is to minimize the difference between generated image G(z) and sample image x by change the value of z. This approach takes the desired output z as optimization goal. To get a corresponding z we have to use the gradient descent. It\u2019 very simple and easy. But it is not very practical because it doesn\u2019t provide a real inverse function. We have to calculate the z by using multiple gradient descents every time. In the invertible conditional GANs(ICGAN) proposed by Perarnau et al. [9], the inverse model are trained by directly minimizing the difference between Encode(x) and (z, y). Encoder(x) has two outputs. The first is z and the second is label information vector y. The label information limits the freedom of z space. And the conditional vector y is helpful for training the inverse model. But this approach requires abundant label information and such data sets are very rare.\nHence we propose a new approach of learning the inverse mapping by AntoEncoder based on GANs(AEGAN) as a complement of their works. We use the idea of training the AutoEncoder to train a inverse model of a pre-trained generator. And this approach doesn\u2019t have the shortcomings mentioned above. The experiments show that AEGAN successfully learns the inverse mapping of original generator of GAN. And the corresponding vectors of images contains rich semantic information."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Generative Adversarial Nets", "text": "A generative model aims to generate high quality artificial data by using adversarial strategy from game theory [5]. The GAN trains a discriminator and a generator at the same time and make them compete each other. The generator is aimed at approximating the underlying unknown data distribution to fool the discriminator, while the discriminator aims to tell which samples are real or fake. At the end the generator can learn the distribution of real data."}, {"heading": "2.2 Autoencoder", "text": "AutoEncoder is an artificial neural network used for coding by unsupervised leaning [1].The aim of an autoencoder is to learn the features of data and representing the data using the extracted features. In AutoEncoder, an encoder and a decoder are trained at a same time.. Encoder take original data as input and the decoder tries to reconstruct the original data using the output of encoder."}, {"heading": "3 Autoencoder based Generative Adversarial Nets", "text": "The idea of AEGAN is inspired from Autoencoder. We define two models G(z; \u03b8g) and D(x; \u03b8d) representing the generator and discrimiator of original GAN. And we define a inverse generator IG(x; \u03b8ig) representing the inverse model of the generator. \u03b8g, \u03b8d, \u03b8ig are the parameters of the generator G, discriminator D and inverse generator IG. In here we regard the generator G(z; \u03b8g) as the decoder part of the Autoencoder and the desired inverse generator IG(x; \u03b8ig) as the encoder part."}, {"heading": "3.1 Training the Generator", "text": "First we train the generator using the normal approach of training GANs(The structure of GANs in this paper is based on the DCGAN of Radford [10]). The detail of training GANs can be found from the paper of Goodfellow [5].\nThe first training objective is:\nmin G max D V (D,G) = Ex\u223cpdata(x)[logD(x)] + Ez\u223cp(z)[log(1\u2212D(G(z)))] (1)"}, {"heading": "3.2 Training the Inverse Generator", "text": "Then we start to train the inverse generator IG by using the information of pre-trained generator G.\nTo avoid the difficulty of directly training the encoder we require the value function of IG to minimize the difference between generator\u2019s original output G(z) and the reconstructed output G(IG(G(z))). So we require z\u2032 can generate the same images as z instead of being same as z. This idea avoids the problem that mapping form Z to X is not a bijection.\nIn AEGAN, we use the cross-entropy cost function to represet the difference between G(z) and the reconstructed output G(IG(G(z))). The second training objective is:\nmax IG V (IG) = Ez\u223cp(z)[G(z)\u2217logG(IG(G(z)))+(1\u2212G(z))\u2217log(1\u2212G(IG(G(z))))] (2)\nFigure 1 shows the basic structure of AEGAN. In AEGAN we don\u2019t directly minimize the difference between z and z\u2032. We try to minimize the difference between G(z) and G(z\u2032). And Alogorithm 1 shows the detail of training the inverse generator IG.\nAlgorithm 1 Training the Inverse Generator\nfor number of training iterations do Sample minibatch of m noise samples (z(1), ..., z(m)) from noise prior z\u223cpg(z). Update the inverse generator by ascending its gradient:\n\u03b8ig = \u03b8ig +\u2207\u03b8ig 1m \u2211m i=1 V (IG(z (i); \u03b8ig))\nwhere V (IG(z(i); \u03b8ig)) is: G(z(i)) \u2217 logG(IG(G(z(i)))) + (1\u2212G(z(i))) \u2217 log(1\u2212G(IG(G(z(i)))))\nend for"}, {"heading": "4 Experiment Results", "text": "We evaluate the ability of this inverse model on CelebFaces Attributes Dataset (CelebA) [8]. It\u2019s a large-scale face attributes dataset with more than 200K celebrity images. The all experiments below are down in unsupervised condition."}, {"heading": "4.1 Reconstructing Samples", "text": "We take the outputs of generator as the samples and use the inverse mapping from these samples to Z space to generate the reconstructed samples. Here, we\ncompare AEGAN with a directly trained inverse model(This model is reformed from ICGAN [9]. We delete the conditional vector from the original model because of the unsupervised condition) and the adversarial based inverse model BiGAN of Donahue [3].\nFigure 2 shows the reconstructed results of AEGAN and directly trained inverse model. Figure 3 shows the reconstructed results of BiGAN. Figure 3 use the different original samples because BiGAN can\u2019t use a pre-trained generator as base, so we compare BiGAN with the generated samples from its own generator for fairness.\nIn addition, we use the dHash as standard to evaluate the similarity of generated images. We take the average similarity as final result. As we can see in Table 1, the result of AEGAN is also the best in this experiemnt."}, {"heading": "4.2 Searching the Similar Images Using AEGAN", "text": "To illustrate the value of AEGAN, we will show its ability in searching the similar images. In here, we only compare with the general image searching algorithm. Because our approach is based on unsupervised learning. We don\u2019t give out any additional information for this task. So it\u2019s fair to make our approach compete with them. We compare AEGAN with three general image searching algorithms: dHash, pHash and color histogram. In addition, we use Euclidean Distance of corresponding z vectors of images to get the similarity. If the distance is smaller we think two pictures are more similar.\nWe take a image from the original data set celebA and add some other factors to form the 3 test images. Then we implement 4 different algorithms to find the closest images in the\nrst 20,000 images of celebA.The Figure 4 proves that AEGAN is very suitable for this task.\n11\nTo evaluate the comprehensive performance, the second experiment is aiming at finding the similar images. We only compare our algorithm with dHash, since it has a very good performance in the first experiment. We take the first 20,000 images of celebA as the test set. And take 64 images from the other part of the celebA as base images.\nAs we can see from Figure 5, AEGAN approach is much better than dHash. AEGAN catches the important features of face images such as the face angle, face similarity, hair style and expression. AEGAN may be not as good as specialized face recognition algorithms. But we have to emphasize again that this approach is unsupervised and universal. In other words, this idea can easily be implemented in other fields. We don\u2019t have to artificially find the best features for the task. This algorithm can automatically find the best features for the task and it only requires unlabeled data."}, {"heading": "4.3 Super-Resolution Using the AEGAN", "text": "To prove our approach did learn the major features of face images, we propose the third experiment. In this experiment we take the Gaussian Blur images as inputs to inverse generator and then use the output of inverse generator to reconstruct the original images.\nWe choose the generated data as the original examples since the inverse model is aimed at learning the inverse mapping of generator. From Figure 6, we can see AEGAN also performs well in super-resolution."}, {"heading": "5 Conclusion And Further Works", "text": "AEGAN uses the idea of Auotoencoder to overcome the difficulty in training a inverse model of generator. And the experiments show that the inverse mapping of generator has a very similar function compared with Word Embedding [6]. This ability can be very helpful in fields of image and video processing. It\u2019s possible to get a universal vector representation of image if we train the AEGAN at large image data sets. And it\u2019s worth trying to apply this approach in video processing by first transforming the each frame of video to the corresponding image vector. In addition, we can use AEGAN to reform the Image-to-Image\nTranslation approach based on GAN [7]. With AEGAN, the training of generator part can be done in unsupervised condition and we only need to train the encoder part in conditional situation. In other words, it\u2019s possible to train a Image-to-Image GAN net in semi-supervised condition if we use the structure of AEGAN. However, one disadvantage of AEGAN is that the performance of AEGAN is limited by the power of generator. To get a good inverse mapping we need to train a good generator at first."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations & Trends in Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Inverting the generator of a generative adversarial network. 2016", "author": ["Antonia Creswell", "Anil Anthony Bharath"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Krhenbhl", "Trevor Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian J Goodfellow", "Jean Pougetabadie", "Mehdi Mirza", "Bing Xu", "David Wardefarley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio", "Zoubin Ghahramani", "Max Welling"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning distributed representations of concepts", "author": ["G E Hinton"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "Image-toimage translation with conditional adversarial networks. 2016", "author": ["Phillip Isola", "Jun Yan Zhu", "Tinghui Zhou", "Alexei A. Efros"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Invertible conditional gans for image editing", "author": ["Guim Perarnau", "Joost Van De Weijer", "Bogdan Raducanu", "Jose M. lvarez"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The paper of Deep convolutional generative adversarial nets [10] shows the great potential in the mapping between image space X and latent space Z.", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Dumoulin [4] and Donahue [3] proposed a way of learning encoder network \u2217", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Creswell and Bharath [2] proposed a different idea that can get a good correlation between samples and reconstructions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "[9], the inverse model are trained by directly minimizing the difference between Encode(x) and (z, y).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "1 Generative Adversarial Nets A generative model aims to generate high quality artificial data by using adversarial strategy from game theory [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "2 Autoencoder AutoEncoder is an artificial neural network used for coding by unsupervised leaning [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "1 Training the Generator First we train the generator using the normal approach of training GANs(The structure of GANs in this paper is based on the DCGAN of Radford [10]).", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "The detail of training GANs can be found from the paper of Goodfellow [5].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "We evaluate the ability of this inverse model on CelebFaces Attributes Dataset (CelebA) [8].", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "compare AEGAN with a directly trained inverse model(This model is reformed from ICGAN [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "We delete the conditional vector from the original model because of the unsupervised condition) and the adversarial based inverse model BiGAN of Donahue [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "And the experiments show that the inverse mapping of generator has a very similar function compared with Word Embedding [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Translation approach based on GAN [7].", "startOffset": 34, "endOffset": 37}], "year": 2017, "abstractText": "Generative Adversarial Net has shown its great ability in generating samples. The inverse mapping of generator also contains a great value. Some works have been developed to construct the inverse function of generator. However, the existing ways of training the inverse model of GANs have many shortcomings. In this paper, we propose a new approach of training the inverse model of generator by regarding a pre-trained generator as the decoder part of an autoencoder network. This model does not directly minimize the difference between original input and inverse output, but try to minimize the difference between the generated data by using original input and inverse output. This strategy overcome the difficulty in training a inverse model of a non one-to-one function. And the inverse mapping we learned can be directly used in image searching and processing.", "creator": "LaTeX with hyperref package"}}}