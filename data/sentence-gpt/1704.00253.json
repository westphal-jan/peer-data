{"id": "1704.00253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "abstract": "Recent works have proved that synthetic parallel data generated by existing translation models can be an effective solution to various neural machine translation (NMT) issues. In this study, we construct NMT systems using only synthetic parallel data. As an effective alternative to real parallel data, we also present a new type of synthetic parallel corpus known as the 'FMS-Langaroo Data Model'. To test our new type of synthetic parallel dataset, we used the NMT models to evaluate the performance of the system. Using an empirical method, we found that the human-language version of the 'FMS-Langaroo Data Model', which was used as a replacement for the English version of the 'FMS-Langaroo Data Model'. The FMS-Langaroo dataset consists of approximately 30,000 pages and contains over 9500,000 lines of data. As shown in Figure 4, the FMS-Langaroo Data Model includes over 1 million pages and contains over 6,000 lines of data. The FMS-Langaroo Data Model uses only one source and a subset of its data. In order to generate and manipulate a parallel corpus, we used the NMT models to construct two models. For this reason, the main feature of our first synthetic parallel model, the FMS-Langaroo Data Model, is to provide a representation of the complete corpus of human-language content in a parallel corpus. In this study, we construct a synthetic parallel corpus named 'FMS-Langaroo Data Model', which is an effective solution to various neural machine translation (NMT) issues. In this study, we construct NMT systems using only synthetic parallel data. As an effective alternative to real parallel data, we also present a new type of synthetic parallel corpus known as the 'FMS-Langaroo Data Model', which is an effective solution to various neural machine translation (NMT) issues. In this study, we construct NMT systems using only synthetic parallel data. As an effective alternative to real parallel data, we also present a new type of synthetic parallel corpus known as the 'FMS-Langaroo Data Model', which is an effective solution to various neural machine translation (NMT) issues. In this study, we construct NMT systems using only synthetic parallel data. As an effective alternative to real parallel data, we also present a new type of synthetic parallel corpus known as the 'FMS-Langaroo Data Model', which is an effective solution to various neural machine translation (NMT) issues", "histories": [["v1", "Sun, 2 Apr 2017 05:54:14 GMT  (236kb,D)", "https://arxiv.org/abs/1704.00253v1", "10 pages, 2 figures, 3 tables"], ["v2", "Mon, 17 Apr 2017 02:58:02 GMT  (231kb,D)", "http://arxiv.org/abs/1704.00253v2", "added small-scale experiments"], ["v3", "Mon, 15 May 2017 09:23:21 GMT  (232kb,D)", "http://arxiv.org/abs/1704.00253v3", "added small-scale experiments and reorganized experimental results"], ["v4", "Sun, 17 Sep 2017 01:58:39 GMT  (269kb,D)", "http://arxiv.org/abs/1704.00253v4", null]], "COMMENTS": "10 pages, 2 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jaehong park", "jongyoon song", "sungroh yoon"], "accepted": false, "id": "1704.00253"}, "pdf": {"name": "1704.00253.pdf", "metadata": {"source": "CRF", "title": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data", "authors": ["Jaehong Park", "Jongyoon Song", "Sungroh Yoon"], "emails": ["sryoon}@snu.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Given the data-driven nature of neural machine translation (NMT), the limited source-to-target bilingual sentence pairs have been one of the major obstacles in building competitive NMT systems. Recently, pseudo parallel data, which refer to the synthetic bilingual sentence pairs automatically generated by existing translation models, have reported promising results with regard to the data scarcity in NMT. Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b). In addition, synthesized parallel data have played vital roles in many NMT problems such as domain adaptation (Sennrich et al., 2015a), zeroresource NMT (Firat et al., 2016b), and the rare word problem (Zhang and Zong, 2016a).\nInspired by their efficacy, we attempt to train NMT models using only synthetic parallel data.\nTo the best of our knowledge, building NMT systems with only pseudo parallel data has yet to be studied. Through our research, we explore the availability of synthetic parallel data as an effective alternative to the real-world parallel corpus. The active usage of synthetic data in NMT particularly has its significance in low-resource environments where the ground truth parallel corpora are very limited or not established. Even in recent approaches such as zero-shot NMT (Johnson et al., 2016) and pivot-based NMT (Cheng et al., 2016a), where direct source-to-target bilingual data are not required, the direct parallel corpus brings substantial improvements in translation quality where the pseudo parallel data can also be employed.\nPreviously suggested synthetic data, however, have several drawbacks to be a reliable alternative to the real parallel corpus. As illustrated in Figure 1, existing pseudo parallel corpora can be classified into two groups: source-originated and target-originated. The common property between them is that ground truth examples exist only on a single side (source or target) of pseudo sentence pairs, while the other side is composed of synthetic sentences only. The bias of synthetic examples in sentence pairs, however, may lead to the imbalance of the quality of learned NMT models when the given pseudo parallel corpus is exploited in bidirectional translation tasks (e.g., French\u2192German and German\u2192French). In addition, the reliability of the synthetic parallel data is heavily influenced by a single translation model where the synthetic examples originate. Lowquality synthetic sentences generated by the translation model would prevent NMT models from learning solid parameters.\nTo overcome these shortcomings, we propose a novel synthetic parallel corpus called PSEUDOmix. In contrast to previous works, PSEUDOmix includes both synthetic and real sentences on either side of sentence pairs. In practice, it can be readily built by mixing sourceand target-originated pseudo parallel corpora for\nar X\niv :1\n70 4.\n00 25\n3v 4\n[ cs\n.C L\n] 1\n7 Se\np 20\n17\na given translation task. Experiments on several language pairs demonstrate that the proposed PSEUDOmix shows useful properties that make it a reliable candidate for real-world parallel data. In detail, we make the following contributions:\n1. PSEUDOmix shows more balanced translation quality compared to existing pseudo parallel corpora in bidirectional translation tasks. For each task, it outperforms both source- and target-originated data when their performance gap is under a certain range.\n2. When fine-tuned using real parallel data, the model trained with PSEUDOmix outperforms other fine-tuned models trained with source-originated and target-originated synthetic parallel data, indicating substantial improvement in translation quality."}, {"heading": "2 Neural Machine Translation", "text": "Given a source sentence x = (x1, . . . , xm) and its corresponding target sentence y = (y1, . . . , yn), the NMT aims to model the conditional probability p(y|x) with a single large neural network. To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014). Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al., 2014a).\nIn this study, we use the attentional NMT architecture proposed by Bahdanau et al. (2014). In their work, the encoder, which is a bidirectional recurrent neural network, reads the source sentence and generates a sequence of source representations h = (h1, . . . ,hm). The decoder, which is another recurrent neural network, produces the target sentence one symbol at a time. The log conditional probability thus can be decomposed as follows:\nlog p(y|x) = n\u2211 t=1 log p(yt|y<t, x) (1)\nwhere y<t = (y1, . . . , yt\u22121). As described in Equation (2), the conditional distribution of p(yt|y<t, x) is modeled as a function of the previously predicted output yt\u22121, the hidden state of the decoder st, and the context vector ct.\np(yt|y<t, x) \u221d exp{g(yt\u22121, st, ct)} (2)\nThe context vector ct is used to determine the relevant part of the source sentence to predict yt. It\nis computed as the weighted sum of source representations h1, . . . ,hm. Each weight \u03b1ti for hi implies the probability of the target symbol yt being aligned to the source symbol xi:\nct = m\u2211 i=1 \u03b1tihi (3)\nGiven a sentence-aligned parallel corpus of size N , the entire parameter \u03b8 of the NMT model is jointly trained to maximize the conditional probabilities of all sentence pairs {(xn, yn)}Nn=1:\n\u03b8\u2217 = argmax \u03b8 N\u2211 n=1 log p(yn|xn) (4)\nwhere \u03b8\u2217 is the optimal parameter."}, {"heading": "3 Related Work", "text": "In statistical machine translation (SMT), synthetic bilingual data have been primarily proposed as a means to exploit monolingual corpora. By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora (Ueffing et al., 2007; Wu et al., 2008). In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011). The primary goal of these works was to adapt trained SMT models to other domains using relatively abundant in-domain monolingual data.\nInspired by the successful application in SMT, there have been efforts to exploit synthetic parallel data in improving NMT systems. Sourceside (Zhang and Zong, 2016b), target-side (Sennrich et al., 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In Sennrich et al., (2015a), domain adaptation of NMT was achieved by finetuning trained NMT models using a synthetic parallel corpus. Firat et al. (2016b) attempted to build NMT systems without any direct source-to-target parallel corpus. In their work, the pseudo parallel corpus was employed in fine-tuning the targetspecific attention mechanism of trained multi-way multilingual NMT (Firat et al., 2016a) models, which enabled zero-resource NMT between the source and target languages. Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons (Zhang and Zong, 2016a)."}, {"heading": "4 Synthetic Parallel Data as an Alternative to Real Parallel Corpus", "text": ""}, {"heading": "4.1 Motivation", "text": "As described in the previous section, synthetic parallel data have been widely used to boost the performance of NMT. In this work, we further extend their application by training NMT with only synthetic data. In certain language pairs or domains where the source-to-target real parallel corpora are very rare or even unprepared, the model trained with synthetic parallel data can function as an effective baseline model. Once the additional ground truth parallel corpus is established, the trained model can be improved by retraining or fine-tuning using the real parallel data."}, {"heading": "4.2 Limits of the Previous Approaches", "text": "For a given translation task, we classify the existing pseudo parallel data into the following groups:\n(a) Source-originated: The source sentences are from a real corpus, and the associated target sentences are synthetic. The corpus can be formed by automatically translating a sourceside monolingual corpus into the target language (Zhang and Zong, 2016a,b). It can also be built from source-pivot bilingual data by\nintroducing a pivot language. In this case, a pivot-to-target translation model is employed to translate the pivot language corpus into the target language. The generated target sentences paired with the original source sentences form a pseudo parallel corpus.\n(b) Target-originated: The target sentences are from a real corpus, and the associated source sentences are synthetic. The corpus can be formed by back-translating a target-side monolingual corpus into the source language (Sennrich et al., 2015a). Similar to the source-originated case, it can be built from a pivot-target bilingual corpus using a pivot-tosource translation model (Firat et al., 2016b).\nThe process of building each synthetic parallel corpus is illustrated in Figure 1. As shown in Figure 1, the previous studies on pseudo parallel data share a common property: synthetic and ground truth sentences are biased on a single side of sentence pairs. In such a case where the synthetic parallel data are the only or major resource used to train NMT, this may severely limit the availability of the given pseudo parallel corpus. For instance, as will be demonstrated in our experiments, synthetic data showing relatively high quality in one translation task (e.g., French\u2192German) can pro-\nduce poor results in the translation task of the reverse direction (German\u2192French).\nAnother drawback of employing synthetic parallel data in training NMT is that the capacity of the synthetic parallel corpus is inherently influenced by the mother translation model from which the synthetic sentences originate. Depending on the quality of the mother model, ill-formed or inaccurate synthetic examples could be generated, which would negatively affect the reliability of the resultant synthetic parallel data. In the previous study, Zhang and Zong (2016b) bypassed this issue by freezing the decoder parameters while training with the minibatches of pseudo bilingual pairs made from a source language monolingual corpus. This scheme, however, cannot be applied to our scenario as the decoder network will remain untrained during the entire training process."}, {"heading": "4.3 Proposed Mixing Approach", "text": "To overcome the limitations of the previously suggested pseudo parallel data, we propose a new type of synthetic parallel corpus called PSEUDOmix. Our approach is quite straightforward: For a given translation task, we first build both sourceoriginated and target-originated pseudo parallel data. PSEUDOmix can then be readily built by mixing them together. The overall process of building PSEUDOmix for the French\u2192German translation task is illustrated in Figure 1.\nBy mixing source- and target-originated pseudo parallel data, the resultant corpus includes both real and synthetic examples on either side of sentence pairs, which is the most evident feature of PSEUDOmix. Through the mixing approach, we attempt to lower the overall discrepancy in the quality of the source and target examples of synthetic sentence pairs, thus enhancing the reliability as a parallel resource. In the following section, we evaluate the actual benefits of the mixed composition in the synthetic parallel data."}, {"heading": "5 Experiments: Effects of Mixing Real and Synthetic Sentences", "text": "In this section, we analyze the effects of the mixed composition in the synthetic parallel data. Mixing pseudo parallel corpora derived from different sources, however, inevitably brings diversity, which affects the capacity of the resulting corpus. We isolate this factor by building both source- and target-originated synthetic corpora from the identical source-to-target real parallel corpus. Our experiments are performed on French (Fr) \u2194 German (De) translation tasks. Throughout the remaining paper, we use the notation * to denote the synthetic part of the pseudo sentence pairs."}, {"heading": "5.1 Data Preparation", "text": "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora (Koehn, 2005), constructing a multiparallel corpus of Fr-En-De. Then each of the Fr*De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model 1 for En\u2192De and train another NMT model for En\u2192Fr using the WMT\u201915 En-Fr parallel corpus (Bojar et al., 2015). A beam of size 5 is used to generate synthetic sentences. Lastly, to match the size of the training data, PSEUDOmix is established by randomly sampling half of each Fr*-De and Fr-De* corpus and mixing them together."}, {"heading": "5.2 Data Preprocessing", "text": "Each training corpus is tokenized using the tokenization script in Moses (Koehn et al., 2007). We represent every sentence as a sequence of subword units learned from byte-pair encoding (Sennrich et al., 2015b). We remove empty lines and all the sentences of length over 50 subword units. For a fair comparison, all cleaned synthetic parallel data have equal sizes. The summary of the final parallel corpora is presented in Table 1."}, {"heading": "5.3 Training and Evaluation", "text": "All networks have 1024 hidden units and 500 dimensional embeddings. The vocabulary size is limited to 30K for each language. Each model is trained for 10 epochs using stochastic gradient descent with Adam (Kingma and Ba, 2014). The Minibatch size is 80, and the training set is reshuffled between every epoch. The norm of the gra-\n1http://data.statmt.org/rsennrich/ wmt16_systems\ndient is clipped not to exceed 1.0 (Pascanu et al., 2013). The learning rate is 2 \u00b7 10\u22124 in every case.\nWe use the newstest 2012 set for a development set and the newstest 2011 and newstest 2013 sets as test sets. At test time, beam search is used to approximately find the most likely translation. We use a beam of size 12 and normalize probabilities by the length of the candidate sentences. The evaluation metric is case-sensitive tokenized BLEU (Papineni et al., 2002) computed with the multi-bleu.perl script from Moses. For each case, we present average BLEU evaluated on three different models trained from scratch."}, {"heading": "5.4 Results and Analysis", "text": ""}, {"heading": "5.4.1 A Comparison between Pivot-based Approach and Back-translation", "text": "Before we choose the pivot language-based method for data synthesis, we conduct a preliminary experiment analyzing both pivot-based and direct back-translation. The model used for direct back-translation was trained with the ground truth Europarl Fr-De data made from the multiparallel corpus presented in Table 2. On the newstest 2012/2013 sets, the synthetic corpus generated using the pivot approach showed higher BLEU (19.11 / 20.45) than the back-translation counterpart (18.23 / 19.81) when used in training a De\u2192Fr NMT model. Although the backtranslation method has been effective in many studies (Sennrich et al., 2015a, 2016), its availability becomes restricted in low-resource cases which is our major concern. This is due to the poor quality of the back-translation model built from the limited source-to-target parallel corpus. Instead, one can utilize abundant pivot-to-target parallel corpora by using a rich-resource language as the pivot language. This consequently improves the reliability of the quality of baseline translation models used for generating synthetic corpora."}, {"heading": "5.4.2 Effects of Mixing Source- and Target-originated Synthetic Data", "text": "From Table 2, we find that the bias of the synthetic examples in pseudo parallel corpora brings\nimbalanced quality in the bidirectional translation tasks. Given that the source- and target-originated classification of a specific synthetic corpus is reversed depending on the direction of the translation, the overall results imply that the targetoriginated corpus for each translation task outperforms the source-originated data. The preference of target-originated synthetic data over the sourceoriginated counterparts was formerly investigated in SMT by Lambert et al., (2011). In NMT, it can be explained by the degradation in quality in the source-originated data owing to the erroneous target language model formed by the synthetic target sentences. In contrast, we observe that PSEUDOmix not only produces balanced results for both Fr\u2192De and De\u2192Fr translation tasks but also shows the best or competitive translation quality for each task.\nWe note that mixing two different synthetic corpora leads to improved BLEU not their intermediate value. To investigate the cause of the improvement in PSEUDOmix, we build additional target-originated synthetic corpora for each Fr\u2194De translation with a beam of size 3. As shown in Table 3, for the De\u2192Fr task, the new target-originated corpus (c) shows higher BLEU than the source-originated corpus (b) by itself. The improvement in BLEU, however, occurs only when mixing the source- and target-originated synthetic parallel data (b+d) compared to mixing two target-originated synthetic corpora (c+d). The same phenomenon is observed in the Fr\u2192De case as well. The results suggest that real and synthetic sentences mixed on either side of sentence pairs enhance the capability of a synthetic parallel corpus. We conjecture that ground truth examples in both encoder and decoder networks not only compensate for the erroneous language model learned from synthetic sentences but also reinforces patterns of use latent in the pseudo sentences."}, {"heading": "5.4.3 A Comparison with Phrase-based Statistical Machine Translation", "text": "We also evaluate the effects of the proposed mixing strategy in phrase-based statistical machine translation (Koehn et al., 2003). We use\nMoses (Koehn et al., 2007) and its baseline configuration for training. A 5-gram Kneser-Ney model is used as the language model. Table 4 shows the translation results of the phrase-based statistical machine translation (PBSMT) systems. In all experiments, NMT shows higher BLEU (2.44- 3.38) compared to the PBSMT setting. We speculate that the deep architecture of NMT provides noise robustness in the synthetic examples. It is also notable that the proposed PSEUDOmix outperforms other synthetic corpora in PBSMT. The results clearly show that the benefit of the mixed composition in synthetic sentence pairs is beyond a specific machine translation framework."}, {"heading": "6 Experiments: Large-scale Application", "text": "The experiments shown in the previous section verify the potential of PSEUDOmix as an efficient alternative to the real parallel data. The condition in the previous case, however, is somewhat artificial, as we deliberately match the sources of all pseudo parallel corpora. In this section, we move on to more practical and large-scale applications of synthetic parallel data. Experiments are conducted on Czech (Cs)\u2194 German (De) and French (Fr)\u2194 German (De) translation tasks."}, {"heading": "6.1 Application Scenarios", "text": "We analyze the efficacy of the proposed mixing approach in the following application scenarios:\n(i) Pseudo Only: This setting trains NMT models using only synthetic parallel data without any ground truth parallel corpus.\n(ii) Real Fine-tuning: Once the training of an NMT model is completed in the Pseudo Only manner, the model is fine-tuned using only a ground truth parallel corpus.\nThe suggested scenarios reflect low-resource situations in building NMT systems. In the Real Fine-tuning, we fine-tune the best model of the Pseudo Only scenario evaluated on the development set."}, {"heading": "6.2 Data Preparation", "text": "We use the parallel corpora from the shared translation task of WMT\u201915 and WMT\u201916 (Bojar et al., 2016). Using the same pivot-based technique as the previous task, Cs-De* and Fr-De* corpora are built from the WMT\u201915 Cs-En and Fr-En parallel data respectively. For Cs*-De and Fr*-De, WMT\u201916 En-De parallel data are employed. We again use pre-trained NMT models for En\u2192Cs, En\u2192De, and En\u2192Fr to generate synthetic sentences. A beam of size 1 is used for fast decoding.\nFor the Real Fine-tuning scenario, we use real parallel corpora from the Europarl and News Commentary11 dataset. These direct parallel corpora are obtained from OPUS (Tiedemann, 2012). The size of each set of ground truth and synthetic parallel data is presented in Table 5. Given that the training corpus for widely studied language pairs amounts to several million lines, the Cs-De language pair (0.6M) reasonably represents a lowresource situation. On the other hand, the Fr-De language pair (1.8M) is considered to be relatively resource-rich in our experiments. The details of the preprocessing are identical to those in the previous case."}, {"heading": "6.3 Training and Evaluation", "text": "We use the same experimental settings that we used for the previous case except for the Real Finetuning scenario. In the fine-tuning step, we use the learning rate of 2 \u00b710\u22125, which produced better results. Embeddings are fixed throughout the finetuning steps. For evaluation, we use the same development and test sets used in the previous task."}, {"heading": "6.4 Results and Analysis", "text": ""}, {"heading": "6.4.1 A Comparison with Real Parallel Data", "text": "Table 6 shows the results of the Pseudo Only scenario on Cs\u2194De and Fr\u2194De tasks. For the baseline comparison, we also present the translation quality of the NMT models trained with the ground truth Europarl+NC11 parallel corpora (a). In Cs\u2194De, the Pseudo Only scenario shows outperforming results compared to the real parallel corpus by up to 3.86-4.43 BLEU on the newstest 2013 set. Even for the Fr\u2194De case, where the size of the real parallel corpus is relatively large, the best BLEU of the pseudo parallel corpora is higher than that of the real parallel corpus by 1.3 (Fr\u2192De) and 0.49 (De\u2192Fr). We list the results on the newstest 2011 and newstest 2012 in the appendix. From the results, we conclude that largescale synthetic parallel data can perform as an effective alternative to the real parallel corpora, particularly in low-resource language pairs."}, {"heading": "6.4.2 Results from the Pseudo Only Scenario", "text": "As shown in Table 6, the model learned from the Cs*-De corpus outperforms the model trained with the Cs-De* corpus in every case. This result is slightly different from the previous case, where the target-originated synthetic corpus for each translation task reports better results than the source-originated data. This arises from the diversity in the source of each pseudo parallel corpus, which vary in their suitability for the given test set. Table 6 also shows that mixing the Cs*-De corpus with the Cs-De* corpus of worse quality brings improvements in the resulting PSEUDOmix, showing the highest BLEU for\nbidirectional Cs\u2194De translation tasks. In addition, PSEUDOmix again shows much more balanced performance in Fr\u2194De translations compared to other synthetic parallel corpora.\nWhile the mixing strategy compensates for most of the gap between the Fr-De* and the Fr*De (3.01\u21920.17) in the De\u2192Fr case, the resulting PSEUDOmix still shows lower BLEU than the target-originated Fr-De* corpus. We thus enhance the quality of the synthetic examples of the source-originated Fr*-De data by further training its mother translation model (En\u2192Fr). As illustrated in Figure 2, with the target-originated FrDe* corpus being fixed, the quality of the models trained with the source-originated Fr*-De data and PSEUDOmix increases in proportion to the quality of the mother model for the Fr*-De corpus. Eventually, PSEUDOmix shows the highest BLEU, outperforming both Fr*-De and Fr-De* data. The results indicate that the benefit of the proposed mixing approach becomes much more evident when the quality gap between the source- and targetoriginated synthetic data is within a certain range."}, {"heading": "6.4.3 Results from the Real Fine-tuning Scenario", "text": "As presented in Table 6, we observe that finetuning using ground truth parallel data brings substantial improvements in the translation qualities of all NMT models. Among all fine-tuned models, PSEUDOmix shows the best performance in all experiments. This is particularly encouraging for the case of De\u2192Fr, where PSEUDOmix reported lower BLEU than the Fr-De* data before it was fine-tuned. Even in the case where PSEUDOmix shows comparable results with other synthetic corpora in the Pseudo Only scenario, it shows higher improvements in the translation quality when fine-tuned with the real parallel data. These results clearly demonstrate the strengths of the proposed PSEUDOmix, which indicate both competitive translation quality by itself and relatively higher potential improvement as a result of the refinement using ground truth parallel corpora.\nIn Table 6 (b), we also present the performance of NMT models learned from the ground truth Europarl+NC11 data merged with the targetoriginated synthetic parallel corpus for each task. This is identical in spirit to the method in Sennrich et al. (2015a) which employs back-translation for data synthesis. Instead of direct back-translation, we used pivot-based back-translation, as we verified the strength of the pivot-based data synthesis in low-resource environments. Although the ground truth data is only used for the refinement, the Real Fine-tuning scheme applied to\nPSEUDOmix shows better translation quality compared to the models trained with the merged corpus (b). Even the results of the Real Fine-tuning on the target-originated corpus provide comparable results to the training with the merged corpus from scratch. The overall results support the efficacy of the proposed two-step methods in practical application: the Pseudo Only method to introduce useful prior on the NMT parameters and the Real Fine-tuning scheme to reorganize the pre-trained NMT parameters using in-domain parallel data."}, {"heading": "7 Conclusion", "text": "In this work, we have constructed NMT systems using only synthetic parallel data. For this purpose, we suggest a novel pseudo parallel corpus called PSEUDOmix where synthetic and ground truth real examples are mixed on either side of sentence pairs. Experiments show that the proposed PSEUDOmix not only shows enhanced results for bidirectional translation but also reports substantial improvement when fine-tuned with ground truth parallel data. Our work has significance in that it provides a thorough investigation on the use of synthetic parallel corpora in low-resource NMT environment. Without any adjustment, the proposed method can also be extended to other learning areas where parallel samples are employed. For future work, we plan to explore robust data sampling methods, which would maximize the quality of the mixed synthetic parallel data."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Nicola Bertoldi", "Marcello Federico."], "venue": "Proceedings of the fourth workshop on statistical machine translation. Association for Computational Linguistics, pages 182\u2013189.", "citeRegEx": "Bertoldi and Federico.,? 2009", "shortCiteRegEx": "Bertoldi and Federico.", "year": 2009}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Ma-", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Neural machine translation with pivot languages", "author": ["Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu."], "venue": "arXiv preprint arXiv:1611.04928 .", "citeRegEx": "Cheng et al\\.,? 2016a", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semisupervised learning for neural machine translation", "author": ["Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1606.04596 .", "citeRegEx": "Cheng et al\\.,? 2016b", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073 .", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multilingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1606.04164 .", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. volume 3, page 413.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "MT summit. volume 5, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Investigations on translation model adaptation using monolingual data", "author": ["Patrik Lambert", "Holger Schwenk", "Christophe Servan", "Sadaf Abdul-Rauf."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Com-", "citeRegEx": "Lambert et al\\.,? 2011", "shortCiteRegEx": "Lambert et al\\.", "year": 2011}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1312.6026 .", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709 .", "citeRegEx": "Sennrich et al\\.,? 2015a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1606.02891 .", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann."], "venue": "LREC. volume 2012, pages 2214\u2013 2218.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Transductive learning for statistical machine translation", "author": ["Nicola Ueffing", "Gholamreza Haffari", "Anoop Sarkar"], "venue": "In Annual Meeting-Association for Computational Linguistics", "citeRegEx": "Ueffing et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ueffing et al\\.", "year": 2007}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["Hua Wu", "Haifeng Wang", "Chengqing Zong."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1. As-", "citeRegEx": "Wu et al\\.,? 2008", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Bridging neural machine translation and bilingual dictionaries", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.07272 .", "citeRegEx": "Zhang and Zong.,? 2016a", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Zhang and Zong.,? 2016b", "shortCiteRegEx": "Zhang and Zong.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 27, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 4, "context": "Many studies have found that the pseudo parallel data combined with the real bilingual parallel corpus significantly enhance the quality of NMT models (Sennrich et al., 2015a; Zhang and Zong, 2016b; Cheng et al., 2016b).", "startOffset": 151, "endOffset": 219}, {"referenceID": 19, "context": "In addition, synthesized parallel data have played vital roles in many NMT problems such as domain adaptation (Sennrich et al., 2015a), zeroresource NMT (Firat et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 8, "context": ", 2015a), zeroresource NMT (Firat et al., 2016b), and the rare word problem (Zhang and Zong, 2016a).", "startOffset": 27, "endOffset": 48}, {"referenceID": 26, "context": ", 2016b), and the rare word problem (Zhang and Zong, 2016a).", "startOffset": 36, "endOffset": 59}, {"referenceID": 9, "context": "Even in recent approaches such as zero-shot NMT (Johnson et al., 2016) and pivot-based NMT (Cheng et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": ", 2016) and pivot-based NMT (Cheng et al., 2016a), where direct source-to-target bilingual data are not required, the direct parallel corpus brings substantial improvements in translation quality where the pseudo parallel data can also be employed.", "startOffset": 28, "endOffset": 49}, {"referenceID": 10, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 6, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 22, "context": "To parameterize the conditional distribution, recent studies on NMT employ the encoder-decoder architecture (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Sutskever et al., 2014).", "startOffset": 108, "endOffset": 183}, {"referenceID": 0, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al.", "startOffset": 36, "endOffset": 79}, {"referenceID": 16, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al.", "startOffset": 36, "endOffset": 79}, {"referenceID": 5, "context": ", 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al., 2014a).", "startOffset": 125, "endOffset": 144}, {"referenceID": 0, "context": "Thereafter, the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) has been introduced and successfully addressed the quality degradation of NMT when dealing with long input sentences (Cho et al., 2014a). In this study, we use the attentional NMT architecture proposed by Bahdanau et al. (2014). In their work, the encoder, which is a bidirectional recurrent neural network, reads the source sentence and generates a sequence of source representations", "startOffset": 37, "endOffset": 308}, {"referenceID": 24, "context": "By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora (Ueffing et al., 2007; Wu et al., 2008).", "startOffset": 140, "endOffset": 179}, {"referenceID": 25, "context": "By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora (Ueffing et al., 2007; Wu et al., 2008).", "startOffset": 140, "endOffset": 179}, {"referenceID": 1, "context": "In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 122, "endOffset": 173}, {"referenceID": 15, "context": "In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 122, "endOffset": 173}, {"referenceID": 27, "context": "Sourceside (Zhang and Zong, 2016b), target-side (Sennrich et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 19, "context": "Sourceside (Zhang and Zong, 2016b), target-side (Sennrich et al., 2015a) and both sides (Cheng et al.", "startOffset": 48, "endOffset": 72}, {"referenceID": 4, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora.", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "In their work, the pseudo parallel corpus was employed in fine-tuning the targetspecific attention mechanism of trained multi-way multilingual NMT (Firat et al., 2016a) models, which enabled zero-resource NMT between the source and target languages.", "startOffset": 147, "endOffset": 168}, {"referenceID": 26, "context": "Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons (Zhang and Zong, 2016a).", "startOffset": 128, "endOffset": 151}, {"referenceID": 3, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In Sennrich et al., (2015a), domain adaptation of NMT was achieved by finetuning trained NMT models using a synthetic parallel corpus.", "startOffset": 25, "endOffset": 282}, {"referenceID": 3, "context": ", 2015a) and both sides (Cheng et al., 2016b) of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In Sennrich et al., (2015a), domain adaptation of NMT was achieved by finetuning trained NMT models using a synthetic parallel corpus. Firat et al. (2016b) attempted to build NMT systems without any direct source-to-target parallel corpus.", "startOffset": 25, "endOffset": 410}, {"referenceID": 19, "context": "The corpus can be formed by back-translating a target-side monolingual corpus into the source language (Sennrich et al., 2015a).", "startOffset": 103, "endOffset": 127}, {"referenceID": 8, "context": "Similar to the source-originated case, it can be built from a pivot-target bilingual corpus using a pivot-tosource translation model (Firat et al., 2016b).", "startOffset": 133, "endOffset": 154}, {"referenceID": 26, "context": "In the previous study, Zhang and Zong (2016b) bypassed this issue by freezing the decoder parameters while training with the minibatches of pseudo bilingual pairs made from a source language monolingual corpus.", "startOffset": 23, "endOffset": 46}, {"referenceID": 12, "context": "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora (Koehn, 2005), constructing a multiparallel corpus of Fr-En-De.", "startOffset": 152, "endOffset": 165}, {"referenceID": 13, "context": "Each training corpus is tokenized using the tokenization script in Moses (Koehn et al., 2007).", "startOffset": 73, "endOffset": 93}, {"referenceID": 20, "context": "We represent every sentence as a sequence of subword units learned from byte-pair encoding (Sennrich et al., 2015b).", "startOffset": 91, "endOffset": 115}, {"referenceID": 11, "context": "Each model is trained for 10 epochs using stochastic gradient descent with Adam (Kingma and Ba, 2014).", "startOffset": 80, "endOffset": 101}, {"referenceID": 18, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": "The evaluation metric is case-sensitive tokenized BLEU (Papineni et al., 2002) computed with the multi-bleu.", "startOffset": 55, "endOffset": 78}, {"referenceID": 15, "context": "The preference of target-originated synthetic data over the sourceoriginated counterparts was formerly investigated in SMT by Lambert et al., (2011). In NMT, it can be explained by the degradation in quality in the source-originated data owing to the erroneous target language model formed by the synthetic target sentences.", "startOffset": 126, "endOffset": 149}, {"referenceID": 14, "context": "We also evaluate the effects of the proposed mixing strategy in phrase-based statistical machine translation (Koehn et al., 2003).", "startOffset": 109, "endOffset": 129}, {"referenceID": 13, "context": "Moses (Koehn et al., 2007) and its baseline configuration for training.", "startOffset": 6, "endOffset": 26}, {"referenceID": 23, "context": "These direct parallel corpora are obtained from OPUS (Tiedemann, 2012).", "startOffset": 53, "endOffset": 70}, {"referenceID": 19, "context": "This is identical in spirit to the method in Sennrich et al. (2015a) which employs back-translation for data synthesis.", "startOffset": 45, "endOffset": 69}], "year": 2017, "abstractText": "Recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation (NMT) issues. In this study, we build NMT systems using only synthetic parallel data. As an efficient alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus.", "creator": "LaTeX with hyperref package"}}}