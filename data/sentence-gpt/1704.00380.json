{"id": "1704.00380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2017", "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings", "abstract": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods. In the first study (15) we analyzed the correlations of translation and word embeddings between word embeddings (S&P), words and word embeddings between each. The two studies show that translation and word embeddings between two different sets of words can be reliably correlated (see section 1). The second study (18) shows that translation and word embeddings between two different sets of words can be reliably correlated (see section 1). However, the latter study showed that the difference in translation between the two types of translation was not statistically significant. We investigated the differences between two types of translation. We determined that translation and word embeddings between two different sets of words are positively correlated (see section 1). The three studies examined whether translation and word embeddings between two different types of translation were positively correlated (see section 1). For example, in the first study (18) the correlation of translation and word embeddings between two different types of translation was not statistically significant (see section 1). However, the difference between translation and word embeddings between two different types of translation was not statistically significant (see section 1). Moreover, the difference between translation and word embeddings between two different types of translation was not statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sun, 2 Apr 2017 22:36:56 GMT  (141kb,D)", "http://arxiv.org/abs/1704.00380v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["junki matsuo", "mamoru komachi", "katsuhito sudoh"], "accepted": false, "id": "1704.00380"}, "pdf": {"name": "1704.00380.pdf", "metadata": {"source": "CRF", "title": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings", "authors": ["Junki Matsuo", "Mamoru Komachi", "Katsuhito Sudoh"], "emails": ["matsuo-junki@ed.tmu.ac.jp,", "komachi@tmu.ac.jp", "sudoh@is.naist.jp"], "sections": [{"heading": null, "text": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods."}, {"heading": "1 Introduction", "text": "Automatic evaluation of machine translation (MT) systems without human intervention has gained importance. For example, BLEU (Papineni et al., 2002) has improved the MT research in the last decade. However, BLEU has little correlation with human judgment on the segment level since it is originally proposed for system-level evaluation. Segment-level evaluation is crucial for analyzing MT outputs to improve the system accuracy, but there are few studies addressing the issue of segment-level evaluation of MT outputs.\nAnother issue in MT evaluation is to evaluate MT hypotheses that are semantically equivalent with different surfaces from the reference. For instance, BLEU does not consider any words that do not match the reference at the surface level. METEOR-Universal (Denkowski and Lavie, 2014) handles word similarities better,\n\u2217The last author is currently affiliated with Nara Institute of Science and Technology, Japan.\nbut it uses external resources that require timeconsuming annotations. It is also not as simple as BLEU and its score is difficult to interpret. DREEM (Chen and Guo, 2015), another metric that addresses the issue of word similarity, does not require human annotations and uses distributed representations for MT evaluation. It shows higher accuracy than popular metrics such as BLEU and METEOR.\nTherefore, we follow the approach of DREEM to propose a lightweight MT evaluation measure that employs only a raw corpus as an external resource. We adopt sentence similarity measures proposed by Song and Roth (2015) for a Semantic Textual Similarity (STS) task. They use word embeddings to align words so that the sentence similarity score takes near-synonymous expressions into account and propose three types of heuristics using m:n (average), 1:n (maximum) and 1:1 (Hungarian) alignments. It has been reported that sentence similarity calculated with a word alignment based on word embeddings shows high accuracy on STS tasks.\nWe evaluated the word-alignment-based sentence similarity for MT evaluation to use the WMT12, WMT13, and WMT15 datasets of European\u2013English translation and WAT2015 and NTCIR8 datasets of Japanese\u2013English translation. Experimental results confirmed that the maximum alignment similarity outperforms previous word embeddings-based methods in European\u2013 English translation tasks and the average alignment similarity has the highest human correlation in Japanese\u2013English translation tasks."}, {"heading": "2 Related Work", "text": "Several studies have examined automatic evaluation of MT systems. The de facto standard automatic MT evaluation metrics BLEU\nar X\niv :1\n70 4.\n00 38\n0v 1\n[ cs\n.C L\n] 2\nA pr\n2 01\n7\n(Papineni et al., 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al., 2006). METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface mismatch by using a thesaurus and a stemmer but it needs external resources, such as WordNet. In this work, we used a distributed word representation to evaluate semantic relatedness between the hypothesis and reference sentences. This approach has the advantage that it can be implemented only with only a raw monolingual corpus.\nTo address the problem of word n-gram precision, Wang and Merlo (2016) propose to smooth it by word embeddings. They also employ maximum alignment between n-grams of hypothesis and reference sentences and a threshold to cut off n-gram embeddings with low similarity. Their work is similar to our maximum alignment similarity method, but they only experimented in European\u2013English datasets, where maximum alignment works better than average alignment.\nThe previous method most similar to ours is DREEM (Chen and Guo, 2015). It has shown to achieve state-of-the-art accuracy compared with popular metrics such as BLEU and METEOR. It uses various types of representations such as word and sentence representations. Word representations are trained with a neural network and sentence representations are trained with a recursive auto-encoder, respectively. DREEM uses cosine similarity between distributed representations of hypothesis and reference as a translation evaluation score. Both their and our methods employ word embeddings to compute sentence similarity score, but our method differs in the use of alignment and length penalty. As for alignment, we set a threshold to remove noisy alignments, whereas they use a hyper-parameter to down-weight overall sentence similarity. As for length penalty, we compared average, maximum, and Hungarian alignments to compensate for the difference between the lengths of translation hypothesis and reference, whereas they use an exponential penalty to normalize the length.\nAnother way to improve the robustness of MT evaluation is to use a character-based model. CHRF (Popovic\u0301, 2015) is one such metric that uses character n-grams. It is a harmonic mean of character n-gram precision and recall. It works\nwell for morphologically rich languages. We, instead, adopt a word-based approach because our target language, English, is morphologically simple but etymologically complex."}, {"heading": "3 Word-Alignment-Based Sentence Similarity using Word Embeddings", "text": "In this section, we introduce word-alignmentbased sentence similarity (Song and Roth, 2015) applied as an MT evaluation metrics. Song and Roth (2015) propose to use word embeddings to align words in a pair of sentences. Their approach shows promising results in STS tasks.\nIn MT evaluation, a word in the source language aligns to either a word or a phrase in the target language; therefore, it is not likely for a word to align with the whole sentence. Thus, we use several heuristics to constrain word alignment between the hypothesis and reference sentences.\nIn the following subsections, we present three sentence similarity measures. All of them use cosine similarity to calculate word similarity. To avoid alignment between unrelated words, we cut off word alignment whose similarity is less than a threshold value."}, {"heading": "3.1 Average Alignment Similarity", "text": "First, the average alignment similarity (AAS) heuristic aligns a word with multiple words in a sentence pair. Similarity of words between a hypothesis sentence and a reference sentence is calculated. AAS is given by averaging word similarity scores of all combinations of words in |x||y|.\nAAS(x, y) = 1\n|x||y| |x|\u2211 i=1 |y|\u2211 j=1 \u03c6(xi, yj) (1)\nHere, x is a hypothesis and y is a reference; and xi and yj represent words in each sentence."}, {"heading": "3.2 Maximum Alignment Similarity", "text": "Second, we propose the maximum alignment similarity (MAS) heuristic averaging only the word that has the maximum similarity score of each aligned word pair. By definition, MAS itself is an asymmetric score so we symmetrize it by averaging the score in both directions.\nMASasym(a, b) = 1\n|a| |a|\u2211 i=1 max j \u03c6(ai, bj) (2)\nMAS(x, y) = 1\n2 (MASasym(x, y)+MASasym(y, x))\n(3) Here, a and b are words in a hypothesis and a reference sentence, respectively."}, {"heading": "3.3 Hungarian Alignment Similarity", "text": "Third, we introduce the Hungarian alignment similarity (HAS) to restrict word alignment to 1:1. HAS formulates the task of word alignment as bipartite graph matching where the words in a hypothesis and a reference are represented as nodes whose edges have weight \u03c6(xi, yi). One-to-one word alignment is achieved by calculating maximum alignment of the perfect bipartite graph. For each word xi included in a hypothesis sentence, HAS chooses the word h(xi) in a reference sen-\ntence y by the Hungarian method (Kuhn, 1955).\nHAS(x, y) = 1\nmin(|x|, |y|) |x|\u2211 i=1 \u03c6(xi, h(xi)) (4)"}, {"heading": "4 Experiment", "text": "We report the results of MT evaluation in a European\u2013English translation task of the WMT12, WMT13, and WMT15 datasets and Japanese\u2013 English task of WAT2015 and NTCIR8 datasets. For the WMT datasets, we compared our metrics with BLEU and DREEM taken from the official score of the WMT15 metric task (Stanojevic\u0301 et al., 2015). For WAT2015 and NTCIR8 datasets, the three types of proposed methods are compared."}, {"heading": "4.1 Experimental Setting", "text": "We used the WMT12, WMT13, and WMT15 datasets containing a total of 137,007 sentences in French, Finnish, German, Czech, and Russian translated to English. As Japanese\u2013English translation datasets, WAT2015 includes 600 sentences and NTCIR8 includes 1,200 sentences. We measured correlation between human adequacy score and each of the evaluation metrics. We used Kendall\u2019s \u03c4 for segment-level evaluation. We used a pre-trained model of word2vec using the Google News corpus for calculating word similarity using our proposed methods.1"}, {"heading": "4.2 Result", "text": "Table 1 shows a breakdown of correlation scores for each language pair in WMT15. MAS shows the best accuracy among all the proposed metrics for all language pairs. Its accuracy is better than that of DREEM for all language pairs except for Czech\u2013English. This result shows that removal of noisy word embeddings by either using a threshold or 1:n alignment is important for European\u2013 English datasets.\nFigure 1 shows correlation of word-alignmentbased methods for WMT datasets with varying threshold values. For the WMT datasets, MAS has the highest correlation scores among the three word-alignment-based methods. A threshold value of 0.2 gives the maximum correlation for MAS for all WMT datasets.\nFigure 2 shows correlation of word-alignmentbased methods for the two Japanese\u2013English\n1https://code.google.com/archive/p/ word2vec/\ndatasets with a varying threshold. Although MAS has the highest correlation for the WMT datasets, AAS has the highest correlation for the WAT2015 and NTCIR8 datasets.\nTable 2 describes segment-level correlation results for WMT, WAT2015, and NTCIR8 datasets. MAS has the highest correlation score for the WMT datasets, whereas AAS has the highest correlation score for WAT2015 and NTCIR8 datasets."}, {"heading": "5 Discussion", "text": "Figure 1 demonstrated that MAS and AAS are more stable than HAS for European\u2013English datasets. This may be because it is relatively easy for the AAS and MAS to perform word alignment using word embeddings in translation pairs of similar languages, but HAS suffers from alignment sparsity more than the other methods. In European\u2013English translation, all the wordalignment-based methods perform poorly when using no word embeddings.\nUnlike the European\u2013English translation task, the Japanese\u2013English translation task exhibits a different tendency. Figure 2 shows the comparison between three types of word-alignment-based methods for each threshold. This is partly because word embeddings help evaluating lexically similar word pairs but fail to model syntactic variations. Also, we note that in Japanese\u2013English datasets, AAS achieved the highest correlation. We suppose that this is because in Japanese\u2013English transla-\ntion, it is difficult to cover all the source information in the target language, resulting in misalignment of inadequate words by HAS and MAS.\nTable 2 shows that MAS performs stably on the WMT datasets. In particular, Kendall\u2019s \u03c4 score of HAS in WMT12 exhibits very low correlation. It seems that the 1:1 alignment is too strict to calculate sentence similarity in MT evaluation, while the 1:m (MAS) alignment performs well, possibly because of the removal of noisy word alignment. On the other hand, AAS is more stable than MAS and HAS for WAT2015 and NTCIR8 datasets. As a rule of thumb, AAS with high threshold values (0.6\u20130.9) shows stable high correlation across all language pairs, but if it is possible to use development data to tune the parameters, MAS with different values of thresholds should be considered."}, {"heading": "6 Conclusion", "text": "In this paper, we presented word-alignment-based MT evaluation metrics using distributed word representations. In our experiments, MAS showed higher correlation with human evaluation than other automatic MT metrics such as BLEU and DREEM for European\u2013English datasets. On the other hand, for Japanese\u2013English datasets, AAS showed higher correlation with human evaluation than other metrics. These results indicate that appropriate word alignment using word embeddings is helpful in evaluating the MT output."}], "references": [{"title": "Re-evaluating the Role of BLEU in Machine Translation Research", "author": ["Chris Callison-Burch", "Miles Osborne", "Philipp Koehn."], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. pages", "citeRegEx": "Callison.Burch et al\\.,? 2006", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2006}, {"title": "Representation Based Translation Evaluation Metrics", "author": ["Boxing Chen", "Hongyu Guo."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-", "citeRegEx": "Chen and Guo.,? 2015", "shortCiteRegEx": "Chen and Guo.", "year": 2015}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. pages 376\u2013380.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "The Hungarian Method for the Assignment Problem", "author": ["Harold W. Kuhn."], "venue": "Naval Research Logistics Quarterly. pages 83\u201397.", "citeRegEx": "Kuhn.,? 1955", "shortCiteRegEx": "Kuhn.", "year": 1955}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computa-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "ChrF: Character n-gram F-score for Automatic MT Evaluation", "author": ["Maja Popovi\u0107."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 392\u2013395.", "citeRegEx": "Popovi\u0107.,? 2015", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "Unsupervised Sparse Vector Densification for Short Text Similarity", "author": ["Yangqui Song", "Dan Roth."], "venue": "Proceedings of the 2015 Annual Conference of the North American Chapter of the ACL. pages 1275\u20131280.", "citeRegEx": "Song and Roth.,? 2015", "shortCiteRegEx": "Song and Roth.", "year": 2015}, {"title": "Results of the WMT15 Metrics Shared Task", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran", "Philipp Koehn", "Ond\u0159ej Bojar."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256\u2013273.", "citeRegEx": "Stanojevi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Stanojevi\u0107 et al\\.", "year": 2015}, {"title": "Modifications of Machine Translation Evaluation Metrics by Using Word Embeddings", "author": ["Haozhou Wang", "Paola Merlo."], "venue": "Proceedings of the Sixth Workshop on Hybrid Approaches to Translation (HyTra6). pages 33\u201341.", "citeRegEx": "Wang and Merlo.,? 2016", "shortCiteRegEx": "Wang and Merlo.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "For example, BLEU (Papineni et al., 2002) has improved the MT research in the last decade.", "startOffset": 18, "endOffset": 41}, {"referenceID": 2, "context": "METEOR-Universal (Denkowski and Lavie, 2014) handles word similarities better,", "startOffset": 17, "endOffset": 44}, {"referenceID": 1, "context": "DREEM (Chen and Guo, 2015), another metric that addresses the issue of word similarity, does not require human annotations and uses distributed representations for MT evaluation.", "startOffset": 6, "endOffset": 26}, {"referenceID": 6, "context": "We adopt sentence similarity measures proposed by Song and Roth (2015) for a Semantic Textual Similarity (STS) task.", "startOffset": 50, "endOffset": 71}, {"referenceID": 4, "context": "(Papineni et al., 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": ", 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al., 2006).", "startOffset": 153, "endOffset": 182}, {"referenceID": 2, "context": "METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface mismatch by using a thesaurus and a stemmer but it needs external resources, such as WordNet.", "startOffset": 17, "endOffset": 44}, {"referenceID": 1, "context": "The previous method most similar to ours is DREEM (Chen and Guo, 2015).", "startOffset": 50, "endOffset": 70}, {"referenceID": 5, "context": "CHRF (Popovi\u0107, 2015) is one such metric that uses character n-grams.", "startOffset": 5, "endOffset": 20}, {"referenceID": 0, "context": ", 2002) may assign inappropriate score to a translation hypothesis that uses similar but different words because it considers only word n-gram precision (Callison-Burch et al., 2006). METEOR-Universal (Denkowski and Lavie, 2014) alleviates the problem of surface mismatch by using a thesaurus and a stemmer but it needs external resources, such as WordNet. In this work, we used a distributed word representation to evaluate semantic relatedness between the hypothesis and reference sentences. This approach has the advantage that it can be implemented only with only a raw monolingual corpus. To address the problem of word n-gram precision, Wang and Merlo (2016) propose to smooth it by word embeddings.", "startOffset": 154, "endOffset": 665}, {"referenceID": 6, "context": "In this section, we introduce word-alignmentbased sentence similarity (Song and Roth, 2015) applied as an MT evaluation metrics.", "startOffset": 70, "endOffset": 91}, {"referenceID": 6, "context": "In this section, we introduce word-alignmentbased sentence similarity (Song and Roth, 2015) applied as an MT evaluation metrics. Song and Roth (2015) propose to use word embeddings to align words in a pair of sentences.", "startOffset": 71, "endOffset": 150}, {"referenceID": 3, "context": "For each word xi included in a hypothesis sentence, HAS chooses the word h(xi) in a reference sentence y by the Hungarian method (Kuhn, 1955).", "startOffset": 129, "endOffset": 141}, {"referenceID": 7, "context": "For the WMT datasets, we compared our metrics with BLEU and DREEM taken from the official score of the WMT15 metric task (Stanojevi\u0107 et al., 2015).", "startOffset": 121, "endOffset": 146}, {"referenceID": 7, "context": "BLEU (Stanojevi\u0107 et al., 2015) 0.", "startOffset": 5, "endOffset": 30}, {"referenceID": 1, "context": "349 DREEM (Chen and Guo, 2015) 0.", "startOffset": 10, "endOffset": 30}], "year": 2017, "abstractText": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.", "creator": "LaTeX with hyperref package"}}}