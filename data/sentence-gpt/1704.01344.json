{"id": "1704.01344", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via Deep Layer Cascade", "abstract": "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.", "histories": [["v1", "Wed, 5 Apr 2017 09:58:51 GMT  (3280kb,D)", "http://arxiv.org/abs/1704.01344v1", "To appear in CVPR 2017 as a spotlight paper"]], "COMMENTS": "To appear in CVPR 2017 as a spotlight paper", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["xiaoxiao li", "ziwei liu", "ping luo", "chen change loy", "xiaoou tang"], "accepted": false, "id": "1704.01344"}, "pdf": {"name": "1704.01344.pdf", "metadata": {"source": "CRF", "title": "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade", "authors": ["Xiaoxiao Li", "Ziwei Liu", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "emails": ["lx015@ie.cuhk.edu.hk", "lz013@ie.cuhk.edu.hk", "pluo@ie.cuhk.edu.hk", "ccloy@ie.cuhk.edu.hk", "xtang@ie.cuhk.edu.hk"], "sections": [{"heading": "1. Introduction", "text": "Semantic image segmentation enjoys wide applications, such as video surveillance [9, 36] and autonomous driving [10, 5]. Recent advanced deep architectures, such as the residual network (ResNet) [13] and Inception [32], significantly improve the accuracy of image segmentation by increasing the depth and number of parameters in deep models. For example, ResNet-101 is six times deeper than VGG-16 [29] network, with the former outperforms the latter by 4 percent on the challenging PASCAL VOC 2012 image segmentation benchmark [8].\nAlthough promising results can be achieved through the increase of model capacity, they come with a price of runtime complexity, which impedes the deployments of\nexisting deep models in many applications that demand real-time performance. For instance, the segmentation speeds of VGG, ResNet-101, and Inception-ResNet on a 300\u00d7500 image are 5.7, 7.1 and 9.0 frame per second (FPS), which are far away from real time. To address this issue, this work presents Deep Layer Cascade (LC), which not only substantially reduces the runtime of deep models, but also improves their segmentation accuracies. Many deep architectures, including VGG, ResNet, and Inception, can benefit from the above appealing properties by adapting their structures into LC.\nLayer Cascade inherits the advantage of the conventional model cascade (MC) [18, 35], which has multiple stages and usually trains one classifier in each stage. MC is capable of increasing both speed and accuracy for object\n1\nar X\niv :1\n70 4.\n01 34\n4v 1\n[ cs\n.C V\n] 5\nA pr\n2 01\ndetection, because the earlier stages (classifiers) reject most of the easy samples (detection windows) and the later stages can pay attention on a small number of difficult samples, thus reducing false alarms. Different from MC, LC is carefully devised for deep models in the task of image segmentation. It considers different layers in a deep network as different stages. In particular, most of the pixels in an image are recognizable by the lower stages and the higher stages, which typically possess far more parameters than the bottom layers, are learned to recognize a small set of challenging pixels. In this case, the runtime of deep models can be significantly reduced by LC. Moreover, unlike MC that learns the current stage by keeping all previous stages fixed, LC trains all stages jointly to boost performance.\nAnother important difference between LC and MC is the cascade strategy. In MC, the current stage propagates a sample to the next stage, if its classification score or probability (i.e. the response after softmax) is higher than a large threshold, such as 0.95, indicating that this sample is classified as positive by the current stage with 95% confidence. In other words, later stages refine the labels of samples that are considered highly positive in the previous stages, so as to reduce false alarms.\nIn contrast, LC \u2018rejects\u2019 samples with high scores in earlier stages, but those samples with low and moderate confidences are propagated forward. Figure 1 takes the segmentation results of LC as an example to illustrate this cascade strategy. In (a), an image of \u2018cow\u2019 and \u2018background\u2019 and its ground truth label map from the VOC validation set (VOC val) are shown on the left and middle respectively. We partition all pixels in the validation set into three different sets, namely \u201ceasy\u201d, \u201cmoderate\u201d, and \u201cextremely hard\u201d sets. The easy set (ES) contains pixels that are correctly classified with larger than 95% confidence, while the extremely hard set (HS) comprises pixels that are misclassified with larger than 95% confidence. The moderate set (MS) covers pixels that have classification scores smaller than 0.95.\nIn a certain stage of LC, ES and HS are discarded and MS is propagated to the next stage, because of the following two reasons. First, as shown in the right histogram of Fig. 1(b), we observe that almost 70 percent1 pixels in HS are located on the boundaries between objects, demonstrating that these pixels are extremely hard to be recognized because of large ambiguity. An example is given by the right image of Fig. 1(a). Fitting HS during training may lead to over-fitting in the test stage. Second, the left histogram of Fig. 1(b) plots the percentages of pixels with respect to each object category in VOC val. For most of the categories, we\n1We found that the other 30 percent pixels in HS have wrong annotations. Since our purpose is to improve speed and accuracy of deep models, we do not correct those wrong annotations to enable a fair comparison with previous works.\nfound that at least 30 percent pixels belong to ES. As the background pixels are dominated (72.5%), rejecting ES and HS reduces more than 40 present pixels in earlier stages and thus significantly reduces computations of deep networks, while improves accuracy, by enabling deeper layers to focus on foreground objects.\nThis study makes three main contributions. (1) This is the first attempt to identify the segmentation difficulty of pixels for deep models. With this observation, a novel Deep Layer Cascade (LC) approach is proposed to significantly reduce computations of deep networks while improving their segmentation accuracies. (2) LC\u2019s properties can be easily applied to many recent advanced network structures. After applying LC on Inception-ResNet-v2 (IRNet) [32], its speed and accuracy are improved by 42.8% and 1.7%, respectively. (3) Connections between LC and previous models such as model cascade, deeply supervised network [17], and dropout [30] are clearly presented. Extensive studies are conducted to demonstrate the superiority of LC."}, {"heading": "2. Related Work", "text": "Semantic Image Segmentation. While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40]. For instance, Long et al. [25] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [7]. Chen et al. [3], Zheng et al. [40], and Liu et al. [22, 23] further showed that back-propagation and inference of Markov Random Field (MRF) can be incorporated into CNN. Though attaining high accuracy, these models generally have high computational costs, preventing them from deploying in real-time.\nAnother line of research [1, 21, 27] alleviates this problem by using lightweight network architectures. For example, SegNet [1] adopted a convolutional encoder-decoder and removed unnecessary layers to reduce the number of parameters. ENet [27] utilized a bottleneck module to reduce computation of convolutions. Although these networks are speeded up, they sacrificed high performances as presented in previous deep models. This work proposes Deep Layer Cascade (LC), which improves both speed and accuracy of existing deep networks. It achieves state-ofthe-art performances on both Pascal VOC and Cityscape datasets, and runs in real time. Deep Learning Cascade. Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33]. For example, Deep Decision Network [26] improved the image classification performance by dividing easy data from the\nhard ones. The hard cases with high confusion will be propagated and handled by the subsequent expert networks. Li et al. [18] used CNN cascade for face detection, which rejects false detections quickly in early stages and carefully refines detections in later stages. DeepPose [33] employed a divide-and-conquer strategy and designed a cascaded deep regression framework for human pose estimation. Different from previous network cascades that train each network separately, LC is jointly optimized to boost the segmentation accuracy."}, {"heading": "3. Deep Layer Cascade (LC)", "text": "Sec. 3.1 takes Inception-ResNet-v2 [32] as an example to illustrate how one could turn a deep model into LC. The approach can be easily generalized to the other deep networks. Sec. 3.3 introduces the training algorithm of LC."}, {"heading": "3.1. Turning a Deep Model into LC", "text": "Network Overview. To illustrate the effectiveness of LC, we choose Inception-ResNet-v2 pre-trained on ImageNet dataset as a strong baseline, denoted as IRNet, which outperforms ResNet-101 by 1.2% on the Pascal VOC2012 validation set. Experiments demonstrate that LC is able to achieve 1.7% improvement on this competitive baseline.\nFigure 2 (a) visualizes the architecture of IRNet, which has six different components, including \u2018Stem\u2019, \u2018IRNetA/B/C\u2019, and \u2018Reduction-A/B\u2019. Different components have different configurations of layers, such as convolution, pooling, and concatenation layer. The right column of Fig. 2 shows the structures of \u2018Stem\u2019 and \u2018IRNet-A/B/C\u2019 respectively, including layer types, kernel sizes, and the number of channels (in bracket). The stride typical equals one unless otherwise stated. For example, \u2018Stem\u2019 employs an RGB image as an input and produces features of 384\nchannels. More specifically, the input image is forwarded to three convolutional layers with 3\u00d73 kernels, and then the learned features are split into two streams, which have 3 and 5 convolutional layers respectively.\nSimilar network structure as IRNet has achieved great success in image recognition [32]. However, two important modifications are necessary to adapt it to image segmentation. Firstly, to increase the resolution of prediction, we remove the pooling layer at the end of IRNet and enlarge the size of feature maps by decreasing the convolutional strides in \u2018Reduction-A/B\u2019 (from 2 to 1). In this case, we expand the size of network outputs (label maps) by 4\u00d7. We also replace convolutions in \u2018IRNet-B/C\u2019 by the dilated convolutions similar to [3]. Secondly, as feature maps with high resolution consume a large amount of GPU memory in the learning process, they limit the size of minibatch (e.g. 8), making the batch normalization (BN) layers [14] unstable (as which need to estimate sample mean and variance from the data in a mini-batch). We cope with this issue by simply fixing the values of all parameters in BNs. This strategy works well in practice.\nFrom IRNet to LC (IRNet-LC). IRNet is turned into LC by dividing its different components as different stages. The number of stages is three, which is a common setting in previous cascade methods [18, 31, 33]. As shown in Fig. 2 (b), components before \u2018Reduction-A\u2019 are considered as the first stage, components between \u2018Reduction-A\u2019 and \u2018-B\u2019 are the second stage, and the remaining layers become the third stage. In Fig. 2 (b), these three stages are distinguished in yellow, green, and blue respectively. For instance, stage-1 contains one \u2018Stem\u2019, five \u2018IRNet-A\u2019, and one \u2018ReductionA\u2019. In addition, we append two convolutional layers and a softmax loss at the end of each stage. In this case, the original IRNet with one loss function develops into multiple\nstages, where each stage has its own loss function.\nNow we introduce the information flows for three stages in IRNet-LC. In the first stage as shown in Fig. 2 (b), given a 3\u00d7512\u00d7512 image I , stage-1 predicts a 21\u00d764\u00d764 segmentation label map L1, where each 21\u00d71 column vector, denoted as L1i \u2208 R21\u00d71, indicates the probabilities (confidence scores) of the i-th pixel belonging to 21 object categories in VOC respectively. We have \u221121 j=1 L 1 ij = 1, which can be satisfied by using the softmax function. If the maximum score of the i-th pixel, `1i = max(L 1 i ) and `1i \u2208 {L1ij |j = 1...21}, is larger than a threshold \u03c1 (`1i \u2265 \u03c1), we accept its prediction and do not propagate it forward to stage-2. The value of \u03c1 is usually larger than 0.95. As introduced in Sec. 1, those pixels in stage-1 that fulfil `1 \u2265 0.95 occupy nearly 40% region of an image, containing a lot of easy pixels and a small number of extremely hard pixels that have high confidence to be misclassified. Removing them from the network significantly reduces computations and improves accuracy, by enabling deeper layers to focus on foreground objects.\nStage-2 strictly follows the same procedure as above to determine which pixel is forwarded to stage-3. In other words, LC only introduces one hyper-parameter \u03c1 to IRNet. In our implementation, the value of \u03c1 is the same for both stage-1 and -2. Specifically, \u03c1 represents how many easy and extremely hard pixels are rejected (discarded) in each stage. A larger value of \u03c1 rejects a smaller number of pixels, whilst smaller \u03c1 discards more pixels. To the extreme, when \u03c1 = 1.0, no pixels are rejected. IRNet-LC becomes the original IRNet. When \u03c1 = 0.9, 52% and 35% pixels are discarded in stage-1 and -2 respectively.\nHowever, if \u03c1 becomes smaller, i.e. \u03c1 < 0.9, more \u2018moderate\u2019 pixels that locate on the important parts of objects are discarded, hindering the performance of the deep model. Experiments show that IRNet-LC is robust when \u03c1 \u2208 [0.9, 1.0]. For example, when \u03c1 = 0.95, IRNetLC obtains nearly realtime of 18 FPS compared to 9 FPS of IRNet, while outperforms it by 0.8% accuracy on VOC val. When \u03c1 = 0.985, IRNet-LC improves IRNet by 1.7% with a speed of 15 FPS.\nAfter propagating an image through all three stages, we directly combine the predicted label maps of these stages as the final prediction, because different stages predict different regions. For example, as shown in Fig. 2 (b), stage1 trusts the predictions in most of the \u2018background\u2019 (pixels with `1i \u2265 \u03c1) and propagates the other region forward. Pixels in this region are marked as \u2018unknown\u2019 because `1i < \u03c1. In stage-2, \u2018IRNet-B\u2019 and \u2018Reduction-B\u2019 only compute convolutions with respect to the forwarded region. It is learned to predict \u2018harder\u2019 region, such as \u2018person\u2019 and \u2018horse\u2019. This process is repeated in stage-3."}, {"heading": "3.2. Region Convolution", "text": "As presented above, stage-2 and -3 only calculate convolutions on those pixels that have been propagated forward. Fig. 3(b) illustrates this region convolution (RC) compared to the traditional convolution in (a), which is applied on an entire feature map. The filters in RC only convolves a region of interest, denoted as M , and ignores the other region, reducing computations a lot. The values of the other region are directly set as zeros. M can be implemented as a binary mask, where the pixels inside M equal one, otherwise zero.\nSpecifically, (c) shows how to apply RC on a residual module, which can be represented as h(I) = I + conv(I), where feature h is attained by an identity mapping [13] of I and a convolution over I . We replace the conventional convolution with a RC as introduced above, and the feature h\u2032(I) is the elementwise sum between I and the output of RC. This is equivalent to learn a masked residual representation, where values inside M are the outputs of RC and those outside M are copied from I . It works well because different stages in LC cope with different non-overlapping regions, and each stage only needs to learn features of regions it concerns."}, {"heading": "3.3. Training IRNet-LC", "text": "The parameters of IRNet are initialized by pre-training in ImageNet. Since IRNet-LC has additional convolutional layers stacked before each loss function, their parameters are initialized by sampling from a normal distribution. Given a set of images and their per-pixel label maps, IRNetLC is learned in two steps, where the first one aims at initial training and the second one employs cascade training. Initial Training. This step is similar to deeply supervised network (DSN) [17], which has multiple identical loss functions in different layers of the network. Its objective\nis to adapt IRNet pre-trained by classifying one thousand image categories in ImageNet to the task of image segmentation. It learns discriminative and robust features. In IRNet-LC, every stage is trained to minimize a pixelwise softmax loss function, measuring the discrepancies between the predicted label map and the ground truth label map of the entire image. These loss functions are jointly optimized by using back-propagation (BP) and stochastic gradient descent (SGD). Cascade Training. Once we finish the initial training, we fine-tune each stage of IRNet-LC by leveraging the cascade strategy of \u03c1 as introduced in Sec. 3.1. Similar to the previous step, all stages are trained jointly, but different stages minimize their pixel-wise softmax losses with respect to different regions. More specific, the gradients in BP are only propagated to the region of interest in each stage, which is able to learn discriminative features corresponding to regions (pixels) in a specific difficulty-level. Intuitively, the current stage is fine-tuned on pixels that have low confidences in the previous stage, enabling \u2018harder\u2019 pixels to be captured by deeper layers to improve segmentation accuracy and reduce computation. Training Details. We fix a mini-batch size of 12 images, momentum 0.9 and weight decay of 0.0005 for both two steps. In the initial training, we set the initial learning rate to be 10\u22124 and drop it by a factor of 10 after every 10 epochs. In the cascade training, we also set the initial learning rate to be 10\u22124 and drop it by a factor of 10 after every 15 epochs."}, {"heading": "3.4. Relations with Previous Models", "text": "The relationships and differences between LC and MC have been discussed in Sec. 1. LC also relates to deeply supervised nets (DSN) [17] and dropout [30]. DSN. Similar to DSN, LC adds supervision to each stage. However, to enable adaptive processing of hard/easy regions, LC employs different supervisions for different stages. In contrast, the supervision used in each stage of DSN are kept the same. Specifically, the stage-wise supervision in LC is determined by the estimated difficulty of each pixel. In this way, each stage in LC is able to focus on regions with a similar difficulty level. Dropout. LC connects to dropout in the sense that both methods discard some regions in the feature maps, but they are essentially different. LC drops those pixels with high confidences and only propagates difficult pixels forward to succeeding stages. The easy and ambiguous regions are perpetually dropped in upper layers so as to reduce computations and the deeper layers focus more on \u2018hard\u2019 regions such as foreground objects. Dropout randomly zeros out pixels in each layer independently. It prevents over-fitting but slightly increases computations. In the experiment, LC is compared with dropout to identify that the performance gain mainly comes from the proposed\ncascade strategy."}, {"heading": "4. Experiments", "text": "Settings. We evaluate our method on the PASCAL VOC 2012 (VOC12) [8] and Cityscapes [5] datasets. VOC12 dataset is a generic object segmentation benchmark with 21 classes. Following previous works, we also use the extra annotations provided by [12], which contains 10, 582 images for training, 1, 449 images for validation, and 1, 456 images for testing. Cityscapes dataset, on the other hand, focuses on street scenes segmentation and contains 19 categories. In our experiments, we only employ images with fine pixel-level annotations. There are 2975 training, 500 validation and 1525 testing images. This is consistent with existing studies [19, 4]. We adopt mean intersection over union (mIoU) to evaluate the performance of different methods."}, {"heading": "4.1. Ablation Study", "text": "In this section, we investigate the effects of adjusting probability threshold in LC and demonstrate the merits of LC by comparing it to other counterparts. All performance are reported on the validation set of VOC12. Probability Thresholds. In each stage of LC, we employ a pixel-wise probability from softmax layer to represent the confidence of prediction. By choosing appropriate probability threshold \u03c1, LC can separate easy regions, moderate regions and extremely hard regions for adaptive processing. As discussed in Sec. 3.1, \u03c1 controls how many easy and extremely hard pixels are discarded in each stage.\nTable 1 lists the processed pixel percentage in stage-1 & -2 and the overall performance as \u03c1 varies. If \u03c1 = 1, LC will degenerate to DSN, which is slightly better than fully convolutional IRNet. When \u03c1 decreases, more easy regions are classified in early stages while hard regions are progressively handled by later stages. It can be understood\nTable 1: Ablation study on probability thresholds \u03c1.\n\u03c1 1 0.995 0.985 0.970 0.950 0.930 0.900 0.800\nstage-1 (%) 0 15 23 30 35 35 44 56 stage-2 (%) 0 14 29 31 30 41 31 29 mIoU (%) 72.70 73.56 73.91 73.63 73.03 72.53 71.20 66.95\nas hard negative mining [11, 28] which improves the performance. On the other hand, if the value of \u03c1 is too small, the algorithm might become too optimistic, i.e. many hard regions are processed in early stages and early decisions are made. The performance will be harmed by overly early decisions when hard regions do not receive sufficient inference using deeper layers. As shown in Table 1, when \u03c1 = 0.985, i.e., LC processes around 52% regions in early stages and achieves the best performance. This value is used in all the following experiments. In practice, the value of \u03c1 can be chosen empirically using a validation set. Effectiveness of Layer Cascade. To show the merits of LC, we compare it to some important counterparts as discussed in Sec. 3.4, including:\n\u2022 IRNet [32]: We use the model describe in Sec. 3.1 as baseline. To conduct a fair comparison, all the following methods are based on this backbone network.\n\u2022 DSN [17]: By setting \u03c1 = 1, we make LC degenerate to a DSN, where each stage process all regions and has full supervision as the final target.\n\u2022 DSN [17] + Dropout [30]: To distinguish our method from dropout, LC is compared against DSN equipped with random label dropout in each stage. We keep the dropout ratio identical as that in LC.\n\u2022 Model Cascade: MC has a similar network architecture to LC, but with different training strategy as discussed in Sec. 1. Specifically, MC divides the IRNet into three stages, and each stage is trained separately. When we train a certain stage, we fix the parameters of all previous stages. The same threshold as in LC is employed here, i.e., \u03c1 = 0.985.\nThe results are summarized in Table 2. We have three observations here. Firstly, the improvement from deep supervision (DSN) is relatively limited, which only leads to 0.48 mIoU gain in comparison to the baseline IRNet. Since pre-training on ImageNet has been a common practice\nin semantic segmentation [25], which effectively prevents gradients exploding or vanishing, it renders the advantages of deeply supervision marginal. Secondly, random label dropout does not bring significant effect to the result. The result is expected because the dropout technique is designed to alleviate the hazard of overfitting given small training data size. However, semantic segmentation is a per-pixel labeling task and we have abundant training data to support the learning task. Thirdly, Model Cascade (MC) performs even worse than the baseline IRNet. It is because MC divides the IRNet into several independent sub-models. But each sub-model is shallow and therefore weaken the overall modeling capacity. On the contrary, LC has the appealing properties of cascading and also keeping the intrinsic depth for the whole model. The capability of maintaining the model depth adaptively for hard regions makes our approach outstanding in the comparison."}, {"heading": "4.2. Stage-wise Analysis", "text": "In this section, we demonstrate how LC enables adaptive processing for different classes and visualize the regions handled by different regions. Stage-wise Label Distribution. First, we provide a label distribution analysis across different stages. Here we take the 20 classes (excluding \u201cbackground\u201d) in VOC12 as an example. Fig. 5 (a) shows how the number of pixels changes with respect to each class in stage-2 and -3. For example, the upper histogram shows a ratio for each class, obtained by dividing its number of pixels in stage-2 by those in stage-1. Ratios larger than one indicates stage-2 focus more on the corresponding classes than stage-1 does. We find that all ratios have increased and belong to the range of 1 to 1.4. It is because stage-1 already handles the easy regions (i.e. \u201cbackground\u201d) and leaves the hard regions (i.e. \u201cforeground\u201d) to stage-2. Ratios of stage-3 can be obtained similarly in the bottom histogram. When comparing stage-3 to -2, we can see that stage-3 further focus on harder classes\n(e.g. \u201cbicycle\u201d, \u201cchair\u201d and \u201cdining table\u201d). LC learns to process samples in a \u201cdifficulty-aware\u201d manner. We also conduct a per-class analysis as illustrated in Fig. 5 (b). Harder classes like \u201cchair\u201d and \u201ctable\u201d have more pixels handled by deeper layers (stage-3). Stage-wise Visualization. Here we visualize the output label maps of different stages for both VOC12 and Cityscapes, as shown in Fig. 4 and 6. The uncertain regions in different stages are also marked out. In VOC12, the easy regions like \u201cbackground\u201d and \u201chuman faces\u201d are first labeled by stage-1 in LC. The remaining foreground and boundary regions are then progressively labeled by stage-2 and stage-3 in LC. Similarly, in Cityscapes, the easy regions like \u201croad\u201d and \u201cbuilding\u201d are first labeled by stage-1. Other small objects and fine details like \u201cpole\u201d and \u201cpedestrian\u201d are handled by stage-2 and -3."}, {"heading": "4.3. Performance and Speed Analysis", "text": "Comparisons with DeepLab and SegNet. To highlight the trade-off between performance and speed, we compare the proposed LC model with two representative state-ofthe-art methods, DeepLab-v2 [4] and SegNet [1]. The performance are reported on VOC12 and summarized in Table 3. The runtime speed is measured on a single Titan X GPU. To ensure a fair comparison, we evaluate DeepLabv2 and SegNet without any pre- and post-processing, e.g., training with extra data, multi-scale fusion, or smoothing with conditional random fields (CRF).\nDeepLab-v2 achieves an acceptable mIoU of 70.42. Nonetheless, it uses an ultra-deep ResNet-101 model as the\nbackbone network, its speed of inference is thus slow (7.1 FPS). On the contrary, SegNet is faster due to a smaller model size, however, its accuracy is greatly compromised. In particular, it increases its speed to 14.6 FPS through sacrificing of over 10 mIoU. The proposed LC alleviates the need of trading-off speed with a large drop in performance. The cascaded end-to-end trainable framework with region convolution allows it to achieve the best performance (73.91 mIoU) with an acceptable speed (14.7 FPS). Further Performance and Speed Trade-off. It is worth pointing out that the runtime of LC can be further reduced by decreasing \u03c1 to allow more regions to be handled by early stages. The performance and speed trade-off is depicted in Fig. 7 (a) with the corresponding \u03c1 values. It is observed that decreasing \u03c1 slightly affects the accuracy, but it greatly reduces the computation time. Notably, when LC attains real-time inference at 23.6 FPS, it still exhibits competitive mIoU of 66.95, in comparison to mIoU of 70.42 yielded by at 7.1 FPS. We also include the per-stage runtime in Fig. 7 (b). The increasing computation for higher performance mainly comes from later stages.\nTable 5: Per-class results on Cityscapes test set. \u201csub\u201d denotes whether the method used subsampling images for training.\nsub road swalk build. wall fence pole tlight sign veg. terrain sky person rider car truck bus train mbike bike mIoU\nRNN [40] 2 96.3 73.9 88.2 47.6 41.3 35.2 49.5 59.7 90.6 66.1 93.5 70.4 34.7 90.1 39.2 57.5 55.4 43.9 54.6 62.5 DeepLab [3] 2 97.3 77.7 87.7 43.6 40.5 29.7 44.5 55.4 89.4 67.0 92.7 71.2 49.4 91.4 48.7 56.7 49.1 47.9 58.6 63.1 FCN [25] no 97.4 78.4 89.2 34.9 44.2 47.4 60.1 65 91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 65.3 DPN [22] no 97.5 78.5 89.5 40.4 45.9 51.1 56.8 65.3 91.5 69.4 94.5 77.5 54.2 92.5 44.5 53.4 49.9 52.1 64.8 66.8 Dilation10 [39] no 97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55 93.3 45.5 53.4 47.7 52.2 66 67.1 DeepLab-v2 [4] no 97.8 81.3 90.3 48.7 47.3 49.5 57.8 67.2 91.8 69.4 94.1 79.8 59.8 93.7 56.5 67.4 57.4 57.6 68.8 70.4 Adelaide [19] no 98.0 82.6 90.6 44.0 50.7 51.1 65.0 71.7 92.0 72.0 94.1 81.5 61.1 94.3 61.1 65.1 53.8 61.6 70.6 71.6 LC no 97.9 83.1 91.6 53.7 57.4 58.4 62.0 73.3 91.9 61.3 93.8 78.8 53.1 93.4 62.2 76.9 53.5 57.0 74.7 71.1\nFigure 7: (a) shows the performance and speed trade-off in Layer Cascade (LC) by adjusting \u03c1. (b) is the time spent in each stage."}, {"heading": "4.4. Benchmark", "text": "In this section, we show that LC can achieve state-of-theart performance on standard benchmarks like VOC12 [8] and Cityscapes [5] datasets. Following [4], atrous spatial pyramid pooling [4], three-scale testing and dense CRF [16] are employed. VOC12. Table 4 lists the per-class and overall mean IoU on VOC12 test set. The approaches pre-trained on COCO [20] are marked with \u2020. LC achieves a mIoU of 80.3 and further improves the mIoU to 82.7 with pre-training on COCO, which is the best-performing method on VOC12 benchmark. By inspecting closer, we observe that LC wins 16 out of 20 foreground classes. For other 4 classes, LC also achieves competitive performance. Large gain is observed in some particular classes such as \u201cbike\u201d, \u201cchair\u201d, \u201cplant\u201d, and \u201csofa\u201d. Based on our statistics in Fig. 5, we found that these few classes, in general, require a deeper stage to make decisions on hard regions. Cityscapes. Next, we evaluate LC on Cityscapes benchmark, with results summarized in Table 5. \u201csub\u201d denotes\nwhether the method used subsampling images for training. LC also achieves promising performance with a mIoU of 71.1, which shows its great generalization ability to diverse objects and scenes. Lin et al. [19]\u2019s performance is slightly better than ours, however, LC still wins on 9 out of 19 classes. It is noticed that [19] used a deeper backbonenetwork and explored richer contextual information. We believe that further performance gain can be achieved if LC is incorporated with these techniques. LC gains outstanding performance on the classes that are \u2018traditionally regarded\u2019 as hard classes, e.g., \u201cfence\u201d, \u201cpole\u201d, \u201csign\u201d, \u201ctruck\u201d, \u201cbus\u201d and \u201cbike\u201d, which usually exhibit flexible shapes and finegrained details. The results suggest that the end-to-end cascading mechanism in LC is meaningful, especially in alleviating the burden of deeper layers on analyzing easy regions but focusing themselves on hard regions adaptively."}, {"heading": "4.5. More Comparisons between IRNet-LC and", "text": "state-of-the-art Methods\nIn Table 6, we compare the settings of different bestperforming methods on VOC12 [8] test set, including CRFRNN [40], DPN [22] and DeepLab-v2 [4]. These methods are summarized in terms of \u2018backbone network\u2019, \u2018number of parameters\u2019, \u2018pre-trained using MS COCO\u2019, \u2018multi-scale training/test\u2019, \u2018MRF/CRF\u2019, \u2018frame per second (FPS)\u2019, and \u2018mIOU\u2019. Note that \u2018-\u2019 indicates the corresponding information was not disclosed in previous paper.\nIRNet-LC uses Inception-ResNet-v2(IRNet) [32] as backbone network, which is smaller than ResNet-101 (35.5M vs. 44.5M). Following DeepLab-v2 [4], atrous spatial pyramid pooling is employed in IRNet-LC. As shown in Table 6, IRNet-LC achieves the best performance even\nwithout pre-training on MS COCO [20], demonstrating the effectiveness of the Layer Cascade framework. When all components of pre- and post-processing such as \u2018COCO\u2019, \u2018multiscale\u2019, and \u2018CRF\u2019 are removed, IRNet-LC still obtains comparable performance with respect to DeepLab-v2 (78.2% vs. 79.7%), but significantly outperforms DeepLabv2 in terms of FPS (14.3 fps vs. 0.9 fps). In other words, IRNet-LC improves FPS of DeepLab-v2 by 15 times with merely 1.5% decrease in accuracy. Note that IRNet-LC outperforms state-of-the-art systems like CRF-RNN and DPN by 3.5% and 0.7% respectively, without employing any pre- and post-processing steps."}, {"heading": "4.6. Visual Quality Comparison", "text": "We inspect visual quality of obtained label maps on VOC12 validation set. Fig. 8 demonstrates the comparisons of LC with DPN [22] and DeepLab-v2 [4]. We use the publicly released model to re-generate label maps of DeepLab-v2 while the results of DPN are downloaded from their project page. LC generally makes more accurate predictions. We also include more examples of LC label maps on Cityscapes dataset [5] in Fig. 9."}, {"heading": "5. Conclusion", "text": "Deep layer cascade (LC) is proposed in this work to simultaneously improve the accuracy and speed of semantic image segmentation. It has three advantages over previous approaches. First, LC adopts a \u201cdifficulty-aware\u201d learning paradigm, where earlier stages are trained to handle easy and confident regions and hard regions are progressively forwarded to later stages. Secondly, since each stage only processes part of the input, LC can accelerate both training and testing by the usage of region convolution. Thirdly, LC is an end-to-end trainable framework that jointly optimizes the feature learning for different regions, thus achieving state-of-the-art performance on both PASCAL VOC and Cityscapes datasets. LC is capable of running in real-time yet still yielding competitive accuracies."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning complexity-aware cascades for deep pedestrian detection", "author": ["Z. Cai", "M. Saberian", "N. Vasconcelos"], "venue": "ICCV, pages 3361\u20133369", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "atrous convolution, and fully connected crfs. arXiv:1606.00915", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "arXiv:1503.01640v2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR, pages 248\u2013255", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI, 35(8):1915\u2013 1929", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR, pages 3354\u20133361", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, pages 580\u2013587", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "ICCV, pages 991\u2013998", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Augmenting crfs with boltzmann machine shape priors for image labeling", "author": ["A. Kae", "K. Sohn", "H. Lee", "E. Learned-Miller"], "venue": "CVPR, pages 2019\u20132026", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Deeplysupervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "AISTATS, volume 2, page 6", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A convolutional neural network cascade for face detection", "author": ["H. Li", "Z. Lin", "X. Shen", "J. Brandt", "G. Hua"], "venue": "CVPR, pages 5325\u20135334", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid", "A. Hengel"], "venue": "arXiv:1504.01013v2,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning dynamic hierarchical models for anytime scene labeling", "author": ["B. Liu", "X. He"], "venue": "ECCV, pages 650\u2013666. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation via deep parsing network", "author": ["Z. Liu", "X. Li", "P. Luo", "C.-C. Loy", "X. Tang"], "venue": "ICCV, pages 1377\u20131385", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning markov random field for semantic segmentation", "author": ["Z. Liu", "X. Li", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "arXiv preprint arXiv:1606.07230", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Fashion landmark detection in the wild", "author": ["Z. Liu", "S. Yan", "P. Luo", "X. Wang", "X. Tang"], "venue": "ECCV, pages 229\u2013245", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, pages 3431\u2013 3440", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep decision network for multi-class image classification", "author": ["V.N. Murthy", "V. Singh", "T. Chen", "R. Manmatha", "D. Comaniciu"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["A. Paszke", "A. Chaurasia", "S. Kim", "E. Culurciello"], "venue": "arXiv preprint arXiv:1606.02147", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Training regionbased object detectors with online hard example mining", "author": ["A. Shrivastava", "A. Gupta", "R. Girshick"], "venue": "CVPR, pages 761\u2013769", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural  (a) input image (b) ground truth (c) LC Figure 9: Visual quality of LC label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions) and (c) LC", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "networks from overfitting. JMLR, 15(1):1929\u20131958", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional network cascade for facial point detection", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "CVPR, pages 3476\u2013 3483", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Inception-v4", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "inception-resnet and the impact of residual connections on learning. arXiv preprint arXiv:1602.07261", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "CVPR, pages 1653\u20131660", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Filter-based meanfield inference for random fields with higher-order terms and product label-spaces", "author": ["V. Vineet", "J. Warrell", "P.H. Torr"], "venue": "In ECCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "CVPR, pages I\u2013511", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Temporal segment networks: Towards good practices for deep action recognition", "author": ["L. Wang", "Y. Xiong", "Z. Wang", "Y. Qiao", "D. Lin", "X. Tang", "L. Val Gool"], "venue": "ECCV", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "and A", "author": ["Z. Wu", "C. Shen"], "venue": "v. d. Hengel. High-performance semantic segmentation using very deep fully convolutional networks. arXiv preprint arXiv:1604.04339", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Context driven scene parsing with attention to rare classes", "author": ["J. Yang", "B. Price", "S. Cohen", "M.-H. Yang"], "venue": "CVPR, pages 3294\u20133301", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields as recurrent neural networks. arXiv:1502.03240v2", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Semantic image segmentation enjoys wide applications, such as video surveillance [9, 36] and autonomous driving [10, 5].", "startOffset": 81, "endOffset": 88}, {"referenceID": 35, "context": "Semantic image segmentation enjoys wide applications, such as video surveillance [9, 36] and autonomous driving [10, 5].", "startOffset": 81, "endOffset": 88}, {"referenceID": 9, "context": "Semantic image segmentation enjoys wide applications, such as video surveillance [9, 36] and autonomous driving [10, 5].", "startOffset": 112, "endOffset": 119}, {"referenceID": 4, "context": "Semantic image segmentation enjoys wide applications, such as video surveillance [9, 36] and autonomous driving [10, 5].", "startOffset": 112, "endOffset": 119}, {"referenceID": 12, "context": "Recent advanced deep architectures, such as the residual network (ResNet) [13] and Inception [32], significantly improve the accuracy of image segmentation by increasing the depth and number of parameters in deep models.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Recent advanced deep architectures, such as the residual network (ResNet) [13] and Inception [32], significantly improve the accuracy of image segmentation by increasing the depth and number of parameters in deep models.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "For example, ResNet-101 is six times deeper than VGG-16 [29] network, with the former outperforms the latter by 4 percent on the challenging PASCAL VOC 2012 image segmentation benchmark [8].", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "For example, ResNet-101 is six times deeper than VGG-16 [29] network, with the former outperforms the latter by 4 percent on the challenging PASCAL VOC 2012 image segmentation benchmark [8].", "startOffset": 186, "endOffset": 189}, {"referenceID": 17, "context": "Layer Cascade inherits the advantage of the conventional model cascade (MC) [18, 35], which has multiple stages and usually trains one classifier in each stage.", "startOffset": 76, "endOffset": 84}, {"referenceID": 34, "context": "Layer Cascade inherits the advantage of the conventional model cascade (MC) [18, 35], which has multiple stages and usually trains one classifier in each stage.", "startOffset": 76, "endOffset": 84}, {"referenceID": 31, "context": "After applying LC on Inception-ResNet-v2 (IRNet) [32], its speed and accuracy are improved by 42.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "(3) Connections between LC and previous models such as model cascade, deeply supervised network [17], and dropout [30] are clearly presented.", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "(3) Connections between LC and previous models such as model cascade, deeply supervised network [17], and dropout [30] are clearly presented.", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 75, "endOffset": 91}, {"referenceID": 15, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 75, "endOffset": 91}, {"referenceID": 33, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 75, "endOffset": 91}, {"referenceID": 37, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 75, "endOffset": 91}, {"referenceID": 2, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 241, "endOffset": 260}, {"referenceID": 21, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 241, "endOffset": 260}, {"referenceID": 22, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 241, "endOffset": 260}, {"referenceID": 24, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 241, "endOffset": 260}, {"referenceID": 39, "context": "While early efforts focused on structural models with handcrafted features [15, 16, 34, 38], recent studies employ deep convolutional neural network (CNN) to learning strong representation, which improves segmentation accuracy significantly [3, 22, 23, 25, 40].", "startOffset": 241, "endOffset": 260}, {"referenceID": 24, "context": "[25] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [7].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[25] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "[3], Zheng et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 39, "context": "[40], and Liu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22, 23] further showed that back-propagation and inference of Markov Random Field (MRF) can be incorporated into CNN.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[22, 23] further showed that back-propagation and inference of Markov Random Field (MRF) can be incorporated into CNN.", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "Another line of research [1, 21, 27] alleviates this problem by using lightweight network architectures.", "startOffset": 25, "endOffset": 36}, {"referenceID": 20, "context": "Another line of research [1, 21, 27] alleviates this problem by using lightweight network architectures.", "startOffset": 25, "endOffset": 36}, {"referenceID": 26, "context": "Another line of research [1, 21, 27] alleviates this problem by using lightweight network architectures.", "startOffset": 25, "endOffset": 36}, {"referenceID": 0, "context": "For example, SegNet [1] adopted a convolutional encoder-decoder and removed unnecessary layers to reduce the number of parameters.", "startOffset": 20, "endOffset": 23}, {"referenceID": 26, "context": "ENet [27] utilized a bottleneck module to reduce computation of convolutions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 17, "endOffset": 36}, {"referenceID": 17, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 17, "endOffset": 36}, {"referenceID": 25, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 17, "endOffset": 36}, {"referenceID": 32, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 17, "endOffset": 36}, {"referenceID": 23, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 17, "endOffset": 36}, {"referenceID": 25, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 32, "context": "Network cascades [2, 18, 26, 33, 24] have been studied to improve the performance in classification [26], detection [18], and pose estimation [33].", "startOffset": 142, "endOffset": 146}, {"referenceID": 25, "context": "For example, Deep Decision Network [26] improved the image classification performance by dividing easy data from the", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "[18] used CNN cascade for face detection, which rejects false detections quickly in early stages and carefully refines detections in later stages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "DeepPose [33] employed a divide-and-conquer strategy and designed a cascaded deep regression framework for human pose estimation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "1 takes Inception-ResNet-v2 [32] as an example to illustrate how one could turn a deep model into LC.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "Similar network structure as IRNet has achieved great success in image recognition [32].", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "We also replace convolutions in \u2018IRNet-B/C\u2019 by the dilated convolutions similar to [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "8), making the batch normalization (BN) layers [14] unstable (as which need to estimate sample mean and variance from the data in a mini-batch).", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "The number of stages is three, which is a common setting in previous cascade methods [18, 31, 33].", "startOffset": 85, "endOffset": 97}, {"referenceID": 30, "context": "The number of stages is three, which is a common setting in previous cascade methods [18, 31, 33].", "startOffset": 85, "endOffset": 97}, {"referenceID": 32, "context": "The number of stages is three, which is a common setting in previous cascade methods [18, 31, 33].", "startOffset": 85, "endOffset": 97}, {"referenceID": 12, "context": "Specifically, (c) shows how to apply RC on a residual module, which can be represented as h(I) = I + conv(I), where feature h is attained by an identity mapping [13] of I and a convolution over I .", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "This step is similar to deeply supervised network (DSN) [17], which has multiple identical loss functions in different layers of the network.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "LC also relates to deeply supervised nets (DSN) [17] and dropout [30].", "startOffset": 48, "endOffset": 52}, {"referenceID": 29, "context": "LC also relates to deeply supervised nets (DSN) [17] and dropout [30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "We evaluate our method on the PASCAL VOC 2012 (VOC12) [8] and Cityscapes [5] datasets.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "We evaluate our method on the PASCAL VOC 2012 (VOC12) [8] and Cityscapes [5] datasets.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Following previous works, we also use the extra annotations provided by [12], which contains 10, 582 images for training, 1, 449 images for validation, and 1, 456 images for testing.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "This is consistent with existing studies [19, 4].", "startOffset": 41, "endOffset": 48}, {"referenceID": 3, "context": "This is consistent with existing studies [19, 4].", "startOffset": 41, "endOffset": 48}, {"referenceID": 31, "context": "IRNet [32] 72.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "22 DSN [17] 72.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "70 DSN [17] + Dropout [30] 72.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "70 DSN [17] + Dropout [30] 72.", "startOffset": 22, "endOffset": 26}, {"referenceID": 10, "context": "as hard negative mining [11, 28] which improves the performance.", "startOffset": 24, "endOffset": 32}, {"referenceID": 27, "context": "as hard negative mining [11, 28] which improves the performance.", "startOffset": 24, "endOffset": 32}, {"referenceID": 31, "context": "\u2022 IRNet [32]: We use the model describe in Sec.", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "\u2022 DSN [17]: By setting \u03c1 = 1, we make LC degenerate to a DSN, where each stage process all regions and has full supervision as the final target.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "\u2022 DSN [17] + Dropout [30]: To distinguish our method from dropout, LC is compared against DSN equipped with random label dropout in each stage.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "\u2022 DSN [17] + Dropout [30]: To distinguish our method from dropout, LC is compared against DSN equipped with random label dropout in each stage.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "in semantic segmentation [25], which effectively prevents gradients exploding or vanishing, it renders the advantages of deeply supervision marginal.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "To highlight the trade-off between performance and speed, we compare the proposed LC model with two representative state-ofthe-art methods, DeepLab-v2 [4] and SegNet [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "To highlight the trade-off between performance and speed, we compare the proposed LC model with two representative state-ofthe-art methods, DeepLab-v2 [4] and SegNet [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "DeepLab-v2 [4] 70.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "1 SegNet [1] 59.", "startOffset": 9, "endOffset": 12}, {"referenceID": 19, "context": "Approaches pre-trained on COCO [20] are marked with \u2020.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "FCN [25] 76.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "2 DeepLab [3] 84.", "startOffset": 10, "endOffset": 13}, {"referenceID": 39, "context": "6 RNN [40] 87.", "startOffset": 6, "endOffset": 10}, {"referenceID": 36, "context": "0 Adelaide [37] 91.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "1 RNN\u2020 [40] 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "7 BoxSup\u2020 [6] 89.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "2 DPN\u2020 [22] 89.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "5 DeepLab-v2\u2020 [4] 92.", "startOffset": 14, "endOffset": 17}, {"referenceID": 39, "context": "RNN [40] 2 96.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "5 DeepLab [3] 2 97.", "startOffset": 10, "endOffset": 13}, {"referenceID": 24, "context": "1 FCN [25] no 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "3 DPN [22] no 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "8 Dilation10 [39] no 97.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "1 DeepLab-v2 [4] no 97.", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "4 Adelaide [19] no 98.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "In this section, we show that LC can achieve state-of-theart performance on standard benchmarks like VOC12 [8] and Cityscapes [5] datasets.", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "In this section, we show that LC can achieve state-of-theart performance on standard benchmarks like VOC12 [8] and Cityscapes [5] datasets.", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Following [4], atrous spatial pyramid pooling [4], three-scale testing and dense CRF [16] are employed.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Following [4], atrous spatial pyramid pooling [4], three-scale testing and dense CRF [16] are employed.", "startOffset": 46, "endOffset": 49}, {"referenceID": 15, "context": "Following [4], atrous spatial pyramid pooling [4], three-scale testing and dense CRF [16] are employed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "The approaches pre-trained on COCO [20] are marked with \u2020.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "[19]\u2019s performance is slightly better than ours, however, LC still wins on 9 out of 19 classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "It is noticed that [19] used a deeper backbonenetwork and explored richer contextual information.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "In Table 6, we compare the settings of different bestperforming methods on VOC12 [8] test set, including CRFRNN [40], DPN [22] and DeepLab-v2 [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 39, "context": "In Table 6, we compare the settings of different bestperforming methods on VOC12 [8] test set, including CRFRNN [40], DPN [22] and DeepLab-v2 [4].", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "In Table 6, we compare the settings of different bestperforming methods on VOC12 [8] test set, including CRFRNN [40], DPN [22] and DeepLab-v2 [4].", "startOffset": 122, "endOffset": 126}, {"referenceID": 3, "context": "In Table 6, we compare the settings of different bestperforming methods on VOC12 [8] test set, including CRFRNN [40], DPN [22] and DeepLab-v2 [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 31, "context": "IRNet-LC uses Inception-ResNet-v2(IRNet) [32] as backbone network, which is smaller than ResNet-101 (35.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "Following DeepLab-v2 [4], atrous spatial pyramid pooling is employed in IRNet-LC.", "startOffset": 21, "endOffset": 24}, {"referenceID": 39, "context": "CRF-RNN [40] VGG [29] 134.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "CRF-RNN [40] VGG [29] 134.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "7 DPN [22] VGG [29] 134.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "7 DPN [22] VGG [29] 134.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "5 DeepLab-v2 [4] ResNet-101 [13] 44.", "startOffset": 13, "endOffset": 16}, {"referenceID": 12, "context": "5 DeepLab-v2 [4] ResNet-101 [13] 44.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "IRNet-LC IRNet [32] 35.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "2 IRNet-LC IRNet [32] 35.", "startOffset": 17, "endOffset": 21}, {"referenceID": 31, "context": "5 IRNet-LC IRNet [32] 35.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "without pre-training on MS COCO [20], demonstrating the effectiveness of the Layer Cascade framework.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "8 demonstrates the comparisons of LC with DPN [22] and DeepLab-v2 [4].", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "8 demonstrates the comparisons of LC with DPN [22] and DeepLab-v2 [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "We also include more examples of LC label maps on Cityscapes dataset [5] in Fig.", "startOffset": 69, "endOffset": 72}], "year": 2017, "abstractText": "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and \u2018difficulty-aware\u2019 learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an endto-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.", "creator": "LaTeX with hyperref package"}}}