{"id": "1704.01631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2017", "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition", "abstract": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set. For the first time, the signal encoding of low-level tasks is performed in a high-order encoding model using intermediate-level control. Importantly, these studies provide evidence that lower-level performance in the low-level control is more effective than in the high-order encoding model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 5 Apr 2017 19:44:23 GMT  (109kb,D)", "http://arxiv.org/abs/1704.01631v1", null], ["v2", "Wed, 19 Apr 2017 16:01:53 GMT  (109kb,D)", "http://arxiv.org/abs/1704.01631v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shubham toshniwal", "hao tang", "liang lu", "karen livescu"], "accepted": false, "id": "1704.01631"}, "pdf": {"name": "1704.01631.pdf", "metadata": {"source": "CRF", "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition", "authors": ["Shubham Toshniwal", "Hao Tang", "Liang Lu", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "1. Introduction Automatic speech recognition (ASR) has historically been addressed with modular approaches, in which multiple parts of the system are trained separately. For example, traditional ASR systems include components like frame classifiers, phonetic acoustic models, lexicons (which may or may not be learned from data), and language models [1]. These components typically correspond to different levels of representation, such as frame-level triphone states, phones, and words. Breaking up the task into such modules makes it easy to train each of them separately, possibly on different data sets, and to study the effect of modifying each component separately.\nOver time, ASR research has moved increasingly toward training multiple components of ASR systems jointly. Typically, such approaches involve training initial separate modules, followed by joint fine-tuning using sequence-level losses [2, 3]. Recently, completely integrated end-to-end training approaches, where all parameters are learned jointly using a loss at the final output level, have become viable and popular. End-to-end training is especially natural for deep neural network-based models, where the final loss gradient can be backpropagated through all layers. Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].\nEnd-to-end training is appealing because it is conceptually simple and allows all model parameters to contribute to the same final goal, and to do so in the context of all other model parameters. End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13]. On the other hand, end-to-end training has some drawbacks: Optimization can be challenging; the intermediate learned repre-\nsentations are not interpretable, making the system hard to debug; and the approach ignores potentially useful domain-specific information about intermediate representations, as well as existing intermediate levels of supervision.\nPrior work on analyzing deep end-to-end models has found that different layers tend to specialize for different sub-tasks, with lower layers focusing on lower-level tasks and higher ones on higher-level tasks. This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].\nWe propose an approach for deep neural ASR that aims to maintain the advantages of end-to-end approaches, while also including the domain knowledge and intermediate supervision used in modular systems. We use a multitask learning approach that combines the final task loss (in our case, log loss on the output labels) with losses corresponding to lower-level tasks (such as phonetic recognition) applied on lower layers. This approach is intended to encapsulate the intuitive and empirical observation that different layers encode different levels of information, and to encourage this effect more explicitly. In other words, while we want the end-to-end system to take input acoustics and produce output text, we also believe that at some appropriate intermediate layer, the network should do a good job at distinguishing more basic units like states or phones. Similarly, while end-to-end training need not require supervision at intermediate (state/phone) levels, if they are available then our multitask approach can take advantage of them.\nWe demonstrate this approach on a neural attention-based encoder-decoder character-level ASR model. Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification. We find that applying an auxiliary loss at an appropriate intermediate layer of the encoder improves performance over the baseline."}, {"heading": "2. Related Work", "text": "Multitask training has been studied extensively in the machine learning literature [21]. Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27]. Most prior work combines multiple losses applied at the final output layer of the model, such as joint Mandarin character and phonetic recognition in [26] and joint CTC and attention-based training for English ASR [25]. Our work differs from this prior work in that our losses relate to different types of supervision and are applied different levels of the model.\nThe idea of using low-level supervision at lower levels was, to our knowledge, first introduced by S\u00f8gaard & Goldberg [28] for natural language processing tasks, and has since been extended by [29]. The closest work to ours is the approach of Rao and Sak [30] using phoneme labels for training a multi-accent CTC-based ASR system in a multitask setting. Here we study\nar X\niv :1\n70 4.\n01 63\n1v 1\n[ cs\n.C L\n] 5\nA pr\n2 01\n7\nthe approach in the context of encoder-decoder models, and we compare a number of low-level auxiliary losses.\n3. Models The multitask approach we propose can in principle be applied to any type of deep end-to-end model. Here we study the approach in the context of attention-based deep RNNs. Below we describe the baseline model, followed by the auxiliary low-level training tasks."}, {"heading": "3.1. Baseline Model", "text": "The model is based on attention-enabled encoder-decoder RNNs, proposed by [19]. The speech encoder reads in acoustic features x = (x1, . . . ,xT ) and outputs a sequence of high-level features (hidden states) h which the character decoder attends to in generating the output character sequence y = (y1, . . . , yK), as shown in Figure 1 (the attention mechanism and a pyramidal LSTM layer are not shown in the figure for simplicity).\n3.1.1. Speech Encoder\nThe speech encoder is a deep pyramidal bidirectional Long ShortTerm Memory [31] (BiLSTM) network [4]. In the first layer, a BiLSTM reads in acoustic features x and outputs h(1) = (h\n(1) 1 , . . . ,h (1) T ) given by:\n\u2212\u2212\u2192 h\n(1) i = f (1)(xi, \u2212\u2212\u2192 h (1) i\u22121)\n\u2190\u2212\u2212 h\n(1) i = b (1)(xi, \u2190\u2212\u2212 h (1) i+1)\nh (1) i = (\n\u2212\u2212\u2192 h (1) i ; \u2190\u2212\u2212 h (1) i )\nwhere i \u2208 {1, . . . , T} denotes the index of the timestep; f (1)(\u00b7) and b(1)(\u00b7) denote the first layer forward and backward LSTMs respectively1.\nThe first layer output h(1) = (h(1)1 , . . . ,h (1) T ) is then pro-\n1For brevity we exclude the LSTM equations. The details can be found, e.g., in Zaremba et al. [32].\ncessed as follows: \u2212\u2212\u2192 h\n(j) i = f (j)([h (j\u22121) 2i\u22121 ;h (j\u22121) 2i ],\n\u2212\u2212\u2192 h\n(j) i\u22121)\u2190\u2212\u2212 h (j) i = b (j)([h (j\u22121) 2i\u22121 ;h (j\u22121) 2i ], \u2190\u2212\u2212 h (j) i+1)\nh (j) i = (\n\u2212\u2212\u2192 h (j) i ; \u2190\u2212\u2212 h (j) i )  for j = 2, 3, 4 where f (j) and b(j) denote the forward and backward running LSTMs at layer j. Following [4], we use pyramidal layers to reduces the time resolution of the final state sequence h(4) by a factor of 23 = 8. This reduction brings down the input sequence length, initially T = |x|, where | \u00b7 | denotes the length of a sequence of vectors, close to the output sequence length2, K = |y|. For simplicity, we will refer to h(4) as h.\n3.1.2. Character Decoder\nThe character decoder is a single-layer LSTM that predicts a sequence of characters y as follows:\nP (y|x) = P (y|h) = K\u220f t=1 P (yt|h,y<t).\nThe conditional dependence on the encoder state vectors h is represented by context vector ct, which is a function of the current decoder hidden state and the encoder state sequence:\nuit = v >tanh(W1hi +W2dt + ba) \u03b1t = softmax(ut) ct = |h|\u2211 i=1 \u03b1ithi\nwhere the vectors v, ba and the matricesW1,W2 are learnable parameters; dt is the hidden state of the decoder at time step t. The time complexity of calculating the context vector ct for every time step is O(|h|); reducing the resolution on encoder side is crucial to reducing this runtime.\nThe hidden state of the decoder, dt, which captures the previous character context y<t, is given by:\ndt = g(y\u0303t\u22121,dt\u22121, ct\u22121)\n2For Switchboard, the average of number of frames per character is about 7.\nwhere g(\u00b7) is the transformation of the single-layer LSTM, dt\u22121 is the previous hidden state of the decoder, and y\u0303t\u22121 is a character embedding vector for yt\u22121, as is typical practice in RNNbased language models. Finally, the posterior distribution of the output at time step t is given by:\nP (yt|h,y<t) = softmax(Ws[ct;dt] + bs),\nand the character decoder loss function is then defined as\nLc = \u2212 logP (y|x)."}, {"heading": "3.2. Low-Level Auxiliary Tasks", "text": "As shown in Figure 1, we explore multiple types of auxiliary tasks in our multitask approach. We explore two types of auxiliary labels for multitask learning: phonemes and sub-phonetic states. We hypothesize that the intermediate representations needed for sub-phonetic state classification are learned at the lowest layers of the encoder, while representations for phonetic prediction may be learned at a somewhat higher level.\n3.2.1. Phoneme-Based Auxiliary Tasks\nWe use phoneme-level supervision obtained from the word-level transcriptions and pronunciation dictionary. We consider two types of phoneme transcription loss: Phoneme Decoder Loss: Similar to the character decoder described above, we can attach a phoneme decoder to the speech encoder as well. The phoneme decoder has exactly the same mathematical form as the character decoder, but with a phoneme label vocabulary at the output. Specifically, the phoneme decoder loss is defined as\nLDecp = \u2212 logP (z|x),\nwhere z is the target phoneme sequence. Since this decoder can be attached at any depth of the four-layer encoder described above, we have four depths to choose from. We attach the phoneme decoder to layer 3 of the speech encoder, and also compare this choice to attaching it to layer 4 (the final layer) for comparison with a more typical multitask training approach.\nCTC Loss: A CTC [33] output layer can also be added to various layers of the speech encoder [30]. This involves adding an extra softmax output layer on top of the chosen intermediate layer of the encoder, and applying the CTC loss to the output of this softmax layer. Specifically, let z be the target phoneme sequence, and k be the speech encoder layer where the loss is applied. The probability of z given the input sequence is\nP (z|x) = \u2211\n\u03c0\u2208B\u22121(z)\nP (\u03c0|h(k)) = \u2211\n\u03c0\u2208B\u22121(z)\nJ\u220f j=1 P (\u03c0j |h(k)j ),\nwhere B(\u00b7) removes repetitive symbols and blank symbols, B\u22121 is B\u2019s pre-image, J is the number of frames at layer k and P (\u03c0j |h(k)j ) is computed by a softmax function. The final CTC objective is LCTCp = \u2212 logP (z|x). The CTC objective computation requires the output length to be less than the input length, i.e., |z| < J . In our case the encoder reduces the time resolution by a factor of 8 between the input and the top layer, making the top layer occasionally shorter than the number of phonemes in an utterance. We therefore cannot apply this loss to the topmost layer, and use it only at the third layer.3\n3In fact, even at the third layer we find occasional instances (about 10 utterances in our training set) where the hidden state sequence is shorter\n3.2.2. State-Level Auxiliary Task\nSub-phonetic state labels provide another type of low-level supervision that can be borrowed from traditional modular HMMbased approaches. We apply this type of supervision at the frame level, as shown in Figure 1, using state alignments obtained from a standard HMM-based system. We apply this auxiliary task at layer 2 of the speech encoder. The probability of a sequence of states s is defined as\nP (s|x) = M\u220f m=1 P (sm|x) = M\u220f m=1 P (sm|h(2)m ),\nwhere P (sm|h(2)m ) is computed by a softmax function, and M is the number of frames at layer 2 (in this case dT/2e). Since we use this task at layer 2, we subsample the state labels to match the reduced resolution. The final state-level loss is\nLs = \u2212 logP (s|x).\n3.2.3. Training Loss\nThe final loss function that we minimize is the average of the losses involved. For example, in the case where we use the character and phoneme decoder losses and the state-level loss, the loss would be\nL = 1\n3 (Lc + L\nDec p + Ls).\n4. Experiments We use the Switchboard corpus (LDC97S62) [34], which contains roughly 300 hours of conversational telephone speech, as our training set. We reserve the first 4K utterances as a development set. Since the training set has a large number of repetitions of short utterances (\u201cyeah\u201d, \u201cuh-huh\u201d, etc.), we remove duplicates beyond a count threshold of 300. The final training set has about 192K utterances. For evaluation, we use the HUB5 Eval2000 data set (LDC2002S09), consisting of two subsets: Switchboard (SWB), which is similar in style to the training set, and CallHome (CHE), which contains unscripted conversations between close friends and family.\nFor input features, we use 40-dimensional log-mel filterbank features along with their deltas, normalized with per-speaker mean and variance normalization. The phoneme labels for the auxiliary task are generated by mapping words to their canonical pronunciations, using the lexicon in the Kaldi Switchboard training recipe. The HMM state labels were obtained via forced alignment using a baseline HMM/DNN hybrid system using the Kaldi NNet1 recipe. The HMM/DNN has 8396 tied states, which makes the frame-level softmax costly for multitask learning. We use the importance sampling technique described in [35] to reduce this cost."}, {"heading": "4.1. Model Details and Inference", "text": "The speech encoder is a 4-layer pyramidal bidirectional LSTM, resulting in a 8-fold reduction in time resolution. We use 256 hidden units in each direction of each layer. The decoder for all tasks is a single-layer LSTM with 256 hidden units. We represent the decoders\u2019 output symbols (both characters and, at training time, phonemes) using 256-dimensional embedding vectors. At test time, we use a greedy decoder (beam size =\nthan the input sequence, due to sequences of phonemes of duration less than 4 frames each. Anecdotally, these examples appear to correspond to incorrect training utterance alignments\n1) to generate the character sequence. The character with the maximum posterior probability is chosen at every time step and fed as input into the next time step. The decoder stops after encountering the \u201cEOS\u201d (end-of-sentence) symbol. We use no explicit language model.\nWe train all models using Adam [36] with a minibatch size of 64 utterances. The initial learning rate is 1e-3 and is decayed by a factor of 0.95, whenever there is an increase in log-likelihood of the development data, calculated after every 1K updates, over its previous value. All models are trained for 75K gradient updates (about 25 epochs) and early stopping. To further control overfitting we: (a) use dropout [37] at a rate of 0.1 on the output of all LSTM layers (b) sample the previous step\u2019s prediction [20] in the character decoder, with a constant probability of 0.1 as in [4]."}, {"heading": "4.2. Results", "text": "We evaluate performance using word error rate (WER). We report results on the combined Eval2000 test set as well as separately on the SWB and CHE subsets. We also report character error rates (CER) on the development set.\nDevelopment set results are shown in Table 1. We refer to the baseline model as \u201cEnc-Dec\u201d and the models with multitask training as \u201cEnc-Dec + [auxiliary task]-[layer]\u201d. Adding phoneme recognition as an auxiliary task at layer 3, either with a separate LSTM decoder or with CTC, reduces both the character error rates and the final word error rates.\nIn order to determine whether the improved performance is a basic multitask training effect or is specific to the low-level application of the loss, we compare these results to those of adding the phoneme decoder at the topmost layer (Enc-Dec + PhoneDec-4). The top-layer application of the phoneme loss produces worse performance than having the supervision at the lower (third) layer. Finally, we obtain the best results by adding both phoneme decoder supervision at the third layer and framelevel state supervision at the second layer (Enc-Dec + PhoneDec3 + State-2). The results support the hypothesis that lower-level supervision is best provided at lower layers. Table 2 provides test set results, showing the same pattern of improvement on both\nthe SWB and CHE subsets. For comparison, we also include a variety of other recent results with neural end-to-end approaches on this task. Our baseline model has better performance than the most similar previous encoder-decoder result [7]. With the addition of the low-level auxiliary task training, our models are competitive with all of the previous end-to-end systems that do not use a language model.\nFigure 2 shows the training set log-likelihood for the baseline model and two multitask variants. The plot suggests that multitask training helps with optimization (improving the training error). Training error is very similar for both multitask models, while the development set performance is better for one of them (see Table 1), suggesting that there may also be an improved generalization effect and not only improved optimization."}, {"heading": "5. Conclusion", "text": "We have presented a multitask training approach for deep endto-end ASR models in which lower-level task losses are applied at lower levels, and we have explored this approach in the context of attention-based encoder-decoder models. Results on Switchboard and CallHome show consistent improvements over baseline attention-based models and support the hypothesis that lower-level supervision is more effective when applied at lower layers of the deep model. We have compared several types of auxiliary tasks, obtaining the best performance with a combination of a phoneme decoder and frame-level state loss. Analysis of model training and performance suggests that the addition of auxiliary tasks can help in either optimization or generalization.\nFuture work includes studying a broader range of auxiliary tasks and model configurations. For example, it would be interesting to study even deeper models and word-level output, which would allow for more options of intermediate tasks and placements of the auxiliary losses. Viewing the approach more broadly, it may be fruitful to also consider higher-level task supervision, incorporating syntactic or semantic labels, and to view the ASR output as an intermediate output in a more general hierarchy of tasks."}, {"heading": "6. Acknowledgements", "text": "We are grateful to William Chan for helpful discussions, and to the speech group at TTIC, especially Shane Settle, Herman Kamper, Qingming Tang, and Bowen Shi for sharing their data processing code. This research was supported by a Google faculty research award.\n7. References [1] M. Gales and S. Young, \u201cThe application of hidden markov models\nin speech recognition,\u201d Foundations and trends in signal processing, vol. 1, 2008.\n[2] K. Vesely\u0300, A. Ghoshal, L. Burget, and D. Povey, \u201cSequencediscriminative training of deep neural networks.\u201d in Interspeech, 2013.\n[3] D. Povey and B. Kingsbury, \u201cEvaluation of proposed modifications to mpe for large scale discriminative training,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2007.\n[4] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.\n[5] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, \u201cAttention-based models for speech recognition,\u201d in Neural Information Processing Systems (NIPS), 2015.\n[6] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, \u201cEnd-to-end attention-based large vocabulary speech recognition,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.\n[7] L. Lu, X. Zhang, and S. Renals, \u201cOn training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.\n[8] A. L. Maas, Z. Xie, D. Jurafsky, and A. Y. Ng, \u201cLexicon-free conversational speech recognition with neural networks,\u201d in North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL HLT), 2015.\n[9] G. Zweig, C. Yu, J. Droppo, and A. Stolcke, \u201cAdvances in allneural speech recognition,\u201d CoRR, vol. abs/1609.05935, 2016.\n[10] Y. Miao, M. Gowayyed, and F. Metze, \u201cEESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.\n[11] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep networks with stochastic depth,\u201d in European Conference on Computer Vision (ECCV), 2016.\n[12] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. E. Hinton, \u201cGrammar as a foreign language,\u201d in Neural Information Processing Systems (NIPS), 2015.\n[13] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. B. Vie\u0301gas, M. Wattenberg, G. Corrado, M. Hughes, and J. Dean, \u201cGoogle\u2019s multilingual neural machine translation system: Enabling zero-shot translation,\u201d CoRR, vol. abs/1611.04558, 2016.\n[14] A.-r. Mohamed, G. E. Hinton, and G. Penn, \u201cUnderstanding how deep belief networks perform acoustic modelling,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2012.\n[15] T. Nagamine, M. L. Seltzer, and N. Mesgarani, \u201cOn the role of nonlinear transformations in deep neural network acoustic models,\u201d Interspeech, 2016.\n[16] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolutional networks,\u201d in European Conference on Computer Vision (ECCV), 2014.\n[17] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature hierarchies for accurate object detection and semantic segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n[18] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Neural Information Processing Systems (NIPS), 2014.\n[19] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d in International Conference on Learning Representations (ICLR), 2015.\n[20] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled sampling for sequence prediction with recurrent neural networks,\u201d in Neural Information Processing Systems (NIPS), 2015.\n[21] R. Caruana, \u201cMultitask learning,\u201d Machine Learning, 1997.\n[22] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, \u201cNatural language processing (almost) from scratch,\u201d Journal of Machine Learning Research (JMLR), 2011.\n[23] M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, \u201cMultitask sequence to sequence learning,\u201d in International Conference on Learning Representations (ICLR), 2016.\n[24] A. Eriguchi, Y. Tsuruoka, and K. Cho, \u201cLearning to parse and translate improves neural machine translation,\u201d CoRR, vol. abs/1702.03525, 2017.\n[25] S. Kim, T. Hori, and S. Watanabe, \u201cJoint CTC-attention based end-to-end speech recognition using multi-task learning,\u201d CoRR, vol. abs/1609.06773, 2016.\n[26] W. Chan and I. Lane, \u201cOn online attention-based speech recognition and joint Mandarin character-Pinyin training,\u201d in Interspeech, 2016.\n[27] Z. Wu, C. Valentini-Botinhao, O. Watts, and S. King, \u201cDeep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\n[28] A. S\u00f8gaard and Y. Goldberg, \u201cDeep multi-task learning with low level tasks supervised at lower layers,\u201d in Annual Meeting of the Association for Computational Linguistics (ACL), 2016.\n[29] K. Hashimoto, C. Xiong, Y. Tsuruoka, and R. Socher, \u201cA joint many-task model: Growing a neural network for multiple NLP tasks,\u201d CoRR, vol. abs/1611.01587, 2016.\n[30] K. Rao and H. Sak, \u201cMulti-accent speech recognition with hierarchical grapheme based models,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.\n[31] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, 1997.\n[32] W. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural network regularization,\u201d CoRR, vol. abs/1409.2329, 2014.\n[33] A. Graves, S. Ferna\u0301ndez, and F. Gomez, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in International Conference on Machine Learning (ICML), 2006.\n[34] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSWITCHBOARD: Telephone speech corpus for research and development,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992.\n[35] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, \u201cOn using very large target vocabulary for neural machine translation,\u201d in Annual Meeting of the Association for Computational Linguistics (ACL), 2015.\n[36] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Journal of Machine Learning Research (JMLR), vol. 12, 2011.\n[37] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, \u201cDropout improves recurrent neural networks for handwriting recognition,\u201d in International Conference on Frontiers in Handwriting Recognition (ICFHR), 2014."}], "references": [{"title": "The application of hidden markov models in speech recognition", "author": ["M. Gales", "S. Young"], "venue": "Foundations and trends in signal processing, vol. 1, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequencediscriminative training of deep neural networks.", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "in Interspeech,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Evaluation of proposed modifications to mpe for large scale discriminative training", "author": ["D. Povey", "B. Kingsbury"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["L. Lu", "X. Zhang", "S. Renals"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL HLT), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in allneural speech recognition", "author": ["G. Zweig", "C. Yu", "J. Droppo", "A. Stolcke"], "venue": "CoRR, vol. abs/1609.05935, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "European Conference on Computer Vision (ECCV), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["M. Johnson", "M. Schuster", "Q.V. Le", "M. Krikun", "Y. Wu", "Z. Chen", "N. Thorat", "F.B. Vi\u00e9gas", "M. Wattenberg", "G. Corrado", "M. Hughes", "J. Dean"], "venue": "CoRR, vol. abs/1611.04558, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Understanding how deep belief networks perform acoustic modelling", "author": ["A.-r. Mohamed", "G.E. Hinton", "G. Penn"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "On the role of nonlinear transformations in deep neural network acoustic models", "author": ["T. Nagamine", "M.L. Seltzer", "N. Mesgarani"], "venue": "Interspeech, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "International Conference on Learning Representations (ICLR), 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research (JMLR), 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Multitask sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["A. Eriguchi", "Y. Tsuruoka", "K. Cho"], "venue": "CoRR, vol. abs/1702.03525, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Joint CTC-attention based end-to-end speech recognition using multi-task learning", "author": ["S. Kim", "T. Hori", "S. Watanabe"], "venue": "CoRR, vol. abs/1609.06773, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "On online attention-based speech recognition and joint Mandarin character-Pinyin training", "author": ["W. Chan", "I. Lane"], "venue": "Interspeech, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["A. S\u00f8gaard", "Y. Goldberg"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "A joint many-task model: Growing a neural network for multiple NLP tasks", "author": ["K. Hashimoto", "C. Xiong", "Y. Tsuruoka", "R. Socher"], "venue": "CoRR, vol. abs/1611.01587, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-accent speech recognition with hierarchical grapheme based models", "author": ["K. Rao", "H. Sak"], "venue": "International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2017.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, 1997.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez"], "venue": "International Conference on Machine Learning (ICML), 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "SWITCHBOARD: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research (JMLR), vol. 12, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "International Conference on Frontiers in Handwriting Recognition (ICFHR), 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For example, traditional ASR systems include components like frame classifiers, phonetic acoustic models, lexicons (which may or may not be learned from data), and language models [1].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "Typically, such approaches involve training initial separate modules, followed by joint fine-tuning using sequence-level losses [2, 3].", "startOffset": 128, "endOffset": 134}, {"referenceID": 2, "context": "Typically, such approaches involve training initial separate modules, followed by joint fine-tuning using sequence-level losses [2, 3].", "startOffset": 128, "endOffset": 134}, {"referenceID": 3, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 4, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 5, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 6, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 87, "endOffset": 99}, {"referenceID": 7, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 160, "endOffset": 166}, {"referenceID": 8, "context": "Typical end-to-end models are based on recurrent neural network (RNN) encoder-decoders [4, 5, 6, 7] or connectionist temporal classification (CTC)-based models [8, 9].", "startOffset": 160, "endOffset": 166}, {"referenceID": 3, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 8, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 9, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 67, "endOffset": 77}, {"referenceID": 10, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 11, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 12, "context": "End-to-end approaches have also achieved impressive results in ASR [4, 9, 10] as well as other domains [11, 12, 13].", "startOffset": 103, "endOffset": 115}, {"referenceID": 13, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 60, "endOffset": 68}, {"referenceID": 14, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 60, "endOffset": 68}, {"referenceID": 15, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 96, "endOffset": 104}, {"referenceID": 16, "context": "This effect has been found in systems for speech processing [14, 15] as well as computer vision [16, 17].", "startOffset": 96, "endOffset": 104}, {"referenceID": 17, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 7, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 18, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 3, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 6, "context": "Our baseline model is inspired by prior work [18, 8, 19, 4, 7], and our lowerlevel auxiliary tasks are based on phonetic recognition and framelevel state classification.", "startOffset": 45, "endOffset": 62}, {"referenceID": 20, "context": "Multitask training has been studied extensively in the machine learning literature [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 22, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 23, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 24, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 26, "context": "Its application to deep neural networks has been successful in a variety of settings in speech and language processing [22, 23, 24, 25, 26, 27].", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "Most prior work combines multiple losses applied at the final output layer of the model, such as joint Mandarin character and phonetic recognition in [26] and joint CTC and attention-based training for English ASR [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 24, "context": "Most prior work combines multiple losses applied at the final output layer of the model, such as joint Mandarin character and phonetic recognition in [26] and joint CTC and attention-based training for English ASR [25].", "startOffset": 214, "endOffset": 218}, {"referenceID": 27, "context": "The idea of using low-level supervision at lower levels was, to our knowledge, first introduced by S\u00f8gaard & Goldberg [28] for natural language processing tasks, and has since been extended by [29].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "The idea of using low-level supervision at lower levels was, to our knowledge, first introduced by S\u00f8gaard & Goldberg [28] for natural language processing tasks, and has since been extended by [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 29, "context": "The closest work to ours is the approach of Rao and Sak [30] using phoneme labels for training a multi-accent CTC-based ASR system in a multitask setting.", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The dotted line in the character decoder denotes the use of (sampled) model predictions [20] during training (for the phone decoder only the ground-truth prior phone is used in training).", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "The model is based on attention-enabled encoder-decoder RNNs, proposed by [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "The speech encoder is a deep pyramidal bidirectional Long ShortTerm Memory [31] (BiLSTM) network [4].", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "The speech encoder is a deep pyramidal bidirectional Long ShortTerm Memory [31] (BiLSTM) network [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Following [4], we use pyramidal layers to reduces the time resolution of the final state sequence h by a factor of 2 = 8.", "startOffset": 10, "endOffset": 13}, {"referenceID": 32, "context": "CTC Loss: A CTC [33] output layer can also be added to various layers of the speech encoder [30].", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "CTC Loss: A CTC [33] output layer can also be added to various layers of the speech encoder [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 33, "context": "We use the Switchboard corpus (LDC97S62) [34], which contains roughly 300 hours of conversational telephone speech, as our training set.", "startOffset": 41, "endOffset": 45}, {"referenceID": 34, "context": "We use the importance sampling technique described in [35] to reduce this cost.", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "We train all models using Adam [36] with a minibatch size of 64 utterances.", "startOffset": 31, "endOffset": 35}, {"referenceID": 36, "context": "To further control overfitting we: (a) use dropout [37] at a rate of 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "1 on the output of all LSTM layers (b) sample the previous step\u2019s prediction [20] in the character decoder, with a constant probability of 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "1 as in [4].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "[7] Enc-Dec 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] CTC 38.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Iterated CTC 24.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Our baseline model has better performance than the most similar previous encoder-decoder result [7].", "startOffset": 96, "endOffset": 99}], "year": 2017, "abstractText": "End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.", "creator": "LaTeX with hyperref package"}}}