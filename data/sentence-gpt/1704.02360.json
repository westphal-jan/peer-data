{"id": "1704.02360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities", "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. However, in theory, such pre-processing techniques do not directly distinguish speech patterns and speech quality from other neural models of speech processing, in addition to providing a high-resolution image-quality representation of speech. Furthermore, for this purpose, there is no direct or indirect test that can identify the identity of speech components or other associated factors. In addition, the standard VIC model may provide support for using such pre-processing techniques, and thus may be a useful framework to evaluate the effects of the VIC model on speech processing, as well as to improve how specific the sample was, as well as to assess potential differences in speech patterns. A combination of current and future studies may be required to further elucidate the impact of the VIC model on speech processing, and could improve the ability to measure the effects of a number of different experimental tools. Finally, for further research on the effects of learning and prediction on speech processing in humans, there is currently a theoretical framework for working with VIC models to obtain a set of models of speech processing and prediction of speech processing that provides a baseline for how the human brain functions and the development of neural systems of speech processing, as well as to assess the contribution of the VIC model to a variety of brain mechanisms. The study of human speech processing was performed in humans at a distance of 50 km, from which a neural network of four different neural networks could be implemented with the help of a computer generated dataset. The neural network is currently developed by a team of researchers working on a set of models of speech processing in humans that employ a common neural network model called vivodile, which is designed for modeling the human brain's cognitive processes. The neural network is derived from the neural network of the brain that is derived from the brain, and is designed for generating speech and prediction of speech. The brain system is based on a neural network of the same size and location, as well as a network of neurons that are involved in cognitive processing. In contrast, vivodile is a neural network of the same size and location. These neural networks include", "histories": [["v1", "Mon, 10 Apr 2017 12:35:33 GMT  (578kb)", "http://arxiv.org/abs/1704.02360v1", null], ["v2", "Mon, 17 Apr 2017 04:43:37 GMT  (583kb)", "http://arxiv.org/abs/1704.02360v2", "Submitted to INTERSPEECH 2017"], ["v3", "Mon, 22 May 2017 08:11:02 GMT  (583kb)", "http://arxiv.org/abs/1704.02360v3", "Accepted to INTERSPEECH 2017"], ["v4", "Mon, 7 Aug 2017 02:42:01 GMT  (675kb)", "http://arxiv.org/abs/1704.02360v4", "Accepted to INTERSPEECH 2017"]], "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["hiroyuki miyoshi", "yuki saito", "shinnosuke takamichi", "hiroshi saruwatari"], "accepted": false, "id": "1704.02360"}, "pdf": {"name": "1704.02360.pdf", "metadata": {"source": "CRF", "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities", "authors": ["Hiroyuki Miyoshi", "Yuki Saito"], "emails": ["mathma1306@gmail.com", "saruwatari}@ipc.i.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n02 36\n0v 1\n[ cs\n.S D\n] 1\n0 A\npr 2\n01 7\nof context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC. Index Terms: voice conversion, context posterior probabilities, sequence-to-sequence learning"}, {"heading": "1. Introduction", "text": "Voice conversion (VC) is a technique for converting para- and non-linguistic information while keeping linguistic information. VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3]. It is mainly classified into two types: text-independent VC and text-dependent VC. Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data. Since the models are often trained using parallel speech data, the conversion quality of the performance is typically highly accurate. However, in most cases, parallel data is not readily available. Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information. This type consists of two modules: speech recognition that estimates the textual information from the source speech, and speech synthesis that predicts target speech from the textual information. Basically, parallel data are not required to build the VC, and the training data are easily available. However, the conversion units of this method are rougher (e.g., phoneme, word, or other linguistic units) than those of text-independent VC (e.g., frame). VC using shared context posterior probabilities [9] is classified\nin text-dependent VC, but the conversion unit is frame level. The context posterior probability of source speech parameters is estimated frame by frame, and then the target speech parameters are predicted from the estimated posterior probabilities. This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11]. However, it cannot convert speaker individuality (e.g., speaking rates and phonetic properties) included in the context posterior probabilities because the posterior probabilities of the source speech are directly used for predicting target speech parameters.\nIn light of the above, we propose a sequence-to-sequence learning of the context posterior probabilities. Assuming that the training data partly include parallel speech data (parallel utterances of phrases), we build an encoder-decoder model [12] that converts the posterior probabilities of the source speech parameters into those of the target speech parameters. The proposed posterior probability conversion module is inserted between conventional speech recognition and synthesis. When we do not build the conversion model or do not have parallel data, conventional VC [9] is available. Further, we propose a joint training algorithm. Whereas the conventional VC [9] separately trains speech recognition and speech synthesis, our approach jointly trains these modules (like auto-encoding) and the proposed conversion module. We found through experiment that the proposed methods outperform the conventional VC [9]."}, {"heading": "2. VC Using Shared Context Posterior Probabilities", "text": "Conventional VC using shared context posterior probabilities [9] contains two modules: speech recognition and speech synthesis. They are separately trained, and the voice conversion is performed by concatenating them. Figure 1 shows an example of context posterior probabilities. The upper and middle parts of Fig. 2 show the details of these processes."}, {"heading": "2.1. Training Stage", "text": "Recognition models estimate context posterior probability sequence from the speech parameter sequence. Let x = [x\u22a41 , \u00b7 \u00b7 \u00b7 ,x \u22a4 Tx ]\u22a4 and y = [y\u22a41 , \u00b7 \u00b7 \u00b7 , y \u22a4 Ty ]\u22a4 be source and target speech parameter sequences, respectively. xt and yt are the parameters at frame t. Tx and Ty are their frame lengths. Also, let l(x) = [l (x) 1 , \u00b7 \u00b7 \u00b7 , l (x) Tx ]\u22a4 and l(y) = [l (y) 1 , \u00b7 \u00b7 \u00b7 , l (y) Ty ]\u22a4 be the context label sequence (such as quin-phone) corresponding to x and y, respectively. Speaker-independent neural network R(\u00b7) is trained using speech data including x and y, and the training criterion is minimizing the cross entropy LC(lx,R(x)).\nSynthesis models predict target speech parameter sequence y from the corresponding context posterior probability se-\nquence p\u0302y , using the trained recognition models, i.e., p\u0302y = R(y). The target-speaker-dependent neural networks G(\u00b7) are trained to minimize the mean squared error LG(y,G(p\u0302y)) between y and G(p\u0302y)."}, {"heading": "2.2. Conversion Stage", "text": "In conversion, the converted speech parameter sequence y\u0302 is predicted by concatenating speech recognition and speech synthesis, i.e., y\u0302 = G(p\u0302x) = G(R(x)), where p\u0302x is the context posterior probability sequence of x. Note that the frame lengths of x, p\u0302x and y\u0302 are the same, i.e., Tx."}, {"heading": "2.3. Problems", "text": "Since the posterior probabilities estimated in speech recognition are directly used for speech synthesis, it is difficult to convert speaker individuality included in the posterior probabilities, such as the speaking rate (frame length) and phonetic characteristics (see Fig. 1). Also, improving recognition accuracy does not always improve speech quality in converted speech (except zero error in recognition)."}, {"heading": "3. Proposed VC using Sequence-to-Sequence Learning of Context Posterior Probabilities", "text": "To overcome the limitation of the conventional method, we propose an approach for converting source context posterior probabilities to target context posterior probabilities using sequenceto-sequence learning."}, {"heading": "3.1. Sequence-to-Sequence Learning", "text": "Sequence-to-sequence learning using recurent neural networks (RNNs) [13] can be applied to the problem that the source and target sequences have different lengths. An encoder-decoder model we adopt here maps a variable-length source sequence to a fixed-length vector, and maps the vector to the variable-length target sequence. At each frame, the source side RNN (encoder) and target side RNN (decoder) predict the source and target features of the next frame, respectively. As discussed below, we adopt this in order to convert a source posterior probability sequence to a target that has a different length. The proposed procedure is shown in the lower part of Fig. 2."}, {"heading": "3.2. Training Stage", "text": "We propose two algorithms to perform the posterior probability conversion. The first algorithm separately trains speech recog-\n! \" # $\n#%&'('()\n!\"#$\n% & ' () $ * + $ $ ) , -.$ / !* 0\n1 / (2 $ ! * + $ $ ) , -.$ / !* 0 ! \" # $\n3&*!$(\"&(-+(&4*0 / \"\n'\n*+(,-%.'+(/01+(,-(2'+(&34\n0\n1 / (2 $ ! + & * !$ (\" & (- + (& 4 * 0\nnition and synthesis the same as the conventional algorithms and separately trains probability conversion models using the source and target posterior probabilities. The second algorithm jointly trains the recognition and synthesis and trains the conversion models considering not only posterior probabilities conversion but also speech synthesis. In conversion, we concatenate the three models for converting input speech parameters."}, {"heading": "3.2.1. Training of probability conversion models", "text": "Given the parallel sequences of source and target context posterior probabilities, we train encoder-decoder models C(\u00b7) that convert the source and target sequences. The loss function to be minimized is as follows:\nL(ly, p\u0302x, p\u0302y) = LG(p\u0302y ,C(p\u0302x)) + LC(ly ,C(p\u0302x)). (1)\nThe first term minimizes the conversion error between the predicted and target sequences. The second term minimizes the cross-entropy using ly , which was obtained by the training of R(\u00b7), and can decrease the recognition error included in p\u0302y. Our preliminary evaluation demonstrated that using this formulation results in better conversion accuracy than using only the first term.\nSequence-to-sequence learning suffers from long-term dependencies, i.e., error accumulation, so in our approach, we implement phoneme-by-phoneme probability conversion. Given the phoneme boundary of the source and target probability sequence, phoneme-independent encoder-decoder models are trained to convert the probability sequence within the current phoneme."}, {"heading": "3.2.2. Jointly training of recognition, synthesis, and conversion", "text": "Since the final goal of the method is to minimize synthesis error, its pre-processes (i.e., recognition and conversion) must be trained by considering this error. We train speech recognition R(\u00b7) to minimize not only recognition error but also synthesis\nerror (e.g., reconstruction error of auto-encoders): the loss function is LC(lx,R(x))+LG(x,G(R(x))). The speech synthesis G(\u00b7) is trained in the conventional manner. We further train the conversion models to minimize not only conversion error but also synthesis error: the loss function is the sum of Eq. (1) and LG(G(y,C(p\u0302(x))))."}, {"heading": "3.3. Discussion", "text": "Text-dependent VC forcibly aligns the input speech feature segment into a single context (e.g., phoneme, syllable, or word unit) and generates output speech features from the context sequence. Although this method can flexibly transform the context sequence (e.g., variable-length conversion), it cannot avoid the effect of time quantization with mapping from speech feature segments. Meanwhile, text-independent VC with dynamic time warping (DTW) [4] aligns speech features in a frame level, but it limits the transformation of (implicitly considered) context sequence, e.g., the sequence length is fixed. The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech. In comparison with these methods, since the proposed algorithm performs frame-level conversion without forced alignments, it can avoid the effect of time quantization and convert context sequence flexibly.\nJoint training of recognition and synthesis, which is proposed in Section 3.2.2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15]. Therefore, we expect that these processes can be extended to the supervised learning of variational auto-encoders [16] that have not only class labels (e.g., context labels) but also the hidden variables [17, 18]."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "Although the conventional VC [9] and proposed VC accept non-parallel speech data and partly included parallel data, we used fully parallel data in this evaluation. We prepared speech data of eight speakers taken from the ATR Japanese speech database [19]. The speaker uttered 503 phonetically balanced sentences. We built the speaker-independent speech recognition module with speaker codes by using the speech data of eight speakers including source and target speakers. We built conversion and synthesis modules by using speech data of source female and target male speakers. We used 450 sentences (subsets A to I) for the training and 53 sentences (subset J) for the evaluation. Speech signals were sampled at a rate of 16 kHz, and the shift length was set to 5 ms. The 0th\u2013through\u201324th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters. We used the STRAIGHT analysis-synthesis system [22] for the parameter extraction and waveform synthesis. To improve training accuracy, speech parameter trajectory smoothing [23] with a 50 Hz cutoff modulation frequency was applied to the spectral parameters in the training data. In the training phase, spectral features were normalized to have zero-mean unit-variance, and 80 % of the silent frames were removed from the training data. We used AdaGrad [24] as the optimization algorithm, setting the learning rate to 0.01. Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units. For probability conversion, we used bi-directional LSTM in encoder, and LSTM in decoder. Each hidden layers contained 256\nunits.\nQuin-phone was used as the context labels. For training the recognition models, we divided the quin-phone into five groups: previous phoneme, current phoneme, and next phoneme, and so on. The cross-entropy loss was calculated for each group, and the loss function for training the recognition models was the sum of each loss [25]. Only spectral and their delta features were used for recognition and synthesis. In the proposed method, F0 was linearly transformed [4] first, and we modified its length using DTW between the source context posterior probability sequence and the converted posterior probability sequence. Only DTW was used for band-aperiodicity conversion. This evaluation uses reference phoneme duration for converting posterior probabilities in order to address the sequence length determination problem to which sequence-to-sequence learning is prone [26]. Given the phoneme duration of the target natural speech parameter sequence in conversion data, we performed phoneme-level probability conversion. Given the phoneme duration of the source and target speech parameters in the training and conversion data, the conversion models converted the probabilities within the current phoneme. The finally generated posterior probability sequence was then calculated by concatenating the converted probabilities.\nTwo evaluations were performed to compare the conventional and proposed VC. First, we evaluated the effectiveness of the proposed posterior probability conversion, and then, we evaluated the effect of the proposed joint training algorithms."}, {"heading": "4.2. Evaluations", "text": "We discuss the effectiveness of the proposed posterior probability conversion. The separately trained modules were used here."}, {"heading": "4.2.1. Objective Evaluation", "text": "We calculated mel-cepstral distortion between the target and converted speech parameters of the conventional VC [9] and proposed VC. DTW was used to align the target and converted parameters by the conventional VC. The difference between the two methods is the time warping method, i.e., DTW or sequence-to-sequence learning. The results (Fig. 3) clearly show that the proposed VC outperforms the conventional VC, we demonstrate that spectral distortion caused by DTW can be alleviated by the use of sequence-to-sequence learning."}, {"heading": "4.2.2. Subjective Evaluation", "text": "To subjectively evaluate the conventional and proposed VC, we performed a preference AB test to evaluate the converted speech quality. We presented every pair of converted speech of the two sets in random order and had listeners select the speech sample that sounded better. Similarly, we performed an XAB test on the speaker individuality using the natural speech as a reference \u201cX.\u201d Seven listeners participated in each assessment.\nThe results are shown in Fig. 4. Althogh the proposed VC performed better in speaker similarity (Fig. 4(a)) thanks to posterior probability conversion, it degrades speech quality (Fig. 4(b)). It seems that the probability conversion caused con-\nversion error that missed the phonetic properties of the source speech parameters, which is probably what resulted in the degraded quality."}, {"heading": "4.3. Evaluation of Joint Training", "text": ""}, {"heading": "4.3.1. Joint Training of Recognition and Synthesis", "text": "We evaluated the effectiveness of joint training of recognition and synthesis modules, in comparison to conventional separately trained modules [9]. We calculated mel-cepstral distortion in the auto-encoding case, i.e., reconstruction error of source speech parameters through recognition and synthesis. As shown in Fig. 5, the proposed joint training achieved better distortion than conventional separated training.\nWe also performed an AB test on speech quality and XAB test on speaker similarity in the VC case, as similar as in Section 4.2.2. As shown in Fig. 6, the proposed joint-training overcomes the conventional separated training in both speaker similarity and speech quality."}, {"heading": "4.3.2. Joint Training of Recognition, Conversion, and Synthesis", "text": "To evaluate of the joint training of recognition, conversion, and synthesis, we calculated the mel-cepstral distortion of three systems: (1) conventional VC [9], (2) separately trained modules (equal to \u201dProposed\u201d in Section 4.2.1), and (3) jointly trained modules.\nThe results are shown in Fig. 7. The joint training scored better than conventional VC but worse than separated training. To clarify this, we show in Fig. 8 an example of probability sequence estimated through speech recognition. The separately trained recognition module outputs harder probabilities, i.e., the values are close to 0 or 1 for all frames. However, we can see that the joint training of recognition and synthesis tends to make the values soft. This requires deeper investigation, but we suspect this tendency is one of the reasons."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities. Since conventional VC directly uses the posterior probabilities of source speech for predicting target speech, it is difficult to convert the speaker individuality included in the posterior probabilities. To address this, we built sequence-to-sequence conversion models that convert the source context posterior probability sequence into a target one. Further, we proposed joint training algorithms for speech recognition, speech synthesis, and posterior probability conversion. Experimental results demonstrated that (1) the proposed algorithms outperformed the conventional VC in speaker similarity, and (2) joint training of recognition and synthesis outperformed the conventional VC in both speaker similarity and speech quality. As future work, we will investigate how to determine the frame length of the converted posterior probability sequence.\nAcknowledgements: Part of this work was supported by ImPACT Program of Council for Science, Technology and Innovation (Cabinet Ofce, Government of Japan), SECOM Science and Technology Foundation, and JSPS KAKENHI Grant Number 16H06681."}, {"heading": "6. References", "text": "[1] A. B. Kain, J.-P. Hosom, X. Niu, J. P. H. van Santen, M. F.Oken, and J. Staehely, \u201cImproving the intelligibility of dysarthric speech,\u201d Speech Communication, vol. 49, no. 9, pp. 743\u2013759, 2007.\n[2] F. Rudzicz, \u201cAcoustic transformations to improve the intelligibility of dysarthric speech,\u201d in Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11\u201321.\n[3] S. Aryal and R. G.-Osuna, \u201cCan voice conversion be used to reduce non-native accents?\u201d in Proc. ICASSP, Florence, Italy, May 2014, pp. 7929\u20137933.\n[4] T. Toda, A. W. Black, and K. Tokuda, \u201cVoice conversion based on maximum likelihood estimation of spectral parameter trajectory,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222\u20132235, 2007.\n[5] Y. Stylianou, O. Cappe\u0301, and E. Moulines, \u201cContinuous probabilistic transform for voice conversion,\u201d IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131\u2013142, 1998.\n[6] S. Desai, E. V. Raghavendra, B. Yegnanarayana, A. W. Black, and K. Prahallad, \u201cVoice conversion using artificial neural networks,\u201d in Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893\u20133896.\n[7] A. Kain and M. W. Macon, \u201cSpectral voice conversion for text-tospeech synthesis,\u201d in Proc. ICASSP, Seattle, U.S.A., May 1998, pp. 285\u2013288.\n[8] D. Sunderman, H. Hoge, A. Bonafonte, H. Ney, A. W. Black, and S. Narayanan, \u201cText-independent voice conversion based on unit selection,\u201d in Proc. ICASSP, Toulouse, France, May 2006.\n[9] L. Sun, K. Li, H. Wang, S. Kang, and H. Meng, \u201cPhonetic posteriorgrams for many-to-one voice conversion without parallel data training,\u201d in Proc. ICME, Seattle, U.S.A., Jul. 2016.\n[10] L. Sun, S. Kang, K. Li, and H. Meng, \u201cPersonalized, cross-lingual TTS using phonetic posteriorgrams,\u201d in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 322\u2013326.\n[11] F.-L. Xie, F. K. Soong, and H. Li, \u201cA KL divergence and DNNbased approach to voice conversion without parallel training sentences,\u201d in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 287\u2013291.\n[12] K. Cho, D. Bahdanau, F. Vougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using RNN encoderdecoder for statistical machine translation,\u201d in Proc. EMNLP, Doha, Qatar, Oct. 2014, pp. 1724\u20131734.\n[13] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning with neural networks,\u201d in Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3104\u20133112.\n[14] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy layer-wise training of deep networks,\u201d in Proc. NIPS, Vancouver, Canada, Dec. 2006, pp. 153\u2013160.\n[15] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma, \u201cDual learning for machine translation,\u201d in Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 820\u2013828.\n[16] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d in Proc. ICLR, Banff, Canada, Apr. 2014.\n[17] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, \u201cVoice conversion from non-parallel corpora using variational auto-encoder,\u201d in Proc. APSIPA ASC, Jeju, Korea, Dec. 2016.\n[18] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling, \u201cSemi-supervised learning with deep generative models,\u201d in Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3581\u20133589.\n[19] Y. Sagisaka, K. Takeda, M. Abe, S. Katagiri, T. Umeda, and H. Kuawhara, \u201cA large-scale Japanese speech database,\u201d in ICSLP90, Kobe, Japan, Nov. 1990, pp. 1089\u20131092.\n[20] H. Kawahara, J. Estill, and O. Fujimura, \u201cAperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT,\u201d in MAVEBA 2001, Firentze, Italy, Sep. 2001, pp. 1\u20136.\n[21] Y. Ohtani, T. Toda, H. Saruwatari, and K. Shikano, \u201cMaximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation,\u201d in Proc. INTERSPEECH, Pittsburgh, U.S.A., Sep. 2006, pp. 2266\u20132269.\n[22] H. Kawahara, I. Masuda-Katsuse, and A. D. Cheveigne, \u201cRestructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds,\u201d Speech Communication, vol. 27, no. 3\u20134, pp. 187\u2013207, 1999.\n[23] S. Takamichi, K. Kobayashi, K. Tanaka, T. Toda, and S. Nakamura, \u201cThe NAIST text-to-speech system for the Blizzard Challenge 2015,\u201d in Proc. Blizzard Challenge workshop, Berlin, Germany, Sep. 2015.\n[24] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient methods for online learning and stochastic optimization,\u201d Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, Jul. 2011.\n[25] B. Huang, D. Ke, H. Zheng, B. Xu, Y. Xu, and K. Su, \u201cMulti-task learning deep neural networks for speech feature denoising,\u201d in Proc. INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 2464\u2013 2468.\n[26] W. Wang, S. Xu, and B. Xu, \u201cFirst step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention,\u201d in Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 2243\u20132247."}], "references": [{"title": "Improving the intelligibility of dysarthric speech", "author": ["A.B. Kain", "J.-P. Hosom", "X. Niu", "J.P.H. van Santen", "M.F.- Oken", "J. Staehely"], "venue": "Speech Communication, vol. 49, no. 9, pp. 743\u2013759, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Acoustic transformations to improve the intelligibility of dysarthric speech", "author": ["F. Rudzicz"], "venue": "Proc. SLPAT, Edinburgh, Scotland, Jul. 2011, pp. 11\u201321.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "G.-Osuna, \u201cCan voice conversion be used to reduce non-native accents?", "author": ["R.S. Aryal"], "venue": "in Proc. ICASSP, Florence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Voice conversion based on maximum likelihood estimation of spectral parameter trajectory", "author": ["T. Toda", "A.W. Black", "K. Tokuda"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2222\u20132235, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuous probabilistic transform for voice conversion", "author": ["Y. Stylianou", "O. Capp\u00e9", "E. Moulines"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 6, no. 2, pp. 131\u2013142, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Voice conversion using artificial neural networks", "author": ["S. Desai", "E.V. Raghavendra", "B. Yegnanarayana", "A.W. Black", "K. Prahallad"], "venue": "Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893\u20133896.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral voice conversion for text-tospeech synthesis", "author": ["A. Kain", "M.W. Macon"], "venue": "Proc. ICASSP, Seattle, U.S.A., May 1998, pp. 285\u2013288.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Text-independent voice conversion based on unit selection", "author": ["D. Sunderman", "H. Hoge", "A. Bonafonte", "H. Ney", "A.W. Black", "S. Narayanan"], "venue": "Proc. ICASSP, Toulouse, France, May 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training", "author": ["L. Sun", "K. Li", "H. Wang", "S. Kang", "H. Meng"], "venue": "Proc. ICME, Seattle, U.S.A., Jul. 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Personalized, cross-lingual TTS using phonetic posteriorgrams", "author": ["L. Sun", "S. Kang", "K. Li", "H. Meng"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 322\u2013326.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A KL divergence and DNNbased approach to voice conversion without parallel training sentences", "author": ["F.-L. Xie", "F.K. Soong", "H. Li"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 287\u2013291.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["K. Cho", "D. Bahdanau", "F. Vougares", "H. Schwenk", "Y. Bengio"], "venue": "Proc. EMNLP, Doha, Qatar, Oct. 2014, pp. 1724\u20131734.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3104\u20133112.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proc. NIPS, Vancouver, Canada, Dec. 2006, pp. 153\u2013160.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual learning for machine translation", "author": ["D. He", "Y. Xia", "T. Qin", "L. Wang", "N. Yu", "T. Liu", "W.-Y. Ma"], "venue": "Proc. NIPS, Barcelona, Spain, Dec. 2016, pp. 820\u2013828.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "Proc. ICLR, Banff, Canada, Apr. 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Voice conversion from non-parallel corpora using variational auto-encoder", "author": ["C.-C. Hsu", "H.-T. Hwang", "Y.-C. Wu", "Y. Tsao", "H.-M. Wang"], "venue": "Proc. APSIPA ASC, Jeju, Korea, Dec. 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "Proc. NIPS, Montreal, Canada, Dec. 2014, pp. 3581\u20133589.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A large-scale Japanese speech database", "author": ["Y. Sagisaka", "K. Takeda", "M. Abe", "S. Katagiri", "T. Umeda", "H. Kuawhara"], "venue": "IC- SLP90, Kobe, Japan, Nov. 1990, pp. 1089\u20131092.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1990}, {"title": "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT", "author": ["H. Kawahara", "J. Estill", "O. Fujimura"], "venue": "MAVEBA 2001, Firentze, Italy, Sep. 2001, pp. 1\u20136.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum likelihood voice conversion based on GMM with STRAIGHT mixed excitation", "author": ["Y. Ohtani", "T. Toda", "H. Saruwatari", "K. Shikano"], "venue": "Proc. INTERSPEECH, Pittsburgh, U.S.A., Sep. 2006, pp. 2266\u20132269.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Restructuring speech representations using a pitch-adaptive timefrequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds", "author": ["H. Kawahara", "I. Masuda-Katsuse", "A.D. Cheveigne"], "venue": "Speech Communication, vol. 27, no. 3\u20134, pp. 187\u2013207, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "The NAIST text-to-speech system for the Blizzard Challenge 2015", "author": ["S. Takamichi", "K. Kobayashi", "K. Tanaka", "T. Toda", "S. Nakamura"], "venue": "Proc. Blizzard Challenge workshop, Berlin, Germany, Sep. 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, Jul. 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-task learning deep neural networks for speech feature denoising", "author": ["B. Huang", "D. Ke", "H. Zheng", "B. Xu", "Y. Xu", "K. Su"], "venue": "Proc. INTERSPEECH, Dresden, Germany, Sep. 2015, pp. 2464\u2013 2468.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "First step towards end-to-end parametric TTS synthesis: Generating spectral parameters with neural attention", "author": ["W. Wang", "S. Xu", "B. Xu"], "venue": "Proc. INTERSPEECH, San Francisco, U.S.A., Sep. 2016, pp. 2243\u20132247.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 67, "endOffset": 73}, {"referenceID": 1, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "VC is used in a variety of applications such as speech enhancement [1, 2] and language education for non-native speakers [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 150, "endOffset": 156}, {"referenceID": 4, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 150, "endOffset": 156}, {"referenceID": 5, "context": "Text-independent VC directly predicts target speech parameters from the source speech parameters, and acoustic models such as Gaussian mixture models [4, 5] or deep neural network [6] are trained using only speech data.", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 7, "context": "Text-dependent VC [7, 8], in contrast, converts speech parameters through textual information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 8, "context": "VC using shared context posterior probabilities [9] is classified in text-dependent VC, but the conversion unit is frame level.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].", "startOffset": 101, "endOffset": 109}, {"referenceID": 10, "context": "This VC is interpreted as soft text-dependent VC and can be extended to cross-lingual text-to-speech [10, 11].", "startOffset": 101, "endOffset": 109}, {"referenceID": 11, "context": "Assuming that the training data partly include parallel speech data (parallel utterances of phrases), we build an encoder-decoder model [12] that converts the posterior probabilities of the source speech parameters into those of the target speech parameters.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "When we do not build the conversion model or do not have parallel data, conventional VC [9] is available.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Whereas the conventional VC [9] separately trains speech recognition and speech synthesis, our approach jointly trains these modules (like auto-encoding) and the proposed conversion module.", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "We found through experiment that the proposed methods outperform the conventional VC [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 8, "context": "Conventional VC using shared context posterior probabilities [9] contains two modules: speech recognition and speech synthesis.", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "Sequence-to-sequence learning using recurent neural networks (RNNs) [13] can be applied to the problem that the source and target sequences have different lengths.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Meanwhile, text-independent VC with dynamic time warping (DTW) [4] aligns speech features in a frame level, but it limits the transformation of (implicitly considered) context sequence, e.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "The conventional method [9, 10] also corresponds to the latter because the source posterior probability is directly used for synthesizing the target speech.", "startOffset": 24, "endOffset": 31}, {"referenceID": 13, "context": "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "2, is similar to auto-encoding processes [14] with the referred class labels and dual learning [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Therefore, we expect that these processes can be extended to the supervised learning of variational auto-encoders [16] that have not only class labels (e.", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": ", context labels) but also the hidden variables [17, 18].", "startOffset": 48, "endOffset": 56}, {"referenceID": 17, "context": ", context labels) but also the hidden variables [17, 18].", "startOffset": 48, "endOffset": 56}, {"referenceID": 8, "context": "Although the conventional VC [9] and proposed VC accept non-parallel speech data and partly included parallel data, we used fully parallel data in this evaluation.", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "We prepared speech data of eight speakers taken from the ATR Japanese speech database [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The 0th\u2013through\u201324th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "The 0th\u2013through\u201324th melcepstral coefcients were used as a spectral parameter and F0 and 5 band-aperiodicity [20, 21] were used as excitation parameters.", "startOffset": 109, "endOffset": 117}, {"referenceID": 21, "context": "We used the STRAIGHT analysis-synthesis system [22] for the parameter extraction and waveform synthesis.", "startOffset": 47, "endOffset": 51}, {"referenceID": 22, "context": "To improve training accuracy, speech parameter trajectory smoothing [23] with a 50 Hz cutoff modulation frequency was applied to the spectral parameters in the training data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "We used AdaGrad [24] as the optimization algorithm, setting the learning rate to 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.", "startOffset": 102, "endOffset": 109}, {"referenceID": 9, "context": "Both in speech recognition and generation stage, we used bi-directional long short-term memory (LSTM) [9, 10] and each hidden layer contained 256 units.", "startOffset": 102, "endOffset": 109}, {"referenceID": 24, "context": "The cross-entropy loss was calculated for each group, and the loss function for training the recognition models was the sum of each loss [25].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "In the proposed method, F0 was linearly transformed [4] first, and we modified its length using DTW between the source context posterior probability sequence and the converted posterior probability sequence.", "startOffset": 52, "endOffset": 55}, {"referenceID": 25, "context": "This evaluation uses reference phoneme duration for converting posterior probabilities in order to address the sequence length determination problem to which sequence-to-sequence learning is prone [26].", "startOffset": 197, "endOffset": 201}, {"referenceID": 8, "context": "We calculated mel-cepstral distortion between the target and converted speech parameters of the conventional VC [9] and proposed VC.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "We evaluated the effectiveness of joint training of recognition and synthesis modules, in comparison to conventional separately trained modules [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "To evaluate of the joint training of recognition, conversion, and synthesis, we calculated the mel-cepstral distortion of three systems: (1) conventional VC [9], (2) separately trained modules (equal to \u201dProposed\u201d in Section 4.", "startOffset": 157, "endOffset": 160}], "year": 2017, "abstractText": "Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC.", "creator": "LaTeX with hyperref package"}}}