{"id": "1704.02813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2017", "title": "Character-Word LSTM Language Models", "abstract": "We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters. The results of this study support this assumption, which has been supported by previous studies, that the high-frequency content (LPM) of the word-level vocabulary is only about 15-30 times as much as the length of the vocabulary that precedes the semantic content of a given word.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 10 Apr 2017 11:42:09 GMT  (39kb,D)", "http://arxiv.org/abs/1704.02813v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lyan verwimp", "joris pelemans", "hugo van hamme", "patrick wambacq"], "accepted": false, "id": "1704.02813"}, "pdf": {"name": "1704.02813.pdf", "metadata": {"source": "CRF", "title": "Character-Word LSTM Language Models", "authors": ["Lyan Verwimp", "Joris Pelemans", "Hugo Van hamme", "Patrick Wambacq"], "emails": ["firstname.lastname@esat.kuleuven.be"], "sections": [{"heading": "1 Introduction", "text": "Language models (LMs) play a crucial role in many speech and language processing tasks, among others speech recognition, machine translation and optical character recognition. The current state of the art are recurrent neural network (RNN) based LMs (Mikolov et al., 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al., 2012) and their variants (e.g. gated recurrent units (GRU) (Cho et al., 2014)). LSTMs and GRUs are usually very similar in performance, with GRU models often even outperforming LSTM models despite the fact that they have less parameters to train. However, Jozefowicz et al. (2015) recently showed that for the task of language modeling LSTMs work better than GRUs, therefore we focus on LSTM-based LMs.\nIn this work, we address some of the drawbacks of NN based LMs (and many other types of LMs).\nA first drawback is the fact that the parameters for infrequent words are typically less accurate because the network requires a lot of training examples to optimize the parameters. The second and most important drawback addressed is the fact that the model does not make use of the internal structure of the words, given that they are encoded as one-hot vectors. For example, \u2018felicity\u2019 (great happiness) is a relatively infrequent word (its frequency is much lower compared to the frequency of \u2018happiness\u2019 according to Google Ngram Viewer (Michel et al., 2011)) and will probably be an out-of-vocabulary (OOV) word in many applications, but since there are many nouns also ending on \u2018ity\u2019 (ability, complexity, creativity . . . ), knowledge of the surface form of the word will help in determining that \u2018felicity\u2019 is a noun. Hence, subword information can play an important role in improving the representations for infrequent words and even OOV words.\nIn our character-word (CW) LSTM LM, we concatenate character and word embeddings and feed the resulting character-word embedding to the LSTM. Hence, we provide the LSTM with information about the structure of the word. By concatenating the embeddings, the individual characters (as opposed to e.g. a bag-of-characters approach) are preserved and the order of the characters is implicitly modeled. Moreover, since we keep the total embedding size constant, the \u2018word\u2019 embedding shrinks in size and is partly replaced by character embeddings (with a much smaller vocabulary and hence a much smaller embedding matrix), which decreases the number of parameters of the model.\nWe investigate the influence of the number of characters added, the size of the character embeddings, weight sharing for the characters and the size of the (hidden layer of the) model. Given that common or similar character sequences do not always occur at the beginning of words (e.g. \u2018overfitting\u2019 \u2013 \u2018underfitting\u2019), we also examine adding the charac-\nar X\niv :1\n70 4.\n02 81\n3v 1\n[ cs\n.C L\n] 1\n0 A\npr 2\n01 7\nters in forward order, backward order or both orders. We test our CW LMs on both English and Dutch. Since Dutch has a richer morphology than English due to among others its productive compounding (see e.g. (Re\u0301veil, 2012)), we expect that it should benefit more from a LM augmented with formal/morphological information.\nThe contributions of this paper are the following:\n1. We present a method to combine word and subword information in an LSTM LM: concatenating word and character embeddings. As far as we know, this method has not been investigated before.\n2. By decreasing the size of the word-level embedding (and hence the huge word embedding matrix), we effectively reduce the number of parameters in the model (see section 3.3).\n3. We find that the CW model both outperforms word-level LMs with the same number of hidden units (and hence a larger number of parameters) and word-level LMs with the same number of parameters. These findings are confirmed for English and Dutch, for a small model size and a large model size. The size of the character embeddings should be proportional to the total size of the embedding (the concatenation of characters should not exceed the size of the word-level embedding), and using characters in the backward order improves the perplexity even more (see sections 3.1, 4.3 and 4.4).\n4. The LM improves the modeling of OOV words by exploiting their surface form (see section 4.7).\nThe remainder of this paper is structured as follows: first, we discuss related work (section 2); then the CW LSTM LM is described (section 3) and tested (section 4). Finally, we give an overview of the results and an outlook to future work (section 5)."}, {"heading": "2 Related work", "text": "Other work that investigates the use of character information in RNN LMs either completely replaces the word-level representation by a character-level one or combines word and character information. Much research has also been done on modeling other types of subword information (e.g. morphemes, syllables), but in this discussion, we limit ourselves to characters as subword information.\nResearch on replacing the word embeddings entirely has been done for neural machine translation (NMT) by Ling et al. (2015) and Costa-jussa\u0300 and Fonollosa (2016), who replace word-level embeddings with character-level embeddings. Chung et al. (2016) use a subword-level encoder and a character-level decoder for NMT. In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level.\nCharacter-level word representations can also be generated with convolutional neural networks (CNNs), as Zhang et al. (2015) and Kim et al. (2016) have proven for text classification and language modeling respectively. Kim et al. (2016) achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway (Srivastava et al., 2015) and LSTM layers. However, the major improvement is achieved by adding the highway layers: for a small model size, the purely character-level model without highway layers does not perform better than the word-level model (perplexity of 100.3 compared to 97.6), even though the character model has two hidden layers of 300 LSTM units each and is compared to a word model of two hidden layers of only 200 units (in order to keep the number of parameters similar). For a model of larger size, the character-level LM improves the word baseline (84.6 compared to 85.4), but the largest improvement is achieved by adding two highway layers (78.9). Finally, Jozefowicz et al. (2016) also describe character embeddings generated by a CNN, but they test on the 1B Word Benchmark, a data set of an entirely different scale than the one we use.\nOther authors combine the word and character information (as we do in this paper) rather than doing away completely with word inputs. Chen et al. (2015) and Kang et al. (2011) work on models combining words and Chinese characters to learn embeddings. Note however that Chinese characters more closely match subwords or words than phonemes. Bojanowski et al. (2015) operate on the character level but use knowledge about the context words in two variants of character-level RNN LMs. Dos Santos and Zadrozny (2014) join word and character representations in a deep neural network for part-of-speech tagging. Finally, Miyamoto and\nCho (2016) describe a LM that is related to our model, although their character-level embedding is generated by a bidirectional LSTM and we do not use a gate to determine how much of the word and how much of the character embedding is used. However, they only compare to a simple baseline model of 2 LSTM layers of each 200 hidden units without dropout, resulting in a higher baseline perplexity (as mentioned in section 4.3, our CW model also achieves larger improvements than reported in this paper with respect to that baseline).\nWe can conclude that in various NLP tasks, characters have recently been introduced in several different manners. However, the models investigated in related work are either not tested on a competitive baseline (Miyamoto and Cho, 2016) or do not perform better than our models (Kim et al., 2016). In this paper, we introduce a new and straightforward manner to incorporate characters in a LM that (as far as we know) has not been investigated before."}, {"heading": "3 Character-Word LSTM LMs", "text": "A word-level LSTM LM works as follows: a word encoded as a one-hot column vector wt (at time step t) is fed to the input layer and multiplied with the embedding matrix Ww, resulting in a word embedding et:\net=Ww\u00d7wt (1)\nThe word embedding of the current word et will be the input for a series of non-linear operations in the LSTM layer (we refer to (Zaremba et al., 2015) for more details about the equations of the LSTM cell). In the output layer, probabilities for the next word are calculated based on a softmax function.\nIn our character-word LSTM LM, the only difference with the baseline LM is the computation of the \u2018word\u2019 embedding, which is now the result of word and character input rather than word input only. We concatenate the word embedding with embeddings of the characters occurring in that word:\ne>t =[(Ww\u00d7wt)>(W1c\u00d7c1t )>\n(W2c\u00d7c2t )> ... (Wnc \u00d7cnt )>] (2)\nwhere c1t is the one-hot encoding of the first character added, W1c its embedding matrix and n the total number of characters added to the model. The word wt and its characters c1t ,c2t ...cnt are each projected onto their own embeddings, and the concatenation\n1\nof the embeddings is the input for the LSTM layer. By concatenating the embeddings, we implicitly preserve the order of the characters: the embedding for e.g. the first character of a word will always correspond to the same portion of the input vector for the LSTM (see figure 1). We also experimented with adding word and character embeddings (a method which does not preserve the order of the characters), but that did not improve the perplexity of the LM.\nThe number of characters added (n) is fixed. If a word is longer than n characters, only the first (or last, depending on the order in which they are added) n characters are added. If the word is shorter than n, it is padded with a special symbol. Because we can still model the surface form of OOV words with the help of their characters, this model reduces the number of errors made immediately after OOV words (see section 4.7)."}, {"heading": "3.1 Order of the characters", "text": "The characters can be added in the order in which they appear in the word (in the experiments this is called \u2018forward order\u2019), in the reversed order (\u2018backward order\u2019) or both (\u2018both orders\u2019). In English and Dutch (and many other languages), suffixes can bear meaningful relations (such as plurality and verb conjugation) and compounds typically have word-final heads. Hence, putting more emphasis on the end of a word might help to better model those properties."}, {"heading": "3.2 Weight sharing", "text": "Note that in equation 2 each position in the word is associated with different weights: the weights for the first character c1t , W1c , are different from the weights for the character in the second position, W2c . Given that the input \u2018vocabulary\u2019 for characters is always the same, one could argue that the same set of weights Wc could be used for all positions in the word:\ne>t =[(Ww\u00d7wt)>(Wc\u00d7c1t )>\n(Wc\u00d7c2t )> ... (Wc\u00d7cnt )>] (3)\nHowever, one could also argue in favor of the opposite case (no shared weights between the characters): for example, an \u2018s\u2019 at the end of a word often has a specific meaning, such as indicating a third person singular verb form of the present tense (in English), which it does not have at other positions in the word. Both models with and without weight sharing are tested (see section 4.6)."}, {"heading": "3.3 Number of parameters", "text": "Given that a portion of the total embedding is used for modeling the characters, the actual \u2018word\u2019 embedding is smaller which reduces the number of parameters significantly. In a normal word-level LSTM LM, the number of parameters in the embedding matrix is\nV \u00d7E (4)\nwith V the vocabulary size and E = Ew the total embedding size/word embedding size. In our CW model however, the number of parameters is\nV \u00d7(E\u2212n\u00d7Ec)+n\u00d7(C\u00d7Ec) (5)\nwith n the number of characters,Ec the size of the character embedding andC the size of the character vocabulary. SinceV is by far the dominant factor, reducing the size of the purely word-level embedding vastly reduces the total number of parameters to train. If we share the character weights, that number becomes even smaller:\nV \u00d7(E\u2212n\u00d7Ec)+C\u00d7Ec (6)"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "All LMs were trained and tested with TensorFlow (Abadi et al., 2015). We test the performance of\nthe CW architectures for a small model and a large model, with hyperparameters based on Zaremba et al. (2015) and Kim et al. (2016)). The small LSTM consists of 2 layers of 200 hidden units and the large LSTM has 2 layers of 650 hidden units. The total size of the embedding layer always equals the size of the hidden layer. During the first 4/6 (small/large model) epochs, the learning rate is 1, after which we apply an exponential decay:\n\u03b7i=\u03b1 \u03b7i\u22121 (7)\nwhere \u03b7i is the learning rate at epoch i and \u03b1 the learning rate decay, which is set to 0.5 for the small LSTM and to 0.8 for the large LSTM. The smaller \u03b1, the faster the learning rate decreases. The total number of epochs is fixed to 13/39 (small/large model). During training, 25% of the neurons are dropped (Srivastava et al., 2014) for the small model and 50% for the large model. The weights are randomly initialized to small values (between -0.1 and 0.1 for the small model and between -0.05 and 0.05 for the large model) based on a uniform distribution. We train on mini-batches of 20 with backpropagation through time, where the network is unrolled for 20 steps for the small LSTM and 35 for the large LSTM. The norm of the gradients is clipped at 5 for both models.\nFor English, we test on the publicly available Penn Treebank (PTB) data set, which contains 900k word tokens for training, 70k word tokens as validation set and 80k words as test set. This data set is small but widely used in related work (among others Zaremba et al. (2015) and Kim et al. (2016)), enabling the comparison between different models. We adopt the same pre-processing as used by previous work (Mikolov et al., 2010) to facilitate comparison, which implies that the dataset contains only lowercase characters (the size of the character vocabulary is 48). Unknown words are mapped to \u3008unk\u3009, but since we do not have the original text, we cannot use the characters of the unknown words for PTB.\nThe Dutch data set consists of components g, h, n and o of the Corpus of Spoken Dutch (CGN) (Oostdijk, 2000), containing recordings of meetings, debates, courses, lectures and read text. Approximately 80% was chosen as training set (1.4M word tokens), 10% as validation set (180k word tokens) and 10% as test set (190k word tokens). The size of the Dutch data set is chosen to be similar to the size of the English data set. We also use the same vocabulary size as used for Penn Treebank (10k), since\nwe want to compare the performance on different languages and exclude any effect of the vocabulary size. However, we do not convert all uppercase characters to lowercase (although the data is normalized such that sentence-initial words with a capital are converted to lowercase if necessary) because the fact that a character is uppercase is meaningful in itself. The character vocabulary size is 88 (Dutch also includes more accented characters due to French loan words, e.g. \u2018cafe\u0301\u2019). Hence, we do not only compare two different languages but also models with only lowercase characters and models with both upper- and lowercase characters. Moreover, since we have the original text at our disposal (as opposed to PTB), we can use the characters of the unknown words and still have a character-level representation."}, {"heading": "4.2 Baseline models", "text": "In our experiments, we compare the CW model with two word-level baselines: one with the same number of hidden units in the LSTM layers (thus containing more parameters) and one with approximately the same number of parameters as the CW model (like Kim et al. (2016) do), because we are interested in both reducing the number of parameters and improving the performance. For the latter baseline, this implies that we change the number of hidden units from 200 to 175 for the small model and from 650 to 475 for the large, keeping the other hyperparameters the same.\nThe number of parameters for those models is larger than for all CW models except when only 1 or 2 characters are added. The size difference between the CW models and the smaller word-level models becomes larger if more characters are added, if the size of the characters embeddings is larger and if the character weights are shared. The size of the embedding matrix for a word-level LSTM of size 475 is 10,000\u00d7 475 = 475,000 (V is 10k in all our experiments), whereas for a CW model with 10 character embeddings of size 25 it is of size 10,000\u00d7 (650 - 10\u00d7 25) + 10\u00d7 (48\u00d7 25) = 412,000 (the size of the character vocabulary for PTB is 48), following equation 5. If the character weights are shared, the size of the embedding matrix is only 401,200 (equation 6).\nThe baseline perplexities for the smaller wordlevel models are shown in table 1. In the remainder of this paper, \u2018wx\u2019 = means word embeddings of size x for a word-level model and \u2018cx\u2019 means character embeddings of size x for CW models."}, {"heading": "4.3 English", "text": "In figure 2, the results for a small model trained on Penn Treebank are shown. Almost all CW models outperform the word-based baseline with the same number of parameters (2 LSTM layers of 175 units). Only the CW models in which the concatenated character embeddings take up the majority of the total embedding (more than 7 characters of embedding size 15) perform worse. With respect to the word-level LM with more parameters, only small improvements are obtained. The smaller the character embeddings, the better the performance of the CW model. For example, for a total embedding size of only 200, adding 8 character embeddings of size 15 results in an embedding consisting of 120 units \u2018character embedding\u2019 and only 80 units \u2018word embedding\u2019, which is not sufficient. The two best performing models add 3 and 7 character embeddings of size 5, giving a perplexity of 100.12 and 100.25 respectively, achieving a relative improvement of 2.44%/2.31% w.r.t. the w175 baseline and 0.58%/0.45% w.r.t. the w200 baseline. For those models, the \u2018word embedding\u2019 consists of 185 and 165 units respectively.\nWe test the performance of the CW architecture on a large model too. In figure 3, the results for different embedding sizes are shown. Just like we saw for the small model, the size of the character embeddings should not be too large: for embeddings of size 50 (\u2018c50\u2019), the performance drops when a larger number of characters is added. The best result is obtained by adding 8 characters with embeddings of size 25 (\u2018c25\u2019): a perplexity of 85.97 (2.74%/1.61% relative improvement with respect to\nthe w475/w650 baseline). For embeddings of size 10, adding more than 10 characters gives additional improvements (see figure 4).\nWe also verify whether the order in which the characters are added is important (figure 4). The best result is achieved by adding the first 3 and the last 3 characters to the model (\u2018both orders\u2019), giving a perplexity of 85.69, 3.05%/1.87% relative improvement with respect to the w475/w650 baseline. However, adding more characters in both orders causes a decrease in performance. When only adding the characters in the forward order or the backward order, adding the characters in backward order seems to perform slightly better overall (best result: adding 9 characters in the backward order gives a perplexity of 85.7 or 3.04%/1.92% improvement with respect to the w475/w650 baseline).\nWe can conclude that the size of the character embeddings should be proportional to the total embedding size: the word-level embedding should be larger than the concatenation of the character-level embeddings. Adding characters in the backward order is slightly better than adding them in the forward order, and the largest improvement is made for the large LSTM LM. The test perplexities for some of the best performing models (table 2) confirm these findings.\nIf we compare the test perplexities with two related models that incorporate characters, we see that our models perform better. Kim et al. (2016) generate character-level embeddings with a convolutional neural network and also report results for both a small and a large model. Their small character-level model has more hidden units than ours (300 compared to 200), but it does not improve with respect to the word-level baseline (since we do not use highway layers, we only compare with the results for models without highway layers). Their large model slightly improves their own baseline perplexity (85.4) by 0.94%. Compare with our results: 2.64% perplexity reduction for the best small LSTM (c5 with n=3) and 2.77% for the best large LSTM (c10 with n = 3+3(b)). Miyamoto and Cho (2016) only report results for a small model that is trained without dropout, resulting in a baseline perplexity of 115.65. If we train our small model without dropout we get a comparable baseline perplexity (116.33) and a character-word perplexity of 110.54 (compare to 109.05 reported by Miyamoto and Cho (2016)). It remains to be seen whether their model performs equally well compared to better baselines. Moreover, their hybrid character-word model is more complex than ours because it uses a bidirectional LSTM to generate\nthe character-level embedding (instead of a lookup table) and a gate to determine the mixing weights between the character- and word-level embeddings."}, {"heading": "4.4 Dutch", "text": "As we explained in the introduction, we expect that using information about the internal structure of the word will help more for languages with a richer morphology. Although Dutch is still an analytic language (most grammatical relations are marked with separate words or word order rather than morphemes), it has a richer morphology than English because compounding is a productive and widely used process and because it has more lexical variation due to inflection (e.g. verb conjugation, adjective inflection). The results for the LSTM LMs trained on Dutch seem to confirm this hypothesis (see figure 5).\nThe CW model outperforms the baseline word-level LM both for the small model and the large model. The best result for the small model is obtained by adding 2 or 3 characters, giving a perplexity of 67.59 or 67.65 which equals a relative improvement of 2.89%/2.23% (w175/w200) and 2.80%/2.14% (w175/w200) respectively.\nFor the large model, we test several embedding sizes and orders for the characters. The best model is the one to which 6 characters in backward order are added, with a perplexity of 60.88 or\n4.70%/3.91% (w475/w650) relative improvement. Just like for PTB, an embedding size of 25 proves to be the best compromise: if the characters are added in the normal order, 4 characters with embeddings of size 25 is the best model (perplexity 61.47 or 3.77%/2.98% (w475/w650) relative improvement).\nThese results are confirmed for the test set (table 3). The best small model has a perplexity of 75.04 which is 2.27% compared to the baseline and the best large model has a perplexity of 67.64, a relative improvement of 4.57%. The larger improvement for Dutch might be due to the fact that it has a richer morphology and/or the fact that we can use the surface form of the OOV words for the Dutch data set (see sections 4.1 and 4.7)."}, {"heading": "4.5 Random CW models", "text": "In order to investigate whether the improvements of the CW models are not caused by the fact that the characters add some sort of noise to the input, we experiment with adding real noise \u2013 random \u2018character\u2019 information \u2013 rather than the real characters. Both the number of characters (the length of the random \u2018word\u2019) and the \u2018characters\u2019 themselves are generated based on a uniform distribution. In table 4, the relative change in perplexity, averaged over models to which 1 to 10 characters are added,\nwith respect to the baseline word-level LM and the CW model with real characters is shown.\nFor English, adding random information had a negative impact on the performance with respect to both the baseline and the CW model. For Dutch on the other hand, adding some random noise to the word-level model gave small improvements. However, the random models perform much worse than the CW models. We can conclude that the characters provide meaningful information to the LM."}, {"heading": "4.6 Sharing weights", "text": "We repeat certain experiments with the CW models, but with embedding matrices that are shared across all character positions (see section 3.2). Note that sharing the weights does not imply that the position information is lost, because for example the first portion of the character-level embedding always corresponds to the character on the first position. Sharing the weights ensures that a character is always mapped onto the same embedding, regardless of the position of that character in the word, e.g. both occurrences of \u2018i\u2019 in \u2018felicity\u2019 are represented by the same embedding. This effectively reduces the number of parameters.\nWe compare the performance of the CW models with weight sharing with the baseline word-level LM and the CW model without weight sharing. In table 5, the relative change with respect to those LMs is listed.\nCW models with weight sharing generally improve with respect to a word-level baseline, except for the small English LM. For Dutch, the improvements are more pronounced. The difference with the CW model without weight sharing is small (right column), although not sharing the weights works slightly better, which suggests that characters can convey different meanings depending on the position in which they occur. Again, the results are more clear-cut for Dutch than for English."}, {"heading": "4.7 Dealing with out-of-vocabulary words", "text": "As we mentioned in the introduction, we expect that by providing information about the surface form of OOV words (namely, their characters), the number of errors induced by those words should decrease.\nWe conduct the following experiment to check whether this is indeed the case: for the CGN test set, we keep track of the probabilities of each word during testing. If an OOV word is encountered, we check the probability of the target word given by a word-level LM and a CW LM. The word-level model is a large model of size 475 and the CW model is a large model in which 6 characters embeddings of size 25 in the backward order are used (the best performing CW model in our experiments).\nWe observe that in 17,483 of the cases, the CW model assigns a higher probability to the target word following an OOV word, whereas the opposite happens only in 10,724 cases. This is an indication that using the character information indeed helps in better modeling the OOV words."}, {"heading": "5 Conclusion and future work", "text": "We investigated a character-word LSTM language model, which combines character and word information by concatenating the respective embeddings. This both reduces the size of the LSTM and improves the perplexity with respect to a baseline word-level LM. The model was tested on English and Dutch, for different model sizes, several embedding sizes for the characters, different orders in which the characters are added and for weight sharing of the characters. We can conclude that for almost all setups, the CW LM outperforms the word-level model, whereas it has fewer parameters than the word-level model with the same number of LSTM units. If we compare with a word-level LM with approximately the same number of parameters, the improvement is larger.\nOne might argue that using a CNN or an RNN\nto generate character-level embeddings is a more general approach to incorporate characters in a LM, but this model is simple, easier to train and smaller. Moreover, related models using a CNN-based character embedding (Kim et al., 2016) do not perform better.\nFor both English and Dutch, we see that the size of the character embedding is important and should be proportional to the total embedding size: the total size of the concatenated character embeddings should not be larger than the word-level embedding. Not using the characters in the order in which they appear in the word, but in the reversed order (and hence putting more emphasis on the end of the word), performs slightly better, although adding only a few characters both from the beginning and the end of the word achieves good performance too.\nUsing random inputs instead of the characters performed worse than using the characters themselves, thus refuting the hypothesis that the characters simply introduce noise. Sharing the weights/embedding matrices for the characters reduces the size of the model even more, but causes a small increase in perplexity with respect to a model without weight sharing. Finally, we observe that the CW models are better able to deal with OOV words than word-level LMs.\nIn future work, we will test other architectures to incorporate character information in a word-level LSTM LM, such as combining a character-level LSTM with a word-level LSTM. Another representation that might be useful uses character co-occurrence vectors (by analogy with the acoustic co-occurrences used by Van hamme (2008; 2012)) rather than one-hot character vectors, because co-occurrences intrinsically give information about the order of the characters. Other models could be more inspired by human language processing: according to the theory of blocking, we humans have both a mental lexicon of frequent words and a morphological module that is used to process infrequent/ unknown words or to create new words (see e.g. (Aronoff and Anshen, 2001)). This could correspond to a word-level LM for frequent words and a subword-level LM for infrequent words."}, {"heading": "Acknowledgments", "text": "This research is funded by the Flemish government agency IWT (project 130041, SCATE)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Vijay Vasudevan", "Fernanda Vigas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng."], "venue": "Software available from tensorflow.org.", "citeRegEx": "Vasudevan et al\\.,? 2015", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2015}, {"title": "Morphology and the lexicon: Lexicalization and productivity", "author": ["Marc Aronoff", "Frank Anshen."], "venue": "Andrew Spencer and Arnold M. Zwicky, editors, The Handbook of Morphology, pages 237\u2013247. Blackwell Publishing.", "citeRegEx": "Aronoff and Anshen.,? 2001", "shortCiteRegEx": "Aronoff and Anshen.", "year": 2001}, {"title": "Improved Transition-Based Parsing by Modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Alternative structures for character-level RNNs", "author": ["Piotr Bojanowski", "Armand Joulin", "Tom\u00e1s\u0306 Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2015}, {"title": "Joint Learning of Character and Word Embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan."], "venue": "Conference on Artificial Intelligence (AAAI), pages 1236\u20131242.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ieee:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based Neural Machine Translation", "author": ["Marta R. Costa-juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "Proceedings of the Association for Computational Linguistics (ACL), 2:357\u2013361.", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Learning Character-level Representations for Part-of-Speech Tagging", "author": ["C\u0131\u0301cero N. dos Santos", "Bianca Zadrozny"], "venue": "Proceedings of The 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "HAC-models: a novel approach to continuous speech recognition", "author": ["Hugo Van hamme."], "venue": "Proceedings Interspeech, pages 2554\u20132557.", "citeRegEx": "hamme.,? 2008", "shortCiteRegEx": "hamme.", "year": 2008}, {"title": "An on-line NMF model for temporal pattern learning: Theory with application to automatic speech recognition", "author": ["Hugo Van hamme."], "venue": "International Conference on Latent Variable Analysis and Signal Separation (LVA/ICA), pages 306\u2013313.", "citeRegEx": "hamme.,? 2012", "shortCiteRegEx": "hamme.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "arXiv:1602.02410.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Mandarin word-character hybrid-input Neural Network Language Model", "author": ["Moonyoung Kang", "Tim Ng", "Long Nguyen."], "venue": "Proceedings Interspeech, pages 1261\u20131264.", "citeRegEx": "Kang et al\\.,? 2011", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proc. Conference on Artificial Intelligence (AAAI), pages 2741\u20132749.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Character-based Neural Machine Translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan Black."], "venue": "arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Quantitative Analysis of Culture Using Millions of Digitized Books", "author": ["Erez Lieberman Aiden."], "venue": "Science, 331(6014):176\u2013182.", "citeRegEx": "Aiden.,? 2011", "shortCiteRegEx": "Aiden.", "year": 2011}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1s\u0306 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s\u0306 Burget", "Jan C\u0306ernock\u00fd", "Sanjeev Khudanpur"], "venue": "Proceedings Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Gated Word-Character Recurrent Language Model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "arXiv:1606.01700.v1.", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "The Spoken Dutch Corpus", "author": ["Nelleke Oostdijk."], "venue": "Overview and first Evaluation. Proceedings Language Resources and Evaluation Conference (LREC), pages 887\u2013894.", "citeRegEx": "Oostdijk.,? 2000", "shortCiteRegEx": "Oostdijk.", "year": 2000}, {"title": "Optimizing the Recognition Lexicon for Automatic Speech Recognition", "author": ["Bert R\u00e9veil."], "venue": "PhD thesis, University of Ghent, Belgium.", "citeRegEx": "R\u00e9veil.,? 2012", "shortCiteRegEx": "R\u00e9veil.", "year": 2012}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training Very Deep Networks", "author": ["Rupesh K. Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Neural Information Processing Systems Conference (NIPS), pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Proceedings Interspeech, pages 1724\u20131734.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Neural Language Correction with Character-Based Attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivzhagan", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "arXiv:1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Recurrent Neural Network Regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2015", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Character-level Convolutional Networks for Text Classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCunn."], "venue": "Neural Information Processing Systems Conference (NIPS), pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "The current state of the art are recurrent neural network (RNN) based LMs (Mikolov et al., 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 11, "context": ", 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al.", "startOffset": 68, "endOffset": 102}, {"referenceID": 24, "context": ", 2010), and more specifically long short-term memory models (LSTM) (Hochreiter and Schmidhuber, 1997) LMs (Sundermeyer et al., 2012) and their variants (e.", "startOffset": 107, "endOffset": 133}, {"referenceID": 5, "context": "gated recurrent units (GRU) (Cho et al., 2014)).", "startOffset": 28, "endOffset": 46}, {"referenceID": 5, "context": "gated recurrent units (GRU) (Cho et al., 2014)). LSTMs and GRUs are usually very similar in performance, with GRU models often even outperforming LSTM models despite the fact that they have less parameters to train. However, Jozefowicz et al. (2015) recently showed that for the task of language modeling LSTMs work better than GRUs, therefore we focus on LSTM-based LMs.", "startOffset": 29, "endOffset": 250}, {"referenceID": 21, "context": "(R\u00e9veil, 2012)), we expect that it should benefit more from a LM augmented with formal/morphological information.", "startOffset": 0, "endOffset": 14}, {"referenceID": 12, "context": "entirely has been done for neural machine translation (NMT) by Ling et al. (2015) and Costa-juss\u00e0 and Fonollosa (2016), who replace word-level embeddings with character-level embeddings.", "startOffset": 63, "endOffset": 82}, {"referenceID": 5, "context": "(2015) and Costa-juss\u00e0 and Fonollosa (2016), who replace word-level embeddings with character-level embeddings.", "startOffset": 11, "endOffset": 44}, {"referenceID": 5, "context": "Chung et al. (2016) use a subword-level encoder and a character-level decoder for NMT.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM.", "startOffset": 23, "endOffset": 49}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level.", "startOffset": 23, "endOffset": 156}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level. Character-level word representations can also be generated with convolutional neural networks (CNNs), as Zhang et al. (2015) and Kim et al.", "startOffset": 23, "endOffset": 423}, {"referenceID": 2, "context": "In dependency parsing, Ballesteros et al. (2015) achieve improvements by generating character-level embeddings with a bidirectional LSTM. Xie et al. (2016) work on natural language correction and also use an encoder-decoder, but operate for both the encoder and the decoder on the character level. Character-level word representations can also be generated with convolutional neural networks (CNNs), as Zhang et al. (2015) and Kim et al. (2016) have proven for text classification and language", "startOffset": 23, "endOffset": 445}, {"referenceID": 23, "context": "(2016) achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway (Srivastava et al., 2015) and LSTM layers.", "startOffset": 131, "endOffset": 156}, {"referenceID": 15, "context": "Kim et al. (2016) achieve state-of-the-art results in language modeling for several languages by combining a character-level CNN with highway (Srivastava et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Finally, Jozefowicz et al. (2016) also describe character embeddings generated by a CNN, but they test on the 1B Word Benchmark, a data set of an entirely different scale than the one we use.", "startOffset": 9, "endOffset": 34}, {"referenceID": 3, "context": "Chen et al. (2015) and Kang et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Chen et al. (2015) and Kang et al. (2011) work on models combining words and Chinese characters to learn embeddings.", "startOffset": 0, "endOffset": 42}, {"referenceID": 3, "context": "Bojanowski et al. (2015) operate on the character level but use knowledge about the context words in two variants of character-level RNN LMs.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Bojanowski et al. (2015) operate on the character level but use knowledge about the context words in two variants of character-level RNN LMs. Dos Santos and Zadrozny (2014) join word and character representations in a deep neural network for part-of-speech tagging.", "startOffset": 0, "endOffset": 173}, {"referenceID": 19, "context": "However, the models investigated in related work are either not tested on a competitive baseline (Miyamoto and Cho, 2016) or do not perform better than our models (Kim et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 15, "context": "However, the models investigated in related work are either not tested on a competitive baseline (Miyamoto and Cho, 2016) or do not perform better than our models (Kim et al., 2016).", "startOffset": 163, "endOffset": 181}, {"referenceID": 26, "context": "The word embedding of the current word et will be the input for a series of non-linear operations in the LSTM layer (we refer to (Zaremba et al., 2015) for more details about the equations of the LSTM", "startOffset": 129, "endOffset": 151}, {"referenceID": 25, "context": "model, with hyperparameters based on Zaremba et al. (2015) and Kim et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 15, "context": "(2015) and Kim et al. (2016)).", "startOffset": 11, "endOffset": 29}, {"referenceID": 22, "context": "During training, 25% of the neurons are dropped (Srivastava et al., 2014) for the small model", "startOffset": 48, "endOffset": 73}, {"referenceID": 25, "context": "This data set is small but widely used in related work (among others Zaremba et al. (2015) and Kim et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "(2015) and Kim et al. (2016)),", "startOffset": 11, "endOffset": 29}, {"referenceID": 18, "context": "We adopt the same pre-processing as used by previous work (Mikolov et al., 2010) to facilitate comparison, which implies that the dataset contains only lowercase characters (the size of the character vocabulary is 48).", "startOffset": 58, "endOffset": 80}, {"referenceID": 20, "context": "The Dutch data set consists of components g, h, n and o of the Corpus of Spoken Dutch (CGN) (Oostdijk, 2000), containing recordings of meetings, debates, courses, lectures and read text.", "startOffset": 92, "endOffset": 108}, {"referenceID": 15, "context": "with two word-level baselines: one with the same number of hidden units in the LSTM layers (thus containing more parameters) and one with approximately the same number of parameters as the CW model (like Kim et al. (2016) do), because we are interested in both reducing the number of", "startOffset": 204, "endOffset": 222}, {"referenceID": 15, "context": "Kim et al. (2016) generate character-level embeddings with a convolutional neural network and also report results", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "Miyamoto and Cho (2016) only report results for a small model that is trained without dropout, resulting in a baseline perplexity of 115.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "Miyamoto and Cho (2016) only report results for a small model that is trained without dropout, resulting in a baseline perplexity of 115.65. If we train our small model without dropout we get a comparable baseline perplexity (116.33) and a character-word perplexity of 110.54 (compare to 109.05 reported by Miyamoto and Cho (2016)).", "startOffset": 0, "endOffset": 331}, {"referenceID": 15, "context": "(Kim et al., 2016) 100.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "3 (Miyamoto and Cho, 2016) 109.", "startOffset": 2, "endOffset": 26}, {"referenceID": 15, "context": "(Kim et al., 2016) 84.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "Comparison with other character-level LMs (Kim et al., 2016) (we only compare to models without highway layers) and", "startOffset": 42, "endOffset": 60}, {"referenceID": 19, "context": "character-word models (Miyamoto and Cho, 2016) (they do not use dropout and only report results for a small model).", "startOffset": 22, "endOffset": 46}, {"referenceID": 15, "context": "Moreover, related models using a CNN-based character embedding (Kim et al., 2016) do not perform better.", "startOffset": 63, "endOffset": 81}, {"referenceID": 1, "context": "(Aronoff and Anshen, 2001)).", "startOffset": 0, "endOffset": 26}], "year": 2017, "abstractText": "We present a Character-Word Long ShortTerm Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.", "creator": "TeX"}}}