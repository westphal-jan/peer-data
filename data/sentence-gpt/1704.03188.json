{"id": "1704.03188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2017", "title": "Simplified Stochastic Feedforward Neural Networks", "abstract": "It has been believed that stochastic feedforward neural networks (SFNNs) have several advantages beyond deterministic deep neural networks (DNNs): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training large-scale SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for SFNN, in particular using known architectures and pre-trained parameters of DNNs. First, we plan to work with different SFNN implementations for the entire set of datasets and compare them across the datasets. Second, we plan to test new approach to training data with the DNN system. Third, we are looking for ways to improve training parameters and the performance of a single machine learning algorithm. We are looking at various approaches to using DNNs with a very high degree of accuracy for our model.\n\n\n\n\nWe hope to show that the training of DNNs in this paper is the first step toward efficient training of large-scale SFNN.\nThis paper has been supported by the German Institute for Neural Processing (Germany).", "histories": [["v1", "Tue, 11 Apr 2017 08:19:00 GMT  (1370kb,D)", "http://arxiv.org/abs/1704.03188v1", "22 pages, 6 figures"]], "COMMENTS": "22 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kimin lee", "jaehyung kim", "song chong", "jinwoo shin"], "accepted": false, "id": "1704.03188"}, "pdf": {"name": "1704.03188.pdf", "metadata": {"source": "CRF", "title": "Simplified Stochastic Feedforward Neural Networks", "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "emails": ["kiminlee@kaist.ac.kr,", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu,", "jinwoos@kaist.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "Recently, deterministic deep neural networks (DNNs) have demonstrated state-of-the-art performance on many supervised tasks, e.g., speech recognition [24] and object recognition [11]. One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28]. On the other hand, stochastic feedforward neural networks (SFNNs) [5] having random latent units are often necessary in to model the complex stochastic natures of many real-world tasks, e.g., structured prediction [8] and image generation [41]. Furthermore, it is believed that SFNN has several advantages beyond DNN [9]: it has more expressive power for multi-modal learning and regularizes better for large-scale networks.\nTraining large-scale SFNN is notoriously hard since backpropagation is not directly applicable. Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation with variational techniques and reparameterization tricks [32, 1]. On the other hand, training SFNN having discrete, i.e., binary or multi-modal, random units is more difficult since intractable probabilistic inference is involved requiring too \u2217K. Lee, J. Kim, S. Chong and J. Shin are with School of Electrical Engineering at Korea Advanced Institute of Science Technology, Republic of Korea. Authors\u2019 e-mails: kiminlee@kaist.ac.kr, jaehyungkim@kaist.ac.kr songchong@kaist.edu, jinwoos@kaist.ac.kr\nar X\niv :1\n70 4.\n03 18\n8v 1\n[ cs\n.L G\n] 1\n1 A\npr 2\n01 7\nmany random samples. There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.1 for more details). However, training a SFNN is still significantly slower than training a DNN of the same architecture, consequently most prior works have considered a small number (at most 5 or so) of layers in SFNN. We aim for the same goal, but our direction is complementary to them.\nInstead of training a SFNN directly, we study whether pre-trained parameters from a DNN (or easier models) can be transferred to it, possibly with further low-cost fine-tuning. This approach can be attractive since one can utilize recent advances in DNN design and training. For example, one can design the network structure of SFNN following known specialized ones of DNN and use their pre-trained parameters. To this end, we first try transferring pre-trained parameters of DNN using sigmoid activation functions to those of the corresponding SFNN directly. In our experiments, the heuristic reasonably works well. For multi-modal learning, SFNN under such a simple transformation outperforms DNN. Even for the MNIST classification, the former performs similarly as the latter (see Section 2 for more details). However, it is questionable whether a similar strategy works in general, particularly for other unbounded activation functions like ReLU [33] since SFNN has binary, i.e., bounded, random latent units. Moreover, it loses the regularization benefit of SFNN: it is believed that transferring parameters of stochastic models to DNN helps its regularization, but the opposite is unlikely.\nContribution. To address these issues, we propose a special form of stochastic neural networks, named Simplified-SFNN, which is intermediate between SFNN and DNN, having the following properties. First, Simplified-SFNN can be built upon any baseline DNN, possibly having unbounded activation functions. The most significant part of our approach lies in providing rigorous network knowledge transferring [22] between Simplified-SFNN and DNN. In particular, we prove that parameters of DNN can be transformed to those of the corresponding Simplified-SFNN while preserving the performance, i.e., both represent the same mapping. Second, Simplified-SFNN approximates certain SFNN, better than DNN, by simplifying its upper latent units above stochastic ones using two different non-linear activation functions. Simplified-SFNN is much easier to train than SFNN while still maintaining its stochastic regularization effect. We also remark that SFNN is a Bayesian network, while Simplified-SFNN is not.\nThe above connection DNN\u2192 Simplified-SFNN\u2192 SFNN naturally suggests the following training procedure for both SFNN and Simplified-SFNN: train a baseline DNN first and then fine-tune its corresponding Simplified-SFNN initialized by the transformed DNN parameters. The pre-training stage accelerates the training task since DNN is faster to train than Simplified-SFNN. In addition, one can also utilize known DNN training techniques such as dropout and batch normalization for fine-tuning Simplified-SFNN. In our experiments, we train SFNN and Simplified-SFNN under the proposed strategy. They consistently outperform the corresponding DNN for both multi-modal and classification tasks, where the former and the latter are for measuring the model expressive power and the regularization effect, respectively. To the best of our knowledge, we are the first to confirm that SFNN indeed regularizes better than DNN. We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39]. In particular, WRN (wide residual network) of 28 layers and 36 million parameters has shown the stateof-art performances on CIFAR-10 and CIFAR-100 classification datasets, and our stochastic models built upon WRN outperform the deterministic WRN on the datasets."}, {"heading": "2 Simple Transformation from DNN to SFNN", "text": ""}, {"heading": "2.1 Preliminaries for SFNN", "text": "Stochastic feedforward neural network (SFNN) is a hybrid model, which has both stochastic binary and deterministic hidden units. We first introduce SFNN with one stochastic hidden layer (and without deterministic hidden layers) for simplicity. Throughout this paper, we commonly denote the bias for unit i and the weight matrix of the `-th hidden layer by b`i and W`, respectively. Then, the stochastic hidden layer in SFNN is defined as a binary random vector with N1 units, i.e., h1 \u2208 {0, 1}N1 , drawn under the following distribution:\nP ( h1 | x ) = N1\u220f i=1 P ( h1i | x ) ,where P ( h1i = 1 | x ) = \u03c3 ( W1i x+ b 1 i ) . (1)\nIn the above, x is the input vector and \u03c3 (x) = 1/ (1 + e\u2212x) is the sigmoid function. Our conditional distribution of the output y is defined as follows:\nP (y | x) = EP (h1|x) [ P ( y | h1 )] = EP (h1|x) [ N ( y |W2h1 + b2, \u03c32y )] ,\nwhere N (\u00b7) denotes the normal distribution with mean W2h1 + b2 and (fixed) variance \u03c32y . Therefore, P (y | x) can express a very complex, multi-modal distribution since it is a mixture of exponentially many normal distributions. The multi-layer extension is straightforward via a combination of stochastic and deterministic hidden layers (see [8, 9]). Furthermore, one can use any other output distributions as like DNN, e.g., softmax for classification tasks.\nThere are two computational issues for training SFNN: computing expectations with respect to stochastic units in the forward pass and computing gradients in the backward pass. One can notice that both are computationally intractable since they require summations over exponentially many configurations of all stochastic units. First, in order to handle the issue in the forward pass, one can use the following Monte Carlo approximation for estimating the ex-\npectation: P (y | x) w 1M M\u2211 m=1 P (y | h(m)), where h(m) \u223c P ( h1 | x ) and M is the number of samples. This random estimator is unbiased and has relatively low variance [8] since one can draw samples from the exact distribution. Next, in order to handle the issue in backward pass, [5] proposed Gibbs sampling, but it is known that it often mixes poorly. [38] proposed a variational learning based on the mean-field approximation, but it has additional parameters making the variational lower bound looser. More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34]."}, {"heading": "2.2 Simple transformation from sigmoid-DNN and ReLU-DNN to SFNN", "text": "Despite the recent advances, training SFNN is still very slow compared to DNN due to the sampling procedures: in particular, it is notoriously hard to train SFNN when the network structure is deeper and wider. In order to handle these issues, we consider the following approximation:\nP (y | x) = EP (h1|x) [ N ( y |W2h1 + b2, \u03c32y )] w N ( y | EP (h1|x) [ W2h1 ] + b2, \u03c32y\n) = N ( y |W2\u03c3 ( W1x+ b1 ) + b2, \u03c32y ) . (2)\nNote that the above approximation corresponds to replacing stochastic units by deterministic ones such that their hidden activation values are same as marginal distributions of stochastic units, i.e., SFNN can be approximated by DNN using sigmoid activation functions, say sigmoid-DNN. When there exist more latent layers above the stochastic one, one has to apply similar approximations to all of them, i.e., exchanging the orders of expectations and nonlinear functions, for making the DNN and SFNN are equivalent. Therefore, instead of training SFNN directly, one can try transferring pre-trained parameters of sigmoid-DNN to those of the corresponding SFNN directly: train sigmoid-DNN instead of SFNN, and replace deterministic units by stochastic ones for the inference purpose. Although such a strategy looks somewhat \u2018rude\u2019, it was often observed in the literature that it reasonably works well for SFNN [9] and we also evaluate it as reported in Table 1. We also note that similar approximations appear in the context of dropout: it trains a stochastic model averaging exponentially many DNNs sharing parameters, but also approximates a single DNN well.\nNow we investigate a similar transformation in the case when the DNN uses the unbounded ReLU activation function, say ReLU-DNN. Many recent deep networks are of the ReLU-DNN type because they mitigate the gradient vanishing problem, and their pre-trained parameters are often available. Although it is straightforward to build SFNN from sigmoid-DNN, it is less clear in this case since ReLU is unbounded. To handle this issue, we redefine the stochastic latent units of SFNN:\nP ( h1 | x ) = N1\u220f i=1 P ( h1i | x ) ,where P ( h1i = 1 | x ) = min { \u03b1f ( W1i x+ b 1 i ) , 1 } . (3)\nIn the above, f(x) = max{x, 0} is the ReLU activation function and \u03b1 is some hyperparameter. A simple transformation can be defined similarly as the case of sigmoid-DNN via replacing deterministic units by stochastic ones. However, to preserve the parameter information of ReLU-DNN, one has to choose \u03b1 such that \u03b1f ( W1i x+ b 1 i ) \u2264 1 and rescale upper parameters W2 as follows:\n\u03b1\u22121 \u2190 max i,x \u2223\u2223\u2223f (W\u03021i x+ b\u03021i)\u2223\u2223\u2223 , (W1, b1)\u2190 (W\u03021, b\u03021) , (W2, b2)\u2190 ( W\u03022/\u03b1, b\u03022) . (4)\nThen, applying similar approximations as in (2), i.e., exchanging the orders of expectations and non-linear functions, one can observe that ReLU-DNN and SFNN are equivalent.\nWe evaluate the performance of the simple transformations from DNN to SFNN on the MNIST dataset [6] and the synthetic dataset [36], where the former and the latter are popular datasets used for a classification task and a multi-modal (i.e., one-to-many mappings) prediction learning, respectively. In all experiments reported in this paper, we commonly use the softmax and Gaussian with standard deviation of \u03c3y = 0.05 are used for the output probability on classification and regression tasks, respectively. The only first hidden layer of DNN is replaced by a stochastic layer, and we use 500 samples for estimating the expectations in the SFNN inference. As reported in Table 1, we observe that the simple transformation often works well for both tasks: the SFNN and sigmoid-DNN inferences (using same parameters trained by sigmoid-DNN) perform similarly for the classification task and the former significantly outperforms the latter for the multi-modal task (also see Figure 1). It might suggest some possibilities that the expensive SFNN training might not be not necessary, depending on the targeted learning quality. However, in case of ReLU, SFNN performs much worse than ReLU-DNN for the MNIST classification task under the parameter transformation."}, {"heading": "3 Transformation from DNN to SFNN via Simplified-SFNN", "text": "In this section, we propose an advanced method to utilize the pre-trained parameters of DNN for training SFNN. As shown in the previous section, simple parameter transformations from DNN to SFNN do not clearly work in general, in particular for activation functions other than sigmoid. Moreover, training DNN does not utilize the stochastic regularizing effect, which is an important benefit of SFNN. To address the issues, we design an intermediate model, called Simplified-SFNN. The proposed model is a special form of stochastic neural network, which approximates certain SFNN by simplifying its upper latent units above stochastic ones. Then, we establish more rigorous connections between three models: DNN\u2192 Simplified-SFNN\u2192 SFNN, which leads to an efficient training procedure of the stochastic models utilizing pretrained parameters of DNN. In our experiments, we evaluate the strategy for various tasks and popular DNN architectures.\n\u2223\u2223\u2223h`i (x)\u2212 h\u0302`i (x)\u2223\u2223\u2223 is de-\ncreasing."}, {"heading": "3.1 Simplified-SFNN of two hidden layers and non-negative activation functions", "text": "For clarity of presentation, we first introduce Simplified-SFNN with two hidden layers and non-negative activation functions, where its extensions to multiple layers and general activation functions are presented in Section 4. We also remark that we primarily describe fullyconnected Simplified-SFNNs, but their convolutional versions can also be naturally defined. In Simplified-SFNN of two hidden layers, we assume that the first and second hidden layers consist of stochastic binary hidden units and deterministic ones, respectively. As in (3), the first layer is defined as a binary random vector withN1 units, i.e., h1 \u2208 {0, 1}N1 , drawn under the following distribution:\nP ( h1 | x ) = N1\u220f i=1 P ( h1i | x ) ,where P ( h1i = 1 | x ) = min { \u03b11f ( W1i x+ b 1 i ) , 1 } . (5)\nwhere x is the input vector, \u03b11 > 0 is a hyper-parameter for the first layer, and f : R \u2192 R+ is some non-negative non-linear activation function with |f \u2032(x)| \u2264 1 for all x \u2208 R, e.g., ReLU and sigmoid activation functions. Now the second layer is defined as the following deterministic vector with N2 units, i.e., h2(x) \u2208 RN2 :\nh2 (x) = [ f ( \u03b12 ( EP (h1|x) [ s ( W2jh 1 + b2j )] \u2212 s (0) )) : \u2200j ] , (6)\nwhere \u03b12 > 0 is a hyper-parameter for the second layer and s : R \u2192 R is a differentiable function with |s\u2032\u2032(x)| \u2264 1 for all x \u2208 R, e.g., sigmoid and tanh functions. In our experiments, we use the sigmoid function for s(x). Here, one can note that the proposed model also has the same computational issues with SFNN in forward and backward passes due to the complex expectation. One can train Simplified-SFNN similarly as SFNN: we use Monte Carlo approximation for estimating the expectation and the (biased) estimator of the gradient for approximating backpropagation inspired by [9] (see Section 3.3 for more details).\nWe are interested in transferring parameters of DNN to Simplified-SFNN to utilize the training benefits of DNN since the former is much faster to train than the latter. To this end, we consider the following DNN of which `-th hidden layer is deterministic and defined as follows:\nh\u0302` (x) = [ h\u0302`i (x) = f ( W\u0302`i h\u0302 `\u22121 (x) + b\u0302`i ) : \u2200i ] , (7)\nwhere h\u03020(x) = x. As stated in the following theorem, we establish a rigorous way how to initialize parameters of Simplified-SFNN in order to transfer the knowledge stored in DNN.\nTheorem 1 Assume that both DNN and Simplified-SFNN with two hidden layers have same network structure with non-negative activation function f . Given parameters {W\u0302`, b\u0302` : ` = 1, 2} of DNN and input dataset D, choose those of Simplified-SFNN as follows:(\n\u03b11,W 1, b1 ) \u2190 ( 1\n\u03b31 , W\u03021, b\u03021\n) , ( \u03b12,W 2, b2 ) \u2190 ( \u03b32\u03b31 s\u2032 (0) , 1 \u03b32 W\u03022, 1 \u03b31\u03b32 b\u03022 ) , (8)\nwhere \u03b31 = max i,x\u2208D \u2223\u2223\u2223f (W\u03021i x+ b\u03021i)\u2223\u2223\u2223 and \u03b32 > 0 is any positive constant. Then, for all j,x \u2208 D, it follows that\n\u2223\u2223\u2223h2j (x)\u2212 h\u03022j (x)\u2223\u2223\u2223 \u2264 \u03b31 (\u2211 i \u2223\u2223\u2223W\u0302 2ij\u2223\u2223\u2223+ b\u03022j\u03b3\u221211 )2 2s\u2032 (0) \u03b32 .\nThe proof of the above theorem is presented in Section 5.1. Our proof is built upon the firstorder Taylor expansion of non-linear function s(x). Theorem 1 implies that one can make\nSimplified-SFNN represent the function values of DNN with bounded errors using a linear transformation. Furthermore, the errors can be made arbitrarily small by choosing large \u03b32, i.e., lim\n\u03b32\u2192\u221e \u2223\u2223\u2223h2j (x)\u2212 h\u03022j (x)\u2223\u2223\u2223 = 0, \u2200j,x \u2208 D. Figure 2(c) shows that knowledge transferring loss decreases as \u03b32 increases on MNIST classification. Based on this, we choose \u03b32 = 50 commonly for all experiments."}, {"heading": "3.2 Why Simplified-SFNN ?", "text": "Given a Simplified-SFNN model, the corresponding SFNN can be naturally defined by taking out the expectation in (6). As illustrated in Figure 2(a), the main difference between SFNN and Simplified-SFNN is that the randomness of the stochastic layer propagates only to its upper layer in the latter, i.e., the randomness of h1 is averaged out at its upper units h2 and does not propagate to h3 or output y. Hence, Simplified-SFNN is no longer a Bayesian network. This makes training Simplified-SFNN much easier than SFNN since random samples are not required at some layers1 and consequently the quality of gradient estimations can also be improved, in particular for unbounded activation functions. Furthermore, one can use the same approximation procedure (2) to see that Simplified-SFNN approximates SFNN. However, since Simplified-SFNN still maintains binary random units, it uses approximation steps later, in comparison with DNN. In summary, Simplified-SFNN is an intermediate model between DNN and SFNN, i.e., DNN\u2192 Simplified-SFNN\u2192 SFNN.\nThe above connection naturally suggests the following training procedure for both SFNN and Simplified-SFNN: train a baseline DNN first and then fine-tune its corresponding SimplifiedSFNN initialized by the transformed DNN parameters. Finally, the fine-tuned parameters can be used for SFNN as well. We evaluate the strategy for the MNIST classification. The MNIST dataset consists of 28\u00d7 28 pixel greyscale images, each containing a digit 0 to 9 with 60,000 training and 10,000 test images. For this experiment, we do not use any data augmentation or pre-processing. The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128. We used an exponentially decaying learning rate. Hyper-parameters are tuned on the validation set consisting of the last 10,000 training images. All Simplified-SFNNs are constructed by replacing the first hidden layer of a baseline DNN with stochastic hidden layer.\n1 For example, if one replaces the first feature maps in the fifth residual unit of Pre-ResNet having 164 layers [45] by stochastic ones, then the corresponding DNN, Simplified-SFNN and SFNN took 1 mins 35 secs, 2 mins 52 secs and 16 mins 26 secs per each training epoch, respectively, on our machine with one Intel CPU (Core i7-5820K 6-Core@3.3GHz) and one NVIDIA GPU (GTX Titan X, 3072 CUDA cores). Here, we trained both stochastic models using the biased estimator [9] with 10 random samples on CIFAR-10 dataset.\nWe first train a baseline DNN for first 200 epochs, and the trained parameters of DNN are used for initializing those of Simplified-SFNN. For 50 epochs, we train simplified-SFNN. We choose the hyper-parameter \u03b32 = 50 in the parameter transformation. All Simplified-SFNNs are trained with M = 20 samples at each epoch, and in the test, we use 500 samples. Table 2 shows that SFNN under the two-stage training always performs better than SFNN under a simple transformation (4) from ReLU-DNN. More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when we train both models using dropout [16] and batch normalization [17]. This implies that the proposed stochastic model can be used for improving the performance of DNNs and it can be also combined with other regularization methods such as dropout batch normalization. In order to confirm the regularization effects, one can again approximate a trained Simplified-SFNN by a new deterministic DNN which we call DNN\u2217 and is different from its baseline DNN under the following approximation at upper latent units above binary random units:\nEP(h`|x) [ s ( W`+1j h ` )] w s ( EP(h`|x) [ W`+1j h ` ]) = s (\u2211 i W `+1ij P ( h`i = 1 | x )) .\n(9)\nWe found that DNN\u2217 using fined-tuned parameters of Simplified-SFNN also outperforms the baseline DNN as shown in Table 2 and Figure 2(b)."}, {"heading": "3.3 Training Simplified-SFNN", "text": "The parameters of Simplified-SFNN can be learned using a variant of the backpropagation algorithm [29] in a similar manner to DNN. However, in contrast to DNN, there are two computational issues for simplified-SFNN: computing expectations with respect to stochastic units in forward pass and computing gradients in back pass. One can notice that both are intractable since they require summations over all possible configurations of all stochastic units. First, in order to handle the issue in forward pass, we use the following Monte Carlo approximation for estimating the expectation:\nEP (h1|x) [ s ( W2jh 1 + b2j )] w 1\nM M\u2211 m=1 s ( W2jh (m) + b2j ) ,\nwhere h(m) \u223c P ( h1 | x ) andM is the number of samples. This random estimator is unbiased and has relatively low variance [8] since its accuracy does not depend on the dimensionality of h1 and one can draw samples from the exact distribution. Next, in order to handle the issue in back pass, we use the following approximation inspired by [9]:\n\u2202\n\u2202W2j EP (h1|x)\n[ s ( W2jh 1 + b2j )] w 1\nM \u2211 m \u2202 \u2202W2j s ( W2jh (m) + b2j ) ,\n\u2202\n\u2202W1i EP (h1|x)\n[ s ( W2jh 1 + b2j )] w W 2ij M \u2211 m s\u2032 ( W2jh (m) + b2j ) \u2202 \u2202W1i P ( h1i = 1 | x ) ,\nwhere h(m) \u223c P ( h1 | x ) andM is the number of samples. In our experiments, we commonly choose M = 20."}, {"heading": "4 Extensions of Simplified-SFNN", "text": "In this section, we describe how the network knowledge transferring between SimplifiedSFNN and DNN, i.e., Theorem 1, generalizes to multiple layers and general activation functions."}, {"heading": "4.1 Extension to multiple layers", "text": "A deeper Simplified-SFNN with L hidden layers can be defined similarly as the case of L = 2. We also establish network knowledge transferring between Simplified-SFNN and DNN with L hidden layers as stated in the following theorem. Here, we assume that stochastic layers are not consecutive for simpler presentation, but the theorem is generalizable for consecutive stochastic layers.\nTheorem 2 Assume that both DNN and Simplified-SFNN with L hidden layers have same network structure with non-negative activation function f . Given parameters {W\u0302`, b\u0302` : ` = 1, . . . , L} of DNN and input dataset D, choose the same ones for Simplified-SFNN initially and modify them for each `-th stochastic layer and its upper layer as follows:\n\u03b1` \u2190 1 \u03b3` , ( \u03b1`+1,W `+1, b`+1 ) \u2190 ( \u03b3`\u03b3`+1 s\u2032 (0) , W\u0302`+1 \u03b3`+1 , b\u0302`+1 \u03b3`\u03b3`+1 ) , (10)\nwhere \u03b3` = max i,x\u2208D \u2223\u2223\u2223f (W\u0302`ih`\u22121(x) + b\u0302`i)\u2223\u2223\u2223 and \u03b3`+1 is any positive constant. Then, it follows that\nlim \u03b3`+1\u2192\u221e\n\u2200 stochastic hidden layer ` \u2223\u2223\u2223hLj (x)\u2212 h\u0302Lj (x)\u2223\u2223\u2223 = 0, \u2200j,x \u2208 D. The above theorem again implies that it is possible to transfer knowledge from DNN to Simplified-SFNN by choosing large \u03b3l+1. The proof of Theorem 2 is similar to that of Theorem 1 and given in Section 5.2."}, {"heading": "4.2 Extension to general activation functions", "text": "In this section, we describe an extended version of Simplified-SFNN which can utilize any activation function. To this end, we modify the definitions of stochastic layers and their upper layers by introducing certain additional terms. If the `-th hidden layer is stochastic, then we slightly modify the original definition (5) as follows:\nP ( h` | x ) = N`\u220f i=1 P ( h`i | x ) with P ( h`i = 1 | x ) = min { \u03b1`f ( W1i x+ b 1 i + 1 2 ) , 1 } ,\nwhere f : R \u2192 R is a non-linear (possibly, negative) activation function with |f \u2032(x)| \u2264 1 for all x \u2208 R. In addition, we re-define its upper layer as follows:\nh`+1 (x) = [ f ( \u03b1`+1 ( EP(h`|x) [ s ( W`+1j h ` + b`+1j )] \u2212 s (0)\u2212s \u2032 (0)\n2 \u2211 i W `+1ij )) : \u2200j ] ,\nwhere h0(x) = x and s : R\u2192 R is a differentiable function with |s\u2032\u2032(x)| \u2264 1 for all x \u2208 R. Under this general Simplified-SFNN model, we also show that transferring network knowledge from DNN to Simplified-SFNN is possible as stated in the following theorem. Here, we again assume that stochastic layers are not consecutive for simpler presentation.\nTheorem 3 Assume that both DNN and Simplified-SFNN with L hidden layers have same network structure with non-linear activation function f . Given parameters {W\u0302`, b\u0302` : ` = 1, . . . , L} of DNN and input dataset D, choose the same ones for Simplified-SFNN initially and modify them for each `-th stochastic layer and its upper layer as follows:\n\u03b1` \u2190 1 2\u03b3` , ( \u03b1`+1,W `+1, b`+1 ) \u2190 ( 2\u03b3`\u03b3`+1 s\u2032(0) , W\u0302`+1 \u03b3`+1 , b\u0302`+1 2\u03b3`\u03b3`+1 ) ,\nwhere \u03b3` = max i,x\u2208D \u2223\u2223\u2223f (W\u0302`ih`\u22121(x) + b\u0302`i)\u2223\u2223\u2223, and \u03b3`+1 is any positive constant. Then, it follows that\nlim \u03b3`+1\u2192\u221e\n\u2200 stochastic hidden layer ` \u2223\u2223\u2223hLj (x)\u2212 h\u0302Lj (x)\u2223\u2223\u2223 = 0, \u2200j,x \u2208 D. We omit the proof of the above theorem since it is somewhat direct adaptation of that of Theorem 2."}, {"heading": "5 Proofs of Theorems", "text": ""}, {"heading": "5.1 Proof of Theorem 1", "text": "First consider the first hidden layer, i.e., stochastic layer. Let \u03b31 = max i,x\u2208D\nf ( W\u03021i x+ b\u0302 1 i ) be\nthe maximum value of hidden units in DNN. If we initialize the parameters ( \u03b11,W 1, b1 ) \u2190(\n1 \u03b31 , W\u03021, b\u03021\n) , then the marginal distribution of each hidden unit i becomes\nP ( h1i = 1 | x,W1,b1 ) = min { \u03b11f ( W\u03021i x+ b\u0302 1 i ) , 1 } = 1 \u03b31 f ( W\u03021i x+ b\u0302 1 i ) , \u2200i,x \u2208 D.\n(11)\nNext consider the second hidden layer. From Taylor\u2019s theorem, there exists a value z between 0 and x such that s(x) = s(0) + s\u2032(0)x+ R(x), where R(x) = s \u2032\u2032(z)x2\n2! . Since we consider a binary random vector, i.e., h1 \u2208 {0, 1}N1 , one can write\nEP (h1|x) [ s ( \u03b2j ( h1 ))]\n= \u2211 h1 ( s (0) + s\u2032 (0)\u03b2j ( h1 ) +R ( \u03b2j ( h1 ))) P ( h1 | x ) = s (0) + s\u2032 (0)\n(\u2211 i W 2ijP (h 1 i = 1 | x) + b2j ) + EP (h1|x) [ R(\u03b2j(h 1)) ] , (12)\nwhere \u03b2j ( h1 ) := W2jh\n1+b2j is the incoming signal. From (6) and (11), for every hidden unit j, it follows that\nh2j ( x;W2,b2 ) =f ( \u03b12 ( s\u2032(0) ( 1\n\u03b31 \u2211 i W 2ij h\u0302 1 i (x) + b 2 j\n) + EP (h1|x) [ R ( \u03b2j ( h1 ))])) .\nSince we assume that |f \u2032(x)| \u2264 1, the following inequality holds:\u2223\u2223\u2223\u2223\u2223h2j (x;W2,b2)\u2212 f ( \u03b12s \u2032(0) ( 1\n\u03b31 \u2211 i W 2ij h\u0302 1 i (x) + b 2 j ))\u2223\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u03b12EP (h1|x) [R(\u03b2j(h1))]\u2223\u2223 \u2264 \u03b122 EP (h1|x) [(W2jh1 + b2j)2] , (13)\nwhere we use |s\u2032\u2032(z)| < 1 for the last inequality. Therefore, it follows that\n\u2223\u2223\u2223h2j (x;W2,b2)\u2212 h\u03022j (x;W\u03022, b\u03022)\u2223\u2223\u2223 \u2264 \u03b31 (\u2211 i \u2223\u2223\u2223W\u0302 2ij\u2223\u2223\u2223+ b\u03022j\u03b3\u221211 )2 2s\u2032(0)\u03b32 , \u2200j,\nsince we set ( \u03b12,W 2, b2 ) \u2190 ( \u03b32\u03b31 s\u2032(0) , W\u03022 \u03b32 , \u03b3\u221211 \u03b32 b\u03022 ) . This completes the proof of Theorem 1."}, {"heading": "5.2 Proof of Theorem 2", "text": "For the proof of Theorem 2, we first state the two key lemmas on error propagation in SimplifiedSFNN.\nLemma 4 Assume that there exists some positive constant B such that\u2223\u2223\u2223h`\u22121i (x)\u2212 h\u0302`\u22121i (x)\u2223\u2223\u2223 \u2264 B, \u2200i,x \u2208 D, and the `-th hidden layer of Simplified-SFNN is standard deterministic layer as defined in (7). Given parameters {W\u0302`, b\u0302`} of DNN, choose same ones for Simplified-SFNN. Then, the following inequality holds:\u2223\u2223\u2223h`j (x)\u2212 h\u0302`j (x)\u2223\u2223\u2223 \u2264 BN `\u22121W\u0302 `max, \u2200j,x \u2208 D. where W\u0302 `max = max\nij \u2223\u2223\u2223W\u0302 `ij\u2223\u2223\u2223. Proof. See Section 5.3.\nLemma 5 Assume that there exists some positive constant B such that\u2223\u2223\u2223h`\u22121i (x)\u2212 h\u0302`\u22121i (x)\u2223\u2223\u2223 \u2264 B, \u2200i,x \u2208 D, and the `-th hidden layer of simplified-SFNN is stochastic layer. Given parameters {W\u0302`,W\u0302`+1, b\u0302`, b\u0302`+1} of DNN, choose those of Simplified-SFNN as follows:\n\u03b1` \u2190 1 \u03b3` , ( \u03b1`+1,W `+1, b`+1 ) \u2190 ( \u03b3`\u03b3`+1 s\u2032 (0) , W\u0302`+1 \u03b3`+1 , b\u0302`+1 \u03b3`\u03b3`+1 ) ,\nwhere \u03b3` = max j,x\u2208D \u2223\u2223\u2223f (W\u0302`jh`\u22121(x) + b\u0302`j)\u2223\u2223\u2223 and \u03b3`+1 is any positive constant. Then, for all j,x \u2208 D, it follows that\n\u2223\u2223\u2223h`+1k (x)\u2212 h\u0302`+1k (x)\u2223\u2223\u2223 \u2264BN `\u22121N `W\u0302 `maxW\u0302 `+1max + \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03b3` ( N `W\u0302 `+1max + b\u0302 `+1 max\u03b3 \u22121 ` )2 2s\u2032(0)\u03b3`+1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 , where b\u0302`max = max\nj \u2223\u2223\u2223\u0302b`j\u2223\u2223\u2223 and W\u0302 `max = max ij \u2223\u2223\u2223W\u0302 `ij\u2223\u2223\u2223.\nProof. See Section 5.4.\nAssume that `-th layer is first stochastic hidden layer in Simplified-SFNN. Then, from Theorem 1, we have\n\u2223\u2223\u2223h`+1j (x)\u2212 h\u0302`+1j (x)\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03b3` ( N `W\u0302 `+1max + b\u0302 `+1 max\u03b3 \u22121 ` )2 2s\u2032(0)\u03b3`+1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 , \u2200j,x \u2208 D. (14) According to Lemma 4 and 5, the final error generated by the right hand side of (14) is bounded by\n\u03c4`\u03b3` ( N `W\u0302 `+1max + b\u0302 `+1 max\u03b3 \u22121 ` )2 2s\u2032 (0) \u03b3`+1 , (15)\nwhere \u03c4` = L\u220f\n`\u2032=l+2\n( N ` \u2032\u22121W\u0302 ` \u2032 max ) . One can note that every error generated by each stochastic\nlayer is bounded by (15). Therefore, for all j,x \u2208 D, it follows that\n\u2223\u2223\u2223hLj (x)\u2212 h\u0302Lj (x)\u2223\u2223\u2223 \u2264 \u2211 `:stochastic hidden layer\n\u03c4`\u03b3` ( N `W\u0302 `+1max + b\u0302 `+1 max\u03b3 \u22121 ` )2 2s\u2032 (0) \u03b3`+1  . From above inequality, we can conclude that\nlim \u03b3`+1\u2192\u221e\n\u2200 stochastic hidden layer ` \u2223\u2223\u2223hLj (x)\u2212 h\u0302Lj (x)\u2223\u2223\u2223 = 0, \u2200j,x \u2208 D. This completes the proof of Theorem 2."}, {"heading": "5.3 Proof of Lemma 4", "text": "From assumption, there exists some constant i such that | i| < B and\nh`\u22121i (x) = h\u0302 `\u22121 i (x) + i, \u2200i,x.\nBy definition of standard deterministic layer, it follows that\nh`j (x) = f (\u2211 i W\u0302 `ijh `\u22121 i (x) + b\u0302 `\u22121 j ) = f (\u2211 i W\u0302 `ij h\u0302 `\u22121 i (x) + \u2211 i W\u0302 `ij i + b\u0302 ` j ) .\nSince we assume that |f \u2032(x)| \u2264 1, one can conclude that\u2223\u2223\u2223\u2223\u2223h`j (x)\u2212 f (\u2211\ni\nW\u0302 `ij h\u0302 `\u22121 i (x) + b\u0302 ` j )\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2211 i W\u0302 `ij i \u2223\u2223\u2223\u2223\u2223 \u2264 B \u2223\u2223\u2223\u2223\u2223\u2211 i W\u0302 `ij \u2223\u2223\u2223\u2223\u2223 \u2264 BN `\u22121W\u0302 `max. This completes the proof of Lemma 4."}, {"heading": "5.4 Proof of Lemma 5", "text": "From assumption, there exists some constant `\u22121i such that \u2223\u2223\u2223 `\u22121i \u2223\u2223\u2223 < B and\nh`\u22121i (x) = h\u0302 `\u22121 i (x) + `\u22121 i , \u2200i,x. (16)\nLet \u03b3` = max j,x\u2208D \u2223\u2223\u2223f (W\u0302`jh`\u22121(x) + b\u0302`j)\u2223\u2223\u2223 be the maximum value of hidden units. If we initialize the parameters ( \u03b1`,W `, b` ) \u2190 (\n1 \u03b3` , W\u0302`, b\u0302`\n) , then the marginal distribution becomes\nP ( h`j = 1 | x,W`,b` ) = min { \u03b1`f ( W\u0302`jh `\u22121 (x) + b\u0302`j ) , 1 } = 1\n\u03b3` f ( W\u0302`jh `\u22121 (x) + b\u0302`j ) , \u2200j,x.\nFrom (16), it follows that\nP ( h`j = 1 | x,W`,b` ) = 1\n\u03b3` f\n( W\u0302`jh\u0302 `\u22121 (x) + \u2211 i W\u0302 `ij `\u22121 i + b\u0302 ` j ) , \u2200j,x.\nSimilar to Lemma 4, there exists some constant `j such that \u2223\u2223\u2223 `j\u2223\u2223\u2223 < BN `\u22121W\u0302 `max and\nP ( h`j = 1 | x,W`,b` ) = 1\n\u03b3`\n( h\u0302`j (x) + ` j ) , \u2200j,x. (17)\nNext, consider the upper hidden layer of stochastic layer. From Taylor\u2019s theorem, there exists a value z between 0 and t such that s(x) = s(0) + s\u2032(0)x + R(x), where R(x) = s \u2032\u2032(z)x2\n2! . Since we consider a binary random vector, i.e., h` \u2208 {0, 1}N` , one can write\nEP (h`|x)[s(\u03b2k(h`))] = \u2211 h` ( s(0) + s\u2032(0)\u03b2k(h `) +R ( \u03b2k(h `) )) P (h` | x)\n= s(0) + s\u2032(0) \u2211 j W `+1jk P (h ` j = 1 | x) + b`+1k +\u2211 h` R(\u03b2k(h `))P (h` | x),\nwhere \u03b2k(h`) = W`+1k h ` + b`+1k is the incoming signal. From (17) and above equation, for every hidden unit k, we have\nh`+1k (x;W `+1,b`+1)\n= f ( \u03b1`+1 ( s\u2032(0) ( 1\n\u03b3` \u2211 j W `+1jk h\u0302 ` j(x) + \u2211 j W `+1jk ` j + b`+1k ) + EP (h`|x) [ R(\u03b2k(h `)) ])) .\nSince we assume that |f \u2032(x)| < 1, the following inequality holds:\u2223\u2223\u2223\u2223\u2223h`+1k (x;W`+1,b`+1)\u2212 f \u03b1`+1s\u2032(0)  1 \u03b3` \u2211 j W `+1ij h\u0302 ` j(x) + b `+1 j \u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u03b1`+1s \u2032(0) \u03b3` \u2211 j W `+1jk ` j + \u03b1`+1EP (h`|x) [ R(\u03b2k(h `)) ]\u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2223\u2223\u2223\u03b1`+1s \u2032(0) \u03b3` \u2211 j W `+1jk ` j \u2223\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u03b1`+12 EP (h`|x) [( W`+1k h ` + b`+1k )2]\u2223\u2223\u2223\u2223 , (18) where we use |s\u2032\u2032(z)| < 1 for the last inequality. Therefore, it follows that\n\u2223\u2223\u2223h`+1k (x)\u2212 h\u0302`+1k (x)\u2223\u2223\u2223 \u2264 BN `\u22121N `W\u0302 `maxW\u0302 `+1max + \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u03b3` ( N `W\u0302 `+1max + b\u0302 `+1 max\u03b3 \u22121 ` )2 2s\u2032(0)\u03b3`+1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223 , since we set ( \u03b1`+1,W `+1, b`+1 ) \u2190 ( \u03b3`+1\u03b3` s\u2032(0) , W\u0302`+1 \u03b3`+1 , \u03b3\u22121` b\u0302 `+1 \u03b3`+1 ) . This completes the proof of Lemma 5."}, {"heading": "6 Experimental Results", "text": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26]. The softmax and Gaussian with the standard deviation of 0.05 are used as the output probability for the classification task and the multi-modal prediction, respectively. In all experiments, we first train a baseline model, and the trained parameters are used for further fine-tuning those of Simplified-SFNN."}, {"heading": "6.1 Multi-modal regression", "text": "We first verify that it is possible to learn one-to-many mapping via Simplified-SFNN on the TFD, MNIST and CASIA datasets. The TFD dataset consists of 48 \u00d7 48 pixel greyscale images, each containing a face image of 900 individuals with 7 different expressions. Similar to [9], we use 124 individuals with at least 10 facial expressions as data. We randomly choose 100 individuals with 1403 images for training and the remaining 24 individuals with 326 images for the test. We take the mean of face images per individual as the input and set the output as the different expressions of the same individual. The MNIST dataset consists of greyscale images, each containing a digit 0 to 9 with 60,000 training and 10,000 test images. For this experiments, each pixel of every digit images is binarized using its grey-scale value similar to [9]. We take the upper half of the MNIST digit as the input and set the output as the lower half of it. We remark that both tasks are commonly performed in recent other works to test the multi-modal learning using SFNN [9, 34]. The CASIA dataset consists of greyscale images, each containing a handwritten Chinese character. We use 10 offline isolated characters produced by 345 writers as data.2 We randomly choose 300 writers with 3000 images for training and the remaining 45 writers with 450 images for testing. We take the radical character per writer as the input and set the output as either the related character or the radical character itself of the same writer (see Figure 5). The bicubic interpolation [4] is used for re-sizing all images as 32\u00d7 32 pixels.\nFor both TFD and MNIST datasets, we use fully-connected DNNs as the baseline models similar to other works [9, 34]. All Simplified-SFNNs are constructed by replacing the first hidden layer of a baseline DNN with stochastic hidden layer. The loss was minimized using\n2We use 5 radical characters (e.g., wei, shi, shui, ji and mu in Mandarin) and 5 related characters (e.g., hui, si, ben, ren and qiu in Mandarin).\nADAM learning rule [7] with a mini-batch size of 128. We used an exponentially decaying learning rate. We train Simplified-SFNNs with M = 20 samples at each epoch, and in the test, we use 500 samples. We use 200 hidden units for each layer of neural networks in two experiments. Learning rate is chosen from {0.005 , 0.002, 0.001, ... , 0.0001}, and the best result is reported for both tasks. Table 3 reports the test negative log-likelihood on TFD and MNIST. One can note that SFNNs fine-tuned by Simplified-SFNN consistently outperform SFNN under the simple transformation.\nFor the CASIA dataset, we choose fully-convolutional network (FCN) models [2] as the baseline ones, which consists of convolutional layers followed by a fully-convolutional layer. In a similar manner to the case of fully-connected networks, one can define a stochastic convolution layer, which considers the input feature map as a binary random matrix and generates\nthe output feature map as defined in (6). For this experiment, we use a baseline model, which consists of convolutional layers followed by a fully convolutional layer. The convolutional layers have 64, 128 and 256 filters respectively. Each convolutional layer has a44receptive field applied with a stride of 2 pixel. All Simplified-SFNNs are constructed by replacing the first hidden feature maps of baseline models with stochastic ones. The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128. We used an exponentially decaying learning rate. We train Simplified-SFNNs with M = 20 samples at each epoch, and in the test, we use 100 samples due to the memory limitations. One can note that SFNNs fine-tuned by Simplified-SFNN outperform SFNN under the simple transformation as reported in Table 4 and Figure 5."}, {"heading": "6.2 Classification", "text": "We also evaluate the regularization effect of Simplified-SFNN for the classification tasks on CIFAR-10, CIFAR-100 and SVHN. The CIFAR-10 and CIFAR-100 datasets consist of 50,000 training and 10,000 test images. They have 10 and 100 image classes, respectively. The SVHN dataset consists of 73,257 training and 26,032 test images.3 It consists of a house number 0 to 9 collected by Google Street View. Similar to [39], we pre-process the data using global contrast normalization and ZCA whitening. For these datasets, we design a convolutional version of Simplified-SFNN using convolutional neural networks such as Lenet-5 [6], NIN [13] and WRN [39]. All Simplified-SFNNs are constructed by replacing a hidden feature map\n3We do not use the extra SVHN dataset for training.\nof a baseline models, i.e., Lenet-5, NIN and WRN, with stochastic one as shown in Figure 3(d). We use WRN with 16 and 28 layers for SVHN and CIFAR datasets, respectively, since they showed state-of-the-art performance as reported by [39]. In case of WRN, we introduce up to two stochastic convolution layers. Similar to [39], the loss was minimized using the stochastic gradient descent method with Nesterov momentum. The minibatch size is set to 128, and weight decay is set to 0.0005. For 100 epochs, we first train baseline models, i.e., Lenet-5, NIN and WRN, and trained parameters are used for initializing those of Simplified-SFNNs. All Simplified-SFNNs are trained with M = 5 samples and the test error is only measured by the approximation (9). The test errors of baseline models are measured after training them for 200 epochs similar to [39]. All models are trained using dropout [16] and batch normalization [17]. Table 5 reports the classification error rates on CIFAR-10, CIFAR-100 and SVHN. Due to the regularization effects, Simplified-SFNNs consistently outperform their baseline DNNs. In particular, WRN\u2217 of 28 layers and 36 million parameters outperforms WRN by 0.08% on CIFAR-10 and 0.58% on CIFAR-100. Figure 6 shows that the error rate is decreased more when we introduce more stochastic layers, but it increases the fine-tuning time-complexity of Simplified-SFNN."}, {"heading": "7 Conclusion", "text": "In order to develop an efficient training method for large-scale SFNN, this paper proposes a new intermediate stochastic model, called Simplified-SFNN. We establish the connection between three models, i.e., DNN \u2192 Simplified-SFNN \u2192 SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. This connection naturally leads an efficient training procedure of the stochastic models utilizing pre-trained parameters and architectures of DNN. Using several popular DNNs including Lenet-5, NIN, FCN and WRN, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and non-multi-modal tasks. We believe that our work brings a new angle for training stochastic neural networks, which would be of\nbroader interest in many related applications."}], "references": [{"title": "The generalized reparameterization gradient", "author": ["F.R. Ruiz", "M.T.R. AUEB", "D. Blei"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "CASIA online and offline Chinese handwriting databases", "author": ["C.L. Liu", "F. Yin", "D.H. Wang", "Q.F. Wang"], "venue": "In International Conference on Document Analysis and Recognition (ICDAR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Cubic convolution interpolation for digital image processing", "author": ["R. Keys"], "venue": "In IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1981}, {"title": "Learning stochastic feedforward networks", "author": ["R.M. Neal"], "venue": "Department of Computer Science, University of Toronto,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R.R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["T. Raiko", "M. Berglund", "G. Alain", "L. Dinh"], "venue": "arXiv preprint arXiv:1406.2989,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y.L. Cun", "R. Fergus"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Masters thesis,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Weight uncertainty in neural networks", "author": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "Artificial intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Sum-product networks: A new deep architecture", "author": ["H. Poon", "P. Domingos"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Net2Net: Accelerating Learning via Knowledge Transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Networks of spiking neurons: the third generation of neural network models", "author": ["W. Maass"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.R. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.P. David"], "venue": "arXiv preprint arXiv:1511.00363,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Noisy activation functions", "author": ["C. Gulcehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.00391,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "In Neurocomputing: Foundations of research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1988}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1951}, {"title": "Adaptive dropout for training deep neural networks", "author": ["J. Ba", "B. Frey"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "MuProp: Unbiased backpropagation for stochastic neural networks", "author": ["S. Gu", "S. Levine", "I. Sutskever", "A. Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. Lonard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": "Aston University,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}, {"title": "The toronto face database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Department of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Artificial intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1996}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Reinforcement learning neural Turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["T. Salimans", "D.P. Kingma"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": ", speech recognition [24] and object recognition [11].", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", speech recognition [24] and object recognition [11].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 140, "endOffset": 144}, {"referenceID": 28, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 200, "endOffset": 208}, {"referenceID": 9, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 200, "endOffset": 208}, {"referenceID": 15, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 237, "endOffset": 245}, {"referenceID": 43, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 237, "endOffset": 245}, {"referenceID": 31, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 280, "endOffset": 288}, {"referenceID": 26, "context": "One of the main components underlying these successes is the efficient training methods for large-scale DNNs, which include backpropagation [29], stochastic gradient descent [30], dropout/dropconnect [16, 10], batch/weight normalization [17, 46], and various activation functions [33, 28].", "startOffset": 280, "endOffset": 288}, {"referenceID": 4, "context": "On the other hand, stochastic feedforward neural networks (SFNNs) [5] having random latent units are often necessary in to model the complex stochastic natures of many real-world tasks, e.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": ", structured prediction [8] and image generation [41].", "startOffset": 24, "endOffset": 27}, {"referenceID": 38, "context": ", structured prediction [8] and image generation [41].", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Furthermore, it is believed that SFNN has several advantages beyond DNN [9]: it has more expressive power for multi-modal learning and regularizes better for large-scale networks.", "startOffset": 72, "endOffset": 75}, {"referenceID": 30, "context": "Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation with variational techniques and reparameterization tricks [32, 1].", "startOffset": 183, "endOffset": 190}, {"referenceID": 0, "context": "Certain stochastic neural networks using continuous random units are known to be trainable efficiently using backpropagation with variational techniques and reparameterization tricks [32, 1].", "startOffset": 183, "endOffset": 190}, {"referenceID": 4, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 36, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 7, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 33, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 8, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 32, "context": "There have been several efforts toward developing efficient training methods for SFNN having binary random latent units [5, 38, 8, 35, 9, 34] (see Section 2.", "startOffset": 120, "endOffset": 141}, {"referenceID": 31, "context": "However, it is questionable whether a similar strategy works in general, particularly for other unbounded activation functions like ReLU [33] since SFNN has binary, i.", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "The most significant part of our approach lies in providing rigorous network knowledge transferring [22] between Simplified-SFNN and DNN.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 111, "endOffset": 114}, {"referenceID": 11, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "We also construct the stochastic models following the same network structure of popular DNNs including Lenet-5 [6], NIN [13], FCN [2] and WRN [39].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "The multi-layer extension is straightforward via a combination of stochastic and deterministic hidden layers (see [8, 9]).", "startOffset": 114, "endOffset": 120}, {"referenceID": 8, "context": "The multi-layer extension is straightforward via a combination of stochastic and deterministic hidden layers (see [8, 9]).", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "This random estimator is unbiased and has relatively low variance [8] since one can draw samples from the exact distribution.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "Next, in order to handle the issue in backward pass, [5] proposed Gibbs sampling, but it is known that it often mixes poorly.", "startOffset": 53, "endOffset": 56}, {"referenceID": 36, "context": "[38] proposed a variational learning based on the mean-field approximation, but it has additional parameters making the variational lower bound looser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 140, "endOffset": 146}, {"referenceID": 33, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 8, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 32, "context": "More recently, several other techniques have been proposed including unbiased estimators of the variational bound using importance sampling [8, 9] and biased/unbiased estimators of the gradient for approximating backpropagation [35, 9, 34].", "startOffset": 228, "endOffset": 239}, {"referenceID": 8, "context": "Although such a strategy looks somewhat \u2018rude\u2019, it was often observed in the literature that it reasonably works well for SFNN [9] and we also evaluate it as reported in Table 1.", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "We evaluate the performance of the simple transformations from DNN to SFNN on the MNIST dataset [6] and the synthetic dataset [36], where the former and the latter are popular datasets used for a classification task and a multi-modal (i.", "startOffset": 96, "endOffset": 99}, {"referenceID": 34, "context": "We evaluate the performance of the simple transformations from DNN to SFNN on the MNIST dataset [6] and the synthetic dataset [36], where the former and the latter are popular datasets used for a classification task and a multi-modal (i.", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "One can train Simplified-SFNN similarly as SFNN: we use Monte Carlo approximation for estimating the expectation and the (biased) estimator of the gradient for approximating backpropagation inspired by [9] (see Section 3.", "startOffset": 202, "endOffset": 205}, {"referenceID": 6, "context": "The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 48, "endOffset": 51}, {"referenceID": 42, "context": "1 For example, if one replaces the first feature maps in the fifth residual unit of Pre-ResNet having 164 layers [45] by stochastic ones, then the corresponding DNN, Simplified-SFNN and SFNN took 1 mins 35 secs, 2 mins 52 secs and 16 mins 26 secs per each training epoch, respectively, on our machine with one Intel CPU (Core i7-5820K 6-Core@3.", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "Here, we trained both stochastic models using the biased estimator [9] with 10 random samples on CIFAR-10 dataset.", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when we train both models using dropout [16] and batch normalization [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "More interestingly, Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when we train both models using dropout [16] and batch normalization [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "3 Training Simplified-SFNN The parameters of Simplified-SFNN can be learned using a variant of the backpropagation algorithm [29] in a similar manner to DNN.", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "This random estimator is unbiased and has relatively low variance [8] since its accuracy does not depend on the dimensionality of h1 and one can draw samples from the exact distribution.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Next, in order to handle the issue in back pass, we use the following approximation inspired by [9]:", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 95, "endOffset": 98}, {"referenceID": 35, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 128, "endOffset": 132}, {"referenceID": 2, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 140, "endOffset": 143}, {"referenceID": 12, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 165, "endOffset": 169}, {"referenceID": 24, "context": "We present several experimental results for both multi-modal and classification tasks on MNIST [6], Toronto Face Database (TFD) [37], CASIA [3], CIFAR-10, CIFAR-100 [14] and SVHN [26].", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "Similar to [9], we use 124 individuals with at least 10 facial expressions as data.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "For this experiments, each pixel of every digit images is binarized using its grey-scale value similar to [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We remark that both tasks are commonly performed in recent other works to test the multi-modal learning using SFNN [9, 34].", "startOffset": 115, "endOffset": 122}, {"referenceID": 32, "context": "We remark that both tasks are commonly performed in recent other works to test the multi-modal learning using SFNN [9, 34].", "startOffset": 115, "endOffset": 122}, {"referenceID": 3, "context": "The bicubic interpolation [4] is used for re-sizing all images as 32\u00d7 32 pixels.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "For both TFD and MNIST datasets, we use fully-connected DNNs as the baseline models similar to other works [9, 34].", "startOffset": 107, "endOffset": 114}, {"referenceID": 32, "context": "For both TFD and MNIST datasets, we use fully-connected DNNs as the baseline models similar to other works [9, 34].", "startOffset": 107, "endOffset": 114}, {"referenceID": 6, "context": "ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "For the CASIA dataset, we choose fully-convolutional network (FCN) models [2] as the baseline ones, which consists of convolutional layers followed by a fully-convolutional layer.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "The loss was minimized using ADAM learning rule [7] with a mini-batch size of 128.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "For these datasets, we design a convolutional version of Simplified-SFNN using convolutional neural networks such as Lenet-5 [6], NIN [13] and WRN [39].", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "For these datasets, we design a convolutional version of Simplified-SFNN using convolutional neural networks such as Lenet-5 [6], NIN [13] and WRN [39].", "startOffset": 134, "endOffset": 138}, {"referenceID": 14, "context": "All models are trained using dropout [16] and batch normalization [17].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "All models are trained using dropout [16] and batch normalization [17].", "startOffset": 66, "endOffset": 70}], "year": 2017, "abstractText": "It has been believed that stochastic feedforward neural networks (SFNNs) have several advantages beyond deterministic deep neural networks (DNNs): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training large-scale SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN \u2192 Simplified-SFNN \u2192 SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CASIA, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, we train a stochastic model of 28 layers and 36 million parameters, where training such a largescale stochastic network is significantly challenging without using Simplified-SFNN.", "creator": "LaTeX with hyperref package"}}}