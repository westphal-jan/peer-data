{"id": "1704.03969", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Convergence analysis of the information matrix in Gaussian belief propagation", "abstract": "Gaussian belief propagation (BP) has been widely used for distributed estimation in large-scale networks such as the smart grid, communication networks, and social networks, where local measurements/observations are scattered over a wide geographical area. However, the convergence of Gaus- sian BP is still an open issue. In this paper, we consider the convergence of Gaussian BP, focusing in particular on the convergence of the information matrix. We show analytically that the exchanged message information matrix converges for arbitrary positive semidefinite initial value, and its dis- tance to the unique positive definite limit matrix decreases exponentially fast. As an example, the signal of the distribution of the value of the original signal has a strong statistical significance (H=n, E=n, G=n, G=n). It was recently found that Gaussian BP was only a single value, not a whole. This gives the possibility that this distribution can be derived by the diffusion of the information matrix between regions.\n\n\n\nIn this paper, we present the convergence of the data matrix of the signal in the presence of a specific information matrix for a continuous interval. The information matrix is used to determine the probability of each individual signal reaching a given location in a particular region. As we do not have the ability to generate individual information matrix to converge on the signal, we conclude that the distribution of the value of the original signal could be obtained via this network approach, because it is the first of its kind for large network operations. We also present a method to obtain the distribution of the values from the data matrix in a continuous interval using a large-scale, uniform data matrix.\n\nIn the paper, we perform a regression in the distribution of the value of the original signal in the presence of a single data matrix. In practice, the results of this technique are very similar to that of the first approach. Using a large-scale, uniform data matrix we could obtain a probability that each individual signal reaches a given location in a particular region. Thus, a Gaussian BP distribution of the original signal with the maximum probability of a single signal reaching a given location has a strong statistical significance (E=n, E=n, G=n, G=n).\nAlthough the distribution of the total value of the original signal can be quite different, it is still the most reliable method for obtaining the distribution of the value of the original signal. This is the one of the major problems that we face with a Gaussian BP. For", "histories": [["v1", "Thu, 13 Apr 2017 02:07:15 GMT  (21kb)", "http://arxiv.org/abs/1704.03969v1", "arXiv admin note: substantial text overlap witharXiv:1611.02010"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1611.02010", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian du", "shaodan ma", "yik-chung wu", "soummya kar", "jos\\'e m f moura"], "accepted": false, "id": "1704.03969"}, "pdf": {"name": "1704.03969.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shaodan Ma", "Yik-Chung Wu", "Soummya Kar", "Jos\u00e9 M. F. Moura"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n03 96\n9v 1\n[ cs\n.L G\n] 1\n3 A\npr 2\ndistributed estimation in large-scale networks such as the smart grid, communication networks, and social networks, where local measurements/observations are scattered over a wide geographical area. However, the convergence of Gaussian BP is still an open issue. In this paper, we consider the convergence of Gaussian BP, focusing in particular on the convergence of the information matrix. We show analytically that the exchanged message information matrix converges for arbitrary positive semidefinite initial value, and its distance to the unique positive definite limit matrix decreases exponentially fast.\nIndex Terms\u2014 graphical model, belief propagation,\nlarge-scale networks, Markov random field."}, {"heading": "1. INTRODUCTION", "text": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10]. It has been shown that Gaussian BP computes the optimal centralized estimator if it converges [11].\nAlthough with great empirical success, the major challenge that hinders Gaussian BP to realize its full potential is the lack of theoretical guarantees of convergence in loopy networks. Sufficient convergence conditions for Gaussian BP have been developed in [1,12\u201314] when the underlying Gaussian distribution is expressed in terms of pairwise connections between scalar variables (also known as Markov random field (MRF)). These works focus on the convergence analysis\nThis work is partially supported by NSF grant # CCF1513936.\nof Gaussian BP for computing the marginal distribution of a joint distribution with pairwise factors. However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved. Therefore, these existing conditions and analysis methods are not applicable to distributed estimation problems. In this paper, we study the convergence analysis of Gaussian BP for distributed parameter estimation focusing on the convergence of message information matrix. We show analytically that, with arbitrary positive semidefinite matrix initialization, the message information matrix being exchanged among nodes converges and its distance to the unique positive definite limit matrix decreases exponentially.\nNote that distributed estimation based on the consensus+ innovations philosophy proposed in [16, 17] (see also the related family of diffusion algorithms [18]) converges to the optimal centralized estimator under the assumption of global observability of the (aggregate) sensing model and connectivity of the inter-agent communication network. In particular, these algorithms allow the communication or message exchange network to be different from the physical coupling network and the former could be arbitrary with cycles (as long as it is connected). The results in [16, 17] imply that the unknown variables x can be reconstructed completely at each node in the network. For large-scale networks with high dimensional x, it may be impractical to reconstruct x at every node. In [19, section 3.4], the author developed approaches to address this problem, where each node can reconstruct a set of unknown variables that should be larger than the set of variables that influence its local measurement. This paper studies a different distributed estimation problem when each node estimates only its own unknown variables under pairwise independence condition of the unknown variables; this leads to lower dimensional data exchanges between neighbors."}, {"heading": "2. COMPUTATION MODEL", "text": "Consider a general connected network ofM nodes, with V = {1, . . . ,M} denoting the set of nodes, and ENet \u2282 V \u00d7 V as\nthe set of all undirect communication links in the network, i.e., if i and j are within the communication range, (i, j) \u2208 ENet. At every node n \u2208 V , the local observations are in the form of yn = \u2211 i\u2208n\u222aI(n)An,ixi + zn, where I(n) denotes the set of direct neighbors of node n (i.e., all nodes i with (n, i) \u2208 ENet), An,i is a known coefficient matrix with full column rank, xi is the local unknown parameter at node iwith dimension Ni \u00d7 1, and with the prior distribution p(xi) \u223c N (xi|0,Wi), and zn is the additive noise with distribution zn \u223c N (zn|0,Rn). It is assumed that p(xi, xj) = p(xi)p(xj) and p(zi, zj) = p(zi)p(zj) for i 6= j. The goal is to estimate xi, based on yn, p(xi) and p(zn).\nThe Gaussian BP algorithm can be derived over the corresponding factor graph to compute the estimate of xn for all n \u2208 V [20]. It involves two kinds of messages: One is the message from a variable node xj to its neighboring factor node fn, defined as\nm (\u2113) j\u2192fn (xj) = p(xj) \u220f\nfk\u2208B(j)\\fn\nm (\u2113\u22121) fk\u2192j (xj), (1)\nwhere B(j) denotes the set of neighbouring factor nodes of xj , andm (\u2113\u22121) fk\u2192j\n(xj) is the message from fk to xj at time l\u22121. The second type of message is from a factor node fn to a neighboring variable node xi, defined as\nm (\u2113) fn\u2192i (xi) =\n\u222b \u00b7 \u00b7 \u00b7 \u222b fn \u00d7 \u220f\nj\u2208B(fn)\\i\nm (\u2113) j\u2192fn (xj) d{xj}j\u2208B(fn)\\i,\n(2)\nwhere B(fn) denotes the set of neighboring variable nodes of fn. The process iterates between equations (1) and (2). At each iteration l, the approximate marginal distribution, also named belief, on xi is computed locally at xi as\nb (\u2113) BP (xi) = p(xi)\n\u220f\nfn\u2208B(i)\nm (\u2113) fn\u2192i (xi). (3)\nIt can be shown [20] that the general expression for the\nmessage from variable node to factor node is\nm (\u2113) j\u2192fn (xj) \u221d exp { \u2212 1\n2 ||xj \u2212 v\n(\u2113) j\u2192fn || C\n(\u2113) j\u2192fn\n} , (4)\nwhere C (\u2113) j\u2192fn and v (\u2113) j\u2192fn are the message covariance matrix and mean vector received at variable node j at the l-th iteration, with\n[ C\n(\u2113) j\u2192fn ]\u22121 = W\u22121j +\n\u2211\nfk\u2208B(j)\\fn\n[ C\n(\u2113\u22121) fk\u2192j\n]\u22121 . (5)\nFurthermore, the message from factor node to variable node is given by [20]\nm (\u2113) fn\u2192i (xi) \u221d exp { \u2212 1\n2 ||xi \u2212 v\n(\u2113) fn\u2192i || C\n(\u2113) fn\u2192i\n} , (6)\nwhere C (\u2113\u22121) fk\u2192j and v (\u2113\u22121) fk\u2192j are the message covariance matrix and mean vector received at variable node j at the l\u2212 1 iteration with\n[C (\u2113) fn\u2192i ]\u22121 =ATn,i [ Rn + \u2211\nj\u2208B(fn)\\i\nAn,jC (\u2113) j\u2192fn ATn,j ]\u22121 An,i. (7)\nThe following lemma shown in [20] indicates that setting\nthe initial message covariances [C (0) fn\u2192i ]\u22121 0 for all n, i \u2208 V guarantees [C (\u2113) j\u2192fn ]\u22121 \u227b 0 for l \u2265 1.\nLemma 1. Let the initial messages at factor node fk be in Gaussian function forms with covariance [C (0) fk\u2192j ]\u22121 0 for all k \u2208 V and j \u2208 B(fk). Then [C (\u2113) j\u2192fn ]\u22121 \u227b 0 and [C (\u2113) fk\u2192j ]\u22121 \u227b 0 for all l \u2265 1 with j \u2208 V and fn, fk \u2208 B(j). Furthermore, in this case, all the messages m (\u2113) j\u2192fn (xj) and m (\u2113) fk\u2192j (xi) exist and are in Gaussian form.\nFor this factor graph based approach, according to the message updating procedure (4) and (6), message exchange is only needed between neighboring nodes. For example, the messages transmitted from node n to its neighboring node i are m (\u2113) fn\u2192i (xi) and m (\u2113) n\u2192fi\n(xn). Thus, the message passing scheme given in (1) and (2) automatically conforms with the network topology. Furthermore, if the messages m (\u2113) j\u2192fn (xj) and m (\u2113) fn\u2192i\n(xi) exist for all l (which can be achieved using Lemma 1), the messages are Gaussian, therefore only the correspondingmean vectors and informationmatrices (inverse of covariance matrices) are needed to be exchanged.\nFinally, if the BP messages exist, according to the definition of belief in (3), b (\u2113) BP (xi) at iteration l is computed as [20]\nb (\u2113) BP (xi) = p(xi)\n\u220f\nfn\u2208B(i)\nm (\u2113) fn\u2192i (xi) \u221d N ( xi|\u00b5 (\u2113) i ,P (\u2113) i ) ,\n(8)\nwith P (\u2113) i = [ W\u22121i + \u2211 fn\u2208B(i) [ C (\u2113) fn\u2192i ]\u22121]\u22121 , and \u00b5 (\u2113) i = P (\u2113) i [\u2211 fn\u2208B(i) [ C (\u2113) fn\u2192i ]\u22121 v (\u2113) fn\u2192i ] . The iterative computation terminates when message (4) or message (6) converges to a fixed value or the maximum number of iterations is reached."}, {"heading": "3. CONVERGENCE OF INFORMATION MATRICES", "text": "The challenge of deploying the BP algorithm for large-scale networks is determining whether it will converge. In particular, it is generally known that if the factor graph contains cycles, the BP algorithm may diverge. Thus, determining convergence conditions for the BP algorithm is very important. Sufficient conditions for the convergence of Gaussian BP with scalar variable in loopy graphs are available in [1, 12, 14]. However, they are derived based on pairwise graphs with local functions that only involve two variables. This is in sharp\ncontrast to the model considered in this paper, where the fn involves high-order interactions between vector variables, and thus the convergence results in [1,12,14] cannot be applied to the factor graph based vector-form Gaussian BP.\nDue to the recursively updating property of m (\u2113) j\u2192fn (xj)\nand m (\u2113) fn\u2192i\n(xi) in (4) and (6), the message evolution can be simplified by combining these two kinds of messages into one. By substituting [ C\n(\u2113) j\u2192fn\n]\u22121 in (5) into (7), the updat-\ning of the message covariancematrix inverse, named message information matrix in the following, can be denoted as\n[ C\n(\u2113) fn\u2192i ]\u22121 =ATn,i [ Rn + \u2211\nj\u2208B(fn)\\i\nAn,j [ W\u22121j\n+ \u2211\nfk\u2208B(j)\\fn\n[ C\n(\u2113\u22121) fk\u2192j ]\u22121]\u22121 ATn,j ]\u22121 An,i\n,Fn\u2192i ( { [ C\n(\u2113\u22121) fk\u2192j ]\u22121 }(fk,j)\u2208B\u0303(fn,i) ) ,\nwhere B\u0303(fn, i) = {(fk, j)|j \u2208 B(fn)\\i, fk \u2208 B(j)\\fn}. Observing that C (\u2113) fn\u2192i in (9) is independent of v (\u2113) j\u2192fn and v (\u2113) fn\u2192i in (4) and (6), we can focus on the convergence property of [C (\u2113) fn\u2192i\n]\u22121 alone. To consider the updates of all message information matrices, we introduce the following definitions. Let C(\u2113\u22121) , Bdiag({[C (\u2113\u22121) fn\u2192i\n]\u22121}n\u2208V,i\u2208B(fn)) be a block diagonal matrix with diagonal blocks being the message information matrices in the network at time l \u2212 1 with index arranged in ascending order first on n and then on i. Using the definition of C(\u2113\u22121), the term \u2211\nfk\u2208B(j)\\fn [C (\u2113\u22121) fk\u2192j ]\u22121 in (9)\ncan be written as \u039en,jC (\u2113\u22121) \u039e T n,j , where \u039en,j is for selecting appropriate components from C(\u2113\u22121) to form the summation. Further, define Hn,i = [{An,j}j\u2208B(fn)\\i], \u03a8n,i = Bdiag({W\u22121j }j\u2208B(fn)\\i) andKn,i=Bdiag({\u039en,j}j\u2208B(fn)\\i), all with component blocks arranged with ascending order on j. Then (9) can be written as\n[C (\u2113) fn\u2192i ]\u22121 =ATn,i { Rn +Hn,i[\u03a8n,i +Kn,i(I|B(fn)|\u22121\n\u2297 C(\u2113\u22121))KTn,i] \u22121HTn,i }\u22121 An,i.\n(9)\nNow, we define the functionF , {F1\u2192k, . . . ,Fn\u2192i, . . . , Fn\u2192M} that satisfies C (\u2113) = F(C(\u2113\u22121)). Then, by stacking[\nC (\u2113) fn\u2192i\n]\u22121 on the left side of (9) for all n and i as the block\ndiagonal matrix C(\u2113), we obtain\nC(\u2113) = AT { \u2126+H[\u03a8+K(I\u03d5 \u2297 C (\u2113\u22121))KT ]\u22121HT }\u22121 A,\n, F(C(\u2113\u22121)), (10)\nwhere A, H, \u03a8, and K are block diagonal matrices with block elements An,i, Hn,i, \u03a8n,i, and Kn,i, respectively, arranged in ascending order, first on n and then on i (i.e., the same order as [C (\u2113) fn\u2192i ]\u22121 in C(\u2113)). Furthermore, \u03d5 =\n\u2211M n=1 |B(fn)|(|B(fn)| \u2212 1) and \u2126 is a block diagonal matrix with diagonal blocks I|B(fn)| \u2297 Rn with ascending order on n. We first present properties of the updating operator F(\u00b7), with the proof given in [20].\nProperty 1. The updating operator F(\u00b7) satisfies the following properties:\nP 1.1: F(C(\u2113)) F(C(\u2113\u22121)), if C(\u2113) C(\u2113\u22121) 0. P 1.2: \u03b1F(C(\u2113)) \u227b F(\u03b1C(\u2113)) andF(\u03b1\u22121C(\u2113)) \u227b \u03b1\u22121F(C(\u2113)), if C(\u2113) \u227b 0 and \u03b1 > 1. P 1.3: DefineU , AT\u2126\u22121A andL , AT [ \u2126+H\u03a8\u22121HT ]\u22121 A.\nWith arbitrary C(0) 0, F(C(\u2113)) is bounded by U F(C(\u2113)) L \u227b 0 for l \u2265 1. In this paper, X Y (X \u227b Y) means that X\u2212 Y is positive semidefinite (definite). Based on the above properties of F(\u00b7), we can establish the convergence property for the information matrices. The following theorem establishes that there exists a unique fixed point for the mappingF(\u00b7). The proof is omitted due to space restrictions; it is provided in [20].\nTheorem 1. With C(0) 0, there exists a unique positive definite fixed point for the mapping F(\u00b7).\nLemma 1 states that with arbitrary positive semidefinite (p.s.d.) initial message information matrices, the message information matrices will be kept as positive definite (p.d.) at every iteration. On the other hand, Theorem 1 indicates that there exists a unique fixed point for the mapping F . Next, we will show that with arbitrary initial value C(0) 0, C(\u2113) converges to a unique p.d. matrix.\nTheorem 2. The matrix sequence {C(\u2113)}l=0,1,... defined by (10) converges to a unique positive definite matrix for any initial covariance matrix C(0) 0.\nProof. With arbitrary initial value C(0) 0, following P 1.3, we have U C(1) L \u227b 0. On the other hand, according to Theorem 1, (10) has a unique fixed point C\u2217 \u227b 0. Notice that we can always choose a scalar \u03b1 > 1 such that\n\u03b1C\u2217 C(1) L. (11)\nApplying F(\u00b7) to (11) l times, and using P 1.1, we have\nF l(\u03b1C\u2217) F l+1(C(0)) F l(L), (12)\nwhere F l(X) denotes applying F on X for l times. We start from the left inequality in (12). Following the fixed point definition, \u03b1C\u2217 = \u03b1F(C\u2217). Then, according to P 1.2, \u03b1C\u2217 \u227b F(\u03b1C\u2217). Applying F again gives F(\u03b1C\u2217) \u227b F2(\u03b1C\u2217). Applying F(\u00b7) repeatedly, we can obtain F2(\u03b1C\u2217) \u227b F3(\u03b1C\u2217) \u227b F4(\u03b1C\u2217), etc. Thus F l(\u03b1C\u2217) is a decreasing sequence with respect to the partial order induced by the cone of p.s.d. matrices as l increases. Furthermore,\nsince F(\u00b7) is bounded below by L, F l(\u03b1C\u2217) is convergent. Finally, since there exists only one fixed point for F(\u00b7), liml\u2192\u221e F l(\u03b1C\n\u2217) = C\u2217. On the other hand, for the right hand side of (12), as F(\u00b7) L, we have F(L) L. Applying F repeatedly gives F2(L) F(L), F3(L) F2(L), etc. So, F l(L) is an increasing sequence (with respect to the partial order induced by the cone of p.s.d. matrices). Since F(\u00b7) is upper bounded by U, F l(L) is a convergent sequence. Again due to the unique fixed point, we have liml\u2192\u221e F l(L) = C \u2217. Finally, taking the limit with respect to l on (12) we have liml\u2192\u221e F l(C (0)) = C\u2217, for arbitrary initial C(0) 0.\nAccording to Theorem 2, the covariance matrix C (\u2113) fn\u2192i converges if all initial information matrices are p.s.d., i.e.,[ C\n(0) fn\u2192i ]\u22121 0 for all i \u2208 V and fn \u2208 B(i). Notice that, for the pairwise model, the information matrix does not necessarily converge for all initial non-negative value (in the scalar variable case) as shown in [12,13]. Moreover, due to the computation of [C (\u2113) fn\u2192i\n]\u22121 being independent of the local observations yn, as long as the network topology does not change, the converged value [C\u2217fn\u2192i] \u22121 can be precomputed offline and stored at each node, and there is no need to re-compute [C\u2217fn\u2192i] \u22121 even if yn varies.\nAnother fundamental question is how fast the convergence is, and this is the focus of the discussion below. Since the convergence of a dynamic system is often studied with the part metric [21], in the following, we start by introducing the part metric.\nDefinition 1. Part (Birkhoff) Metric [21]: For arbitrary matrices X and Y with the same dimension, if there exists \u03b1 \u2265 1 such that \u03b1X Y \u03b1\u22121X, X and Y are called the parts, and d(X,Y) , inf{log\u03b1 : \u03b1X Y \u03b1\u22121X, \u03b1 \u2265 1} defines a metric called the part metric.\nNext, we will show that {C(\u2113)}l=1,.. converges at a geometric rate with respect to the part metric in C, which is constructed as\nC = {C(\u2113)|U C(\u2113) C\u2217+\u01ebI}\u222a{C(\u2113)|C\u2217\u2212\u01ebI C(\u2113) L},\nwhere \u01eb > 0 is a scalar and can be arbitrarily small.\nTheorem 3. With the initial covariance matrix set to be an arbitrary p.s.d. matrix, i.e., [C (0) fn\u2192i ]\u22121 0, the sequence {C(\u2113)}l=0,1,... converges at a geometric rate with respect to the part metric in C.\nProof. Consider two matrices C(\u2113) \u2208 C, and C\u2217 6\u2208 C, according to Definition 1, we have d(C(\u2113),C\u2217) , inf{log\u03b1 : \u03b1C(\u2113) C\u2217 \u03b1\u22121C(\u2113)}. Since d(C(\u2113),C\u2217) is the smallest number satisfying \u03b1C(\u2113) C\u2217 \u03b1\u22121C(\u2113), this is equivalent to\nexp{d(C(\u2113),C\u2217)}C(\u2113) C\u2217 exp{\u2212d(C(\u2113),C\u2217)}C(\u2113). (13)\nApplying P 1.1 to (13), we have exp{d(C(\u2113),C\u2217)}F(C(\u2113) F(C\u2217) exp{\u2212d(C(\u2113),C\u2217)}F(C(\u2113)). Then applying P 1.2 and considering that exp{d(C(\u2113),C\u2217)} > 1 and exp{\u2212d(C(\u2113),C\u2217)} < 1, we obtain\nexp{d(C(\u2113),C\u2217)}F(C(\u2113)) \u227b F(C\u2217) \u227b exp{\u2212d(C(\u2113),C\u2217)}F(C(\u2113)). (14)\nNotice that, for arbitrary p.d. matricesX andY, ifX\u2212kY \u227b 0 then, by definition that, we have xTXx \u2212 kxTYx > 0. Then there must exist o > 0 that is small enough such that xTXx\u2212 (k + o)xTYx > 0 or equivalently X \u227b (k + o)Y. Thus, as exp (\u00b7) is a continuous function, there must exist some\u25b3d > 0 such that\nexp{\u2212\u25b3d + d(C(\u2113),C\u2217)}F(C(\u2113)) \u227b F(C\u2217) \u227b exp{\u25b3d\u2212 d(C(\u2113),C\u2217)}F(C(\u2113)). (15)\nNow, using the definition of part metric, (15) is equivalent to\n\u2212\u25b3d + d(C(\u2113),C\u2217) \u2265 d(F(C(\u2113)),F(C\u2217)). (16)\nHence, we obtain d(F(C(\u2113)),F(C\u2217)) < d(C(\u2113),C\u2217). This result holds for anyC(\u2113) \u2208 C, d(F(C(\u2113)),F(C\u2217)) < cd(C(\u2113),C\u2217), where c = supCl\u2208C d(F(Cl),F(C\u2217)) d(Cl,C\u2217) < 1. Consequently, we have d(C(\u2113),C\u2217) < cld(C(0),C\u2217). Thus the sequence {C(\u2113)}l=1,... converges at a geometric rate with respect to the part metric.\nIt is useful to have an estimate of the convergence rate of C(\u2113) in terms of the more standard inducedmatrix norms. According to [22, Lemma 2.3], the convergence rate of ||C(0) \u2212 C\u2217|| is dominated by that of d(C(0),C\u2217), where || \u00b7 || is a monotone norm defined on the p.s.d. cone, with || \u00b7 ||2 and || \u00b7 ||F being examples of such matrix norms [23, 2.2-10]. More specifically,\n(2 exp{d(C(\u2113),C\u2217)} \u2212 exp{\u2212d(C(\u2113),C\u2217)} \u2212 1)\n\u00d7min{||C(\u2113)||, ||C\u2217||} \u2265 ||C(\u2113) \u2212 C\u2217||. (17)\nThe physical meaning of Theorem 3 is that the sequence {C(\u2113)}l=1,... converges at a geometric rate (the distance between C(\u2113) and C\u2217 decreases exponentially) before C(\u2113) enters C\u2217\u2019s neighborhood, which can be chosen arbitrarily small."}, {"heading": "4. CONCLUSION", "text": "This paper has established the convergence of the exchanged message information matrix of Gaussian belief propagation (BP) for distributed estimation. We have shown analytically that, with arbitrary positive semidefinite initial value, the information matrix converges to a unique positive definite matrix at geometric rate. The convergence guaranteed property and fast convergence rate of the message information matrix pave the way for the convergence analysis of the Gaussian BP message mean vector."}, {"heading": "5. REFERENCES", "text": "[1] Y. Weiss and W. T. Freeman, \u201cCorrectness of belief\npropagation in Gaussian graphical models of arbitrary topology,\u201d Neural Computation, vol. 13, no. 10, pp. 2173\u20132200, Mar. 2001.\n[2] Y. Hu, A. Kuh, T. Yang, and A. Kavcic, \u201cA belief propa-\ngation based power distribution system state estimator,\u201d IEEE Comput. Intell. Mag., vol. 6, no. 3, pp. 36\u201346, Aug 2011.\n[3] B. L. Ng, J. Evans, S. Hanly, and D. Aktas, \u201cDistributed\ndownlink beamforming with cooperative base stations,\u201d IEEE Trans. Inf. Theory, vol. 54, no. 12, pp. 5491\u20135499, Dec 2008.\n[4] J. Du and Y.-C. Wu, \u201cDistributed clock skew and offset\nestimation in wireless sensor networks: Asynchronous algorithm and convergence analysis,\u201d IEEE Trans. Wireless Commun., vol. 12, no. 11, pp. 5908\u20135917, Nov 2013.\n[5] \u2014\u2014, \u201cNetwork-wide distributed carrier frequency off-\nsets estimation and compensation via belief propagation,\u201d IEEE Trans. Signal Process., vol. 61, no. 23, pp. 5868\u20135877, 2013.\n[6] O. Shental, P. Siegel, J. Wolf, D. Bickson, and D. Dolev,\n\u201cGaussian belief propagation solver for systems of linear equations,\u201d in 2008 IEEE International Symposium on Information Theory (ISIT 2008), July 2008, pp. 1863\u20131867.\n[7] G. Zhang, W. Xu, and Y. Wang, \u201cFast distributed\nrate control algorithm with QoS support in ad-hoc,\u201d in 2010 IEEE Global Telecommunications Conference (GLOBECOM 2010).\n[8] B. J. Frey, \u201cLocal probability propagation for factor\nanalysis,\u201d in Neural Information Processing Systems (NIPS), Dec 1999, pp. 442\u2013448.\n[9] X. Tan and J. Li, \u201cComputationally efficient sparse\nBayesian learning via belief propagation,\u201d IEEE Trans. Signal Process., vol. 58, no. 4, pp. 2010\u20132021, April 2010.\n[10] D. Bickson and D. Malkhi, \u201cA unifying framework for\nrating users and data items in peer-to-peer and social networks,\u201d Peer-to-Peer Networking and Applications (PPNA) Journal, vol. 1, no. 2, pp. 93\u2013103, 2008.\n[11] Y. Weiss and W. Freeman, \u201cOn the optimality of solu-\ntions of the max-product belief-propagation algorithm in arbitrary graphs,\u201d IEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 736\u2013744, Feb. 2001.\n[12] D. M. Malioutov, J. K. Johnson, and A. S. Willsky,\n\u201cWalk-sums and belief propagation in Gaussian graphical models,\u201d Journal of Machine Learning Research, vol. 7, no. 2, pp. 2031\u20132064, Feb. 2006.\n[13] C. C. Moallemi and B. V. Roy, \u201cConvergence of min-\nsum message passing for quadratic optimization,\u201d IEEE Trans. Inf. Theory, vol. 55, no. 5, pp. 2413\u20132423, 2009.\n[14] Q. Su and Y.-C. Wu, \u201cOn convergence conditions of\nGaussian belief propagation,\u201d IEEE Trans. Signal Process., vol. 63, no. 5, pp. 1144\u20131155, March 2015.\n[15] F. Lehmann, \u201cIterative mitigation of intercell interfer-\nence in cellular networks based on Gaussian belief propagation,\u201d IEEE Trans. Veh. Technol., vol. 61, no. 6, pp. 2544\u20132558, July 2012.\n[16] S. Kar and J. M. F. Moura, \u201cConsensus+innovations dis-\ntributed inference over networks: cooperation and sensing in networked systems,\u201d IEEE Signal Process. Mag., vol. 30, no. 3, pp. 99\u2013109, 2013.\n[17] S. Kar, J. M. F. Moura, and H. Poor, \u201cDistributed linear\nparameter estimation: asymptotically efficient adaptive strategies,\u201d SIAM Journal on Control and Optimization, vol. 51, no. 3, pp. 2200\u20132229, 2013.\n[18] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS strate-\ngies for distributed estimation,\u201d IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1035\u20131048, 2010.\n[19] S. Kar, \u201cLarge scale networked dynamical systems: Dis-\ntributed inference,\u201d Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, PA, Department of Electrical and Computer Engineering, June 2010.\n[20] J. Du, S. Ma, Y.-C. Wu, S. Kar, and J. M. F.\nMoura, \u201cConvergence analysis of distributed inference with vector-valued Gaussian belief propagation,\u201d submitted to Journal of Machine Learning Research [Preprint Available]: https://users.ece.cmu.edu/\u223csoummyak/GBP convergence.\n[21] I. Chueshov, Monotone Random Systems Theory and\nApplications. New York: Springer, 2002.\n[22] U. Krause and R. Nussbaum, \u201cA limit set trichotomy\nfor self-mappings of normal cones in Banach spaces,\u201d Nonlinear Analysis, Theory, Methods&Applications, vol. 20, no. 7, pp. 855\u2013870, 1993.\n[23] P. G. Ciarlet, Introduction to Numerical Linear Algebra\nand Optimisation. Cambridge University Press, 1989."}], "references": [{"title": "Correctness of belief propagation in Gaussian graphical models of arbitrary topology", "author": ["Y. Weiss", "W.T. Freeman"], "venue": "Neural Computation, vol. 13, no. 10, pp. 2173\u20132200, Mar. 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A belief propagation based power distribution system state estimator", "author": ["Y. Hu", "A. Kuh", "T. Yang", "A. Kavcic"], "venue": "IEEE Comput. Intell. Mag., vol. 6, no. 3, pp. 36\u201346, Aug 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed downlink beamforming with cooperative base stations", "author": ["B.L. Ng", "J. Evans", "S. Hanly", "D. Aktas"], "venue": "IEEE Trans. Inf. Theory, vol. 54, no. 12, pp. 5491\u20135499, Dec 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed clock skew and offset estimation in wireless sensor networks: Asynchronous algorithm and convergence analysis", "author": ["J. Du", "Y.-C. Wu"], "venue": "IEEE Trans. Wireless Commun., vol. 12, no. 11, pp. 5908\u20135917, Nov 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Network-wide distributed carrier frequency offsets estimation and compensation via belief propagation", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 23, pp. 5868\u20135877, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian belief propagation solver for systems of linear equations", "author": ["O. Shental", "P. Siegel", "J. Wolf", "D. Bickson", "D. Dolev"], "venue": "2008 IEEE International Symposium on Information Theory (ISIT 2008), July 2008, pp. 1863\u20131867.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast distributed rate control algorithm with QoS support in ad-hoc", "author": ["G. Zhang", "W. Xu", "Y. Wang"], "venue": "2010 IEEE Global Telecommunications Conference (GLOBECOM 2010).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Local probability propagation for factor analysis", "author": ["B.J. Frey"], "venue": "Neural Information Processing Systems (NIPS), Dec 1999, pp. 442\u2013448.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Computationally efficient sparse Bayesian learning via belief propagation", "author": ["X. Tan", "J. Li"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 4, pp. 2010\u20132021, April 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A unifying framework for rating users and data items in peer-to-peer and social networks", "author": ["D. Bickson", "D. Malkhi"], "venue": "Peer-to-Peer Networking and Applications (PPNA) Journal, vol. 1, no. 2, pp. 93\u2013103, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "On the optimality of solutions of the max-product belief-propagation algorithm in arbitrary graphs", "author": ["Y. Weiss", "W. Freeman"], "venue": "IEEE Trans. Inf. Theory, vol. 47, no. 2, pp. 736\u2013744, Feb. 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Walk-sums and belief propagation in Gaussian graphical models", "author": ["D.M. Malioutov", "J.K. Johnson", "A.S. Willsky"], "venue": "Journal of Machine Learning Research, vol. 7, no. 2, pp. 2031\u20132064, Feb. 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence of minsum message passing for quadratic optimization", "author": ["C.C. Moallemi", "B.V. Roy"], "venue": "IEEE Trans. Inf. Theory, vol. 55, no. 5, pp. 2413\u20132423, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On convergence conditions of Gaussian belief propagation", "author": ["Q. Su", "Y.-C. Wu"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 5, pp. 1144\u20131155, March 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Iterative mitigation of intercell interference in cellular networks based on Gaussian belief propagation", "author": ["F. Lehmann"], "venue": "IEEE Trans. Veh. Technol., vol. 61, no. 6, pp. 2544\u20132558, July 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Consensus+innovations distributed inference over networks: cooperation and sensing in networked systems", "author": ["S. Kar", "J.M.F. Moura"], "venue": "IEEE Signal Process. Mag., vol. 30, no. 3, pp. 99\u2013109, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed linear parameter estimation: asymptotically efficient adaptive strategies", "author": ["S. Kar", "J.M.F. Moura", "H. Poor"], "venue": "SIAM Journal on Control and Optimization, vol. 51, no. 3, pp. 2200\u20132229, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Diffusion LMS strategies for distributed estimation", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1035\u20131048, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale networked dynamical systems: Distributed inference", "author": ["S. Kar"], "venue": "Ph.D. dissertation, Carnegie Mellon University, Pittsburgh, PA, Department of Electrical and Computer Engineering, June 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence analysis of distributed inference with vector-valued Gaussian belief propagation", "author": ["J. Du", "S. Ma", "Y.-C. Wu", "S. Kar", "J.M.F. Moura"], "venue": "submitted to Journal of Machine Learning Research [Preprint Available]: https://users.ece.cmu.edu/\u223csoummyak/GBP convergence.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 0}, {"title": "Monotone Random Systems Theory and Applications", "author": ["I. Chueshov"], "venue": "New York: Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "A limit set trichotomy for self-mappings of normal cones in Banach spaces", "author": ["U. Krause", "R. Nussbaum"], "venue": "Nonlinear Analysis, Theory, Methods&Applications, vol. 20, no. 7, pp. 855\u2013870, 1993.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Introduction to Numerical Linear Algebra and Optimisation", "author": ["P.G. Ciarlet"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 304, "endOffset": 307}, {"referenceID": 2, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 347, "endOffset": 350}, {"referenceID": 3, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 371, "endOffset": 377}, {"referenceID": 4, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 371, "endOffset": 377}, {"referenceID": 5, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 457, "endOffset": 460}, {"referenceID": 6, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 506, "endOffset": 509}, {"referenceID": 7, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 535, "endOffset": 538}, {"referenceID": 8, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 565, "endOffset": 568}, {"referenceID": 9, "context": "In large-scale linear parameter estimation with Gaussian measurements, Gaussian Belief Propagation (BP) [1] provides an efficient distributed way to compute the marginal distribution of the unknown variables, and it has been adopted in a variety of topics ranging from distributed power state estimation [2] in smart grid, distributed beamforming [3] and synchronization [4, 5] in wireless communication networks, fast solver for system of linear equations [6], distributed rate control in ad-hoc networks [7], factor analyzer network [8], sparse Bayesian learning [9], to peer-to-peer rating in social networks [10].", "startOffset": 612, "endOffset": 616}, {"referenceID": 10, "context": "It has been shown that Gaussian BP computes the optimal centralized estimator if it converges [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "Sufficient convergence conditions for Gaussian BP have been developed in [1,12\u201314] when the underlying Gaussian distribution is expressed in terms of pairwise connections between scalar variables (also known as Markov random field (MRF)).", "startOffset": 73, "endOffset": 82}, {"referenceID": 11, "context": "Sufficient convergence conditions for Gaussian BP have been developed in [1,12\u201314] when the underlying Gaussian distribution is expressed in terms of pairwise connections between scalar variables (also known as Markov random field (MRF)).", "startOffset": 73, "endOffset": 82}, {"referenceID": 12, "context": "Sufficient convergence conditions for Gaussian BP have been developed in [1,12\u201314] when the underlying Gaussian distribution is expressed in terms of pairwise connections between scalar variables (also known as Markov random field (MRF)).", "startOffset": 73, "endOffset": 82}, {"referenceID": 13, "context": "Sufficient convergence conditions for Gaussian BP have been developed in [1,12\u201314] when the underlying Gaussian distribution is expressed in terms of pairwise connections between scalar variables (also known as Markov random field (MRF)).", "startOffset": 73, "endOffset": 82}, {"referenceID": 1, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 2, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 3, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 4, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 5, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 6, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 7, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 8, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 14, "context": "However, the iterative equations for Gaussian BP on MRFs are different from that for distributed estimation problems such as in [2\u20139,15], where high order factors (non-pairwise) and vector-valued variables are involved.", "startOffset": 128, "endOffset": 136}, {"referenceID": 15, "context": "Note that distributed estimation based on the consensus+ innovations philosophy proposed in [16, 17] (see also the related family of diffusion algorithms [18]) converges to the optimal centralized estimator under the assumption of global observability of the (aggregate) sensing model and connectivity of the inter-agent communication network.", "startOffset": 92, "endOffset": 100}, {"referenceID": 16, "context": "Note that distributed estimation based on the consensus+ innovations philosophy proposed in [16, 17] (see also the related family of diffusion algorithms [18]) converges to the optimal centralized estimator under the assumption of global observability of the (aggregate) sensing model and connectivity of the inter-agent communication network.", "startOffset": 92, "endOffset": 100}, {"referenceID": 17, "context": "Note that distributed estimation based on the consensus+ innovations philosophy proposed in [16, 17] (see also the related family of diffusion algorithms [18]) converges to the optimal centralized estimator under the assumption of global observability of the (aggregate) sensing model and connectivity of the inter-agent communication network.", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "The results in [16, 17] imply that the unknown variables x can be reconstructed completely at each node in the network.", "startOffset": 15, "endOffset": 23}, {"referenceID": 16, "context": "The results in [16, 17] imply that the unknown variables x can be reconstructed completely at each node in the network.", "startOffset": 15, "endOffset": 23}, {"referenceID": 19, "context": "The Gaussian BP algorithm can be derived over the corresponding factor graph to compute the estimate of xn for all n \u2208 V [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "It can be shown [20] that the general expression for the message from variable node to factor node is", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Furthermore, the message from factor node to variable node is given by [20]", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "The following lemma shown in [20] indicates that setting", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "tion of belief in (3), b (l) BP (xi) at iteration l is computed as [20]", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "Sufficient conditions for the convergence of Gaussian BP with scalar variable in loopy graphs are available in [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 11, "context": "Sufficient conditions for the convergence of Gaussian BP with scalar variable in loopy graphs are available in [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 13, "context": "Sufficient conditions for the convergence of Gaussian BP with scalar variable in loopy graphs are available in [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 0, "context": "contrast to the model considered in this paper, where the fn involves high-order interactions between vector variables, and thus the convergence results in [1,12,14] cannot be applied to the factor graph based vector-form Gaussian BP.", "startOffset": 156, "endOffset": 165}, {"referenceID": 11, "context": "contrast to the model considered in this paper, where the fn involves high-order interactions between vector variables, and thus the convergence results in [1,12,14] cannot be applied to the factor graph based vector-form Gaussian BP.", "startOffset": 156, "endOffset": 165}, {"referenceID": 13, "context": "contrast to the model considered in this paper, where the fn involves high-order interactions between vector variables, and thus the convergence results in [1,12,14] cannot be applied to the factor graph based vector-form Gaussian BP.", "startOffset": 156, "endOffset": 165}, {"referenceID": 19, "context": "We first present properties of the updating operator F(\u00b7), with the proof given in [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "The proof is omitted due to space restrictions; it is provided in [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "Notice that, for the pairwise model, the information matrix does not necessarily converge for all initial non-negative value (in the scalar variable case) as shown in [12,13].", "startOffset": 167, "endOffset": 174}, {"referenceID": 12, "context": "Notice that, for the pairwise model, the information matrix does not necessarily converge for all initial non-negative value (in the scalar variable case) as shown in [12,13].", "startOffset": 167, "endOffset": 174}, {"referenceID": 20, "context": "Since the convergence of a dynamic system is often studied with the part metric [21], in the following, we start by introducing the part metric.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "Part (Birkhoff) Metric [21]: For arbitrary matrices X and Y with the same dimension, if there exists \u03b1 \u2265 1 such that \u03b1X Y \u03b1X, X and Y are called the parts, and d(X,Y) , inf{log\u03b1 : \u03b1X Y \u03b1X, \u03b1 \u2265 1} defines a metric called the part metric.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "Gaussian belief propagation (BP) has been widely used for distributed estimation in large-scale networks such as the smart grid, communication networks, and social networks, where local measurements/observations are scattered over a wide geographical area. However, the convergence of Gaussian BP is still an open issue. In this paper, we consider the convergence of Gaussian BP, focusing in particular on the convergence of the information matrix. We show analytically that the exchanged message information matrix converges for arbitrary positive semidefinite initial value, and its distance to the unique positive definite limit matrix decreases exponentially fast.", "creator": "LaTeX with hyperref package"}}}