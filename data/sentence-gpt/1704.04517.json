{"id": "1704.04517", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "ShapeWorld - A new test methodology for multimodal language understanding", "abstract": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter's specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. This is achieved in a practical way, using a simple interface that allows a single experiment to be performed within the context of a typical training task.\n\n\n\n\n\nAs the method described above, the problem of deep learning requires a very simple interface to describe neural networks that are capable of generating complex neural networks. These neural networks are capable of learning specific neural networks, like neural networks. These neural networks are built on top of the functional networks described in the previous paper. Using complex representations of a neural network, the neural network may be able to learn the concept of neural networks without a neural network. However, we have to deal with a problem in which neural networks do not represent the basic model of the human brain, because of its lack of information. Thus, when the neural networks are implemented in neural networks, it will be difficult to represent the neural networks in general in the context of specific neural networks. This problem is solved when we combine neural networks with a single machine. In the case of neural networks, a task is defined by the state of the network. Therefore, when the neural networks have been implemented, they can be defined by the state of the network. For example, the neural network can be a network that has a state of its own for the entire time, so the state of the network is expressed by the function of the state of the network. Thus, when the neural networks have been implemented, the state of the network may be expressed by the function of the state of the network.\nTo summarize, neural networks with multiple networks can be implemented in different ways in order to be able to perform the same basic neural networks, but each of these neural networks will have multiple tasks that require no training. Furthermore, they can be implemented by a single machine, as in previous work.\nUsing the concept of neural networks with multiple models, the neural network may be able to learn the concept of neural networks using a single machine. Although the functional network is not yet fully developed, there is currently very little use for it in human cognition. This limitation affects the ability of the neural networks to perform complex neural", "histories": [["v1", "Fri, 14 Apr 2017 19:01:51 GMT  (264kb,D)", "http://arxiv.org/abs/1704.04517v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["alexander kuhnle", "ann copestake"], "accepted": false, "id": "1704.04517"}, "pdf": {"name": "1704.04517.pdf", "metadata": {"source": "CRF", "title": "SHAPEWORLD: A new test methodology for multimodal language understanding", "authors": ["Alexander Kuhnle", "Ann Copestake"], "emails": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Deep learning methods have had a major impact on research in natural language processing and raised performance substantially in many of the standard evaluations. Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (VQA) (Antol et al., 2015) can now be tackled with great success. Such systems seem to solve the problems entirely on a sub-symbolic level, based only on raw image (and text) input, whereas previous approaches required a hand-crafted combination of various higher-level components.\nThere is, however, concern about how deep neural networks learn to solve such tasks. Investigations for image recognition (Szegedy et al.,\n2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al., 2015). Deep networks for language tasks may exhibit similarly odd behavior (Sproat and Jaitly, 2016; Arthur et al., 2016). Moreover, it was recently found that datasets \u2013 for instance in VQA \u2013 contain various unexpected biases and peculiarities, which systems can exploit to answer complex questions, sometimes even without looking at the image at all (Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016). Such results cast doubt on whether deep learning systems actually acquire appropriate generalizations. However, given the recursive nature of language and the potentially enormous problem space of VQA and similar tasks, acquiring the ability for reliable generalization will eventually be essential.\nA more theoretical issue is the ability of network architectures, in principle, to learn certain classes of structure. For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001). However, the formal experiments that have been done along such lines are limited, particularly in the multimodal domain of vision and language. While recent work indicates that the information encoded in image embeddings might be rich enough for good captioning results, it is an open question whether current architectures are able, in principle, to combine visual information effectively to handle the full range of linguistic constructions.\nThis paper introduces a new test methodology for multimodal deep learning models. SHAPEWORLD is a framework for specifying VQA-style datasets. An instance here consists of an image and a caption, and the evaluated model has to decide about their agreement, hence a form of yes/no\nar X\niv :1\n70 4.\n04 51\n7v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nquestion answering which we call image caption agreement (ICA). Figure 1 illustrates the task and nature of the data with some example instances.\nA SHAPEWORLD dataset differs from standard VQA evaluation datasets in three main ways. Firstly, a SHAPEWORLD dataset defines a process for generating artificial data consisting of abstract colored shapes, which is randomly sampled during training/testing according to constraints specified by the experimenter. Secondly, the evaluation focus is on linguistic understanding capabilities of the type investigated by formal semantics. The visual complexity and open-class vocabulary size is reduced to a minimum, while potentially allowing indefinitely complex syntactic constructions. Finally, the distribution of the evaluation data is deliberately kept different from the training distribution. Controlled data generation enables us to introduce previously unseen instance configurations during evaluation, which require the system to recombine learned concepts to be able to understand these novel instances (see figure 1) \u2013 hence a form of zero-shot learning. We think of the SHAPEWORLD tasks as unit-testing multimodal systems for specific linguistic generalization capabilities, in a similar way to the bAbI tasks (Weston et al., 2015) for text-only understanding.\nWe also present results for various VQA models on four SHAPEWORLD datasets targeting different multimodal language understanding abilities. The artificial data allows for a detailed analysis of the models\u2019 strengths and weaknesses, and reveals unexpected shortcomings. As such, it offers a significantly different and interesting resource to complement standard evaluation practice. By exposing problematic instance patterns where these systems fail (e.g. spatial relations), and by providing a configurable, extensible testbed for systematic, detailed and comparable evaluation, we hope to stimulate progress in the field."}, {"heading": "2 Related work", "text": "With the increasing popularity of deep learning approaches, artificial data of various kinds is again seen as a valuable tool in experimentation. Recently, the simulation paradigm has been argued to be a promising driver for artificial intelligence research (Kiela et al., 2016). Various platforms following this paradigm have been released, mostly aimed at reinforcement learning: the Arcade Learning Environment / Atari 2600 games (Bellemare et al., 2013), OpenAI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al., 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular. An important advantage of simulated data is its infinite availability, particularly in light of the need of many deep learning models for huge amounts of data. Automatically generating data greatly reduces the cost, time and human effort. Moreover, it allows researchers to focus on specific problem situations, isolated from a noisy and complex real-world environment.\nWhen focusing on language tasks, the simulation paradigm faces the problem that interesting language generation is a difficult task in its own right, and that the difficulty increases with the complexity of the underlying world. The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it. A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game. The MazeBase game environment (Sukhbaatar et al., 2015) uses language as a mean to represent the game world. However, the descriptions are in an abstract, formulaic format, and the focus of the simulation is much more on the planning than the language component. The long-term research proposal of Mikolov et al. (2015) also simulates a world where\nan agent learns to solve tasks by communication with a teacher module. At least for a start, this module is supposed to be scripted to automatically generate appropriate responses, given its internal knowledge of the world state.\nAutomatically generated data is common for tasks specifically focusing on the ability to efficiently process data of a certain formal structure. Here, data is deliberately stripped of any realworld connection to create an abstract capability check. Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al., 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others. This kind of task is particularly common for neural network models (see, for instance, Bengio et al. (1994) more than twenty years ago). The reason for interest in abstract capability checks is that the learning process and decisions of deep networks are more difficult to interpret than shallower machine learning methods. Bowman et al. (2015) and Sorodoc et al. (2016) are more similar to our work in focusing on specific linguistic aspects. Both generate artificial data automatically based on abstract models for tasks targeting logical semantics and quantifiers, respectively.\nThe multimodal tasks of image captioning and VQA are closely related to our evaluation goal, but usually consist of \u201crepurposed\u201d real-world photos and human-written descriptions.1 However, there have been experiments in which parts of the data are artificial and/or generated automatically, for instance, automatic question generation from annotation (Ren et al., 2015) or systematic modification of captions (Hodosh and Hockenmaier, 2016). Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al., 2016). Most similar to the SHAPEWORLD framework is the CLEVR dataset (Johnson et al., 2017). It contains images of rendered abstract 3-dimensional scenes and complex questions generated from a variety of templates. As with our work, they propose that their artificial dataset complements evaluation on real-world VQA datasets.\nOur own work is based on automatically generated, fully artificial data. This data is not specif-\n1Although we contrast such \u201creal-world\u201d data with artificial simulations, it should be clear that this is very unlike the visual experience of an entity situated in the real world.\nically designed to address only a single structural problem, but is a testbed able to cover a whole range of linguistic phenomena. In fact, our generation system closely resembles classical work in formal semantics, where a statement corresponds to a logical expression which can be evaluated against an abstract world model (Montague, 1970). We utilize semantic representations based on Minimal Recursion Semantics (Copestake et al., 2005) and broad-coverage, grammarbased realization driven by the English Resource Grammar (Flickinger, 2000) to make the internal world model compatible with language. However, while SHAPEWORLD uses these abstract representations internally, the external representation presented to the system under evaluation does not involve any abstract formalization of visual and textual input. It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases or hidden correlations, which can obfuscate results when using real-world images and text (Goyal et al., 2016; Agrawal et al., 2016)."}, {"heading": "3 The SHAPEWORLD framework", "text": "The SHAPEWORLD framework2 is based on microworlds \u2013 small and self-contained artificial scenarios \u2013 which guide the data creation process. The SHAPEWORLD microworlds simply consist of colored shapes. This closed-world domain allows for exhaustive coverage of the space of possible microworlds and associated captions. The vocabulary used has an emphasis on closed-class words \u2013 the open-class vocabulary is currently far less than 100 words. In the following we explain the details of the data generation process inside the SHAPEWORLD framework. A schematic illustration of the process is shown in figure 2."}, {"heading": "3.1 Image caption agreement task", "text": "In this paper we focus on the task of image caption agreement (ICA). The system to be evalu-\n2 The SHAPEWORLD code is written in Python 3 and is available on GitHub (https://github.com/ AlexKuhnle/ShapeWorld). The generated data is returned as NumPy arrays, so that it is possible to integrate it into Python-based deep learning projects using common frameworks like TensorFlow, Theano, etc. In our experiments, we use TensorFlow and we provide the models in this paper as part of the package. For the internal DMRS-based caption generation, the Python package pydmrs (Copestake et al., 2016), as well as a reduced version of the English Resource Grammar (Flickinger, 2000) and of Packard\u2019s Answer Constraint Engine (http://sweaglesw.org/ linguistics/ace/) is included.\nated is presented with an image and a natural language caption and has to decide whether they are consistent with each other.\nCompared to the classic image captioning task, ICA emphasizes the understanding rather than the synthesis part of language use. We therefore avoid the problem of evaluating the appropriateness of a caption. The setup allows us to control the content of both modalities and consequently force a system to cope with difficult types of captions while obtaining a clear indicator of successful understanding. Although very similar to the VQA setup (i.e., yes/no questions), it neither requires the evaluated model to generate answers nor to rephrase the problem to fit it into a classification task of some sort \u2013 for instance, over the 1000/3000 most common answers, as is common practice recently (Lu et al., 2016; Fukui et al., 2016). ICA most closely corresponds to the work of Jabri et al. (2016), who present VQA as a binary classification of image-question-answer triples.\nOne further motivation for the task is that human performance could be measured using the same setup. We would expect close-to-perfect human performance on the tasks described here, assuming time is not tightly constrained. Interesting comparisons are potentially possible where human performance depends on presentation: e.g., quantifiers such as most (Pietroski et al., 2009). However, we will not discuss this further in the current paper."}, {"heading": "3.2 World and image generation", "text": "At the core of each microworld instance lies an abstract world model. The internal representation\nof a microworld is simply a list of entities, given as records containing their primary attributes, such as position, shape, color, which are considered to be high-level semantic aspects reflected in captions. In addition, an entity has secondary attributes and methods which control, for instance, details of visual appearance, visual noise infusion, or the collision-free placement of entities. Importantly, all these ways of infusing noise can be controlled as well, which is useful particularly since noise is often seen as important for successful training of deep models.\nThe generator module automatically generates a world model by randomly sampling all these attributes from a set of available values. Both these values and other aspects of the generation process can be specified and adjusted appropriately for each dataset. The internal abstract representation is then used as a basis to extract a concrete microworld instance consisting of image and caption. The image (of size 64\u00d764 in this work) is just a straightforward visualization of the world model. The table below gives an overview of the primary and secondary attributes, together with the value ranges and sampling details used for experiments in this paper (\u201cdistortion\u201d here means width divided by height for rectangles and ellipses).\nshape choose(square, rectangle, triangle, pentagon, cross, circle, semicircle, ellipse) color choose(red, green, blue, yellow,magenta, cyan,white) location uniform(a = (0, 0), b = (64, 64)) object size uniform(a = 0.15, b = 0.3) distortion uniform(a = 2.0, b = 3.0) rotation uniform(a = 0.0, b = 1.0) shade trunc normal(\u00b5 = 0.0, \u03c3 = 0.5) pixel noise trunc normal(\u00b5 = 0.0, \u03c3 = 0.1)\nMost squares are green and there are some circles which are blue.\n\u2248 [ #{s1\u2208World : square(s1.shape)\u2227 green(s1.color)}\n#{s2\u2208World : square(s2.shape)} > 1 2\n] \u2227 [ \u2203s3 \u2208 World : circle(s3.shape) \u2227 blue(s3.color) ]\nFigure 3: An example of a DMRS graph corresponding to a more complex caption, with compositional components colored. The logical formula gives the formal semantic interpretation over a world model."}, {"heading": "3.3 Caption generation", "text": "We currently provide an implementation of the SHAPEWORLD captioner interface using a grammar-based approach. More specifically, Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2016) is an abstract semantic graph representation designed for use with highprecision grammars, such as those distributed by the DELPH-IN consortium.3\nA semantic representation like DMRS is particularly suited for the SHAPEWORLD framework, since it essentially mirrors the internal world model and hence acts like a (partial) languagespecific annotation. Here, noun nodes correspond to entities, adjective nodes add attributes, and verb phrase nodes/sub-graphs specify relations between entities. The semantics of words like \u201csquare\u201d or \u201cred\u201d is interpreted as iteratively filtering a subset of agreeing entities, while transitive relations like \u201cto the left of\u201d act similarly on pairs of entity sets, and quantifiers compare the cardinality of two entity sets. Below an example of a DMRS semantic graph with its compositional components colored:\nThere is a blue circle. Compositionality of the semantic representation is a useful property and an important reason for our use of DMRS. Given compositionality, it is enough to specify the semantics of words \u2013 or, more precisely, of the linguistic atoms in the SHAPEWORLD context, which potentially\n3Although we currently use the English Resource Grammar (Flickinger, 2000), other DELPH-IN grammars use a compatible approach, so SHAPEWORLD can easily be ported to other languages.\nare sub-graphs with multiple nodes and inner link structure \u2013 to be able to obtain the corresponding semantics of composed sub-graphs, and so generate a wide range of different captions.\nFigure 3 shows an example of a more complex compositional caption, which contains the DMRS graph above as sub-graph. It also illustrates how various details are automatically inferred by the English Resource Grammar, including number-agreement between subject and verb, and between quantifier and noun, and realization of an adjective as relative clause. This greatly facilitates the generation of a combinatorially large amount of captions and makes the DMRS graph patterns reusable. Finally, figure 3 gives a formal semantic interpretation of the caption meaning as logical formula over a world model. This indicates how the agreement of a caption with a microworld is computed in the SHAPEWORLD framework.\nSimilar to the generator module, the captioner module randomly samples from a set of datasetspecific DMRS graph patterns, which are then applied to a world model to construct an agreeing caption object (see figure 2). The DMRS graph can be turned into an MRS representation, from which a corresponding English sentence can be generated with a bi-directional grammar like the English Resource Grammar and a parser-generator like Packard\u2019s Answer Constraint Engine.\nThe captioner module\u2019s ability to check whether another world model would agree with the semantics of this caption is important for the generation of negative instances, i.e., caption/microworld pairs that do not agree. These instances are obtained by either sampling a second, false world model, or by producing a false caption object via modification of the agreeing caption. In either case, the system ensures that false microworld and caption object do not accidentally agree."}, {"heading": "3.4 Training and testing on SHAPEWORLD datasets", "text": "Since SHAPEWORLD datasets are actually data generation processes, training and evaluation work differently from classic datasets. Where usually one has a fixed set of instances, here models are trained and tested on a fixed set of higher-level generator configuration constraints. In particular, the constraints for evaluation differ from the training constraints, hence requiring true generalization abilities. For instance, a certain shapecolor-combination, a specific number of objects, a spatial location or a caption type can be heldout and never generated during training, such that concepts need to be recombined at test time. It is thus possible for a system to achieve optimal performance during training, but completely fail the evaluation.\nAnother important property of the SHAPEWORLD datasets, particularly for future extensions, is their compositionality. Instead of having to define a dataset from scratch every time, we can specify atomic datasets and then combine them in a mixer dataset, which tests for various different aspects of multimodal language understanding simultaneously. Reusability in fact applies even further down in the component hierarchy. For instance, we use the same generic world generator module for all four datasets. This is also useful for caption generation where, for instance, a logical combinator dataset can reuse different world captioner modules to generate simple statements which then are merged by logical connectives."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "In this paper we look at four datasets, each designed to investigate an aspect of the capability to understand language in a multimodal setup. Figure 4 gives further information about these datasets. Note that since we first sample a microworld model and subsequently a caption, we cannot always easily control the generation process to sample each possible caption perfectly uniformly. This is in particular the case when focusing on more specific captions which might not apply to a microworld and hence require resampling."}, {"heading": "4.2 Network architecture", "text": "We evaluate several multimodal deep neural network architectures that were recently proposed for VQA (Goyal et al., 2016; Agrawal et al., 2016; Jabri et al., 2016; Antol et al., 2015; Ren et al., 2015). Figure 5 shows the general architecture underlying all of these models. Implementations in TensorFlow, adapted for the ICA task, are included as part of the GitHub repository4. Each model is trained end-to-end on the task, including the CNN module and the word embeddings, as opposed to using pre-trained, general-purpose versions. We train for 5000 iterations5 with a batch size of 128, using Adam optimization (Kingma and Ba, 2014) with learning rate 0.001.\n4https://github.com/AlexKuhnle/ ShapeWorld\n5We tracked the validation performance and found that learning essentially plateaus after at most half the iterations.\nLSTM-only and CNN-only are simple unimodal baselines. CNN+{BoW,LSTM,GRU}:Mult obtain the caption embedding via BoW, LSTM or GRU, respectively, then fuse visual and textual information via pointwise multiplication. CNN+LSTM:Add and CNN+LSTM:Concat, i.e., pointwise addition and concatenation, are alternative basic ways of combining image and caption embeddings. Instead of concatenating the image embedding with the output of the LSTM, in CNN:LSTM it is concatenated with each word embedding before being processed by the LSTM. Finally, hierarchical co-attention (Lu et al., 2016) combines visual information on word-, phrase- and sentence-level with the language input, which is processed by a CNN. CNN+CNN:HCA-{par,alt} implements this approach with the two proposed co-attention mechanisms, parallel and alternating.\nIn the near future, we plan to also adapt the technique of multimodal compact bilinear pooling (Fukui et al., 2016), neural module networks (Andreas et al., 2016a,b) and potentially also relation networks (Raposo et al., 2017) to the ICA task, and upload implementations to the GitHub repository."}, {"heading": "4.3 Results", "text": "Figure 6 reports the train6/validation/test performance of four models. In addition to the overall accuracy, it contains a detailed analysis of the models\u2019 ability to handle certain instance types. The accuracies for these dataset partitions were\n6Note that training accuracy here represents an interesting measure on its own, since no exact same instance is ever seen twice.\nobtained by restricting the dataset generator to sample an evaluation set of only one instance type.\nDue to space limitations, we do not report detailed numbers for the other models. Essentially, they all show the same (or worse) behavior as the LSTM-only model, apart from CNN+GRU:Mult which is similar to CNN+LSTM:Mult.\nA number of conclusions from these results:\n\u2022 The consistently low performance (best: 60%) indicates that all models essentially fail to learn spatial relations, in line with the findings of Johnson et al. (2017).7\n\u2022 The numbers for the HCA models on the QUANTIFICATION dataset indicate that quantifiers are not fully learned. They may be approximated by a rough number/existence/majority estimate \u2013 something we plan to investigate further.\n\u2022 Unsurprisingly, LSTM-only, CNN-only and also CNN+BoW:Mult are not able to learn actual multimodal understanding, in contrast to their good performance on real-world data (Jabri et al., 2016). In our data, the failure in learning clearly shows in the tendency of these models to fall back to always-correct or always-incorrect predictions.\n\u2022 Although sometimes lower than training accuracy, the above-chance-level validation/test accuracy indicate that in some cases the models are able to generalize (to some degree).\n7Contrary to what they report, our instances almost always require relational spatial reasoning.\nthe corresponding instances were relatively harder or easier in comparison to the overall accuracy on the dataset, or whether the tendency is inconsistent across train/validation/test accuracies.\n\u2022 Object recognition itself is not an issue \u2013 the CNN-only model trained for shape-color classification obtains \u223c98% accuracy.\nThere are many more interesting aspects that could be discussed, including learning curves, transfer learning and so on. However, the main point here is that a detailed investigation and error analysis like the one in figure 6 would be very difficult, if not impossible, to conduct with realworld data. It consequently shows the potential of artificial data as basis for a complementary evaluation methodology for multimodal language understanding systems."}, {"heading": "4.4 Future work", "text": "The basic SHAPEWORLD framework can be elaborated in many ways. We plan to add new datasets addressing other aspects of language, as well as integrating options to enhance the language generation module, with the aim of providing more varied and natural image descriptions. For instance, we expect to integrate a subsequent step applying paraphrase rules after caption generation \u2013 Copestake et al. (2016) describe how this can be implemented on the level of DMRS graphs."}, {"heading": "5 Conclusion", "text": "We have presented a new evaluation methodology and framework, SHAPEWORLD, for multimodal deep learning models, with a focus on formalsemantic style generalization capabilities. In this framework, artificial data is automatically generated according to predefined specifications. This controlled data generation makes it possible to introduce previously unseen instance configurations during evaluation, which consequently require the system to recombine learned concepts in novel ways, i.e., true generalization.\nWe evaluated various VQA models on four image caption agreement datasets, where the system has to decide whether a statement applies to an image. We showed how the SHAPEWORLD framework can be used to investigate in detail what these models learn with respect to multimodal language understanding. By exposing specific multimodal scenarios where current multimodal systems fail (e.g. spatial relations), and by providing a configurable, extensible testbed for systematic, detailed and comparable evaluation, we hope to stimulate progress in the field of multimodal language understanding."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, EMNLP 2016, pages", "citeRegEx": "Agrawal et al\\.,? 2016", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics. NAACL", "citeRegEx": "Andreas et al\\.,? 2016a", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2016.", "citeRegEx": "Andreas et al\\.,? 2016b", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision. ICCV 2015.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP). Austin, Texas, USA.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling."], "venue": "Journal of Artificial Intelligence Research 47(1):253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "Transactions on Neural Networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Recursive neural networks can learn logical semantics", "author": ["Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. Association for Compu-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "OpenAI Gym", "author": ["Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Brockman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brockman et al\\.", "year": 2016}, {"title": "Resources for building applications with Dependency Minimal Recursion Semantics", "author": ["Ann Copestake", "Guy Emerson", "Michael W. Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszy\u0144ska."], "venue": "Proceedings of the 10th International", "citeRegEx": "Copestake et al\\.,? 2016", "shortCiteRegEx": "Copestake et al\\.", "year": 2016}, {"title": "Minimal Recursion Semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A. Sag."], "venue": "Research on Language and Computation 3(4):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "On building a more efficient grammar by exploiting types", "author": ["Dan Flickinger."], "venue": "Natural Language Engineering 6(1):15\u201328.", "citeRegEx": "Flickinger.,? 2000", "shortCiteRegEx": "Flickinger.", "year": 2000}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "Proceedings of the 2016 Conference on Empirical", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A. Gers", "J\u00fcrgen Schmidhuber."], "venue": "Transactions on Neural Networks 12(6):1333\u20131340.", "citeRegEx": "Gers and Schmidhuber.,? 2001", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2001}, {"title": "Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh."], "venue": "CoRR abs/1612.00837.", "citeRegEx": "Goyal et al\\.,? 2016", "shortCiteRegEx": "Goyal et al\\.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV). Santiago,", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["Micah Hodosh", "Julia Hockenmaier."], "venue": "Proceedings of the 5th Workshop on Vision and Language. Berlin, Germany.", "citeRegEx": "Hodosh and Hockenmaier.,? 2016", "shortCiteRegEx": "Hodosh and Hockenmaier.", "year": 2016}, {"title": "Revisiting Visual Question Answering Baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten"], "venue": null, "citeRegEx": "Jabri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick."], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "Johnson et al\\.,? 2017", "shortCiteRegEx": "Johnson et al\\.", "year": 2017}, {"title": "The Malmo platform for artificial intelligence experimentation", "author": ["Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell."], "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). AAAI Press, Palo Alto,", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov."], "venue": "Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov.,? 2015", "shortCiteRegEx": "Joulin and Mikolov.", "year": 2015}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015, pages 3128\u20133137.", "citeRegEx": "Karpathy and Li.,? 2015", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Virtual embodiment: A scalable long-term strategy for artificial intelligence research", "author": ["Douwe Kiela", "Luana Bulat", "Anita L. Vero", "Stephen Clark."], "venue": "CoRR abs/1610.07432.", "citeRegEx": "Kiela et al\\.,? 2016", "shortCiteRegEx": "Kiela et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "A roadmap towards machine intelligence", "author": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni."], "venue": "CoRR abs/1511.08130.", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "English as a formal language", "author": ["Richard Montague."], "venue": "Bruno Visentini, editor, Linguaggi nella societa e nella tecnica, Edizioni di Communita, pages 188\u2013221.", "citeRegEx": "Montague.,? 1970", "shortCiteRegEx": "Montague.", "year": 1970}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associ-", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "The meaning of \u2019most\u2019: Semantics, numerosity and psychology", "author": ["Paul Pietroski", "Jeffrey Lidz", "Tim Hunter", "Justin Halberda."], "venue": "Mind and Language 24(5):554\u2013585.", "citeRegEx": "Pietroski et al\\.,? 2009", "shortCiteRegEx": "Pietroski et al\\.", "year": 2009}, {"title": "Discovering objects and their relations from entangled scene representations", "author": ["David Raposo", "Adam Santoro", "David Barrett", "Razvan Pascanu", "Timothy Lillicrap", "Peter W. Battaglia."], "venue": "International Conference on Learning Representations 2017 (ICLR", "citeRegEx": "Raposo et al\\.,? 2017", "shortCiteRegEx": "Raposo et al\\.", "year": 2017}, {"title": "Image question answering: A visual semantic embedding model and a new dataset", "author": ["Mengye Ren", "Ryan Kiros", "Richard S. Zemel."], "venue": "CoRR abs/1505.02074.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Look, some green circles!\u201d: Learning to quantify from images", "author": ["Ionut Sorodoc", "Angeliki Lazaridou", "Gemma Boleda", "Aur\u00e9lie Herbelot", "Sandro Pezzelle", "Raffaella Bernardi."], "venue": "Proceedings of the 5th Workshop on Vision and Language. Berlin,", "citeRegEx": "Sorodoc et al\\.,? 2016", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "RNN Approaches to Text Normalization: A Challenge", "author": ["Richard Sproat", "Navdeep Jaitly."], "venue": "CoRR abs/1611.00068.", "citeRegEx": "Sproat and Jaitly.,? 2016", "shortCiteRegEx": "Sproat and Jaitly.", "year": 2016}, {"title": "MazeBase: A sandbox for learning from games", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Gabriel Synnaeve", "Soumith Chintala", "Rob Fergus."], "venue": "CoRR abs/1511.07401.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus."], "venue": "CoRR abs/1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems. MIT Press, Montreal, Canada, NIPS\u201915, pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "CoRR abs/1410.4615.", "citeRegEx": "Zaremba and Sutskever.,? 2014", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals."], "venue": "International Conference on Learning Representations 2017 (ICLR 2017) .", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Yin and Yang: Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh."], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C. Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2013, pages 3009\u20133016.", "citeRegEx": "Zitnick and Parikh.,? 2013", "shortCiteRegEx": "Zitnick and Parikh.", "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C. Lawrence Zitnick", "Ramakrishna Vedantam", "Devi Parikh."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38(4):627\u2013638.", "citeRegEx": "Zitnick et al\\.,? 2016", "shortCiteRegEx": "Zitnick et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (VQA) (Antol et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 3, "context": "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (VQA) (Antol et al., 2015) can now be tackled with great success.", "startOffset": 108, "endOffset": 128}, {"referenceID": 36, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 29, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 40, "context": "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al.", "startOffset": 37, "endOffset": 100}, {"referenceID": 15, "context": ", 2017) have shown surprising behavior very different from what would be expected of their \u201csurpassing human-level performance\u201d (He et al., 2015).", "startOffset": 128, "endOffset": 145}, {"referenceID": 14, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 0, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 41, "context": "(Goyal et al., 2016; Agrawal et al., 2016; Zhang et al., 2016).", "startOffset": 0, "endOffset": 62}, {"referenceID": 16, "context": "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).", "startOffset": 97, "endOffset": 159}, {"referenceID": 13, "context": "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).", "startOffset": 97, "endOffset": 159}, {"referenceID": 38, "context": "We think of the SHAPEWORLD tasks as unit-testing multimodal systems for specific linguistic generalization capabilities, in a similar way to the bAbI tasks (Weston et al., 2015) for text-only understanding.", "startOffset": 156, "endOffset": 177}, {"referenceID": 23, "context": "gued to be a promising driver for artificial intelligence research (Kiela et al., 2016).", "startOffset": 67, "endOffset": 87}, {"referenceID": 5, "context": "Various platforms following this paradigm have been released, mostly aimed at reinforcement learning: the Arcade Learning Environment / Atari 2600 games (Bellemare et al., 2013), OpenAI Gym (Brockman", "startOffset": 153, "endOffset": 177}, {"referenceID": 20, "context": ", 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular.", "startOffset": 23, "endOffset": 45}, {"referenceID": 38, "context": "The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it.", "startOffset": 15, "endOffset": 36}, {"referenceID": 35, "context": "The MazeBase game environment (Sukhbaatar et al., 2015) uses language as a mean to represent the game world.", "startOffset": 30, "endOffset": 55}, {"referenceID": 27, "context": "A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game.", "startOffset": 31, "endOffset": 56}, {"referenceID": 26, "context": "The long-term research proposal of Mikolov et al. (2015) also simulates a world where", "startOffset": 35, "endOffset": 57}, {"referenceID": 21, "context": "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al.", "startOffset": 79, "endOffset": 105}, {"referenceID": 37, "context": "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al., 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.", "startOffset": 130, "endOffset": 152}, {"referenceID": 39, "context": ", 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.", "startOffset": 48, "endOffset": 77}, {"referenceID": 33, "context": "(2015) and Sorodoc et al. (2016) are more similar to our work in focusing on specific linguistic aspects.", "startOffset": 11, "endOffset": 33}, {"referenceID": 32, "context": "1 However, there have been experiments in which parts of the data are artificial and/or generated automatically, for instance, automatic question generation from annotation (Ren et al., 2015) or systematic modification of captions (Hodosh and Hockenmaier, 2016).", "startOffset": 173, "endOffset": 191}, {"referenceID": 17, "context": ", 2015) or systematic modification of captions (Hodosh and Hockenmaier, 2016).", "startOffset": 47, "endOffset": 77}, {"referenceID": 43, "context": "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al.", "startOffset": 60, "endOffset": 108}, {"referenceID": 42, "context": "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al.", "startOffset": 60, "endOffset": 108}, {"referenceID": 41, "context": ", 2016; Zitnick and Parikh, 2013) and to balance existing VQA datasets (Zhang et al., 2016).", "startOffset": 71, "endOffset": 91}, {"referenceID": 19, "context": "Most similar to the SHAPEWORLD framework is the CLEVR dataset (Johnson et al., 2017).", "startOffset": 62, "endOffset": 84}, {"referenceID": 27, "context": "In fact, our generation system closely resembles classical work in formal semantics, where a statement corresponds to a logical expression which can be evaluated against an abstract world model (Montague, 1970).", "startOffset": 194, "endOffset": 210}, {"referenceID": 10, "context": "We utilize semantic representations based on Minimal Recursion Semantics (Copestake et al., 2005) and broad-coverage, grammarbased realization driven by the English Resource Grammar (Flickinger, 2000) to make the internal world model compatible with language.", "startOffset": 73, "endOffset": 97}, {"referenceID": 11, "context": ", 2005) and broad-coverage, grammarbased realization driven by the English Resource Grammar (Flickinger, 2000) to make the internal world model compatible with language.", "startOffset": 92, "endOffset": 110}, {"referenceID": 14, "context": "It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases or hidden correlations, which can obfuscate results when using real-world images and text (Goyal et al., 2016; Agrawal et al., 2016).", "startOffset": 185, "endOffset": 227}, {"referenceID": 0, "context": "It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases or hidden correlations, which can obfuscate results when using real-world images and text (Goyal et al., 2016; Agrawal et al., 2016).", "startOffset": 185, "endOffset": 227}, {"referenceID": 9, "context": "For the internal DMRS-based caption generation, the Python package pydmrs (Copestake et al., 2016), as well as a reduced version of the English Resource Grammar (Flickinger, 2000) and of Packard\u2019s Answer Constraint Engine (http://sweaglesw.", "startOffset": 74, "endOffset": 98}, {"referenceID": 11, "context": ", 2016), as well as a reduced version of the English Resource Grammar (Flickinger, 2000) and of Packard\u2019s Answer Constraint Engine (http://sweaglesw.", "startOffset": 70, "endOffset": 88}, {"referenceID": 25, "context": ", yes/no questions), it neither requires the evaluated model to generate answers nor to rephrase the problem to fit it into a classification task of some sort \u2013 for instance, over the 1000/3000 most common answers, as is common practice recently (Lu et al., 2016; Fukui et al., 2016).", "startOffset": 246, "endOffset": 283}, {"referenceID": 12, "context": ", yes/no questions), it neither requires the evaluated model to generate answers nor to rephrase the problem to fit it into a classification task of some sort \u2013 for instance, over the 1000/3000 most common answers, as is common practice recently (Lu et al., 2016; Fukui et al., 2016).", "startOffset": 246, "endOffset": 283}, {"referenceID": 12, "context": ", 2016; Fukui et al., 2016). ICA most closely corresponds to the work of Jabri et al. (2016), who present VQA as a binary classification of image-question-answer triples.", "startOffset": 8, "endOffset": 93}, {"referenceID": 30, "context": ", quantifiers such as most (Pietroski et al., 2009).", "startOffset": 27, "endOffset": 51}, {"referenceID": 9, "context": "(Copestake et al., 2016) is an abstract semantic graph representation designed for use with highprecision grammars, such as those distributed by the DELPH-IN consortium.", "startOffset": 0, "endOffset": 24}, {"referenceID": 11, "context": "Although we currently use the English Resource Grammar (Flickinger, 2000), other DELPH-IN grammars use a compatible approach, so SHAPEWORLD can easily be ported to other languages.", "startOffset": 55, "endOffset": 73}, {"referenceID": 24, "context": "We train for 5000 iterations5 with a batch size of 128, using Adam optimization (Kingma and Ba, 2014) with learning rate 0.", "startOffset": 80, "endOffset": 101}, {"referenceID": 25, "context": "Finally, hierarchical co-attention (Lu et al., 2016) combines visual information on word-, phrase- and sentence-level with the language input, which is processed by a CNN.", "startOffset": 35, "endOffset": 52}, {"referenceID": 12, "context": "In the near future, we plan to also adapt the technique of multimodal compact bilinear pooling (Fukui et al., 2016), neural module networks (Andreas et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 31, "context": ", 2016a,b) and potentially also relation networks (Raposo et al., 2017) to the ICA task, and upload implementations to the GitHub repository.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "60%) indicates that all models essentially fail to learn spatial relations, in line with the findings of Johnson et al. (2017).7", "startOffset": 105, "endOffset": 127}, {"referenceID": 18, "context": "\u2022 Unsurprisingly, LSTM-only, CNN-only and also CNN+BoW:Mult are not able to learn actual multimodal understanding, in contrast to their good performance on real-world data (Jabri et al., 2016).", "startOffset": 172, "endOffset": 192}, {"referenceID": 9, "context": "For instance, we expect to integrate a subsequent step applying paraphrase rules after caption generation \u2013 Copestake et al. (2016) describe how this can be implemented on the level of DMRS graphs.", "startOffset": 108, "endOffset": 132}], "year": 2017, "abstractText": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter\u2019s specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By opensourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.", "creator": "LaTeX with hyperref package"}}}