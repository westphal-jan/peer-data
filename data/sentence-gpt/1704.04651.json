{"id": "1704.04651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "The Reactor: A Sample-Efficient Actor-Critic Architecture", "abstract": "In this work we present a new reinforcement learning agent, called Reactor (for Retrace-actor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy {\\pi} (the actor), an action-value Q-function (the critic) evaluating the current policy {\\pi}, and an estimated behavioral policy {\\hat \\mu} which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel {\\beta}-leave-one-out policy gradient estimate (which uses both the off-policy corrected return and the estimated Q-function). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance. We tested the model using the original Turing test.\n\n\n\n\nRecurrent Neural Networks:\nIn the past, we have introduced a recurrent neural network and re-evaluated it to solve a problem that we had not previously explored. Our algorithm is a direct response-response approach that aims to mimic our current state and to learn from the existing model (e.g., learning over time). Our model is scalable with the new generation of neural networks.\nAs we have shown in the earlier example of the Reactor, we can model an action-value Q-function using a recursive recurrent neural network for function approximation. We will also show that the model contains the following features:\nThe algorithm is recursive.\nWe can train the model using the same model of Reactor, using the same model of Reactor.\nFor the implementation of the model, we can train the model using the same model of Reactor. For the implementation of the model, we can train the model using the same model of Reactor. For the implementation of the model, we can train the model using the same model of Reactor.\nThe model used in the previous example of the Reactor is a recursion-based neural network.\nThe model was developed by Nivam K. Y. H\u00e4llev. The neural network is a recurrent neural network.\nThe model uses a recurrent neural network.\nTo use the Rec", "histories": [["v1", "Sat, 15 Apr 2017 15:38:23 GMT  (727kb,D)", "http://arxiv.org/abs/1704.04651v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["audrunas gruslys", "mohammad gheshlaghi azar", "marc g bellemare", "remi munos"], "accepted": false, "id": "1704.04651"}, "pdf": {"name": "1704.04651.pdf", "metadata": {"source": "META", "title": "The Reactor: A Sample-Efficient Actor-Critic Architecture", "authors": ["Audrunas Gruslys", "Mohammad Gheshlaghi Azar", "Marc G. Bellemare", "Remi Munos"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Model free deep reinforcement learning has achieved remarkable success lately in many domains ranging from achieving human and super-human level control in video games (Mnih et al., 2016), (Mnih et al., 2015) to continuous motor control tasks (Lillicrap et al., 2015). However, two of the most popular frameworks, DQN (Mnih et al., 2015) and A3C (Mnih et al., 2016), have several disadvantages: DQN suffers from potentially slow learning caused by single-step temporal difference updates, while A3C has a relatively low data-efficiency (since it does not use a memory buffer) and requires being trained on several copies of the same environment simultaneously. Also A3C is an on-policy algorithm which does not separate the evaluation policy from the behaviour policy, making it difficult to explore safely. Data-efficiency and off-policy learning are arguably impor-\n1DeepMind, London, UK.\ntant for many real-world problems where interactions with the environment is expensive. For this reason we wanted to exploit the off-policy advantage of DQN while also getting the benefits of using multi-step returns and an explicit policy evaluation of A3C framework.\nIn this work we introduce a new agent architecture, called Reactor (for Retrace-actor), which has the following features. The Reactor uses an off-policy multi-step learning algorithm based on an actor-critic architecture. The critic implements the Retrace (Munos et al., 2016) algorithm, while the actor is trained by a new policy gradient algorithm, called \u03b2-leave-one-out, which makes use of both the off-policy Retrace-corrected return and the estimated Q-function.\nThe Reactor is sample-efficient thanks to the use of memory replay, and numericaly efficient since it uses multi-step returns which improves the speed of reward propagation backwards in time, while the off-policy nature of the algorithm enables to reuse old data and be sample efficient.\nThe reason we use an actor-critic architecture is to explicitly evaluate behavioural policies which may differ from policies derived from learned Q-values (such as epsilongreedy policies). This can be beneficial for several reasons. Firstly, a policy may be easier to learn than action values or action advantages. A second advantage is that estimating an explicit behavioural policy may reduce the problem of Q-value over-estimation when bootstrapping in the presence of noise, as the best action and the actual action value are evaluated by different functional approximations. Similar reasoning led to the introduction of Double DQN algorithm (Van Hasselt et al., 2016). The third advantage is that evaluating non-deterministic policies can potentially lead to less trace cutting while using Retrace algorithm. For this reason it is beneficial to be stochastic when actions are of equal value and the easiest way to do this is to have an explicit actor which is trained with some entropy reward.\nA main difference compared to DQN is that in order to implement the multi-step Retrace algorithm, we need to replay sequences of (instead of individual) transitions. Since we do so, using a recurrent network architecture comes at no cost. Actually, using a recurrent network architecture avoids complications related to frame-stacking and gives computational gains, as each frame has to be processed by\nar X\niv :1\n70 4.\n04 65\n1v 1\n[ cs\n.A I]\n1 5\nA pr\n2 01\n7\na convolutional network only once in each sequence. While using a recurrent network architecture may not be required in Atari, it may become useful in more complex partially observable domains. And even in Atari domain, recent work (Harb & Precup) showed that using LSTM (combined with Watkins Q(\u03bb) algorithm) can improve learning compared to feed-forward networks.\nLastly, we decoupled acting from learning by allowing the actor and the learner to run in parallel. This does not block the actor during batched learning and does not block the learner while the actor is acting which in turn allows to achieve greater CPU utilization.\nRelated works: Like A3C (Mnih et al., 2016), Reactor is an actor-critic multi-step returns algorithm. Reactor differs from A3C in that it uses a memory buffer and an offpolicy learning algorithm. Compared to DQN, Reactor brings the multi-step returns and the actor-critic architecture. DDPG (Lillicrap et al., 2015) uses an actor-critic architecture but in a continuous control setting making use of the deterministic policy gradient algorithm and does not consider multi-step returns. UNREAL agent (Jaderberg et al., 2016) improved final performance and data efficiency of A3C framework by introducing replay memory and unsupervised auxiliary tasks.\nThe closest work to ours is the ACER algorithm (Wang et al., 2017) which is also an actor-critic based on the Retrace algorithm which makes use of memory replay. The main differences are (1) Reactor use a different policy gradient algorithm for the actor (see discussion in Section 2.2), (2) ACER mixes on-policy (from current run) and offpolicy (from memory) updates whereas Reactor uses offpolicy updates replayed from the memory exclusively, (3) Reactor makes use of an estimated \u00b5\u0302 behaviour policy instead of storing \u00b5 in memory, and (4) Reactor uses an actor and learner that can run in parallel independently from each other."}, {"heading": "2. The actor-critic algorithm", "text": "In this section we first describe the general algorithm we used for the critic (Retrace) and the actor (\u03b2-leave-oneout). We describe our specific implementation in the next section. We first define some notation. We consider a Markov decision process with state space X and finite action space A. A (stochastic) policy is a mapping from X to distributions over actions. We consider a \u03b3-discounted infinite-horizon criterion and define for any policy \u03c0, the Q-value of any state-action pair (x, a) as\nQ\u03c0(x, a) def = E [\u2211 t\u22650 \u03b3tr(xt, at)|x0 = x, a0 = a, \u03c0 ] ,\nwhere ({xt}t\u22650) is a trajectory generated by choosing a in x and following \u03c0 thereafter, i.e., at \u223c \u03c0(\u00b7|xt) (for t \u2265 1), and r(xt, at) is a reward function. Our goal is to find an optimal policy \u03c0\u2217, which maximizes Q\u03c0(x, a). Q\u2217(x, a) = max\u03c0 Q\n\u03c0(x, a) represents the optimal Qvalues. We have the property that any policy defined in any state x as \u2208 arg maxaQ\u2217(x, a) is an optimal policy."}, {"heading": "2.1. The critic: Retrace(\u03bb)", "text": "For the critic we use the Retrace(\u03bb) algorithm introduced in (Munos et al., 2016). This is a general off-policy RL algorithm which uses multi-step returns. Assume that some trajectory {x0, a0, r0, x1, a1, r1, . . . , xt, at, rt, . . . , } has been generated according to some behaviour policy \u00b5, i.e., at \u223c \u00b5(\u00b7|xt). Now we want to evaluate the value of a different target policy \u03c0, i.e. we want to estimate Q\u03c0 . The Retrace algorithm will update our current estimateQ ofQ\u03c0 in the direction of\n\u2206Q(xt, at) def = \u2211 s\u2265t \u03b3s\u2212t(ct+1 . . . cs)\u03b4 \u03c0 sQ, (1)\nwhere \u03b4\u03c0sQ def = rs + \u03b3E\u03c0[Q(xs+1, \u00b7)] \u2212 Q(xs, as) is the temporal difference of Q at time s under policy \u03c0, and\ncs = \u03bbmin ( 1, \u03c0(as|xs) \u00b5(as|xs) ) . (2)\nThe Retrace algorithm comes with the theoretical guarantee that in finite state and action spaces, repeatedly updating our current estimate Q according to (1) produces a sequence which converges to Q\u03c0 for a fixed \u03c0 or to Q\u2217 if we consider a sequence of policies \u03c0 which become increasing greedy w.r.t. the Q estimates, see (Munos et al., 2016)."}, {"heading": "2.2. The actor: \u03b2-LOO policy gradient algorithm", "text": "The Reactor architecture represents both a policy \u03c0(a|x) and Q-values Q(x, a). We use a policy gradient algorithm to train the actor \u03c0 which makes use of our current estimate Q(x, a) of Q\u03c0(x, a). Let V \u03c0(x0) be the value function at some initial state x0, the policy gradient theorem (Sutton et al., 2000) says that \u2207V \u03c0(x0) = E [\u2211 t \u03b3 t \u2211 aQ \u03c0(xt, a)\u2207\u03c0(a|xt) ] , where \u2207 refers to the gradient w.r.t. policy parameters. We now consider several possible ways to estimate this gradient.\nTo simplify notation, we drop the dependence on the state x for now and consider the problem of estimating the quantity\nG = \u2211 a Q\u03c0(a)\u2207\u03c0(a). (3)\nSince we consider the off-policy case, consider estimating G using a single action A drawn from a (possibly different from \u03c0) behaviour distribution A \u223c \u00b5. Let us assume that\nfor the chosen actionAwe have access to an estimateR(A) of Q\u03c0(A). Then we can use likelihood ratio (LR) method combined with an importance sampling (IS) ratio (which we call ISLR) to build an unbiased estimate of G:\nG\u0302ISLR = \u03c0(A)\n\u00b5(A) (R(A)\u2212 V )\u2207 log \u03c0(A),\nwhere V is a baseline that depend on the state but not on the chosen action. However this estimate suffers from high variance. A possible way for reducing variance is to estimate G directly from (3) by using the return R(A) for the chosen action A and our current estimate Q of Q\u03c0 for the other actions, which lead to the so-called leave-one-out (LOO) policy gradient estimate:\nG\u0302LOO = R(A)\u2207\u03c0(A) + \u2211 a6=A Q(a)\u2207\u03c0(a). (4)\nThis estimate has low variance but may be biased if the estimated Q values differ from Q\u03c0 . A better bias-variance tradeoff may be obtained by considering the more general \u03b2-LOO policy gradient estimate:\nG\u0302\u03b2-LOO = \u03b2(R(A)\u2212Q(A))\u2207\u03c0(A) + \u2211 a Q(a)\u2207\u03c0(a),\n(5) where \u03b2 = \u03b2(\u00b5, \u03c0,A) can be a function of both policies \u03c0 and \u00b5 and the selected action A. Notice that when \u03b2 = 1, (5) reduces to (4), and when \u03b2 = 1/\u00b5(A), then (5) is\nG\u0302 1 \u00b5\n-LOO = \u03c0(A)\n\u00b5(A) (R(A)\u2212Q(A))\u2207 log \u03c0(A)+ \u2211 a Q(a)\u2207\u03c0(a).\n(6) This estimate is unbiased and can be seen as a generalization of G\u0302ISLR where instead of using a state-only dependent baseline, we use a state-and-action-dependent baseline (our current estimate Q) and add the correction term\u2211 a\u2207\u03c0(a)Q(a) to cancel the bias. We now analyze the bias of the G\u03b2-LOO estimate.\nProposition 1. Assume A \u223c \u00b5 and that E[R(A)] = Q\u03c0(A). Then, the bias of G\u03b2-LOO is \u2223\u2223\u2211 a(1 \u2212\n\u00b5(a)\u03b2(a))\u2207\u03c0(a)[Q(a)\u2212Q\u03c0(a)] \u2223\u2223.\nProof. We have that E[G\u0302]\u03b2-LOO is\n= \u2211 a \u00b5(a)[\u03b2(a)(E[R(a)]\ufe38 \ufe37\ufe37 \ufe38 =Q\u03c0(a) \u2212Q(a))]\u2207\u03c0(a) + \u2211 a Q(a)\u2207\u03c0(a)\n= G+ \u2211 a (1\u2212 \u00b5(a)\u03b2(a))[Q(a)\u2212Q\u03c0(a)]\u2207\u03c0(a)\nThus the bias is small when \u03b2(a) is close to 1/\u00b5(a) (and unbiased for G\u0302 1\n\u00b5 -LOO whatever theQ estimates are) or when\nthe Q-estimates are close to the true Q\u03c0 values. Now its variance is low when \u03b2 is small. So in order to have a good bias-variance tradeoff we recommend using the \u03b2-LOO estimate with \u03b2 defined as: \u03b2(A) = min ( c, 1\u00b5(A) ) , for some constant c \u2265 1. Note that \u03b2(A) = 1 when c = 1.\nThis truncated 1/\u00b5 coefficient shares similarities with the truncated IS gradient estimate introduced in (Wang et al., 2017) (which we call TISLR for truncated-ISLR):\nG\u0302TISLR = min ( c, \u03c0(A)\n\u00b5(A)\n) (R(A)\u2212 V )\u2207 log \u03c0(A)\n+ \u2211 a (\u03c0(a) \u00b5(a) \u2212 c ) + \u00b5(a)(Q\u03c0(a)\u2212 V )\u2207 log \u03c0(a).\nThe differences are: (i) we truncate 1/\u00b5(A) = \u03c0(A)/\u00b5(A) \u00d7 1/\u03c0(A) instead of truncating \u03c0(A)/\u00b5(A), which provides an additional variance reduction due to the variance of the LR \u2207 log \u03c0(A) = \u2207\u03c0(A)\u03c0(A) (since this LR may be large when a low probability action is chosen), and (ii) we use ourQ-baseline instead of a V baseline, reducing further the variance of the LR estimate.\nIn our experiments we compare the different policy gradient estimates G\u0302ISLR, G\u0302TISLR, and G\u0302\u03b2-LOO for \u03b2 = 1, \u03b2 = 1/\u00b5, \u03b2 = min(5, 1/\u00b5).\nRemark: In off-policy learning it is very difficult to produce an unbiased sample R(A) of Q\u03c0(A) when following another policy \u00b5. This would require using full importance sampling correction along the trajectory. Instead, in our implementation we use the off-policy corrected return computed by the Retrace algorithm, which produces a (biased) estimate ofQ\u03c0(A) but whose bias vanishes asymptotically, see (Munos et al., 2016)."}, {"heading": "3. Reactor implementation", "text": ""}, {"heading": "3.1. Off-policy past actions handled by Retrace", "text": "Since the agent\u2019s policy changes with time, we need to take into account that past actions {as \u223c \u03c0s}s\u2264t, which have been stored in memory, have been generated according to a policy which is different from the policy \u03c0t we wish to evaluate at current time t. As described in Section 2.1, the Retrace algorithm makes use of two policies, the behaviour and target policies. In Reactor, the behaviour policy \u00b5 corresponds to the actions {as}s\u2264t that have been generate by the agent in the past and have been stored in the memory buffer. The target policy corresponds to the current policy \u03c0t of the agent, which is the one we wish to evaluate."}, {"heading": "3.2. Behaviour probability estimates \u00b5\u0302", "text": "The agent\u2019s behaviour is defined by sampling actions from the current policy \u03c0t and stores observed states, actions, re-\nwards and selected action probabilities (xt, at, rt, \u00b5t) into the replay memory. Retrace uses an off-policy correction which requires the knowledge of the probability under which actions have been generated (i.e. the trace cutting coefficient cs in (2) depends on \u00b5(as|xs)). We can either use stored in the memory the behaviour probabilities \u00b5(as|xs) of the chosen actions or we can construct and estimate \u00b5\u0302 based on past samples by training to predict previously taken actions P (as|xs). The replay memory contains a mixture of trajectories generated by the sequence of policies \u03c01, \u03c02, ..., \u03c0t during the learning process. To see the difference between \u00b5 and \u00b5\u0302, consider that from a state x, actions 1 and 2 have been chosen in a deterministic way with same proportion. Thus the replay memory contains equal amounts of samples for both actions (in that state). From the off-policy batched learning perspective, from that state, the empirical distribution of actions is similar to that of a stochastic policy which would have chosen both actions with equal probabilities of 1/2. Thus the memorized probabilities \u00b5(as|x) are either 1 or 0, whereas a learnt \u00b5\u0302(as|x) will predict 1/2. It can be beneficial to use \u00b5\u0302 since the trace cutting coefficients ct would be higher leading to less cutting and faster reward propagation by Retrace algorithm. This might make off-policy corrected returns less biased but at the cost of a higher variance. On the other hand if behavioural probability estimates are inaccurate, Retrace algorithm may produce biased returns.\nIn the experimental section we report experiments for both approaches. Learnt estimate \u00b5\u0302 may improve performance compared to using the true action probabilities in some circumstances, which is an interesting finding since those probabilities may not always be available (such as when learning from offline log data). Note that some theoretical findings (Li et al., 2015) support that using a regression estimate may indeed improve importance sampling."}, {"heading": "3.3. Network architecture", "text": "The Reactor architecture uses a recurrent neural network architecture which takes an observation xt as input and produces three outputs: current action-value estimates Q(xt, a), current policy \u03c0(a|xt) and estimated behavioural policy \u00b5\u0302(a|xt), for all actions a (Figure 1).\nAction-values use a duelling architecture (Wang et al., 2015) which internally splits Q-value into state-value and advantage estimation, which in turn is connected to an LSTM recurrent network (Hochreiter & Schmidhuber, 1997). Policy and behaviour policy heads use last softmax layer mixed with a fixed uniform distribution of choosing a random action where this mixing ratio is a hyperparameter. Each policy network \u00b5\u0302 and \u03c0 have their own separate LSTM, as this was found to work much better than sharing the same LSTM with action-value head Q.\nAll three LSTMs in turn are connected to a shared linear layer which is connected to a shared convolutional neural network (Krizhevsky et al., 2012).\nGradients coming from both policy LSTMs are blocked and only gradients originating from Q value LSTM is allowed to back-propagate to the convolutional neural network. We block gradients from \u03c0 head for increasing stability, as this avoids positive feedback loops between \u03c0 and Q caused by shared representations. Gradients originating from the \u00b5\u0302 LSTM are also blocked in order to make experiments with memorized \u00b5 and estimated \u00b5\u0302 values more comparable.\nConcatenated rectified linear units are used as nonlinearities in the network (Shang et al., 2016). We did this for two reasons. Firstly, as was shown by (Shang et al., 2016), convolutional neural networks with rectified linear units tend to lead to pairs of opposite features. Using concatenated rectified linear units allows to reduce the size of the filters by half, saving computation. Secondly, neural networks with rectified linear units tend to age, when a given unit gets pushed below zero by a wild gradient and being clamped at zero there is no gradient to ever pull it back up again. Using concatenated rectified linear units helps to solve this problem as if a neuron gets pushed below zero, a value pops up again on the negative side. It is up to the next linearity to learn not to use a value if it is not necessary.\nFinally, although we have chosen a recurrent (LSTM) network, a feed-forward implementation is still possible. However in some problems (such as in the Atari domain) it is often useful to base our decisions on a short history of past observations. We chose a recurrent network architecture instead of frame stacking for the reasons of implementation simplicity and computational efficiency. As Re-\ntrace algorithm requires evaluating Q-values over contiguous sequences of trajectories, using a recurrent architecture allowed each frame to be processed by the convolutional network only once, as opposed to several times if several frame concatenations were used.\nPrecise network specification is given in Table 1. The value of is the minimum probability of choosing a random action and it is hard-coded into the policy network."}, {"heading": "3.4. Target networks", "text": "We used a target networkQT (fixed copy of the current network) similar to that of DQN (Mnih et al., 2015). Our implementation of the Retrace algorithm makes use of the target networkQT . Contiguous sequences {xu, . . . , xu+\u03c4} of length \u03c4 are sampled uniformly at random from our memory buffer. We use the target networkQT to update our current Q-estimate Q(xt, at) at every state xt (t \u2208 [u, u+ \u03c4 ]) in the direction of the new target value: rt + \u03b3E\u03c0[QT (xt+1, \u00b7)] + u+\u03c4\u2211 s=t+1 \u03b3s\u2212t(ct+1 . . . cs)\u03b4 \u03c0 sQT , (7)\nwhere \u03b4\u03c0sQT = rs + \u03b3E\u03c0[QT (xs+1, \u00b7)]\u2212QT (xs, as).\nThe target network is updated every Tupdate \u2208 {1000, 10000} learning steps. We trained all neural networks by sampling contiguous sequences of length \u03c4 = 33 and during learning time we always reset the LSTM internal state. In order not to unroll target network over longer\ndistances than it was trained on, we evaluated target Q values on sub-sequences of 32 shifted by 1 comparing to the current learning network (Figure 2).\nWhen the agent was acting, the internal state of its policy LSTM was reset only at the beginning of each episode."}, {"heading": "3.5. Learning Q, \u03c0, \u00b5\u0302, from a replayed sequence", "text": "In each learning step, a sequence of length \u03c4 is sampled (uniformly) at random from the memory. The target QT values are evaluated along the sequence and the current policy \u03c0 and behaviour policy \u00b5\u0302 probabilities are evaluated by the current network.\nLearning Q-values: Retrace algorithm is used to regress\n(using an L2-loss) all current Q-values along the sequence towards the new target-values defined by (7). If \u00b5\u0302 values are used they are evaluated from the moving network in order to get the most up-to-date behavioural estimates. Learning \u03c0: the current policy \u03c0 is updated by using the \u03b2-LOO policy gradient estimate discussed earlier. In order to avoid converging too quickly to deterministic policies we add a scaled entropy reward to the policy gradient. In order to make entropy reward scale to be more uniform across games with different reward amplitudes and frequencies, during each learning step we evaluated a mean action-gap over a batch of sampled sequences and used it to scale entropy. We defined a mean action-gap as a mean difference between the best action value and a (uniformly) randomly chosen other action. We used 0.1 as a scaling constant for entropy reward. Learning \u00b5\u0302: We regress \u00b5\u0302 towards actions sampled from replay memory using a cross entropy loss.\nWe summed all losses, evaluated gradients and used ADAM optimizer (Kingma & Ba, 2014) to train the neural network."}, {"heading": "3.6. Parallelism", "text": "In order to improve CPU/GPU utilization we can decouple acting from learning. This is an important aspect of our architecture: an acting thread receives observations, submits actions to the environment, and stores transition information into the memory, while an learning thread resamples sequences of experiences from the memory and learns from them (Figure 4). Comparing to the standard DQN framework (Mnih et al., 2015) Reactor is able to improve CPU/GPU utilization by learning while acting, and comparing to A3C framework (Mnih et al., 2016) we can\nexploit computational benefits of batched learning."}, {"heading": "4. Results", "text": ""}, {"heading": "4.1. Comparison of different policy gradients within Reactor architecture", "text": "We trained different versions of Reactor on 57 Atari games for 200 million frames with 3 randomly initialized seeds per experiment. We used the following method to compare algorithm performance across all games as a function of training time: (1). For each game for each algorithm pair and for each time step we evaluated the probability that algorithm 1 has more score than algorithm 2 by comparing all seeds pair-wise. This produced a tensor of size P[game x alg1 x alg2 x time]. (2). For each algorithm pair and for each time-step we averaged probabilities across games producing Q[alg1 x alg2 x time]. This quantity has an interpretable meaning of the probability that a random instance of algorithm 1 would be outperforming a random instance of algorithm 2 at time t given a randomly chosen game. (3). For each algorithm and each time we evaluated the average probability by averaging across all other algorithms. This produced a matrix S[alg x time] which has a meaning of a probabiliy that an algorithm outperforms any other randomly selected algorithm on a randomly selected game at training step t. We plot those curves as a function of time in order to compare relative algorithm performance at different stages of learning."}, {"heading": "4.2. Estimating versus memorizing behavioural probabilities", "text": "We evaluated inter-game performance metric described in section 4.1. Using the slowest target network update frequency Tupdate = 10000 gave better results than using Tupdate = 1000 and using a lower value of \u03bb = 0.9 gave better results than \u03bb = 1.0. The results are shown in Figures 5 and 6. As it is seen, memorizing behavioural probability values on average gave better results for most algorithms and most parameter configurations that when using estimated values, especially for target network update period Tupdate = 10000. A notable exception is TISLR algorithm, where estimated behavioural probabilities gave better scores over most training steps and most parameter configurations. Still, a difference in performance was not so huge, as at 200 million steps learning behavioural probabilities had roughly 1/3 chance of giving better results than memorizing values for both beta-LOO algorithms and gave similar performance in the case of TSILR (Figure 5). This demonstrates that estimating behavioural probabilities is a feasible approach when actual behaviour probability values are not available, which could happen when learning from offline log data or in the case of apprentice learning."}, {"heading": "4.3. Comparing different policy gradients", "text": "As can be seen from Figure 6, leave-one-out with \u03b2 = 1 performed slightly better than leave-one-out with \u03b2 = min(1/\u00b5, 5) for both memorized and estimated behaviour probability values. Both algorithms performed better than TISLR for memorized behavioural probabilities and performed similarly when probabilities were estimated."}, {"heading": "4.4. Comparing different architectures", "text": "In order to compare different algorithms, we evaluated the best Reactor version obtained from Figure 6 by evaluating it on 200 episodes with 30 random human starts on each Atari game. We evaluated mean episode rewards for each atari game and human normalized scores. We calculated mean and median human normalized scores across all games. In addition to that, we ranked all algorithms (including random and human scores) for each game and evaluated mean rank of each algorithm across all 57 Atari games. We believe that this is a better metric to compare algorithms across different games with vastly different rewards scales because it is less sensitive to the errors introduced when measuring human play, which otherwise is present in the denominator when calculating human normalized scores. We also evaluated an Elo scores for each algorithm, where algorithm A is considered to win over al-\ngorithm B if it collects more score on a given Atari game when using 30 random human starts. In our Elo evaluation difference in ranking 400 correspond to the odds of winning of 10:1.\nTable 2 contains a comparison of our algorithm with several other synchronous state-of-art algorithms across 57 Atari games for a fixed random seed across all games (Bellemare et al., 2013). The algorithms that we compare Reactor to are: DQN (Mnih et al., 2015), Double DQN (Van Hasselt et al., 2016), DQN with prioritized experience replay (Schaul et al., 2015), duelling architecture and prioritized duelling architecture (Wang et al., 2015). Each algorithm was exposed to 200 million frames of experience and the\nsame pre-processing pipeline including 4 action repeats was used as in the original DQN paper (Mnih et al., 2015). We tested the version of Reactor which memorized past behaviour probabilities and used \u03b2 = 1. Reactor was able to improve over prioritized duelling agent in terms of median human normalized scores, mean rank and Elo, but was slightly worse in terms of mean human normalized scores.\nTable 3 contains a comparison of our algorithm with ACER algorithm (Wang et al., 2017). In the case of Reactor, the table was produced in the following way. While the agent was training on each game, scores were averaged over 200 windows of length 1 million. The maximum value was taken from each curve and it was human-normalized. A median value was evaluated for each algorithm across all games. The best value for ACER was obtained from Figure 1 in (Wang et al., 2017) by reading off the highest learning curve below 200 million steps. As it is seen from the results two versions of Reactor improved over the ACER algorithm in terms of median human normalized scores. On the other hand our evaluation was somewhat more pessimistic than the one used by (Wang et al., 2017), because we averaged rewards over windows of 1 million steps, while in the case of ACER the rewards were averaged over windows of 200 thousand frames. Averaging over longer windows leads to less variance, which in turn introduces less optimistic bias encountered during the maximization process. We were not able to evaluate Rank and Elo scores for ACER because we did not have per-game evaluations. One interesting finding was that our implementation of TISLR algorithm with learnt probabilities achieved better results than the ones reported in the ACER paper when using learn behavioural probabilities."}, {"heading": "5. Conclusion", "text": "In this work we presented a new off-policy agent based on Retrace actor critic architecture and demonstrated that it can achieve similar performance as the current state of the art. We also demonstrated that estimated behavioural probabilities can outperformed memorized behavioural probabilities under some circumstances when used for off-policy correction. As the framework is fully off-policy, it can be used to test different exploration ideas."}, {"heading": "6. Appendix", "text": ""}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Toward minimax off-policy value estimation", "author": ["Li", "Lihong", "Munos", "R\u00e9mi", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["Munos", "R\u00e9mi", "Stepleton", "Tom", "Harutyunyan", "Anna", "Bellemare", "Marc"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Understanding and improving convolutional neural networks via concatenated rectified linear units", "author": ["Shang", "Wenling", "Sohn", "Kihyuk", "Almeida", "Diogo", "Lee", "Honglak"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Shang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "Mcallester", "David", "Singh", "Satinder", "Mansour", "Yishay"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "Schaul", "Tom", "Hessel", "Matteo", "van Hasselt", "Hado", "Lanctot", "Marc", "de Freitas", "Nando"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wang et al\\.", "year": 1995}, {"title": "Sample efficient actor-critic with experience replay", "author": ["Wang", "Ziyu", "Bapst", "Victor", "Heess", "Nicolas", "Mnih", "Volodymyr", "Munos", "Remi", "Kavukcuoglu", "Koray", "de Freitas", "Nando"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Model free deep reinforcement learning has achieved remarkable success lately in many domains ranging from achieving human and super-human level control in video games (Mnih et al., 2016), (Mnih et al.", "startOffset": 168, "endOffset": 187}, {"referenceID": 6, "context": ", 2015) to continuous motor control tasks (Lillicrap et al., 2015).", "startOffset": 42, "endOffset": 66}, {"referenceID": 7, "context": ", 2015) and A3C (Mnih et al., 2016), have several disadvantages: DQN suffers from potentially slow learning caused by single-step temporal difference updates, while A3C has a relatively low data-efficiency (since it does not use a memory buffer) and requires being trained on several copies of the same environment simultaneously.", "startOffset": 16, "endOffset": 35}, {"referenceID": 8, "context": "The critic implements the Retrace (Munos et al., 2016) algorithm, while the actor is trained by a new policy gradient algorithm, called \u03b2-leave-one-out, which makes use of both the off-policy Retrace-corrected return and the estimated Q-function.", "startOffset": 34, "endOffset": 54}, {"referenceID": 7, "context": "Related works: Like A3C (Mnih et al., 2016), Reactor is an actor-critic multi-step returns algorithm.", "startOffset": 24, "endOffset": 43}, {"referenceID": 6, "context": "DDPG (Lillicrap et al., 2015) uses an actor-critic architecture but in a continuous control setting making use of the deterministic policy gradient algorithm and does not consider multi-step returns.", "startOffset": 5, "endOffset": 29}, {"referenceID": 2, "context": "UNREAL agent (Jaderberg et al., 2016) improved final performance and data efficiency of A3C framework by introducing replay memory and unsupervised auxiliary tasks.", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "The closest work to ours is the ACER algorithm (Wang et al., 2017) which is also an actor-critic based on the Retrace algorithm which makes use of memory replay.", "startOffset": 47, "endOffset": 66}, {"referenceID": 8, "context": "For the critic we use the Retrace(\u03bb) algorithm introduced in (Munos et al., 2016).", "startOffset": 61, "endOffset": 81}, {"referenceID": 8, "context": "the Q estimates, see (Munos et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "Let V (x0) be the value function at some initial state x0, the policy gradient theorem (Sutton et al., 2000) says that \u2207V (x0) = E [\u2211 t \u03b3 t \u2211 aQ (xt, a)\u2207\u03c0(a|xt) ] , where \u2207 refers to the gradient w.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "This truncated 1/\u03bc coefficient shares similarities with the truncated IS gradient estimate introduced in (Wang et al., 2017) (which we call TISLR for truncated-ISLR):", "startOffset": 105, "endOffset": 124}, {"referenceID": 8, "context": "Instead, in our implementation we use the off-policy corrected return computed by the Retrace algorithm, which produces a (biased) estimate ofQ(A) but whose bias vanishes asymptotically, see (Munos et al., 2016).", "startOffset": 191, "endOffset": 211}, {"referenceID": 5, "context": "Note that some theoretical findings (Li et al., 2015) support that using a regression estimate may indeed improve importance sampling.", "startOffset": 36, "endOffset": 53}, {"referenceID": 4, "context": "All three LSTMs in turn are connected to a shared linear layer which is connected to a shared convolutional neural network (Krizhevsky et al., 2012).", "startOffset": 123, "endOffset": 148}, {"referenceID": 10, "context": "Concatenated rectified linear units are used as nonlinearities in the network (Shang et al., 2016).", "startOffset": 78, "endOffset": 98}, {"referenceID": 10, "context": "Firstly, as was shown by (Shang et al., 2016), convolutional neural networks with rectified linear units tend to lead to pairs of opposite features.", "startOffset": 25, "endOffset": 45}, {"referenceID": 7, "context": ", 2015) Reactor is able to improve CPU/GPU utilization by learning while acting, and comparing to A3C framework (Mnih et al., 2016) we can exploit computational benefits of batched learning.", "startOffset": 112, "endOffset": 131}, {"referenceID": 14, "context": "We reported the final performance by taking the best training curve value from each learning curve in order to make evaluation comparable to the results reported by (Wang et al., 2017).", "startOffset": 165, "endOffset": 184}, {"referenceID": 0, "context": "Table 2 contains a comparison of our algorithm with several other synchronous state-of-art algorithms across 57 Atari games for a fixed random seed across all games (Bellemare et al., 2013).", "startOffset": 165, "endOffset": 189}, {"referenceID": 9, "context": ", 2016), DQN with prioritized experience replay (Schaul et al., 2015), duelling architecture and prioritized duelling architecture (Wang et al.", "startOffset": 48, "endOffset": 69}, {"referenceID": 14, "context": "Table 3 contains a comparison of our algorithm with ACER algorithm (Wang et al., 2017).", "startOffset": 67, "endOffset": 86}, {"referenceID": 14, "context": "The best value for ACER was obtained from Figure 1 in (Wang et al., 2017) by reading off the highest learning curve below 200 million steps.", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": "On the other hand our evaluation was somewhat more pessimistic than the one used by (Wang et al., 2017), because we averaged rewards over windows of 1 million steps, while in the case of ACER the rewards were averaged over windows of 200 thousand frames.", "startOffset": 84, "endOffset": 103}], "year": 2017, "abstractText": "In this work we present a new reinforcement learning agent, called Reactor (for Retraceactor), based on an off-policy multi-step return actor-critic architecture. The agent uses a deep recurrent neural network for function approximation. The network outputs a target policy \u03c0 (the actor), an action-value Q-function (the critic) evaluating the current policy \u03c0, and an estimated behavioural policy \u03bc\u0302 which we use for off-policy correction. The agent maintains a memory buffer filled with past experiences. The critic is trained by the multi-step off-policy Retrace algorithm and the actor is trained by a novel \u03b2-leave-oneout policy gradient estimate (which uses both the off-policy corrected return and the estimated Qfunction). The Reactor is sample-efficient thanks to the use of memory replay, and numerical efficient since it uses multi-step returns. Also both acting and learning can be parallelized. We evaluated our algorithm on 57 Atari 2600 games and demonstrate that it achieves state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}