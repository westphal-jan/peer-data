{"id": "1704.04960", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Adversarial and Clean Data Are Not Twins", "abstract": "Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into wrong predictions with very high confidence. In this paper, however, we show that we can build a simple binary classifier separating the adversarial apart from the clean data with accuracy over 99% of the time.\n\n\n\n\n\nWith all this in mind, here are some of the most promising results from Deep Neural Networks. For example, the following techniques were tested using the DNN (a type of classifier which is called a \"defensive defence\"). These neural networks are a series of recurrent neural networks. These neural networks are called the \"SNS\" (a kind of deep neural network that combines convolutional neural networks) and include a small set of neural networks called \"problems\".\nThe first task in this classifier was to perform a Deep Neural Network in parallel to our original convolutional neural networks. The convolutional neural networks are able to perform their own classification of the adversarial. The problem of this is that it is impossible to have the best algorithms. Instead, it's very hard to show a perfect classification between two adversarial networkes. This problem is exacerbated by the difficulty of using a deep neural network for classification of the adversarial. The problem is that it is difficult to compute a complete classification. Even after the first task, the deep neural network cannot perform this classification. The problem is that it can only perform the two adversarial systems at once.\nThe first step is to analyze the Deep Neural Network and examine all of the adversarial elements in the graph. Using the Deep Neural Network, we will explore all of the parameters of the neural network and learn how they interact with each other. The Deep Neural Network, on the other hand, is a simple representation of the two neural networks.\nIn addition to being able to show the depth of the deep neural network, we will also be able to detect the complexity of the deep neural network. In other words, we will be able to show how deep the network is.\nIn the next paper, we will explore the complexity of the neural network and analyze the complexity of the neural network.\nThe deep neural network is a great example of deep neural networks. It is the only network which can perform deep neural networks. Deep Neural networks can produce a complex set of possible problems which will affect the performance of deep neural networks.\nOne of the", "histories": [["v1", "Mon, 17 Apr 2017 13:25:17 GMT  (85kb,D)", "http://arxiv.org/abs/1704.04960v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhitao gong", "wenlu wang", "wei-shinn ku"], "accepted": false, "id": "1704.04960"}, "pdf": {"name": "1704.04960.pdf", "metadata": {"source": "META", "title": "Adversarial and Clean Data Are Not Twins", "authors": ["Zhitao Gong", "Wenlu Wang", "Wei-Shinn Ku"], "emails": ["<gong@auburn.edu>."], "sections": [{"heading": "1. Introduction", "text": "Deep neural networks have been successfully adopted to many life critical areas, e.g., skin cancer detection (Esteva et al., 2017), auto-driving (Santana & Hotz, 2016), traffic sign classification (Cires\u0327An et al., 2012), etc. A recent study (Szegedy et al., 2013), however, discovered that deep neural networks are susceptible to adversarial images. Figure 1 shows an example of adversarial images generated via fast gradient sign method (Kurakin et al., 2016; Kurakin et al., 2016) on MNIST. As we can see that although the adversarial and original clean images are almost identical from the perspective of human beings, the deep neural network\n1Auburn University, Auburn, AL. Correspondence to: Zhitao Gong <gong@auburn.edu>.\nwill produce wrong predictions with very high confidence. Similar techniques can easily fool the image system into mistaking a stop sign for a yield sign, a dog for a automobile, for example. When leveraged by malicious users, these adversarial images pose a great threat to the deep neural network systems.\nAlthough adversarial and clean images appear visually indiscernible, their subtle differences can successfully fool the deep neural networks. This means that deep neural networks are sensitive to these subtle differences. So an intuitively question to ask is: can we leverage these subtle differences to distinguish between adversarial and clean images? Our experiment suggests the answer is positive. In this paper we demonstrate that a simple binary classifier can separate the adversarial from the original clean images with very high accuracy (over 99%). However, we also show that the binary classifier approach suffers from the generalization limitation, i.e., it is sensitive 1) to a hyper-parameter used in crafting adversarial dataset, and 2) to different adversarial crafting algorithms. In addition to that, we also discovered that this limitation is also shared among other proposed methods against adversarial attacking, e.g., defensive retraining (Huang et al., 2015; Kurakin et al., 2016), knowledge distillation (Papernot et al., 2015b), etc. We empirically investigate the limitation and propose the hypothesis that the adversarial and original dataset are, in effect, two completely different datasets, despite being visually similar.\nThis article is organized as follows. In Section 2, we give an overview of the current research in adversarial attack and defense, with a focus on deep neural networks. Then, it is followed by a brief summary of the state-of-the-art adversarial crafting algorithms in Section 3. Section 4 presents our experiment results and detailed discussions. And we conclude in Section 5.\nar X\niv :1\n70 4.\n04 96\n0v 1\n[ cs\n.L G\n] 1\n7 A\npr 2\n01 7"}, {"heading": "2. Related Work", "text": "The adversarial image attack on deep neural networks was first investigated in (Szegedy et al., 2013). The authors discovered that when added some imperceptible carefully chosen noise, an image may be wrongly classified with high confidence by a well-trained deep neural network. They also proposed an adversarial crafting algorithm based on optimization. We will briefly summarize it in section 3. They also proposed the hypothesis that the adversarial samples exist as a result of the high nonlinearity of deep neural network models.\nHowever, (Goodfellow et al., 2014) proposed a counter-intuitive hypothesis explaining the cause of adversarial samples. They argued that adversarial samples are caused by the models being too linear, rather than nonlinear. They proposed two adversarial crafting algorithms based on this hypothesis, i.e., fast gradient sign method (FGSM) and least-likely class method (LLCM) (Goodfellow et al., 2014). The least-likely class method is later generalized to target class gradient sign method (TGSM) in (Kurakin et al., 2016).\n(Papernot et al., 2015a) proposed another gradient based adversarial algorithm, the Jacobian-based saliency map approach (JSMA) which can successfully alter the label of an image to any desired category.\nThe adversarial images have been shown to be transferable among deep neural networks (Szegedy et al., 2013; Kurakin et al., 2016). This poses a great threat to current learning systems in that the attacker needs not the knowledge of the target system. Instead, the attacker can train a different model to create adversar-\nial samples which are still effective for the target deep neural networks. What\u2019s worse, (Papernot et al., 2016) has shown that adversarial samples are even transferable among different machine learning techniques, e.g., deep neural networks, support vector machine, decision tree, logistic regression, etc.\nSmall steps have been made towards the defense of adversarial images. (Kurakin et al., 2016) shows that some image transformations, e.g., Gaussian noise, Gaussian filter, JPEG compression, etc., can effectively recover over 80% of the adversarial images. However, in our experiment, the image transformation defense does not perform well on images with low resolution, e.g., MNIST. Knowledge distillation is also shown to be an effective method against most adversarial images (Papernot et al., 2015b). The restrictions of defensive knowledge distillation are 1) that it only applies to models that produce categorical probabilities, and 2) that it needs model retraining. Adversarial training (Kurakin et al., 2016; Huang et al., 2015) was also shown to greatly enhance the model robustness to adversarials. However, as discussed in Section 4.2, defensive distillation and adversarial training suffers from, what we call, the generalization limitations. Our experiment suggests this seems to be an intrinsic property of adversarial datasets."}, {"heading": "3. Crafting Adversarials", "text": "The are mainly two categories of algorithms to generate adversarial samples, model independent and model dependent. We briefly summarize these two classes of methods in this section.\nBy conventions, we useX to represent input image set (usually a 3-dimension tensor), and Y to represent the label set, usually one-hot encoded. Lowercase represents an individual data sample, e.g., x for one input image. Subscript to data samples denotes one of its elements, e.g., xi denotes one pixel in the image, yi denotes probability for the i-th target class. f denotes the model, \u03b8 the model parameter, J the loss function. We use the superscript adv to denote adversarial related variables, e.g., xadv for one adversarial image. \u03b4x denotes the adversarial noise for one image, i.e., xadv = x+ \u03b4x. For clarity, we also include the model used to craft the adversarial samples where necessary, e.g., xadv(f1) denotes the adversarial samples created with model f1. D denotes the image value domain, usually [0, 1] or [0, 255]. And is a scalar controlling the scale of the adversarial noise, another hyper-parameter to choose."}, {"heading": "3.1. Model Independent Method", "text": "A box-constrained minimization algorithm based on L-BFGS was the first algorithm proposed to generate adversarial data (Szegedy et al., 2013). Concretely we want to find the smallest (in the sense of L2-norm) noise \u03b4x such that the adversarial image belongs to a different category, i.e., f(xadv) 6= f(x).\n\u03b4x = argmin r\nc\u2016r\u2016\u221e + J(x+ r, y adv)\ns.t. x+ r \u2208 D (1)"}, {"heading": "3.2. Model Dependent Methods", "text": "There are mainly three methods that rely on model gradient, i.e., fast gradient sign method (FGSM) (Kurakin et al., 2016), target class method (Kurakin et al., 2016; Kurakin et al., 2016) (TGSM) and Jacobianbased saliency map approach (JSMA) (Papernot et al., 2015a). We will see in Section 4 that despite that they all produce highly disguising adversarials, FGSM and TGSM produce compatible adversarial datasets which are complete different from adversarials generated via JSMA.\nFA S T G R A D I E N T S I G N M E T H O D ( F G S M )\nFGSM tries to modify the input towards the direction where J increases, i.e., dJ(x, yadv) / dx , as shown in\nEquation 2.\n\u03b4x = sign\n( dJ(x, y)\ndx\n) (2)\nOriginally (Kurakin et al., 2016) proposes to generate adversarial samples by using the true label i.e., yadv = ytrue, which has been shown to suffer from the label leaking problem (Kurakin et al., 2016). Instead of true labels, (Kurakin et al., 2016) proposes to use the predicted label, i.e., y = f(x), to generate adversarial examples.\nThis method can also be used iteratively as shown in Equation 3. Iterative FGSM has much higher success rate than the one-step FGSM. However, the iterative version is less robust to image transformation (Kurakin et al., 2016).\nxadvk+1 = x adv k + sign\n( dJ(xadvk , yk)\ndx ) xadv0 = x\nyk = f(x adv k )\n(3)\nTA R G E T C L A S S G R A D I E N T S I G N M E T H O D ( T G S M )\nThis method tries to modify the input towards the direction where p(yadv | x) increases.\n\u03b4x = \u2212 sign ( dJ(x, yadv)\ndx\n) (4)\nOriginally this method was proposed as the least-likely class method (Kurakin et al., 2016) where yadv was chosen as the least-likely class predicted by the model as shown in Equation 5.\nyadv = OneHotEncode (argmin f(x)) (5)\nAnd it was extended to a more general case where yadv could be any desired target class (Kurakin et al., 2016).\nJA C O B I A N - B A S E D S A L I E N C Y M A P A P P R O A C H ( J S M A )\nSimilar to the target class method, JSMA (Papernot et al., 2015a) allows to specify the desired target class. However, instead of adding noise to the whole input,\nJSMA changes only one pixel at a time. A saliency score is calculated for each pixel and pixel with the highest score is chosen to be perturbed.\ns(xi) = { 0 if st < 0 or so > 0 st|so| otherwise\nst = \u2202yt \u2202xi\nso = \u2211 j 6=t \u2202yj \u2202xi\n(6)\nConcretely, st is the Jacobian value of the desired target class yt w.r.t an individual pixel, so is the sum of Jacobian values of all non-target class. Intuitively, saliency score indicates the sensitivity of each output class w.r.t each individual pixel. And we want to perturb the pixel towards the direction where p(yt | x) increases the most."}, {"heading": "4. Experiment", "text": "Generally, we follow the steps below to test the effectiveness and limitation of the binary classifier approach.\n1. Train a deep neural network f1 on the original clean training data Xtrain, and craft adversarial dataset from the original clean data, Xtrain \u2192 X adv(f1) train , Xtest \u2192 X adv(f1) test . f1 is used to gen-\nerate the attacking adversarial dataset which we want to filter out.\n2. Train a binary classifier f2 on the combined (shuffled) training data {Xtrain, Xadv(f1)train }, where Xtrain is labeled 0 and X adv(f1) train labeled 1.\n3. Test the accuracy of f2 on Xtest and X adv(f1) test ,\nrespectively.\n4. Construct second-round adversarial test data, {Xtest, Xadv(f1)test } \u2192 {Xtest, X adv(f1) test }adv(f2)\nand test f2 accuracy on this new adversarial dataset. Concretely, we want to test whether we could find adversarial samples 1) that can successfully bypass the binary classifier f2, and 2) that can still fool the target model f1 if they bypass the binary classifier. Since adversarial datasets are shown to be transferable among different machine learning techniques (Papernot et al., 2016), the binary classifier approach will be seriously flawed if f2 failed this second-round attacking test.\nThe code to reproduce our experiment are available https://github.com/gongzhitaao/ adversarial-classifier."}, {"heading": "4.1. Efficiency and Robustness of the Classifier", "text": "We evaluate the binary classifier approach on MNIST, CIFAR10, and SVHN datasets. Of all the datasets, the binary classifier achieved accuracy over 99% and was shown to be robust to a second-round adversarial attack. The results are summarized in Table 1. Each column denotes the model accuracy on the corresponding dataset. The direct conclusions from Table 1 are summarized as follows.\n1. Accuracy on Xtest and X adv(f1) test suggests that\nthe binary classifier is very effective at separating adversarial from clean dataset. Actually during our experiment, the accuracy on Xtest is always near 1, while the accuracy on Xadv(f1)test is either near 1 (successful) or near 0 (unsuccessful). Which means that the classifier either successfully detects the subtle difference completely or fails completely. We did not observe any values in between.\n2. Accuracy on {Xadv(f1)test }adv(f2) suggests that we were not successful in disguising adversarial\nsamples to bypass the the classifier. In other words, the binary classifier approach is robust to a second-round adversarial attack.\n3. Accuracy on {Xtest}adv(f2) suggests that in case of the second-round attack, the binary classifier has very high false negative. In other words, it tends to recognize them all as adversarials. This, does not pose a problem in our opinion. Since our main focus is to block adversarial samples."}, {"heading": "4.2. Generalization Limitation", "text": "Before we conclude too optimistic about the binary classifier approach performance, however, we discover that it suffers from the generalization limitation.\n1. When trained to recognize adversarial dataset generated via FGSM/TGSM, the binary classifier is sensitive to the hyper-parameter .\n2. The binary classifier is also sensitive to the adversarial crafting algorithm.\nIn out experiment, the aforementioned limitations also apply to adversarial training (Kurakin et al., 2016; Huang et al., 2015) and defensive distillation (Papernot et al., 2015b).\nS E N S I T I V I T Y T O\nTable 2 summarizes our tests on CIFAR10. For brevity, we use f2 \u2223\u2223 = 0\nto denote that the classifier f2 is trained on adversarial data generated on f1 with = 0. The binary classifier is trained on mixed clean data and adversarial dataset which is generated via FGSM with = 0.03. Then we re-generate adversarial dataset via FGSM/TGSM with different values.\nTable 2. sensitivity on CIFAR10\nf2\n\u2223\u2223\n=0.03\nXtest X adv(f1) test\n0.3 0.9996 1.0000 0.1 0.9996 1.0000 0.03 0.9996 0.9997 0.01 0.9996 0.0030\nAs shown in Table 2, f2 \u2223\u2223 = 0\ncan correctly filter out adversarial dataset generated with \u2265 0, but fails when\nadversarial data are generated with < 1. Results on MNIST and SVHN are similar. This phenomenon was also observed in defensive retraining (Kurakin et al., 2016). To overcome this issue, they proposed to use mixed values to generate the adversarial datasets. However, Table 2 suggests that adversarial datasets generated with smaller are superset of those generated with larger . This hypothesis could be well explained by the linearity hypothesis (Kurakin et al., 2016; Warde-Farley & Goodfellow, 2016). The same conclusion also applies to adversarial training. In our experiment, the results of defensive retraining are similar to the binary classifier approach.\nD I S PA R I T Y A M O N G A D V E R S A R I A L S A M P L E S\nIn our experiment, we also discovered that the binary classifier is also sensitive to the algorithms used to generate the adversarial datasets.\nSpecifically, the binary classifier trained on FGSM adversarial dataset achieves good accuracy (over 99%) on FGSM adversarial dataset, but not on adversarial generated via JSMA, and vise versa. However, when binary classifier is trained on a mixed adversarial dataset from FGSM and JSMA, it performs well (with accuracy over 99%) on both datasets. This suggests that FGSM and JSMA generate adversarial datasets that are far away from each other. It is too vague without defining precisely what is being far away. In our opinion, they are far away in the same way that CIFAR10 is far away from SVHN. A well-trained model on CIFAR10 will perform poorly on SVHN, and vise versa. However, a well-trained model on the the mixed dataset of CIFAR10 and SVHN will perform just as well, if not better, on both datasets, as if it is trained solely on one dataset.\nThe adversarial datasets generated via FGSM and TGSM are, however, compatible with each other. In other words, the classifier trained on one adversarial datasets performs well on adversarials from the other algorithm. They are compatible in the same way that training set and test set are compatible. Usually we expect a model, when properly trained, should generalize well to the unseen data from the same distribution, e.g., the test dataset.\nIn effect, it is not just FGSM and JSMA are incompati-\nrepresents the last adversarial sample created from x via FGSM that is used in the adversarial training and the blue dot represents a random adversarial sample created from x that cannot be recognized with adversarial training. The three digits below each image, from left to right, are the data samples that correspond to the black dot, orange dot and blue dot, respectively. ( ) represents the data samples that are always correctly (incorrectly) recognized by the model. represents the adversarial samples that can be correctly recognized without adversarial training only. And represents the data points that were correctly recognized with adversarial training only, i.e., the side effect of adversarial training.\nble. We can generate adversarial data samples by a linear combination of the direction computed by FGSM and another random orthogonal direction, as illustrated in a church plot (Warde-Farley & Goodfellow, 2016) Figure 2. Figure 2 visually shows the effect of adversarial training (Kurakin et al., 2016). Each image represents adversarial samples generated from one data sample, which is represented as a black dot in the center of each image, the last adversarial sample used in adversarial training is represented as an orange dot (on the right of black dot, i.e., in the direction computed by FGSM). The green area represents the adversarial samples that cannot be correctly recognized without adversarial training but can be correctly recognized with adversarial training. The red area represents data samples that can be correctly recognized without adversarial training but cannot be correctly recognized with adversarial training. In other words, it represents the side effect of adversarial training, i.e., slightly reducing the model accuracy. The white (gray) area represents the data samples that are always correctly (incorrectly) recognized with or without adversarial training.\nAs we can see from Figure 2, adversarial training does make the model more robust against the adversarial sample (and adversarial samples around it to some extent) used for training (green area). However, it does not rule out all adversarials. There are still adversarial samples (gray area) that are not affected by the adversarial training. Further more, we could observe that the green area largely distributes along the horizontal direction, i.e., the FGSM direction. In (Nguyen et al., 2014), they observed similar results for fooling images. In their experiment, adversarial training with fooling\nimages, deep neural network models are more robust against a limited set of fooling images. However they can still be fooled by other fooling images easily."}, {"heading": "5. Conclusion", "text": "We show in this paper that the binary classifier is a simple yet effective and robust way to separating adversarial from the original clean images. Its advantage over defensive retraining and distillation is that it serves as a preprocessing step without assumptions about the model it protects. Besides, it can be readily deployed without any modification of the underlying systems. However, as we empirically showed in the experiment, the binary classifier approach, defensive retraining and distillation all suffer from the generalization limitation. For future work, we plan to extend our current work in two directions. First, we want to investigate the disparity between different adversarial crafting methods and its effect on the generated adversarial space. Second, we will also carefully examine the cause of adversarial samples since intuitively the linear hypothesis does not seem right to us."}], "references": [{"title": "Multi-column deep neural network for traffic sign classification", "author": ["Cire\u015fAn", "Dan", "Meier", "Ueli", "Masci", "Jonathan", "Schmidhuber", "J\u00fcrgen"], "venue": null, "citeRegEx": "Cire\u015fAn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cire\u015fAn et al\\.", "year": 2012}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning with a strong adversary", "author": ["Huang", "Ruitong", "Xu", "Bing", "Schuurmans", "Dale", "Szepesv\u00e1ri", "Csaba"], "venue": "URL http:// arxiv.org/abs/1511.03034", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Adversarial Examples in the Physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": null, "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Adversarial machine learning at scale. abs/1611.01236, 2016", "author": ["Kurakin", "Alexey", "Goodfellow", "Ian J", "Bengio", "Samy"], "venue": "URL http://arxiv. org/abs/1611.01236", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Nguyen", "Anh Mai", "Yosinski", "Jason", "Clune", "Jeff"], "venue": "images. abs/1412.1897,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Transferability in Machine Learning: From Phenomena To Black-Box Attacks Using Adversarial Samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick", "Jha", "Somesh", "Fredrikson", "Matt", "Celik", "Z. Berkay", "Swami", "Ananthram"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Papernot", "Nicolas", "McDaniel", "Patrick Drew", "Wu", "Xi", "Jha", "Somesh", "Swami", "Ananthram"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Learning a driving", "author": ["Santana", "Eder", "Hotz", "George"], "venue": "simulator. abs/1608.01230,", "citeRegEx": "Santana et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santana et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Ian J", "Fergus", "Rob"], "venue": "URL http: //arxiv.org/abs/1312.6199", "citeRegEx": "J. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "J. et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", 2017), auto-driving (Santana & Hotz, 2016), traffic sign classification (Cire\u015fAn et al., 2012), etc.", "startOffset": 74, "endOffset": 96}, {"referenceID": 3, "context": "Figure 1 shows an example of adversarial images generated via fast gradient sign method (Kurakin et al., 2016; Kurakin et al., 2016) on MNIST.", "startOffset": 88, "endOffset": 132}, {"referenceID": 3, "context": "Figure 1 shows an example of adversarial images generated via fast gradient sign method (Kurakin et al., 2016; Kurakin et al., 2016) on MNIST.", "startOffset": 88, "endOffset": 132}, {"referenceID": 2, "context": ", defensive retraining (Huang et al., 2015; Kurakin et al., 2016), knowledge distillation (Papernot et al.", "startOffset": 23, "endOffset": 65}, {"referenceID": 3, "context": ", defensive retraining (Huang et al., 2015; Kurakin et al., 2016), knowledge distillation (Papernot et al.", "startOffset": 23, "endOffset": 65}, {"referenceID": 1, "context": "However, (Goodfellow et al., 2014) proposed a counter-intuitive hypothesis explaining the cause of adversarial samples.", "startOffset": 9, "endOffset": 34}, {"referenceID": 1, "context": ", fast gradient sign method (FGSM) and least-likely class method (LLCM) (Goodfellow et al., 2014).", "startOffset": 72, "endOffset": 97}, {"referenceID": 3, "context": "The least-likely class method is later generalized to target class gradient sign method (TGSM) in (Kurakin et al., 2016).", "startOffset": 98, "endOffset": 120}, {"referenceID": 3, "context": "The adversarial images have been shown to be transferable among deep neural networks (Szegedy et al., 2013; Kurakin et al., 2016).", "startOffset": 85, "endOffset": 129}, {"referenceID": 6, "context": "What\u2019s worse, (Papernot et al., 2016) has shown that adversarial samples are even transferable among different machine learning techniques, e.", "startOffset": 14, "endOffset": 37}, {"referenceID": 3, "context": "(Kurakin et al., 2016) shows that some image transformations, e.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Adversarial training (Kurakin et al., 2016; Huang et al., 2015) was also shown to greatly enhance the model robustness to adversarials.", "startOffset": 21, "endOffset": 63}, {"referenceID": 2, "context": "Adversarial training (Kurakin et al., 2016; Huang et al., 2015) was also shown to greatly enhance the model robustness to adversarials.", "startOffset": 21, "endOffset": 63}, {"referenceID": 3, "context": ", fast gradient sign method (FGSM) (Kurakin et al., 2016), target class method (Kurakin et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 3, "context": ", 2016), target class method (Kurakin et al., 2016; Kurakin et al., 2016) (TGSM) and Jacobianbased saliency map approach (JSMA) (Papernot et al.", "startOffset": 29, "endOffset": 73}, {"referenceID": 3, "context": ", 2016), target class method (Kurakin et al., 2016; Kurakin et al., 2016) (TGSM) and Jacobianbased saliency map approach (JSMA) (Papernot et al.", "startOffset": 29, "endOffset": 73}, {"referenceID": 3, "context": "Originally (Kurakin et al., 2016) proposes to generate adversarial samples by using the true label i.", "startOffset": 11, "endOffset": 33}, {"referenceID": 3, "context": ", yadv = ytrue, which has been shown to suffer from the label leaking problem (Kurakin et al., 2016).", "startOffset": 78, "endOffset": 100}, {"referenceID": 3, "context": "Instead of true labels, (Kurakin et al., 2016) proposes to use the predicted label, i.", "startOffset": 24, "endOffset": 46}, {"referenceID": 3, "context": "However, the iterative version is less robust to image transformation (Kurakin et al., 2016).", "startOffset": 70, "endOffset": 92}, {"referenceID": 3, "context": "Originally this method was proposed as the least-likely class method (Kurakin et al., 2016) where yadv was chosen as the least-likely class predicted by the model as shown in Equation 5.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": "And it was extended to a more general case where yadv could be any desired target class (Kurakin et al., 2016).", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": "Since adversarial datasets are shown to be transferable among different machine learning techniques (Papernot et al., 2016), the binary classifier approach will be seriously flawed if f2 failed this second-round attacking test.", "startOffset": 100, "endOffset": 123}, {"referenceID": 3, "context": "In out experiment, the aforementioned limitations also apply to adversarial training (Kurakin et al., 2016; Huang et al., 2015) and defensive distillation (Papernot et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 2, "context": "In out experiment, the aforementioned limitations also apply to adversarial training (Kurakin et al., 2016; Huang et al., 2015) and defensive distillation (Papernot et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 3, "context": "This phenomenon was also observed in defensive retraining (Kurakin et al., 2016).", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "This hypothesis could be well explained by the linearity hypothesis (Kurakin et al., 2016; Warde-Farley & Goodfellow, 2016).", "startOffset": 68, "endOffset": 123}, {"referenceID": 2, "context": "Adversarial training (Huang et al., 2015; Kurakin et al., 2016) does not work.", "startOffset": 21, "endOffset": 63}, {"referenceID": 3, "context": "Adversarial training (Huang et al., 2015; Kurakin et al., 2016) does not work.", "startOffset": 21, "endOffset": 63}, {"referenceID": 3, "context": "Figure 2 visually shows the effect of adversarial training (Kurakin et al., 2016).", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": "In (Nguyen et al., 2014), they observed similar results for fooling images.", "startOffset": 3, "endOffset": 24}], "year": 2017, "abstractText": "Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into wrong predictions with very high confidence. In this paper, however, we show that we can build a simple binary classifier separating the adversarial apart from the clean data with accuracy over 99%. We also empirically show that the binary classifier is robust to a secondround adversarial attack. In other words, it is difficult to disguise adversarial samples to bypass the binary classifier. Further more, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of intrinsic property of adversarial crafting algorithms.", "creator": "Emacs 25.1.1 (Org mode 9.0.5)"}}}