{"id": "1704.05550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics", "abstract": "Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 18 Apr 2017 22:21:22 GMT  (141kb,D)", "http://arxiv.org/abs/1704.05550v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["rakesh verma", "daniel lee"], "accepted": false, "id": "1704.05550"}, "pdf": {"name": "1704.05550.pdf", "metadata": {"source": "CRF", "title": "Extractive Summarization: Limits, Compression, Generalized Model and Heuristics", "authors": ["Rakesh Verma", "Daniel Lee"], "emails": ["rmverma@cs.uh.edu,", "dljr0122@cs.uh.edu"], "sections": [{"heading": "1 Introduction", "text": "Automatic text summarization is the holy grail for people battling information overload, which becomes more and more acute over time. Hence it has attracted many researchers from diverse fields since the 1950s. However, it has remained a serious challenge, especially in the case of single news articles. The single document summarization competition at Document Understanding Conferences (DUC) was abandoned after only two years, 2001-2002, since many automatic summarizers could not outperform a baseline summary consisting of the first 100 words of a news article. Those that did outperform the baseline could not do so in a statistically significant way [27]. Summarization can be extractive or abstractive [21]: in extractive summarization sentences are chosen from the article(s) given as input, whereas in abstractive summarization sentences may be generated or a new representation of the article(s) may be output.\nExtractive summarization is popular, so we explore whether there are inherent limits on the performance of such systems.1 We then generalize existing ? Research supported in part by NSF grants DUE 1241772, CNS 1319212 and DGE 1433817 1 Surprisingly, despite all the attention extractive summarization has received, to our knowledge,\nno one has explored this question before.\nar X\niv :1\n70 4.\n05 55\n0v 1\n[ cs\n.C L\n] 1\n8 A\nmodels for summarization and define compressibility of a document. We explore this concept for documents from three genres and then unify new and existing heuristics for summarization in a single framework. Our contributions:\n1. We show the limitations of single and multi-document extractive summarization when the comparison is with respect to gold-standard human-constructed abstractive summaries on DUC data (Section 3). (a) Specifically, we show that when the documents themselves from the\nDUC 2001-2002 datasets are compared using ROUGE [19] to abstractive summaries, the average Rouge-1 (unigram) recall is around 90%. On ROUGE evaluations, no extractive summarizer can do better than just returning the document itself (in practice it will do much worse because of the size constraint on summaries). (b) For multi-document summarization, we show limits in two ways: (i) we concatenate the documents in each set and examine how this \u201csuperdocument\u201d performs as a summary with respect to the manual abstractive summaries, and (ii) we study how each document measures up against the manual summaries and then average the performance of all the documents in each set. 2. Inspired by this view of documents as summaries, we introduce and explore a generalized model of summarization (Section 4) that unifies the three different dimensions: abstractive versus extractive, single versus multidocument and syntactic versus semantic. (a) We prove (in Appendix) that constructing certain extractive summaries\nis isomorphic to the min cover problem for sets, which shows that not only is the optimal summary problem NP-complete but it has a greedy heuristic that gives a multiplicative logarithmic approximation. (b) Based on our model, we can define the compressibility of a document. We study this notion for different genres of articles including: news articles, scientific articles and short stories. (c) We present new and existing heuristics for single-document summarization, which represent different time and compressibility trade-offs. We compare them against existing summarizers proven on DUC datasets.\nAlthough many metrics have been proposed (more in Section 2), we use ROUGE because of its popularity, ease of use and correlation with human evaluations."}, {"heading": "2 Related Work", "text": "Most of the summarization literature focuses on single-document and multidocument summarization algorithms and frameworks rather than limits on the\nperformance of summarization systems. As pointed out by [8], competitive summarization systems are typically extractive, selecting representative sentences, concatenating them and often compressing them to squeeze in more sentences within the constraint. The summarization literature is vast, so we refer the reader to the recent survey [11], which is fairly comprehensive for summarization research until 2015. Here, we give a sampling of the literature and focus more on recent research and/or evaluation work.\nSingle-document extractive summarization. For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions. In [28], the authors present results for single-document summarization on a subset of PLOS Medicine articles and DUC 2002 dataset without mentioning the number of articles used. An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17]. Several systems were compared against a newly-devised supervised method on a dataset from Yahoo in [24].\nMulti-document extractive summarization. For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35]. Supervised and semi-supervised learning based extractive summarization was studied in [34]. Of course, single-document summarization can be considered as a special case, but no experimental results are presented for this important special case in the papers cited in this paragraph.\nAbstractive summarization. Abstractive summarization systems include [5,12,6,20,30,7].\nFrameworks. Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].\nMetrics and Evaluation. Of course, ROUGE is not the only metric for evaluating summaries. Human evaluators were used at NIST for scoring summaries on seven different metrics such as linguistic quality, etc. There is also the Pyramid approach [29] and BE [32], for example. Our choice of ROUGE is based on its popularity, ease of use, and correlation with human assessment [19]. Our choice of ROUGE configurations includes the one that was found to be best according to the paper [14]."}, {"heading": "3 Limits on Extractive Summarization", "text": "In all instances the ROUGE evaluations include the best schemes as shown by [14], which are usually Rouge-2 (bigram) and Rouge-3 (trigram) with stemming and stopword elimination. We also include the results without stopword elimination. The only modification was if the original parameters limited the size of the generated summary; we removed that option."}, {"heading": "3.1 Single-document Summarization", "text": "To study limits on extractive summarization, we will pretend that the document is itself a summary that needs to be evaluated against the human (abstractive) summaries created by NIST experts. Of course, the \u201cprecision\u201d of such a summary will be very low, so we focus on recall (and F-score by letting the document get all its recall from the same size as the human summary (100 words)). Table 2 shows that, for the DUC 20022 dataset, when the document themselves are considered as summaries and evaluated against a set of 100-word human abstractive summaries, the average Rouge-1 (unigram) [19] score is approximately 91 %. Tables 1 through 4 and Figures 1 and 2 use the following abbreviations: (i) R-n means ROUGE metric using n-gram matching, and (ii) lowercase s denotes the use of stopword removal option.\nMetric \u00b5 \u03c3 Range R-1 0.907 0.045 0.57-1.00 R-1s 0.889 0.059 0.64-1.00 R-2 0.555 0.111 0.22-0.85 R-2s 0.509 0.117 0.21-0.87 R-3 0.372 0.124 0.04-0.75 R-3s 0.311 0.123 0.04-0.76 R-4 0.272 0.118 0.01-0.67 R-4s 0.204 0.112 0.01-0.68\nTable 2: Rouge Recall on DUC 2002, Document as summary.\nThis means that on the average about 9% of the words in the human abstractive summaries do not appear in the documents. Since extractive automatic summarizers extract all the sentences from the documents given to them for summarization, clearly no extractive summarizer can have Rouge-1 recall score\n2 2002 was the last year in which the single document summarization competition was held by NIST.\nhigher than the documents themselves on any dataset, and, in general, the recall score will be lower since the summaries are limited to 100 words whereas the documents themselves can be arbitrarily long. Thus, we establish a limit on the Rouge recall scores for extractive summarizers on the DUC datasets. The DUC 2002 dataset has 533 unique documents and most include two 100-word human abstractive summaries. We note that if extractive summaries are also exactly 100 words each, then the precision can also be no higher than recall score. In addition, since the F1-score is upper bounded by the highest possible recall score. Therefore in the single document summarization, no extractive summarizer can have an average F1-score better than about 91%. When considered in this light, the best current extractive single-document summarizers achieve about 54% of this limit on DUC datasets, e.g., see [2,17].\nROUGE insights In Table 2, comparing R-1 and R-1s, we can see an increase in the lower range of recall values with stopword removal. This occurred with Document #250 (App. C). Upon deeper analysis of ROUGE, we found that it does not remove numbers under stopword removal option. Document #250 had a table with several numbers. In addition ROUGE treats numbers with the comma character (and also decimals such as 7.3) as two different numbers (e.g. 50,000 become 50 and 000). This boosted the recall because after stopword removal, the summaries significantly decreased in unigram count, whereas the overlapping unigrams between document and summary did not drop as much. Another discovery is that documents with long descriptive explanations end up with lower recall values with stopword removal. Tabel 1 shows a steep drop on the lower range values from R-1 to R-1s. When looking at the lower scoring documents, the documents usually had explanations about events, whereas the summary skipped these explanations."}, {"heading": "3.2 Multi-document Extractive Summarization", "text": "For multi-document summarization, there are at least two different scenarios in which to explore limits on extractive summarization. The first is where documents belonging to the same topic are concatenated together into one superdocument and then it is treated as a summary. In the second, we compare each document as a summary with respect to the model summaries and then average the results for documents belonging to the same topic.\nFor multi-document summarization, experiments were done on data from DUC datasets for 2004 and 2005. The data was grouped into document clusters. Each cluster held documents that were about a single topic. For the 2004 competition (DUC 2004), we focused on the English document clusters. There were\na total of 50 document clusters and each document cluster had an average of 10 documents. DUC 2005 also had 50 documents clusters, however, there were a minimum of 25 documents for each set.\nPlease note that since the scores for R-3 and R-4 were quite low (best being 0.23) these scores are not reported here.\nSuper-document Approach Now we consider the overlap between the documents of a cluster with the human summaries of those clusters. So for this limit on recall, we create super-documents. Each super-document is the concatenation of all the documents for a given document set. These super-documents are then evaluated with ROUGE against the model human summaries. Any extractive summary is limited to only these words, so the recall of a perfect extractive system can only reach this limit. The results can be seen in Table 3 and Table 4.\nMetric \u00b5 \u03c3 Range R-1 0.969 0.018 0.88-0.99 R-1s 0.949 0.029 0.81-0.99 R-2 0.537 0.080 0.30-0.73 R-2s 0.396 0.087 0.18-0.64\nTable 4: ROUGE Recall on DUC 2005, Super-document as summary.\nAveraging Results of Individual Documents Here we show a different perspective on the upper limit of extractive systems. We treat each document as a summary to compare against the human summaries. Since all the documents are articles related to a specific topic, these documents can be viewed as a standalone perspective. For this experiment we obtained the ROUGE recall of each document and then averaged them for each cluster. The distribution of the averages are presented in Figure 1 and Figure 2. Here the best distribution average is only about 60% and 42% for DUC 2004 and DUC 2005, respectively. The best system did approximated 38% in DUC 2004 and 46% in DUC 2005"}, {"heading": "4 A General Model for Summarization", "text": "Now we introduce our model and study its implications. Consider the process of human summarization. The starting point is a document, which contains a\nsequence of sentences that in turn are sequences of words. However, when a human is given a document to summarize, the human does not choose full sentences to extract from the document like extractive summarizers. Rather, the human first tries to understand the document, i.e., builds an abstract mental representation of it, and then writes a summary of the document based on this.\nTherefore, we formulate a model for semantic summarization in the abstract world of thought units,3 which can be specialized to syntactic summarization by using words in place of thought units. We hypothesize that a document is a collection of thought units, some of which are more important than others, with a mapping of sentences to thought units. The natural mapping is that of implication or inclusion, but this could be partial implication, not necessarily full implication. That is, the mapping could associate a degree to represent that the sentence only includes the thought unit partially. A summary must be constructed from sentences, not necessarily in the document, that cover as many of the important thought units as possible, i.e., maximize the importance score of the thought units selected, within a given size constraintC. We now define it formally for single and multi-document summarization. Our model can naturally represent abstractive versus extractive dimension of summarization.\nLet S denote an infinite set of sentences, T an infinite set of thought units, and I : S \u00d7 T \u2192 R be a mapping that associates a non-negative real number for each sentence s and thought unit t that measures the degree to which the thought unit is implied by the sentence s. Given a document D, which is a finite sequence of sentences from S, let S(D) \u2282 S be the finite set of sentences in D and T (D) \u2282 T be the finite set of thought units of D. Once thoughts are assembled into sentences in a document with its sequencing (a train of thought)\n3 we prefer thought units because a sentence is defined as a complete thought\nand title(s), this imposes a certain ordering4 of importance on these thought units, which is denoted by a scoring function WD : T \u2192 R. The size of a document is denoted by |D|, which could be, for example, the total number of words or sentences in the document. A size constraint, C, for the summary, is a function of |D|, e.g., a percentage of |D|, or a fixed number of words or sentences in which case it is a constant function. A summary of D, denoted by summ(D) \u2282 S, is a finite sequence of sentences that attempts to represent the thought units of D as best as possible within the constraint C. The size of a summary, |summ(D)| is measured using the same procedure for measuring |D|. With these notations, for each thought unit t \u2208 T (D), we define the score assigned to summ(D) for expressing thought unit t as Ts(t, summ(D)) = max{I(s, t) | s \u2208 summ(D)}. Formally, the summarization problem is then, select summ(D) to maximize Utility(summ(D)):\n\u2211 t\u2208T (D) WD(t) \u2217 Ts(t, summ(D))\nsubject to the constraint |summ(D)| \u2264 C. Note that our model can represent some aspects of summary coherence as well by imposing the constraint that the sequencing of thought units in the summary be consistent with the ordering of thought units in the document.\nFor the multi-document case, we are given aCorpus = {D1, D2, . . . Dn}, each Di has its own sequencing of sentences and thought units, which could conflict with other documents. One must resolve the conflicts somehow when constructing a single summary of the corpus. Thus, for multi-document summarization, we hypothesize that WCorpus is a total ordering that is maximally consistent with the WDi\u2019s by which we mean that if two thought units are assigned the same relative importance by every document in the collection that includes them, then the same relative order is imposed by W as well, otherwise W chooses a relative order that is best represented by the collection and this could be based on a majority of the documents or in other ways. With this, our previous definition extends to multi-document summarization as well, but we replace summ(D) by summ(Corpus), WD by WCorpus, and T (D) by T (Corpus). In the multi-document case, the summary coherence can be defined as the constraint that the sequencing of thought units in a summary be maximally consistent with the sequencing of thought units in the documents and in conflicting cases makes the same choices as implied by WCorpus.\nThe functionW is a crucial ingredient that allows us to capture the sequencing chosen by the author(s) of the document(s), without W we would get the\n4 Since some thought units in the same sentence\nbag of words models popular in previous work. We note that W does need to respect the sequencing in the sense that it is not required to be a decreasing (or even non-increasing) function with sequence position. This flexibility is needed since W must fit the document structure.\nAs defined our model covers abstractive summarization directly since it is based on sentences that are not restricted to those within D. For extractive summarization, we need to impose the additional constraint summ(D) \u2286 S(D) for single-document, and summ(D) \u2286 S(Corpus), where S(Corpus) = \u222aiS(Di), for multi-document summarization. Some other important special cases of our model as as follows:\n1. Restricting I(S, T ) to a boolean-valued function. This gives rise to the \u201cmembership\u201d model and avoids partial membership. 2. Restricting WD(t) to a constant function. This would give rise to a \u201cbag of thought units\u201d model and it would treat all thought units the same. 3. Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31]. This also means that the optimization problem of our model is NP-hard at least and NP-complete when WD(t) is a constant function and I(S, T ) is boolean-valued.\nTheorem 1. The optimization problem of the model is at least NP-hard. It is NP-complete when I(S, T ) is boolean-valued, WD(t) is a constant function and thought units are: words, or all words minus stopwords or key phrases of the document, with sentence size and summary size constraint being measured in these same syntactic units. We call these NP-complete cases extractive coverage summarization collectively.\nProof: Reduction from the set cover problem - proof in Appendix.\nBased on this generalized model, we can define:\nDefinition 1. The extractive compressibility of a document is the smallest size collection of sentences from the document that cover its thought units. If the thought units are words, we call it the word extractive compressibility.\nDefinition 2. The abstractive compressibility of a document is the smallest size collection of arbitrary sentences that cover its thought units. If the thought units are words, we call it the word abstractive compressibility.\nDefinition 3. The compression rate or incompressibility of a document is defined as \u03ba/N , where \u03ba is the size of the compressibility of the document, and N is the original size of the document.\nSimilarly, we can define corresponding compressibility notions for key phrases, words minus stopwords, and thought units.\nWe investigate compressibility of three different genres: news articles, scientific papers and short studies. For this purpose, 25 news articles, 25 scientific papers, and 25 short stories were collected. The 25 news articles were randomly selected from several sources and covered disasters, disaster recovery, prevention, and critical infrastructures. Five scientific papers, on each of the following five topics: cancer research, nanotechnology, physics, NLP and security, were chosen at random. Five short stories each by Cather, Crane, Chekhov, Kate Chopin, and O\u2019Henry were randomly selected. Experiments showed that large sentence counts lead to decrease imcompressibility. Figure 3 shows a direct relationship between document size and incompressibility."}, {"heading": "4.1 Algorithms for Single-document Summarization", "text": "We have implemented several new and existing heuristcs in a tool called DocSumm written in Python. Many of our heuristics revolve around the TF/IDF ranking, which has been shown to do well in tasks involving summarization.\nTF/IDF ranks the importance of words across a corpus. This ranking system was compared to other popular keyword identification algorithms and was found to be quite competitive in results [25]. In this paper, the authors compared\nTextRank, SingleRank, ExpandRank, KeyCluster, Latent Semantics Analysis, Latent Dirichlet Analysis and TF/IDF. N keywords, where N varied from 5 to 100 in steps of 5, from the DUC documents were extracted using each algorithm and the F1-score was calculated using human summaries as models. The experiments showed that TF/IDF consistently performs as well if not better than other algorithms. To apply to the domain of single-document summarization, we define a corpus as the document itself. The documents referred to in inverse document frequency are the individual sentences and the terms remain the same, words. The value of a sentence is then the sum of the TF/IDF scores of the words in the sentence.\nDocSumm, includes both greedy and dynamic programming based algorithms. The greedy algorithms use the chosen scoring metric to evaluate every sentence of a document. It then simply selects the highest scoring sentence, until either a given threshold of words are met or every word is covered in the document. Besides the choices for the scoring metrics, there are several other options (normalization of weights, stemming, etc.) that can be toggled for evaluation. Appendix B gives a brief description of those options.\nDocSumm includes two dynamic programming algorithms. One provides an optimal solution, i.e., the minimum number sentences necessary to cover all words of the document. This can be viewed as the bound on maximum compression of a document for extractive summary. This algorithm is a bottom-up approach that builds set covers of all subsets of the original document\u2019s thought units (i.e. words for our experiments), beginning with the smallest unit, a single word. We did implement a top-down version based on recursion, but this algorithm quickly runs out of time/space because of repeated computations.\nIn addition to this optimal algorithm, DocSumm also implements a version of the algorithm presented in [23]. McDonald frames the problem of document summarization as the maximization of a scoring function that is based on relevance and redundancy. In essence, selected sentences are scored higher for relevance and scored lower for redundancy. If the sentences of a document are considered on a inclusion/exclusion basis, then the problem of document summarization reduces to the 0-1 Knapsack problem. However, McDonald\u2019s algorithm is approximate, because the inclusion/exclusion of the algorithm influences the score of other sentences. A couple of greedy algorithms and a dynamic programming algorithm of DocSumm appeared in [31], the rest are new."}, {"heading": "4.2 Results", "text": "Our results include experiments on running time comparisons of DocSumm\u2019s algorithms. In addition we compare the performance measures of DocSumm on DUC 2001 and DUC 2002 datasets.\nRun times The dataset for running times is created by sampling sentences from the book of Genesis. We created documents of increasing lengths, where length is measured in verses. The verse count ranged from 4 to 320. However, for documents greater than 20 sentences, the top-down dynamic algorithm runs out of memory. So there are no results on the top-down exhaustive algorithm. Table 5 shows slight increases in time as the document size increase. For both tfidf and bottom-up there is a significant increase in running time.\nSummarization We now compare the heuristics for single-document summarization on DUC 2001 and DUC 2002 dataset. For the 305 unique documents of the DUC 2001 dataset we compared the summaries of DocSumm algorithms. The results were in line with the analysis of the three domains.\nFor each algorithm, we truncated the solution set as soon as a threshold of 100 words was covered. The ROUGE scores of the algorithms were in line with the compressibility performances. The size algorithms performed similarly and the best was the bottom-up algorithm with ROUGE F1 scores of 0.444, 0.273 and 0.408 for ROUGE-1, ROUGE-2 and ROUGE-LCS, respectively. The tfidf algorithm performance was not significantly different.\nComparison On the 533 unique articles in the DUC 2002 dataset, we now compare our greedy and dynamic solutions against the following classes of systems: (i) two top of the line single-document summarizers, SynSem [2], and the best extractive summarizer from [17], which we call KKV, (ii) top five (out of 13) systems, S28, S19, S29, S21, S23, from DUC 2002 competition, (iii) TextRank, (iv) MEAD, (v) McDonald Algorithm and (vi) the DUC 2002 Baseline summaries consisting of the first 100 words of news articles. The Baseline did\nvery well in the DUC 2002 competition - only two out of 13 systems, S28 and S19, managed to get a higher F1 score than the Baseline. For this comparison, all manual abstracts and system summaries are truncated to exactly 100 words whenever they exceed this limit.\nNote that the results for SynSem are from [2], who also used only the 533 unique articles in the DUC 2002 dataset. Unfortunately, the authors did not report the Rouge bigram (ROUGE-2) and Rouge LCS (ROUGE-L) F1 scores in [2]. KKV\u2019s results are from [17], who did not remove the 33 duplicate articles in the DUC 2002 dataset, which is why we flagged those entries in Table 6 with *. Hence their results are not comparable to ours. In addition KKV did not report ROUGE-LCS scores. We observe that for Rouge unigram (ROUGE-1) F1-scores the dynamic optimal algorithm performs the best amongst the algorithms of DocSumm. However, it still falls behind the Baseline. When we consider Rouge bigram (ROUGE-2) F1-scores Dynamic and Greedy outperform the rest of the field (surprisingly even [17]). The margin of out-performance is even more pronounced in ROUGE-LCS F1-scores."}, {"heading": "5 Conclusions and Future Work", "text": "We have shown limits on the recall of automatic extractive summarization on DUC datasets under ROUGE evaluations. Our limits show that the current stateof-the-art systems evaluated on DUC data [2,17] are achieving about 54% of this limit (Rouge-1 recall) for single-document summarization and the best systems for multi-document summarization are achieving only about a third of their\nlimit. This is encouraging news, but at the same time there is much work remaining to be done on summarization. We also explored compressibility, a generalized model, and new and existing heuristics for single-document summarization.\nTo our knowledge, compressibility the way we have defined and studied it is a new concept and we plan to investigate it further in future work. We believe that compressibility could prove to be a useful measure to study the performance of automatic summarization systems and also perhaps for authorship detection if, for instance, authors are shown to be consistently compressible."}, {"heading": "Acknowledgments", "text": "We thank the reviewers of CICLING 2017 for their constructive comments."}, {"heading": "A Appendix - Proof of Theorem 1", "text": "Reduction from the set cover problem for NP-hardness. Given a universe U, and a family of S of subsets of U, a cover is a subfamily C of S whose union is U. In the set cover problem the input is a pair (U,S) and a number k, the question is whether there is a cover of size at most k. We reduce set cover to summarization as follows. For each member u of U , we select a thought unit t from T and a clause c that expresses t. For each set S in the family, we construct a sentence s that consists of the clauses corresponding to the members of S (I is boolean-valued). We assemble all the sentences into a document. The capacity constraint C = k and represents the number of sentences that we can select for the summary. It is easy to see that a cover corresponds to a summary that maximizes the Utility and satisfies the capacity constraint and vice versa. ut\nOf course, the document constructed above could be somewhat repetitive, but even \u201creal\u201d single documents do have some redundancy. Connectivity of clauses appearing in the same sentence can be ensured by choosing them to be facts about a person\u2019s life for example. We call the NP-complete cases of the theorem, extractive coverage summarization collectively. For this case, it is easy to design a greedy strategy that gives a logarithmic approximation ratio [16] and an optimal dynamic programming one that is exponential in the worst case."}, {"heading": "B Appendix - DocSumm Tool", "text": ""}, {"heading": "C Appendix - Document 250, AP900625-0153, from DUC 2002", "text": ""}], "references": [{"title": "Fast and robust compressive summarization with dual decomposition and multi-task learning", "author": ["M.B. Almeida", "A.F. Martins"], "venue": "ACL (1). pp. 196\u2013206", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining syntax and semantics for automatic extractive singledocument summarization", "author": ["A. Barrera", "R. Verma"], "venue": "CICLING. vol. LNCS 7182, pp. 366\u2013377", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Jointly learning to extract and compress", "author": ["T. Berg-Kirkpatrick", "D. Gillick", "D. Klein"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. pp. 481\u2013490. Association for Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions", "author": ["F. Boudin", "H. Mougard", "B. Favre"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 1914\u20131918", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Extractive vs", "author": ["G. Carenini", "J.C.K. Cheung"], "venue": "nlg-based abstractive summarization of evaluative text: The effect of corpus controversiality. In: Proceedings of the Fifth International Natural Language Generation Conference. pp. 33\u201341. Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised sentence enhancement for automatic summarization", "author": ["J.C.K. Cheung", "G. Penn"], "venue": "EMNLP. pp. 775\u2013786", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["S. Chopra", "M. Auli", "A.M. Rush", "S. Harvard"], "venue": "Proceedings of NAACL-HLT16 pp. 93\u201398", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Overview of the tac 2008 update summarization task", "author": ["H.T. Dang", "K. Owczarzak"], "venue": "Proceedings of text analysis conference. pp. 1\u201316", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research pp. 457\u2013479", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "A formal model for information selection in multisentence text extraction", "author": ["E. Filatova", "V. Hatzivassiloglou"], "venue": "Proceedings of the 20th international conference on Computational Linguistics. p. 397. Association for Computational Linguistics", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Recent automatic text summarization techniques: a survey", "author": ["M. Gambhir", "V. Gupta"], "venue": "Artif. Intell. Rev. 47(1), 1\u201366", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions", "author": ["K. Ganesan", "C. Zhai", "J. Han"], "venue": "Proceedings of the 23rd international conference on computational linguistics. pp. 340\u2013348. Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A scalable global model for summarization", "author": ["D. Gillick", "B. Favre"], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. pp. 10\u201318. Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE", "author": ["Y. Graham"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 128\u2013137", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-document summarization as a tree knapsack problem", "author": ["T. Hirao", "Y. Yoshida", "M. Nishino", "N. Yasuda", "M. Nagata"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1515\u20131520", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Approximation algorithms for NP-hard problems", "author": ["D.S. Hochbaum"], "venue": "PWS Publishing Co.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "A knowledge induced graph-theoretical model for extract and abstract single document summarization", "author": ["N. Kumar", "K. Srinathan", "V. Varma"], "venue": "Computational Linguistics and Intelligent Text Processing, pp. 408\u2013423. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving multi-documents summarization by sentence compression based on expanded constituent parse trees", "author": ["C. Li", "Y. Liu", "F. Liu", "L. Zhao", "F. Weng"], "venue": "EMNLP. pp. 691\u2013701. Citeseer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Evaluation of Summaries Using n-gram Co-occurrence Statistics", "author": ["C. Lin", "E. Hovy"], "venue": "HTL-NAACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression", "author": ["F. Liu", "Y. Liu"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on 21(7), 1469\u20131480", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Advances in Automatic Summarization", "author": ["I. Mani", "M. Maybury"], "venue": "MIT Press, Cambridge, Massachusetts", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Summarization with a joint model for sentence extraction and compression", "author": ["A.F. Martins", "N.A. Smith"], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. pp. 1\u20139. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["R. McDonald"], "venue": "Proc. of the 29th ECIR. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Extractive summarization under strict length constraints", "author": ["Y. Mehdad", "A. Stent", "K. Thadani", "D. Radev", "Y. Billawala", "K. Buchner"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Ranking systems evaluation for keywords and keyphrases detection", "author": ["M. Meseure"], "venue": "Tech. rep., Department of Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Textrank: Bringing order into text", "author": ["R. Mihalcea", "P. Tarau"], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Automatic Text Summarization of Newswire: Lessons Learned from the document understanding conference", "author": ["A. Nenkova"], "venue": "AAAI. pp. 1436\u20131441", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["D. Parveen", "H. Ramsl", "M. Strube"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 1949\u20131954", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated pyramid scoring of summaries using distributional semantics", "author": ["R.J. Passonneau", "E. Chen", "W. Guo", "D. Perin"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers. pp. 143\u2013147", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 379\u2013389", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Text summarization model based on maximum coverage problem and its variant", "author": ["H. Takamura", "M. Okumura"], "venue": "Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. pp. 781\u2013789. Association for Computational Linguistics", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Summarization evaluation using transformed basic elements", "author": ["S. Tratz", "E.H. Hovy"], "venue": "Proceedings of the First Text Analysis Conference, TAC 2008, Gaithersburg, Maryland, USA, November 17-19, 2008", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Event-centric summary generation", "author": ["L. Vanderwende", "M. Banko", "A. Menezes"], "venue": "Working notes of DUC pp. 127\u2013132", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Extractive summarization using supervised and semi-supervised learning", "author": ["K. Wong", "M. Wu", "W. Li"], "venue": "COLING 2008, 22nd International Conference on Computational Linguistics, Proceedings of the Conference, 18-22 August 2008, Manchester, UK. pp. 985\u2013992", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Extractive summarization by maximizing semantic volume", "author": ["D. Yogatama", "F. Liu", "N.A. Smith"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pp. 1961\u20131966", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Those that did outperform the baseline could not do so in a statistically significant way [27].", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "Summarization can be extractive or abstractive [21]: in extractive summarization sentences are chosen from the article(s) given as input, whereas in abstractive summarization sentences may be generated or a new representation of the article(s) may be output.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "(a) Specifically, we show that when the documents themselves from the DUC 2001-2002 datasets are compared using ROUGE [19] to abstractive summaries, the average Rouge-1 (unigram) recall is around 90%.", "startOffset": 118, "endOffset": 122}, {"referenceID": 7, "context": "As pointed out by [8], competitive summarization systems are typically extractive, selecting representative sentences, concatenating them and often compressing them to squeeze in more sentences within the constraint.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "The summarization literature is vast, so we refer the reader to the recent survey [11], which is fairly comprehensive for summarization research until 2015.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "For single-document summarization, [22] explicitly model extraction and compression, but their results showed a wide variation on a subset of 140 documents from the DUC 2002 dataset, and [28] focused on topic coherence with a graphical structure with separate importance, coherence and topic coverage functions.", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "In [28], the authors present results for single-document summarization on a subset of PLOS Medicine articles and DUC 2002 dataset without mentioning the number of articles used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 72, "endOffset": 75}, {"referenceID": 32, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 8, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 25, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 16, "context": "An algorithm combining syntactic and semantic features was presented by [2], and graph-based summarization methods in [33,9,26,17].", "startOffset": 118, "endOffset": 130}, {"referenceID": 23, "context": "Several systems were compared against a newly-devised supervised method on a dataset from Yahoo in [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 12, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 2, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 0, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 17, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 3, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 34, "context": "For multi-document summarization, extraction and redundancy/compression of sentences have been modeled by integer linear programming and approximation algorithms [23,13,3,1,18,4,35].", "startOffset": 162, "endOffset": 181}, {"referenceID": 33, "context": "Supervised and semi-supervised learning based extractive summarization was studied in [34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 11, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 5, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 19, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 29, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 6, "context": "Abstractive summarization systems include [5,12,6,20,30,7].", "startOffset": 42, "endOffset": 58}, {"referenceID": 9, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 22, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 30, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 63, "endOffset": 73}, {"referenceID": 14, "context": "Frameworks for single-document summarization were presented in [10,23,31], and some multi-document summarization frameworks are in [15,36].", "startOffset": 131, "endOffset": 138}, {"referenceID": 28, "context": "There is also the Pyramid approach [29] and BE [32], for example.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "There is also the Pyramid approach [29] and BE [32], for example.", "startOffset": 47, "endOffset": 51}, {"referenceID": 18, "context": "Our choice of ROUGE is based on its popularity, ease of use, and correlation with human assessment [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Our choice of ROUGE configurations includes the one that was found to be best according to the paper [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "In all instances the ROUGE evaluations include the best schemes as shown by [14], which are usually Rouge-2 (bigram) and Rouge-3 (trigram) with stemming and stopword elimination.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Table 2 shows that, for the DUC 20022 dataset, when the document themselves are considered as summaries and evaluated against a set of 100-word human abstractive summaries, the average Rouge-1 (unigram) [19] score is approximately 91 %.", "startOffset": 203, "endOffset": 207}, {"referenceID": 1, "context": ", see [2,17].", "startOffset": 6, "endOffset": 12}, {"referenceID": 16, "context": ", see [2,17].", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 22, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 30, "context": "Further, if \u201cthought units\u201d are limited to be all words, or all words minus stopwords, or key phrases of the document, and under extractive constraint, we get previous models of [10,23,31].", "startOffset": 178, "endOffset": 188}, {"referenceID": 24, "context": "This ranking system was compared to other popular keyword identification algorithms and was found to be quite competitive in results [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 22, "context": "In addition to this optimal algorithm, DocSumm also implements a version of the algorithm presented in [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 30, "context": "A couple of greedy algorithms and a dynamic programming algorithm of DocSumm appeared in [31], the rest are new.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Comparison On the 533 unique articles in the DUC 2002 dataset, we now compare our greedy and dynamic solutions against the following classes of systems: (i) two top of the line single-document summarizers, SynSem [2], and the best extractive summarizer from [17], which we call KKV, (ii) top five (out of 13) systems, S28, S19, S29, S21, S23, from DUC 2002 competition, (iii) TextRank, (iv) MEAD, (v) McDonald Algorithm and (vi) the DUC 2002 Baseline summaries consisting of the first 100 words of news articles.", "startOffset": 213, "endOffset": 216}, {"referenceID": 16, "context": "Comparison On the 533 unique articles in the DUC 2002 dataset, we now compare our greedy and dynamic solutions against the following classes of systems: (i) two top of the line single-document summarizers, SynSem [2], and the best extractive summarizer from [17], which we call KKV, (ii) top five (out of 13) systems, S28, S19, S29, S21, S23, from DUC 2002 competition, (iii) TextRank, (iv) MEAD, (v) McDonald Algorithm and (vi) the DUC 2002 Baseline summaries consisting of the first 100 words of news articles.", "startOffset": 258, "endOffset": 262}, {"referenceID": 1, "context": "Note that the results for SynSem are from [2], who also used only the 533 unique articles in the DUC 2002 dataset.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Unfortunately, the authors did not report the Rouge bigram (ROUGE-2) and Rouge LCS (ROUGE-L) F1 scores in [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "KKV\u2019s results are from [17], who did not remove the 33 duplicate articles in the DUC 2002 dataset, which is why we flagged those entries in Table 6 with *.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "When we consider Rouge bigram (ROUGE-2) F1-scores Dynamic and Greedy outperform the rest of the field (surprisingly even [17]).", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Our limits show that the current stateof-the-art systems evaluated on DUC data [2,17] are achieving about 54% of this limit (Rouge-1 recall) for single-document summarization and the best systems for multi-document summarization are achieving only about a third of their", "startOffset": 79, "endOffset": 85}, {"referenceID": 16, "context": "Our limits show that the current stateof-the-art systems evaluated on DUC data [2,17] are achieving about 54% of this limit (Rouge-1 recall) for single-document summarization and the best systems for multi-document summarization are achieving only about a third of their", "startOffset": 79, "endOffset": 85}], "year": 2017, "abstractText": "Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data.", "creator": "LaTeX with hyperref package"}}}