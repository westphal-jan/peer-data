{"id": "1704.05569", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Using Contexts and Constraints for Improved Geotagging of Human Trafficking Webpages", "abstract": "Extracting geographical tags from webpages is a well-motivated application in many domains. In illicit domains with unusual language models, like human trafficking, extracting geotags with both high precision and recall is a challenging problem. In this paper, we describe a geotag extraction framework in which context, constraints and the openly available Geonames knowledge base work in tandem in an Integer Linear Programming (ILP) model to achieve good performance. In preliminary empirical investigations, the framework improves precision by 28.57% and F-measure by 36.9% on a difficult human trafficking geotagging task compared to a machine learning-based baseline. The method is already being integrated into an existing knowledge base construction system widely used by US law enforcement agencies to combat human trafficking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 19 Apr 2017 00:52:02 GMT  (929kb,D)", "http://arxiv.org/abs/1704.05569v1", "6 pages, GeoRich 2017 workshop at ACM SIGMOD conference"]], "COMMENTS": "6 pages, GeoRich 2017 workshop at ACM SIGMOD conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["rahul kapoor", "mayank kejriwal", "pedro szekely"], "accepted": false, "id": "1704.05569"}, "pdf": {"name": "1704.05569.pdf", "metadata": {"source": "META", "title": "Using Contexts and Constraints for Improved Geotagging of Human Trafficking Webpages", "authors": ["Rahul Kapoor", "Marina Del Rey", "Mayank Kejriwal", "Pedro Szekely"], "emails": ["rahulkap@isi.edu", "kejriwal@isi.edu", "pszekely@isi.edu", "permissions@acm.org."], "sections": [{"heading": "KEYWORDS", "text": "Integer Linear Programming; Information Extraction; Named Entity Recognition; Human Tra cking; Feature-agnostic; Distributional Semantics"}, {"heading": "ACM Reference format:", "text": "Rahul Kapoor, Mayank Kejriwal, and Pedro Szekely. 2017. Using Contexts and Constraints for Improved Geotagging of Human Tra cking Webpages. In Proceedings of GeoRich\u201917 , Chicago, IL, USA, May 14, 2017, 6 pages. DOI: h p://dx.doi.org/10.1145/3080546.3080547"}, {"heading": "1 INTRODUCTION", "text": "e ubiquity of the Web has also had the unfortunate consequence of lowering the barrier of entry for players engaging in illicit activities. One such activity is human tra cking. Although precise numbers for human tra cking Web advertising activity are not known, they are very high, possibly in the tens of millions of (not necessarily unique) advertisements posted on the Web [3].\nRecent advances in information extraction and knowledge base construction technology, especially using techniques like deep neural networks and word embeddings [9], [2], gives investigators (such as law enforcement and intelligence agencies) the valuable opportunity to turn the Web against illicit players. Exploiting this\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. GeoRich\u201917 , Chicago, IL, USA \u00a9 2017 ACM. 978-1-4503-5047-1/17/05. . .$15.00 DOI: h p://dx.doi.org/10.1145/3080546.3080547\nopportunity for the human tra cking domain involves solving some speci c challenges [6], [15].\nFirst, human tra cking advertisements deliberately obfuscate key pieces of information like names and phone numbers to avoid automated search, indexing and discovery. Second, like Twi er and social media, the language model in human tra cking is nontraditional, using words, phrases and slang that impair performance of traditional extractors. As a representative example, consider the sentences \u2018Hey gentleman im neWYOrk and i\u2019m looking for generous\u2026\u2019 and \u2018AVAILABLE NOW! - (4 two 4) six 5 two - 0 9 three 1 - 21\u2019. In the rst instance, the correct extraction for a Name a ribute is neWYOrk, while in the second instance, the correct extraction for an Age a ribute is 21. Automatic, reliable information extraction is hard in such domains. More generally, illicit domains tend to frequently exhibit such heterogeneity; the ndings in this paper would also apply to them.\nA speci c a ribute that is extremely important to investigators is the geotag implicitly conveyed by the webpage. Such tags are o en present in free text elds like description or the page body, and not within structured HTML tags (hence, cannot be extracted by wrapper-based extractors [7]). Even though geolocations like cities are not obfuscated1 in human tra cking pages, automatically extracting them is problematic both due to the language model, and due to ambiguity. For example, Charlo e may refer either to the city in North Carolina or to a person. Using a lexicon to directly extract geolocations is problematic for this reason; richer clues like context (such as the words surrounding an extraction) are necessary for disambiguation [6], [2]. Looking at the context of neWYOrk in the earlier example, for instance, one can deduce that it most likely refers to a name, not the city.\nWe also note that, to infer a geotag (referring to a single identi able location in the world), prior knowledge and relational information can both prove necessary. For example, there is a Los Angeles in both California and Texas. Given only a Los Angeles geolocation extraction, one is more inclined to infer Los Angeles, California, since it has much higher population than Los Angeles, Texas. However, if Texas were also extracted as a geolocation, the probability of the la er increases due to the relational connection.\nIn this paper, we present a geotagging framework that holistically integrates the strengths of semantic lexicons, extraction context, relational constraints and prior cues like city populations to deliver high performance. e work ow is illustrated in Figure 1. First, the corpus of pages is preprocessed using text scrapers and tokenization. Next, the Geonames dictionary is used to label tokens\n1Advertisements want to be easily searchable on location facets, as tra cking victims change locations frequently.\nar X\niv :1\n70 4.\n05 56\n9v 1\n[ cs\n.A I]\n1 9\nA pr\n2 01\n7\nas geolocation candidates [16]. For example, Charlo e would be annotated as a geolocation candidate regardless of whether it is a name or a city in the underlying webpage. To determine the probability of the candidate being a geolocation, we use a recent machine learning-based approach that uses context features [6]. e next few steps involve building and solving an Integer Linear Programming (ILP) model that integrates relational constraints and external domain knowledge (such as city populations) from Geonames as ILP constraints. A er solving the model (per webpage), the result is a set of extractions that achieves high precision, without signi cantly hurting recall compared to non-ILP processing. We apply the framework in Figure 1 to the problem of geotagging a corpus of real-world human tra cking webpages.\nContributions. We summarize our main contributions as follows. (1) We present a novel framework that integrates the strengths of several approaches to achieve good geotagging performance. (2) We show how domain constraints that are assumed as obvious by human beings can be encoded as constraints in an ILP model to improve performance over a machine learning-only approach. (3) We present some preliminary empirical results on a real-world human tra cking geotagging task illustrating the promise of the approach.\nStructure of the paper. Section 2 covers some related work, Section 3 describes the overall framework, including sub-components, Section 4 describes the experimental results and Section 5 concludes the work."}, {"heading": "2 RELATEDWORK", "text": "e extraction of geolocation information such as cities and countries from unstructured data is an important problem that more generally falls within Information Extraction (IE). IE is an old research area for which a wide range of techniques have been proposed; for an accessible survey of Web IE approaches, we refer the reader to [1].\ne goals of this work are similar to other geolocation prediction system for \u2018di cult\u2019 datasets like Twi er, a good example being [5]. However, we note that illicit domain challenges are di erent from those of social media, an important example being information obfuscation [3], [6], [15].\ne individual components that are used for building the framework are well established in the research community. For example, word embedding methods, used in the contextual classi er in our framework, have achieved notable advances in NLP (and especially IE) performance [2]. Integer Linear Programming has also been used in many applications, and o ers a powerful and exible way to represent constrained optimization problems [4]. Lexicon-based IE has also received much coverage in the literature, an in uential recent work being [12]. Given its importance, geolocation extraction has received a lot of focused a ention in the literature, an important related work being the recent text and context-based approach by Speriosu and Baldridge [14]. Some of the techniques in this work, such as usage of text and populations, derive from extant techniques on toponym resolution [8], [14]. A good description may be found in the book by Leidner [8]. We note, however, that except for a recent paper that we published [6], no work has tackled the challenges of high-performance geolocation extraction in domains\nlike human tra cking. is work improves the performance of our recent work by a signi cant margin by incorporating constraints into a geolocation-speci c ILP model. Additionally, unlike our previous work, the system in this paper is optimized speci cally for geolocation extraction, given its importance to human tra cking investigators."}, {"heading": "3 FRAMEWORK", "text": "e overall approach is illustrated in Figure 1. e input to the system is a corpus of webpages, serialized as raw HTML obtained by a domain-discovery crawling system, and the nal output is a set of high-precision geolocation extractions for each webpage. We detail the individual steps in the approach below. In this paper, we assume that the webpages are from the human tra cking domain, a di cult domain whose challenges were earlier described."}, {"heading": "3.1 Preprocessing", "text": "Since the relevant geotags in the webpage are typically present in natural language elements like title, description and text (as opposed to structured elds), the rst step is to extract the text from the webpage. is preprocessing step is non-trivial, and involves automatically removing extraneous elements like HTML tags and irrelevant characters. Like other extractors, there is often a precision-recall tradeo i.e. aggressively removing irrelevant information can also lead to the removal of relevant information. For that reason, we used an openly available tunable text extractor called the Readability Text Extractor2, and optimized it for separately achieving high text extraction recall and precision. e lowercased extracted text is tokenized using whitespace and punctuation as delimiters. Extraction and tokenization are performed independently for webpage title, using only the high precision setting, and main body text, using both high recall and high precision se ings.\n2h ps://www.readability.com/\nAs a running example, alluded to earlier in the introduction, the title and main text (i.e. body) from the small HTML fragment \u00a1html\u00bf \u00a1head\u00bf \u00a1title\u00bf Los Angeles Escort Listing \u00a1/title\u00bf \u00a1/head\u00bf\u00a1body\u00bf \u00a1p\u00bf My name is Charlo e \u00a1/p\u00bf. I come from Mexico and am new in the city of Los Angeles \u00a1/body\u00bf \u00a1/html\u00bf are separately preprocessed and passed to the next step as three lists of tokens (two for the text, and one for the title)."}, {"heading": "3.2 Dictionary-based Candidate Extraction", "text": "Due to explosion of structured data on the Web, there are openly available dictionaries, also called semantic lexicons [13], for identifying candidate geotags from text. For geotags, a standard nearcomprehensive lexicon is Geonames [16]. Because Geonames contains many geotags that are not interesting in our domain, such as unpopulated places, we limit our lexicon to a subset containing states3, countries, and also those cities with a population greater than 15,000.\nFor e ciency, we process the items in the lexicon using a trie data structure. Using the trie, we implement an exact string matching algorithm on the lists of tokens output by preprocessing. One reason for using the trie is that there are locations in the lexicon that span multiple contiguous tokens (e.g., \u2019los angeles\u2019); the trie can e ciently extract such token spans as candidates. Using the running example, the token spans \u2019los angeles\u2019, \u2019charlo e\u2019, \u2019mexico\u2019, \u2019the city\u2019 and \u2019angeles\u2019 are all marked as city tokens4, while \u2019mexico\u2019 is marked as a country token.\nIt is important to note that these candidate extractions are a (typically large) superset of the true positive extractions. In that sense, the candidates are high-recall (o en, perfect-recall) and lowprecision. ere are multiple reasons for this phenomenon; we note three important ones. e rst problem is the existence of generic or slang terms like \u2018the city\u2019 in the lexicon (present in Geonames because it is an alternative term for the city of London) that cause signi cant false positive extractions. e second problem is ambiguity e.g., Charlo e is both a name and a city. e third, possibly most di cult problem, is that even when correctly extracted, a city might not be the relevant extraction. is last problem occurs when there are multiple cities that got extracted, but only one is relevant, usually the place where the subject of the webpage (in the human tra cking domain, an escort) is physically advertising from."}, {"heading": "3.3 Context Based Classi cation", "text": "To improve the precision of the candidates, we propose using the context of the candidate in the text. Intuitively, even the local context is o en a very revealing clue to humans about whether, for example, \u2018Charlo e\u2019 is a name or a city. In the running example, the words preceding Charlo e (\u2018My name is\u2019) allow us to tag Charlo e as a name with near certainty. Context-based classi cation marks candidate annotations as positive or negative using a supervised machine learning procedure that is trained on true positives and negatives using features derived from the context.\nIn earlier work, CRFs, with manually cra ed feature functions, were o en used for this purpose. As described in our recent work,\n3In Geonames, states are usually marked as rst-level administrative divisions. 4Both \u2018mexico\u2019 and \u2018angeles\u2019 are individually present in theCity subset of the Geonames lexicon.\nsuch feature functions are o en problematic for irregular, obfuscated domains like human tra cking [6]. Another problem is the large number of annotations typically required for CRF-based taggers. In previous work, we presented a minimally supervised context-based classi er that avoids both the feature cra ing and high supervision problems by rst e ciently deriving low-dimensional word embeddings from an extracted text corpora as word feature vectors (WFVs) [6]. A contextual classi er is trained by using the WFVs to derive a contextual feature vector (CFV) for each candidate token span. Intuitively, this is done by aggregating and normalizing the vectors of tokens occurring in a window of 5 words (on either side of the candidate).\nUsing a small training set, a random forest classi er is trained to probabilistically mark each candidate as correct. Because of the low dimensional feature space, even a small training set allows a classi er to quickly generalize.\nIn previous work, we used a threshold on the probability scores to determine which candidates were relevant. In Section 4, we consider a variant of this approach (choosing the most probable candidate from a webpage as the correct extraction) as a baseline.\ne hypothesis guiding the framework in Figure 1 is that systematically processing probability-annotated candidates using domain knowledge can aggressively improve precision. Domain knowledge is captured as a set of constraints used e ciently in an ILP framework to determine the subset of correct geotag candidates."}, {"heading": "3.4 Context-rich ILP Framework", "text": "Before presenting the details of the approach, we introduce some formalism for ILP.\nDe nition (ILP). Integer Linear Programming (ILP) is an optimization problem of the form:\nMaximize: n\u2211 j=1 c jx j\nSubject to: n\u2211 j=1 ai jx j <= bi\nx j \u2265 0 x j \u2208 Z\nIntuitively, ILP a empts to maximize an objective function subject to a given set of linear constraints (in the formulation above, i ranges over the set of constraints). A provably optimal solution to ILP is known to be NP-Complete; hence, solutions must be approximated. Good so ware packages for this problem already exist; we use a solution described further in Section 4.\n3.4.1 Variable Selection. At a high level, our framework encodes both candidate annotations and domain-speci c constraints as ILP variables and constraints respectively. is involves some nontrivial modeling problems such as variable selection.\nWe model the ILP variables x j in our framework as binary variables (x j \u2208 {0, 1}). is reduces ILP to 0-1 linear programming, which is still NP-complete. Each variable is a placeholder for a candidate, with the simple semantics that a value of 1 (in an ILP solution) represents correctness with respect to its semantic type. In an ideal solution, for example, \u2018Mexico\u2019 is correct with respect to\nsemantic type Country but not City, for which it was also extracted by the lexicon as a candidate.\nFor consistency, we refer to the x j variables as token semantic type (TST) variables. Note that, using the above example, if a token such as \u2018Mexico\u2019 is marked with two semantic types, two TST variables will be created (mnemonically, Mexico-City and MexicoState). However, if a token occurs multiple times in the extracted, preprocessed text, only one variable is created for the token. In other words, the number of occurrences of a token is not taken into account in variable selection, as long as it occurs (i.e. marked as a candidate by the lexicon) at least once.\nExample: e TST variables created for the running example in Section 3.1 are: Los Angeles - City, Charlo e - City, Mexico - City, the city - City, Angeles - City, Mexico - Country.\nA problem with directly using TST variables as ILP variables is that, even if a particular city such as Los Angeles is set to 1, there could potentially be multiple cities in the world with the same name. Additionally, there is no way in the simple formulation to relate the city TST variables with the state and country TST variables. For canonical geotagging, such relational information is vital.\nTo accommodate this issue in our modeling, we introduce additional variables, denoted herein as composite TST variables, to encode the intuitive notion that a city is part of a state, and a state is part of a country. As before, we use Geonames for obtaining this relational information.\ne new relational variables created are for each possible citystate, city-country and state-country pair applicable for each candidate geotag. To clarify what we mean by applicable, we take the following scenario: suppose the respective sets of candidate states and cities annotated on a webpage are S and C . To form applicable city-state composite TSTs, we take the cross-product of S and C , and eliminate all pairs that do not occur in Geonames. In this way, we make novel use of Geonames as a relational lexicon within our geotagging framework.\nOnce created (by verifying against Geonames), the composite TST variables are included as additional ILP variables. For each webpage, the set of (composite and non-composite) TSTs is the set of x j variables for an ILP model. Note that the relational information also leads to the introduction of new non-composite TSTs (at the state and country level) as the example below illustrated. Because we do not assume relational connections between webpages in this paper, each model is optimized independently, as described subsequently.\nExample: For the running example, the composite TST variables are: Los Angeles(city) in California(state), Los Angeles(city) in Texas (state), Charlo e(city) in North Carolina(state), the city(city) in England(state), Angeles(city) in Pampanga(state), California(state) in"}, {"heading": "United States(country), Texas(state) in Unites States(country), North Carolina(state) in United States (country), England(state) in United", "text": ""}, {"heading": "Kingdom(country), Pampanga (state) in Philippines(country).", "text": "e new non-composite TSTs introduced by the composite TSTs are California - State, Texas - State, North Carolina - State, England -"}, {"heading": "State, Pampanga - State, United States - Country, United Kingdom -", "text": ""}, {"heading": "Country, Philippines - Country.", "text": "3.4.2 Model Formulation. To fully specify the ILP model de ned earlier, we need to formulate the objective function and the model\nconstraints. is section describes both formulations. Note that the ILP model described below is independently constructed for each webpage, just like text preprocessing and candidate extractions. e model for each page can be optimized in parallel with other models, although we do not consider this option for the preliminary experiments in this work.\nObjective Function. e weight (c j in the ILP de nition) of variable x j in the objective function corresponds to how likely it is for the variable to be selected. e various factors that go into determining the weight are described below. We use two factors for the non-composite TST variables (token source and context probability), and two factors for the composite TST variables (population and zero weight).\nToken Source. e rst factor is the part of the webpage that yielded the token list from which the candidate was extracted. From the experiments, we found a weight of 1.0 for title tokens lists, 0.5 for main body text extractor using strict (i.e. high precision) se ing and 0.4 for relaxed main body text extractor to be optimal. Intuitively, if the candidate is extracted from the title, it is more likely to be the the correct city, and gets a higher weight.\nContext Probability. As described in Section 3.3, each candidate has a probability associated with it, based on the output of the random forest classi er. Since a token might occur multiple times in the text, each with a di erent context and a di erent probability, we de ne the context probability weight of the candidate as the maximum over the context probabilities for all candidate occurrences within the page.\nWe take the average of the two weights above as the weight of the corresponding non-composite TST in the ILP model. e weight is guaranteed to be between 0.0 and 1.0. Next, we describe the factors determining composite TST weights:\nPopulation. Since multiple cities with the same name might occur in di erent states and countries, we bias our solution towards the city with the highest population. is is done by assigning population weights to city-state and city-country variables. For example the city Los Angeles occurs both in California and Texas in United States. Here, the variable Los Angeles - California would have a higher weight than the variable Los Angeles - Texas. e speci c formula used to assign the weight c-populationi for the composite TST variable xi is given by:\nc-populationi = Cpop/K (1)\nwhere K is a constant population factor (for normalization purposes) and Cpop is the city population, which is obtained from Geonames.\nZero Weight. Other composite variables are assigned a weight of 0, to make each selection equally likely. e likelihood that these variables are set to 1 by an ILP solver depends on the subsequently described model constraints.\nConstraints. We design a set of ILP constraints to limit candidate selection to extractions that are feasible and highly probable, using simple knowledge about cities, states and countries, and their relationships to each other.\nSemantic Type Exclusivity. A candidate in our framework can be one of a city, state, or country (i.e. a semantic type) in an actual ILP model instantiation. Equationally, denoting candidates and types\nas the set of extracted candidates and relevant semantic types in our ontology,\n\u2200candidatei \u2208 candidates, types\u2211 j=1 candidatei typej \u2264 1 (2)\nNumber of Extractions of a Semantic Type is constraint limits how many extractions should be selected for a particular semantic type. is number is 1 for our purposes, as tra cking ads very rarely have more than one geotag.\n\u2200typej \u2208 types, candidates\u2211\ni=1 candidatei typej \u2264 1 (3)\nCity-State/Country Feasibility. A nal solution to the modeled ILP problem would only be feasible if a chosen city is actually present in a chosen state or country5. For example, if the variable for Los Angeles is set to 1, and the variable for United States is set to 0, the composite variable representing the pair Los Angeles-United States should not be set to 1 in a meaningful solution.\nFormally, the constraint can be expressed by stating that for each country, the sum of all city-country variables must be less than the country variable; similarly for states. If a country variable is 0, no corresponding city-country variables can be set to 1. If it is 1, at most one of the city-country variables can feasibly be set to 1 (because of objective maximization, exactly one is set to 1).\n\u2200countryj \u2208 countries, cit ies\u2211 i=1 cityicountryj \u2264 countryj (4)\n\u2200statej \u2208 states, cit ies\u2211 i=1 cityistatej \u2264 statej (5)\nCity-State/Country Exclusivity. is constraint ensures that the chosen city has exactly one corresponding city-state and city-country variable set to 1. For each city, the sum of the city-country variables is equal to the city variable. If the city variable is set to 1, exactly one of the city-country variables must be set to 1; if 0, none of the city-country variables can be 1 in a valid solution:\n\u2200cityi \u2208 cities, countr ies\u2211\nj=1 cityicountryj = cityi (6)\n\u2200cityi \u2208 cities, states\u2211 j=1 cityistatej = cityi (7)"}, {"heading": "3.5 Extraction Selection", "text": "With the model as speci ed earlier, we solve the ILP problem for each webpage. Because variables are binary in our model, each variable can be set to only 1 or 0. As earlier, we refer to the variables set to 1 as chosen variables.\nFor non-composite TST variables, the candidate underlying a chosen variable is marked as a correct extraction of its semantic type. Due to domain semantics, we permit at most one TST to be chosen per semantic type. 5In the solution, the composite TST of such a \u2018chosen\u2019 city-country or city-state pair would be set to 1.\nFor composite TST variables, a chosen variable de nes the underlying relationship to be correct. For example if Los Angeles(city) - in - California(state) is set to 1, the selected candidate city is Los Angeles, the selected candidate state is California, and the relationship denotes that we are referring to Los Angeles in California. Because of exclusivity and feasibility constraints, note that all three variables must have been chosen for this to occur. Furthermore, the chosen relationships permit us to determine city geotags canonically, which is necessary both for visualization and for accurate geolocation analytics."}, {"heading": "4 PRELIMINARY EXPERIMENTS", "text": ""}, {"heading": "4.1 Setup", "text": "Datasets. e datasets for the experiments are sampled from webpages in the human tra cking domain, crawled as a part of the DARPA MEMEX program6.\nSince we use the supervised contextual classi er described in our previous work [6], in conjunction with the Geonames lexicon (described in Sections 3.2 and 3.3) [16], a training dataset is required. We train the contextual classi er on a sample of 75 webpages with manually annotated geotags7.\nWe use 20 webpages for the test dataset, which are manually annotated with canonical geotags (e.g., \u2018Los Angeles, California, United States\u2019, which has a Geonames identi er).\nBaselines. We consider two baselines. We compare (using subsequently described metrics) the chosen candidate city per page to the correct city. e rst baseline, Random, randomly selects a candidate extraction as the correct one. e second baseline, Top Ranked, chooses the candidate assigned the highest probability by the contextual classi er. Comparisons with Top Ranked allows us to compute the e ect of the ILP model on geotagging performance.\nMetrics. We report precision and recall on the test set to evaluate performance.\nImplementation. We used the licensed version of Gurobi Optimizer [10] for modeling and solving ILP. All other code is wri en in Python 2.7. All experiments were run on a machine running 64bit Ubuntu 16.04 with Intel Core i7-4700MQ CPU @ 2.40GHz x 8 and 8GB RAM."}, {"heading": "4.2 Results", "text": "Comparison With Baselines Table 1 shows the performance of the proposed framework against the two baselines. Unsurprisingly, the random baseline performs the worst, and using contextual classi ers provides a signi cant improvement in comparison. e additive e ects of ILP to performance are promising, leading to improvements in both precision and recall.\nError Analysis. On further exploration of the wrong results, we found that ILP marks an incorrect candidate as correct when a city with high population is present in the footer (or other areas) of the webpage. We believe that, with be er text extraction, this issue can potentially be mitigated (by recognizing a segment of the\n6h p://www.darpa.mil/program/memex 7Because the ILP is unsupervised, these geotags are non-canonically annotated; i.e. a geotag is simply marked as correct or incorrect, but the canonical geotag is unknown (at least directly) to the framework algorithm. In that sense, the training set is weaker than the test set, which is annotated with canonical tags.\ntext as the footer, and ignoring it) or by assigning more weight to the main sections of the page.\nWrong candidates are also chosen when the name of a city in the lexicon corresponds to an alternate name of a popular city. For example, in some cases, the candidate \u2018the city\u2019 was labeled as the correct city. is is because the city of London is popularly known as \u2018the city\u2019 by locals. e context in which the candidate \u2018the city\u2019 is present also points to it being a city name e.g., \u2018I have recently moved to the city\u2019. Since the population of London is much higher than actual city the page is referring to, it is o en selected as the correct geolocation of the page. is is a more di cult problem to avoid without ad-hoc engineering e ort; we are currently investigating automated solutions to the problem."}, {"heading": "4.3 Discussion", "text": "Early results show that ILP provides a simple, unsupervised way to improve geotags output by upstream machine learning models. Although these results are preliminary, and experiments on more datasets and domains are required to validate them completely, we hypothesize that the main advantage of such a model is a systematic encoding of constraints that hold universally for the domain. In practice, the constraints are able to successfully deal with noisy candidates and candidate classi cations by using exclusivity and feasibility constraints.\nInterestingly, the ILP model also allows us to encode other pieces of geographical information, such as city population, that are clearly important in the real world when identifying geolocations in the face of noisy and uncertain information. Other pieces of information, also readily available from Geonames and other knowledge bases, can also be included in a similar manner. Some of these were already used in the present system e.g., slang/local terms, like \u2018the city\u2019 (for London), available in Geonames. An open issue is whether we can automatically distinguish between slang terms that lead to be er recall without necessarily harming precision, and those that end up causing more noise (such as \u2018the city\u2019). A promising alternative, on which there is limited work, is to acquire more labels by crowd-sourcing to facilitate be er training of the context-based classi er [11]. We are currently investigating such possibilities in human tra cking geotagging.\nFinally, we note that, although the model was used primarily for geotagging, it can also be systematically extended to other semantic types such as names and nationalities. Early experiments on some other semantic types important for human tra cking have yielded promising results. We are continuing to investigate the model further."}, {"heading": "5 FUTUREWORK", "text": "We will integrate more constraints into the ILP based model to improve performance even further, as well as more experiments\n(using both more datasets and more illicit domains) to validate the early results in this paper. Active e orts are already underway to scale the system on many millions of scraped human tra cking webpages.\nAcknowledgements is research is supported by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under contract number FA8750- 14-C0240. e views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the o cial policies or endorsements, either expressed or implied, of DARPA, AFRL, or the U.S. Government."}], "references": [{"title": "A survey of web information extraction systems", "author": ["C.-H. Chang", "M. Kayed", "M.R. Girgis", "K.F. Shaalan"], "venue": "IEEE transactions on knowledge and data engineering, 18(10):1411\u20131428", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "A uni\u0080ed architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Leveraging publicly available data to discern pa\u008aerns of human-tra\u0081cking activity", "author": ["A. Dubrawski", "K. Miller", "M. Barnes", "B. Boecking", "E. Kennedy"], "venue": "Journal of Human Tra\u0081cking, 1(1):65\u201385", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Text-based twi\u008aer user geolocation prediction", "author": ["B. Han", "P. Cook", "T. Baldwin"], "venue": "Journal of Arti\u0080cial Intelligence Research, 49:451\u2013500", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Information extraction in illicit domains", "author": ["M. Kejriwal", "P. Szekely"], "venue": "arXiv preprint arXiv:1703.03097", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Wrapper induction for information extraction", "author": ["N. Kushmerick"], "venue": "PhD thesis, University of Washington", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Toponym resolution in text: Annotation", "author": ["J.L. Leidner"], "venue": "evaluation and applications of spatial grounding of place names. Universal-Publishers", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Deepdive: Web-scale knowledge-base construction using statistical learning and inference", "author": ["F. Niu", "C. Zhang", "C. R\u00e9", "J.W. Shavlik"], "venue": "VLDS, 12:25\u201328", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Gurobi optimizer reference manual", "author": ["G. Optimization"], "venue": "URL: h\u0088p://www. gurobi. com,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Hybrid geo-information processing: Crowdsourced supervision of geo-spatial machine learning tasks", "author": ["F. Ostermann"], "venue": "Proceedings of the 18th AGILE International Conference on Geographic Information Science, Lisbon, Portugal, pages 9\u201312", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning dictionaries for information extraction by multi-level bootstrapping", "author": ["E. Rilo", "R. Jones"], "venue": "In AAAI/IAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Noun-phrase co-occurrence statistics for semiautomatic semantic lexicon construction", "author": ["B. Roark", "E. Charniak"], "venue": "Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 1110\u20131116. Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Text-driven toponym resolution using indirect supervision", "author": ["M. Speriosu", "J. Baldridge"], "venue": "ACL (1), pages 1466\u20131476", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["P. Szekely", "C.A. Knoblock", "J. Slepicka", "A. Philpot", "A. Singh", "C. Yin", "D. Kapoor", "P. Natarajan", "D. Marcu", "K. Knight"], "venue": "Building and using a knowledge graph to combat human tra\u0081cking. In International Semantic Web Conference, pages 205\u2013221. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Geonames", "author": ["M. Wick", "C. Boutreux"], "venue": "GeoNames Geographical Database", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Although precise numbers for human tra\u0081cking Web advertising activity are not known, they are very high, possibly in the tens of millions of (not necessarily unique) advertisements posted on the Web [3].", "startOffset": 199, "endOffset": 202}, {"referenceID": 7, "context": "Recent advances in information extraction and knowledge base construction technology, especially using techniques like deep neural networks and word embeddings [9], [2], gives investigators (such as law enforcement and intelligence agencies) the valuable opportunity to turn the Web against illicit players.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Recent advances in information extraction and knowledge base construction technology, especially using techniques like deep neural networks and word embeddings [9], [2], gives investigators (such as law enforcement and intelligence agencies) the valuable opportunity to turn the Web against illicit players.", "startOffset": 165, "endOffset": 168}, {"referenceID": 4, "context": "some speci\u0080c challenges [6], [15].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "some speci\u0080c challenges [6], [15].", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "Such tags are o\u0089en present in free text \u0080elds like description or the page body, and not within structured HTML tags (hence, cannot be extracted by wrapper-based extractors [7]).", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Using a lexicon to directly extract geolocations is problematic for this reason; richer clues like context (such as the words surrounding an extraction) are necessary for disambiguation [6], [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Using a lexicon to directly extract geolocations is problematic for this reason; richer clues like context (such as the words surrounding an extraction) are necessary for disambiguation [6], [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 14, "context": "as geolocation candidates [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "To determine the probability of the candidate being a geolocation, we use a recent machine learning-based approach that uses context features [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "IE is an old research area for which a wide range of techniques have been proposed; for an accessible survey of Web IE approaches, we refer the reader to [1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "\u008ce goals of this work are similar to other geolocation prediction system for \u2018di\u0081cult\u2019 datasets like Twi\u008aer, a good example being [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 145, "endOffset": 148}, {"referenceID": 13, "context": "However, we note that illicit domain challenges are di\u0082erent from those of social media, an important example being information obfuscation [3], [6], [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "For example, word embedding methods, used in the contextual classi\u0080er in our framework, have achieved notable advances in NLP (and especially IE) performance [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "Lexicon-based IE has also received much coverage in the literature, an in\u0083uential recent work being [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "Given its importance, geolocation extraction has received a lot of focused a\u008aention in the literature, an important related work being the recent text and context-based approach by Speriosu and Baldridge [14].", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "Some of the techniques in this work, such as usage of text and populations, derive from extant techniques on toponym resolution [8], [14].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "Some of the techniques in this work, such as usage of text and populations, derive from extant techniques on toponym resolution [8], [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "A good description may be found in the book by Leidner [8].", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "We note, however, that except for a recent paper that we published [6], no work has tackled the challenges of high-performance geolocation extraction in domains Figure 1: A work\u0083ow-level illustration of the geotagging", "startOffset": 67, "endOffset": 70}, {"referenceID": 11, "context": "available dictionaries, also called semantic lexicons [13], for identifying candidate geotags from text.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "For geotags, a standard nearcomprehensive lexicon is Geonames [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "such feature functions are o\u0089en problematic for irregular, obfuscated domains like human tra\u0081cking [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "high supervision problems by \u0080rst e\u0081ciently deriving low-dimensional word embeddings from an extracted text corpora as word feature vectors (WFVs) [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "Since we use the supervised contextual classi\u0080er described in our previous work [6], in conjunction with the Geonames lexicon (described in Sections 3.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "3) [16], a training dataset is required.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "We used the licensed version of Gurobi Optimizer [10] for modeling and solving ILP.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "A promising alternative, on which there is limited work, is to acquire more labels by crowd-sourcing to facilitate be\u008aer training of the context-based classi\u0080er [11].", "startOffset": 161, "endOffset": 165}], "year": 2017, "abstractText": "Extracting geographical tags from webpages is a well-motiva-ted application in many domains. In illicit domains with unusual language models, like human tra\u0081cking, extracting geotags with both high precision and recall is a challenging problem. In this paper, we describe a geotag extraction framework in which context, constraints and the openly available Geonames knowledge base work in tandem in an Integer Linear Programming (ILP) model to achieve good performance. In preliminary empirical investigations, the framework improves precision by 28.57% and F-measure by 36.9% on a di\u0081cult human tra\u0081cking geotagging task compared to a machine learning-based baseline. \u008ce method is already being integrated into an existing knowledge base construction system widely used by US law enforcement agencies to combat human tra\u0081cking.", "creator": "LaTeX with hyperref package"}}}