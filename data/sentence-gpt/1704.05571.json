{"id": "1704.05571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain", "abstract": "Word embeddings have made enormous inroads in recent years in a wide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two financial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classifier using the labeled context vectors, and use the trained classifier to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires little feature crafting effort and performs well across roles. The goal of this paper is to present a framework that can be easily modified without resorting to any specific features, including automated, dynamic, and dynamic code.", "histories": [["v1", "Wed, 19 Apr 2017 00:55:23 GMT  (787kb)", "http://arxiv.org/abs/1704.05571v1", "DSMM 2017 workshop at ACM SIGMOD conference"]], "COMMENTS": "DSMM 2017 workshop at ACM SIGMOD conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mayank kejriwal"], "accepted": false, "id": "1704.05571"}, "pdf": {"name": "1704.05571.pdf", "metadata": {"source": "META", "title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain", "authors": ["Mayank Kejriwal"], "emails": ["kejriwal@isi.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n05 57\n1v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\nWord embeddings have made enormous inroads in recent years in awide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two nancial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classi er using the labeled context vectors, and use the trained classi er to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires li le feature cra ing e ort and performswell across roles."}, {"heading": "KEYWORDS", "text": "Role Relevance; Distributional Semantics; Triples Ranking; Word2Vec"}, {"heading": "ACM Reference format:", "text": "Mayank Kejriwal. 2017. Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain. In Proceedings of DSMM\u201917, Chicago, IL, USA, May 14, 2017, 3 pages. DOI: h p://dx.doi.org/10.1145/3077240.3077249"}, {"heading": "METHODS AND RESULTS", "text": "Problem Statement and Preliminaries: Given a triple t = (fh , r , ft ) that consists of a head nancial entity fh , a tail nancial entity ft , and a role r , typically from a closed domain-speci c universe R of roles, role relevance prediction is de ned as the problem of assigning a relevance score in [0, 1] to the triple in a given context c . In the FEIII 2017 challenge, c is just a set of three sentences in nancial document lings by the head entity, with the tail entity extracted from the sentences. For the preliminary results in this paper, we use the labeled data provided by the challenge organizers for predicting role relevance.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. DSMM\u201917, Chicago, IL, USA \u00a9 2017 ACM. 978-1-4503-5031-0/17/05. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077240.3077249\nRole relevance is not unlike an ad-hoc Information Retrieval (IR) problem in that the relevance function is domain-speci c and adhoc [3]. is is indicated by the fact that, in some instances, manual annotators disagree on relevance. One reason could be that the two entities are subjectively relevant (in a generic sense), but not with respect to the stated role or context in the triple.\nHowever, role relevance is also di erent from ad-hoc IR because, unlike traditional IR, where a query is usually phrasal and a list of ranked documents is retrieved and evaluated on metrics like Normalized Discounted Cumulative Gain (NDCG), the query in role relevance is simply a role rq , and the goal is to rank available triples in a nancial knowledge graph with the caveat that (1) the role r in the triple is exactly rq for a non-zero relevance score to be assigned to the triple, (2) the triple must be scored \u2018contextually\u2019.\nIn a machine learning framework, one can approach this problem by using labeled training data to train a classi er for each role. In order to do this e ectively, one also has to devise \u2018feature cra - ing functions\u2019 that convert each triple and context to a feature vector. Feature cra ing o en involves trial and error, and manual labor, usually in increasing proportion to domain speci city. For example, one could search for the presence of certain \u2018role-relevant\u2019 keywords in the context as features that indicate high role relevance.\nApproach: In contrast with feature cra ing approaches, we propose an approach that uses skip-gram neural networks to convert contexts into role-independent, low-dimensional feature vectors to leverage data in a pooled fashion across roles, and then trains a role-speci c classi er like a random forest to learn the nuances of each role.\ne overall approach is described as follows. First, we collect available context sentences, whether labeled (either as relevant, highly relevant or irrelevant with respect to any given role) or unlabeled, and build a \u2018corpus\u2019 where each sentence is akin to a document. We train aword embeddingmodel using skip-gramword2vec [1], and with only 30 dimensions in the latent space. e low dimensionality is to prevent the vectors from being too unstable, as the domain-speci c corpus is not large enough (i.e. does not have enough unique words) to usefully accommodate more parameters. Each word vector thus obtained is normalized by the model to lie on the 30-dimensional unit hypersphere. e dimensionality was set heuristically; we leave for future work to evaluate sensitivity of our results to this hyperparameter.\nOnce a word embedding model has been trained, we can generate feature vectors for each set of context sentences in a triple in several di erent ways. For the initial system we built, described herein, we consider a normalized bag-of-words average: we add"}, {"heading": "DSMM\u201917, May 14, 2017, Chicago, IL, USA Mayank Kejriwal", "text": "the corresponding vectors of all words occurring in the context sentences, without regard for word order, and then (l2-)normalize the summed vector so that it lies on the same hypersphere as each word vector. Note that the number of times a word occurs in the context sentences does ma er as it will get included in the sum each time it occurs. In that sense, the summed \u2018context feature vector\u2019 (CFV) thus obtained is like a low-dimensional, continuous version of a document vector obtained using a classic measure like tf-idf.\nWe can now approach the problem from a traditional (binary classi cation) machine learning standpoint by training a di erent classi er for each role. For our experiments, we use a random forest classi er, o en known to work extremely well, and for simplicity, all contextual triples marked as \u2018highly relevant\u2019 or \u2018relevant\u2019 are assigned a positive-class label of 1.0 while \u2018irrelevant\u2019 triples are assigned a negative-class label of 0.0. e CFV is the feature vector assigned to the triple. At test time, we compute the CFV for the context sentences accompanying the contextual test triple, and apply the classi er corresponding to the role in the triple. e score output by the classi er, always a real value between 0.0 and 1.0 is interpreted as the role relevance probability of the triple. ese scores can then be used to compute metrics such as precision, recall and NDCG, a er ranking the triples in decreasing order of score.\nAdvantages: e proposed approach has some important advantages. Perhaps the most important advantage is that it does not require a human to devise \u2018clever\u2019 features or an arduous trial-anderror process of labor-intensive feature experimentation. A second important advantage is that, due to using machine learning on two fronts (feature cra ing, as well as relevance prediction), any additional data (whether labeled or unlabeled) can be leveraged to improve performance. For example, in the FEIII 2017 role relevance prediction challenge, a much larger (than the labeled training set) \u2018working\u2019 set of unlabeled contextual triples1 was released by challenge organizers. Clearly, the availability of this extra information, even without labels, presents an opportunity. A third advantage is that, while features are domain dependent, they are role independent. We subsequently illustrate some intuitive semantic properties that arise from the features as a natural consequence of this.\nPreliminary Results: We describe some results obtained by using labeled data provided by the FEIII 2017 challenge organizers.\ne details of the challenge and dataset are described on the website2. ese results were obtained prior to the test dataset being released by challenge organizers, or test results being submi ed (and scored) by participants. e original dataset contained 20 roles, of which 10 were \u2018plural\u2019 equivalents of 10 singular roles3. For better generalization, we combined singular-plural equivalents so that there were ten unique roles.\nOn a prototypical pre-challenge evaluation for three of the ten roles, namely a liate, trustee and issuer across three partitions of the data (10%, 50% and 90% across each role used for training, with the rest withheld for testing), we obtained precision and recall metrics across the three roles (Table 1). In no case is the F1-Measure\n1By a contextual triple, we mean a triple that is accompanied by a set (usually three) of context sentences. 2h ps://ir.nist.gov/ds n/2017-challenge.html 3For example, a liate and a liates.\nbelow 90%, and inmany cases it is over 95%. In futurework, wewill be validating these results further across roles and with more experimental trials; however, early results do seem to indicate that a promising word embedding-based approach is a promising avenue for obtaining accurate role relevance predictions without investing signi cantly in manual feature engineering e ort.\ne post-challenge evaluation resultswere largely consistent with\nthe numbers in Table 1. For example, against a (withheld) groundtruth where highly relevant triples had to be ranked over relevant triples, which in turn had to be ranked over neutral and irrelevant triples (in that order), to receive optimal scores, our approach scored 91.62% NDCG [2].\nSemantic Similarity Experiment: Even more interesting are the semantics captured by the word embeddings. To show this qualitatively, we located the nearest neighbors of three \u2018seed\u2019 keywords, namely \u2018company\u2019, \u2018regulators\u2019 and \u2018city\u2019, in the word embedding vector space (Table 2). Results show that the embedding space has captured the \u2018meaning\u2019 of the words remarkably well in a contextual sense, despite not having much data (incl. training+working) per word embedding standards. is is in agreement with previous results obtained over large corpora, but to the best of our knowledge, has never been satisfactorily illustrated for smaller domain-speci c corpora such as in the nance/SEC domain. Nonetheless, we do believe that with more data, the results should further improve.\nFutureWork: e approach and results described in this paper were highly preliminary, as to the best of our knowledge, word embeddings have thus far not been used for nancial role relevance prediction. We believe that many avenues for future work exist; at present, we are exploring (1) intelligent ways of computing CFVs rather than just normalized average (e.g., using recently proposed techniques such as paragraph2vec), (2) using external information, including distant supervision, to boost training power, (3) pooling labeled data between classi ers to minimize labeling e ort."}, {"heading": "Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain DSMM\u201917, May 14, 2017, Chicago, IL, USA", "text": "e report of the organizing commi ee. In Proceedings of the Workshop on Data Science for Macro-Modeling (DSMM@SIGMOD), 2017. [3] C. Zhai and J. La erty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342. ACM, 2001.\nThis figure \"approach.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"cc.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"cities-tsne.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"contextualclassifier.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"fs-1.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"riexample.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1\nThis figure \"run-time.png\" is available in \"png\" format from:\nhttp://arxiv.org/ps/1704.05571v1"}], "references": [{"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Financial entity identi\u0080cation and information integration (FEIII) challenge 2017:  Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain DSMM\u201917, May 14, 2017, Chicago, IL, USA \u008ce report of the organizing commi\u008aee", "author": ["L. Raschid", "D. Burdick", "M. Flood", "J. Grant", "J. Langsam", "I. Soboro", "E. Zotkina"], "venue": "In Proceedings of the Workshop on Data Science for Macro-Modeling (DSMM@SIGMOD),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "La\u0082erty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 334\u2013342", "author": ["J.C. Zhai"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "(fh , r , ft ) that consists of a head \u0080nancial entity fh , a tail \u0080nancial entity ft , and a role r , typically from a closed domain-speci\u0080c universe R of roles, role relevance prediction is de\u0080ned as the problem of assigning a relevance score in [0, 1] to the triple in a given context c .", "startOffset": 248, "endOffset": 254}, {"referenceID": 2, "context": "3077249 Role relevance is not unlike an ad-hoc Information Retrieval (IR) problem in that the relevance function is domain-speci\u0080c and adhoc [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "[1], and with only 30 dimensions in the latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "62% NDCG [2].", "startOffset": 9, "endOffset": 12}], "year": 2017, "abstractText": "Word embeddings have made enormous inroads in recent years in awide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two \u0080nancial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classi\u0080er using the labeled context vectors, and use the trained classi\u0080er to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires li\u008ale feature cra\u0089ing e\u0082ort and performswell across roles.", "creator": "LaTeX with hyperref package"}}}