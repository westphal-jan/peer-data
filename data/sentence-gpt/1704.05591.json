{"id": "1704.05591", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "OCRAPOSE II: An OCR-based indoor positioning system using mobile phone images", "abstract": "In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition. We report the system using a simple database model. In the present paper, we present a system with an OCR system consisting of a linear, linear and linear OCR system. These models generate a small sample of high quality text and images using a matrix algorithm. We further explore the feasibility of the OCR system for its own application. In a recent paper, we demonstrated a computational algorithm that can perform the localization of the OCR system in real-time. This system provides us with the optimal translation time and data size and the necessary computational resources. In a future paper, we might use the system in future studies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 19 Apr 2017 02:43:23 GMT  (8582kb,D)", "http://arxiv.org/abs/1704.05591v1", "14 pages, 22 Figures"]], "COMMENTS": "14 pages, 22 Figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed sadeghi", "shahrokh valaee", "shahram shirani"], "accepted": false, "id": "1704.05591"}, "pdf": {"name": "1704.05591.pdf", "metadata": {"source": "CRF", "title": "OCRAPOSE II: An OCR-based indoor positioning system using mobile phone images", "authors": ["Hamed Sadeghi", "Shahrokh Valaee", "Shahram Shirani"], "emails": ["hsadeghi@ece.utoronto.ca.", "valaee@ece.utoronto.ca."], "sections": [{"heading": null, "text": "Index Terms\u2014Indoor localization, depth estimation, angle-of-view estimation, OCR, vanishing point.\nF"}, {"heading": "1 INTRODUCTION", "text": "THE most prevalent method for indoor localization isbased on fusion of Wi-Fi RSS fingerprints with inertial sensors data [1] . These methods require a fair number of Wi-Fi access points to be visible at each location [2]. Under these conditions, they demonstrate localization errors about 2 meters, where training points have granularity of 1 meter [3]. In scenarios where enough Wi-Fi access points are not available; access to Wi-Fi RSS reader hardware is blocked (such as iPhones), or when greater localization accuracy (ex. sub-meter accuracy) is required, image-based methods can be used as an effective solution.\nAlthough image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8]. The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.g. logo-based) [12], [13].\nBoth categories of image-based localization methods, i.e. landmark-based and image retrieval-based, require a database of images as well as 3D coordinates/locations to be measured and stored in the training phase. The data gathering is labor intensive, which is not always possible. Furthermore, most of the methods proposed in the literature, utilize feature extraction and matching for localization [10], [13], while feature extraction and corresponding database creation for a large environment is highly time consuming. Moreover, simple feature matching is not robust to large changes in angle-of-view (AOV). That is, almost the same scene (i.e. set of objects) present in two images cannot be acceptably matched if the difference in AOV is large [14], [15]. Methods such as ASIFT (affine SIFT) [16], which are"}, {"heading": "H. Sadeghi and S. Valaee are with the Department of Electrical and Computer Engineering, University of Toronto, ON, Canada, M5S 2E4. E-mail: hsadeghi,valaee@ece.utoronto.ca.", "text": ""}, {"heading": "S. Shirani is with the Department of Electrical and Computer Engineering,", "text": "McMaster University, ON, Canada, L8S 4L8. Email: shirani@mcmaster.ca.\nrobust to AOV changes have high complexity. For instance, the complexity of ASIFT is at least 1.5 times of that of SIFT [17]. More importantly, we have observed that in a number of indoor environments such as office buildings, parkings, airports, grocery stores, etc., where the distinctive landmarks are text and/or numbers (characters in general), the aforementioned methods fail to provide good location recognition performance for a considerable percentage of queries.\nIn scenarios with large image databases, stereo feature matching is computationally expensive and is not performed for best matches detection in the image retrievalbased methods. Instead, bag of features-based methods are used to find the best matching landmark/image(s). Here, we use stereo feature matching to illustrate the existence of confusing similar features and lack of distinctive features as the main reasons of failure in best match detection (retrieval). Fig. 1 depicts a university building scenario, where existing literature methods demonstrate poor localization performance.\nThe reason of failure in detecting the correct existing characters using the landmark-based methods is that the characters (numbers) are not as textured as commonly used landmarks such as commercial logos or fiducial markers [12]. Hence, point feature-based recognition approaches [18] fail to extract enough distinctive features required to distinguish different numbers from each other. For instance, Fig. 1a illustrates why stereo feature matching cannot distinguish which one of the existing database numbers, i.e. 4000, 4148, 4110 or 4010 corresponds to the query number (4010). As seen, although digit 4 is common among all different numbers, most of features are concentrated around it. This confuses the feature-based recognizer and it cannot find the correct one, i.e. the last bottom one. The same problem exists when we want to distinguish between gate numbers in airports, for instance between gate numbers B42 and\nar X\niv :1\n70 4.\n05 59\n1v 1\n[ cs\n.C V\n] 1\n9 A\npr 2\n01 7\n2 (a)\n(b)\nFig. 1. Sample scenarios, where (a) landmark-based and (b) image retrieval-based methods fail to find the correct match due to the lack of enough distinctive features; the left-hand images are the same query images, the right-hand ones are different database images containing different numbers\nB43 . Furthermore, there is also a high probability of seeing similar (repeated) scenery from different locations in the mentioned scenarios. Hence, image retrieval-based methods might frequently fail to find the best matching image, i.e.\nthe closest location to the query. As seen in Fig. 1b, the last bottom image, which is the correct database image containing the same numbers (4170) has achieved the minimum number of common matching features due to the huge difference in the angle-of-view. As seen, for the selected matching threshold, the door handle has got matched as well as the digit 1. On the other hand, although the middle right image contains the number 4177, it has gained the maximum number of common matches with the query due to a angle-of-view similar to that of the query.\nOther image retrieval techniques such as bag of features or localization-specific ones [10] might also fail to retrieve the correct matching characters. This is due to the existence of more similar (confusing) objects (features) rather than distinctive ones. Doors, windows, door handles, etc are among such objects in our scenario (Fig. 1b). We evaluate the recognition performance of the Liang\u2019s method [10] as an example in our experiments. It should be noted that the the image-retrieval based methods mentioned here use general point feature-based retrieval techniques or retrieval methods designed for location recognition. Use of point feature-based techniques proposed originally for character recognition such as [19] will result in an OCR-based localization method, which resides in the same category as our method.\nThe mentioned issues, i.e. labor extensiveness and poor recognition performance of the conventional point featurebased methods, motivate the use of OCR to recognize the existing location-distinctive characters. By recognizing these characters in the query image, OCR can provide a rough location estimate using a building floor plan. We assume a floor plan tagged with location-distinctive characters and their locations. An example is seen in Fig. 15. We do not use non location-distinctive characters shared at different locations such as exit signs that potentially confuse the localizer. Furthermore, we only rely on stable characters such as room/gate numbers rather than the texts/numbers on the bulletin boards, which could be replaced or removed after a while. Besides location recognition, OCR provides some clues for fine localization as will be demonstrated later. Hence, the proposed image-based system performs location recognition and location estimation based on OCR.\nWe do not collect a location-tagged image database for localization. Our system only requires a floor plan tagged with characters locations and the width of the OCR boxes, which can be measured or determined based on typical values. Furthermore, it does not require any feature detection, extraction or matching. Another characteristic of the proposed localization system is that it can be integrated with other feature-based systems, either landmark-based or image retrieval-based. In [20], we demonstrated how OCR can improve a landmark-based localization system in terms of location recognition and localization accuracy. It should be noted that we are not proposing a character detection/recognition algorithm here. Instead, we focus on OCRbased location recognition as well as the fine localization of user using two novel formulas for depth and angle-of-view estimation. Our contributions are three-fold\n\u2022 An indoor localization system, called OCRAPOSE II, based on OCR\n3 \u2022 A novel formula for query angle-of-view estimation \u2022 A novel formula for query depth estimation\nTaking advantage of OCR for rough localization and utilizing the proposed formulas to refine the estimate, we provide better localization results in two sample scenarios. The OCRAPOSE II results are better than that of the stateof-the-art in terms of location recognition rate and mean localization error. Furthermore, it shown that the proposed system localization performance is maintained under sparse database locations condition, while that of the benchmarks degrades significantly.\nThis paper is organized as follows. Section 2 briefly introduces the related literature work and explains the benchmarks in details. Section 3 introduces the proposed system and explains its details. Section 5 compares the performance of proposed system with the benchmarks through extensive experiments in two scenarios. Finally, Section 6 concludes the paper."}, {"heading": "2 RELATED WORK", "text": "Although image-based robot localization and augmented reality applications have rich literature, image-based user localization has been an active field of research only over the last decade. As explained, the proposed image-based methods can be categorized in two classes. Here, we provide some examples from each category and explain a few methods related works as well as the selected benchmarks in more details.\nImage retrieval-based methods can be applied in general scenarios. These methods perform very well in applications with abundant distinctive landmarks such as shopping malls [10] or scenarios with unrepeated scenery such as outdoors (using Google street view database) [21]. They work on the basis that visually different images are taken from different positions. That is, the captured image works as the fingerprint of its (camera) location. Due to this implicit assumption, these methods give poor location estimates in environments with repeated scenery such as office buildings, parkings, airports, etc. Moreover, a large database of images is required to be collected and Geo-tagged (posetagged [11]) in the training phase. Furthermore, in the test phase, the query image is compared with the entire (or part of) database to find the best match [10] or best pair [21]. Hence, a considerable processing time is required to perform localization during both training and query phases.\nLandmark-based methods are applicable in scenarios, where highly textured and distinctive landmarks are present [10], [11]. These methods use logo/landmark detection techniques [22], [23] to detect existing landmarks/logos in the query image. Afterwards, the previously measured 3D coordinates of the detected landmark image is used to estimate the camera (full) matrix. The query location can be obtained afterwards from the estimated camera matrix. For instance, in shopping malls, where textured commercial logos are ubiquitous, these methods can provide great localization accuracy [10]. Texture is usually required to extract feature points necessary for unique logo detection. In terms of database size, in contrary to image retrieval-based methods, landmark-based methods require a small image database of existing landmarks as well as some of their actual 3D\ncoordinates. Therefore, their running time is much less than the image retrieval-based methods.\nAmong a few works proposed in the literature that utilize OCR for localization, [24] can be mentioned for its explicit use of OCR as a rough localizer. In fact, it proposes an OCR-assisted multi sensor method for navigation in emergency indoor scenarios. The proposed method only provides rough location estimates and no fine localization method has been proposed.\nA good indoor localization system that we use as one of our benchmarks is proposed in [10] that uses CBIR (contentbased image retrieval) to perform rough localization. It requires a location-tagged database. In the training phase, SIFT features of all images are extracted and loaded into a single FLANN kd-tree. In the test phase, SIFT features of the query image are extracted and each one is looked up in the FLANN tree. In fact, K nearest neighbors of each feature are found and the votes of the corresponding image in the database is increased by one, i.e. voting scheme. Thereafter, database images are ranked based on the collected votes. After finding the K best matches, geometric consistency and SIFT features angle difference checks are performed to prune the matched features between the query image and best matches. Next, the best match is selected among K best matches. Following this stage, they perform pose estimation (fine localization). In fact, they use phone sensors including accelerometer and magnetometer in order to find rotation angles. Since, this side information is not available in our problem, we only use their best match selection as a benchmark coarse localizer and refer to this work as Liang\u2019s method in the experimental results.\nThe second benchmark is the method proposed in [21], which proposes a feature-based method for fine localization. The proposed method is composed of two stages. In the first stage, it finds the best pair of images in the database. The best pair is defined as the pair of images that the linear combination of their image descriptors estimates the query descriptor with the least error. Next, it linearly combines (i.e. interpolates) the locations of best matches with an interpolation factor in order to estimate the query location. It suggests that the interpolation factor be an affine function of the visual similarity between database and query images. The visual similarity is computed using the well-known bag of feature representation of database and query images. The offset and slope parameters of the linear relation between the interpolation factor and the visual similarity is learned on an independent database in the training phase. For image similarity computation, the bag of features representation is used in [21]. In this paper, we use the modified VLAD\u2019s representation [25], which has demonstrated supreme performance. We refer to this benchmark as Torii\u2019s method.\nIn [20], we proposed an OCR-aided localization system called OCRAPOSE. The system utilizes OCR to read the existing characters in the query image and provides a rough location estimate using character location-tagged floor plan. Afterwards, it performs OCR-aided stereo feature matching between the query and database images containing the same characters. The stereo matching results are used to estimate a homography matrix. The homography is used to find the world coordinates of the query features. Next, a PnP problem is solved for the query feature points and their world\n4 coordinates to obtain a fine location estimate. The main difference between the OCRAPOSE and the OCRAPOSE II proposed here is that teh new system does not require any world coordinates measurement, image database collection or point feature extraction/matching. Hence, it is more practical in terms of alleviating the requirement for huge image database collection and avoiding point feature processing complexity. Furthermore, two novel projective Geometrybased formulas are proposed in OCRAPOSE II for depth and angle-of-view estimation.\nAll of the mentioned benchmarks require training images. Therefore, we have to collect a number of training images in each scenario. It should be noted that our method does not require any image databases. The only side information required is the 3D locations of characters centroid in indoor scenarios as well as the width of the OCR boxes defined later. This information are required to perform query fine localization on the floor plan."}, {"heading": "3 THE PROPOSED SYSTEM (OCRAPOSE II)", "text": "Fig. 2 depicts the structure of the proposed system. As seen, the proposed system detects the horizontal vanishing point in order to estimate the user\u2019s AOV with respect to the location of the centroid of the characters. Furthermore, it detects and recognizes the characters in the query image, which along with the floor plan provides the required information for rough localization. Next, using the estimated AOV and the width of the OCR box in the image and real world, depth of the query is estimated. Finally, a fine estimate of the user\u2019s location is computed using the estimated depth and AOV. In the sequel, we explain the role of system blocks in details."}, {"heading": "3.1 Characters detection", "text": "As seen in Fig. 2, the query image is input to the characters detection block. The role of the characters detection block is to detect the regions of interest (RoIs) that contain the characters. False positive regions (non-character regions detected as character regions) are tolerable as long as we are able to identify and remove them from the list of true positives.\nText detection in natural scene images is an extremely difficult problem [26], [27] and might need parameters fine tuning in the scenario under investigation. On the other hand, proposing a text detection algorithm is out of the scope of this paper. Hence, we use some of the state-of-theart techniques to design our text detection block. It should be noted that utilizing a better text detection algorithm results in higher location recognition performance of the proposed method. Consequently, it results in larger performance gap between the proposed method and the featurebased ones.\nIn indoor scenarios, text is usually printed uniformly over a board (plate). It causes the text regions to appear with uniform colors (intensities) inside the query images. Such regions are good candidates to be detected as maximally stable extremal regions (MSER) [14] as also suggested by [27]. Hence, we extract MSER regions as potential characters.\nIn the next step, we perform Geometric filtering on the detected MSER regions. In works such as [27], this Geometric filtering is carried out after processing the MSER regions.\nWe found this to be inefficient since it is computationally cheaper to remove the improbable regions first and avoid the required excess processing later. Therefore, we perform Geometric filtering first as follows and refine the MSER regions afterwards. Define the set (R) of n detected MSER regions as\nR = {R1, R2, \u00b7 \u00b7 \u00b7 , Rn}. (1)\nOur Geometric filtering is composed of two steps. First, we remove large regions whose area (i.e. number of pixels) is more than N times the median area of all regions. Next, we remove regions whose orientation is more than degrees away from the vertical orientation. This is since each character is usually oriented vertically in a query camera image captured with zero pitch angle.\nMathematically, a region Ri passes through the Geometric filter if\n\u2022 |Ri| < N \u00b7median(|R|) \u2022 |\u2220Ri \u2212 90\u25e6| <\nwhere |Ri| = # pixels(Ri), |R| = (|R1|, \u00b7 \u00b7 \u00b7 , |Rn|)T and \u2220Ri = Orientation(Ri). In our experiments, we found appropriate values of N and to be 10 and 5, respectively.\nMSER region detector is known to be sensitive to image blur [27]. Blurring causes the MSER detector to miss or detect distorted regions, which makes the OCR difficult. Hence, we enhance the MSER regions using [27]. [27] uses Canny edge detector to enhance the outline of detected extremal regions. In fact, it prunes the MSER regions along the detected gradient direction suggested by the Canny edge detector.\nWe avoid further refinement or using other detection techniques proposed in the literature such as stroke width transform because of their complexity. The mentioned level of refinement turned out to be sufficient in our applications. We only perform simple preprocessing operations before inputting the image to the OCR engine as explained in Section 3.2."}, {"heading": "3.2 OCR", "text": "The OCR block recognizes the existing characters in the detected regions. We perform some preprocessing prior to inputting the enhanced MSER regions to the OCR block. This preprocessing includes global binarization and removing relatively small/large regions. Next, we input the remaining regions to the OCR engine. In order to perform OCR, we use MATLAB OCR function, which is using Google Tesseract engine. The engine is based on convolutional neural network approach for character recognition. Tesseract is believed to be one of the most accurate OCR engines [28]."}, {"heading": "3.3 angle-of-view (AOV) estimation", "text": "We detect the line segments in the query image and utilize them in order to estimate the horizontal vanishing point (VP). Next, we derive a novel formula for AOV estimation using the detected vanishing point.\n5\nFig. 2. Block diagram of the proposed localization system (OCRAPOSE II)\nFig. 3. Output of OCR"}, {"heading": "3.3.1 Vanishing point detection", "text": "Vanishing points present in the image contain information about query camera rotation with respect to the seen objects [29]. If the roll angle of the query camera is zero with respect to the horizon, horizontal (vertical) line segments will intersect in the horizontal (vertical) VP. Moreover, the perpendicular (out of plane) VP can be obtained by the cross product of other two VPs. Therefore, image line segments can help us find all vanishing points hence the complete query rotation matrix.\nWe perform line segments processing for VP detection. As we will explain in Section 3.3, only horizontal VP is needed in our method. Hence, we detect it by solving a novel robust optimization problem partly inspired by [30]. In [30], line segments are detected using an edge-based method proposed in [31]. We also use this method to extract line segments. Afterwards, we estimate the horizontal vanishing point location in the image using detected line segments. In order to do this, we propose a novel robust optimization problem inspired by [30] using RANSAC [32].\nAssume the total number of detected lines is L and RANSAC selects n line segments in each iteration. n should be at least 2. In order to find the vanishing point, the proposed optimization problem finds the point in the image plane that has the minimum total weighted distances from all n lines considered in the current iteration (i.e. potential\ninliers). In fact, in each iteration of RANSAC, we solve\nV P = argmin x n\u2211 i=1 |Li| (dist(x, Li))2 (2)\nwhere dist(x, Li) represents the distance of image point x from the ith line (i.e. Li). Furthermore, |Li| stands for the length of Li in terms of number of pixels. In fact, we minimize the weighted sum of horizontal VP distances from the detected lines. Hence, the solution is essentially the MAP estimate of the VP in which the priors are assigned proportional to the line lengths. Moreover, the distance noise is assumed Gaussian. Under these conditions, minimizing the proposed cost function maximizes the A-posteriori probability. The proposed minimization problem has a closed form solution that follows. Proposition 1. Assume the ith almost horizontal line (Li)\ndetected in the image is defined as follows\nLi : aix+ biy + c = 0 for i = 1, \u00b7 \u00b7 \u00b7 , n. (3)\nThe image coordinates of the horizontal VP can be estimated as\nV Px = BF\u2212CE AE\u2212BD (4) V Py = CD\u2212AF AE\u2212BD (5)\nwhere\nA = \u2211n i=1 1 \u03b3i |Li|a2i\nB = \u2211n i=1 1 \u03b3i |Li|aibi\nC = \u2211n i=1 1 \u03b3i |Li|aici D = B\nE = \u2211n i=1 1 \u03b3i |Li|b2i\nF = \u2211n i=1 1 \u03b3i |Li|bici (6)\nand \u03b3i = a2i + b 2 i .\nProof 1. The unconstrained optimization problem defined in (2) should be solved in order to find the VP. In order to perform this, we should set the derivative of the cost function\nCost = n\u2211 i=1 |Li|(dist(x, Li))2 (7)\n6 pixels -1500 -1000 -500 0 500 1000 1500 2000 2500 3000 3500 pi xe ls 0 500 1000 1500 2000 2500\nFig. 4 shows the result of applying the proposed method on a sample query image taken inside a university building. As stated, in order to derive practical formulas for AOV and depth estimation, we make some assumptions about the query rotation angles with respect to the characters centroid (i.e. reference frame origin). First, we assume that the roll angle is zero. This assumption can be easily relaxed by some image preprocessing. As stated in [33], if the vertical vanishing point (i.e. V V P = (xver yver)T ) is known, the roll angle can be estimated as\nroll = arctan( xver yver ) (10)\nVertical vanishing point can also be estimated using a method similar to the one proposed for the horizontal VP. Once the roll angle is estimated, the query image can be rotated accordingly in order to remove the non-zero roll effect. For human user localization, we need to perform this pre-processing to compensate for nonzero roll.\nWe assume the tilt angle is zero. This assumption is not exactly met in practice but the practical values are fairly close to zero. The difference comes from the height difference between the characters centroid and camera center. In Section 4, we study the range of (non-zero) tilt angles in practice and demonstrate their negligible effect on the accuracy of AOV and depth formulas.\nThe last assumption is about the pan angle. For indoor human user localization, we assume that the pan angle is equal to the AOV as depicted in Fig. 5. This is a reasonable assumption since humans tend to orient their phone\nFig. 5. The indoor scenario considered in this paper\ntowards the point of interest. Technically speaking, they effectively align the normal vector of the image plane with the line connecting the target (characters centroid) to the camera center. The motivation is to place the location of the OCR detected characters in the center of attention, i.e. the middle of the image. It can be used as a test to see whether the pan angle conditions are met.\nSince tilt and roll are assumed to be zero, the only unknown angle is pan. As mentioned in [10], in indoor images, most of the visible objects are located on a single wall. In conclusion, lines seen (almost) horizontal/vertical in the query image are actually horizontal/vertical in the real world.\nConsider the scenario depicted in Fig. 5. As seen, the pan angle is equal to the AOV. The location of horizontal VP is a function of the pan angle. Hence, knowing the horizontal VP, we can estimate the AOV. In the following section, we propose a novel formula that estimates the AOV in the scenario depicted in Fig. 5."}, {"heading": "3.3.2 Estimating AOV from the horizontal vanishing point", "text": "In this section, we propose a novel formula for estimating the AOV in the practical scenario depicted in Fig. 5 using the estimated horizontal VP. We only use the OCR box corners in the derivation of the formula and it is not required to detect corners in the actual query phase in practice. The OCR box is a fictitious box surrounding the characters as depicted in Fig. 6.\nThe approximate formula that will be derived for AOV estimation is independent of the dimensions of the OCR box. It is only the depth estimation formula that requires the actual width of the box. Hence, what we need to do is to only measure the width (W ) of the OCR (characters) box in the environment. Since, this value is the same with high probability for the text/number plates in a single building, we only need to perform the measurement once. If the character boxes are different in width size in an environment, it is possible to tabulate the sizes and use them once the OCR engine has identified the characters.\n7\nIn order to derive the AOV formula, we consider an OCR box of arbitrary dimensions. We analyze the projection of this box in the query image. From the projected coordinates, a formula is derived for obtaining the horizontal VP. By manipulating this formula, a closed-form formula is derived for AOV estimation in terms of horizontal VP location. Proposition 2 summarizes the results.\nProposition 2. If the actual width of the OCR box, depth and AOV of the query are W , d and \u03b8, respectively, the x coordinate (xhor) of the horizontal VP can be obtained as\nxhor = \u2212 cos \u03b8(4d2 \u2212W 2 sin2 \u03b8 + 2dW sin \u03b8)\n4d2 sin \u03b8 \u2212W 2 sin3 \u03b8 (11)\nProof 2. Without loss of generality, assume the OCR box is located at the origin of 3D reference frame as depicted in Fig. 5. Furthermore, according to the scenario depicted in Fig. 5, the user, i.e. the query camera, is located at\nXq =  d sin \u03b8 0 d cos \u03b8\n1\n (12)\nwhere d and \u03b8 are depth and AOV of the query with respect to the characters centroid. We use normalized camera matrix (i.e. P\u2032 = K\u22121P) in our analysis to make it independent from the camera calibration matrix. Using our assumptions, the normalized camera matrix can be written as\nP\u2032 = R (I | \u2212Xq) (13)\nwhere R and I are the query camera rotation matrix and the 3 \u00d7 3 identity matrix, respectively. R can be written as\nR = Rroll Rtilt Rpan (14)\n8 \u03b8 [degrees] -50 -40 -30 -20 -10 0 10 20 30 40 50 [m ] -60 -40 -20 0 20 40 60 xhor Exact value Approximate value\nAs seen, xhor is not a function of H. The proposed formula can be approximated to derive a simple formula for AOV as follows. Corollary 1. The AOV can be estimated by approximating\n(11) as\n\u03b8 \u2248 \u2212 1 xhor\n(21)\nProof 3. Assume\nsin \u03b8 \u2248 \u03b8 (22) cos \u03b8 \u2248 1 (23)\nApplying these assumptions to (11), we get\nxhor \u2248 \u2212 1\n\u03b8 (24)\nSolving for \u03b8 results in (21).\nAs seen, the AOV formula is independent of depth as desired. It is also independent of W. So, using this approximation, there is no need to measure any dimensions in the training phase for the AOV estimation .\nFig. 8 depicts the true and approximated values of xhor. As seen, the approximation is close to the true value for almost the entire range of [\u221245\u25e6, 45\u25e6]. Fig 9 depicts the error in \u03b8 estimation for the mentioned range. As seen, the closer the \u03b8 to the boundaries of the interval, the worse the approximation. The RMS value of the error for the entire range is 4.68\u25e6.\nWe investigate the sensitivity as a measure of robustness for the derived formulas. For, we take the derivative of the formula derived for the quantity of interest with respect to the parameters in the problem. Fig. 10 depicts the sensitivity of the estimated \u03b8 over a range of xhor values for the SAMSUNG Galaxy S5 cellphone. For instance, for the xhor\n\u03b8 [degrees] -50 -40 -30 -20 -10 0 10 20 30 40 50\n[d eg re es ]\n0\n2\n4\n6\n8\n10\n12\n14\n|\u03b8 \u2212 (\u2212 1 xhor(\u03b8) )|\nFig. 9. Error in \u03b8 estimation [degrees] computed at different AOVs using a SAMSUNG Galaxy S5 cellphone camera calibration parameters\nxhor [pixels] 0 500 1000 1500 2000 2500 3000\n[d eg re es /p\nix el s]\n0\n0.5\n1\n1.5\n2\n2.5\n\u2202\u03b8AOV \u2202xhor\nFig. 10. Sensitivity of \u03b8 w.r.t. xhor [pixels] computed using a SAMSUNG Galaxy S5 cellphone camera calibration parameters\nof 1000 pixels, which is corresponding to a \u03b8 of about 23\u25e6, the sensitivity is less than 0.2 degrees/pixel.\nHaving recognized the existing characters, a rough estimate of the user\u2019s location can be obtained using the floor plan. Moreover, the user\u2019s AOV can be estimated from (21). The only missing information about the user\u2019s fine location is its distance (depth) to the characters centroid. Hence, we input the OCR output image to the depth estimation block. Notice that the existence of the OCR block is required since it provides a rough location estimate and side information required for depth estimation."}, {"heading": "3.4 Depth estimation", "text": "We propose a new practical formula for depth estimation, which utilizes the actual and image width of the OCR box."}, {"heading": "3.4.1 Depth estimation using width of OCR box", "text": "It is obvious that as user gets further from the characters center, the width of OCR box in the query image gets smaller. Here, we formulate this relation.\n9 Proposition 3. If the user\u2019s angle-of-view is \u03b8 and actual and image width of the OCR box are W and w, respectively, user\u2019s depth can be calculated as\nd = 1\n2 cos \u03b8\nW w (1 +\n\u221a 1 + w2 tan2 \u03b8)\nwhere \u03b8 is the query AOV.\nProof 4. Consider the OCR box in Fig. 6. For AOV estimation, we studied top and bottom horizontal line segments of this box. Here, we calculate the box width (w) and show that it is a function of depth, AOV and W . Similar to the angle-of-view formula derivation, we only use the corners coordinates in order to derive the depth formula. In fact, it is not required to detect the corners in the actual query phase. Consider matrix xbox that contains the image coordinates of OCR box corners. Due to projective distortion, the image of the OCR box is a quadrilateral in general. Considering the fact that vertical lines remain vertical if there is neither in-plane rotation (or compensated for) nor tilt angle, w can be calculated as\nw = x2 \u2212 x1 = x3 \u2212 x4 (25)\nSimilar to the \u03b8 formula derivation, we know the complete query camera matrix in terms of d, \u03b8 and 3D coordinates of the OCR box corners. Hence, using the parameterized camera matrix to find the OCR box corner location in the image, that is xis, and using (25) one can get\nw = 4dW cos \u03b8\n4d2 \u2212W 2 sin2 \u03b8 (26)\nExact depth formula can be derived by solving for d in (26). It will result in a second-order equation as\n(4w) d2 \u2212 (4W cos \u03b8) d\u2212 wW 2 sin2 \u03b8 = 0 (27)\nThe solutions are\nd1&2 = 1\n2 cos \u03b8\nW w (1\u00b1\n\u221a 1 + w2 tan2 \u03b8) (28)\nwhere d2 is the width in the image plane reflection with respect to the camera center. Hence, only d1 is acceptable and we have\nd = d1 = 1\n2 cos \u03b8\nW w (1 +\n\u221a 1 + w2 tan2 \u03b8) (29)\nThe depth formula is in concord with our intuition that smaller w corresponds to greater distance (d) from the characters. Fig. 11 shows the sensitivity of the depth estimate with respect to the estimated AOV, i.e. \u2202d\u2202\u03b8 . As seen, in a distance of 7m from the characters center and at an AOV of 60 degrees, which is one of the worst cases for localization accuracy, each 1 degree of error in AOV will result in about 0.1m of error in depth estimate.\nFig. 12 depicts the sensitivity of depth estimate w.r.t. w (in pixels) for the SAMSUNG Galaxy S5. For, we used the calibration matrix of the phone camera to obtain the sensitivity in meters/pixel. As seen, when w \u2248 50 pixels, 1 pixel of error in w results in about 10 cm of error in depth estimation. Finally, the sensitivity of the depth formula is linear with respect to the measured W .\n\u03b8 [degrees] -80 -60 -40 -20 0 20 40 60 80\n[m /d\neg re es ]\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15 \u2202d/\u2202\u03b8\nFig. 11. Depth sensitivity versus \u03b8 for d = 7m, W = 0.1m and w = W d\nwi [pixels] 0 50 100 150 200 250 300\n[m /p\nix el s]\n-0.1\n-0.09\n-0.08\n-0.07\n-0.06\n-0.05\n-0.04\n-0.03\n-0.02\n-0.01\n0 \u2202d/\u2202wi\nFig. 12. Depth sensitivity versus the image width of OCR box for d = 7m, W = 0.1m and w = W d"}, {"heading": "3.5 Location estimation", "text": "Once depth and AOV are computed, one can obtain the X and Z coordinates of the user location for the scenario depicted in Fig. 5 as"}, {"heading": "X = d sin \u03b8", "text": "Z = d cos \u03b8 (30)\nThe y coordinate is not of interest for 2D localization in indoor applications, whereas it is essential for other applications such as augmented reality (AR)."}, {"heading": "4 NON-ZERO TILT ANGLE EFFECT ON AOV AND DEPTH ESTIMATION ERROR", "text": "In the derivation of xhor and w formulas, we assumed tilt angle (\u03c6) is zero. In order to study the effect of non-zero tilt angle in practice, we re-derive the formulas for non-zero tilt angle as\nxhor = cos \u03b8 2d cos\u03c6 sin \u03b8 ( s1 + s2 s3 + s4 ) (31)\n10\nand\nw = { x2 \u2212 x1, \u03c6 > 0 x3 \u2212 x4, \u03c6 < 0 = | s5 s6 + s7 | (32)\nwhere\ns1 = 8d 3 cos2 \u03c6+ 2dH2 sin2 \u03c6\u2212 2dW 2 cos2 \u03c6 sin2 \u03b8 s2 = 4d 2H sin 2\u03c6+ 4d2W cos2 \u03c6 sin \u03b8 \u2212H2W sin \u03b8 sin2 \u03c6 s3 = 4d 2 cos2 \u03c6+H2 sin2 \u03c6 s4 = \u2212W 2 cos2 \u03c6 sin2 \u03b8 + 2dH sin 2\u03c6 s5 = 2W cos \u03b8(2d cos\u03c6\u2212H sin\u03c6) s6 = 4d 2 cos2 \u03c6+H2 sin2 \u03c6 s7 = \u2212W 2 cos2 \u03c6 sin2 \u03b8 \u2212 2dH sin 2\u03c6 (33)\nIt should be considered in the derivation of w formula that the rectangular box provided by the OCR engine always frames the entire area of the characters. Hence, the box width is equal to the maximum horizontal distance among xis based on the sign of \u03c6 as stated in the formula.\nEquation (31) is not obviously the same as (11) hence cannot be approximated in the same way. Furthermore, it should be noted that the yhor is not zero in the case of non-zero tilt angle. But, if we still utilize xhor to estimate \u03b8 using (21), there would be some error. Fig. 13 depicts the RMS value of the error over a range of \u03b8 w.r.t. different values of tilt angle (\u03c6). As seen, for tilt angles in the range of [\u221230, 30], the RMS error in the AOV estimate is decreasing as the absolute value of the tilt increases. This shows that the error induced by the non-zero tilt angle is canceling the approximation error existing in (21). The reason is that the absolute value of estimated AOV, i.e. the inverse of xhor, is always greater than the absolute value of the actual AOV, i.e. |\u03b8actual| < |\u03b8estimated| = 1|xhor| . In addition, increasing the absolute value of \u03c6 from 0 to about 35\u25e6 (i.e. \u03c6 \u2208 [\u221230, 30]) decreases the |xhor| hence increases |\u03b8estimated|. In essence\n|\u03c6| \u2191 \u21d2 |xhor| \u2191 \u21d2 |\u03b8estimated| \u2192 |\u03b8actual| (34)\nIn other words, for a relatively large range of tilt angles, the estimated AOV is still close to the actual value.\nFig. 14 depicts the normalized RMS error in the depth estimate for different values of \u03c6. Normalized error is defined as the error in the depth estimate divided by the actual depth. As seen, when \u03c6 \u2208 [\u221220, 20], the RMS value of the normalized error is less than 6%.\nIn conclusion, if \u03c6 \u2208 [\u221220, 20], both formulas for AOV and depth estimation are still effective and can be used without any change. Furthermore, the mentioned [\u221220, 20] is a quite large interval for \u03c6 in practical scenarios. For example, in the university scenario of our experiments, the centroid of the characters region is located at the height of 1.5 m. As stated, human users naturally point their phones towards the centroid of the characters plate. Furthermore, they usually hold the phones at a height approximately equal to the eye level. Hence, non-zero tilt comes from the height difference between the phone and the characters centroid as stated before. In conclusion, if the user is horizontally 1m away from the characters centroid and the phone is held at a height of 113cm to 186cm, the tilt varies in the mentioned interval. As user gets further, the tilt range corresponds to even greater range of heights. Hence, the mentioned range is quite large and contains almost all possible people eye heights in indoor scenarios. We can have a similar discussion for the airport scenario. In the worst case scenario, assume that the gate centroid is located at a height of 6m and the query camera at 1m. In this case, if the user horizontal distance from the centroid is greater than about 14m, the tilt angle will be less than 20\u25e6."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "The localization performance of the proposed system is compared with the state-of-the-art works of Liang\u2019s [10] and Torii\u2019s [21]. We compare the methods in terms of the recognition error rate and location estimation error. In Liang\u2019s method, the recognition error percentage is defined as the percentage of the images wrong detected as the best match. In our location recognition problem, wrong best match is a database image that does not contain the same characters as the query. In Torii\u2019s method, the recognition\n11\nrate is meaningless since there is no image retrieval phase and the best image pair is selected from the database images. In our method, i.e. OCRAPOSE II, the wrong recognition corresponds to recognizing one or more characters wrong. In the conventional buildings, wrong recognition of a single character can lead to a false location right besides the query, meters away or even in other floors depending on the location of the mis-recognized character. Furthermore, estimation error for all methods is defined as the fine localization error among the truly recognized locations.\nThe camera calibration matrices of the phones cameras are assumed known in all scenarios. Feature extraction and processing needed for the benchmarks is performed using the codes available at [34]. In Torii\u2019s method, we use the modified VLAD decriptors [25] for image description to obtain high recognition performance. The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].\nWe studied two university building scenarios and one parking area as the representatives of indoor scenarios. The building scenarios contain a large number of different numbers located at different locations, which makes them appropriate candidates for location recognition error evaluation. On the other hand, the parking scenario only contains one word that can be seen from far distances and different angleof-views. Hence, it is a suitable scenario for localization (estimation) error evaluation. In addition, two commercial mobile phones, i.e. Google Nexus 4 and Samsung Galaxy S5 were used in experiments to provide device diversity conditions. Furthermore, for both university building scenarios, the W measurement was done only once since all set of characters were of the same width."}, {"heading": "5.1 Scenario I - University building", "text": "In this scenario, there exist light characters printed on dark plates. Sample query images are shown in Fig. 16. Furthermore, the floor plan is depicted in Fig. 15 and the trace of locations can be seen as a red dotted line. For the training and query phases of the benchmarks, images are captured at regular locations. That is, for each room number in the Fig. 15, we capture three images, i.e. left, middle and right, with the following (depth, AOV) pair information\n(1.8 m,\u221233\u25e6), (1.5 m, 0\u25e6), (1.8 m,+33\u25e6)\nThis was done at 17 locations and a total of 51 images were taken in this scenario. We consider odd-index images as queries and even ones as database images. Furthermore, a SAMSUNG Galaxy S5 cellphone is used for image capturing in this scenario. Fig. 16 shows a number of sample images. The location recognition rate (in Liang\u2019s method) is about 8%, i.e. 2 correct detections among 51 queries. Hence, we do not include more results from this method in this scenario. The low recognition rate demonstrates how weakly image retrieval-based method might perform in the mentioned indoor scenarios.\nTable 1 compares the results of different methods in terms of recognition rate and mean error of location estimation. Furthermore, Fig. 17 depicts the CDF (cumulative\nFig. 15. Scenario I floor plan\nFig. 16. Scenario I sample images\ndistribution function) of the localization error for the OCRAPOSE II and Torii\u2019s methods. As seen, the OCRAPOSE II demonstrates better localization error compared to that of Torii\u2019s. The reason is Torii\u2019s method interpolates the image pair locations hence performs poorly when query is located outside of the database locations, i.e. extrapolation case. However, the proposed method is capable of fine estimation via depth and AOV estimation in all cases including that of extrapolation.\nFinally, we downsample the database locations to evaluate the robustness of the methods to coarse databases with fewer locations. In fact, we keep the query points as before and downsample the database locations set. It should be noted that the results of the OCRAPOSE II method are the same for different downsampling factors since it does not rely on the database images. Fig. 18 shows the average location estimate error for different methods. As seen, the Torii\u2019s error is generally increasing with the database downsampling factor. On the contrary, the OCRAPOSE II error is fixed and always less than that of Torii\u2019s method.\nWe also performed the performance comparison in another university building scenario with larger number of\n12\nFig. 17. location estimation results in scenario I\nDatabase locations downsampling factor 1 1.5 2 2.5 3 3.5 4 4.5 5\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6 Average location estimation error (m)\nTorii's method OCRAPOSE II\nFig. 18. Downsampling of the database locations in scenario I\nlocations. In the scenario, we sampled images regularly at 120 locations with a distance of 1 m using a Google Nexus 4 cellphone. Depths are in the range of [1, 2.7] m and AOVs belong to [\u221256\u25e6, 56\u25e6]. Here, we only show a brief summary of results in Table 2 in favor of the limited space available. As seen, the location recognition of the proposed system is comparable to Liang\u2019s method. But, it demonstrates smaller localization error on average. The reason is Liang\u2019s method only performs location recognition and has no image-based location refinement stage. However, as stated, OCRAPOSE II refines the location estimate in all cases hence outperforms both benchmarks in terms of localization accuracy."}, {"heading": "5.2 Scenario II - Parking", "text": "As stated before, this scenario is designed to compare the location estimation error of the methods only. In fact, only the word East, as depicted in Fig 19, is used for localization. The actual width (W ) of the word is 95.4 cm. Fig. 20 depicts sample images taken in this scenario. Images are taken from three angle-of-views of 0, +45\u25e6 and \u221245\u25e6. Depths belong to the interval of [1, 40] m.\nTable 3 shows a summary of the location estimation error results. As seen, the OCRAPOSE II is outperforming benchmarks in terms of location accuracy with a large gap. It is due to existence of a larger OCR box in this scenario compared to previous ones. It leads to lower relative error in the estimation of the box width. Fig. 21 is comparing the localization error results. As seen, OCRAPOSE II demonstrates smaller average localization errors. Moreover, Fig. 22 shows the effect of database locations downsampling on the average localization error. As depicted, the OCRAPOSE II demonstrates much less error compared to the benchmarks.\nThe studied scenarios demonstrate the applicability of the proposed OCR-based method. The assumptions we made regarding the alignment of characters and relative position and orientation of the user with respect to the characters usually hold in practice. Furthermore, the size of the conventional characters is large enough so that they can be recognized with high probability even at the furthest possible locations of the user. In fact, texts and gate numbers are bigger in larger areas such as airports, parkings, etc., where user might be tens of meters away. Based on our experiments, each character is seen at least with a width of 10 \u223c 15 pixels in the query image, which is large enough for the OCR engines to recognize.\n13"}, {"heading": "6 CONCLUSION", "text": "In this paper, we discuss a number of indoor scenarios, which are challenging for the existing localization methods and propose using OCR to recognize characters as suitable location distinctive landmarks. A novel system is proposed\nthat utilizes OCR to perform rough localization. Two novel formulas are also proposed for angle-of-view and depth estimation, which are used to refine the location estimate. Our experiments demonstrate that the proposed OCR-based system achieves better performance compared with the state-of-the-art localization methods in terms of location recognition rate and average localization error. It is also shown that benchmarks performance degrades as database locations set becomes sparser, while the performance of the proposed system is independent of the database sparsity and remains constant."}], "references": [{"title": "Tracking mobile users in wireless networks via semi-supervised colocalization", "author": ["J. Pan", "S. Pan", "J. Yin", "L. Ni", "Q. Yang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 34, no. 3, pp. 587\u2013600, March 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Compressive sensing based positioning using rss of wlan access points", "author": ["C. Feng", "W.S.A. Au", "S. Valaee", "Z. Tan"], "venue": "INFOCOM, 2010 Proceedings IEEE. IEEE, 2010, pp. 1\u20139.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Received-signal-strength-based indoor positioning using compressive sensing", "author": ["\u2014\u2014"], "venue": "Mobile Computing, IEEE Transactions on, vol. 11, no. 12, pp. 1983\u20131993, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1983}, {"title": "Monoslam: Realtime single camera slam", "author": ["A. Davison", "I. Reid", "N. Molton", "O. Stasse"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 29, no. 6, pp. 1052\u20131067, June 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Autonomous visual self-localization in completely unknown environment", "author": ["P. Sadeghi-Tehran", "S. Behera", "P. Angelov", "J. Andreu"], "venue": "Evolving and Adaptive Intelligent Systems (EAIS), 2012 IEEE Conference on, May 2012, pp. 90\u201395.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Location-based augmented reality on mobile phones", "author": ["R. Paucher", "M. Turk"], "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on. IEEE, 2010, pp. 9\u201316.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Going out: robust model-based tracking for outdoor augmented reality", "author": ["G. Reitmayr", "T. Drummond"], "venue": "Mixed and Augmented Reality, 2006. ISMAR 2006. IEEE/ACM International Symposium on, Oct 2006, pp. 109\u2013118.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Sextant: Towards ubiquitous indoor localization service by photo-taking of the environment", "author": ["R. Gao", "Y. Tian", "F. Ye", "G. Luo", "K. Bian", "Y. Wang", "T. Wang", "X. Li"], "venue": "Mobile Computing, IEEE Transactions on, vol. PP, no. 99, pp. 1\u20131, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Fiducial marker indoor localization with artificial neural network", "author": ["G. Kim", "E. Petriu"], "venue": "Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME International Conference on, July 2010, pp. 961\u2013966.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Image based localization in indoor environments", "author": ["J.Z. Liang", "N. Corso", "E. Turner", "A. Zakhor"], "venue": "Computing for Geospatial Research and Application (COM. Geo), 2013 Fourth International Conference on. IEEE, 2013, pp. 70\u201375.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A weighted knn epipolar geometry-based approach for vision-based indoor localization using smartphone cameras", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "Sensor Array and Multichannel Signal Processing Workshop (SAM), 2014 IEEE 8th. IEEE, 2014, pp. 37\u201340.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fiducial marker indoor localization with artificial neural network", "author": ["G. Kim", "E. Petriu"], "venue": "Advanced Intelligent Mechatronics (AIM), 2010 IEEE/ASME International Conference on, July 2010, pp. 961\u2013966.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised logo-based indoor localization using smartphone cameras", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "IEEE PIMRC, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust wide-baseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "Image and vision computing, vol. 22, no. 10, pp. 761\u2013767, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Wide baseline stereo matching", "author": ["P. Pritchett", "A. Zisserman"], "venue": "Computer Vision, 1998. Sixth International Conference on, Jan 1998, pp. 754\u2013760.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Asift: A new framework for fully affine invariant image comparison", "author": ["J.-M. Morel", "G. Yu"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 438\u2013469, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Asift: an algorithm for fully affine invariant comparison", "author": ["G. Yu", "J.-M. Morel"], "venue": "Image Processing On Line, vol. 2011, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning visibility of landmarks for vision-based localization", "author": ["P. Alcantarilla", "S.M. Oh", "G. Mariottini", "L. Bergasa", "F. Dellaert"], "venue": "Robotics and Automation (ICRA), 2010 IEEE International Conference on, May 2010, pp. 4881\u20134888.  14", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Recognition of multiple characters in a scene image using arrangement of local features", "author": ["M. Iwamura", "T. Kobayashi", "K. Kise"], "venue": "Document Analysis and Recognition (ICDAR), 2011 International Conference on, Sept 2011, pp. 1409\u20131413.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Ocrapose: An indoor positioning system using smartphone/tablet cameras and ocr-aided stereo feature matching", "author": ["H. Sadeghi", "S. Valaee", "S. Shirani"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 1473\u20131477.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual localization by linear combination of image descriptors", "author": ["A. Torii", "J. Sivic", "T. Pajdla"], "venue": "Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, Nov 2011, pp. 102\u2013109.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextdependent logo matching and recognition", "author": ["H. Sahbi", "L. Ballan", "G. Serra", "A. Del Bimbo"], "venue": "Image Processing, IEEE Transactions on, vol. 22, no. 3, pp. 1018\u20131031, March 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Logo recognition and localization in real-world images by using visual patterns", "author": ["W.-T. Chu", "T.-C. Lin"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, March 2012, pp. 973\u2013976.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "On-body multi-input indoor localization for dynamic emergency scenarios: fusion of magnetic tracking and optical character recognition with mixed-reality display", "author": ["J. Orlosky", "T. Toyama", "D. Sonntag", "A. Sarkany", "A. Lorincz"], "venue": "Pervasive Computing and Communications Workshops (PERCOM Workshops), 2014 IEEE International Conference on. IEEE, 2014, pp. 320\u2013325.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "All about vlad", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, June 2013, pp. 1578\u20131585.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Scene text detection via connected component clustering and nontext filtering", "author": ["H.I. Koo", "D.H. Kim"], "venue": "Image Processing, IEEE Transactions on, vol. 22, no. 6, pp. 2296\u20132305, June 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust text detection in natural images with edgeenhanced maximally stable extremal regions", "author": ["H. Chen", "S.S. Tsai", "G. Schroth", "D.M. Chen", "R. Grzeszczuk", "B. Girod"], "venue": "Image Processing (ICIP), 2011 18th IEEE International Conference on. IEEE, 2011, pp. 2609\u20132612.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "An overview of the tesseract ocr engine.", "author": ["R. Smith"], "venue": "in ICDAR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Camera parameters estimation in soccer scenes on the basis of points at infinity", "author": ["V.B. Kashany", "H. Pourreza"], "venue": "IET computer vision, vol. 6, no. 2, pp. 133\u2013139, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Single view pose estimation of mobile devices in urban environments", "author": ["A. Hallquist", "A. Zakhor"], "venue": "Applications of Computer Vision (WACV), 2013 IEEE Workshop on. IEEE, 2013, pp. 347\u2013354.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Lsd: A fast line segment detector with a false detection control", "author": ["R.G. Von Gioi", "J. Jakubowicz", "J.-M. Morel", "G. Randall"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 4, pp. 722\u2013732, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Communications of the ACM, vol. 24, no. 6, pp. 381\u2013395, 1981.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1981}, {"title": "Minimal solution for uncalibrated absolute pose problem with a known vanishing point", "author": ["B. Micusik", "H. Wildenauer"], "venue": "3D Vision - 3DV 2013, 2013 International Conference on, June 2013, pp. 143\u2013150.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "http://www.vlfeat.org/, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3304\u20133311.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "THE most prevalent method for indoor localization is based on fusion of Wi-Fi RSS fingerprints with inertial sensors data [1] .", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "These methods require a fair number of Wi-Fi access points to be visible at each location [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Under these conditions, they demonstrate localization errors about 2 meters, where training points have granularity of 1 meter [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Although image-based localization has been studied for a long time in the fields of robotics [4], [5] and augmented reality [6], [7], it has only been pursued over the past decade for mobile phones in indoor scenarios [8].", "startOffset": 218, "endOffset": 221}, {"referenceID": 8, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "The proposed methods can be categorized into two classes [9], image retrieval-based (fingerprinting-based) [10], [11] and landmark-based (e.", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "logo-based) [12], [13].", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "logo-based) [12], [13].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Furthermore, most of the methods proposed in the literature, utilize feature extraction and matching for localization [10], [13], while feature extraction and corresponding database creation for a large environment is highly time consuming.", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "Furthermore, most of the methods proposed in the literature, utilize feature extraction and matching for localization [10], [13], while feature extraction and corresponding database creation for a large environment is highly time consuming.", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "set of objects) present in two images cannot be acceptably matched if the difference in AOV is large [14], [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "set of objects) present in two images cannot be acceptably matched if the difference in AOV is large [14], [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "Methods such as ASIFT (affine SIFT) [16], which are", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "5 times of that of SIFT [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "The reason of failure in detecting the correct existing characters using the landmark-based methods is that the characters (numbers) are not as textured as commonly used landmarks such as commercial logos or fiducial markers [12].", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "Hence, point feature-based recognition approaches [18] fail to extract enough distinctive features required to distinguish different numbers from each other.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "Other image retrieval techniques such as bag of features or localization-specific ones [10] might also fail to retrieve the correct matching characters.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "We evaluate the recognition performance of the Liang\u2019s method [10] as an example in our experiments.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Use of point feature-based techniques proposed originally for character recognition such as [19] will result in an OCR-based localization method, which resides in the same category as our method.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "In [20], we demonstrated how OCR can improve a landmark-based localization system in terms of location recognition and localization accuracy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "These methods perform very well in applications with abundant distinctive landmarks such as shopping malls [10] or scenarios with unrepeated scenery such as outdoors (using Google street view database) [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "These methods perform very well in applications with abundant distinctive landmarks such as shopping malls [10] or scenarios with unrepeated scenery such as outdoors (using Google street view database) [21].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Moreover, a large database of images is required to be collected and Geo-tagged (posetagged [11]) in the training phase.", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Furthermore, in the test phase, the query image is compared with the entire (or part of) database to find the best match [10] or best pair [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": "Furthermore, in the test phase, the query image is compared with the entire (or part of) database to find the best match [10] or best pair [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "Landmark-based methods are applicable in scenarios, where highly textured and distinctive landmarks are present [10], [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Landmark-based methods are applicable in scenarios, where highly textured and distinctive landmarks are present [10], [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "These methods use logo/landmark detection techniques [22], [23] to detect existing landmarks/logos in the query image.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "These methods use logo/landmark detection techniques [22], [23] to detect existing landmarks/logos in the query image.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "For instance, in shopping malls, where textured commercial logos are ubiquitous, these methods can provide great localization accuracy [10].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "Among a few works proposed in the literature that utilize OCR for localization, [24] can be mentioned for its explicit use of OCR as a rough localizer.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "A good indoor localization system that we use as one of our benchmarks is proposed in [10] that uses CBIR (contentbased image retrieval) to perform rough localization.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "The second benchmark is the method proposed in [21], which proposes a feature-based method for fine localization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "For image similarity computation, the bag of features representation is used in [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "In this paper, we use the modified VLAD\u2019s representation [25], which has demonstrated supreme performance.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "In [20], we proposed an OCR-aided localization system called OCRAPOSE.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Text detection in natural scene images is an extremely difficult problem [26], [27] and might need parameters fine tuning in the scenario under investigation.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "Text detection in natural scene images is an extremely difficult problem [26], [27] and might need parameters fine tuning in the scenario under investigation.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Such regions are good candidates to be detected as maximally stable extremal regions (MSER) [14] as also suggested by [27].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "Such regions are good candidates to be detected as maximally stable extremal regions (MSER) [14] as also suggested by [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "In works such as [27], this Geometric filtering is carried out after processing the MSER regions.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "MSER region detector is known to be sensitive to image blur [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "Hence, we enhance the MSER regions using [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "[27] uses Canny edge detector to enhance the outline of detected extremal regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Tesseract is believed to be one of the most accurate OCR engines [28].", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "Vanishing points present in the image contain information about query camera rotation with respect to the seen objects [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 29, "context": "Hence, we detect it by solving a novel robust optimization problem partly inspired by [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "In [30], line segments are detected using an edge-based method proposed in [31].", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "In [30], line segments are detected using an edge-based method proposed in [31].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "In order to do this, we propose a novel robust optimization problem inspired by [30] using RANSAC [32].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "In order to do this, we propose a novel robust optimization problem inspired by [30] using RANSAC [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "As stated in [33], if the vertical vanishing point (i.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "As mentioned in [10], in indoor images, most of the visible objects are located on a single wall.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "The localization performance of the proposed system is compared with the state-of-the-art works of Liang\u2019s [10] and Torii\u2019s [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "The localization performance of the proposed system is compared with the state-of-the-art works of Liang\u2019s [10] and Torii\u2019s [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 33, "context": "Feature extraction and processing needed for the benchmarks is performed using the codes available at [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "In Torii\u2019s method, we use the modified VLAD decriptors [25] for image description to obtain high recognition performance.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "The modified VLAD descriptor [35] is an improved version of the descriptor suggested in [35] that has demonstrated supreme performance compared to the tf-idf methods such as the one used in [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "Depths belong to the interval of [1, 40] m.", "startOffset": 33, "endOffset": 40}], "year": 2017, "abstractText": "In this paper, we propose an OCR (optical character recognition)-based localization system called OCRAPOSE II, which is applicable in a number of indoor scenarios including office buildings, parkings, airports, grocery stores, etc. In these scenarios, characters (i.e. texts or numbers) can be used as suitable distinctive landmarks for localization. The proposed system takes advantage of OCR to read these characters in the query still images and provides a rough location estimate using a floor plan. Then, it finds depth and angle-of-view of the query using the information provided by the OCR engine in order to refine the location estimate. We derive novel formulas for the query angle-of-view and depth estimation using image line segments and the OCR box information. We demonstrate the applicability and effectiveness of the proposed system through experiments in indoor scenarios. It is shown that our system demonstrates better performance compared to the state-of-the-art benchmarks in terms of location recognition rate and average localization error specially under sparse database condition.", "creator": "LaTeX with hyperref package"}}}