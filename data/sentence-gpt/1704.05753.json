{"id": "1704.05753", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection", "abstract": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection on the Internet, which aims to present the first systematic review of the data collection models. In the first review of the original paper, we are interested in the data collection model that provides the basis for the generalization of the literature and its implications for computer science. We are looking for people who are highly skilled in machine learning to interpret information from the internet and are interested in building and disseminating highly relevant insights, particularly on network architectures and the use of distributed network architectures. We are interested in providing relevant, reliable, and comprehensive estimates of the average number of human users who are in the Internet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 19 Apr 2017 14:41:21 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v1", "Accepted for publication at ACL 2017"], ["v2", "Thu, 19 Oct 2017 23:04:39 GMT  (91kb,A)", "http://arxiv.org/abs/1704.05753v2", "Published at ACL 2017"]], "COMMENTS": "Accepted for publication at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["youxuan jiang", "jonathan k kummerfeld", "walter s lasecki"], "accepted": true, "id": "1704.05753"}, "pdf": {"name": "1704.05753.pdf", "metadata": {"source": "META", "title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection", "authors": ["Youxuan Jiang", "Jonathan K. Kummerfeld"], "emails": ["lyjiang@umich.edu", "jkummerf@umich.edu", "wlasecki@umich.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n05 75\n3v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\nLinguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. Wemanually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures."}, {"heading": "1 Introduction", "text": "Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013). Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write.\nIn this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing. For worker incentives, we either provide a bonus payment when a paraphrase is novel (encouraging diversity) or\nwhen it matches a paraphrase from another worker (encouraging agreement/correctness). We also varied the type of example paraphrases shown to workers, the number of paraphrases requested from each worker per sentence, the subject domain of the data, whether to show answers to questions, and whether the prompt sentence is the same for multiple workers or varies, with alternative prompts drawn from the output of other workers.\nEffective paraphrasing has two desired properties: correctness and diversity. To measure correctness, we hand-labeled all paraphrases with semantic equivalence and grammaticality scores. For diversity, we measure the fraction of paraphrases that are distinct, as well as Paraphrase In N-gram Changes (PINC), a measure of n-gram variation. We have released all 2,600 paraphrases along with accuracy annotations. Our analysis shows that the most important factor is how workers are primed for a task, with the choice of examples and the prompt sentence affecting diversity and correctness significantly."}, {"heading": "2 Related Work", "text": "Previous work on crowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output. Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases."}, {"heading": "2.1 Workflows for Crowd-Paraphrasing", "text": "The most common approach to crowdsourcing paraphrase generation is to provide a sentence as a prompt and request a single paraphrase from a worker. One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013). Negri et al. (2012) also explored an alternate workflow in which each worker writes\ntwo paraphrases, which are then given to other workers as the prompt sentence, forming a binary tree of paraphrases. They found that paraphrases deeper in the tree were more diverse, but understanding how correctness and grammaticality vary across such a tree still remains an open question. Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers. Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings. The only prior work regarding incentives we are aware of is by Chklovski (2005), who collected paraphrases in a game where the goal was to match an existing paraphrase, with extra points awarded for doing so with fewer hints. The disadvantage of this approach was that 29% of the collected paraphrases were duplicates. In our experiments, duplication ranged from 1% to 13% in each condition."}, {"heading": "2.2 The Effects of Priming", "text": "When crowd workers perform a task, they are primed (influenced) by the examples, instructions, and context that they see. This priming can result in systematic variations in the resulting paraphrases. Mitchell et al. (2014) showed that providing context, in the form of previous utterances from a dialogue, only provides benefits once four or more are included. Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence. When each sentence expresses a small set of slot-filler predicates, Wang et al. (2012) found that providing the list of predicates led to slightly faster paraphrasing than giving either a complete sentence or a short sentence for each predicate. We further expand on this work by exploring how the type of examples shown affects paraphrasing."}, {"heading": "2.3 Expert and Automated Generation", "text": "Finally, there are two general lines of research on paraphrasing not focused on using crowds. The first of these is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Bouamor et al., 2012; Pavlick et al.,\n2015). These resources are extremely large, but usually (1) do not provide the strong semantic equivalence we are interested in, and (2) focus on phrases rather than complete sentences. The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013). Unfortunately, these lattices are typically expensive to produce, taking experts one to three hours per sentence."}, {"heading": "3 Experimental Design", "text": "We conducted a series of experiments to investigate factors in crowdsourced paraphrase creation. To do so in a controlled manner, we studied a single variation per condition."}, {"heading": "3.1 Definition of Valid Paraphrases", "text": "This project was motivated by the need for strongly equivalent paraphrases in semantic parsing datasets. We consider two sentences paraphrases if they would have equivalent interpretations when represented as a structured query, i.e., \u201da pair of units of text deemed to be interchangeable\u201d (Dras, 1999). For example:\nPrompt: Which upper-level classes are four credits? Are there any four credit upper-level classes?\nWe considered the above two questions as paraphrases since they are both requests for a list of classes, explicit and implicit, respectively, although the second one is a polar question and the first one is not. However:\nPrompt:Which is easier out of EECS 378 and EECS 280? Is EECS 378 easier than EECS 280?\nWe did not consider the above two questions as paraphrases since the first one is requesting one of\ntwo class options and the second one is requesting a yes or no answer."}, {"heading": "3.2 Baseline", "text": "We used Amazon Mechanical Turk, presenting workers with the instructions and examples in Figure 1. Workers were shown prompt sentences one at a time, and asked to provide two paraphrases for each. To avoid confusion or training effects between different conditions, we only allowed workers to participate once across all conditions. The initial instructions shown to workers were the same across all conditions (variations were only seen after a worker accepted the task).\nWorkers were paid 5 cents per paraphrase they wrote plus, once all workers were done, a 5 cent bonus for paraphrases that matched another worker\u2019s paraphrase in the same condition. While we do not actually want duplicate paraphrases, this incentive may encourage workers to more closely follow the instructions, producing grammatical and correct sentences. We chose this payment rate to give around minimum wage, estimating time based on prior work."}, {"heading": "3.3 Conditions", "text": "Examples We provided workers with an example prompt sentence and two paraphrases, as shown in Figure 1. We showed either: no examples (No Examples), two examples with lexical changes only (Lexical Examples), one example with lexical changes and one with syntactic changes (Mixed Examples), or two examples that each contained both lexical and syntactic changes (Baseline). The variations between these conditions may prime workers differently, leading them to generate different paraphrases.\nIncentive The 5 cent bonus payment per paraphrase was either not included (No Bonus), awarded for each sentence that was a duplicate at the end of the task (Baseline), or awarded for each sentence that did not match any other worker\u2019s paraphrase (Novelty Bonus). Bonuses that depend on other workers\u2019 actions may encourage either creativity or conformity. We did not vary the base level of payment because prior work has found that workers work quality is not increased by increased financial incentives due to an anchoring effect relative to the base rate we define (Mason and Watts, 2010).\nWorkflow We considered three variations to workflow. First, for each sentence, we either asked workers to provide two paraphrases (Baseline), or one (One Paraphrase). Asking for multiple paraphrases reduces duplication (since workers will not repeat themselves), but may result in lower diversity. Second, since our baseline prompt sentences are questions, we ran a condition with answers shown to workers (Answers). Third, we started all conditions with the same set of prompt sentences, but once workers had produced paraphrases, we had the option to either prompt future workers with the original prompt, or to use paraphrase from another worker. Treating sentences as points and the act of paraphrasing as creating an edge, the space can be characterized as a graph. We prompted workers with either the original sentences only (Baseline), or formed a chain structured graph by randomly choosing a sentence that was (1) not a duplicate, and (2) furthest from the original sentence (Chain). These changes could impact paraphrasing because the prompt sentence is a form of priming.\nData domains We ran with five data sources: questions about university courses (Baseline), messages from dialogues between two students in a simulated academic advising session (ADVISING), questions about US geography (GEOQUERY Tang and Mooney, 2001), text from the Wall Street Journal section of the Penn Treebank (WSJ Marcus et al., 1993), and discussions on the Ubuntu IRC channel (UBUNTU). We randomly selected 20 sentences as prompts from each data source with the lengths representative of the sentence length distribution in that source."}, {"heading": "3.4 Metrics", "text": "Semantic Equivalence For a paraphrase to be valid, its meaning must match the original sentence. To assess this match, two of the authors\u2014 one native speaker and one non-native but fluent speaker\u2014rated every sentence independently, then discussed every case of disagreement to determine a consensus judgement. Prior to the consensusfinding step, the inter-annotator agreement kappa scores were .50 for correctness (moderate agreement), and .36 for grammaticality (fair agreement) (Altman, 1990). For the results in Table 1, we used a \u03c72 test to measure significance, since this is a binary classification process.\nGrammaticality We also judged whether the sentences were grammatical, again with two annotators rating every sentence and resolving disagreements. Again, since this was a binary classification, we used a \u03c72 test for significance.\nTime The time it takes to write paraphrases is important for estimating time-to-completion, and ensuring workers receive fair payment. We measured the time between when a worker submitted one pair of paraphrases and the next. The first paraphrase was excluded since it would skew the data by including the time spent reading the instructions and understanding the task. We report the median time to avoid skewing due to outliers, e.g. a value of five minutes when a worker probably took a break. We apply Mood\u2019s Median test for statistical significance.\nDiversity We use two metrics for diversity, measured over correct sentences only. First, a simple measurement of exact duplication: the number of distinct paraphrases divided by the total number of paraphrases, as a percentage (Distinct). Second, a measure of n-gram diversity (PINC Chen and Dolan, 2011)1. In both cases, a higher score means greater diversity. For PINC, we used a ttest for statistical significance, and for Distinct we used a permutation test."}, {"heading": "4 Results", "text": "We collected 2600 paraphrases: 10 paraphrases per sentence, for 20 sentences, for each of the 13 conditions. The cost, including initial testing, was $196.30, of which $20.30 was for bonus payments. Table 1 shows the results for all metrics."}, {"heading": "4.1 Discussion: Task Variation", "text": "Qualitatively, we observed a wide variety of lexical and syntactic changes, as shown by these example prompts and paraphrases (one low PINC and one high PINC in each case):\nPrompt: How long has EECS 280 been offered for? How long has EECS 280 been offered? EECS 280 has been in the course listings how many years?\nPrompt: Can I take 280 on Mondays and Wednesdays? On Mondays and Wednesdays, can I take 280? Is 280 available as a Monday/Wednesday class?\nThere was relatively little variation in grammaticality or time across the conditions. The times\n1 We also considered BLEU (Papineni et al., 2002), which measures n-gram overlap and is used as a proxy for correctness in MT. As expected, it strongly correlated with PINC.\nwe observed are consistent with prior work: e.g. Wang et al. (2015) report \u223c28 sec/paraphrase.\nPriming had a major impact, with the shift to lexical examples leading to a significant improvement in correctness, but much lower diversity. The surprising increase in correctness when providing no examples has a p-value of 0.07 and probably reflects random variation in the pool of workers. Meanwhile, changing the incentives by providing either a bonus for novelty, or no bonus at all, did not substantially impact any of the metrics.\nChanging the number of paraphrases written by each worker did not significantly impact diversity (we worried that collecting more than one may lead to a decrease). We further confirmed this by calculating PINC between the two paraphrases provided by each user, which produced scores similar to comparing with the prompt. However, the One Paraphrase condition did have lower grammaticality, emphasizing the value of evaluating and filtering out workers who write ungrammatical paraphrases.\nChanging the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity. This fits our intuition that the prompt is a form of priming. However, correctness decreases along the chain, suggesting the need to check paraphrases against the original sentence during the overall process, possibly using other workers as described in \u00a7 2.1. Meanwhile, showing the answer to the question being para-\nphrased did not significantly affect correctness or diversity, and in 2.5% of cases workers incorrectly used the answer as part of their paraphrase.\nWe also analyzed the distribution of incorrect or ungrammatical paraphrases by worker. 7% of workers accounted for 25% of incorrect paraphrases, while the best 30% of workers made no mistakes at all. Similarly, 8% of workers wrote 50% of the ungrammatical paraphrases, while 70% of workers wrote only grammatical paraphrases. Many crowdsourcing tasks address these issues by showing workers some gold standard instances, to evaluate workers\u2019 performance during annotation. Unfortunately, in paraphrasing there is no single correct answer, though other workers could be used to check outputs.\nFinally, we checked the distribution of incorrect paraphrases per prompt sentence. Two prompts accounted for 22% of incorrect paraphrases:\nPrompt:Which is easier out of EECS 378 and EECS 280? Is EECS 378 easier than EECS 280?\nPrompt: Is Professor Stout the only person who teaches Algorithms? Are there professors other than Stout who teach Algorithms?\nThese paraphrases are not semantically equivalent to the original question, but they would elicit equivalent information, which explains why workers provided them. Providing negative examples may help guide workers to avoid such mistakes."}, {"heading": "4.2 Discussion: Domains", "text": "The bottom section of Table 1 shows measurements using the baseline setup, but with variations in the source domain of data. The only significant change in correctness is on UBUNTU, which is probably due to the extensive use of jargon in the dataset, for example:\nPrompt: ok, what does journalctl show That journalistic show is about what?\nFor grammaticality, GEOQUERY is particularly low; common mistakes included confusion between singular/plural and has/have. WSJ is the domain with the greatest variations. It has considerably longer sentences on average, which explains the greater time taken. This could also explain the lower distinctness and PINC score, because workers would often retain large parts of the sentence, sometimes re-arranged, but otherwise unchanged."}, {"heading": "5 Conclusion", "text": "While previous work has used crowdsourcing to generate paraphrases, we perform the first systematic study of factors influencing the process. We find that the most substantial variations are caused by priming effects: using simpler examples leads to lower diversity, but more frequent semantic equivalence. Meanwhile, prompting workers with paraphrases collected from other workers (rather than re-using the original prompt) increases diversity. Our findings provide clear guidance for future paraphrase generation, supporting the creation of larger, more diverse future datasets."}, {"heading": "6 Acknowledgements", "text": "We would like to thank the members of the UMich/IBM Sapphire project, as well as all of our study participants and the anonymous reviewers for their helpful suggestions on this work.\nThis material is based in part upon work supported by IBM under contract 4915012629 . Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM."}], "references": [{"title": "Practical statistics for medical research", "author": ["Douglas G Altman"], "venue": "CRC press. https://www.crcpress.com/Practical-Statistics-for-Medical-Research/Altm", "citeRegEx": "Altman.,? \\Q1990\\E", "shortCiteRegEx": "Altman.", "year": 1990}, {"title": "Vizwiz: nearly real-time answers to visual questions", "author": ["Jeffrey P Bigham", "Chandrika Jayant", "Hanjie Ji", "Greg Little", "Andrew Miller", "Robert C Miller", "Robin Miller", "Aubrey Tatarowicz", "Brandyn White", "Samual White"], "venue": null, "citeRegEx": "Bigham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bigham et al\\.", "year": 2010}, {"title": "Scratching the surface of possible translations", "author": ["Ond\u0159ej Bojar", "Matou\u0161 Mach\u00e1\u010dek", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": null, "citeRegEx": "Bojar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2013}, {"title": "A contrastive review of paraphrase acquisition techniques", "author": ["Houda Bouamor", "Aurlien Max", "Gabriel Illouz", "Anne Vilnat"], "venue": null, "citeRegEx": "Bouamor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouamor et al\\.", "year": 2012}, {"title": "Paraphrase acquisition via crowdsourcing", "author": ["Steven Burrows", "Martin Potthast", "Benno Stein"], "venue": null, "citeRegEx": "Burrows et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Burrows et al\\.", "year": 2013}, {"title": "Error driven paraphrase annotation using mechanical turk", "author": ["Olivia Buzek", "Philip Resnik", "Ben Bederson"], "venue": "In Proc. of the NAACL", "citeRegEx": "Buzek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buzek et al\\.", "year": 2010}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David Chen", "William Dolan"], "venue": "In Proc. of the 49th Annual Meet-", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Collecting paraphrase corpora from volunteer contributors", "author": ["Timothy Chklovski"], "venue": null, "citeRegEx": "Chklovski.,? \\Q2005\\E", "shortCiteRegEx": "Chklovski.", "year": 2005}, {"title": "Tree adjoining grammar and the reluctant paraphrasing of text", "author": ["Mark Dras"], "venue": "Ph.D. thesis, Macquarie University NSW 2109 Australia. http://web.science.mq.edu.au/madras/papers/thesis.pdf", "citeRegEx": "Dras.,? \\Q1999\\E", "shortCiteRegEx": "Dras.", "year": 1999}, {"title": "HyTER: Meaning-equivalent semantics for translation evaluation", "author": ["Markus Dreyer", "Daniel Marcu"], "venue": "In Proc. of the 2012 Conference of the North", "citeRegEx": "Dreyer and Marcu.,? \\Q2012\\E", "shortCiteRegEx": "Dreyer and Marcu.", "year": 2012}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni"], "venue": "In Proc. of the 51st", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proc. of the", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "A simple sequentially rejective multiple test procedure", "author": ["Sture Holm"], "venue": "Scandinavian Journal of Statistics", "citeRegEx": "Holm.,? \\Q1979\\E", "shortCiteRegEx": "Holm.", "year": 1979}, {"title": "Paraphrasing for automatic evaluation", "author": ["David Kauchak", "Regina Barzilay"], "venue": null, "citeRegEx": "Kauchak and Barzilay.,? \\Q2006\\E", "shortCiteRegEx": "Kauchak and Barzilay.", "year": 2006}, {"title": "Online gaming for crowd-sourcing phrase-equivalents", "author": ["A Kumaran", "Melissa Densmore", "Shaishav Kumar"], "venue": "In Proc. of COL-", "citeRegEx": "Kumaran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2014}, {"title": "Conversations in the crowd: Collecting data for task-oriented dialog learning. In Scaling Speech, Language Understanding and Dialogue through Crowdsourcing Workshop at the First AAAI Con", "author": ["Walter S. Lasecki", "Ece Kamar", "Dan Bohus"], "venue": null, "citeRegEx": "Lasecki et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2013}, {"title": "Real\u2013 time crowd control of existing interfaces", "author": ["Walter S Lasecki", "Kyle I Murray", "Samuel White", "Robert C Miller", "Jeffrey P Bigham"], "venue": "In Proc. of the 24th annual ACM symposium on User interface software and technology", "citeRegEx": "Lasecki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2011}, {"title": "Chorus: a crowd-powered conversational assistant", "author": ["Walter S Lasecki", "Rachel Wesley", "Jeffrey Nichols", "Anand Kulkarni", "James F Allen", "Jeffrey P Bigham"], "venue": "In Proc. of the 26th annual ACM symposium on User interface soft-", "citeRegEx": "Lasecki et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lasecki et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Financial incentives and the performance of crowds", "author": ["Winter Mason", "Duncan J Watts"], "venue": "ACM SigKDD Explorations Newsletter", "citeRegEx": "Mason and Watts.,? \\Q2010\\E", "shortCiteRegEx": "Mason and Watts.", "year": 2010}, {"title": "Crowdsourcing language generation templates for dialogue systems", "author": ["Margaret Mitchell", "Dan Bohus", "Ece Kamar"], "venue": "In Proc. of the INLG and SIGDIAL 2014 Joint Session. http://www.aclweb.org/anthology/W14-5003", "citeRegEx": "Mitchell et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2014}, {"title": "Chinese whispers: Cooperative paraphrase acquisition", "author": ["Matteo Negri", "Yashar Mehdad", "Alessandro Marchetti", "Danilo Giampiccolo", "Luisa Bentivogli"], "venue": "In Proc. of the Eighth International Conference on Lan-", "citeRegEx": "Negri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negri et al\\.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Using multiple clause constructors in inductive logic programming for semantic parsing", "author": ["Lappoon R. Tang", "Raymond J. Mooney"], "venue": "In Proc. of the 12th European Conference on Machine Learning", "citeRegEx": "Tang and Mooney.,? \\Q2001\\E", "shortCiteRegEx": "Tang and Mooney.", "year": 2001}, {"title": "Leveraging crowdsourcing for paraphrase recognition", "author": ["Martin Tschirsich", "Gerold Hintz"], "venue": "In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse", "citeRegEx": "Tschirsich and Hintz.,? \\Q2013\\E", "shortCiteRegEx": "Tschirsich and Hintz.", "year": 2013}, {"title": "Crowdsourcing the acquisition of natural language corpora: Methods and observations", "author": ["W.Y.Wang", "D. Bohus", "E. Kamar", "E. Horvitz"], "venue": "IEEE Spoken Language Technology Workshop (SLT). http://ieeexplore.ieee.org/document/6424200/", "citeRegEx": "Y.Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Y.Wang et al\\.", "year": 2012}, {"title": "Building a semantic parser overnight", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al.", "startOffset": 86, "endOffset": 114}, {"referenceID": 27, "context": "Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 10, "context": ", 2015), and question answering (Fader et al., 2013).", "startOffset": 32, "endOffset": 52}, {"referenceID": 21, "context": "Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been", "startOffset": 100, "endOffset": 167}, {"referenceID": 25, "context": "Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been", "startOffset": 100, "endOffset": 167}, {"referenceID": 5, "context": "One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013).", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": "One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013).", "startOffset": 113, "endOffset": 155}, {"referenceID": 4, "context": ", 2010; Burrows et al., 2013). Negri et al. (2012) also explored an alternate workflow in which each worker writes", "startOffset": 8, "endOffset": 51}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 16, "context": "Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers.", "startOffset": 30, "endOffset": 82}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers. Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings.", "startOffset": 30, "endOffset": 281}, {"referenceID": 1, "context": "Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers. Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings. The only prior work regarding incentives we are aware of is by Chklovski (2005), who collected paraphrases in a game where the goal was to match an existing paraphrase, with extra points awarded for doing so with fewer hints.", "startOffset": 30, "endOffset": 540}, {"referenceID": 19, "context": "Mitchell et al. (2014) showed that providing context, in the form of previous utterances from a dialogue, only provides benefits once four or more are included.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence. When each sentence expresses a small set of slot-filler predicates, Wang et al. (2012) found that providing the list of predicates led to slightly faster paraphrasing than giving either a complete sentence or a short sentence for each predicate.", "startOffset": 0, "endOffset": 210}, {"referenceID": 9, "context": "The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013).", "startOffset": 117, "endOffset": 161}, {"referenceID": 2, "context": "The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013).", "startOffset": 117, "endOffset": 161}, {"referenceID": 8, "context": "\u201da pair of units of text deemed to be interchangeable\u201d (Dras, 1999).", "startOffset": 55, "endOffset": 67}, {"referenceID": 19, "context": "level of payment because prior work has found that workers work quality is not increased by increased financial incentives due to an anchoring effect relative to the base rate we define (Mason and Watts, 2010).", "startOffset": 186, "endOffset": 209}, {"referenceID": 0, "context": "36 for grammaticality (fair agreement) (Altman, 1990).", "startOffset": 39, "endOffset": 53}, {"referenceID": 22, "context": "1 We also considered BLEU (Papineni et al., 2002), which measures n-gram overlap and is used as a proxy for correctness in MT.", "startOffset": 26, "endOffset": 49}, {"referenceID": 12, "context": "01 level, both after applying the HolmBonferroni method across each row (Holm, 1979).", "startOffset": 72, "endOffset": 84}, {"referenceID": 27, "context": "Wang et al. (2015) report \u223c28 sec/paraphrase.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. Wemanually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.", "creator": "LaTeX with hyperref package"}}}