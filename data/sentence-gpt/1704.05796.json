{"id": "1704.05796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations", "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "histories": [["v1", "Wed, 19 Apr 2017 16:10:38 GMT  (7585kb,D)", "http://arxiv.org/abs/1704.05796v1", "First two authors contributed equally. Oral presentation at CVPR 2017"]], "COMMENTS": "First two authors contributed equally. Oral presentation at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["david bau", "bolei zhou", "aditya khosla", "aude oliva", "antonio torralba"], "accepted": false, "id": "1704.05796"}, "pdf": {"name": "1704.05796.pdf", "metadata": {"source": "CRF", "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations", "authors": ["David Bau", "Bolei Zhou", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"], "emails": ["torralba}@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig. 1). This internal structure has appeared in situations where the networks are not constrained to decompose problems in any interpretable way.\nThe emergence of interpretable structure suggests that deep networks may be learning disentangled representations spontaneously. While it is commonly understood that a network can learn an efficient encoding that makes economical use of hidden variables to distinguish its states, the appear-\nlamps in places net wheels in object net people in video net\nance of a disentangled representation is not well-understood. A disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure, and encouraging disentangled representations is a significant area of research [5]. If the internal representation of a deep network is partly disentangled, one possible path for understanding its mechanisms is to detect disentangled structure, and simply read out the separated factors.\nHowever, this proposal raises questions which we address in this paper: \u2022 What is a disentangled representation, and how can its\nfactors be quantified and detected? \u2022 Do interpretable hidden units reflect a special alignment\nof feature space, or are interpretations a chimera? \u2022 What conditions in state-of-the-art training lead to rep-\nresentations with greater or lesser entanglement? To examine these issues, we propose a general analytic framework, network dissection, for interpreting deep visual representations and quantifying their interpretability. Using Broden, a broadly and densely labeled data set, our framework identifies hidden units\u2019 semantics for any given CNN, then aligns them with human-interpretable concepts. We evaluate our method on various CNNs (AlexNet, VGG, GoogLeNet, ResNet) trained on object and scene recognition, and show that emergent interpretability is an axis-aligned property of a representation that can be destroyed by rotation without affecting discriminative power. We further examine how interpretability is affected by training data sets, training techniques like dropout [28] and batch normalization [13], and supervision by different primary tasks.\n\u2217 indicates equal contribution Source code and data available at http://netdissect.csail.mit.edu\nar X\niv :1\n70 4.\n05 79\n6v 1\n[ cs\n.C V\n] 1\n9 A\npr 2\n01 7"}, {"heading": "1.1. Related Work", "text": "A growing number of techniques have been developed to understand the internal representations of convolutional neural networks through visualization. The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37]. The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2]. Visualizations digest the mechanisms of a network down to images which themselves must be interpreted; this motivates our work which aims to match representations of CNNs with labeled interpretations directly and automatically.\nMost relevant to our current work are explorations of the roles of individual units inside neural networks. In [40] human evaluation was used to determine that individual units behave as object detectors in a network that was trained to classify scenes. [20] automatically generated prototypical images for individual units by learning a feature inversion mapping; this contrasts with our approach of automatically assigning concept labels. Recently [3] suggested an approach to testing the intermediate layers by training simple linear probes, which analyzes the information dynamics among layers and its effect on the final prediction."}, {"heading": "2. Network Dissection", "text": "How can we quantify the clarity of an idea? The notion of a disentangled representation rests on the human perception of what it means for a concept to be mixed up. Therefore when we quantify interpretability, we define it in terms of alignment with a set of human-interpretable concepts. Our measurement of interpretability for deep visual representations proceeds in three steps:\n1. Identify a broad set of human-labeled visual concepts.\n2. Gather hidden variables\u2019 response to known concepts.\n3. Quantify alignment of hidden variable\u2212concept pairs.\nThis three-step process of network dissection is reminiscent of the procedures used by neuroscientists to understand similar representation questions in biological neurons [23]. Since our purpose is to measure the level to which a representation is disentangled, we focus on quantifying the correspondence between a single latent variable and a visual concept.\nIn a fully interpretable local coding such as a one-hotencoding, each variable will match exactly with one humaninterpretable concept. Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2],\nour present aim is to assess how well a representation is disentangled. Therefore we measure the alignment between single units and single interpretable concepts. This does not gauge the discriminative power of the representation; rather it quantifies its disentangled interpretability. As we will show in Sec. 3.2, it is possible for two representations of perfectly equivalent discriminative power to have very different levels of interpretability.\nTo assess the interpretability of any given CNN, we draw concepts from a new broadly and densely labeled image data set that unifies labeled visual concepts from a heterogeneous collection of labeled data sources, described in Sec. 2.1. We then measure the alignment of each hidden unit of the CNN with each concept by evaluating the feature activation of each individual unit as a segmentation model for each concept. To quantify the interpretability of a layer as a whole, we count the number of distinct visual concepts that are aligned with a unit in the layer, as detailed in Sec. 2.2."}, {"heading": "2.1. Broden: Broadly and Densely Labeled Dataset", "text": "To be able to ascertain alignment with both low-level concepts such as colors and higher-level concepts such as objects, we have assembled a new heterogeneous data set.\nThe Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7]. These data sets contain examples of a broad range of objects, scenes, object parts, textures, and materials in a variety of contexts. Most examples are segmented down to the pixel level except textures and scenes which are given for full-images. In addition, every image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer [31]. A sample of the types of labels in the Broden dataset are shown in Fig. 2.\nThe purpose of Broden is to provide a ground truth set of exemplars for a broad set of visual concepts. The concept labels in Broden are normalized and merged from their original data sets so that every class corresponds to an English word. Labels are merged based on shared synonyms, disregarding positional distinctions such as \u2018left\u2019 and \u2018top\u2019 and\navoiding a blacklist of 29 overly general synonyms (such as \u2018machine\u2019 for \u2018car\u2019). Multiple Broden labels can apply to the same pixel: for example, a black pixel that has the Pascal-Part label \u2018left front cat leg\u2019 has three labels in Broden: a unified \u2018cat\u2019 label representing cats across data sets; a similar unified \u2018leg\u2019 label; and the color label \u2018black\u2019. Only labels with at least 10 image samples are included. Table 1 shows the average number of image samples per label class."}, {"heading": "2.2. Scoring Unit Interpretability", "text": "The proposed network dissection method evaluates every individual convolutional unit in a CNN as a solution to a binary segmentation task to every visual concept in Broden (Fig. 3). Our method can be applied to any CNN using a forward pass without the need for training or backpropagation.\nFor every input image x in the Broden dataset, the activation map Ak(x) of every internal convolutional unit k is collected. Then the distribution of individual unit activations ak is computed. For each unit k, the top quantile level Tk is determined such that P (ak > Tk) = 0.005 over every spatial location of the activation map in the data set.\nTo compare a low-resolution unit\u2019s activation map to the input-resolution annotation mask Lc for some concept c, the activation map is scaled up to the mask resolution Sk(x) from Ak(x) using bilinear interpolation, anchoring interpolants at the center of each unit\u2019s receptive field. Sk(x) is then thresholded into a binary segmentation: Mk(x) \u2261 Sk(x) \u2265 Tk, selecting all regions for which the activation exceeds the threshold Tk. These segmentations are evaluated against every concept c in the data set by computing intersections Mk(x) \u2229 Lc(x), for every (k, c) pair.\nThe score of each unit k as segmentation for concept c is reported as a data-set-wide intersection over union score\nIoUk,c = \u2211 |Mk(x) \u2229 Lc(x)|\u2211 |Mk(x) \u222a Lc(x)| , (1)\nwhere | \u00b7 | is the cardinality of a set. Because the data set contains some types of labels which are not present on some subsets of inputs, the sums are computed only on the subset of images that have at least one labeled concept of the same category as c. The value of IoUk,c is the accuracy of unit k in detecting concept c; we consider one unit k as a detector for concept c if IoUk,c exceeds a threshold. Our qualitative results are insensitive to the IoU threshold: different thresholds denote different numbers of units as concept detectors\nacross all the networks but relative orderings remain stable. For our comparisons we report a detector if IoUk,c > 0.04. Note that one unit might be the detector for multiple concepts; for the purpose of our analysis, we choose the top ranked label. To quantify the interpretability of a layer, we count the number unique concepts aligned with units. We call this the number of unique detectors.\nThe IoU evaluating the quality of the segmentation of a unit is an objective confidence score for interpretability that is comparable across networks. Thus this score enables us to compare interpretability of different representations and lays the basis for the experiments below. Note that network dissection works only as well as the underlying data set: if a unit matches a human-understandable concept that is absent in Broden, then it will not score well for interpretability. Future versions of Broden will be expanded to include more kinds of visual concepts."}, {"heading": "3. Experiments", "text": "For testing we prepare a collection of CNN models with different network architectures and supervision of primary tasks, as listed in Table 2. The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12]. For supervised training, the models are trained from scratch (i.e., not pretrained) on ImageNet [25], Places205 [42], and Places365 [41]. ImageNet is an object-centric data set, which contains 1.2 million images from 1000 classes. Places205 and Places365 are two subsets of the Places Database, which is a scene-centric data set with categories such as kitchen, living room, and coast. Places205 contains 2.4 million images from 205 scene categories, while Places365 contains 1.6 million images from 365 scene categories. \u201cHybrid\u201d refers to a combination of ImageNet and Places365. For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22]. The self-supervised models we analyze are comparable to each other in that they all use AlexNet\nor an AlexNet-derived architecture. In the following experiments, we begin by validating our method using human evaluation. Then, we use random unitary rotations of a learned representation to test whether interpretability of CNNs is an axis-independent property; we find that it is not, and we conclude that interpretability is not an inevitable result of the discriminative power of a representation. Next, we analyze all the convolutional layers of AlexNet as trained on ImageNet [15] and as trained on Places [42], and confirm that our method reveals detectors for higher-level concepts at higher layers and lower-level concepts at lower layers; and that more detectors for higher-level concepts emerge under scene training. Then, we show that different network architectures such as AlexNet, VGG, and ResNet yield different interpretability, while differently supervised training tasks and self-supervised training tasks also yield a variety of levels of interpretability. Finally we show the impact of different training conditions, examine the relationship between discriminative power and interpretability, and investigate a possible way to improve the interpretability of CNNs by increasing their width."}, {"heading": "3.1. Human Evaluation of Interpretations", "text": "We evaluate the quality of the unit interpretations found by our method using Amazon Mechanical Turk (AMT). Raters were shown 15 images with highlighted patches showing the most highly-activating regions for each unit in AlexNet trained on Places205, and asked to decide (yes/no) whether a given phrase describes most of the image patches.\nTable 3 summarizes the results. First, we determined the set of interpretable units as those units for which raters agreed with ground-truth interpretations from [40]. Over this set of units, we report the portion of interpretations generated by our method that were rated as descriptive. Within this set we also compare to the portion of ground-truth labels that were found to be descriptive by a second group of raters. The proposed method can find semantic labels for units that are comparable to descriptions written by human annotators at the highest layer. At the lowest layer, the low-level color and texture concepts available in Broden are only sufficient\nto match good interpretations for a minority of units. Human consistency is also highest at conv5, which suggests that humans are better at recognizing and agreeing upon highlevel visual concepts such as objects and parts, rather than the shapes and textures that emerge at lower layers."}, {"heading": "3.2. Measurement of Axis-Aligned Interpretability", "text": "We conduct an experiment to determine whether it is meaningful to assign an interpretable concept to an individual unit. Two possible hypotheses can explain the emergence of interpretability in individual hidden layer units:\nHypothesis 1. Interpretable units emerge because interpretable concepts appear in most directions in representation space. If the representation localizes related concepts in an axis-independent way, projecting to any direction could reveal an interpretable concept, and interpretations of single units in the natural basis may not be a meaningful way to understand a representation.\nHypothesis 2. Interpretable alignments are unusual, and interpretable units emerge because learning converges to a special basis that aligns explanatory factors with individual units. In this model, the natural basis represents a meaningful decomposition learned by the network.\nHypothesis 1 is the default assumption: in the past it has been found [30] that with respect to interpretability \u201cthere is no distinction between individual high level units and random linear combinations of high level units.\u201d\nNetwork dissection allows us to re-evaluate this hypothesis. We apply random changes in basis to a representation\nlearned by AlexNet. Under hypothesis 1, the overall level of interpretability should not be affected by a change in basis, even as rotations cause the specific set of represented concepts to change. Under hypothesis 2, the overall level of interpretability is expected to drop under a change in basis.\nWe begin with the representation of the 256 convolutional units of AlexNet conv5 trained on Places205 and examine the effect of a change in basis. To avoid any issues of conditioning or degeneracy, we change basis using a random orthogonal transformation Q. The rotation Q is drawn uniformly from SO(256) by applying GramSchmidt on a normally-distributed QR = A \u2208 R2562 with positive-diagonal right-triangular R, as described by [8]. Interpretability is summarized as the number of unique visual concepts aligned with units, as defined in Sec. 2.2.\nDenoting AlexNet conv5 as f(x), we find that the number of unique detectors in Qf(x) is 80% fewer than the number of unique detectors in f(x). Our finding is inconsistent with hypothesis 1 and consistent with hypothesis 2.\nWe also test smaller perturbations of basis using Q\u03b1 for 0 \u2264 \u03b1 \u2264 1, where the fractional powers Q\u03b1 \u2208 SO(256) are chosen to form a minimal geodesic gradually rotating from I to Q; these intermediate rotations are computed using a Schur decomposition. Fig. 4 shows that interpretability of Q\u03b1f(x) decreases as larger rotations are applied.\nEach rotated representation has exactly the same discriminative power as the original layer. Writing the original network as g(f(x)), note that g\u2032(r) \u2261 g(QT r) defines a neural network that processes the rotated representation r = Qf(x) exactly as the original g operates on f(x). We conclude that interpretability is neither an inevitable result of discriminative power, nor is it a prerequisite to discriminative power. Instead, we find that interpretability is a different quality that must be measured separately to be understood."}, {"heading": "3.3. Disentangled Concepts by Layer", "text": "Using network dissection, we analyze and compare the interpretability of units within all the convolutional layers of Places-AlexNet and ImageNet-AlexNet. Places-AlexNet is trained for scene classification on Places205 [42], while ImageNet-AlexNet is the identical architecture trained for object classification on ImageNet [15].\nThe results are summarized in Fig. 5. A sample of units are shown together with both automatically inferred interpretations and manually assigned interpretations taken from [40]. We can see that the predicted labels match the human annotation well, though sometimes they capture a different description of a visual concept, such as the \u2018crosswalk\u2019 predicted by the algorithm compared to \u2018horizontal lines\u2019 given by the human for the third unit in conv4 of PlacesAlexNet in Fig. 5. Confirming intuition, color and texture concepts dominate at lower layers conv1 and conv2 while more object and part detectors emerge in conv5."}, {"heading": "3.4. Network Architectures and Supervisions", "text": "How do different network architectures and training supervisions affect disentangled interpretability of the learned representations? We apply network dissection to evaluate a range of network architectures and supervisions. For simplicity, the following experiments focus on the last convolutional layer of each CNN, where semantic detectors emerge most.\nResults showing the number of unique detectors that emerge from various network architectures trained on ImageNet and Places are plotted in Fig. 7, with examples shown in Fig. 6. In terms of network architecture, we find that interpretability of ResNet > VGG > GoogLeNet > AlexNet. Deeper architectures appear to allow greater interpretability. Comparing training data sets, we find Places > ImageNet. As discussed in [40], one scene is composed of multiple objects, so it may be beneficial for more object detectors to emerge in CNNs trained to recognize scenes.\nResults from networks trained on various supervised and self-supervised tasks are shown in Fig. 8. Here the network architecture is AlexNet for each model, We observe that training on Places365 creates the largest number of unique detectors. Self-supervised models create many texture detectors but relatively few object detectors; apparently, supervision from a self-taught primary task is much weaker at inferring interpretable concepts than supervised training on a large annotated data set. The form of self-supervision makes a difference: for example, the colorization model is trained on colorless images, and almost no color detection units emerge. We hypothesize that emergent units represent concepts required to solve the primary task.\nFig. 9 shows some typical visual detectors identified in the self-supervised CNN models. For the models audio and puzzle, some object and part detectors emerge. Those detectors may be useful for CNNs to solve the primary tasks:\nthe audio model is trained to associate objects with a sound source, so it may be useful to recognize people and cars; while the puzzle model is trained to align the different parts of objects and scenes in an image. For colorization and tracking, recognizing textures might be good enough for the CNN to solve primary tasks such as colorizing a desaturated natural image; thus it is unsurprising that the texture detectors dominate."}, {"heading": "3.5. Training Conditions vs. Interpretability", "text": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks. To analyze the effect of training conditions on interpretability, we take the Places205AlexNet as the baseline model and prepare several variants of it, all using the same AlexNet architecture. For the vari-\naudio puzzle colorization tracking chequered (texture) 0.102 head (part) 0.091 dotted (texture) 0.140 chequered (texture) 0.167\ncar (object) 0.063 perforated (texture) 0.085 head (part) 0.056 grass (object) 0.120\n10 0\n10 2\n10 4\n10 6\nTraining iteration\n0\n10\n20\n30\n40\nN u\nm b\ne r\no f\nu n\niq u\ne d\ne te\nc to\nrs\nobject part scene material texture color\nba se\nlin e\nre pe\nat 1\nre pe\nat 2\nre pe\nat 3\nN oD\nro po\nut\nBa tc hN\nor m\n0\n20\n40\n60\n80\n100\nN u\nm b\ne r\no f\nu n\niq u\ne d\ne te\nc to\nrs object part\nscene material texture color\nFigure 10. The evolution of the interpretability of conv5 of Places205-AlexNet over 2,400,000 training iterations. The baseline model is trained to 300,000 iterations (marked at the red line).\nNumber of detectors\nba se\nlin e\nre pe\nat 1\nre pe\nat 2\nre pe\nat 3 N oD ro po ut Ba tc hN or m\n0\n50\n100\n150\n200 object\npart scene material texture color\nNumber of unique detectors\nba se\nlin e\nre pe\nat 1\nre pe\nat 2\nre pe\nat 3 N oD ro po ut Ba tc hN or m\n0\n20\n40\n60\n80\n100 object\npart scene material texture color\nFigure 11. Effect of regularizations on the interpretability of CNNs.\nants Repeat1, Repeat2 and Repeat3, we randomly initialize the weights and train them with the same number of iterations. For the variant NoDropout, we remove the dropout in the FC layers of the baseline model. For the variant BatchNorm, we apply batch normalization at each convolutional layers of the baseline model. Repeat1, Repeat2, Repeat3 all have nearly the same top-1 accuracy 50.0% on the validation set. The variant without dropout has top-1 accuracy 49.2%. The variant with batch norm has top-1 accuracy 50.5%.\nIn Fig. 10 we plot the interpretability of snapshots of the baseline model at different training iterations. We can see that object detectors and part detectors begin emerging at about 10,000 iterations (each iteration processes a batch of 256 images). We do not find evidence of transitions across different concept categories during training. For example, units in conv5 do not turn into texture or material detectors before becoming object or part detectors.\nFig. 11 shows the interpretability of units in the CNNs over different training conditions. We find several effects: 1) Comparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning discussed in [16]. 2) For the network without dropout, more texture detectors emerge but fewer object detectors. 3) Batch normalization seems to decrease interpretability significantly.\nThe batch normalization result serves as a caution that discriminative power is not the only property of a representation that should be measured. Our intuition for the loss of interpretability under batch normalization is that the batch normalization \u2018whitens\u2019 the activation at each layer, which smooths out scaling issues and allows a network to easily rotate axes of intermediate representations during training. While whitening apparently speeds training, it may also have an effect similar to random rotations analyzed in Sec. 3.2 which destroy interpretability. As discussed in Sec. 3.2, however, interpretability is neither a prerequisite nor an obstacle to discriminative power. Finding ways to capture the benefits of batch normalization without destroying interpretability is an important area for future work."}, {"heading": "3.6. Discrimination vs. Interpretability", "text": "Activations from the higher layers of CNNs are often used as generic visual features, showing great discrimination\nand generalization ability [42, 24]. Here we benchmark deep features from several networks trained on several standard image classification data sets for their discrimination ability on a new task. For each trained model, we extract the representation at the highest convolutional layer, and train a linear SVM withC = 0.001 on the training data for action40 action recognition task [34]. We compute the classification accuracy averaged across classes on the test split.\nFig. 12 plots the number of the unique object detectors for each representation, compared to that representation\u2019s classification accuracy on the action40 test set. We can see there is positive correlation between them. Thus the supervision tasks that encourage the emergence of more concept detectors may also improve the discrimination ability of deep features. Interestingly, the best discriminative representation for action40 is the representation from ResNet152ImageNet, which has fewer unique object detectors compared to ResNet152-Places365. We hypothesize that the accuracy on a representation when applied to a task is dependent not only on the number of concept detectors in the representation, but on the suitability of the set of represented concepts to the transfer task."}, {"heading": "3.7. Layer Width vs. Interpretability", "text": "From AlexNet to ResNet, CNNs for visual recognition have grown deeper in the quest for higher classification accuracy. Depth has been shown to be important to high discrimination ability, and we have seen in Sec. 3.4 that interpretability can increase with depth as well. However, the width of layers (the number of units per layer) has been less explored. One reason is that increasing the number of convolutional units at a layer significantly increases computational cost while yielding only marginal improvements in classification accuracy. Nevertheless, some recent work [36] shows that a carefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.\nTo explore how the width of layers affects interpretability of CNNs, we do a preliminary experiment to test how width\naffects emergence of interpretable detectors: we remove the FC layers of the AlexNet, then triple the number of units at the conv5, i.e., from 256 units to 768 units. Finally we put a global average pooling layer after conv5 and fully connect the pooled 768-feature activations to the final class prediction. We call this model AlexNet-GAP-Wide.\nAfter training on Places365, the AlexNet-GAP-Wide obtains similar classification accuracy on the validation set as the standard AlexNet ( 0.5% top1 accuracy lower), but it has many more emergent concept detectors, both in terms of the number of unique detectors and the number of detector units at conv5, as shown in Fig. 13. We have also increased the number of units to 1024 and 2048 at conv5, but the number of unique concepts does not significantly increase further. This may indicate a limit on the capacity of AlexNet to separate explanatory factors; or it may indicate that a limit on the number of disentangled concepts that are helpful to solve the primary task of scene classification."}, {"heading": "4. Conclusion", "text": "This paper proposed a general framework, network dissection, for quantifying interpretability of CNNs. We applied network dissection to measure whether interpretability is an axis-independent phenomenon, and we found that it is not. This is consistent with the hypothesis that interpretable units indicate a partially disentangled representation. We applied network dissection to investigate the effects on interpretability of state-of-the art CNN training techniques. We have confirmed that representations at different layers disentangle different categories of meaning; and that different training techniques can have a significant effect on the interpretability of the representation learned by hidden units.\nAcknowledgements. This work was partly supported by the National Science Foundation under Grant No. 1524817 to A.T.; the Vannevar Bush Faculty Fellowship program sponsored by the Basic Research Office of the Assistant Secretary of Defense for Research and Engineering and funded by the Office of Naval Research through grant N00014-16-1-3116 to A.O.; the MIT Big Data Initiative at CSAIL, the Toyota Research Institute / MIT CSAIL Joint Research Center, Google and Amazon Awards, and a hardware donation from NVIDIA Corporation. B.Z. is supported by a Facebook Fellowship."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "Proc. ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding intermediate layers using linear classifier probes", "author": ["G. Alain", "Y. Bengio"], "venue": "arXiv:1610.01644,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Trans. on Graphics (SIGGRAPH),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing textures in the wild", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "S. Mohamed", "A. Vedaldi"], "venue": "Proc. CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "What is a random matrix", "author": ["P. Diaconis"], "venue": "Notices of the AMS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Object-centric representation learning from unlabeled videos", "author": ["R. Gao", "D. Jayaraman", "K. Grauman"], "venue": "arXiv:1612.00500,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Do semantic parts emerge in convolutional neural networks", "author": ["A. Gonzalez-Garcia", "D. Modolo", "V. Ferrari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "Proc. ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Convergent learning: Do different neural networks learn the same representations", "author": ["Y. Li", "J. Yosinski", "J. Clune", "H. Lipson", "J. Hopcroft"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Shuffle and learn: unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "Proc. ECCV,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "Proc. ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "Proc. ECCV,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R.Q. Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried"], "venue": "Nature, 435(7045):1102\u20131107,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv:1403.6382,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Int\u2019l Journal of Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Conference on Learning Representations Workshop,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv:1312.6199,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning color names for real-world applications", "author": ["J. Van De Weijer", "C. Schmid", "J. Verbeek", "D. Larlus"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Generating videos with scene dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv:1609.02612,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei"], "venue": "Proc. ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv:1605.07146,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Proc. ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "Proc. ECCV. Springer,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "Proc. CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Places: An image database for deep scene understanding", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Torralba", "A. Oliva"], "venue": "arXiv:1610.02055,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Scene parsing through ade20k dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "Proc. CVPR,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 39, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 264, "endOffset": 268}, {"referenceID": 10, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 314, "endOffset": 318}, {"referenceID": 31, "context": "Observations of hidden units in large deep neural networks have revealed that human-interpretable concepts sometimes emerge as individual latent variables within those networks: for example, object detector units emerge within networks trained to recognize places [40]; part detectors emerge in object classifiers [11]; and object detectors emerge in generative video networks [32] (Fig.", "startOffset": 377, "endOffset": 381}, {"referenceID": 39, "context": "Unit 13 in [40] (classifying places) detects table lamps.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "Unit 246 in [11] (classifying objects) detects bicycle wheels.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "A unit in [32] (self-supervised for generating videos) detects people.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "A disentangled representation aligns its variables with a meaningful factorization of the underlying problem structure, and encouraging disentangled representations is a significant area of research [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 27, "context": "We further examine how interpretability is affected by training data sets, training techniques like dropout [28] and batch normalization [13], and supervision by different primary tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "We further examine how interpretability is affected by training data sets, training techniques like dropout [28] and batch normalization [13], and supervision by different primary tasks.", "startOffset": 137, "endOffset": 141}, {"referenceID": 36, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 107, "endOffset": 115}, {"referenceID": 39, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 107, "endOffset": 115}, {"referenceID": 16, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 25, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 36, "context": "The behavior of a CNN can be visualized by sampling image patches that maximize activation of hidden units [37, 40], or by using variants of backpropagation to identify or generate salient image features [17, 26, 37].", "startOffset": 204, "endOffset": 216}, {"referenceID": 34, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 23, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 1, "context": "The discriminative power of hidden layers of CNN features can also be understood by isolating portions of networks, transferring them or limiting them, and testing their capabilities on specialized problems [35, 24, 2].", "startOffset": 207, "endOffset": 218}, {"referenceID": 39, "context": "In [40] human evaluation was used to determine that individual units behave as object detectors in a network that was trained to classify scenes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20] automatically generated prototypical images for individual units by learning a feature inversion mapping; this contrasts with our approach of automatically assigning concept labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Recently [3] suggested an approach to testing the intermediate layers by training simple linear probes, which analyzes the information dynamics among layers and its effect on the final prediction.", "startOffset": 9, "endOffset": 12}, {"referenceID": 22, "context": "This three-step process of network dissection is reminiscent of the procedures used by neuroscientists to understand similar representation questions in biological neurons [23].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 210, "endOffset": 217}, {"referenceID": 1, "context": "Although we expect a network to learn partially nonlocal representations in interior layers [5], and past experience shows that an emergent concept will often align with a combination of a several hidden units [11, 2], street (scene) flower (object) headboard (part)", "startOffset": 210, "endOffset": 217}, {"referenceID": 42, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "The Broadly and Densely Labeled Dataset (Broden) unifies several densely labeled image data sets: ADE [43], OpenSurfaces [4], Pascal-Context [19], Pascal-Part [6], and the Describable Textures Dataset [7].", "startOffset": 201, "endOffset": 204}, {"referenceID": 30, "context": "In addition, every image pixel in the data set is annotated with one of the eleven common color names according to the human perceptions classified by van de Weijer [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 94, "endOffset": 98}, {"referenceID": 42, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 167, "endOffset": 170}, {"referenceID": 6, "context": "Category Classes Sources Avg sample scene 468 ADE [43] 38 object 584 ADE [43], Pascal-Context [19] 491 part 234 ADE [43], Pascal-Part [6] 854 material 32 OpenSurfaces [4] 1,703 texture 47 DTD [7] 140 color 11 Generated 59,250", "startOffset": 192, "endOffset": 195}, {"referenceID": 14, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 26, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "The network architectures include AlexNet [15], GoogLeNet [29], VGG [27], and ResNet [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 46, "endOffset": 50}, {"referenceID": 40, "context": ", not pretrained) on ImageNet [25], Places205 [42], and Places365 [41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 212, "endOffset": 215}, {"referenceID": 17, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 259, "endOffset": 263}, {"referenceID": 32, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 287, "endOffset": 291}, {"referenceID": 9, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 344, "endOffset": 348}, {"referenceID": 37, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 383, "endOffset": 387}, {"referenceID": 38, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 429, "endOffset": 433}, {"referenceID": 21, "context": "For self-supervised training tasks, we select several recent models trained on predicting context (context) [9], solving puzzles (puzzle) [21], predicting ego-motion (egomotion) [14], learning by moving (moving) [1], predicting video frame order (videoorder) [18] or tracking (tracking) [33], detecting object-centric alignment (objectcentric) [10], colorizing images (colorization) [38], predicting cross-channel (crosschannel) [39], and predicting ambient sound from frames (sound) [22].", "startOffset": 484, "endOffset": 488}, {"referenceID": 14, "context": "Next, we analyze all the convolutional layers of AlexNet as trained on ImageNet [15] and as trained on Places [42], and confirm that our method reveals detectors for higher-level concepts at higher layers and lower-level concepts at lower layers; and that more detectors for higher-level concepts emerge under scene training.", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "Next, we analyze all the convolutional layers of AlexNet as trained on ImageNet [15] and as trained on Places [42], and confirm that our method reveals detectors for higher-level concepts at higher layers and lower-level concepts at lower layers; and that more detectors for higher-level concepts emerge under scene training.", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "First, we determined the set of interpretable units as those units for which raters agreed with ground-truth interpretations from [40].", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "Hypothesis 1 is the default assumption: in the past it has been found [30] that with respect to interpretability \u201cthere is no distinction between individual high level units and random linear combinations of high level units.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "The rotation Q is drawn uniformly from SO(256) by applying GramSchmidt on a normally-distributed QR = A \u2208 R2562 with positive-diagonal right-triangular R, as described by [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 41, "context": "Places-AlexNet is trained for scene classification on Places205 [42], while ImageNet-AlexNet is the identical architecture trained for object classification on ImageNet [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Places-AlexNet is trained for scene classification on Places205 [42], while ImageNet-AlexNet is the identical architecture trained for object classification on ImageNet [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 39, "context": "A sample of units are shown together with both automatically inferred interpretations and manually assigned interpretations taken from [40].", "startOffset": 135, "endOffset": 139}, {"referenceID": 39, "context": "As discussed in [40], one scene is composed of multiple objects, so it may be beneficial for more object detectors to emerge in CNNs trained to recognize scenes.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "Training conditions such as the number of training iterations, dropout [28], batch normalization [13], and random initialization [16], are known to affect the representation learning of neural networks.", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "We find several effects: 1) Comparing different random initializations, the models converge to similar levels of interpretability, both in terms of the unique detector number and the total detector number; this matches observations of convergent learning discussed in [16].", "startOffset": 268, "endOffset": 272}, {"referenceID": 41, "context": "and generalization ability [42, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "and generalization ability [42, 24].", "startOffset": 27, "endOffset": 35}, {"referenceID": 33, "context": "001 on the training data for action40 action recognition task [34].", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "Nevertheless, some recent work [36] shows that a carefully designed wide residual network can achieve classification accuracy superior to the commonly used thin and deep counterparts.", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "creator": "LaTeX with hyperref package"}}}