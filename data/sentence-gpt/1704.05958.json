{"id": "1704.05958", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Global Relation Embedding for Relation Extraction", "abstract": "Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much larger-scale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%. We show that in order to extend existing relationship extraction, we also propose using a simple hierarchical model that can be used to estimate the observed value of the inferred value of the inferred value of the inferred value. Finally, we show that using a hierarchical model for an estimate of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of the inferred value of", "histories": [["v1", "Wed, 19 Apr 2017 23:54:46 GMT  (3279kb)", "http://arxiv.org/abs/1704.05958v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu su", "honglei liu", "semih yavuz", "izzeddin gur", "huan sun", "xifeng yan"], "accepted": false, "id": "1704.05958"}, "pdf": {"name": "1704.05958.pdf", "metadata": {"source": "CRF", "title": "Global Relation Embedding for Relation Extraction", "authors": ["Yu Su", "Honglei Liu", "Semih Yavuz", "Izzeddin G\u00fcr", "Xifeng Yan"], "emails": ["ysu@cs.ucsb.edu", "honglei@cs.ucsb.edu", "syavuz@cs.ucsb.edu", "izzeddingur@cs.ucsb.edu", "sun.397@osu.edu", "xyan@cs.ucsb.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n05 95\n8v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\nding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much largerscale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%."}, {"heading": "1 Introduction", "text": "Relation extraction requires deep understanding of the relation between entities. Early studies mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate features (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). Recently neural network models have been introduced to embed words, relations, and\n\u2217 Equally contributed.\nsentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016). In this work, we study the problem of embedding textual relations, defined as the shortest dependency path1 between two entities in the dependency graph of a sentence, to improve relation extraction.\nTextual relations are one of the most discriminative textual signals that lay the foundation of many relation extraction models. Because of their exact feature matching, early kernel based models (Bunescu and Mooney, 2005) can hardly exploit fine-grained word similarities. More recent studies (Xu et al., 2015a,b, 2016; Liu et al., 2016) have explored embedding textual relations via neural networks. However, they have all focused on the supervised setting, where the embedding model is trained on a set of sentences with manually annotated target relation. The high cost of manual annotation limits the scale of their setting: The training data typically consists of several thousands of annotated sentences and around 10 target relations (Liu et al., 2016).\nIn contrast, we embed textual relations with distant supervision (Mintz et al., 2009), which provides much larger-scale training data without the need of manual annotation. However, the assertion of distant supervision, \u201cany sentence containing a pair of entities that participate in a knowledge base (KB) relation is likely to express the relation,\u201d can be violated more often than not, resulting in many wrongly labeled training examples. A representative example is shown in Figure 1. Embedding quality is thus compromised by the noise in training data.\nInstead of using local statistics, i.e., individual\n1We use fully lexicalized shortest dependency path with directional and typed dependency relations.\ntextual-KB relation pairs, like in previous work, we propose to embed textual relations using global statistics, which provide a natural solution to the wrong labeling problem. More specifically, we collect co-occurrence statistics of textual and KB relations from the entire corpus, where the cooccurring relationship is established via distant supervision. The semantics of a textual relation can then be represented by its co-occurrence distribution of KB relations. For example, the distribution in Figure 1 indicates that the textual relation SUBJECT nsubjpass \u2190\u2212\u2212\u2212\u2212 born nmod:in \u2212\u2212\u2212\u2212\u2192 OBJECT mostly means place of birth, and is also a good indicator of nationality, but not place of death. Textual relation embeddings learned on such global statistics are thus more robust to the noise introduced by the wrong labeling problem.\nWe augment existing relation extractions using the learned textual relation embeddings. On a popular dataset introduced by Riedel et al. (2010), we show that a number of recent relation extraction models, which are based on local statistics, can be significantly improved using our textual relation embeddings. Most remarkably, a new best performance is achieved when augmenting the best existing model with our relation embeddings: The precision of the top 1,000 relational facts discovered by the model is improved from 83.9% to 89.3%, a 33.5% decrease in error rate. The results suggest that relation embedding with global statistics can capture complementary information to existing local statistics based models."}, {"heading": "2 Related Work", "text": "Relation extraction is an important task in information extraction, and has attracted substantial attention. Early relation extraction methods are mainly feature-based (Kambhatla, 2004; GuoDong et al., 2005), where features at various levels, including POS tags, constituency and dependency parses, are integrated in a max entropy\nmodel. With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). The most related work to ours is by Bunescu and Mooney (2005), where the authors point out and demonstrate the importance of shortest dependency paths for relation extraction.\nMore recently, the focus of relation extraction research has been revolving around neural network models, which can alleviate the problem of exact feature matching of previous methods and have shown a remarkable success (e.g., (Socher et al., 2012; Zeng et al., 2014)). Among those, the most related are the ones embedding shortest dependency paths with neural networks (Xu et al., 2015a,b, 2016; Liu et al., 2016). For example, Xu et al. (2015b) use a recurrent neural network (RNN) with LSTM units to embed shortest dependency paths without typed dependency relations, while a convolutional neural network is used in (Xu et al., 2015a). However, they are all based on the supervised setting with a limited scale.\nDistant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction. Various efforts have been put to combat the longcriticized wrong labeling problem. Riedel et al. (2010), Hoffmann et al. (2011), and Surdeanu et al. (2012) have attempted a multi-instance learning (Dietterich et al., 1997) framework to soften the assumption of distant supervision, but their models are still feature-based. Zeng et al. (2015) combine multi-instance learning with neural networks, with the assumption that at least one of the contextual sentences of an entity pair is expressing the target relation, but this will lose useful information in the neglected sentences. Instead, Lin et al. (2016) use all the contextual sentences, and\nintroduce an attentive neural network to learn to properly weight the contextual sentences. However, no prior study has exploited global statistics to combat the wrong labeling problem of distant supervision.\nIn universal schema (Riedel et al., 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al., 2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns. A matrix entry is 1 if the relational fact is observed in training, and 0 otherwise. Entity pair and relation embeddings, either directly or via neural networks, are then learned on the matrix entries, which are still individual relational facts, and the wrong labeling problem remains. Global co-occurrence frequencies are not taken into account, which is the focus of this study."}, {"heading": "3 Global Statistics of Relations", "text": "When using a corpus to train statistical models, there are two levels of statistics to exploit: local and global. Take word embedding as an example. The skip-gram model (Mikolov et al., 2013) is based on local statistics: During training, we sweep through the corpus and slightly tune the embedding model based on each local window (e.g., 10 consecutive words). In contrast, in global statistics based methods, exemplified by latent semantic analysis (Deerwester et al., 1990) and GloVe (Pennington et al., 2014), we process the entire corpus to collect global statistics like word-word co-occurrence counts, normalize the raw statistics, and train an embedding model directly on the normalized global statistics.\nMost existing studies on relation extraction are based on local statistics of relations, i.e., models are trained on individual relation examples. In this section, we describe how we collect co-occurrence statistics of textual and KB relations, and how to normalize the raw statistics. By the end of this section a bipartite relation graph like Figure 2 will be constructed, with one node set being textual relations T , and the other being KB relations R. The edges are weighted by the normalized cooccurrence statistics of relations."}, {"heading": "3.1 Relation Graph Construction", "text": "Given a corpus and a KB, we first do entity linking on each sentence, and do dependency pars-\ning if at least two entities are identified2. For each entity pair (e, e\u2032) in the sentence, we extract the fully lexicalized shortest dependency path as a textual relation t, forming a relational fact (e, t, e\u2032). There are two outcomes from this step: a set of textual relations T = {ti}, and the support S(ti) for each ti. The support of a textual relation is a multiset containing the entity pairs of the textual relation. The multiplicity of an entity pair, mS(ti)(e, e \u2032), is the number of occurrences of the corresponding relational fact (e, ti, e \u2032) in the corpus. For example, if the support of ti is S(ti) = {(e1, e \u2032 1) , (e1, e \u2032 1) , (e2, e \u2032 2) , . . . }, entity pair (e1, e \u2032 1) has a multiplicity of 2 because the relational fact (e1, ti, e \u2032 1) occur in two sentences. We also get a set of KB relations R = {rj}, and the support S(rj) of a KB relation rj is the set of entity pairs having this relation in the KB, i.e., there is a relational fact (e, rj , e \u2032) in the KB. The number of co-occurrences of a textural relation ti and a KB relation rj is\nnij = \u2211\n(e,e\u2032)\u2208S(rj)\nmS(ti)(e, e \u2032), (1)\ni.e., every occurrence of the relational fact (e, ti, e \u2032) will be counted as a co-occurrence of ti and rj if (e, e \u2032) \u2208 S(rj). A bipartite relation graph can then be constructed, with T andR as the node sets, and the edge between ti and rj has weight nij (no edge if nij = 0), which will be normalized later.\n2In the experiments entity linking is assumed given, and dependency parsing is done using Stanford Parser (Chen and Manning, 2014) with universal dependencies."}, {"heading": "3.2 Normalization", "text": "The raw co-occurrence counts have a heavily skewed distribution that spans several orders of magnitude: A small portion of relation pairs cooccur highly frequently, while most relation pairs co-occur only a few times. For example, a textual relation, SUBJECT nsubjpass \u2190\u2212\u2212\u2212\u2212 born nmod:in \u2212\u2212\u2212\u2212\u2192 OBJECT, may co-occur with the KB relation place of birth thousands of times (e.g., \u201cMichelle Obama was born in Chicago\u201d), while a synonymous but slightly more compositional textual relation, SUBJECT nsubjpass \u2190\u2212\u2212\u2212\u2212 born nmod:in \u2212\u2212\u2212\u2212\u2192 city nmod:of \u2212\u2212\u2212\u2212\u2192 OBJECT, may cooccur with the KB relation only several times in the whole corpus (e.g., \u201cMichelle Obama was born in the city of Chicago\u201d). Learning directly on the raw co-occurrence counts, an embedding model may put a disproportionate amount of weight on the most frequent relations, and may not learn well on the majority of rarer relations. Proper normalization is therefore necessary.\nA number of normalization strategies have been proposed in the context of word embedding, including correlation- and entropy-based normalization (Rohde et al., 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014). A shared goal is to reduce the impact of the most frequent words, e.g., \u201cthe\u201d and \u201cis,\u201d which tend to be less informative for the purpose of embedding.\nWe have a similar goal, but from preliminary studies we find that a somewhat more aggressive normalization strategy works better for relations: for each textual relation, we normalize its cooccurrence counts to form a probability distribution over KB relations. The new edge weights of the relation graph thus become wij = p\u0303(rj |ti) = nij/ \u2211 j\u2032 nij\u2032. Compared with the existing normalization strategies for word embedding, this will give more weights to the rare relations. Every textual relation now is associated with a set of edges whose weights sume to 1. It can be justified by the different distribution of words and relations. Because textual relations are sequences, they are less likely to collide than individual words, and rare relations are the norm, not the exception (a comparative analysis can be found in Appendix A). Therefore, we need to focus more on the rare relations."}, {"heading": "4 Textual Relation Embedding", "text": "Next we discuss how to learn embedding of textual relations based on the constructed relation graph. We call our approach Global Relation Embedding (GloRE) in light of global statistics of relations."}, {"heading": "4.1 Embedding via RNN", "text": "Given the relation graph, a straightforward way of relation embedding is matrix factorization, similar to latent semantic analysis (Deerwester et al., 1990) for word embedding. However, textual relations are different from words in that they are sequences composed of words and typed dependency relations. Therefore, we use recurrent neural networks (RNNs) for embedding, which respect the compositionality of textual relations and can learn the shared sub-structures of different textual relations (Toutanova et al., 2015). For the examples in Figure 1, an RNN can learn, from both textual relations, that the shared dependency relation \u201cnmod:in\u201d is indicative of location modifiers.\nFor a textual relation, we first decompose it into a sequence of tokens {x1, ..., xm}, which includes lexical words and directional dependency relations. For example, the textual relation SUBJECT nsubjpass \u2190\u2212\u2212\u2212\u2212 born nmod:in \u2212\u2212\u2212\u2212\u2192 OBJECT is decomposed into a sequence of three tokens {\u2212nsubjpass, born, nmod:in}, where \u201c\u2212\u201d represents a left arrow. Note that we include directional dependency relations, because both the relation type and the direction are critical in determining the meaning of a textual relation. For example, the dependency relation \u201cnmod:in\u201d often indicates a location modifier and is thus strongly associated with location-related KB relations like place of birth. The direction also plays an important role. Without knowing the direction of the dependency relations, it is impossible to distinguish child of and parent of.\nAn RNN with gated recurrent units (GRUs) (Cho et al., 2014) is then applied to consecutively process the sequence as shown in Figure 3. We have also explored more advanced constructs like attention, but the results are similar, so we opt for a vanilla RNN in consideration of model simplicity.\nLet \u03c6 denote the function that maps a token xl to a fixed-dimensional vector, the hidden state vectors of the RNN are calculated recursively:\nhl = GRU ( \u03c6(xl),hl\u22121 ) . (2)\nGRU follows the definition in Cho et al. (2014):\nzl = \u03c3 ( Wz\u03c6(xl) +Uzhl\u22121 ) rl = \u03c3 ( Wr\u03c6(xl) +Urhl\u22121 ) h\u0303l = tanh ( Wh\u03c6(xl) +Uh(rl \u25e6 hl\u22121) )\nhl = zl \u25e6 hl\u22121 + (1\u2212 zl) \u25e6 h\u0303l\nwhere \u03c3 is the sigmoid function, the operator \u25e6 is element-wise multiplication, W\u2217 and U\u2217 are parameter matrices of the model, zl is the update gate vector and rl is the reset gate vector. We use the final hidden state vector hm as the embedding of the textual relation."}, {"heading": "4.2 Training Objective", "text": "We use global statistics in the relation graph to train the embedding model. Specifically, we model the semantics of a textual relation as its cooccurrence distribution of KB relations, and learn textual relation embeddings to reconstruct the corresponding co-occurrence distributions.\nWe use a separate GRU cell followed by softmax to map a textual relation embedding to a distribution over KB relations (Figure 3); the full model thus resembles the sequence-to-sequence architecture (Sutskever et al., 2014). Given a textual relation ti and its embedding hm, the predicted conditional probability of a KB relation rj is thus:\np(rj |ti) = softmax(Woho + bo)j , (3)\nwhere ()j denotes the j-th element of a vector, and ho is the state of the output GRU cell:\nho = GRU ( \u03c6(<GO>),hm ) , (4)\nwhere<GO>is a special token indicating the start of decoding. The training objective is to minimize\n\u0398 = 1\n|E|\n\u2211\ni,j:p\u0303(rj |ti)>0\n(log p(rj|ti)\u2212 log p\u0303(rj |ti)) 2 ,\n(5)\nwhere E is the edge set of the relation graph. It is modeled as a regression problem, similar to GloVe (Pennington et al., 2014).\nBaseline. We also define a baseline approach where the unnormalized co-occurrence counts are directly used. The objective is to maximize:\n\u0398\u2032 = 1\u2211\ni,j nij\n\u2211\ni,j:nij>0\nnij log p(rj |ti). (6)\nIt also corresponds to local statistics based embedding, i.e., when the embedding model is trained on individual occurrences of relational facts with distant supervision. Therefore, we call it Local Relation Embedding (LoRE)."}, {"heading": "5 Augmenting Relation Extraction", "text": "Learned from global co-occurrence statistics of relations, our approach provides semantic matching information of textual and KB relations, which is often complementary to the information captured by existing relation extraction models. In this section we discuss how to combine them together to achieve better relation extraction performance.\nWe follow the setting of distantly supervised relation extraction. Given a text corpus and a KB with relation set R, the goal is to find new relational facts from the text corpus that are not already contained in the KB. More formally, for each entity pair (e, e\u2032) and a set of contextual sentences C containing this entity pair, a relation extraction model assigns a scoreE(z|C) to each candidate relational fact z = (e, r, e\u2032), r \u2208 R. On the other hand, our textual relation embedding model works on the sentence level. It assign a score G(z|s) to each contextual sentence s in C as for how well the textual relation t between the entity pair in the sentence matches the KB relation r, i.e., G(z|s) = p(r|t). It poses a challenge to aggregate the sentence-level scores to get a set-level score G(z|C), which can be used to combine with the original score E(z|C) to get a better evaluation of the candidate relational fact.\nOne straightforward aggregation is max pooling, i.e., only using the largest score maxs\u2208C G(z|s), similar to the at-least-one strategy used by Zeng et al. (2015). But it will lose the useful signals from those neglected sentences (Lin et al., 2016). Because of the wrong labeling problem, mean pooling is problematic as well. The wrongly labeled contextual sentences\ntend to make the aggregate scores more evenly distributed and therefore become less informative. The number of contextual sentences positively supporting a relational fact is also an important signal, but is lost in mean pooling.\nInstead, we use summation with a trainable cap:\nG(z|C) = min (cap, \u2211\ns\u2208C\nG(z|s)), (7)\nIn other words, we additively aggregate the signals from all the contextual sentences, but only to a bounded degree.\nWe simply use a weighted sum to combine E(z|C) and G(z|C), where the trainable weights will also handle the possibly different scale of scores generated by different models:\nE\u0303(z|C) = w1E(z|C) +w2G(z|C). (8)\nThe original score E(z|C) is then replaced by the new score E\u0303(z|C). To find the optimal values for w1, w2 and cap, we define a hinge loss:\n\u0398Merge = 1\nK\nK\u2211\nk=1\nmax { 0, 1 + E\u0303(z\u2212k )\u2212 E\u0303(z + k ) } ,\n(9)\nwhere {z+k } K k=1 are the true relational facts from the KB, and {z\u2212k } K k=1 are false relational facts generated by replacing the KB relation in true relational facts with incorrect KB relations."}, {"heading": "6 Experiments", "text": "In this experimental study, we demonstrate the effectiveness of GloRE by showing that it could significantly improve the performance of several recent relation extraction methods."}, {"heading": "6.1 Experimental Setup", "text": "Dataset. Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al., 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al., 2008). Articles from year 2005-2006 are used as training, and articles from 2007 are used as testing. Some statistics are listed in Table 1. There are 53 target KB relations, including a special relation NA indicating that there is no target relation between an entity pair.\nWe follow the approach described in Section 3 to construct the relation graph from the NYT training data. The constructed relation graph contains 321,447 edges with non-zero weight. We further obtain a training set and a validation set from the edges of the relation graph. We have observed that using a validation set totally disjoint from the training set leads to unstable validation loss, so we randomly sample 300K edges as the training set, and another 60K as the validation set. The two sets can have some overlap. For the merging model (Eq. (9)), 10% of the edges are reserved as the validation set.\nRelation extraction models. We evaluate with four recent relation extraction models whose source code is publicly available3 . We use the optimized parameters provided by the authors.\n\u2022 CNN+ONE and PCNN+ONE (Zeng et al., 2015): A convolutional neural network\n(CNN) is used to embed contextual sentences for relation classification. Multi-instance learning with at-least-one (ONE) assumption is used to combat the wrong labeling problem. In PCNN, piecewise max pooling is used to handle the three pieces of a contextual sentence (split by the two entities) separately.\n\u2022 CNN+ATT and PCNN+ATT (Lin et al., 2016): Different from the at-least-one as-\nsumption which loses information in the neglected sentences, these models learn soft attention weights (ATT) over contextual sentences and thus can use the information of all the contextual sentences. PCNN+ATT is the best-performing model on the NYT dataset.\nEvaluation settings and metrics. Similar to previous work (Riedel et al., 2010; Zeng et al., 2015), we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training (Table 1), and is later used to compare against newly discovered relational facts. This setting avoids human labor but can introduce some false negatives because of the\n3 https://github.com/thunlp/NRE\nincompleteness of the KB. (2) Manual evaluation, where the discovered relational facts are manually judged by human experts. For held-out evaluation, we report the precision-recall curve. For manual evaluation, we report Precision@N , i.e., the precision of the top N discovered relational facts.\nParameter settings. Hyper-parameters of our model are selected based on the validation set. For the embedding model, the mini-batch size is set to 128, and the state size of the GRU cells is 300. For the merging model, the mini-batch size is set to 1024. We use Adam (Kingma and Ba, 2014) with parameters suggested by the authors for optimization. Word embeddings are initialized with the 300-dimensional word2vec (Mikolov et al., 2013) vectors pre-trained on the Google News corpus4. Early stopping based on the validation set is employed. Our model is implemented using Tensorflow (Abadi et al., 2016), and the source code is available at https://github.com/ ppuliu/GloRE."}, {"heading": "6.2 Held-out Evaluation", "text": "Existing Models + GloRE. We first show that our approach, GloRE, can improve the performance of the previous best-performing model,\n4 https://code.google.com/archive/p/\nword2vec/\nPCNN+ATT, leading to a new state of the art on the NYT dataset. As shown in Figure 5, when PCNN+ATT is augmented with GloRE, a consistent improvement along the precision-recall curve is observed. It\u2019s worth noting that although PCNN+ATT+GloRE seems to be inferior to PCNN+ATT when recall < 0.05, as we will show via manual evaluation, it is actually due to false negatives.\nWe also show in Figure 4 that the improvement brought by GloRE is general, not just specific to PCNN+ATT; the other three models also get a consistent improvement when augmented with GloRE. The results suggest that our textual relation embedding approach with global statistics indeed captures useful information for relation extraction that is not captured by these sentence embedding based models.\nLoRE v.s. GloRE. We compare GloRE with the baseline approach LoRE (Section 4) to show the advantage of normalization on global statistics. We use PCNN+ATT as the base relation extraction model. As shown in Figure 6, GloRE consis-\ntently outperforms LoRE. It is worth noting that LoRE can still improve the base relation extraction model when recall > 0.15, further confirming the usefulness of directly embedding textual relations in addition to sentences."}, {"heading": "6.3 Manual Evaluation", "text": "Due to the incompleteness of the knowledge base, held-out evaluation introduces some false negatives. The precision from held-out evaluation is therefore a lower bound of the true precision. To get a more accurate evaluation of model performance, we have human experts to manually check the false relational facts judged by heldout evaluation in the top 1,000 predictions of three models: PCNN+ATT, PCNN+ATT+LoRE, and PCNN+ATT+GloRE, and report the corrected results in Table 2. Under manual evaluation, PCNN+ATT+GloRE achieves the best performance in the full range of N . In particular, for the top 1,000 predictions, GloRE improves the precision of the previous best model PCNN+ATT from 83.9% to 89.3%. The manual evaluation results reinforce the previous observations from held-out evaluation."}, {"heading": "6.4 Case Study", "text": "Table 3 shows two examples. For better illustration, we choose entity pairs that have only one contextual sentence.\nFor the first example, PCNN+ATT predicts that most likely there is no KB relation between the entity pair, while both LoRE and GloRE identify the correct relation with high confidence. The textual relation clearly indicates that the head entity is (appos) a criminologist at (nmod:at) the tail entity.\nFor the second example, there is no KB relation between the entity pair, and PCNN+ATT is indeed able to rank NA at the top. However, it is still quite confused by nationality, probably because it has learned that sentences about a person and a country with many words about\nprofession (\u201cpoet,\u201d \u201cplaywright,\u201d and \u201cnovelist\u201d) likely express the person\u2019s nationality. As a result, its prediction on NA is not very confident. On the other hand, GloRE learns that if a person \u201ccame to\u201d a place, likely it is not his/her birthplace. In the training data, due to the wrong labeling problem of distant supervision, the textual relation is wrongly labeled with place of death and nationality a couple of times, and both PCNN+ATT and LoRE suffer from the training noise. Taking advantage of global statistics, GloRE is more robust to such noise introduced by the wrong labeling problem."}, {"heading": "7 Conclusion", "text": "Our results show that textual relation embedding based on global co-occurrence statistics with KB relations captures useful information for relation extraction, and, as a result, can improve existing relation extraction models. Large-scale training data for embedding can be easily solicited from distant supervision, and the global statistics of relations provide a natural way to combat the wrong labeling problem of distant supervision.\nThe idea of relation embedding based on global statistics can be further expanded along several directions. In this work we have focused on embedding textual relations, but it is in principle beneficial to jointly embed knowledge base (KB) relations as well as entities. Recently a joint embedding approach has been attempted in the context of knowledge base completion (Toutanova et al., 2015), but it is still based on local statistics, i.e., individual relational facts. Joint embedding with global statistics remains an open problem. On the other hand, we have analyzed the distribution difference of words and textual relations, and its impact on normalizing the co-occurrence statistics of relations. A more thorough comparative study of normalization strategies can shed light on future use of co-occurrence statistics of relations."}, {"heading": "A Word and Relation Distribution", "text": "Because textual relations are sequences composed of words and dependency relations, they are less likely to collide than words. As a result, most of textual relations only occur a few times. From the New York Times corpus, we collect the occurrence and co-occurrence statistics of words and textual relations. Two words co-occur if they appear in the same sentence. The co-occurring relationship of textual and KB relations are established via distant supervision, as described in Section 3. As can be seen, the relation distributions are more skewed towards the lower end of the xaxis than the word distributions. Very few textual relations occur more than 100 times. Rare relations are the norm, not the exception. But it is worth mentioning that the most frequent textual relation still occurs more than 40 thousand times (not shown in the figures). Normalization of relation co-occurrence counts shall consider the characteristics of relation distributions, and give proper weights to rare relations."}], "references": [{"title": "Tensorflow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the ACM SIGMOD International conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["John A Bullinaria", "Joseph P Levy."], "venue": "Behavior research methods 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy.,? 2007", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan C Bunescu", "Raymond J Mooney."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 724\u2013731.", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Dependency tree kernels for relation extraction", "author": ["Aron Culotta", "Jeffrey Sorensen."], "venue": "Proceedings of the AnnualMeeting of the Association for Computational Linguistics. Association for Computational Linguistics, page 423.", "citeRegEx": "Culotta and Sorensen.,? 2004", "shortCiteRegEx": "Culotta and Sorensen.", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman."], "venue": "Journal of the American society for information science 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Thomas G Dietterich", "Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez."], "venue": "Artificial intelligence 89(1):31\u201371.", "citeRegEx": "Dietterich et al\\.,? 1997", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou GuoDong", "Su Jian", "Zhang Jie", "Zhang Min."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 427\u2013", "citeRegEx": "GuoDong et al\\.,? 2005", "shortCiteRegEx": "GuoDong et al\\.", "year": 2005}, {"title": "Knowledgebased weak supervision for information extraction", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": null, "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations", "author": ["Nanda Kambhatla."], "venue": "Proceedings of the ACL on Interactive poster and demonstration sessions. Association for Computational Linguistics,", "citeRegEx": "Kambhatla.,? 2004", "shortCiteRegEx": "Kambhatla.", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Word embeddings through hellinger PCA", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "European Chapter of the Association for Computational Linguistics page 482.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Compu-", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Relation classification via modeling augmented dependency paths", "author": ["Yang Liu", "Sujian Li", "Furu Wei", "Heng Ji."], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 24(9):1585\u20131594.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems. pages", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguis-", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 148\u2013163.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M Marlin."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["Douglas LT Rohde", "Laura M Gonnerman", "David C Plaut."], "venue": "Communications of the ACM 8:627\u2013633.", "citeRegEx": "Rohde et al\\.,? 2005", "shortCiteRegEx": "Rohde et al\\.", "year": 2005}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher DManning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Multiinstance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Toutanova et al\\.,? 2015", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Asso-", "citeRegEx": "Verga et al\\.,? 2016", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "arXiv:1506.07650 .", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networkswith data augmentation", "author": ["Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin."], "venue": "arXiv:1601.03651 .", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Kernel methods for relation extraction", "author": ["Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella."], "venue": "Journal of machine learning research 3(Feb):1083\u20131106.", "citeRegEx": "Zelenko et al\\.,? 2003", "shortCiteRegEx": "Zelenko et al\\.", "year": 2003}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing. Association for", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of the International Conference on Computational Linguistics", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Exploring syntactic features for relation extraction using a convolution tree kernel", "author": ["Min Zhang", "Jie Zhang", "Jian Su."], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the As-", "citeRegEx": "Zhang et al\\.,? 2006", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Early studies mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate fea-", "startOffset": 47, "endOffset": 86}, {"referenceID": 9, "context": "Early studies mainly use hand-crafted features (Kambhatla, 2004; GuoDong et al., 2005), and later kernel methods are introduced to automatically generate fea-", "startOffset": 47, "endOffset": 86}, {"referenceID": 30, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 6, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 3, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 33, "context": "tures (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 6, "endOffset": 102}, {"referenceID": 22, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 32, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 29, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 31, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 14, "context": "sentences in continuous feature space, and have shown a remarkable success in relation extraction (Socher et al., 2012; Zeng et al., 2014; Xu et al., 2015b; Zeng et al., 2015; Lin et al., 2016).", "startOffset": 98, "endOffset": 193}, {"referenceID": 3, "context": "Because of their exact feature matching, early kernel based models (Bunescu and Mooney, 2005) can hardly exploit fine-grained word similarities.", "startOffset": 67, "endOffset": 93}, {"referenceID": 15, "context": "More recent studies (Xu et al., 2015a,b, 2016; Liu et al., 2016) have explored embedding textual relations via neural networks.", "startOffset": 20, "endOffset": 64}, {"referenceID": 15, "context": "eral thousands of annotated sentences and around 10 target relations (Liu et al., 2016).", "startOffset": 69, "endOffset": 87}, {"referenceID": 17, "context": "In contrast, we embed textual relations with distant supervision (Mintz et al., 2009), which provides much larger-scale training data without the need of manual annotation.", "startOffset": 65, "endOffset": 85}, {"referenceID": 25, "context": "Statistics are based on the annotated ClueWeb data released in (Toutanova et al., 2015).", "startOffset": 63, "endOffset": 87}, {"referenceID": 19, "context": "On a popular dataset introduced by Riedel et al. (2010), we show that a number of recent relation extraction models, which are based on local statistics, can be significantly improved using our textual relation embeddings.", "startOffset": 35, "endOffset": 56}, {"referenceID": 11, "context": "Early relation extraction methods are mainly feature-based (Kambhatla, 2004; GuoDong et al., 2005), where features at various levels, including POS tags, constituency and dependency parses, are integrated in a max entropy model.", "startOffset": 59, "endOffset": 98}, {"referenceID": 9, "context": "Early relation extraction methods are mainly feature-based (Kambhatla, 2004; GuoDong et al., 2005), where features at various levels, including POS tags, constituency and dependency parses, are integrated in a max entropy model.", "startOffset": 59, "endOffset": 98}, {"referenceID": 30, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 6, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 3, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 33, "context": "With the popularity of kernel methods, a large number of kernel-based relation extraction models have been proposed (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006).", "startOffset": 116, "endOffset": 212}, {"referenceID": 3, "context": ", 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhang et al., 2006). The most related work to ours is by Bunescu and Mooney (2005), where the authors point out and demonstrate the importance of shortest dependency paths for relation extraction.", "startOffset": 36, "endOffset": 145}, {"referenceID": 22, "context": ", (Socher et al., 2012; Zeng et al., 2014)).", "startOffset": 2, "endOffset": 42}, {"referenceID": 32, "context": ", (Socher et al., 2012; Zeng et al., 2014)).", "startOffset": 2, "endOffset": 42}, {"referenceID": 15, "context": "Among those, the most related are the ones embedding shortest dependency paths with neural networks (Xu et al., 2015a,b, 2016; Liu et al., 2016).", "startOffset": 100, "endOffset": 144}, {"referenceID": 27, "context": "(2015b) use a recurrent neural network (RNN) with LSTM units to embed shortest dependency paths without typed dependency relations, while a convolutional neural network is used in (Xu et al., 2015a).", "startOffset": 180, "endOffset": 198}, {"referenceID": 15, "context": ", 2015a,b, 2016; Liu et al., 2016). For example, Xu et al. (2015b) use a recurrent neural network (RNN) with LSTM units to embed shortest dependency paths without typed dependency relations, while a convolutional neural network is used in (Xu et al.", "startOffset": 17, "endOffset": 67}, {"referenceID": 17, "context": "Distant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction.", "startOffset": 20, "endOffset": 40}, {"referenceID": 8, "context": "(2012) have attempted a multi-instance learning (Dietterich et al., 1997) framework to soften the assumption of distant supervision, but their", "startOffset": 48, "endOffset": 73}, {"referenceID": 15, "context": "Distant supervision (Mintz et al., 2009) has emerged as an appealing way to solicit largescale training data for relation extraction. Various efforts have been put to combat the longcriticized wrong labeling problem. Riedel et al. (2010), Hoffmann et al.", "startOffset": 21, "endOffset": 238}, {"referenceID": 9, "context": "(2010), Hoffmann et al. (2011), and Surdeanu et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 9, "context": "(2010), Hoffmann et al. (2011), and Surdeanu et al. (2012) have attempted a multi-instance learning (Dietterich et al.", "startOffset": 8, "endOffset": 59}, {"referenceID": 31, "context": "Zeng et al. (2015) combine multi-instance learning with neural networks, with the assumption that at least one of the contextual sentences of an entity pair is express-", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Instead, Lin et al. (2016) use all the contextual sentences, and", "startOffset": 9, "endOffset": 27}, {"referenceID": 20, "context": "In universal schema (Riedel et al., 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 25, "context": ", 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al., 2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns.", "startOffset": 76, "endOffset": 120}, {"referenceID": 26, "context": ", 2013) for KB completion and relation extraction as well as its extensions (Toutanova et al., 2015; Verga et al., 2016), a binary matrix is constructed from the entire corpus, with entity pairs as rows and textual/KB relations as columns.", "startOffset": 76, "endOffset": 120}, {"referenceID": 16, "context": "The skip-gram model (Mikolov et al., 2013) is based on local statistics: During training, we sweep through the corpus and slightly tune the embedding model based on each local window (e.", "startOffset": 20, "endOffset": 42}, {"referenceID": 7, "context": "In contrast, in global statistics based methods, exemplified by latent semantic analysis (Deerwester et al., 1990) and GloVe (Pennington et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 18, "context": ", 1990) and GloVe (Pennington et al., 2014), we process the entire corpus to collect global statistics like", "startOffset": 18, "endOffset": 43}, {"referenceID": 4, "context": "In the experiments entity linking is assumed given, and dependency parsing is done using Stanford Parser (Chen and Manning, 2014) with universal dependencies.", "startOffset": 105, "endOffset": 129}, {"referenceID": 21, "context": "A number of normalization strategies have been proposed in the context of word embedding, including correlation- and entropy-based normalization (Rohde et al., 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 145, "endOffset": 165}, {"referenceID": 2, "context": ", 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 47, "endOffset": 74}, {"referenceID": 13, "context": ", 2005), positive pointwise mutual information (Bullinaria and Levy, 2007), and some square root type transformation (Lebret and Collobert, 2014).", "startOffset": 117, "endOffset": 145}, {"referenceID": 7, "context": "Given the relation graph, a straightforward way of relation embedding is matrix factorization, similar to latent semantic analysis (Deerwester et al., 1990) for word embedding.", "startOffset": 131, "endOffset": 156}, {"referenceID": 25, "context": "Therefore, we use recurrent neural networks (RNNs) for embedding, which respect the compositionality of textual relations and can learn the shared sub-structures of different textual relations (Toutanova et al., 2015).", "startOffset": 193, "endOffset": 217}, {"referenceID": 5, "context": "An RNN with gated recurrent units (GRUs) (Cho et al., 2014) is then applied to consecutively process the sequence as shown in Figure 3.", "startOffset": 41, "endOffset": 59}, {"referenceID": 5, "context": "GRU follows the definition in Cho et al. (2014):", "startOffset": 30, "endOffset": 48}, {"referenceID": 24, "context": "We use a separate GRU cell followed by softmax to map a textual relation embedding to a distribution over KB relations (Figure 3); the full model thus resembles the sequence-to-sequence architecture (Sutskever et al., 2014).", "startOffset": 199, "endOffset": 223}, {"referenceID": 18, "context": "It is modeled as a regression problem, similar to GloVe (Pennington et al., 2014).", "startOffset": 56, "endOffset": 81}, {"referenceID": 14, "context": "But it will lose the useful signals from those neglected sentences (Lin et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 30, "context": ", only using the largest score maxs\u2208C G(z|s), similar to the at-least-one strategy used by Zeng et al. (2015). But it will lose the useful signals from those neglected sentences (Lin et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 10, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 23, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 31, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 14, "context": "Following the literature (Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016), we use the relation extraction dataset introduced in (Riedel et al.", "startOffset": 25, "endOffset": 108}, {"referenceID": 19, "context": ", 2016), we use the relation extraction dataset introduced in (Riedel et al., 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 1, "context": ", 2010), which was generated by aligning New York Times (NYT) articles with Freebase (Bollacker et al., 2008).", "startOffset": 85, "endOffset": 109}, {"referenceID": 31, "context": "\u2022 CNN+ONE and PCNN+ONE (Zeng et al., 2015): A convolutional neural network (CNN) is used to embed contextual sentences for relation classification.", "startOffset": 23, "endOffset": 42}, {"referenceID": 14, "context": "\u2022 CNN+ATT and PCNN+ATT (Lin et al., 2016): Different from the at-least-one assumption which loses information in the ne-", "startOffset": 23, "endOffset": 41}, {"referenceID": 19, "context": "Similar to previous work (Riedel et al., 2010; Zeng et al., 2015), we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training (Table 1), and is later used to compare against newly discovered relational facts.", "startOffset": 25, "endOffset": 65}, {"referenceID": 31, "context": "Similar to previous work (Riedel et al., 2010; Zeng et al., 2015), we use two settings for evaluation: (1) Held-out evaluation, where a subset of relational facts in KB is held out from training (Table 1), and is later used to compare against newly discovered relational facts.", "startOffset": 25, "endOffset": 65}, {"referenceID": 12, "context": "We use Adam (Kingma and Ba, 2014) with parameters suggested by the authors for optimization.", "startOffset": 12, "endOffset": 33}, {"referenceID": 16, "context": "Word embeddings are initialized with the 300-dimensional word2vec (Mikolov et al., 2013) vectors pre-trained on the Google News corpus.", "startOffset": 66, "endOffset": 88}, {"referenceID": 0, "context": "Our model is implemented using Tensorflow (Abadi et al., 2016), and the source code is available at https://github.", "startOffset": 42, "endOffset": 62}, {"referenceID": 25, "context": "Recently a joint embedding approach has been attempted in the context of knowledge base completion (Toutanova et al., 2015), but it is still based on local statistics, i.", "startOffset": 99, "endOffset": 123}], "year": 2017, "abstractText": "Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction. However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data. In this work, we generalize textual relation embedding to the distant supervision setting, where much largerscale but noisy training data is available. We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.", "creator": "LaTeX with hyperref package"}}}