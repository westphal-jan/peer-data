{"id": "1704.05973", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks for Early Rumor Detection", "abstract": "The proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of diffusion is known as \\textit{early rumor detection}, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. Thus, identifying trending rumors demands an efficient yet flexible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection of rumor in different contexts.\n\n\n\nThe application of early detection of fake news as a possible basis for generating news, as well as for promoting other relevant stories in relation to a specific topic or topic.\nThe application of early detection of false news to be considered to be a potential tool for disseminating false news to an audience without actually identifying it in a context that would only be perceived as false news.\nA recent survey of 2,500 social media users in Finland, Germany and the Netherlands by F\u00e4j\u00f6lle found that a majority of social media users, compared to 10% in Finland and 13% in the Netherlands.\nThe current and recent trends in news consumption among social media users and users can be traced to a change in media habits during the last 30 years. The change is well documented in the research conducted in the recent and current trends in the research of social media users.\nThe following data (available online) shows the trends, in comparison to prior data, and the trend in media usage between the two trends:\nThe trends are more prevalent among social media users than it was in the past, while more frequent on social media users are more common among social media users.\nOn top of this, the growth in mobile users is increasing among social media users. The trend is being driven by trends in the growth of mobile users.\nF\u00e4j\u00f6lle has been successful with the development of many new technologies such as artificial intelligence to better distinguish the false news from the fake news (e.g., fake news).", "histories": [["v1", "Thu, 20 Apr 2017 01:22:57 GMT  (2366kb,D)", "http://arxiv.org/abs/1704.05973v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["tong chen", "lin wu", "xue li", "jun zhang", "hongzhi yin", "yang wang"], "accepted": false, "id": "1704.05973"}, "pdf": {"name": "1704.05973.pdf", "metadata": {"source": "META", "title": "Call Attention to Rumors: Deep Attention Based Recurrent Neural Networks for Early Rumor Detection", "authors": ["Tong Chen", "Lin Wu", "Xue Li", "Jun Zhang", "Hongzhi Yin", "Yang Wang"], "emails": ["tong.chen@uq.edu.au", "lin.wu@uq.edu.au", "xue.li@itee.uq.edu.au", "jun.zhang@deakin.edu.au", "h.yin1@uq.edu.au", "wangy@cse.unsw.edu.au", "@mantrackerone:", "@TheHamptonKid:", "@Evertxn:"], "sections": [{"heading": null, "text": "KEYWORDS Early rumor detection, Recurrent neural networks, Deep a ention models"}, {"heading": "1 INTRODUCTION", "text": "e explosive use of contemporary social media in communication has witnessed the widespread of rumors which can pose a threat to the cyber security and social stability. For instance, on April 23rd 2013, a fake news claiming two explosions happened in the White House and Barack Obama got injured was posted by a hacked Twi er account named Associated Press. Although the White House and Associated Press assured the public minutes later the report was not true, the fast di usion to millions of users had \u2217Corresponding author\ncaused severe social panic, resulting in a loss of $136.5 billion in the stock market1. is incident of a false rumor showcases the vulnerability of social media on rumors, and highlights the practical value of automatically predicting the veracity of information.\n@mantrackerone: Donald Trump: Marco Rubio Disqualified, 'Cannot Be President' After 0.91 1.00 0.67 0.67 0.98 0.73 0.67 1.00 0.45 Refusing To Undo Obama Executive Amnesty 0.33 0.50 0.54 0.82 0.59 0.59 @TheHamptonKid: So Donald Trump is disqualified from being president?? 0.11 0.91 1.00 0.63 0.98 0.83 0.67 1.00 @Evertxn: Donald Trump has been disqualified from running for president xD 0.91 1.00 0.25 0.67 0.98 0.83 0.47 0.67 1.00 0.03 @MikeMinusJordan: Trump: Marco Rubio Disqualified, 'Cannot Be President' After 1.00 0.67 0.67 0.98 0.73 0.67 1.00 0.45 Refusing To Undo Obama Executive Amnesty Immediately 0.33 0.50 0.54 0.82 0.59 0.59 0.14 @cigdemk14: Is Donald Trump really disqualified from running for president? 0.63 0.91 1.00 0.20 0.98 0.83 0.47 0.67 1.00 @ia_diego: Donald trump is disqualified from running for president! Bet! 0.91 1.00 0.63 0.98 0.83 0.47 0.67 1.00 0.01\nDebunking rumors at their formative stage is particularly crucial to minimizing their catastrophic e ects. Most existing rumor detection models employ learning algorithms that incorporate a wide variety of features and formulate rumor detection into a binary classi cation task. ey commonly cra features manually from the content, sentiment [51], user pro les [30, 31, 47], and di usion pa erns of the posts [12, 16, 18, 34, 36, 37]. Embedding social graphs into a classi cation model also helps distinguish malicious user comments from normal ones [20, 21]. ese approaches aim at extracting distinctive features to describe rumors faithfully.\n1h p://www.dailymail.co.uk/news/article-2313652/AP-Twi er-hackers-break-newsWhite-House-explosions-injured-Obama.html\nar X\niv :1\n70 4.\n05 97\n3v 1\n[ cs\n.C L\n] 2\n0 A\npr 2\nHowever, feature engineering is extremely time-consuming, biased, and labor-intensive. Moreover, hand-cra ed features are data-dependent, making them incapable of capturing contextual variations in di erent posts.\nMore close examinations on rumors reveal that social posts related to an event under discussion are coming in the form of time series wherein users forward or comment on it continuously over time. As shown in Fig.1 (a), posts in regards to an event of US presidency are coming continuously along the event\u2019s timelines. us, to tackle with time series of posts, descriptive features should be extracted from contexts. However, as shown in Fig.1 (b), users\u2019 posts exhibit high duplication in their textual phrases due to the repeated forwarding, reviews, and/or inquiry behavior [49]. is poses a challenge on e ciently distilling distinct information from duplication, and exible to capture their contextual variations as the rumor di uses over time.\ne propagation of information on social media has temporal characteristics, whilst most existing rumor detection methodologies ignore such a crucial property or are not able to capture the temporal dimension of data. One exception is [17] where Ma et al. uses an RNN to capture the dynamic temporal signals of rumor di usion and learn textual representations under supervision. However, as the rumor di usion evolves over time, users tend to comment di erently in various stages, such as from expressing surprise to questioning, or from believing to debunking. As a consequence, textual features may change their importance with time and we need to determine which of them are more important to the detection task. On the other hand, the existence of duplication in textual phrases impedes the e ciency of training a deep network. Although some studies on duplication detection are available and e ective in di erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time. In this sense, two aspects of temporal long-term characteristic and dynamic duplication should be addressed simultaneously in an early rumor detection model."}, {"heading": "1.1 Challenges and Our Approach", "text": "In summary, there are three challenges in early rumor detection to be addressed: (1) automatically learning representations for rumors instead of using labor-intensive hand-cra ed features; (2) the di culty of maintaining the long-range dependency among variable-length post series to build their internal representations; (3) the issue of high duplication compounded with varied contextual focus. To combat these challenges, we propose a novel deep a ention based recurrent neural network (RNN) for early detection on rumors, namely CallAtRumors (Call Attention to Rumors). e overview of our framework is illustrated in Fig 2. Our model processes streaming textual sequences constructed by encoding contextual information from posts related to one event into a series of feature matrices. en, the RNN with a ention mechanism automatically learns latent representations by feed-forwarding each input weighted by a ention probability distribution while adaptive to contextual variations. Moreover, an additional hidden layer with si\u0434moid activation function using the learned latent representations predicts the event to be rumors or not.\nOur framework is premised on the RNNs which are proved to be e ective in recent machine learning tasks [1, 7] in handling sequential data. is o ers us the opportunity to automatically explore deep feature representations from original inputs for efcient rumor detection, thus avoiding the complexity of feature engineering. With a ention mechanism, the proposed approach is able to selectively associate more importance with relevant features. Hence, we are able to tackle the problem in the context of high textual duplication and the e ciency of feature learning in early detection is ensured."}, {"heading": "1.2 Contributions", "text": "e main contributions of our work are summarized as follows:\n\u2022 We propose a deep a ention based model that learns to perform rumor detection automatically in earliness. e model is based on RNN, and capable of learning continuous hidden representations by capturing long-range dependency an contextual variations of posting series. \u2022 e deterministic so -a ention mechanism is embedded into recurrence to enable distinct feature extraction from high duplication and advanced importance focus that varies over time. \u2022 We quantitatively validate the e ectiveness of a ention in terms of detection accuracy and earliness by comparing with state-of-the-arts on two real social media datasets: Twi er and Weibo.\ne rest of the paper is organized as follows. Section 2 and Section 3 present the relationship to existing work and preliminary on RNN. We introduce the main intuition and formulate the problem in Section 4. Section 5 discusses the experiments and the results on e ectiveness and earliness. We conclude this paper in Section 6 and points out future directions."}, {"heading": "2 RELATEDWORK", "text": "Our work is closely connected with early rumor detection and a ention mechanism. We will brie y introduce the two aspects in this section."}, {"heading": "2.1 Early Rumor Detection", "text": "e problem of rumor detection [4] can be cast as binary classi cation tasks. e extraction and selection of discriminative features signi cantly a ects the performance of the classi er. Hu et al. rst conducted a study to analyze the sentiment di erences between spammers and normal users and then presented an optimization formulation that incorporates sentiment information into a novel social spammer detection framework [10]. Also the propagation pa erns of rumors were developed by Wu et al. through utilizing a message propagation tree where each node represents a text message to classify whether the root of the tree is a rumor or not [37]. In [18], a dynamic time series structure was proposed to capture the temporal features based on the time series context information generated in every rumor\u2019s life-cycle. However, these approaches requires daunting manual e orts in feature engineering and they are restricted by the data structure.\nEarly rumor detection is to detect viral rumors in their formative stages in order to take early action [24]. In [49], some very rare"}, {"heading": "2.90 0.75 0 \u2026", "text": "Posts about One Event Textual Feature Extraction Deep Attention Based Neural Network Latent Additional Classified as\nRepresentations Hidden Rumor or\nLayer Non-rumor\n0 1.15 0.97 \u2026\n3.14 0 0 \u2026\n\u2026\n2.46 0.38 0 \u2026\nbut informative enquiry phrases play an important role in feature engineering when combined with clustering and a classi er on the clusters as they shorten the time for spo ing rumors. Manually de ned features has shown their importance in the research on real-time rumor debunking by Liu et al. [16]. By contrast, Wu et al. proposed a sparse learning method to automatically select discriminative features as well as train the classi er for emerging rumors [39]. As those methods neglect the temporal trait of social media data, a time-series based feature structure[18] is introduced to seize context variation over time. Recently, recurrent neural network was rst introduced to rumor detection by Ma et al. [17], utilizing sequential data to spontaneously capture temporal textual characteristics of rumor di usion which helps detecting rumor earlier with accuracy. However, without abundant data with di erentiable contents in the early stage of a rumor, the performance of these methods drops signi cantly because they fail to distinguish important pa erns."}, {"heading": "2.2 Attention Mechanism", "text": "As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al. extended the basic encoderdecoder architecture of neural machine translation with a ention mechanism to allow the model to automatically search for parts of a source sentence that are relevant to predicting a target word [2], achieving a comparable performance in the English-to-French translation task. Vinyals et al. improved the a ention model in [2], so their model computed an a ention vector re ecting how much a ention should be put over the input words and boosted the performance on large scale translation [29]. In addition, Sharma et al. applied a location so max function [25] to the hidden states of the LSTM (Long Short-Term Memory) layer, thus recognizing more valuable elements in sequential inputs for action recognition. In conclusion, motivated by the successful applications of a ention mechanism, we nd that a ention-based techniques can help better detect rumors with regards to both e ectiveness and earliness because they are sensitive to distinctive textual features."}, {"heading": "3 RECURRENT NEURAL NETWORKS", "text": "Recurrent neural networks, or RNNs [23], are a family of feedforward neural networks for processing sequential data, such as a sequence of values x1, ...,x\u03c4 . RNNs process an input sequence one element xt at a time, updates the hidden units ht , a \u201cstate vector\u201d that implicitly contains information about the history of all the past elements of the sequence (x<t ), and generates output vector ot [13]. e forward propagation begins with a speci cation of the initial state h0, then, for each time step t from t = 1 to t = \u03c4 , the following update equations are applied [6]:\nht = tanh(Uxt +Wht\u22121 + b), ot = Vht + c,\n(1)\nwhere parameters U , V and W are weight matrices for input-tohidden, hidden-to-output and hidden-to-hidden connections, respectively. b and c are the bias vectors. tanh(\u00b7) is a hyperbolic tangent non-linear function.\ne gradient computation of RNNs involves performing backpropagation through time (BPTT) [23]. In practice, a standard RNN is di cult to be trained due to the well-known vanishing or exploding gradients caused by the incapability of RNN in capturing the long-distance temporal dependencies for the gradient based optimization [3, 40]. To tackle this training di culty, an e ective solution is to includes \u201cmemory\u201d cells to store information over time, which are known as Long Short-Term Memory (LSTM) [7, 9]. In this work, we employ LSTM as basic unit to capture long term temporal dependency among streaming variable-length post series."}, {"heading": "4 CALLATRUMORS: EARLY RUMOR DETECTIONWITH DEEP ATTENTION BASED RNN", "text": "In this section, we present the details of our framework with deep a ention for classifying social textual events into rumors and nonrumors. First, we introduce a strategy that converts the incoming streams of social posts into continuous variable-length time series. en, we describe the so a ention mechanism which can be embedded into recurrent neural networks to focus on selectively textual cues to learn distinct representations for rumor and/or nonrumor binary classi cation."}, {"heading": "4.1 Problem Statement", "text": "Individual social posts contain very limited content due to their nature of shortness in context. On the other hand, a claim is generally associated with a number of posts that are relevant to the claim. ese relevant posts regarding a claim can be easily collected to describe the central content more faithfully. Hence, we are interested in detecting rumor on an aggregate level instead of identifying each single posts [17]. In other words, we focus on detecting rumors on event-level wherein sequential posts related to the same topics are batched together to constitute an event, and our model determines whether the event is a rumor or not.\nLet E = {Ei } denote a set of given events, where each event Ei = {(pi, j , ti, j )}nij=1 consists of all relevant posts pi, j at time stamp ti, j , and the task is to classify each event as a rumor or not."}, {"heading": "4.2 Constructing Variable-Length Post Series", "text": "For each event Ei , we collect a set of relevant post series to be the input of our model to learn latent representations. Within every event, posts are divided into time intervals, each of which is regarded as a batch. is is because it is not practical to deal with each post individually in the large number scale. To ensure a similar word density for each time step within one event, we group posts into batches according to a xed post amount N rather than slice the event time span evenly.\nAlgorithm 1 describes the construction of variable-length post series. Speci cally, for every event Ei = {(pi, j , ti, j )}nij=1, post series are constructed with variable lengths due to di erent amount of posts relevant to di erent events. We set a minimum series length Min to maintain the sequential property for all events. For events containing no less than N\u00d7Min posts, we iteratively take the rst N posts out of Ei and feed them into a time interval T . e last ni \u2212 N b niN c posts are treated as the last time interval. For events containing less than N\u00d7Min posts, we put b niMin c posts to the rst Min \u2212 1 intervals and assign the rest into the last interval TMin .\nTo model di erent words, we calculate the tf-idf (Term FrequencyInverse Document Frequency) for the most frequent K vocabularies within all posts. Proved to be an e ective and lightweight textual feature, tf-idf is a numerical statistic that is intended to re ect how important a word is to a document in a collection or corpus [14]. In this case, for each post we have a K-word dictionary re ecting the importance of every word in a post, and the value is 0 if the word never appears in this post. Finally, every post is encoded by the corresponding K-word tf-idf dictionary, and within a speci c internal a matrix of K\u00d7N can be constructed as the input of our model. If there are less than N posts within an interval, we will expand it to the same scale by padding with 0s. Hence, each set of post series consists of at least Min feature matrices with a same size of K (number of vocabularies) \u00d7 N (vocabulary feature dimension)."}, {"heading": "4.3 Long Short-Term Memory (LSTM) with Deterministic So Attention Mechanism", "text": "To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors. e structure of LSTM is formulated as\nit = \u03c3 (Uiht\u22121 +Wixt +Vict\u22121 + bi ), ft = \u03c3 (Uf ht\u22121 +Wf xt +Vf ct\u22121 + bf ), ct = ftct\u22121 + it tanh(Ucht\u22121 +Wcxt + bc ), ot = \u03c3 (Uoht\u22121 +Woxt +Voct + bo ), ht = ot tanh(ct ),\n(2)\nwhere \u03c3 (\u00b7) is the logistic sigmoid function, and it , ft , ot , ct are the input gate, forget gate, output gate and cell input activation vector, respectively. In each of them, there are corresponding input-tohidden, hidden-to-output, and hidden-to-hidden matrices: U\u2022, V\u2022, W\u2022 and the bias vector b\u2022. e LSTM architecture is essentially a memory cell which can maintain its state over time, and non-linear gating units can regulate the information ow into and out of the cell [8]. A LSTM unit is shown graphically in Fig. 3 (a).\nInput :Event-related posts Ei = {(pi, j , ti, j )}nij=1, post amount N , minimum series length Min\nOutput :Post Series Si = {T1, ...,Tv } 1 /*Initialization*/; 2 v = 1; x = 0; y = 0; 3 while true do 4 if ni \u2265 N \u00d7Min then 5 while v \u2264 b niN c do 6 x = N \u00d7 (v \u2212 1) + 1; 7 y = N \u00d7v ; 8 Tv \u2190 (pi,x , ...,pi,y ); 9 v + +;\n10 end 11 Tv \u2190 (pi,y+1, ...,pi,ni ); 12 else 13 while v < Min do 14 x = b niMin c \u00d7 (v \u2212 1) + 1; 15 y = b niMin c \u00d7v ; 16 Tv \u2190 (pi,x , ...,pi,y ); 17 v + +; 18 end 19 Tv \u2190 (pi,y+1, ...,pi,ni ); 20 end 21 end 22 return Si ;\nAlgorithm 1: Constructing Variable-Length Post Series\nIn Eq.(2), the context vector xt is a dynamic representation of the relevant part of the social post input at time t . To calculate xt , we introduce an a ention weight at [i], i = 1, . . . ,K , corresponding to the feature extracted at di erent element positions in a tf-idf matrix dt . Speci cally, at each time stamp t , our model predicts at+1, a so max over K positions, and yt , a so max over the binary class of rumors and non-rumors with an additional hidden layer with si\u0434moid(\u00b7) activations (see Fig.3 (c)). e location so max [25] is thus, applied over the hidden states of the last LSTM layer to calculate at+1, the a ention weight for the next input matrix dt+1:\nat+1[i] = P(Lt+1 = i |ht ) = eWi >ht\u2211K j=1 e W >j ht i \u2208 1, ...,K , (3)\nwhere at+1[i] is the a ention probability for the i-th element (word index) at time step t + 1,Wi is the weight matrix allocated to the i-th element, and Lt+1 is a random variable which represents the word index and takes 1-of-K values. e a ention vector at+1 is a probability distribution, representing the importance a ached to each word in the input matrix dt+1. Our model is optimized to assign higher focus to words that are believed to be distinct in learning rumor/non-rumor representations. A er calculating these probabilities, the so deterministic attention mechanism [2] computes the expected value of the input at the next time step xt+1 by taking expectation over the word matrix at di erent positions:\nxt+1 = EP (Lt+1 |ht )[dt+1] = K\u2211 i=1 at+1[i]dt+1[i], (4)\nwhere dt+1 is the input matrix at time step t + 1 and dt+1[i] is the feature vector of the i-th position in the matrix dt+1. us, Eq.(4) formulates a deterministic a ention model by computing a so a ention weighted word vector \u2211 i at+1[i]dt+1[i]. is corresponds to feeding a so -a-weighted context into the system, whilst the whole model is smooth and di erential under the deterministic a ention, and thus learning end-to-end is trivial by using standard back-propagation.\nWe remark that a ention models can be classi ed into so a ention and hard a ention models. So a ention models are shown to be deterministic and can be trained by using back-propagation whereas hard a ention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45]. If we use hard a ention models, we should sample Lt from a so max distribution of Eq.(3). e input xt+1 would then be the feature at the sampled location instead of taking expectation over all the elements in dt+1. Apparently, hard a ention solutions are not differentiable and have to resort to some sampling, and thus we deploy so a ention in our model."}, {"heading": "4.4 Loss Function and Model Training", "text": "In model training, we employ cross-entropy loss coupled with the doubly stochastic regularization [45] that encourages the model to pay a ention to every element of the input word matrix. is is to impose an additional constraint over the location so max, so that\u2211\u03c4 t=1 at,i \u2248 1. e loss function is de ned as follows:\nL = \u2212 \u03c4\u2211 t=1 C\u2211 i=1 yt,i log y\u0302t,i + \u03bb K\u2211 i=1 (1 \u2212 \u03c4\u2211 t=1 at,i )2 + \u03b3\u03d52, (5)\nwhere yt is the one hot label vector, y\u0302t is the vector of binary class probabilities at time stamp t , \u03c4 is the total number of time stamps, C = 2 is the number of output classes (rumors or non-rumors), \u03bb is the a ention penalty coe cient, \u03b3 is the weight decay coe cient, and \u03d5 represents all the model parameters.\ne cell state and the hidden state for LSTM are initialized using the input tf-idf matrices for faster convergence:\nc0 = fc ( 1 \u03c4 \u03c4\u2211 t=1 ( 1 K K\u2211 i=1 dt [i] )) ,\nh0 = fh ( 1 \u03c4 \u03c4\u2211 t=1 ( 1 K K\u2211 i=1 dt [i] )) ,\n(6)\nwhere fc and fh are two multi-layer perceptrons, and \u03c4 is the number of time stamps in the model. ese values are used to compute the rst location so max a1 which determines the initial input x1."}, {"heading": "5 EXPERIMENTS", "text": "is section reports how we evaluate the performance of our proposed methodology using real-world data collected from two different social media platforms. We rst describe the construction datasets, and then perform self-evaluation to determine optimal parameters. Finally, we assess the e ectiveness and e ciency of our model, CallAtRumors, by comparing with state-of-the-art methods.\nWe use two public datasets published by [17]. e datasets are collected from Twi er2 and Sina Weibo3 respectively. Both of the datasets are organised at event-level, in which the posts related to the same events are aggregated, and each event is labeled to 1 for rumor and 0 for non-rumor. In the following, we describe how the two datasets are originally constructed and how we expand them:\n\u2022 In the Twi er dataset, 498 rumors are collected using the keywords extracted from veri ed fake news published on Snopes4, a real-time rumor debunking website. It also contains 494 normal events from Snopes and two public datasets [4, 12]. For each event, the keywords are extracted and manually re ned until the composed queries can have precise Twi er search results [17]. All labelled events and related Tweet IDs are published by the authors, however some Tweets are no longer available when we crawled those Tweets, causing a 10% shrink on the scale of data compared with the original Twi er dataset. \u2022 e Weibo dataset contains 2,313 rumors and 2,351 nonrumors. e polarity of all events are veri ed on Sina Community Management Center5. en the keywords are manually summarized and modi ed for comprehensive post search for data collection using Weibo API.\nIn addition, to balance the ration of rumors and non-romors, we follow the criteria from [17] to manually gather 4 non-rumors from Twi er and 38 rumors from Weibo to achieve a 1:1 ratio of rumors to non-rumors. e data collection procedure and our dataset structure are shown in Figure 46.\nTable 1 gives statistical details of the two datasets. We observe that more than 80% of the users tend to repost the original news with very short comments to re ect their a itudes towards those news. As a consequence, the contents of the posts related to one event are mostly duplicate, triggering scarcity when extracting distinctive textual pa erns within overlapping context. However, by implementing textual a ention mechanism, CallAtRumors is able to lay more emphasis on discriminative words, and can guarantee high performance in such case."}, {"heading": "5.2 Self Evaluations", "text": "e model is implemented by using eano7. All parameters are set using cross-validation. To generate the input variable-length post series, we set the amount of posts N for each time step as 2www.twi er.com 3www.weibo.com 4www.snopes.com 5h p://service.account.weibo.com 6 e webpage in this gure is downloaded from h p://www.snopes.com/hillaryclinton-accidentally-gave-isis-400-million/ 7h p://deeplearning.net/so ware/theano/\n50 and the minimum post series length Min as 5. We selected K=10,000 top words for the construction tf-idf matrices. Apart from lowercasing, we do not apply any other special preprocessing like stemming [2]. e recurrent neural network with a ention mechanism can automatically learn to ignore those unimportant or irrelevant expressions in the training procedure.\nFor a hold-out dataset occupying 15% of the events in each dataset, a self evaluation is performed to optimize the number of LSTM layers by varying the number of layers from 2 to 6. Results are shown in Figure 5. us, we apply a three-layer LSTM model with descending numbers of hidden states of 1024, 512 and 64 respectively. e learning rate is set as 0.45 and we apply a dropout [27] of 0.3 at all non-recurrent connections. For a ention penalty coe cient, we set \u03bb to be 1.5, and the weight decay is set to be 10\u22125. Our model is trained by measuring the derivative of the loss through back-propagation [5] algorithm, namely the Adam optimization algorithm [11]. We iterate the whole training procedure until the loss value converges."}, {"heading": "5.3 Settings and Baselines", "text": "We evaluate the e ectiveness and e ciency of CallAtRumors by comparing with the following state-of-the-art approaches in terms of precision, recall and F-measure.\n\u2022 DT-Rank [49]: is is a decision-tree based ranking model, and is able to identify trending rumors by recasting the problem as nding entire clusters of posts whose topic is a disputed factual claim. We implement their enquiry phrases and features to make it comparable to our method. \u2022 SVM-TS [18]: is is a SVM (support vector machine) model that uses time-series structures to capture the variation of social context features. SVM-TS can capture the temporal characteristics of these features based on the time series of rumors\u2019 lifecycle with time series modelling technique applied to incorporate carious social context information. We use the features provided by them from contents, users and propagation pa erns. \u2022 LK-RBF [24]: To tackle the problem of implicit data without explicit links and jointed conversations, Sampson et al. proposed two methods based on hashtags and web links to aggregate individual tweets with similar keywords from di erent threads into a conversation. We choose the link-based approach and combine it with the RBF (Radial Basis Function) kernel as a supervised classi er because it achieved the best performance in their experiments. \u2022 ML-GRU [17]: is method utilizes recurrent neural networks to automatically discover deep data representations for e cient rumor detection. It also allows for early rumor detection with e ciency. Following the se ings in their work, we choose the multi-layer GRU (gated recurrent unit) as baseline which shows the best result in the e ectiveness and earliness test. \u2022 CERT [39]: is is a cross-topic emerging rumor detection model which can jointly cluster data, select features and train classi ers by using the abundant labeled data from prior rumors to facilitate the detection of an emerging rumor. CERT is capable of extracting useful pa erns in the case of data scarcity. Since CERT requires Tweet instances\ninstead of event-level data, we use the tf-idf feature vector of the all the Tweets in one event to construct the feature matrix as required.\nWe hold out 15% of the events in each dataset for cross-validation, and split the rest of data with a ratio of 3:2 for training and test respectively. In particular, we keep the ratio between rumor events and normal events in both training and test set as 1:1. In the test on the e ectiveness of CallAtRumors, all posts within each event are used during training and evaluation. In the study of e ciency, we take di erent ratios of the posts starting from the rst post within all events, ranging from 10% to 80% in order to test how early CallAtRumors can detect rumors successfully."}, {"heading": "5.4 E ectiveness Validation", "text": "Table 2 and Table 3 shows the performance of all approaches on Twi er dataset and Weibo dataset respectively. DT-Rank cannot e ectively distinguish rumor from normal events when facing\ndatasets with duplication in contents and scarcity in textual features. LK-RBF and SVM-TS achieve be er results, indicating the ability of feature engineering to help classi ers detect rumors be er. However, both LK-RBF and SVM-TS show the lack of adequate recall which represents how sensitive the models are towards rumors. Since CERT can jointly select discriminative features and train the topic-independent classi er with selected features [39], it achieves be er results than the former three approaches in our datasets. e ML-GRU is competitive in both precision and recall due to its capability of processing sequential data and learning hidden states from raw inputs.\nOn the Twi er dataset, CallAtRumors outperforms competitors by achieving the precision, recall and F-measure of 88.63%, 85.71% and 0.8694 respectively. e same result can be seen on the Weibo dataset, where CallAtRumors achieves the precision, recall and F-measure of 87.10%, 86.34% and 0.8672 respectively. Figure 6 illustrates the intermediate a ention results on di erent words within a detected rumor event. e e ectiveness validation proves the a ect of a ention mechanism in making LSTM units sensitive to distinctive words and tokens by associating more importance to certain locations in every feature matrix against other ones."}, {"heading": "5.5 More Comparison with the State-of-the-art:", "text": "CERT [39]\nTo demonstrate how the conditions of datasets a ect the performance of rumor detection, we compare the performance of CallAtRumors with CERT using di erent datasets. To reproduce the same experimental conditions as [39], we have also organized a sample dataset using the criteria described in the work. We use queries generated from 220 reported rumors on Snopes and regular expressions belonging to the same topics to crawl 7,580 Tweets and manually labeled each Tweet by reading the content and referring\nto the Snopes article. At last we result in a sample dataset contain-\ning 1,193 rumor Tweets and 6,387 non-rumor Tweets, which also\nhas a similar ratio of rumors to non-rumors as the dataset in [39]. Table 4 shows the di erent results when CallAtRumors and CERT are applied to this sample dataset and our Twi er dataset. e results further explains our model\u2019s capability of capturing valuable pa erns within our large-scale duplicated datasets by applying a ention to more representative words."}, {"heading": "5.6 Earliness Analysis", "text": "In this experiment, we study the property of our approach in its earliness. To have fair comparison, we allow exiting rumor detection methods to be trained on rumors that are for evaluation. rough incrementally adding training data in the chronological order, we are able to estimate the time that our method can detect emerging rumors. e results on earliness are shown in Fig 7. At the early stage with 10% to 60% training data, CallAtRomors outperforms four comparative methods by a noticeable margin. In particular, compared with the most relevant method of ML-GRU, as the data proportion ranging from 10% to 20%, CallAtRumors outperforms ML-GRU by 5% on precision and 4% on recall on both Twi er and Weibo datasets. e result shows that a ention mechanism is more\ne ective in early stage detection by focusing on the most distinct features in advance. With more data applied into test, all methods are approaching their best performance. For Twi er dataset and Weibo Dataset with averagely 80% duplicate contents in each event, our method starts with 74.02% and 71.73% in precision while 68.75% and 70.34% in recall, which means an average time lag of 20.47 hours a er the emerge of one event. is result is promising because the average report time over the rumors given by Snopes and Sina Community Management Center is 54 hours and 72 hours respectively [17], and we can save much manual e ort with the help of our deep a ention based early rumor detection technique."}, {"heading": "6 CONCLUSION", "text": "Rumor detection on social media is time-sensitive because it is hard to eliminate the vicious impact in its late period of di usion as rumors can spread quickly and broadly. In this paper, we introduce CallAtRumors, a novel recurrent neural network model based on so a ention mechanism to automatically carry out early rumor detection by learning latent representations from the sequential social posts. We conducted experiments with ve state-of-the-art rumor detection methods to illustrate that CallAtRumors is sensitive to distinguishable words, thus outperforming the competitors even when textual feature is sparse at the beginning stage of a rumor. In addition, we demonstrate the capability of our model to handle duplicate data with a further comparison. In our future work, it would be appealing to investigate the possibility to combine more complexed feature with our deep a ention model. For example, we\ncan model the propagation pa ern of rumors as sequential inputs for RNNs to improve the detection accuracy. e future work may investigate the e ciency issue using hashing techniques [32, 41] over multi-level feature spaces [33, 35, 38, 42, 43]."}], "references": [{"title": "Learning wake-sleep recurrent a\u008aention models", "author": ["Jimmy Ba", "Roger Grosse", "Ruslan Salakhutdinov", "Brendan Frey"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning long-term dependencies with gradient decent is di\u0081cult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Information credibility on twi\u008aer", "author": ["Carlos Castillo", "Marcelo Mendoza", "Barbara Poblete"], "venue": "In Proceedings of the 20th international conference on World wide web", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bo\u008aou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Gre", "Rupesh K Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE transactions on neural networks and learning systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural computation", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Social spammer detection with sentiment information", "author": ["Xia Hu", "Jiliang Tang", "Huiji Gao", "Huan Liu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Prominent features of rumor propagation in online social media", "author": ["Sejeong Kwon", "Meeyoung Cha", "Kyomin Jung", "Wei Chen", "Yajun Wang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Mining of massive datasets", "author": ["Jure Leskovec", "Anand Rajaraman", "Je\u0082rey David Ullman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Presenting diverse location views with real-time near-duplicate photo elimination", "author": ["Jiajun Liu", "Zi Huang", "Hong Cheng", "Yueguo Chen", "Heng Tao Shen", "Yanchun Zhang"], "venue": "In ICDE", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Real-time rumor debunking on twi\u008aer", "author": ["Xiaomo Liu", "Armineh Nourbakhsh", "\u008banzhi Li", "Rui Fang", "Sameena Shah"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Detecting rumors from microblogs with recurrent neural networks", "author": ["Jing Ma", "Wei Gao", "Prasenjit Mitra", "Sejeong Kwon", "Bernard J Jansen", "Kam-Fai Wong", "Meeyoung Cha"], "venue": "Proceedings of IJCAI", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Detect rumors using time series of social context information on microblogging websites", "author": ["Jing Ma", "Wei Gao", "Zhongyu Wei", "Yueming Lu", "Kam-Fai Wong"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Recurrent models of visual a\u008aention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Collective opinion spam detection: Bridging review networks and metadata", "author": ["Shebuti Rayana", "Leman Akoglu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Collective opinion spam detection using active inference", "author": ["Shebuti Rayana", "Leman Akoglu"], "venue": "In Proceedings of the 2016 SIAM International Conference on Data Mining", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Reasoning about entailment with neural a\u008aention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenste\u008ae", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geo\u0082rey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling 5,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Leveraging the Implicit Structure within Social Media for Emergent Rumor Detection", "author": ["Justin Sampson", "Fred Morsta\u008aer", "Liang Wu", "Huan Liu"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Action recognition using visual a\u008aention", "author": ["Shikhar Sharma", "Ryan Kiros", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.04119", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Multiple feature hashing for real-time large scale near-duplicate video retrieval", "author": ["Jingkuan Song", "Yi Yang", "Zi Huang", "Heng Tao Shen", "Richang Hong"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geo\u0082rey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "E\u0082ective Multi-\u008bery Expansions: Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "In ACM Multimedia", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "E\u0082ective Multi-\u008bery Expansions: Collaborative Deep Networks for Robust Landmark Retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "IEEE Trans. Image Processing 26,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "LBMCH: Learning Bridging Mapping for Cross-modal Hashing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "In ACM SIGIR", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Robust Subspace Clustering for Multi-View Data by Exploiting Correlation Consensus", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang", "Xiaodi Huang"], "venue": "IEEE Trans. Image Processing 24,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Towards metric fusion on multi-view data: a cross-view based graph random walk approach", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang"], "venue": "In ACM CIKM", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Shi\u0089ing Hypergraphs by Probabilistic Voting", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang", "Lin Wu"], "venue": "In PAKDD", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Unsupervised Metric Fusion Over Multiview Data by Graph Random Walk-Based Cross-View Di\u0082usion", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Xiang Zhao"], "venue": "IEEE Trans. Neural Netw. Learning Syst 28,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2017}, {"title": "False rumors detection on sina weibo by propagation structures", "author": ["Ke Wu", "Song Yang", "Kenny Q Zhu"], "venue": "In Data Engineering (ICDE),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Geo-location estimation from two shadow trajectories", "author": ["Lin Wu", "Xiaochun Cao"], "venue": "In CVPR", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Gleaning Wisdom from the Past: Early Detection of Emerging Rumors in Social Media", "author": ["Liang Wu", "Jundong Li", "Xia Hu", "Huan Liu"], "venue": "In SDM", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Deep linear discriminant analysis on \u0080sher networks: A hybrid architecture for person reidenti\u0080cation", "author": ["Lin Wu", "Chunhua Shen", "Anton van den Hengel"], "venue": "Pa\u0088ern Recognition", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Robust hashing for multi-view data: Jointly learning low-rank kernelized similarity consensus and hash functions", "author": ["Lin Wu", "Yang Wang"], "venue": "Image Vision Comput", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2017}, {"title": "Exploiting A\u008aribute Correlations: A Novel Trace Lasso based Weakly Supervised Dictionary Learning Method", "author": ["Lin Wu", "Yang Wang", "Shirui Pan"], "venue": "IEEE Trans. Cybernetics", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "E\u0081cient image and tag co-ranking: a bregman divergence optimization method", "author": ["Lin Wu", "Yang Wang", "John Shepherd"], "venue": "In ACM Multimedia", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Answer Selection in Community \u008bestion Answering via A\u008aentive Neural Networks", "author": ["Yang Xiang", "Qingcai Chen", "Xiaolong Wang", "Yang Qin"], "venue": "IEEE Signal Processing Le\u0088ers 24,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2017}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Hierarchical a\u008aention networks for document classi\u0080cation", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "10 bits of surprise: Detecting malicious users with minimum information", "author": ["Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Enquiring minds: Early detection of rumors in social media from enquiry posts", "author": ["Zhe Zhao", "Paul Resnick", "Qiaozhu Mei"], "venue": "In Proceedings of the 24th International Conference on World Wide Web", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "E\u0082ective and E\u0081cient Global Context Veri\u0080cation for Image Copy Detection", "author": ["Zhili Zhou", "Yunlong Wang", "Q.M. Jonathan Wu", "Ching-Nung Yang", "Xingming Sun"], "venue": "IEEE Transactions on Information Forensics and Security 12,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2017}, {"title": "Brand-Related Twi\u008aer Sentiment Analysis Using Feature Engineering and the Dynamic Architecture for Arti\u0080cial Neural Networks", "author": ["David Zimbra", "M Ghiassi", "Sean Lee"], "venue": "In System Sciences (HICSS),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}], "referenceMentions": [{"referenceID": 48, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 28, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 44, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 83, "endOffset": 95}, {"referenceID": 10, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 13, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 15, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 31, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 33, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 34, "context": "\u008cey commonly cra\u0089 features manually from the content, sentiment [51], user pro\u0080les [30, 31, 47], and di\u0082usion pa\u008aerns of the posts [12, 16, 18, 34, 36, 37].", "startOffset": 131, "endOffset": 155}, {"referenceID": 17, "context": "Embedding social graphs into a classi\u0080cation model also helps distinguish malicious user comments from normal ones [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Embedding social graphs into a classi\u0080cation model also helps distinguish malicious user comments from normal ones [20, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 46, "context": "1 (b), users\u2019 posts exhibit high duplication in their textual phrases due to the repeated forwarding, reviews, and/or inquiry behavior [49].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "One exception is [17] where Ma et al.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 23, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 47, "context": "Although some studies on duplication detection are available and e\u0082ective in di\u0082erent tasks [15, 26, 50], these approaches are not applicable in our case where the duplication cannot be determined beforehand but is rather varied across post series over time.", "startOffset": 92, "endOffset": 104}, {"referenceID": 0, "context": "Our framework is premised on the RNNs which are proved to be e\u0082ective in recent machine learning tasks [1, 7] in handling sequential data.", "startOffset": 103, "endOffset": 109}, {"referenceID": 5, "context": "Our framework is premised on the RNNs which are proved to be e\u0082ective in recent machine learning tasks [1, 7] in handling sequential data.", "startOffset": 103, "endOffset": 109}, {"referenceID": 3, "context": "1 Early Rumor Detection \u008ce problem of rumor detection [4] can be cast as binary classi\u0080cation tasks.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "\u0080rst conducted a study to analyze the sentiment di\u0082erences between spammers and normal users and then presented an optimization formulation that incorporates sentiment information into a novel social spammer detection framework [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 34, "context": "through utilizing a message propagation tree where each node represents a text message to classify whether the root of the tree is a rumor or not [37].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "In [18], a dynamic time series structure was proposed to capture the temporal features based on the time series context information generated in every rumor\u2019s life-cycle.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Early rumor detection is to detect viral rumors in their formative stages in order to take early action [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 46, "context": "In [49], some very rare", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "proposed a sparse learning method to automatically select discriminative features as well as train the classi\u0080er for emerging rumors [39].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "As those methods neglect the temporal trait of social media data, a time-series based feature structure[18] is introduced to seize context variation over time.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "[17], utilizing sequential data to spontaneously capture temporal textual characteristics of rumor di\u0082usion which helps detecting rumor earlier with accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 25, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 43, "context": "2 Attention Mechanism As a rising technique in NLP (natural language processing) problems [22, 28, 46], Bahdanau et al.", "startOffset": 90, "endOffset": 102}, {"referenceID": 1, "context": "extended the basic encoderdecoder architecture of neural machine translation with a\u008aention mechanism to allow the model to automatically search for parts of a source sentence that are relevant to predicting a target word [2], achieving a comparable performance in the English-to-French translation task.", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "improved the a\u008aention model in [2], so their model computed an a\u008aention vector re\u0083ecting how much a\u008aention should be put over the input words and boosted the performance on large scale translation [29].", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "improved the a\u008aention model in [2], so their model computed an a\u008aention vector re\u0083ecting how much a\u008aention should be put over the input words and boosted the performance on large scale translation [29].", "startOffset": 197, "endOffset": 201}, {"referenceID": 22, "context": "applied a location so\u0089max function [25] to the hidden states of the LSTM (Long Short-Term Memory) layer, thus recognizing more valuable elements in sequential inputs for action recognition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "3 RECURRENT NEURAL NETWORKS Recurrent neural networks, or RNNs [23], are a family of feedforward neural networks for processing sequential data, such as a sequence of values x1, .", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "\u008ce gradient computation of RNNs involves performing backpropagation through time (BPTT) [23].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "In practice, a standard RNN is di\u0081cult to be trained due to the well-known vanishing or exploding gradients caused by the incapability of RNN in capturing the long-distance temporal dependencies for the gradient based optimization [3, 40].", "startOffset": 231, "endOffset": 238}, {"referenceID": 37, "context": "In practice, a standard RNN is di\u0081cult to be trained due to the well-known vanishing or exploding gradients caused by the incapability of RNN in capturing the long-distance temporal dependencies for the gradient based optimization [3, 40].", "startOffset": 231, "endOffset": 238}, {"referenceID": 5, "context": "To tackle this training di\u0081culty, an e\u0082ective solution is to includes \u201cmemory\u201d cells to store information over time, which are known as Long Short-Term Memory (LSTM) [7, 9].", "startOffset": 166, "endOffset": 172}, {"referenceID": 7, "context": "To tackle this training di\u0081culty, an e\u0082ective solution is to includes \u201cmemory\u201d cells to store information over time, which are known as Long Short-Term Memory (LSTM) [7, 9].", "startOffset": 166, "endOffset": 172}, {"referenceID": 14, "context": "Hence, we are interested in detecting rumor on an aggregate level instead of identifying each single posts [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Proved to be an e\u0082ective and lightweight textual feature, tf-idf is a numerical statistic that is intended to re\u0083ect how important a word is to a document in a collection or corpus [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 41, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 45, "context": "3 Long Short-Term Memory (LSTM) with Deterministic So\u0085 Attention Mechanism To capture the long-distance temporal dependencies among continuous time post series, we employ Long Short-Term Memory (LSTM) unit [7, 44, 48] to learn high-level discriminative representations for rumors.", "startOffset": 206, "endOffset": 217}, {"referenceID": 6, "context": "\u008ce LSTM architecture is essentially a memory cell which can maintain its state over time, and non-linear gating units can regulate the information \u0083ow into and out of the cell [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 22, "context": "\u008ce location so\u0089max [25] is thus, applied over the hidden states of the last LSTM layer to calculate at+1, the a\u008aention weight for the next input matrix dt+1:", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "A\u0089er calculating these probabilities, the so\u0085 deterministic attention mechanism [2] computes the expected value of the input at the next time step xt+1 by taking expectation over the word matrix at di\u0082erent positions:", "startOffset": 80, "endOffset": 83}, {"referenceID": 16, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 266, "endOffset": 273}, {"referenceID": 42, "context": "So\u0089 a\u008aention models are shown to be deterministic and can be trained by using back-propagation whereas hard a\u008aention models are stochastic and the training requires the REINFORCE algorithm [19] or by maximizing a variational lower bound or using importance sampling [1, 45].", "startOffset": 266, "endOffset": 273}, {"referenceID": 42, "context": "4 Loss Function and Model Training In model training, we employ cross-entropy loss coupled with the doubly stochastic regularization [45] that encourages the model to pay a\u008aention to every element of the input word matrix.", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "1 Datasets We use two public datasets published by [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "It also contains 494 normal events from Snopes and two public datasets [4, 12].", "startOffset": 71, "endOffset": 78}, {"referenceID": 10, "context": "It also contains 494 normal events from Snopes and two public datasets [4, 12].", "startOffset": 71, "endOffset": 78}, {"referenceID": 14, "context": "For each event, the keywords are extracted and manually re\u0080ned until the composed queries can have precise Twi\u008aer search results [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "In addition, to balance the ration of rumors and non-romors, we follow the criteria from [17] to manually gather 4 non-rumors from Twi\u008aer and 38 rumors from Weibo to achieve a 1:1 ratio of rumors to non-rumors.", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Apart from lowercasing, we do not apply any other special preprocessing like stemming [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 24, "context": "45 and we apply a dropout [27] of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 4, "context": "Our model is trained by measuring the derivative of the loss through back-propagation [5] algorithm, namely the Adam optimization algorithm [11].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Our model is trained by measuring the derivative of the loss through back-propagation [5] algorithm, namely the Adam optimization algorithm [11].", "startOffset": 140, "endOffset": 144}, {"referenceID": 46, "context": "\u2022 DT-Rank [49]: \u008cis is a decision-tree based ranking model, and is able to identify trending rumors by recasting the problem as \u0080nding entire clusters of posts whose topic is a disputed factual claim.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "\u2022 SVM-TS [18]: \u008cis is a SVM (support vector machine) model that uses time-series structures to capture the variation of social context features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "\u2022 LK-RBF [24]: To tackle the problem of implicit data without explicit links and jointed conversations, Sampson et al.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "\u2022 ML-GRU [17]: \u008cis method utilizes recurrent neural networks to automatically discover deep data representations for e\u0081cient rumor detection.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "\u2022 CERT [39]: \u008cis is a cross-topic emerging rumor detection model which can jointly cluster data, select features and train classi\u0080ers by using the abundant labeled data from prior rumors to facilitate the detection of an emerging rumor.", "startOffset": 7, "endOffset": 11}, {"referenceID": 36, "context": "Since CERT can jointly select discriminative features and train the topic-independent classi\u0080er with selected features [39], it achieves be\u008aer results than the former three approaches in our datasets.", "startOffset": 119, "endOffset": 123}, {"referenceID": 36, "context": "5 More Comparison with the State-of-the-art: CERT [39] To demonstrate how the conditions of datasets a\u0082ect the performance of rumor detection, we compare the performance of CallAtRumors with CERT using di\u0082erent datasets.", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "To reproduce the same experimental conditions as [39], we have also organized a sample dataset using the criteria described in the work.", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "8038 Table 4: More comparison with CERT [39] on the Sample and Twitter datasets to the Snopes article.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "At last we result in a sample dataset containing 1,193 rumor Tweets and 6,387 non-rumor Tweets, which also has a similar ratio of rumors to non-rumors as the dataset in [39].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "\u008cis result is promising because the average report time over the rumors given by Snopes and Sina Community Management Center is 54 hours and 72 hours respectively [17], and we can save much manual e\u0082ort with the help of our deep a\u008aention based early rumor detection technique.", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "\u008ce proliferation of social media in communication and information dissemination has made it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of di\u0082usion is known as early rumor detection, which refers to dealing with sequential posts regarding disputed factual claims with certain variations and highly textual duplication over time. \u008cus, identifying trending rumors demands an e\u0081cient yet \u0083exible model that is able to capture long-range dependencies among postings and produce distinct representations for the accurate early detection. However, it is a challenging task to apply conventional classi\u0080cation algorithms to rumor detection in earliness since they rely on hand-cra\u0089ed features which require intensive manual e\u0082orts in the case of large amount of posts. \u008cis paper presents a deep a\u008aention model on the basis of recurrent neural networks (RNN) to learn selectively temporal hidden representations of sequential posts for identifying rumors. \u008ce proposed model delves so\u0089-a\u008aention into the recurrence to simultaneously pool out distinct features with particular focus and produce hidden representations that capture contextual variations of relevant posts over time. Extensive experiments on real datasets collected from social media websites demonstrate that (1) the deep a\u008aention based RNN model outperforms state-of-thearts that rely on hand-cra\u0089ed features; (2) the introduction of so\u0089 a\u008aention mechanism can e\u0082ectively distill relevant parts to rumors from original posts in advance; (3) the proposed method detects rumors more quickly and accurately than competitors.", "creator": "LaTeX with hyperref package"}}}