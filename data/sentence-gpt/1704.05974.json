{"id": "1704.05974", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Cross-domain Semantic Parsing via Paraphrasing", "abstract": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pre-trained word embeddings that hurdle their direct use in neural networks, and propose standardization techniques as a remedy. On the popular Overnight dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embeddings can bring significant improvement. Our dataset also supports four domains: semantic processing, a cross-domain modeling and a pre-trained word embeddings. These two domains are now supported by a large pre-trained word embeddings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 20 Apr 2017 01:26:23 GMT  (749kb,D)", "https://arxiv.org/abs/1704.05974v1", null], ["v2", "Mon, 24 Jul 2017 19:35:17 GMT  (756kb,D)", "http://arxiv.org/abs/1704.05974v2", "12 pages, 2 figures, accepted by EMNLP2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu su", "xifeng yan"], "accepted": true, "id": "1704.05974"}, "pdf": {"name": "1704.05974.pdf", "metadata": {"source": "CRF", "title": "Cross-domain Semantic Parsing via Paraphrasing", "authors": ["Yu Su", "Xifeng Yan"], "emails": ["ysu@cs.ucsb.edu", "xyan@cs.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "Semantic parsing, which maps natural language utterances into computer-understandable logical forms, has drawn substantial attention recently as a promising direction for developing natural language interfaces to computers. Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al., 2017), and communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer,\n2013; Bisk et al., 2016). Despite the wide applications, studies on semantic parsing have mainly focused on the indomain setting, where both training and testing data are drawn from the same domain. How to build semantic parsers that can learn across domains remains an under-addressed problem. In this work, we study cross-domain semantic parsing. We model it as a domain adaptation problem (Daume\u0301 III and Marcu, 2006), where we are given some source domains and a target domain, and the core task is to adapt a semantic parser trained on the source domains to the target domain (Figure 1). The benefits are two-fold: (1) by training on the source domains, the cost of collecting training data for the target domain can be reduced, and (2) the data of source domains may provide information complementary to the data collected for the target domain, leading to better performance on the target domain.\nThis is a very challenging task. Traditional domain adaptation (Daume\u0301 III and Marcu, 2006; Blitzer et al., 2006) only concerns natural languages, while semantic parsing concerns both natural and formal languages. Different domains often involve different predicates. In Figure 1, from the source BASKETBALL domain a semantic parser can learn the semantic mapping from natural language to predicates like team and season, but in the target SOCIAL domain it needs to handle predicates like employer instead. Worse still, even for the same predicate, it is legitimate to use arbitrarily different predicate symbols, e.g., other symbols like hired by or even predicate1 can also be used for the employer predicate, reminiscent of the symbol grounding problem (Harnad, 1990). Therefore, directly transferring the mapping from natural language to predicate symbols learned from source domains to the target domain may not be much beneficial. ar X iv :1 70 4.\n05 97\n4v 2\n[ cs\n.C L\n] 2\n4 Ju\nl 2 01\n7\nInspired by the recent success of paraphrasing based semantic parsing (Berant and Liang, 2014; Wang et al., 2015), we propose to use natural language as an intermediate representation for crossdomain semantic parsing. As shown in Figure 1, logical forms are converted into canonical utterances in natural language, and semantic parsing is reduced to paraphrasing. It is the knowledge of paraphrasing, at lexical, syntactic, and semantic levels, that will be transferred across domains.\nStill, adapting a paraphrase model to a new domain is a challenging and under-addressed problem. To give some idea of the difficulty, for each of the eight domains in the popular OVERNIGHT (Wang et al., 2015) dataset, 30% to 55% of the words never occur in any of the other domains, a similar problem observed in domain adaptation for machine translation (Daume\u0301 III and Jagarlamudi, 2011). The paraphrase model therefore can get little knowledge for a substantial portion of the target domain from the source domains. We introduce pre-trained word embeddings such as WORD2VEC (Mikolov et al., 2013) to combat the vocabulary variety across domains. Based on recent studies on neural network initialization, we conduct a statistical analysis of pre-trained word embeddings and discover two problems that may hinder their direct use in neural networks: small micro variance, which hurts optimization, and large macro variance, which hurts generalization. We propose to standardize pre-trained word embeddings, and show its advantages both analytically and experimentally.\nOn the OVERNIGHT dataset, we show that crossdomain training under the proposed framework can significantly improve model performance. We\nalso show that, compared with directly using pretrained word embeddings or normalization as in previous work, the proposed standardization technique can lead to about 10% absolute improvement in accuracy."}, {"heading": "2 Cross-domain Semantic Parsing", "text": ""}, {"heading": "2.1 Problem Definition", "text": "Unless otherwise stated, we will use u to denote input utterance, c for canonical utterance, and z for logical form. We denote U as the set of all possible utterances. For a domain, suppose Z is the set of logical forms, a semantic parser is a mapping f : U \u2192 Z that maps every input utterance to a logical form (a null logical form can be included in Z to reject out-of-domain utterances).\nIn cross-domain semantic parsing, we assume there are a set of K source domains {Zi}Ki=1, each with a set of training examples {(uij , zij)} Ni j=1. It is in principle advantageous to model the source domains separately (Daume\u0301 III and Marcu, 2006), which retains the possibility of separating domaingeneral information from domain-specific information, and only transferring the former to the target domain. For simplicity, here we merge the source domains into a single domainZs with training data {(ui, zi)}Nsi=1. The task is to learn a semantic parser f : U \u2192 Zt for a target domain Zt, for which we have a set of training examples {(ui, zi)}Nti=1. Some characteristics can be summarized as follows:\n\u2022 Zt and Zs can be totally disjoint.\n\u2022 The input utterance distribution of the source and the target domains can be independent and differ remarkably.\n\u2022 Typically Nt Ns.\nIn the most general and challenging case, Zt and Zs can be defined using different formal languages. Because of the lack of relevant datasets, here we restrain ourselves to the case where Zt and Zs are defined using the same formal language, e.g., \u03bb-DCS (Liang, 2013) as in the OVERNIGHT dataset."}, {"heading": "2.2 Framework", "text": "Our framework follows the research line of semantic parsing via paraphrasing (Berant and Liang, 2014; Wang et al., 2015). While previous work focuses on the in-domain setting, we discuss its applicability and advantages in the cross-domain setting, and develop techniques to address the emerging challenges in the new setting.\nCanonical utterance. We assume a one-to-one mapping g : Z \u2192 C, where C \u2282 U is the set of canonical utterances. In other words, every logical form will be converted into a unique canonical utterance deterministically (Figure 1). Previous work (Wang et al., 2015) has demonstrated how to design such a mapping, where a domaingeneral grammar and a domain-specific lexicon are constructed to automatically convert every logical form to a canonical utterance. In this work, we assume the mapping is given1, and focus on the subsequent paraphrasing and domain adaptation problems.\nThis design choice is worth some discussion. The grammar, or at least the lexicon for mapping predicates to natural language, needs to be provided by domain administrators. This indeed brings an additional cost, but we believe it is reasonable and even necessary for three reasons: (1) Only domain administrators know the predicate semantics the best, so it has to be them to reveal that by grounding the predicates to natural language (the symbol grounding problem (Harnad, 1990)). (2) Otherwise, predicate semantics can only be learned from supervised training data of each domain, bringing a significant cost on data collection. (3) Canonical utterances are understandable by average users, and thus can also be used for training data collection via crowdsourcing (Wang et al., 2015; Su et al., 2016), which can amortize the cost.\n1In the experiments we use the provided canonical utterances of the OVERNIGHT dataset.\nTake comparatives as an example. In logical forms, comparatives can be legitimately defined using arbitrarily different predicates in different domains, e.g., <, smallerInSize, or even predicates with an ambiguous surface form, like lt. When converting logical form to canonical utterance, however, domain administrators have to choose common natural language expressions like \u201cless than\u201d and \u201dsmaller\u201d, providing a shared ground for cross-domain semantic parsing.\nParaphrase model. In the previous work based on paraphrasing (Berant and Liang, 2014; Wang et al., 2015), semantic parsers are implemented as log-linear models with hand-engineered domainspecific features (including paraphrase features). Considering the recent success of representation learning for domain adaptation (Glorot et al., 2011; Chen et al., 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), which can be trained end to end without feature engineering. We show that it outperforms the previous loglinear models by a large margin in the in-domain setting, and can easily adapt to new domains.\nPre-trained word embeddings. An advantage of reducing semantic parsing to paraphrasing is that external language resources become easier to incorporate. Observing the vocabulary variety across domains, we introduce pre-trained word embeddings to facilitate domain adaptation. For the example in Figure 1, the paraphrase model may have learned the mapping from \u201cplay for\u201d to \u201cwhose team is\u201d in a source domain. By acquiring word similarities (\u201cplay\u201d-\u201cwork\u201d and \u201cteam\u201d\u201cemployer\u201d) from pre-trained word embeddings, it can establish the mapping from \u201cwork for\u201d to \u201cwhose employer is\u201d in the target domain, even without in-domain training data. We analyze statistical characteristics of the pre-trained word embeddings, and propose standardization techniques to remedy some undesired characteristics that may bring a negative effect to neural models.\nDomain adaptation protocol. We will use the following protocol: (1) train a paraphrase model using the data of the source domain, (2) use the learned parameters to initialize a model in the target domain, and (3) fine-tune the model using the training data of the target domain."}, {"heading": "2.3 Prior Work", "text": "While most studies on semantic parsing so far have focused on the in-domain setting, there are a number of studies of particular relevance to this work. In the recent efforts of scaling semantic parsing to large knowledge bases like Freebase (Bollacker et al., 2008), researchers have explored several ways to infer the semantics of knowledge base relations unseen in training, which are often based on at least one (often both) of the following assumptions: (1) Distant supervision. Freebase entities can be linked to external text corpora, and serve as anchors for seeking semantics of Freebase relations from text. For example, Cai and Alexander (2013), among others (Berant et al., 2013; Xu et al., 2016), use sentences from Wikipedia that contain any entity pair of a Freebase relation as the support set of the relation. (2) Self-explaining predicate symbols. Most Freebase relations are described using a carefully chosen symbol (surface form), e.g., place of birth, which provides strong cues for their semantics. For example, Yih et al. (2015) directly compute the similarity of input utterance and the surface form of Freebase relations via a convolutional neural network. Kwiatkowski et al. (2013) also extract lexical features from input utterance and the surface form of entities and relations. They have actually evaluated their model on Freebase subdomains not covered in training, and have shown impressive results. However, in the more general setting of cross-domain semantic parsing, we may have neither of these luxuries. Distant supervision may not be available (e.g., IoT devices involving no entities but actions), and predicate symbols may not provide enough cues (e.g., predicate1). In this case, seeking additional inputs from domain administrators is probably necessary.\nIn parallel of this work, Herzig and Berant (2017) have explored another direction of semantic parsing with multiple domains, where they use all the domains to train a single semantic parser, and attach a domain-specific encoding to the training data of each domain to help the semantic parser differentiate between domains. We pursue a different direction: we train a semantic parser on some source domains and adapt it to the target domain. Another difference is that their work directly maps utterances to logical forms, while ours is based on paraphrasing.\nCross-domain semantic parsing can be seen as\na way to reduce the cost of training data collection, which resonates with the recent trend in semantic parsing. Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al. (2016) manage to employ crowd workers with no linguistic expertise for data collection. Jia and Liang (2016) propose an interesting form of data augmentation. They learn a grammar from existing training data, and generate new examples from the grammar by recombining segments from different examples.\nWe use natural language as an intermediate representation to transfer knowledge across domains, and assume the mapping from the intermediate representation (canonical utterance) to logical form can be done deterministically. Several other intermediate representations have also been used, such as combinatory categorial grammar (Kwiatkowski et al., 2013; Reddy et al., 2014), dependency tree (Reddy et al., 2016, 2017), and semantic role structure (Goldwasser and Roth, 2013). But their main aim is to better represent input utterances with a richer structure. A separate ontology matching step is needed to map the intermediate representation to logical form, which requires domain-dependent training.\nA number of other related studies have also used paraphrasing. For example, Fader et al. (2013) leverage question paraphrases to for question answering, while Narayan et al. (2016) generate paraphrases as a way of data augmentation.\nCross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daume\u0301 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011). For example, Chelba and Acero (2004) use parameters trained in the source domain as prior to regularize parameters in the target domain. The feature augmentation technique from Daume\u0301 III (2009) can be very helpful when there are multiple source domains. We expect to see many of these ideas to be applied in the future."}, {"heading": "3 Paraphrase Model", "text": "In this section we propose a paraphrase model based on the Seq2Seq model (Sutskever et al., 2014) with soft attention. Similar models have been used in semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016) but for directly mapping utterances to logical forms. We demon-\nstrate that it can also be used as a paraphrase model for semantic parsing. Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Schu\u0308tze, 2015), but it is not the focus of this work to compare all the alternatives.\nFor an input utterance u = (u1, u2, . . . , um) and an output canonical utterance c = (c1, c2, . . . , cn), the model estimates the conditional probability p(c|u) = \u220fn j=1 p(cj |u, c1:j\u22121). The tokens are first converted into vectors via a word embedding layer \u03c6. The initialization of the word embedding layer is critical for domain adaptation, which we will further discuss in Section 4.\nThe encoder, which is implemented as a bi-directional recurrent neural network (RNN), first encodes u into a sequence of state vectors (h1, h2, . . . , hm). The state vectors of the forward RNN and the backward RNN are respectively computed as:\n\u2212\u2192 h i = GRUfw(\u03c6(ui), \u2212\u2192 h i\u22121) \u2190\u2212 h i = GRUbw(\u03c6(ui), \u2190\u2212 h i+1)\nwhere gated recurrent unit (GRU) as defined in (Cho et al., 2014) is used as the recurrence. We then concatenate the forward and backward state vectors, hi = [ \u2212\u2192 h i, \u2190\u2212 h i], i = 1, . . . ,m.\nWe use an attentive RNN as the decoder, which will generate the output tokens one at a time. We denote the state vectors of the decoder RNN as (d1, d2, . . . , dn). The attention takes a form similar to (Vinyals et al., 2015). For the decoding step j, the decoder is defined as follows:\nd0 = tanh(W0[ \u2212\u2192 h m, \u2190\u2212 h 1])\nuji = v T tanh(W1hi +W2dj) \u03b1ji = uji\u2211m\ni\u2032=1 uji\u2032\nh\u2032j = m\u2211 i=1 \u03b1jihi\ndj+1 = GRU([\u03c6(cj), h \u2032 j ], dj)\np(cj |u, c1:j\u22121) \u221d exp(U [dj , h\u2032j ])\nwhere W0,W1,W2, v and U are model parameters. The decoder first calculates normalized attention weights \u03b1ji over encoder states, and get a summary state h\u2032j . The summary state is then used to calculate the next decoder state dj+1 and the output probability distribution p(cj |u, c1:j\u22121).\nTraining. Given a set of training examples {(ui, ci)}Ni=1, we minimize the cross-entropy loss \u2212 1N \u2211N i=1 log p(ci|ui), which maximizes the log probability of the correct canonical utterances. We apply dropout (Hinton et al., 2012) on both input and output of the GRU cells to prevent overfitting.\nTesting. Given a domain {Z, C}, there are two ways to use a trained model. One is to use it to generate the most likely output utterance u\u2032 given an input utterance u (Sutskever et al., 2014),\nu\u2032 = argmax u\u2032 \u2208U p(u\u2032|u).\nIn this case u\u2032 can be any utterance permissable by the output vocabulary, and may not necessarily be a legitimate canonical utterance in C. This is more suitable for large domains with a lot of logical forms, like Freebase. An alternative way is to use the model to rank the legitimate canonical utterances (Kannan et al., 2016):\nc = argmax c\u2208C\np(c|u),\nwhich is more suitable for small domains having a limited number of logical forms, like the ones in the OVERNIGHT dataset. We will adopt the second strategy. It is also very challenging; random guessing leads to almost no success. It is also possible to first find a smaller set of candidates to rank via beam search (Berant et al., 2013; Wang et al., 2015)."}, {"heading": "4 Pre-trained Word Embedding for Domain Adaptation", "text": "Pre-trained word embeddings like WORD2VEC have a great potential to combat the vocabulary variety across domains. For example, we can use pre-trained WORD2VEC vectors to initialize the word embedding layer of the source domain, with the hope that the other parameters in the model will co-adapt with the word vectors during training in the source domain, and generalize better to the out-of-vocabulary words (but covered by WORD2VEC) in the target domain. However, deep neural networks are very sensitive to initialization (Erhan et al., 2010), and a statistical analysis of the pre-trained WORD2VEC vectors reveals some characteristics that may not be desired for initializing deep neural networks. In this section we present the analysis and propose a standardization technique to remedy the undesired characteristics.\nAnalysis. Our analysis will be based on the 300- dimensional WORD2VEC vectors trained on the 100B-word Google News corpus2. It contains 3 million words, leading to a 3M-by-300 word embedding matrix. The \u201crule of thumb\u201d to randomly initialize word embedding in neural networks is to sample from a uniform or Gaussian distribution with unit variance, which works well for a wide range of neural network models in general. We therefore use it as a reference to compare different word embedding initialization strategies. Given a word embedding matrix, we compute the L2 norm of each row and report the mean and the standard deviation. Similarly, we also report the variance of each row (denoted as micro variance), which indicates how far the numbers in the row spread out, and pair-wise cosine similarity, which indicates the word similarity captured by WORD2VEC.\nThe statistics of the word embedding matrix with different initialization strategies are shown in Table 1. Compared with random initialization, two characteristics of the WORD2VEC vectors stand out: (1) Small micro variance. Both the L2 norm and the micro variance of the WORD2VEC vectors are much smaller. (2) Large macro variance. The variance of different WORD2VEC vectors, reflected by the standard deviation of L2 norm, is much larger (e.g., the maximum and the minimum L2 norm are 21.1 and 0.015, respectively). Small micro variance can make the variance of neuron activations starts off too small3, implying a poor starting point in the parameter space. On the other hand, because of the magnitude difference, large macro variance may make a model hard to gener-\n2https://code.google.com/archive/p/ word2vec/\n3Under some conditions, including using Xavier initialization (also introduced in that paper and now widely used) for weights, Glorot and Bengio (2010) have shown that the activation variances in a feedforward neural network will be roughly the same as the input variances (word embedding here) at the beginning of training.\nalize to words unseen in training.\nStandardization. Based on the above analysis, we propose to do unit variance standardization (standardization for short) on pre-trained word embeddings. There are two possible ways, perexample standardization, which standardizes each row of the embedding matrix to unit variance by simply dividing by the standard deviation of the row, and per-feature standardization, which standardizes each column instead. We do not make the rows or columns zero mean. Per-example standardization enjoys the goodness of both random initialization and pre-trained word embeddings: it fixes the small micro variance problem as well as the large macro variance problem of pre-trained word embeddings, while still preserving cosine similarity, i.e., word similarity. Perfeature standardization does not preserve cosine similarity, nor does it fix the large macro variance problem. However, it enjoys the benefit of global statistics, in contrast to the local statistics of individual word vectors used in per-example standardization. Therefore, in problems where the testing and training vocabularies are similar, per-feature standardization may be more advantageous. Both standardizations lose vector magnitude information. Levy et al. (2015) have suggested per-example normalization4 of pre-trained word embeddings for lexical tasks like word similarity and analogy, which do no involve deep neural networks. Making the word vectors unit length alleviates the large macro variance problem, but the small micro variance problem remains (Table 1).\nDiscussion. This is indeed a pretty simple trick, and per-feature standardization (with zero mean) is also a standard data preprocessing method. However, it is not self-evident that this kind of standardization shall be applied on pre-trained word embeddings before using them in deep neural networks, especially with the obvious downside of rendering the word embedding algorithm\u2019s loss function sub-optimal.\nWe expect this to be less of a issue for largescale problems with a large vocabulary and abundant training examples. For example, Vinyals et al. (2015) have found that directly using the WORD2VEC vectors for initialization can bring a\n4It can also be found in the implementation of Glove (Pennington et al., 2014): https://github.com/ stanfordnlp/GloVe\nconsistent, though small, improvement in neural constituency parsing. However, for smaller-scale problems (e.g., an application domain of semantic parsing can have a vocabulary size of only a few hundreds), this issue becomes more critical. Initialized with the raw pre-trained vectors, a model may quickly fall into a poor local optimum and may not have enough signal to escape. Because of the large macro variance problem, standardization can be critical for domain adaptation, which needs to generalize to many words unseen in training.\nThe proposed standardization technique appears in a similar spirit to batch normalization (Ioffe and Szegedy, 2015). We notice two computational differences, that ours is applied on the inputs while batch normalization is applied on internal neuron activations, and that ours standardizes the whole word embedding matrix beforehand while batch normalization standardizes each mini-batch on the fly. In terms of motivation, the proposed technique aims to remedy some undesired characteristics of pre-trained word embeddings, and batch normalization aims to reduce the internal covariate shift. It is of interest to study the combination of the two in future work."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Data Analysis", "text": "The OVERNIGHT dataset (Wang et al., 2015) contains 8 different domains. Each domain is based on a separate knowledge base, with logical forms written in \u03bb-DCS (Liang, 2013). Logical forms are converted into canonical utterances via a simple grammar, and the input utterances are collected by asking crowd workers to paraphrase the canonical utterances. Different domains are designed to stress different types of linguistic phenomena. For example, the CALENDAR domain requires a semantic parser to handle temporal language like \u201cmeetings that start after 10 am\u201d, while the BLOCKS domain features spatial language like \u201cwhich block is above block 1\u201d.\nVocabularies vary remarkably across domains (Table 2). For each domain, only 45% to 70% of the words are covered by any of the other 7 domains. A model has to learn the out-of-vocabulary words from scratch using in-domain training data. The pre-trained WORD2VEC embedding covers most of the words of each domain, and thus can connect the domains to facilitate domain adaptation.\nWords that are still missing are mainly stop words and typos, e.g., \u201cealiest\u201d."}, {"heading": "5.2 Experiment Setup", "text": "We compare our model with all the previous methods evaluated on the OVERNIGHT dataset. Wang et al. (2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms. Xiao et al. (2016) use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar. Similar to ours, Jia and Liang (2016) also use a Seq2Seq model with bi-directional RNN encoder and attentive decoder, but it is used to predict linearized logical forms. They also propose a data augmentation technique, which further improves the average accuracy to 77.5%. But it is orthogonal to this work and can be incorporated in any model including ours, therefore not included.\nThe above methods are all based on the indomain setting, where a separate parser is trained for each domain. In parallel of this work, Herzig and Berant (2017) have explored another direction of cross-domain training: they use all of the domains to train a single parser, with a special domain encoding to help differentiate between domains. We instead model it as a domain adaptation problem, where training on the source and the target domains are separate. Their model is the same as Jia and Liang (2016). It is the current best-performing method on the OVERNIGHT dataset.\nWe use the standard 80%/20% split of training and testing, and randomly hold out 20% of training for validation. In cross-domain experiments, for each target domain, all the other domains are combined as the source domain. Hyper-parameters are selected based on the validation set. State size of both the encoder and the decoder are set to 100, and word embedding size is set to 300. Input and output dropout rate of the GRU cells are 0.7 and 0.5, respectively, and mini-batch size is 512. We use Adam with the default parameters suggested in the paper for optimization. We use gradient clipping with a cap for global norm at 5.0 to alleviate the exploding gradients problem of recurrent neural networks. Early stopping based on the validation set is used to decide when to stop training. The selected model is retrained using the whole training set (training + validation). The\nevaluation metric is accuracy, i.e., the proportion of testing examples for which the top prediction yields the correct denotation. Our model is implemented in Tensorflow (Abadi et al., 2016), and the code can be found at https://github.com/ ysu1989/CrossSemparse."}, {"heading": "5.3 Experiment Results", "text": ""}, {"heading": "5.3.1 Comparison with Previous Methods", "text": "The main experiment results are shown in Table 3. Our base model (Random + I) achieves an accuracy comparable to the previous best in-domain model (Jia and Liang, 2016). With our main novelties, cross-domain training and word embedding standardization, our full model is able to outperform the previous best model, and achieve the best accuracy on 6 out of the 8 domains. Next we examine the novelties separately."}, {"heading": "5.3.2 Word Embedding Initialization", "text": "The in-domain results clearly show the sensitivity of model performance to word embedding initialization. Directly using the raw WORD2VEC vectors or with per-example normalization, the performance is significantly worse than random initialization (6.2% and 7.3%, respectively). Based on the previous analysis, however, one should not be too surprised. The small micro variance problem hurts optimization. In sharp contrast, both of the\nproposed standardization techniques lead to better in-domain performance than random initialization (1.4% and 2.5%, respectively), setting a new best in-domain accuracy (78.2%) on OVERNIGHT. The results show that the pre-trained WORD2VEC vectors can indeed provide useful information, but only when they are properly standardized."}, {"heading": "5.3.3 Cross-domain Training", "text": "A consistent improvement from cross-domain training is observed across all word embedding initialization strategies. Even for raw WORD2VEC embedding or per-example normalization, crossdomain training helps the model escape the poor initialization, though still inferior to the alternative initializations. The best results are again obtained with standardization, with per-example standardization bringing a slightly larger improvement than per-feature standardization. We observe that the improvement from cross-domain training is correlated with the abundance of the in-domain training data of the target domain. To further examine this observation, we use the ratio between the number of examples (N ) and the vocabulary size (|V|) to indicate the data abundance of a domain (the higher, the more abundant), and compute the Pearson correlation coefficient between data abundance and accuracy improvement from cross-domain training (X\u2212I). The results in Ta-\nble 4 show a consistent, moderate to strong negative correlation between the two variables. In other words, cross-domain training is more beneficial when in-domain training data is less abundant, which is reasonable because in that case the model can learn more from the source domain data that is missing in the training data of the target domain."}, {"heading": "5.3.4 Using Downsampled Training Data", "text": "Compared with the vocabulary size and the number of logical forms, the in-domain training data in the OVERNIGHT dataset is indeed abundant. In cross-domain semantic parsing, we are more interested in the scenario where there is insufficient training data for the target domain. To emulate this scenario, we downsample the in-domain training data of each target domain, but still use all training data from the source domain (thus Nt Ns). The results are shown in Figure 2. The gain of cross-domain training is most significant when indomain training data is scarce. As we collect more in-domain training data, the gain becomes smaller, which is expected. These results reinforce those from Table 4. It is worth noting that the effect of downsampling varies across domains. For domains with quite abundant training data like SOCIAL, using only 30% of the in-domain training data, the model can achieve an accuracy almost as good as when using all the data."}, {"heading": "6 Discussion", "text": "Scalability, including vertical scalability, i.e., how to scale up to handle more complex inputs and logical constructs, and horizontal scalability, i.e., how to scale out to handle more domains, is one of the most critical challenges semantic parsing is facing today. In this work, we took an early step towards horizontal scalability, and proposed a paraphrasing based framework for cross-domain semantic parsing. With a sequence-to-sequence paraphrase model, we showed that cross-domain training of semantic parsing can be quite effective under a domain adaptation setting. We also studied how to properly standardize pre-trained word embeddings in neural networks, especially for domain adaptation.\nThis work opens up a number of future directions. As discussed in Section 2.3, many conventional domain adaptation and representation learning ideas can find application in cross-domain semantic parsing. In addition to pre-trained word embeddings, other language resources like paraphrase corpora (Ganitkevitch et al., 2013) can be incorporated into the paraphrase model to further facilitate domain adaptation. In this work we require a full mapping from logical form to canonical utterance, which could be costly for large domains. It is of practical interest to study the case where only a lexicon for mapping schema items to natural language is available. We have restrained ourselves to the case where domains are defined using the same formal language, and we look forward to evaluating the framework on domains of different formal languages when such datasets with canonical utterances become available."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their thoughtful comments. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053 and NSF IIS 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein."}], "references": [{"title": "Tensorflow: Large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transactions of the Association for Computational Linguistics, 1:49\u201362.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Semantic parsing on Freebase from question-answer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Natural language communication with robots", "author": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Bisk et al\\.,? 2016", "shortCiteRegEx": "Bisk et al\\.", "year": 2016}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the ACM SIGMOD International conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Semantic parsing freebase: Towards open-domain semantic parsing", "author": ["Qingqing Cai", "Alexander Yates."], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM).", "citeRegEx": "Cai and Yates.,? 2013", "shortCiteRegEx": "Cai and Yates.", "year": 2013}, {"title": "Almond: The architecture of an open, crowdsourced, privacy-preserving, programmable virtual assistant", "author": ["Giovanni Campagna", "Rakesh Ramesh", "Silei Xu", "Michael Fischer", "Monica S Lam."], "venue": "Proceedings of the International Conference", "citeRegEx": "Campagna et al\\.,? 2017", "shortCiteRegEx": "Campagna et al\\.", "year": 2017}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Chelba and Acero.,? 2004", "shortCiteRegEx": "Chelba and Acero.", "year": 2004}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["David L Chen", "Raymond J Mooney."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "arXiv:0907.1815 [cs.LG].", "citeRegEx": "III.,? 2009", "shortCiteRegEx": "III.", "year": 2009}, {"title": "Domain adaptation for machine translation by mining unseen words", "author": ["Hal Daum\u00e9 III", "Jagadeesh Jagarlamudi."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "III and Jagarlamudi.,? 2011", "shortCiteRegEx": "III and Jagarlamudi.", "year": 2011}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research, 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Leveraging domain-independent information in semantic parsing", "author": ["Dan Goldwasser", "Dan Roth."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Goldwasser and Roth.,? 2013", "shortCiteRegEx": "Goldwasser and Roth.", "year": 2013}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad."], "venue": "Physica D: Nonlinear Phenomena, 42(1-3):335\u2013 346.", "citeRegEx": "Harnad.,? 1990", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Neural semantic parsing over multiple knowledge-bases", "author": ["Jonathan Herzig", "Jonathan Berant."], "venue": "arXiv:1702.01569 [cs.CL].", "citeRegEx": "Herzig and Berant.,? 2017", "shortCiteRegEx": "Herzig and Berant.", "year": 2017}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv:1207.0580", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "Proceedings of the International Conference on Machine Learning, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Smart reply: Automated response suggestion for email", "author": ["Anjuli Kannan", "Karol Kurach", "Sujith Ravi", "Tobias Kaufmann", "Andrew Tomkins", "Balint Miklos", "Greg Corrado", "L\u00e1szl\u00f3 Luk\u00e1cs", "Marina Ganea", "Peter Young"], "venue": null, "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Lambda dependency-based compositional semantics", "author": ["Percy Liang."], "venue": "arXiv:1309.4408 [cs.AI].", "citeRegEx": "Liang.,? 2013", "shortCiteRegEx": "Liang.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Paraphrase generation from latent-variable PCFGs for semantic parsing", "author": ["Shashi Narayan", "Siva Reddy", "Shay B Cohen."], "venue": "arXiv:1601.06068 [cs.CL].", "citeRegEx": "Narayan et al\\.,? 2016", "shortCiteRegEx": "Narayan et al\\.", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of Conference on", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Large-scale semantic parsing without questionanswer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:377\u2013392.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Transforming dependency structures to logical forms for semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata."], "venue": "Transactions of the Association for Computational", "citeRegEx": "Reddy et al\\.,? 2016", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Universal semantic parsing", "author": ["Siva Reddy", "Oscar T\u00e4ckstr\u00f6m", "Slav Petrov", "Mark Steedman", "Mirella Lapata."], "venue": "arXiv:1702.03196 [cs.CL].", "citeRegEx": "Reddy et al\\.,? 2017", "shortCiteRegEx": "Reddy et al\\.", "year": 2017}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning."], "venue": "Proceedings of the Annual Conference on Neural Information Pro-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "On generating characteristic-rich question sets for QA evaluation", "author": ["Yu Su", "Huan Sun", "Brian Sadler", "Mudhakar Srivatsa", "Izzeddin G\u00fcr", "Zenghui Yan", "Xifeng Yan."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["Stefanie A Tellex", "Thomas Fleming Kollar", "Steven R Dickerson", "Matthew R Walter", "Ashis Banerjee", "Seth Teller", "Nicholas Roy."], "venue": "Proceedings of the AAAI", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of the Annual Conference on Neural Information Processing Systems.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Building a semantic parser overnight", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Progress in natural language understanding: an application to lunar geology", "author": ["William A Woods."], "venue": "Proceedings of the American Federation of Information Processing Societies Conference.", "citeRegEx": "Woods.,? 1973", "shortCiteRegEx": "Woods.", "year": 1973}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Xiao et al\\.,? 2016", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Question answering on freebase via relation extraction and textual evidence", "author": ["Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Scott Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "MultiGranCNN: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Yin and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2015}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M Zelle", "Raymon J Mooney."], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}], "referenceMentions": [{"referenceID": 46, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 51, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 2, "context": "Semantic parsing has been applied in many domains, including querying data/knowledge bases (Woods, 1973; Zelle and Mooney, 1996; Berant et al., 2013), controlling IoT devices (Campagna et al.", "startOffset": 91, "endOffset": 149}, {"referenceID": 8, "context": ", 2013), controlling IoT devices (Campagna et al., 2017), and", "startOffset": 33, "endOffset": 56}, {"referenceID": 10, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 43, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 1, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 4, "context": "communicating with robots (Chen and Mooney, 2011; Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Bisk et al., 2016).", "startOffset": 26, "endOffset": 118}, {"referenceID": 5, "context": "Traditional domain adaptation (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006) only concerns natural languages, while semantic parsing concerns both natural and formal languages.", "startOffset": 30, "endOffset": 79}, {"referenceID": 23, "context": ", other symbols like hired by or even predicate1 can also be used for the employer predicate, reminiscent of the symbol grounding problem (Harnad, 1990).", "startOffset": 138, "endOffset": 152}, {"referenceID": 3, "context": "Inspired by the recent success of paraphrasing based semantic parsing (Berant and Liang, 2014; Wang et al., 2015), we propose to use natural language as an intermediate representation for crossdomain semantic parsing.", "startOffset": 70, "endOffset": 113}, {"referenceID": 45, "context": "Inspired by the recent success of paraphrasing based semantic parsing (Berant and Liang, 2014; Wang et al., 2015), we propose to use natural language as an intermediate representation for crossdomain semantic parsing.", "startOffset": 70, "endOffset": 113}, {"referenceID": 45, "context": "To give some idea of the difficulty, for each of the eight domains in the popular OVERNIGHT (Wang et al., 2015) dataset, 30% to 55% of the words never occur in any of the other domains, a similar problem observed in domain adaptation for", "startOffset": 92, "endOffset": 111}, {"referenceID": 33, "context": "WORD2VEC (Mikolov et al., 2013) to combat the vocabulary variety across domains.", "startOffset": 9, "endOffset": 31}, {"referenceID": 32, "context": ", \u03bb-DCS (Liang, 2013) as in the OVERNIGHT dataset.", "startOffset": 8, "endOffset": 21}, {"referenceID": 3, "context": "Our framework follows the research line of semantic parsing via paraphrasing (Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 45, "context": "Our framework follows the research line of semantic parsing via paraphrasing (Berant and Liang, 2014; Wang et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 45, "context": "ous work (Wang et al., 2015) has demonstrated how to design such a mapping, where a domaingeneral grammar and a domain-specific lexicon are constructed to automatically convert every logical form to a canonical utterance.", "startOffset": 9, "endOffset": 28}, {"referenceID": 23, "context": "This indeed brings an additional cost, but we believe it is reasonable and even necessary for three reasons: (1) Only domain administrators know the predicate semantics the best, so it has to be them to reveal that by grounding the predicates to natural language (the symbol grounding problem (Harnad, 1990)).", "startOffset": 293, "endOffset": 307}, {"referenceID": 45, "context": "(3) Canonical utterances are understandable by average users, and thus can also be used for training data collection via crowdsourcing (Wang et al., 2015; Su et al., 2016), which can amortize the cost.", "startOffset": 135, "endOffset": 171}, {"referenceID": 41, "context": "(3) Canonical utterances are understandable by average users, and thus can also be used for training data collection via crowdsourcing (Wang et al., 2015; Su et al., 2016), which can amortize the cost.", "startOffset": 135, "endOffset": 171}, {"referenceID": 3, "context": "In the previous work based on paraphrasing (Berant and Liang, 2014; Wang et al., 2015), semantic parsers are implemented as log-linear models with hand-engineered domainspecific features (including paraphrase features).", "startOffset": 43, "endOffset": 86}, {"referenceID": 45, "context": "In the previous work based on paraphrasing (Berant and Liang, 2014; Wang et al., 2015), semantic parsers are implemented as log-linear models with hand-engineered domainspecific features (including paraphrase features).", "startOffset": 43, "endOffset": 86}, {"referenceID": 21, "context": "learning for domain adaptation (Glorot et al., 2011; Chen et al., 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al.", "startOffset": 31, "endOffset": 71}, {"referenceID": 11, "context": "learning for domain adaptation (Glorot et al., 2011; Chen et al., 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al.", "startOffset": 31, "endOffset": 71}, {"referenceID": 42, "context": ", 2012), we propose a paraphrase model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), which can be trained end to end without feature engineering.", "startOffset": 89, "endOffset": 113}, {"referenceID": 6, "context": "In the recent efforts of scaling semantic parsing to large knowledge bases like Freebase (Bollacker et al., 2008), researchers have explored several ways to infer the semantics of knowledge base relations unseen in training, which are often based on at least one (often both) of the following assumptions: (1) Distant supervision.", "startOffset": 89, "endOffset": 113}, {"referenceID": 2, "context": "For example, Cai and Alexander (2013), among others (Berant et al., 2013; Xu et al., 2016), use sentences from Wikipedia that contain any entity pair of a Freebase relation as the support set of the relation.", "startOffset": 52, "endOffset": 90}, {"referenceID": 48, "context": "For example, Cai and Alexander (2013), among others (Berant et al., 2013; Xu et al., 2016), use sentences from Wikipedia that contain any entity pair of a Freebase relation as the support set of the relation.", "startOffset": 52, "endOffset": 90}, {"referenceID": 5, "context": "In the recent efforts of scaling semantic parsing to large knowledge bases like Freebase (Bollacker et al., 2008), researchers have explored several ways to infer the semantics of knowledge base relations unseen in training, which are often based on at least one (often both) of the following assumptions: (1) Distant supervision. Freebase entities can be linked to external text corpora, and serve as anchors for seeking semantics of Freebase relations from text. For example, Cai and Alexander (2013), among others (Berant et al.", "startOffset": 90, "endOffset": 503}, {"referenceID": 49, "context": "For example, Yih et al. (2015) directly compute", "startOffset": 13, "endOffset": 31}, {"referenceID": 30, "context": "Kwiatkowski et al. (2013) also extract lexical features from input utterance and the surface form of entities and relations.", "startOffset": 0, "endOffset": 26}, {"referenceID": 24, "context": "In parallel of this work, Herzig and Berant (2017) have explored another direction of semantic parsing with multiple domains, where they use all the domains to train a single semantic parser, and attach a domain-specific encoding to the training data of each domain to help the semantic parser differentiate between domains.", "startOffset": 26, "endOffset": 51}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al.", "startOffset": 0, "endOffset": 136}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al. (2016) manage to employ crowd workers with no linguistic expertise for data collection.", "startOffset": 0, "endOffset": 157}, {"referenceID": 2, "context": "Berant et al. (2013) propose to learn from utterance-denotation pairs instead of utterance-logical form pairs, while Wang et al. (2015) and Su et al. (2016) manage to employ crowd workers with no linguistic expertise for data collection. Jia and Liang (2016) propose an interesting form of data augmentation.", "startOffset": 0, "endOffset": 259}, {"referenceID": 30, "context": "been used, such as combinatory categorial grammar (Kwiatkowski et al., 2013; Reddy et al., 2014), dependency tree (Reddy et al.", "startOffset": 50, "endOffset": 96}, {"referenceID": 37, "context": "been used, such as combinatory categorial grammar (Kwiatkowski et al., 2013; Reddy et al., 2014), dependency tree (Reddy et al.", "startOffset": 50, "endOffset": 96}, {"referenceID": 22, "context": ", 2016, 2017), and semantic role structure (Goldwasser and Roth, 2013).", "startOffset": 43, "endOffset": 70}, {"referenceID": 18, "context": "For example, Fader et al. (2013) leverage question paraphrases to for question answering, while Narayan et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 18, "context": "For example, Fader et al. (2013) leverage question paraphrases to for question answering, while Narayan et al. (2016) generate paraphrases as a way of data augmentation.", "startOffset": 13, "endOffset": 118}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 35, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 21, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011).", "startOffset": 118, "endOffset": 208}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011). For example, Chelba and Acero (2004) use parameters trained in the source domain as prior to regularize parameters in the target domain.", "startOffset": 146, "endOffset": 247}, {"referenceID": 5, "context": "Cross-domain semantic parsing can greatly benefit from the rich literature of domain adaptation and transfer learning (Daum\u00e9 III and Marcu, 2006; Blitzer et al., 2006; Pan and Yang, 2010; Glorot et al., 2011). For example, Chelba and Acero (2004) use parameters trained in the source domain as prior to regularize parameters in the target domain. The feature augmentation technique from Daum\u00e9 III (2009) can be very helpful when there are multiple source domains.", "startOffset": 146, "endOffset": 404}, {"referenceID": 42, "context": "In this section we propose a paraphrase model based on the Seq2Seq model (Sutskever et al., 2014) with soft attention.", "startOffset": 73, "endOffset": 97}, {"referenceID": 28, "context": "Similar models have been used in semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016) but for directly mapping utterances to logical forms.", "startOffset": 50, "endOffset": 94}, {"referenceID": 16, "context": "Similar models have been used in semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016) but for directly mapping utterances to logical forms.", "startOffset": 50, "endOffset": 94}, {"referenceID": 40, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 26, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 50, "context": "Several other neural models have been proposed for paraphrasing (Socher et al., 2011; Hu et al., 2014; Yin and Sch\u00fctze, 2015), but it is not the focus of this work to compare all the alternatives.", "startOffset": 64, "endOffset": 125}, {"referenceID": 12, "context": "where gated recurrent unit (GRU) as defined in (Cho et al., 2014) is used as the recurrence.", "startOffset": 47, "endOffset": 65}, {"referenceID": 44, "context": "The attention takes a form similar to (Vinyals et al., 2015).", "startOffset": 38, "endOffset": 60}, {"referenceID": 25, "context": "We apply dropout (Hinton et al., 2012) on both input and output of the GRU cells to prevent overfitting.", "startOffset": 17, "endOffset": 38}, {"referenceID": 42, "context": "One is to use it to generate the most likely output utterance u\u2032 given an input utterance u (Sutskever et al., 2014),", "startOffset": 92, "endOffset": 116}, {"referenceID": 29, "context": "An alternative way is to use the model to rank the legitimate canonical utterances (Kannan et al., 2016):", "startOffset": 83, "endOffset": 104}, {"referenceID": 17, "context": "However, deep neural networks are very sensitive to initialization (Erhan et al., 2010), and a statistical analysis of the pre-trained WORD2VEC vectors reveals some characteristics that may not be desired for initializing deep neural networks.", "startOffset": 67, "endOffset": 87}, {"referenceID": 20, "context": "word2vec/ Under some conditions, including using Xavier initialization (also introduced in that paper and now widely used) for weights, Glorot and Bengio (2010) have shown that the activation variances in a feedforward neural network will be roughly the same as the input variances (word embedding here) at the beginning of training.", "startOffset": 136, "endOffset": 161}, {"referenceID": 31, "context": "Levy et al. (2015) have suggested per-example normalization4 of pre-trained word embeddings for lexical tasks like word similarity and analogy, which do no involve deep neu-", "startOffset": 0, "endOffset": 19}, {"referenceID": 44, "context": "For example, Vinyals et al. (2015) have found that directly using the WORD2VEC vectors for initialization can bring a", "startOffset": 13, "endOffset": 35}, {"referenceID": 36, "context": "It can also be found in the implementation of Glove (Pennington et al., 2014): https://github.", "startOffset": 52, "endOffset": 77}, {"referenceID": 27, "context": "The proposed standardization technique appears in a similar spirit to batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 45, "context": "The OVERNIGHT dataset (Wang et al., 2015) contains 8 different domains.", "startOffset": 22, "endOffset": 41}, {"referenceID": 32, "context": "written in \u03bb-DCS (Liang, 2013).", "startOffset": 17, "endOffset": 30}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms.", "startOffset": 107, "endOffset": 134}, {"referenceID": 42, "context": "Wang et al. (2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms. Xiao et al. (2016) use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar.", "startOffset": 108, "endOffset": 178}, {"referenceID": 19, "context": "(2015) use a log-linear model with a rich set of features, including paraphrase features derived from PPDB (Ganitkevitch et al., 2013), to rank logical forms. Xiao et al. (2016) use a multi-layer perceptron to encode the unigrams and bigrams of the input utterance, and then use a RNN to predict the derivation sequence of a logical form under a grammar. Similar to ours, Jia and Liang (2016) also use a Seq2Seq model with bi-directional RNN encoder and attentive decoder, but it is used to predict linearized logical forms.", "startOffset": 108, "endOffset": 393}, {"referenceID": 24, "context": "In parallel of this work, Herzig and Berant (2017) have explored another direction of cross-domain training: they use all of the do-", "startOffset": 26, "endOffset": 51}, {"referenceID": 28, "context": "the same as Jia and Liang (2016). It is the current best-performing method on the OVERNIGHT dataset.", "startOffset": 12, "endOffset": 33}, {"referenceID": 42, "context": "Previous Methods Wang et al. (2015) 74.", "startOffset": 17, "endOffset": 36}, {"referenceID": 42, "context": "Previous Methods Wang et al. (2015) 74.4 41.9 54.0 75.9 59.0 70.8 48.2 46.3 58.8 Xiao et al. (2016) 75.", "startOffset": 17, "endOffset": 100}, {"referenceID": 27, "context": "7 Jia and Liang (2016) 78.", "startOffset": 2, "endOffset": 23}, {"referenceID": 24, "context": "8 Herzig and Berant (2017) 82.", "startOffset": 2, "endOffset": 27}, {"referenceID": 0, "context": "Our model is implemented in Tensorflow (Abadi et al., 2016), and the code can be found at https://github.", "startOffset": 39, "endOffset": 59}, {"referenceID": 28, "context": "Our base model (Random + I) achieves an accuracy comparable to the previous best in-domain model (Jia and Liang, 2016).", "startOffset": 97, "endOffset": 118}, {"referenceID": 19, "context": "In addition to pre-trained word embeddings, other language resources like paraphrase corpora (Ganitkevitch et al., 2013) can be incorporated into the paraphrase model to further facilitate domain adaptation.", "startOffset": 93, "endOffset": 120}], "year": 2017, "abstractText": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pretrained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular OVERNIGHT dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embedding can bring significant improvement.", "creator": "LaTeX with hyperref package"}}}