{"id": "1704.06191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "Softmax GAN", "abstract": "Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of $N$ real training samples and $M$ generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability $\\frac{1}{M}$, and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability $\\frac{1}{M+N}$. While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training. The original GAN model uses linear transformations to evaluate multiple values with a linear transformation over each of two trials. In the original GAN, we were able to detect only one of the different effects of Linear Interpolation, for example, when the number of effects is less than the number of effects. However, even with the addition of a linear transform, the resulting results are not meaningful. We find that the new model reduces the mean of any single test for each parameter in a randomly generated sample. However, in the new model, the resulting results are still relevant to an initializer test for each parameter. In the new model, the probability distribution becomes uniform in order to avoid the problems that arise after generating the parameter. For example, as you can see, only the average parameter is the minimum value, not the maximum value, and the probability distribution has the maximum value for the input. This is important because it means that the first time you test the effect, which is also available to all the sampled samples, you will receive a chance distribution and a chance distribution. The distribution itself is similar to the original model: the distribution of only the average parameter is the minimum value, not the maximum value, and the chance distribution has the maximum value. In the new model, we have also discovered that the parameter distribution is only important when a randomly generated sample is not a factor in a randomizer test. In the new model, we discovered that even when the number of effects is less than the number of effects, the result is not meaningful. This is important because", "histories": [["v1", "Thu, 20 Apr 2017 15:35:14 GMT  (4438kb,D)", "http://arxiv.org/abs/1704.06191v1", "NIPS 2017 submission"]], "COMMENTS": "NIPS 2017 submission", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["min lin"], "accepted": false, "id": "1704.06191"}, "pdf": {"name": "1704.06191.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Softmax GAN", "Min Lin"], "emails": ["mavenlin@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Generative Adversarial Networks(GAN) [4] has achieved great success due to its ability to generate realistic samples. GAN is composed of one Discriminator and one Generator. The discriminator tries to distinguish real samples from generated samples, while the generator counterfeits real samples using information from the discriminator. GAN is unique from the many other generative models. Instead of explicitly sampling from a probability distribution, GAN uses a deep neural network as a direct generator that generates samples from random noises. GAN has been proved to work well on several realistic tasks, e.g. image inpainting, debluring and imitation learning.\nDespite its success in many applications, GAN is highly unstable in training. Careful selection of hyperparameters is often necessary to make the training process converge [11]. It is often believed that this instability is caused by unbalanced discriminator and generator training. As the discriminator utilizes a logistic loss, it saturates quickly and its gradient vanishes if the generated samples are easy to separate from the real ones. When the discriminator fails to provide gradient, the generator stops updating. Softmax GAN overcomes this problem by utilizing the softmax cross-entropy loss, whose gradient is always non-zero unless the softmaxed distribution matches the target distribution."}, {"heading": "2 Related Works", "text": "There are many works related to improving the stability of GAN training. DCGAN proposed by Radford et. al. [11] comes up with several empirical techniques that works well, including how to apply batch normalization, how the input should be normalized, and which activation function to use. Some more techniques are proposed by Salimans et. al. [13]. One of them is minibatch discrimination. The idea is to introduce a layer that operates across samples to introduce coordination between gradients from different samples in a minibatch. In this work, we achieve a similar effect using softmax across the samples. We argue that softmax is more natural and explanable and yet does not require extra parameters.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 4.\n06 19\n1v 1\n[ cs\n.L G\n] 2\n0 A\npr 2\n01 7\nNowozin et. al. [9] generalizes the GAN training loss from Jensen-Shannon divergence to any f-divergence function. Wasserstein distance is mentioned as a member of another class of probability metric in this paper but is not implemented. Under the f-GAN framework, training objectives with more stable gradients can be developed. For example, the Least Square GAN [8] uses l2 loss function as the objective, which achieves faster training and improves generation quality.\nArjovsky et. al. managed to use Wasserstein distance as the objective in their Wasserstein GAN (WGAN) [1] work. This new objective has non-zero gradients everywhere. The implementation is as simple as removing the sigmoid function in the objective and adding weight clipping to the discriminator network. WGAN is shown to be free of the many problems in the original GAN, such as mode collapse and unstable training process. A related work to WGAN is Loss-Sensitive GAN [10], whose objective is to minimize the loss for real data and maximize it for the fake ones. The common property of Least Square GAN, WGAN, Loss-Sensitive GAN and this work is the usage of objective functions with non-vanishing gradients."}, {"heading": "3 Softmax GAN", "text": "We denote the minibatch sampled from the training data and the generated data as B+ and B\u2212 respectively. B = B+ + B\u2212 is the union of B+ and B\u2212. The output of the discriminator is represented by \u00b5\u03b8(x) parameterized by \u03b8. ZB = \u2211 x\u2208B e\n\u2212\u00b5\u03b8(x) is the partition function of the softmax within batch B. We use x for samples from B+ and x\u2032 for generated samples in B\u2212. As in GAN, generated samples are not directly sampled from a distribution. Instead, they are generated directly from a random variable z with a trainable generator G\u03c8 .\nWe softmax normalized the energy of the all data points within B, and use the cross-entropy loss for both the discriminator and the generator. The target of the discriminator is to assign the probability mass equally to all samples in B+, leaving samples in B\u2212 with zero probability.\ntD(x) = { 1 |B+| , if x \u2208 B+ 0, if x \u2208 B\u2212\n(1)\nLD = \u2212 \u2211 x\u2208B tD(x) ln e\u2212\u00b5 \u03b8(x) ZB\n= \u2212 \u2211 x\u2208B+ 1 |B+| ln e\u2212\u00b5 \u03b8(x) ZB \u2212 \u2211 x\u2032\u2208B\u2212 0 ln e\u2212\u00b5 \u03b8(x\u2032) ZB\n= \u2211 x\u2208B+ 1 |B+| \u00b5\u03b8(x) + lnZB (2)\nFor generator, the target is to assign the probability mass equally to all the samples in B.\ntG(x) = 1\n|B| (3)\nLG = \u2212 \u2211 x\u2208B tG(x) ln e\u2212\u00b5 \u03b8(x) ZB\n= \u2212 \u2211 x\u2208B+ 1 |B| ln e\u2212\u00b5 \u03b8(x) ZB \u2212 \u2211 x\u2032\u2208B\u2212 1 |B| ln e\u2212\u00b5 \u03b8(x\u2032) ZB\n= \u2211 x\u2208B+ 1 |B| \u00b5\u03b8(x) + \u2211 x\u2032\u2208B\u2212 1 |B| \u00b5\u03b8(x\u2032) + lnZB (4)"}, {"heading": "4 Relationship to Importance Sampling", "text": "It has been pointed out in the original GAN paper that GAN is similar to NCE [6] in that both of them use a binary logistic classification loss as the surrogate function for training of a generative model. GAN improves over NCE by using a trained generator for noise samples instead of fixing the noise distribution. In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2]. We prove it as follows."}, {"heading": "4.1 Discriminator", "text": "We use \u03be\u03c6(x) to represent energy function and Z = \u222b x e\u2212\u03be \u03c6(x) is the partition function. The probability density function is then p\u03c6(x) = e \u2212\u03be\u03c6(x)\nZ . With O as the observed training example, the maximum likelyhood estimation loss function is as follows\nJ = 1 |O| \u2211 x\u2208O \u03be\u03c6(x) + log \u222b x\u2032 e\u2212\u03be \u03c6(x\u2032)dx (5)\n\u2207\u03c6J = 1 |O| \u2211 x\u2208O \u2207\u03c6\u03be\u03c6(x)\u2212 Ex\u2032\u223cp\u03c6(x\u2032)\u2207\u03c6\u03be\u03c6(x\u2032) (6)\nAs it is usually difficult to sample from p\u03c6, Importance Sampling instead introduces a known distribution q(x) to sample from, resulting in:\n\u2207\u03c6J = 1 |O| \u2211 x\u2208O \u2207\u03c6\u03be\u03c6(x)\u2212 Ex\u2032\u223cq(x\u2032) p\u03c6(x\u2032) q(x\u2032) \u2207\u03c6\u03be\u03c6(x\u2032) (7)\nIn the biased Importance Sampling [3], the above is converted to the following biased estimation which can be calculated without knowing p\u03c6:\n\u2207\u0302\u03c6J = 1 |B+| \u2211 x\u2208B+ \u2207\u03c6\u03be\u03c6(x)\u2212 1 R \u2211 x\u2032\u2208Q r(x\u2032)\u2207\u03c6\u03be\u03c6(x\u2032) (8)\nwhere r(x\u2032) = e \u2212\u03be\u03c6(x\u2032) q(x\u2032) , R = \u2211 x\u2032\u2208Q r(x \u2032). And Q is a batch of data sampled from q. At this point, we reparameterize e\u2212\u03be \u03c6(x) = e\u2212\u00b5 \u03b8(x)q(x).\n\u2207\u0302\u03b8J = 1 |B+| \u2211 x\u2208B+ \u2207\u03b8\u00b5\u03b8(x)\u2212 1\u2211 y\u2208Q e \u2212\u00b5\u03b8(y) \u2211 x\u2032\u2208Q e\u2212\u00b5 \u03b8(x\u2032)\u2207\u03b8\u00b5\u03b8(x\u2032) (9)\nWithout loss of generality, we assume |B+| = |B\u2212| and replace Q with B = B+ +B\u2212 in equation 9, namely q(x) = 12pdata(x) + 1 2pG(x), and compare the above with equation 2. It is easy to see that the above is the gradient of LD. In other words, the discriminator loss function in Softmax GAN is performing maximum likelihood on the observed real data with Importance Sampling to estimate the partition function.\nWith infinite number of real samples, the optimal solution is\ne\u2212\u00b5 \u03b8(x) = C pD pD+pG\n2\n(10)\nC is a constant."}, {"heading": "4.2 Generator", "text": "We substitute equation 10 into 4. The lhs of equation 4 gives\n\u2212 \u2211 x\u2208B 1 |B| ln pD pD+pG 2 \u2212 lnC = KL(pD + pG 2 \u2016pD) (11)\nThe gradient of the rhs can be seen as biased Importance Sampling as well,\n\u2212 \u2211 x\u2208B pD pD+pG\n2 \u2207pG pD+pG\u2211\nx\u2208B pD\npD+pG 2\n\u2248 \u2212Ex\u223cpD \u2207pG\npD + pG (12)\nwhich optimizes \u2212Ex\u223cpD ln(pD + pG) = KL(pD\u2016 pD+pG 2 ) \u2212 Ex\u223cpD ln 2pD. After removing the constants, we get\nLG = KL( pD + pG\n2 \u2016pD) +KL(pD\u2016 pD + pG 2 ) (13)\nThus optimizing the objective of the generator is equivalent to minimizing the Jensen-Shannon divergence between pD and pD+pG2 with Importance Sampling."}, {"heading": "4.3 Importance Sampling\u2019s link to NCE", "text": "Note that Importance Sampling itself is strongly connected to NCE. As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate. The difference is that in NCE, a binary classification task is defined between true and noise samples with a logistic loss, whereas Importance Sampling replaces the logistic loss with a multiclass softmax and cross-entropy loss. We show the relationship between NCE, Importance Sampling, GAN and this work in Figure 1. Softmax GAN is filling the table with the missing item."}, {"heading": "4.4 Infinite batch size", "text": "As pointed out by [3], biased Importance Sampling estimation converges to the real partition function when the number of samples in one batch goes to infinity. In practice, we found that setting |B+| = |B\u2212| = 5 is enough for generating images that are visually realistic."}, {"heading": "5 Experiments", "text": "We run experiments on image generation with the celebA database. We show that although Softmax GAN is minimizing the Jensen Shannon divergence between the generated data and the real data, it is more stable than the original GAN, and is less prone to mode collapsing.\nWe implement Softmax GAN by modifying the loss function of the DCGAN code (https://github.com/carpedm20/DCGAN-tensorflow). As DCGAN is quite stable, we remove the empirical techniques applied in DCGAN and observe instability in the training. On the contrary, Softmax GAN is stable to these changes."}, {"heading": "5.1 Stablized training", "text": "We follow the WGAN paper, by removing the batch normalization layers and using a constant number of filters in the generator network. We compare the results from GAN and Softmax GAN. The results are shown in Figure 2."}, {"heading": "5.2 Mode collapse", "text": "When GAN training is not stable, the generator may generate samples similar to each other. This lack of diversity is called mode collapse because the generator can be seen as sampling only from some of the modes of the data distribution.\nIn the DCGAN paper, the pixels of the input images are normalized to [\u22121, 1). We remove this constraint and instead normalize the pixels to [0, 1). At the same time, we replace the leaky relu with relu, which makes the gradients more sparse (this is unfavorable for GAN training according to the DCGAN authors). Under this setting, the original GAN suffers from a significant degree of mode collape and low image qualities. In contrast, Softmax GAN is robust to this change. Examplars are show in Figure 3."}, {"heading": "5.3 Balance of generator and discriminator", "text": "It is claimed in [11] that manually balancing the number of iterations of the discriminator and generator is a bad idea. The WGAN paper however gives the discriminator phase more iterations to get a better discriminator for the training of the generator. We set the discriminator vs generator ratio to 5 : 1 and 1 : 5 and explore the effects of the this ratio on DCGAN and Softmax GAN. The results are in Figure 4 and 5 respectively."}, {"heading": "6 Conclusions and future work", "text": "We propose a variant of GAN which does softmax across samples in a minibatch, and uses crossentropy loss for both discriminator and generator. The target is to assign all probability to real data for discriminator and to assign probability equally to all samples for the generator. We proved that this objective approximately optimizes the JS-divergence using Importance Sampling. We futhur form the links between GAN, NCE, Importance Sampling and Softmax GAN. We demonstrate with experiments that Softmax GAN consistently gets good results when GAN fails at the removal of empirical techniques.\nIn our future work, we\u2019ll perform a more systematic comparison between Softmax GAN and other GAN variants and verify whether it works on tasks other than image generation."}], "references": [{"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Yoshua Bengio", "Jean-S\u00e9bastien Sen\u00e9cal"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "On distinguishability criteria for estimating generative models", "author": ["Ian J Goodfellow"], "venue": "arXiv preprint arXiv:1412.6515,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["Xudong Mao", "Qing Li", "Haoran Xie", "Raymond YK Lau", "Zhen Wang"], "venue": "arXiv preprint ArXiv:1611.04076,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Loss-sensitive generative adversarial networks on lipschitz densities", "author": ["Guo-Jun Qi"], "venue": "arXiv preprint arXiv:1701.06264,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "On word embeddings - part 2: Approximating the softmax", "author": ["Sebastian Ruder"], "venue": "http://sebastianruder.com/word-embeddings-softmax/index.html# similaritybetweennceandis", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "1 Introduction Generative Adversarial Networks(GAN) [4] has achieved great success due to its ability to generate realistic samples.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "Careful selection of hyperparameters is often necessary to make the training process converge [11].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "[11] comes up with several empirical techniques that works well, including how to apply batch normalization, how the input should be normalized, and which activation function to use.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] generalizes the GAN training loss from Jensen-Shannon divergence to any f-divergence function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For example, the Least Square GAN [8] uses l2 loss function as the objective, which achieves faster training and improves generation quality.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "A related work to WGAN is Loss-Sensitive GAN [10], whose objective is to minimize the loss for real data and maximize it for the fake ones.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "It has been pointed out in the original GAN paper that GAN is similar to NCE [6] in that both of them use a binary logistic classification loss as the surrogate function for training of a generative model.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "In the same sense as GAN is related to NCE [5], this work can be seen as the Importance Sampling version of GAN [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "In the biased Importance Sampling [3], the above is converted to the following biased estimation which can be calculated without knowing p:", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "As pointed out by [7] and [12], both Importance Sampling and NCE are training the underlying generative model with a classification surrogate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "4 Infinite batch size As pointed out by [3], biased Importance Sampling estimation converges to the real partition function when the number of samples in one batch goes to infinity.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "3 Balance of generator and discriminator It is claimed in [11] that manually balancing the number of iterations of the discriminator and generator is a bad idea.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1 M , and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1 M+N . While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training.", "creator": "LaTeX with hyperref package"}}}