{"id": "1704.06300", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2017", "title": "A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units", "abstract": "The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 20 Apr 2017 18:53:51 GMT  (1552kb,D)", "http://arxiv.org/abs/1704.06300v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["niranjani prasad", "li-fang cheng", "corey chivers", "michael draugelis", "barbara e engelhardt"], "accepted": false, "id": "1704.06300"}, "pdf": {"name": "1704.06300.pdf", "metadata": {"source": "CRF", "title": "A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units", "authors": ["Niranjani Prasad", "Li-Fang Cheng", "Corey Chivers"], "emails": [], "sections": [{"heading": null, "text": "The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability."}, {"heading": "1 Introduction", "text": "Mechanical ventilation is one of the most widely used interventions in admissions to the intensive care unit (ICU): around 40% of patients in the ICU are supported on invasive mechanical ventilation at any given hour, accounting for 12% of total hospital costs in the United States (Ambrosino and Gabbrielli [2010], Wunsch et al. [2013]). These are typically patients with acute respiratory failure or compromised lung function caused by some underlying condition such as pneumonia, sepsis, or heart disease, or cases in which breathing support is necessitated by neurological disorders, impaired consciousness, or weakness\nfollowing major surgery. As advances in healthcare enable more patients to survive critical illness or surgery, the need for mechanical ventilation during recovery has risen.\nClosely coupled with ventilation in the care of these patients is sedation and analgesia, which are crucial to maintaining physiological stability and controlling pain levels of patients while intubated. The underlying condition of the patient, as well as factors such as obesity or genetic variations, can have a significant effect on the pharmacology of drugs, and cause high inter-patient variability in response to a given sedative (Patel and Kress [2012]), lending motivation to a personalized approach to sedation strategies.\nWeaning refers to the process of liberating patients from mechanical ventilation. The primary diagnostic tests for determining whether a patient is ready to be extubated involve screening for resolution of the underlying disease, haemodynamic stability, assessment of current ventilator settings and level of consciousness, and finally a series of spontaneous breathing trials (SBTs). Prolonged ventilation\u2014and corresponding over-sedation\u2014is associated with post-extubation delirium, drug dependence, ventilator-induced pneumonia, and higher patient mortality rates (Hughes et al. [2012]), in addition to inflating costs and straining hospital resources. Physicians are often conservative in recognizing patient suitability for extubation, however, as failed breathing trials or premature extubations that necessitate reintubation within 48-72 hours can cause severe patient discomfort and result in even longer ICU stays (Krinsley et al. [2012]). Efficient weaning of sedation and ventilation is therefore a priority both for improving patient outcomes and reducing costs, but a lack of comprehensive evidence and the variability in outcomes between individuals and subpopulations means there is little agreement in clinical literature on the best weaning protocol (Conti et al. [2014], Goldstone [2002]).\nIn this work, we aim to develop a decision support tool that leverages available patient information in the data-rich ICU setting to alert clinicians when a patient is ready for initiation of weaning, and to recommend a personalized treatment protocol. We explore the use of off-policy re-\nar X\niv :1\n70 4.\n06 30\n0v 1\n[ cs\n.A I]\n2 0\nA pr\n2 01\ninforcement learning algorithms, namely fitted Q-iteration (FQI) with different regressors, to determine the optimal treatment at each patient state from sub-optimal historical patient treatment profiles. The setting fits naturally into the framework of reinforcement learning as it is fundamentally a sequential decision making problem rather than purely a prediction task: we wish to choose the best possible action at each time\u2014in terms of sedation drug and dosage, ventilator settings, initiation of a spontaneous breathing trial, or extubation\u2014while capturing the stochasticity of the underlying process, the delayed effects of actions, and the uncertainty in state transitions and outcomes.\nThe problem poses a number of key challenges: there are a multitude of factors that can potentially influence patient readiness for extubation, including some not directly observed in ICU chart data, such as a patient\u2019s inability to protect their airway due to muscle weakness. The data that is recorded is often sparse and noisy. In addition, there is potentially an extremely large space of possible sedatives and ventilator settings that can be leveraged during weaning. We are also posed with the problem of interval censoring, as in other intervention data: given past treatment and vitals trajectories, observing a successful extubation at time t provides us only with an upper bound on the true time to extubation readiness, te \u2264 t; on the other hand, if a breathing trial was unsuccessful, there is uncertainty how premature the intervention was. This presents difficulties both when learning the policy and in evaluating policies.\nThe rest of the paper is organized as follows: Section 2 explores recent efforts in the use of reinforcement learning in clinical settings. In Section 3, we describe the data and methods used here, and Section 4 presents the results. Finally, conclusions and possible directions for further work are discussed in Section 5."}, {"heading": "2 Related Work", "text": "The widespread adoption of electronic health records (EHRs) paved the way for a data-driven approach to healthcare, and recent years have seen a number of efforts towards personalized, dynamic treatment regimes. Reinforcement learning in particular has been explored in various settings, from determining the sequence of drugs to be administered in HIV therapy or cancer treatment, to management of anaemia in haemodialysis patients, and insulin regulation in diabetics. These efforts are typically based on estimating the value, in terms of clinical outcomes, of different treatment decisions given the state of the patient.\nFor example, Ernst et al. [2006] applied fitted Q-iteration with a tree-based ensemble method to learn the optimal HIV treatment in the form of structured treatment interruption strategies, in which patients are cycled on and off drug therapy. The observed reward here is defined in terms of the equilibrium point between healthy and unhealthy blood\ncells in the patient. Zhao et al. [2011] used Q-learning to learn optimal individualized treatment regimens for nonsmall cell lung cancer. The objective is to choose the optimal first and second lines of therapy and the optimal initiation time for the second line treatment such that the overall survival time is maximized. The Q-function with timeindexed parameters is approximated using a modification of support vector regression (SVR) that explicitly handles right-censored data. In this setting, right-censoring arises in measuring the time of death from start of therapy: given that a patient is still alive at the time of the last follow-up, we merely have a lower bound on the exact survival time.\nEscandell-Montero et al. [2014] compared the performance of both Q-learning and fitted Q-iteration with current clinical protocol for informing the delivery of erythropoeisisstimulating agents (ESAs) for treating anaemia. The drug administration strategy is modeled as a Markov decision process (MDP), with the state space expressed by current values and change in haemoglobin levels, the most recent ESA dosage, and the patient subpopulation. The action space is a set of four discretized ESA levels, and the reward function is designed to maintain haemoglobin levels within a healthy range while avoiding abrupt changes.\nOn the problem of administering anaesthesia in an ICU setting, Moore et al. [2004] applied Q-learning with eligibility traces to the administration of intravenous propofol, modeling patient dynamics according to an established pharmacokinetic model, with the aim of maintaining some level of sedation or consciousness. Padmanabhan et al. [2014] also used Q-learning, for the regulation of both sedation level and arterial pressure as an indicator of physiological stability, using propofol infusion rate. All of the aforementioned work rely on model-based approaches to reinforcement learning, and develop treatment policies on simulated patient data. More recently however, Nemati et al. [2016] consider the problem of heparin dosing to maintain blood coagulation levels within some well-defined therapeutic range, modeling the task as a partially observable MDP, using a dynamic Bayesian network trained on real ICU data, and learning a dosing policy with neural fitted Q-iteration (NFQ).\nThere exists some literature on machine learning methods for the problem of ventilator weaning: Mueller et al. [2013] and Kuo et al. [2015] look at prediction of weaning outcomes using supervised learning methods, and suggest that classifiers based on neural networks, logistic regression, or naive Bayes, trained on patient ventilator and blood gas data, show promise in predicting successful extubation. Gao et al. [2017] develops association rule networks for naive Bayes classifiers, to analyze the discriminative power of different feature categories toward each decision outcome class, in order to help inform clinical decision making. Our paper is novel in its use of reinforcement learning methods to directly tackle policy recommendation for\nventilation weaning. Specifically, we incorporate a larger number of possible predictors of weaning readiness, in a 32-dimensional patient state representation, compared with previous works which typically limit features for classification to at most a couple of key vital signs. Moreover, we make use of current clinical protocols to inform the design and tuning of a reward function."}, {"heading": "3 Methods", "text": ""}, {"heading": "3.1 Critical Care Data", "text": "We use the Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC III) database (Johnson et al. [2016]), a freely available source of de-identified critical care data for 53,423 adult admissions and 7,870 neonates. The data includes patient demographics, time-stamped measurements from bedside monitoring of vitals, administration of fluids and medications, results of laboratory tests, observations and notes charted by care providers, as well as diagnoses, procedures and prescriptions for billing.\nWe extract from this database a set of 8,860 admissions from 8,182 unique adult patients undergoing invasive ventilation. In order to train and test our weaning policy, we filter further to include only those admissions in which the patient was kept under ventilator support for more than 24 hours. This allows us to exclude the majority of episodes of routine ventilation following surgery, which are at minimal risk of adverse extubation outcomes. We also filter out admissions in which the patient in not successfully discharged from the hospital by the end of the admission, as in cases where the patient expires in the ICU, failure to discharge is largely due to factors beyond the scope of ventilator weaning, and again, a more informed weaning policy is unlikely to have a significant influence on outcomes. Failure in our problem setting is instead defined as prolonged ventilation, administration of unsuccessful spontaneous breathing trials, or reintubation within the same admission\u2014all of which are associated with adverse outcomes for the patient. A typical patient timeline is illustrated in Figure 1.\nPreliminary guidelines for the weaning protocol, in terms of the desired ranges of physiological parameters (heart rate, respiratory rate, and arterial pH) as well as criteria at time of extubation for the inspired O2 fraction (FiO2), oxygenation pulse oxymetry (SpO2), and positive endexpiratory pressure (PEEP) set, were obtained from clinicians at the Hospital of University of Pennsylvania, HUP (Table 1). These are used in shaping rewards in our MDP to facilitate learning of the optimal policy."}, {"heading": "3.1.1 Preprocessing using Gaussian Processes", "text": "Measurements of vitals and lab results in the ICU data can be irregular, sparse, and error-prone. Non-invasive measurements such as heart rate or respiratory rate are taken\nseveral times an hour, while tests for arterial pH or oxygen pressure, which involve more distress to the patient, may only be administered every few hours as needed. This wide discrepancy in measurement frequency is typically handled by resampling with means in hourly intervals (when we have multiple measurements within an hour), and using sample-and-hold interpolation to impute subsequent missing values. However, patient state\u2014and therefore the need to update management of sedation or ventilation\u2014can change within the space of an hour, and naive methods for interpolation are unlikely to provide the necessary accuracy at higher temporal resolutions. We therefore explore methods for the imputation of patient state that can enable more precise policy estimation.\nOne commonly used approach to resolve missing and irregularly sampled time series data is Gaussian processes (GPs, [Stegle et al., 2008, Du\u0308richen et al., 2015, Ghassemi et al., 2015]).Denoting the observations of the vital signs by v and the measurement time t, we model\nv = f(t) + \u03b5,\nwhere \u03b5 vector represents i.i.d Gaussian noise, and f(t) is the latent noise-free function we would like to estimate. We put a GP prior on the latent function f(t):\nf(t) \u223c GP(m(t), \u03ba(t, t\u2032)),\nwhere m(t) is the mean function and \u03ba(t, t\u2032) is the covariance function or kernel, which shapes the temporal properties of f(t). In this work, we use a multi-output GP to account for temporal correlations between physiological signals during interpolation. We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings. We set m(t) = 0 without loss of generality (Rasmussen and Williams [2006]), and \u03ba(t, t\u2032) as the kernel in the linear model of coregionalization with the spectral kernel as the basis kernel, allowing us to model both smooth correlations in time and periodic variations of these vital signs and lab results. The full joint kernel for each patient i is defined as:\n\u03bai(ti, t \u2032 i) = Q\u2211 q=1 Bq \u2297 \u03baq(ti,\u2217, t\u2032i,\u2217),\nwhere ti,\u2217 represents the time vector of each vital sign. Note that this is a simplified representation based on the\nassumption that we have the same input time vector for each signal, which does not hold in our irregularly sampled data. In practice we have to compute each sub-block \u03baq(ti,d, t \u2032 i,d\u2032) given any pair of input time ti,d and t \u2032 i,d\u2032 from two signals, indexing by d and d\u2032. We useQ to denote the number of mixture kernels, and Bq to encode the scale covariance between any pair of signals, written as\nBq =  bq,(1,1) bq,(1,2) \u00b7 \u00b7 \u00b7 bq,(1,D) bq,(1,1) ... . . . ... ... ... . . .\n... bq,(D,1) bq,(D,2) \u00b7 \u00b7 \u00b7 bq,(D,D)  \u2208 RD\u00d7D. The basis kernel is parameterized as\n\u03baq(t, t \u2032) = exp (\u22122\u03c02\u03c42vq) cos (2\u03c0\u03c4\u00b5q),\n\u03c4 = |t\u2212 t\u2032|.\nWe set Q = 2 and R = 5 for modeling 12 selected physiological signals (D = 12) jointly. For each patient, one structured GP kernel is estimated using the implementation in Cheng et al. [2017]. We then impute the time series with the estimated posterior mean given all the observations across all chosen physiological signals for that patient. In choosing the 12 signals, we exclude vitals that take discrete values, such as ventilator mode or the RASS sedation scale; for these, we simply resample with means and\napply sample-and-hold interpolation. After preprocessing, we obtain complete data for each patient, at a temporal resolution of 10 minutes, from admission time to discharge time (Figure 2)."}, {"heading": "3.2 MDP Formulation", "text": "A Markov decision process is defined by\n(i) A finite state space S such that at each time t, the environment (here, the patient) is in state st \u2208 S.\n(ii) An action space A: at each time t, the agent takes action at \u2208 A, which influences the next state, st+1.\n(iii) A transition function P (st+1|st, at), the probability of the next state given the current state and action, which defines the (unknown) dynamics of the system. (iv) A reward function r(st, at) \u2208 R, the observed feedback following a transition at each time step t.\nThe goal of the reinforcement learning agent is to learn a policy, i.e. a mapping \u03c0(s)\u2192 a from states to actions, that maximizes the expected accumulated reward\nR\u03c0(st) = lim T\u2192\u221e Est+1|st,\u03c0(st) T\u2211 t=1 \u03b3tr(st, at)\nover time horizon T . The discount factor, \u03b3, determines the relative weight of immediate and long-term rewards.\nPatient response to sedation and readiness for extubation can depend on a number of different factors, from demographic characteristics, pre-existing conditions, and comorbidities to specific time-varying vital signs, and there is considerable variability in clinical opinion on the extent of the influence of different factors. Here, in defining each patient state within an MDP, we look to incorporate as many reliable and frequently monitored features as possible, and allow the algorithm to determine the relevant features. The state at each time t is a 32-dimensional feature vector that includes fixed demographic information (patient age, weight, gender, admit type, ethnicity), as well as relevant physiological measurements, ventilator settings, level of consciousness (given by the Richmond Agitation Sedation Scale, or RASS), current dosages of different sedatives, time into ventilation, and the number of intubations so far in the admission. For simplicity, categorical variables admit type and ethnicity are binarized as emergency/nonemergency and white/non-white, respectively.\nIn designing the action space, we develop an approximate mapping of six commonly used sedatives into a single dosage scale, and choose to discretize this scale to four different levels of sedation. The action at \u2208 A at each time step is chosen from a finite two-dimensional set of eight actions, where at[0] \u2208 {0, 1} indicates having the patient off or on the ventilator, respectively, and at[1] \u2208 {0, 1, 2, 3} corresponds to the level of sedation to be administered over the next 10-minute interval:\nA = {[\n0 0\n] , [ 0 1 ] , [ 0 2 ] , [ 0 3 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ]} Finally, we associate a reward signal rt+1 with each state transition\u2014defined by the tuple \u3008st, at, st+1\u3009\u2014to encom-\npass (i) time into ventilation, (ii) physiological stability, i.e. whether vitals are steady and within expected ranges, (iii) failed SBTs or reintubation. The reward at each timestep is defined by a combination of sigmoid, piecewise-linear, and threshold functions that reward closely regulated vitals and successful extubation while penalizing adverse events: rt+1 = r vitals t+1 + r vent off t+1 + r vent on t+1 , where\nrvitalst+1 = C1 \u2211 v [ 1 1 + e\u2212(vt\u2212vmin) \u2212 1 1 + e\u2212(vt\u2212vmax) + 1 2 ] \u2212 C2 [ max ( 0, |vt+1 \u2212 vt|\nvt \u2212 0.2\n)] ,\nrvent offt+1 = 1[st+1(vent on)=0] [ C3 \u00b7 1[st(vent on)=1]\n+C4 \u00b7 1[st(vent on)=0] \u2212 C5 \u2211 vext 1[vextt >v ext max || vextt <vextmin]\n] ,\nrvent ont+1 = 1[st+1(vent on)=1] [ C6 \u00b7 1[st(vent on)=1]\n\u2212C7 \u00b7 1[st(vent on)=0] ] .\nHere, values vt are the measurements of those vitals v (included in the state representation st) believed to be indicative of physiological stability at time t, with desired ranges [vmin, vmax]. The penalty for exceeding these ranges at each time step is given by a truncated sigmoid function (Figure 3a). The system also receives negative feedback when consecutive measurements see a sharp change (Figure 3b).\nVital signs vextt comprise the subset of parameters directly associated with readiness for extubation (FiO2, SpO2, and PEEP set) with weaning criteria defined by the ranges [vextmin, v ext max]. A fixed penalty is applied when these criteria are not met during extubation. The system also accumulates negative rewards for each additional hour spent on the ventilator, and a large positive reward at the time of suc-\ncessful extubation. Constants C1 to C7 determine the relative importance of these reward signals."}, {"heading": "3.3 Learning the Optimal Policy", "text": "The majority of reinforcement learning algorithms are based on estimation of the Q-function, that is, the expected value of state-action pairs Q\u03c0(s, a) : S \u00d7 A \u2192 R, to determine the optimal policy \u03c0. Of these, the most widely used is Q-learning, an off-policy reinforcement learning algorithm in which we start with an initial state and arbitrary approximation of the Q-function, and update this estimate using the reward from the next transition using the Bellman recursion for Q-values:\nQ\u0302(st, at) = Q\u0302(st, at)+\u03b1(rt+1+\u03b3max a\u2208A\nQ\u0302(st+1, a)\u2212Q\u0302(st, at))\nwhere the learning rate \u03b1 gives the relative weight of the current and previous estimate, and \u03b3 is the discount factor.\nFitted Q-iteration (FQI) is a form of off-policy batch-mode reinforcement learning that uses a set of one-step transition tuples:\nF = {(\u3008snt , ant , snt+1\u3009, rnt+1), n = 1, ..., |F|}\nto learn a sequence of function approximators Q\u03021, Q\u03022...Q\u0302K of the value of state-action pairs, by iteratively solving supervised learning problems. Both FQI and Q-learning belong to the class of model-free reinforcement learning methods, which assumes no knowledge of the dynamics of the system. In the case of FQI, there are also no assumptions made on the ordering of tuples; these could correspond to a sequence of transitions from a single admission, or randomly ordered transitions from multiple histories. FQI is therefore more data-efficient, with the full set of samples used by the algorithm at every iteration, and hence typically converges much faster than Q-learning.\nThe training set at the kth supervised learning problem is given by T S = {(\u3008snt , ant \u3009 , Q\u0302k(snt , ant )), n = 1, ..., |F|}. As before, the Q-function is updated at each iteration according to the Bellman equation:\nQ\u0302k(st, at)\u2190 rt+1 + \u03b3max a\u2208A Q\u0302k\u22121(st+1, a)\nwhere Q\u03021(st, at) = rt+1. The approximation of the optimal policy after K iterations is then given by:\n\u03c0\u0302\u2217(s) = arg max a\u2208A Q\u0302K(s, a)\nFQI guarantees convergence for many commonly used regressors, including kernel-based methods (Ormoneit and Sen [2002]) and decision trees. In particular, extremely randomized trees (Extra-Trees: Geurts et al. [2006], Ernst et al. [2005]), a tree-based ensemble method that extends on random forests by introducing randomness in the thresholds chosen at each split, has been applied in the past to learning large or continuous Q-functions in clinical settings (Ernst et al. [2006], Escandell-Montero et al. [2014]).\nNeural Fitted-Q (NFQ, Riedmiller [2005]) on the other hand, looks to leverage the representational power of neural networks as regressors to fitted Q-iteration. Nemati et al. [2016] use NFQ to learn optimal heparin dosages, mapping the patient hidden state to expected return. Neural networks hold an advantage over tree-based methods in iterative settings in that it is possible to simply update network weights at each iteration, rather than rebuilding the trees entirely.\nAlgorithm 1 Fitted Q-iteration with sampling Input: One-step transitions F = {\u3008snt , ant , snt+1\u3009, rnt+1}n=1:|F|; Regression parameters \u03b8; Action space A; subset size N Initialize Q0(st, at) = 0 \u2200st \u2208 F , at \u2208 A for iteration k = 1\u2192 K do\nsubsetN \u223c F S \u2190 [] for i \u2208 subsetN do\nQk(si, ai)\u2190 ri+1+\u03b3 max a\u2032\u2208A (predict(\u3008si+1, a\u2032\u3009, \u03b8)) S \u2190 append(S, \u3008(si, ai), Q(si, ai)\u3009)\nend \u03b8 \u2190 regress(S)\nend Result: \u03b8 \u03c0 \u2190 classify(\u3008snt , ant \u3009)"}, {"heading": "4 Experimental Results", "text": "After extracting relevant ventilation episodes from ICU admissions in the MIMIC III database (Section 3.1), and splitting these into training and test data, we obtain a total of 1,800 distinct admissions in our training set and 664 admissions in our test set. We interpolate timevarying vitals measurements using Gaussian processes or sample-and-hold interpolation, sampling at 10-minute intervals. This yields of the order of 1.5 million one-step transitions in the training set and 0.5 million in the test set respectively, where each transition is a 32-dimensional representation of patient state.\nAs a baseline, we applied Q-learning to the training data to learn the mapping of continuous states to Q-values, with function approximation using a three-layer feedforward neural network. The network is trained using Adam, an efficient stochastic gradient-based optimizer (Kingma and Ba [2014]), and l2 regularization of weights. Each patient admission k is treated as a distinct episode, with on the order of thousands of state transitions in each; the network weights are incrementally updated following each transition. Studying the change between successive episodes in the predicted Q-values for all state-action pairs in the training set (Figure 4), it is unclear whether the algorithm succeeds in converging within the 1,800 training episodes.\nWe then explored the use of FQI to learn our Q-function, first running with an Extra-Trees for function approximation. In our implementation, each iteration of FQI is performed on a random subset of 10% of all transitions in the training set, as described in Algorithm 1, such that on average, each sample is seen in a tenth of all iterations. Though sampling increases the total number of iterations required for convergence, it yields significant speed-ups in building trees at each iteration, and hence in total training time. The ensemble regressor learns 50 trees, with regularization in the form of a minimum leaf node size of 20 samples. We present here results with FQI performed for a fixed number of 100 iterations, though it is possible to use a convergence criterion of the form \u2206(Qk, Qk\u22121) \u2264 \u03b5 for early stopping, to speed up training further.\nFor comparison, we used he same methods to run FQI with neural networks (NFQ) in place of tree-based regression: we train a feedforward network with architecture and techniques identical to those applied in our function approximation for Q-learning. Convergence of the estimated Q-function for both regressors is measured by the mean change in the estimate Q\u0302 for transitions in the training set (Figure 5) which shows that the algorithm takes roughly 60 iterations to converge in both cases. However, NFQ yields approximately a four-fold gain in runtime speed, as\nexpected, since with neural networks we can simply update weights rather than retraining fully at each iteration.\nThe estimated Q-functions from FQI with Extra-Trees (FQIT) and from NFQ are then used to evaluate the optimal action, i.e. that which maximizes the value of the state-action pair, for each state in the training set. We can then train policy functions \u03c0(s) mapping a given patient state to the corresponding optimal action a \u2208 A. To allow for clinical interpretation of the final policy, we choose to train an Extra-Trees classifier with an ensemble of 100 trees to represent the policy function.\nThe relative importance assigned to the top 24 features in the state space for the policy trees learnt, when training on optimal actions from both FQIT and NFQ, show that the five vitals ranking highest in importance across the two policies are arterial O2 pressure, arterial pH, FiO2, O2 flow and PEEP set (Figure 6). These are as expected\u2014 Arterial pH, FiO2, and PEEP all feature in our preliminary HUP guidelines for extubation criteria, and there is considerable literature suggesting blood gases are an important indicator of readiness for weaning (Hoo [2012]). On the other hand, oxygen saturation pulse oxymetry (SpO2) which is also included in HUP\u2019s current extubation criteria, is fairly low in ranking. This may be because these measurements are highly correlated with other factors in the state space, such as arterial O2 pressure (Collins et al. [2015]), that account for its influence on weaning more directly. The limited importance assigned to heart rate and respiratory rate, which can serve as indicators of blood pressure and blood gases, are also likely to be explained by this dependence between vitals.\nIn terms of demographics, weight and age play a significant role in the weaning policy learnt: weight is likely to influence our sedation policy specifically, as dosages are typically adjusted for patient weight, while age is strongly correlated with a patient\u2019s speed of recovery, and hence the time necessary on ventilator support.\nIn order to evaluate the performance of the policies learnt, we compare the algorithm\u2019s recommendations against the true policy implemented by the hospital. Considering ventilation and sedation separately, the policies learnt with FQIT and NFQ achieve similar accuracies in recommending ventilation (both matching the true policy in roughly 85% of transitions), while FQIT far outperforms NFQ in the case of sedation policy (achieving 58% accuracy compared with just 28%, barely above random assignment of one of four dosage levels), perhaps due to overfitting of the neural network on this task. More data may be necessary to develop a meaningful sedation policy with NFQ.\nWe therefore concentrate further analysis of policy recommendations to those produced by FQIT. We divide the 664 test admissions into six groups according to the fraction of FQI policy actions that differ from the hospital\u2019s policy: \u22060 comprises admissions in which the true and recommended policies agree perfectly, while those in \u22065 show the greatest deviation. Plotting the distribution of the number of reintubations and the mean accumulated reward over patient admissions respectively, for all patients in each set (Figures 7a and 7b), we can see that those admissions in set \u22060 undergo no reintubation, and in general the average number of reintubations increases with deviation from the FQIT policy, with up to seven distinct intubations observed in admissions in \u22065. This effect is emphasised by the trend in mean rewards across the six admission groups, which serve primarily as an indicator of the regulation of vitals within desired ranges and whether certain criteria were met at extubation: mean reward over a set is highest (and the\nrange lowest) for admissions in which the policies match exactly; mean reward decreases with increasing divergence of the two policies. A less distinct but comparable pattern is seen when grouping admissions instead by similarity of the sedation policy to the true dosage levels administered by the hospital (Figures 7c and 7d)."}, {"heading": "5 Conclusion", "text": "In this work, we propose a data-driven approach to the optimization of weaning from mechanical ventilation of patients in the ICU. We model patient admissions as Markov decision processes, developing novel representations of the problem state, action space, and reward function in this framework. Reinforcement learning with fitted Q-iteration using different regressors is then used to learn a simple ventilator weaning policy from examples in historical ICU data. We demonstrate that the algorithm is capable of extracting meaningful indicators for patient readiness and shows promise in recommending extubation time and sedation levels, on average outperforming clinical practice in terms of regulation of vitals and reintubations.\nThere are a number of challenges that must be overcome before these methods can be meaningfully implemented in a clinical setting, however: first, in order to generate robust treatment recommendations, it is important to ensure policy invariance to reward shaping: the current methods display considerable sensitivity to the relative weighting of various components of the feedback received after each transition. A more principled approach to the design of the\nreward function, for example by applying techniques in inverse reinforcement learning (Ng et al. [2000]), can help tackle this sensitivity. In addition, addressing the question of censoring in sub-optimal historical data and explicitly correcting for the bias that arises from the timing of interventions is crucial to fair evaluation of learnt policies, particularly where they deviate from the actions taken by the clinician. Finally, effective communication of the best action, expected reward, and the associated uncertainty, calls for a probabilistic approach to estimation of the Q-function, which can perhaps be addressed by pairing regressors such as Gaussian processes with Fitted Q-iteration.\nPossible directions for future work also include increasing the sophistication of the state space, for example by handling long term effects more explicitly using second-order statistics of vitals, applying techniques in inverse reinforcement learning to feature engineering (as in Levine et al. [2010]), or modeling the system as a partially observable MDP, in which observations map to some underlying state space. Extending the action space to include continuous dosages of specific drug types and settings such as ventilator modes and FiO2 will also facilitate directly actionable policy recommendations. With further efforts to tackle these challenges, the reinforcement learning methods explored here will play a crucial role in helping to inform patient-specific decisions in critical care."}], "references": [{"title": "The difficultto-wean patient", "author": ["Nicolino Ambrosino", "Luciano Gabbrielli"], "venue": "Expert Review of Respiratory Medicine,", "citeRegEx": "Ambrosino and Gabbrielli.,? \\Q2010\\E", "shortCiteRegEx": "Ambrosino and Gabbrielli.", "year": 2010}, {"title": "Icu occupancy and mechanical ventilator use in the united states", "author": ["Hannah Wunsch", "Jason Wagner", "Maximilian Herlim", "David Chong", "Andrew Kramer", "Scott D Halpern"], "venue": "Critical care medicine,", "citeRegEx": "Wunsch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wunsch et al\\.", "year": 2013}, {"title": "Sedation and analgesia in the mechanically ventilated patient", "author": ["Shruti B Patel", "John P Kress"], "venue": "American journal of respiratory and critical care medicine,", "citeRegEx": "Patel and Kress.,? \\Q2012\\E", "shortCiteRegEx": "Patel and Kress.", "year": 2012}, {"title": "Sedation in the intensive care setting", "author": ["Christopher G Hughes", "Stuart McGrane", "Pratik P Pandharipande"], "venue": "Clin Pharmacol,", "citeRegEx": "Hughes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hughes et al\\.", "year": 2012}, {"title": "What is the optimal rate of failed extubation", "author": ["James S Krinsley", "Praveen K Reddy", "Abid Iqbal"], "venue": "Critical Care,", "citeRegEx": "Krinsley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krinsley et al\\.", "year": 2012}, {"title": "Sedation and weaning from mechanical ventilation: Time for \u2018best practice\u2019 to catch up with new realities", "author": ["Giorgio Conti", "Jean Mantz", "Dan Longrois", "Peter Tonner"], "venue": "Multidisciplinary respiratory medicine,", "citeRegEx": "Conti et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Conti et al\\.", "year": 2014}, {"title": "The pulmonary physician in critical care: Difficult weaning", "author": ["J Goldstone"], "venue": "Thorax, 57(11):986\u2013991,", "citeRegEx": "Goldstone.,? \\Q2002\\E", "shortCiteRegEx": "Goldstone.", "year": 2002}, {"title": "Clinical data based optimal sti strategies for hiv: A reinforcement learning approach", "author": ["Damien Ernst", "Guy-Bart Stan", "Jorge Goncalves", "Louis Wehenkel"], "venue": "45th IEEE Conference on Decision and Control,", "citeRegEx": "Ernst et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2006}, {"title": "Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer", "author": ["Yufan Zhao", "Donglin Zeng", "Mark A. Socinski", "Michael R. Kosorok"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Intelligent control of closed-loop sedation in simulated icu patients", "author": ["Brett L Moore", "Eric D Sinzinger", "Todd M Quasny", "Larry D Pyeatt"], "venue": "In FLAIRS Conference,", "citeRegEx": "Moore et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2004}, {"title": "Closedloop control of anesthesia and mean arterial pressure using reinforcement learning", "author": ["R. Padmanabhan", "N. Meskin", "W.M. Haddad"], "venue": "In 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Padmanabhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Padmanabhan et al\\.", "year": 2014}, {"title": "Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach", "author": ["S. Nemati", "M.M. Ghassemi", "G.D. Clifford"], "venue": "In 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),", "citeRegEx": "Nemati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nemati et al\\.", "year": 2016}, {"title": "Can machine learning methods predict extubation outcome in premature infants as well as clinicians", "author": ["Martina Mueller", "Jonas S Almeida", "Romesh Stanislaus", "Carol L Wagner"], "venue": "Journal of neonatal biology,", "citeRegEx": "Mueller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mueller et al\\.", "year": 2013}, {"title": "Improvement in the prediction of ventilator weaning outcomes by an artificial neural network in a medical icu", "author": ["Hung-Ju Kuo", "Hung-Wen Chiu", "Chun-Nin Lee", "Tzu-Tao Chen", "Chih-Cheng Chang", "Mauo-Ying Bien"], "venue": "Respiratory care,", "citeRegEx": "Kuo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kuo et al\\.", "year": 2015}, {"title": "Incorporating association rule networks in feature category-weighted naive bayes model to support weaning decision making", "author": ["Yuanyuan Gao", "Anqi Xu", "Paul Jen-Hwa Hu", "TsangHsiang Cheng"], "venue": "Decision Support Systems,", "citeRegEx": "Gao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2017}, {"title": "Mimic-iii, a freely accessible critical care database", "author": ["Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "Li-wei H Lehman", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark"], "venue": "Scientific data,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Multitask Gaussian processes for multivariate physiological timeseries analysis", "author": ["Robert D\u00fcrichen", "Marco A.F. Pimentel", "Lei Clifton", "Achim Schweikard", "David A. Clifton"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "D\u00fcrichen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "D\u00fcrichen et al\\.", "year": 2015}, {"title": "Sparse Multi-Output Gaussian Processes for Medical Time Series Prediction", "author": ["Li-Fang Cheng", "Gregory Darnell", "Corey Chivers", "Michael Draugelis", "Kai Li", "Barbara Engelhardt"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2017}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Edward Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Kernel-based reinforcement learning", "author": ["Dirk Ormoneit", "\u015aaunak Sen"], "venue": "Machine learning,", "citeRegEx": "Ormoneit and Sen.,? \\Q2002\\E", "shortCiteRegEx": "Ormoneit and Sen.", "year": 2002}, {"title": "Extremely randomized trees", "author": ["Pierre Geurts", "Damien Ernst", "Louis Wehenkel"], "venue": "Machine learning,", "citeRegEx": "Geurts et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geurts et al\\.", "year": 2006}, {"title": "Treebased batch mode reinforcement learning", "author": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Riedmiller.,? \\Q2005\\E", "shortCiteRegEx": "Riedmiller.", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Blood gases, weaning, and extubation", "author": ["Guy W Soo Hoo"], "venue": "Respiratory Care,", "citeRegEx": "Hoo.,? \\Q2012\\E", "shortCiteRegEx": "Hoo.", "year": 2012}, {"title": "Relating oxygen partial pressure, saturation and content: the haemoglobin\u2013 oxygen dissociation", "author": ["Julie-Ann Collins", "Aram Rudenski", "John Gibson", "Luke Howard", "Ronan ODriscoll"], "venue": "curve. Breathe,", "citeRegEx": "Collins et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": null, "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Feature construction for inverse reinforcement learning", "author": ["Sergey Levine", "Zoran Popovic", "Vladlen Koltun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Mechanical ventilation is one of the most widely used interventions in admissions to the intensive care unit (ICU): around 40% of patients in the ICU are supported on invasive mechanical ventilation at any given hour, accounting for 12% of total hospital costs in the United States (Ambrosino and Gabbrielli [2010], Wunsch et al.", "startOffset": 283, "endOffset": 315}, {"referenceID": 0, "context": "Mechanical ventilation is one of the most widely used interventions in admissions to the intensive care unit (ICU): around 40% of patients in the ICU are supported on invasive mechanical ventilation at any given hour, accounting for 12% of total hospital costs in the United States (Ambrosino and Gabbrielli [2010], Wunsch et al. [2013]).", "startOffset": 283, "endOffset": 337}, {"referenceID": 2, "context": "The underlying condition of the patient, as well as factors such as obesity or genetic variations, can have a significant effect on the pharmacology of drugs, and cause high inter-patient variability in response to a given sedative (Patel and Kress [2012]), lending motivation to a personalized approach to sedation strategies.", "startOffset": 233, "endOffset": 256}, {"referenceID": 3, "context": "Prolonged ventilation\u2014and corresponding over-sedation\u2014is associated with post-extubation delirium, drug dependence, ventilator-induced pneumonia, and higher patient mortality rates (Hughes et al. [2012]), in addition to inflating costs and straining hospital resources.", "startOffset": 182, "endOffset": 203}, {"referenceID": 4, "context": "hours can cause severe patient discomfort and result in even longer ICU stays (Krinsley et al. [2012]).", "startOffset": 79, "endOffset": 102}, {"referenceID": 5, "context": "between individuals and subpopulations means there is little agreement in clinical literature on the best weaning protocol (Conti et al. [2014], Goldstone [2002]).", "startOffset": 124, "endOffset": 144}, {"referenceID": 5, "context": "between individuals and subpopulations means there is little agreement in clinical literature on the best weaning protocol (Conti et al. [2014], Goldstone [2002]).", "startOffset": 124, "endOffset": 162}, {"referenceID": 7, "context": "For example, Ernst et al. [2006] applied fitted Q-iteration", "startOffset": 13, "endOffset": 33}, {"referenceID": 8, "context": "Zhao et al. [2011] used Q-learning to learn optimal individualized treatment regimens for nonsmall cell lung cancer.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "On the problem of administering anaesthesia in an ICU setting, Moore et al. [2004] applied Q-learning with eligibility traces to the administration of intravenous propofol, modeling patient dynamics according to an established pharmacokinetic model, with the aim of maintaining some level of sedation or consciousness.", "startOffset": 63, "endOffset": 83}, {"referenceID": 9, "context": "On the problem of administering anaesthesia in an ICU setting, Moore et al. [2004] applied Q-learning with eligibility traces to the administration of intravenous propofol, modeling patient dynamics according to an established pharmacokinetic model, with the aim of maintaining some level of sedation or consciousness. Padmanabhan et al. [2014] also used Q-learning, for the regulation of both sedation level and arterial pressure as an indicator of physiological stability, using propofol infusion rate.", "startOffset": 63, "endOffset": 345}, {"referenceID": 9, "context": "On the problem of administering anaesthesia in an ICU setting, Moore et al. [2004] applied Q-learning with eligibility traces to the administration of intravenous propofol, modeling patient dynamics according to an established pharmacokinetic model, with the aim of maintaining some level of sedation or consciousness. Padmanabhan et al. [2014] also used Q-learning, for the regulation of both sedation level and arterial pressure as an indicator of physiological stability, using propofol infusion rate. All of the aforementioned work rely on model-based approaches to reinforcement learning, and develop treatment policies on simulated patient data. More recently however, Nemati et al. [2016] consider the problem of heparin dosing to maintain blood coagulation levels within some well-defined therapeutic range, modeling the task as a partially observable MDP, using a dynamic Bayesian network trained on real ICU data, and learning a dosing policy with neural fitted Q-iteration (NFQ).", "startOffset": 63, "endOffset": 696}, {"referenceID": 12, "context": "There exists some literature on machine learning methods for the problem of ventilator weaning: Mueller et al. [2013] and Kuo et al.", "startOffset": 96, "endOffset": 118}, {"referenceID": 12, "context": "There exists some literature on machine learning methods for the problem of ventilator weaning: Mueller et al. [2013] and Kuo et al. [2015] look at prediction of weaning outcomes using supervised learning methods, and suggest that classifiers based on neural networks, logistic regression, or naive Bayes, trained on patient ventilator and blood gas data, show promise in predicting successful extubation.", "startOffset": 96, "endOffset": 140}, {"referenceID": 12, "context": "There exists some literature on machine learning methods for the problem of ventilator weaning: Mueller et al. [2013] and Kuo et al. [2015] look at prediction of weaning outcomes using supervised learning methods, and suggest that classifiers based on neural networks, logistic regression, or naive Bayes, trained on patient ventilator and blood gas data, show promise in predicting successful extubation. Gao et al. [2017] develops association rule networks for naive Bayes classifiers, to analyze the discriminative power of different feature categories toward each decision outcome class, in order to help inform clinical decision making.", "startOffset": 96, "endOffset": 424}, {"referenceID": 15, "context": "We use the Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC III) database (Johnson et al. [2016]), a freely available source of de-identified critical care data for 53,423 adult admissions and 7,870 neonates.", "startOffset": 90, "endOffset": 112}, {"referenceID": 17, "context": "We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings.", "startOffset": 26, "endOffset": 46}, {"referenceID": 17, "context": "We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings. We set m(t) = 0 without loss of generality (Rasmussen and Williams [2006]), and \u03ba(t, t\u2032) as the kernel in the linear model of coregionalization with the spectral kernel as the basis kernel, allowing us to model both smooth correlations in time and periodic variations of these vital signs and lab results.", "startOffset": 26, "endOffset": 251}, {"referenceID": 17, "context": "For each patient, one structured GP kernel is estimated using the implementation in Cheng et al. [2017]. We then impute the time series with the estimated posterior mean given all the observations across all chosen physiological signals for that patient.", "startOffset": 84, "endOffset": 104}, {"referenceID": 18, "context": "FQI guarantees convergence for many commonly used regressors, including kernel-based methods (Ormoneit and Sen [2002]) and decision trees.", "startOffset": 94, "endOffset": 118}, {"referenceID": 18, "context": "FQI guarantees convergence for many commonly used regressors, including kernel-based methods (Ormoneit and Sen [2002]) and decision trees. In particular, extremely randomized trees (Extra-Trees: Geurts et al. [2006], Ernst et al.", "startOffset": 94, "endOffset": 216}, {"referenceID": 7, "context": "[2006], Ernst et al. [2005]), a tree-based ensemble method that extends on random forests by introducing randomness in the thresholds chosen at each split, has been applied in the past to learning large or continuous Q-functions in clinical settings (Ernst et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 7, "context": "[2006], Ernst et al. [2005]), a tree-based ensemble method that extends on random forests by introducing randomness in the thresholds chosen at each split, has been applied in the past to learning large or continuous Q-functions in clinical settings (Ernst et al. [2006], Escandell-Montero et al.", "startOffset": 8, "endOffset": 271}, {"referenceID": 7, "context": "[2006], Ernst et al. [2005]), a tree-based ensemble method that extends on random forests by introducing randomness in the thresholds chosen at each split, has been applied in the past to learning large or continuous Q-functions in clinical settings (Ernst et al. [2006], Escandell-Montero et al. [2014]).", "startOffset": 8, "endOffset": 304}, {"referenceID": 21, "context": "Neural Fitted-Q (NFQ, Riedmiller [2005]) on the other hand, looks to leverage the representational power of neural networks as regressors to fitted Q-iteration.", "startOffset": 22, "endOffset": 40}, {"referenceID": 11, "context": "Nemati et al. [2016] use NFQ to learn optimal heparin dosages, mapping the patient hidden state to expected return.", "startOffset": 0, "endOffset": 21}, {"referenceID": 23, "context": "The network is trained using Adam, an efficient stochastic gradient-based optimizer (Kingma and Ba [2014]), and l2 regularization of weights.", "startOffset": 85, "endOffset": 106}, {"referenceID": 24, "context": "These are as expected\u2014 Arterial pH, FiO2, and PEEP all feature in our preliminary HUP guidelines for extubation criteria, and there is considerable literature suggesting blood gases are an important indicator of readiness for weaning (Hoo [2012]).", "startOffset": 235, "endOffset": 246}, {"referenceID": 24, "context": "These are as expected\u2014 Arterial pH, FiO2, and PEEP all feature in our preliminary HUP guidelines for extubation criteria, and there is considerable literature suggesting blood gases are an important indicator of readiness for weaning (Hoo [2012]). On the other hand, oxygen saturation pulse oxymetry (SpO2) which is also included in HUP\u2019s current extubation criteria, is fairly low in ranking. This may be because these measurements are highly correlated with other factors in the state space, such as arterial O2 pressure (Collins et al. [2015]), that account for its influence on weaning more directly.", "startOffset": 235, "endOffset": 546}], "year": 2017, "abstractText": "The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Qiteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.", "creator": "LaTeX with hyperref package"}}}