{"id": "1704.06611", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Making Neural Programming Architectures Generalize via Recursion", "abstract": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system's behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion. The challenge facing the neural architecture is the ability to implement the notion of an embedded learning model on a neural network. We also propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose a system that could, at best, produce the best results for the overall system. In this paper, we propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could, at best, produce the best results for the overall system. In this paper, we propose an alternative approach to recursion. We propose a system that could", "histories": [["v1", "Fri, 21 Apr 2017 16:02:26 GMT  (284kb,D)", "http://arxiv.org/abs/1704.06611v1", "Published in ICLR 2017"]], "COMMENTS": "Published in ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.PL", "authors": ["jonathon cai", "richard shin", "dawn song"], "accepted": true, "id": "1704.06611"}, "pdf": {"name": "1704.06611.pdf", "metadata": {"source": "CRF", "title": "MAKING NEURAL PROGRAMMING ARCHITECTURES GENERALIZE VIA RECURSION", "authors": ["Jonathon Cai", "Richard Shin", "Dawn Song"], "emails": ["jonathon@cs.berkeley.edu", "ricshin@cs.berkeley.edu", "dawnsong@cs.berkeley.edu"], "sections": null, "references": [{"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "CoRR, abs/1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["Lukasz Kaiser", "Ilya Sutskever"], "venue": "CoRR, abs/1511.08228,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2015}, {"title": "Neural random access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "ERCIM News,", "citeRegEx": "Kurach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Pointer networks. In Advances in Neural Information Processing Systems", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zaremba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 203}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 230}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al.", "startOffset": 181, "endOffset": 256}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al. (2016), Zaremba et al.", "startOffset": 181, "endOffset": 278}, {"referenceID": 4, "context": "Thus far, to evaluate the efficacy of neural models on programming tasks, the only metric that has been used is generalization of expected behavior to inputs of greater complexity (Vinyals et al. (2015), Kaiser & Sutskever (2015), Reed & de Freitas (2016), Graves et al. (2016), Zaremba et al. (2016)).", "startOffset": 181, "endOffset": 301}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al.", "startOffset": 40, "endOffset": 112}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al. (2016)).", "startOffset": 40, "endOffset": 157}, {"referenceID": 5, "context": "the single-digit multiplication task in Zaremba et al. (2016), the bubble sort task in Reed & de Freitas (2016), and the graph tasks in Graves et al. (2016)). In this version of curriculum learning, even though the inputs are gradually becoming more complex, the semantics of the program is succinct and does not change. Although the model is exposed to more and more data, it might learn spurious and overly complex representations of the program, as suggested in Zaremba et al. (2016). That is to say, the network does not learn the true program semantics.", "startOffset": 40, "endOffset": 487}, {"referenceID": 3, "context": ", 2014), Neural GPU (Kaiser & Sutskever, 2015), Neural Programmer (Neelakantan et al., 2015), Pointer Network (Vinyals et al.", "startOffset": 66, "endOffset": 92}, {"referenceID": 4, "context": ", 2015), Pointer Network (Vinyals et al., 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), and Neural Random Access Machine (Kurach et al.", "startOffset": 25, "endOffset": 47}, {"referenceID": 2, "context": ", 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), and Neural Random Access Machine (Kurach et al., 2016).", "startOffset": 103, "endOffset": 124}], "year": 2017, "abstractText": "Empirically, neural networks that attempt to learn programs from data have exhibited poor generalizability. Moreover, it has traditionally been difficult to reason about the behavior of these models beyond a certain level of input complexity. In order to address these issues, we propose augmenting neural architectures with a key abstraction: recursion. As an application, we implement recursion in the Neural Programmer-Interpreter framework on four tasks: grade-school addition, bubble sort, topological sort, and quicksort. We demonstrate superior generalizability and interpretability with small amounts of training data. Recursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", "creator": "LaTeX with hyperref package"}}}