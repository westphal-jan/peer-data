{"id": "1704.06767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Risk Minimization Framework for Multiple Instance Learning from Positive and Unlabeled Bags", "abstract": "Multiple instance learning (MIL) is a variation of traditional supervised learning problems where data (referred to as bags) are composed of sub-elements (referred to as instances) and only bag labels are available. MIL has a variety of applications such as content-based image retrieval, text categorization and medical diagnosis. Most of the previous work for MIL assume that the training bags are fully labeled (like the MIL program) and the training bags may be labeled in a number of different colors. In addition, the training bags are often labeled according to the type of food they are being fed. The concept of \u201ctrain\u2021 is that training bags contain information about the training needs of those bags and that the training bags contain information about the training requirements of those bags.\u2021\n\n\n\nThe Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training Bag Training", "histories": [["v1", "Sat, 22 Apr 2017 08:50:19 GMT  (234kb)", "http://arxiv.org/abs/1704.06767v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["han bao", "tomoya sakai", "issei sato", "masashi sugiyama"], "accepted": false, "id": "1704.06767"}, "pdf": {"name": "1704.06767.pdf", "metadata": {"source": "CRF", "title": "Risk Minimization Framework for Multiple Instance Learning from Positive and Unlabeled Bags", "authors": ["Han Bao", "Tomoya Sakai", "Issei Sato", "Masashi Sugiyama"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n06 76\n7v 1\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Multiple instance learning (MIL) [8] is a learning problem with bags and instances. Instances are the same as the traditional feature vectors, while bags are sets of instances. The numbers of instances in bags vary among the bags. Bag labels are defined as follows:\n\u2022 If a given bag contains at least one positive instance, then its label is positive. \u2022 If a given bag contains no positive instances, then its label is negative.\nThis is the basic setup of MIL. The goal of MIL is to predict labels of test bags. Here instance labels are unavailable, which makes MIL more difficult than the ordinary classification setting. MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].\nA lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including\n2 the diverse density [16] and its extension, the expectation-maximization diverse density (EM-DD) [26]. These methods find out an instance close to training positive bags and far from training negative bags, which is referred to as the concept point. This process is carried out by gradient-based search from every training instance, which is computationally inefficient. (2) The methods in the second group are based on discriminative modeling. The multiple-instance support vector machine (MI-SVM) [1] is an approach based on SVMs. Empirical evaluation shows that MI-SVM performs well on MIL datasets, but its optimization problem is non-convex and computationally inefficient. The key-instance support vector machine (KI-SVM) [14] converts MI-SVM into a convex problem by using a discrete vector, but it is still difficult to optimize due to a large number of optimization variables. Ga\u0308rtner et al. [11] introduced set kernels (a.k.a. multiple instance kernels), which are extensions of the standard kernel functions to MIL. The set kernels can be used to construct a standard SVM classifier, which performs well on MIL datasets. The optimization problem in this training procedure is convex and the global solution can be obtained efficiently.\nIn this work, we propose a novel method to construct the multiple instance classifiers only from positive and unlabeled bags, while the above standard approaches to MIL assume that training bags are fully labeled. This problem is called PU-MIL. For example, PU-MIL is considered to be applicable to the following situations:\n\u2022 The situation where it is difficult to obtain an enough amount of labeled data due to the significant labeling cost, such as outlier detection. In outlier detection based on supervised classification, it is often difficult to label all outlier samples, so we can label a few outlier samples as positive and regard the other samples as unlabeled. \u2022 The situation where the true negative labels are essentially unavailable, such as music album recommendation, where we try to predict a music album that attracts a consumer from their purchase history. Each album contains some songs, which can be regarded as multiple instances. Whether the album is attracting the consumer or not corresponds to positive and negative, but only whether the consumer bought (positive) or not (unlabeled) is available4.\nOur contribution in this paper is to propose a novel PU-MIL method based on the risk minimization framework [20]. Together with a linear-in-parameter model, the proposed method is formulated as a convex optimization problem, and the global optimal solution can be computed efficiently. To the best of our knowledge, this is the first convex PU-MIL method (see Table 1). Through experiments, we demonstrate that the proposed method combined with the minimax kernel [1] compares favorably with an existing method.\nThe rest of this paper is structured as follows. In Sect. 2, we review existing methods for PU learning [19,20] and MIL [11], on which our proposed method\n4 The fact that the consumer did not buy an album does not directly mean that the album is not attracting (negative). The consumer might not buy just because he or she did not notice it.\n3\nis based. In Sect. 3, we explain the formulation and optimization algorithm of our proposed method, called the positive and unlabeled set kernel classifier (PUSKC). In Sect. 4, we experimentally compare the proposed method (PU-SKC) with the existing method (puMIL) [24]. Finally, we conclude this work in Sect. 5."}, {"heading": "2 Problem Formulation and Related Work", "text": "In this section, we formulate the problems we discuss in this paper (see Fig. 1) and review related works.\n4"}, {"heading": "2.1 Ordinary Binary Classification", "text": "First, we define some notations. Let x \u2208 Rd be a d-dimensional feature vector and y \u2208 {+1,\u22121} be its corresponding class label. In the ordinary binary classification problem, we construct a binary classifier\nf(x) = sign(g(x)) \u2208 {+1,\u22121},\ng : Rd \u2192 R, (1)\nfrom a training dataset D = {(xi, yi)}Ni=1, where N is the number of training samples. Here we use a linear-in-parameter model for g:\ng(x) = \u03b1\u22a4\u03c6(x) + \u03b2,\nwhere \u22a4 denotes the transpose, \u03b1 \u2208 Rm is an m-dimensional parameter vector, \u03b2 \u2208 R is a bias parameter and \u03c6 : Rd \u2192 Rm is a vector of basis functions. The support vector machine (SVM) [6] is one of the standard methods for training a binary classifier. The optimization problem of SVM is as follows:\nmin \u03b1\u2208Rm,\u03b2\u2208R\n1 2 ||\u03b1||2 + C\nN\u2211\ni=1\nmax { 0, 1\u2212 yi ( \u03b1\u22a4\u03c6(xi) + \u03b2 )} , (2)\nwhere C > 0 is a penalty parameter. This problem can be reformulated as the quadratic programming, which can be solved efficiently."}, {"heading": "2.2 Multiple Instance Learning", "text": "Here we formulate the problem of multiple instance learning (MIL) and review an existing method.\nFormulation Hereafter P(A) denotes the power set5 of A. Let X = {xi|xi \u2208 R\nd}ni=1 \u2208 P(R d)\\\u2205 be a bag containing n instances whose dimensions are d, and Y \u2208 {+1,\u22121} be a bag label corresponding to X . The problem is to construct a binary classifier:\nf(X) = sign(g(X)) \u2208 {+1,\u22121},\ng : P(Rd) \\ \u2205 \u2192 R, (3)\nfrom a fully-labeled training dataset D = {(Xb, Yb)}Nb=1, where N denotes the number of bags in D.\n5 The power set of A is a set of all subsets of A, including \u2205 and A itself. In the MIL setting, bags belong to P(Rd), i.e., bags are composed of some elements in Rd.\n5 Multiple Instance Kernels Ga\u0308rtner et al. [11] proposed set kernels (multiple instance kernels). These kernels map bags (sets of instances) to feature spaces.\nA type of the set kernels called the statistic kernel k\u0303 is defined as follows:\nk\u0303(X,X \u2032) := k(s(X), s(X \u2032)),\nwhere k is an arbitrary kernel function such as the Gaussian kernel, and s is called the statistics. For example, the following minimax statistics is a typical choice:\nsmin-max(X) := [ min x\u2208X x(1), . . . , min x\u2208X x(d), max x\u2208X x(1), . . . , max x\u2208X x(d) ]\u22a4 , (4)\nwhere x(i) is the i-th element of an instance x in the bag X . Ga\u0308rtner et al. [11] showed that the statistic kernel with the minimax statistics (4) for s and the polynomial kernel for k outperforms other set kernels in the standard MIL:\nk\u0303min-max(X,X \u2032) := (smin-max(X) \u22a4smin-max(X \u2032) + 1)\u03c1, (5)\nwhere \u03c1 is a positive integer. The statistic kernel (5) is referred to as the minimax kernel. We can then construct the following set kernel classifier g:\ng(X) = \u03b1\u22a4\u03c6(X) + \u03b2, (6)\n\u03c6(X) =   k\u0303min-max(X,C1) ...\nk\u0303min-max(X,CM )\n  ,\nwhere C1, . . . , CM are kernel centers and M is the number of kernel centers. The standard MIL can be solved by using the classifier (6) with SVM (2)."}, {"heading": "2.3 Learning from Positive and Unlabeled Data", "text": "Here we formulate the binary classification problem of learning from positive and unlabeled instances and review existing methods.\nFormulation We assume positive samples {xPi } NP i=1 and unlabeled samples {xUi } NU i=1 are generated as follows:\n{xPi } NP i=1 i.i.d. \u223c p+(x) := p(x|y = +1),\n{xUi } NU i=1 i.i.d. \u223c p(x) := \u03c0p(x|y = +1) + (1 \u2212 \u03c0)p(x|y = \u22121)\n= \u03c0p+(x) + (1\u2212 \u03c0)p\u2212(x), (7)\nwhere \u03c0 := p(y = +1) is called the class prior. Our objective is to construct a binary classifier (1) only from positive samples and unlabeled samples.\n6 Learning Instance-Level Classifiers From Positive and Unlabeled Data du Plessis et al. [19,20] proposed methods based on the empirical risk minimization framework to learn only from positive and unlabeled samples. In the ordinary binary classification setting, the Bayes optimal classifier g\u2217 minimizes the following misclassification rate:\nR0-1(g) = \u03c0EP [\u21130-1(g(x))] + (1 \u2212 \u03c0)EN [\u21130-1(\u2212g(x))] , (8)\nwhere EP [\u00b7] and EN [\u00b7] denote the expectations over p+(x) and p\u2212(x) respectively and \u21130-1 denotes the zero-one loss:\n\u21130-1(z) =\n{ 0 if z \u2265 0,\n1 otherwise.\nIn practice, the misclassification rate (8) is difficult to optimize since the subgradient of \u21130-1 is always 0 except at z = 0. For this reason, we usually use a surrogate loss function6. Then the risk function R with the surrogate loss function \u2113 is written as\nR(g) := \u03c0EP [\u2113(g(x))] + (1 \u2212 \u03c0)EN [\u2113(\u2212g(x))] . (9)\nSince negative samples are not available in the PU learning setup, let us consider expressing the risk (9) without EN [\u00b7]. By the definition of the unlabeled sample distribution (7), the following holds:\n(1 \u2212 \u03c0)EN [\u2113(\u2212g(x))] = EU [\u2113(\u2212g(x))]\u2212 \u03c0EP [\u2113(\u2212g(x))] .\nSubstituting this into the risk (9), we obtain\nR(g) = \u03c0EP [\u2113(g(x))\u2212 \u2113(\u2212g(x))] + EU [\u2113(\u2212g(x))] . (10)\nIf the surrogate loss function \u2113 satisfies\n\u2113(z)\u2212 \u2113(\u2212z) = \u2212z, (11)\nthe risk (10) can be written as\nR(g) = \u03c0EP [\u2212g(x)] + EU [\u2113(\u2212g(x))] . (12)\nThe risk (12) is convex if the surrogate loss function \u2113 is convex. Convex loss functions such as the squared loss \u2113S, the logistic loss \u2113LL and the double hinge loss \u2113DH satisfy condition (11):\n\u2113S(z) = 1\n4 (z \u2212 1)2 \u2212\n1 4 ,\n\u2113LL(z) = log(1 + exp(\u2212z)),\n\u2113DH(z) = max\n( \u2212z,max ( 0, 1\u2212 z\n2\n)) . (13)\nWe use the risk (12) to obtain a convex formulation of PU-MIL in Sect. 3.\n6 For example, the hinge loss \u2113H(z) = max(\u2212z, 0) and the ramp loss \u2113RH(z) = 1\n2 max(0,min(2, 1\u2212 z)) are commonly used.\n7"}, {"heading": "3 Positive and Unlabeled Set Kernel Classifier", "text": "In this section, we propose a convex method for PU-MIL, named the PU-SKC (positive and unlabeled set kernel classifier)."}, {"heading": "3.1 Multiple Instance Learning from Positive and Unlabeled Bags", "text": "Here we formulate the problem of multiple instance learning from positive and unlabeled bags (PU-MIL).\nThe purpose of PU-MIL is to construct the bag-level classifier (3) from a positively labeled training dataset DP = {(XPb , Yb = +1)} NP b=1 and an unlabeled training dataset DU = {XUb\u2032} NU b\u2032=1, where NP and NU denotes the number of positive bags in DP and the number of unlabeled bags in DU, respectively."}, {"heading": "3.2 Formulation", "text": "As we mentioned in Sect. 2.3, du Plessis et al. [19,20] formulated the PU learning problem in the empirical risk minimization framework. If we use a loss function l(z) such that l(z)\u2212 l(\u2212z) = \u2212z, we have the following objective function:\nJ(g) = \u03c0EP [\u2212g(X)] + EU [\u2113(\u2212g(X))] . (14)\nHere we use a linear-in-parameter model with the set kernel function as a classifier:\ng(X) = \u03b1\u22a4\u03c6(X) + \u03b2, (15)\nwhere \u03c6 is the vector of basis functions:\n\u03c6(X) =   k\u0303min-max(X,X P 1 ) ... k\u0303min-max(X,X P NP ) k\u0303min-max(X,X U 1 ) ...\nk\u0303min-max(X,X U NU )\n  . (16)\nk\u0303min-max is the polynomial minimax kernel (5). Similarly to the standard binary classification, we predict a given bag as positive if g(X) \u2265 0, and as negative if g(X) < 0.\nThe risk (14) together with the bag-level classifier (15) and the l2 regularizer induces the following objective function to be minimized by the sample averages:\nJ\u0302(\u03b1, b) = \u03c0 \u00b7 1\nNP\nNP\u2211\nb=1\n( \u2212\u03b1\u22a4\u03c6(XPb )\u2212 \u03b2 )\n+ 1\nNU\nNU\u2211\nb\u2032=1\n\u2113DH ( \u2212\u03b1\u22a4\u03c6(XUb\u2032 )\u2212 \u03b2 ) + \u03bb\n2 \u03b1\u22a4\u03b1, (17)\n8 where \u03bb \u2265 0 is the regularization parameter. Here we use the double hinge loss \u2113DH (13) since du Plessis et al. [20] reported that it achieved the best performance in the ordinary PU classification setting. Note that \u03c0 is the bag-level class prior, i.e., \u03c0 = p(Y = +1), which must be estimated from the training data. We explain how to estimate it in Sect. 3.3. This problem can be rewritten in the form of quadratic programming by using slack variables \u03be as\nmin \u03b1,\u03b2,\u03be\n\u2212 \u03c0\nNP 1\u22a4\u03a6P\u03b1\u2212 \u03c0\u03b2 +\n1\nNU 1\u22a4\u03be +\n\u03bb 2 \u03b1\u22a4\u03b1 (18)\ns.t. \u03be \u2265 0,\n\u03be \u2265 1\n2 1+\n1 2 \u03a6U\u03b1+ 1 2 \u03b21,\n\u03be \u2265 \u03a6U\u03b1+ \u03b21,\nwhere \u2265 for vectors denotes the element-wise inequality and 0, 1 denote the allzero and all-one vector, respectively. Matrices \u03a6P and \u03a6U are defined as follows:\n\u03a6P =   \u03c6\u22a4(XP1 ) ...\n\u03c6\u22a4(XPNP)\n  , \u03a6U =   \u03c6\u22a4(XU1 ) ...\n\u03c6\u22a4(XUNU)\n  ."}, {"heading": "3.3 Bag-Level Class Prior Estimation", "text": "A bag-level class prior estimation algorithm can be obtained by a simple extension of the instance-level version explained in Appendix A. The difference is basis functions used in the empirical estimator of r(x). We use the polynomial minimax kernel (5) to obtain the bag-level basis functions (16). Then the bag-level class prior \u03c0 can be estimated similarly."}, {"heading": "4 Experiments", "text": "In this section, we experimentally compare the proposed method7 with the existing method (see Appendix B) and give answers to the following research questions:\nQ1: Does the proposed method perform well regardless of the true class prior? Q2: Is the proposed method computational efficient?"}, {"heading": "4.1 Dataset", "text": "We use standard MIL datasets: Musk and Corel8. The details about these benchmark datasets are shown in Table 2.\n7 Implementation is published at https://github.com/levelfour/pumil. 8 http://www.cs.columbia.edu/~andrews/mil/datasets.html\n9\nAlgorithm 1 Generation of Training / Test Sets for Benchmark MIL Datasets\nInput: DP: original positive bags, DN: original negative bags, \u03c0: true bag-level class prior, L: #{labeled positive bags}, U : #{unlabeled bags}, T : #{test bags}\n1: DL \u2282 DP \u22b2 |DL| = L 2: DP := DP \\ DL 3: NPU \u223c B(U + T, \u03c0) 4: NNU := U + T \u2212N P U 5: DU := D \u2032 P(\u2282 DP) \u222a D \u2032 N(\u2282 DN) \u22b2 |D \u2032 P| = N P U , |D \u2032 N| = N N U 6: D\u2032U \u2282 DU \u22b2 |D \u2032\nU| = T 7: DU := DU \\ D \u2032\nU\nOutput: DL \u222a DU: training set, D \u2032 U: test set\nSince these datasets are too small to evaluate PU methods, we augment them to increase the number of bags. Specifically, bags chosen randomly from the original datasets are duplicated and then Gaussian noise with mean zero and variance 0.01 is added to each dimension. In this way, we increase the number of samples in Musk (Musk1 and Musk2) 10 times and Corel (Elephant, Fox and Tiger) 5 times. After this augmentation process, we generate a training set (including labeled positive bags and unlabeled bags) and a test set. This generation process is described in Algorithm 1 (we set L = 20, U = 180, T = 200)."}, {"heading": "4.2 Evaluation Method", "text": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [100, 10\u22123, 10\u22126]. We use the area under the receiver operating characteristics curve (AUC) [5] as the evaluation metric, which is robust to the class imbalance, i.e., the situation where the class prior \u03c0 takes extremely high or low values. AUC is defined as follows:\nAUC := EP [ EN [ I(g(xP) \u2265 g(xN)) ]]\n= 1\u2212 EP [ EN [ I(g(xP) < g(xN)) ]] = 1\u2212 EP [ EN [ \u21130-1(g(x P)\u2212 g(xN)) ]] ,\n10\nwhere I(\u00b7) denotes the indicator function. We define the following risk (called the AUC risk) to be minimized:\nRAUC(g) = EP [ EN [ \u21130-1(g(x P)\u2212 g(xN)) ]] .\nHere we have to calculate the AUC risk only from the positive and unlabeled data. We have the following relation from the definition of the marginal probability:\nEP [ EU [ \u21130-1(g(x P)\u2212 g(xU) ]] = \u03c0EP [ EP\u2032 [ \u21130-1(g(x P)\u2212 g(xP \u2032 )) ]]\n+ (1 \u2212 \u03c0)EP [ EN [ \u21130-1(g(x P)\u2212 g(xN) ]] ,\nwhere EP [\u00b7] = ExP\u223cp+(x) [\u00b7] and EP\u2032 [\u00b7] = ExP\u2032\u223cp+(x) [\u00b7]. Then we obtain the following formulation of the AUC risk only from positive and unlabeled data (PU-AUC risk):\nRPU-AUC(g) = 1\n1\u2212 \u03c0 EP\n[ EU [ \u21130-1(g(x P)\u2212 g(xU)) ]]\n\u2212 \u03c0\n1\u2212 \u03c0 EP\n[ EP\u2032 [ \u21130-1(g(x P)\u2212 g(xP \u2032 )) ]] .\nThis risk is used for the cross-validation."}, {"heading": "4.3 Results", "text": "In this section, we show the experimental results and give answers to the research questions."}, {"heading": "A1: The proposed method outperforms the existing method regardless of the true bag-level class prior.", "text": "Table 3 shows averages with standard deviations of the predictions, which are carried out 50 times under each class prior. Bold faces represent the best methods under each class prior. This is tested by the one-sided t-test with 5% significant level (first the higher method is chosen as the best method, then the other method is checked whether it is competitive or not by the one-sided ttest). As it can be seen from Table 3, PU-SKC outperforms the existing method puMIL [24] under the different class priors. Note that the true class prior in Table 3 means the predefined value for dataset generation (see Algorithm 1), not the estimated class prior during the learning process (see Sect. 3.3)."}, {"heading": "A2: The proposed method is much more computationally efficient than the existing method.", "text": "Next, we compare the execution time between the proposed method and the existing method. The result is shown in Figure 2. This result shows PU-SKC is much more computationally efficient than the existing method.\n11"}, {"heading": "5 Conclusion", "text": "Multiple instance learning has versatile applications such as content-based image retrieval, text categorization and medical diagnosis. In this work, we considered a multiple instance learning problem when only positive bags and unlabeled bags are available, which does not require all training bags to be labeled. We proposed a convex method, PU-SKC, to solve PU multiple instance classification. This method is based on the convex formulation of PU learning [20] and the set kernel [11]. PU-SKC performed better than the existing PU multiple instance classification method [24] in the classifications of the benchmark MIL datasets (drug activity prediction and image annotation). Furthermore, we confirmed the proposed method was much more computationally efficient than the existing method through the experiment."}, {"heading": "B Multiple Instance Learning from Positive and Unlabeled Bags", "text": "Wu et al. [24] proposed an instance-level method for the PU-MIL problem (called puMIL). As discussed in the previous work [1,14], the key instance (the most positive instance) in each bag is important in MIL. This is also the case in PUMIL, but the problem is that we cannot tell the labels of unlabeled bags. To address this issue, Wu et al. [24] introduced the bag confidence, which describes how much confident the given unlabeled bag is negative. Let \u03bdb \u2208 [0, 1] be the confidence of the b-th bag. Then the learning problem of an SVM classifier g(X) = w\u22a4x\u0303 (x\u0303 \u2208 X) under the given \u03bd is formulated as\nmin w\u2208Rd,{x\u0303b} NP+NU b=1\n1 2 ||w||2 + C\nNP+NU\u2211\nb=1\nmax(0, 1\u2212 \u03bdbY\u0303bw \u22a4x\u0303b), (21)\nwhere Y\u0303b is the estimated bag label (Y\u0303b = +1 for a positive bag and Y\u0303b = \u22121 for a possibly negative bag coming from the set of unlabeled bags) and x\u0303b is the key instance in the bag Xb. \u03bdb weighs the contribution of x\u0303b according to its confidence. For positive bags, the confidence is simply set as \u03bdb = 1. The overall training method is summarized as follows:\n1. Initialize L distinct bag confidences \u03bd(1), . . . ,\u03bd(L), where L is the number of the bag confidences. 2. Extract NP possibly negative bags from unlabeled bags. 3. Make a positive margin pool (PMP), which consists of the key instances\nfrom positive bags and possibly negative bags. 4. Solve (21) with using the PMP to obtain an SVM classifier, which is evalu-\nated by the F-measure.\nIn Step 2, unlabeled bags with the highest confidences are extracted and regarded as possibly negative. Bag confidences are initialized randomly and updated via a genetic algorithm (see Step 4 and (23)).\n16\nIn Step 3, first the generative distribution of negative instances is estimated by a weighted kernel density estimator [10]:\np\u0302(x|y = \u22121) = 1\nN\u0303N\nN\u0303N\u2211\nb=1\n\u2211\nx\u2032\u2208X\u0303N b\nk(x, \u03bdbx \u2032), (22)\nwhere N\u0303N is the number of all instances included in the extracted bags in Step 2 and k denotes a kernel function and X\u0303Nb is the b-th possibly negative bag. Using the estimator (22), we can obtain a PMP by extracting the key instances from bags:\nx\u0303b = arg min xb\u2208Xb p\u0302(xb|y = \u22121).\nAfter building the PMP, an SVM classifier can be obtained in Step 4. We evaluate the given bag confidences \u03bd by calculating the F-measures9. Here we have L bag confidences, and we update them by a genetic algorithm. First, the bag confidence with the highest F-measure (denoted by \u03bd\u0302) is cloned to replace the other confidences under the fixed rate c \u2208 [0, 1]. This process, called the mutation, is caused by\nvl = \u03bdl + r(1 \u2212 fl)(\u03bd\u0302 \u2212 \u03bdl), (23)\nwhere r is drawn from the standard normal distribution, \u03bdl is the l-th bag confidence and fl is the F-measure obtained from \u03bdl. \u03bdl is to be replaced for vl if vl produces a better result. We iterate Steps 2 \u2013 4 until the best bag confidence \u03bd\u0302 converges or the number of iterations reaches the predefined limit.\nAfter the above training steps, the best bag confidence \u03bd\u0302 is to be obtained. We use \u03bd\u0302 for the initial bag confidence and iterate Steps 2 \u2013 4 again to obtain a classifier for test prediction10.\nHowever, the process to extract possibly negative bags from unlabeled bags based on the bag confidences (Step 2) is not shown to converge to the optimal solution in the previous work, while this method works well experimentally. In the following section, we use the PU risk minimization framework (12) to formulate PU-MIL.\n9 True bag labels are needed to calculate the F-measure, but those of unlabeled bags are unavailable. This point was not discussed in Wu et al. [24], so we assume that possibly negative bags have negative labels and the other bags in the unlabeled set have positive labels when we calculate the F-measure in our experiments. 10 In test prediction, test bag confidences are not defined. This point was also not discussed in Wu et al. [24], so we set all test bag confidences to 1 in our experiments."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in Neural Information Processing Systems 15. pp. 577\u2013584. MIT Press, Cambridge, MA, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M.H. Yang", "S. Belongie"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition. pp. 983\u2013990", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Model-directed learning of production rules", "author": ["B.G. Buchanan", "T.M. Mitchell"], "venue": "Tech. rep., Stanford, CA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1977}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "Proceedings of the 24th International Conference on Machine Learning. pp. 105\u2013112. ACM, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "AUC optimization vs", "author": ["C. Cortes", "M. Mohri"], "venue": "error rate minimization. In: Proceedings of the 16th International Conference on Neural Information Processing Systems. pp. 313\u2013320. MIT Press, Cambridge, MA, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn. 20(3), 273\u2013297", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "An inductive approach to solving the imperfect theory problem", "author": ["T.G. Dietterich", "N.S. Flann"], "venue": "Proceedings of the American Association for Artificial Intelligence Spring Symposium Series: Explanation-Based Learning. pp. 42\u201346", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence 89(1), 31\u201371", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple-instance learning algorithms for computer-aided detection", "author": ["M.M. Dundar", "G. Fung", "B. Krishnapuram", "R.B. Rao"], "venue": "IEEE Transactions on Biomedical Engineering 55(3), 1015\u20131021", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "MILIS: Multiple instance learning with instance selection", "author": ["Z. Fu", "A. Robles-Kelly", "J. Zhou"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 33(5), 958\u2013977", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-instance kernels", "author": ["T. G\u00e4rtner", "P.A. Flach", "A. Kowalczyk", "A.J. Smola"], "venue": "Proceedings of the 19th International Conference on Machine Learning. pp. 179\u2013 186. Morgan Kaufmann", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Scale and rotation invariant color features for weakly-supervised object learning in 3D space", "author": ["A. Kanezaki", "T. Harada", "Y. Kuniyoshi"], "venue": "International Conference on Computer Vision Workshops. pp. 617\u2013624", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple instance learning for soft bags via top instances", "author": ["W. Li", "N. Vasconcelos"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A convex method for locating regions of interest with multi-instance learning", "author": ["Y.F. Li", "J.T. Kwok", "I.W. Tsang", "Z.H. Zhou"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II. pp. 15\u201330. ECML PKDD \u201909", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Applications of artificial intelligence for organic chemistry : the DENDRAL project", "author": ["R.K. Lindsay"], "venue": "McGraw-Hill Book Co New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1980}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Advances in Neural Information Processing Systems 10. pp. 570\u2013576. MIT Press, Cambridge, MA, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "Proceedings of the 15th International Conference on Machine Learning. pp. 341\u2013349. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Scene recognition and weakly supervised object localization with deformable part-based models", "author": ["M. Pandey", "S. Lazebnik"], "venue": "Proceedings of the 2011 International Conference on Computer Vision. pp. 1307\u20131314. IEEE Computer Society, Washington, DC, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis of learning from positive and unlabeled data", "author": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "Advances in Neural Information Processing Systems 27, pp. 703\u2013711. Curran Associates, Inc.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex formulation for learning from positive and unlabeled data", "author": ["M.C. du Plessis", "G. Niu", "M. Sugiyama"], "venue": "Proceedings of the 32nd International Conference on Machine Learning. pp. 1386\u20131394. JMLR Workshop and Conference Proceedings", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Class prior estimation from positive and unlabeled data", "author": ["M.C. du Plessis", "M. Sugiyama"], "venue": "IEICE Transactions on Information and Systems 97(5), 1358\u20131362", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple instance learning for classification of dementia in brain MRI", "author": ["T. Tong", "R. Wolz", "Q. Gao", "R. Guerrero", "J.V. Hajnal", "D. Rueckert"], "venue": "Medical Image Analysis 18(5), 808\u2013818", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.D. Zucker"], "venue": "Proceedings of the 17th International Conference on Machine Learning. pp. 1119\u20131126. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-instance learning from positive and unlabeled bags", "author": ["J. Wu", "X. Zhu", "C. Zhang", "Z. Cai"], "venue": "Proceeding of the 18th Pacific-Asia Conference on Knowledge Discovery and Data Mining. pp. 237\u2013248. Springer International Publishing", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multiple instance learning for image classification and auto-annotation", "author": ["J. Wu", "Yinan Yu", "Chang Huang", "Kai Yu"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition pp. 3460\u20133469", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "EM-DD: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S.A. Goldman"], "venue": "Advances in Neural Information Processing Systems 13. pp. 1073\u20131080. MIT Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-instance learning by treating instances as non-i.i.d. samples", "author": ["Z.H. Zhou", "Y.Y. Sun", "Y.F. Li"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "On the relation between multi-instance learning and semisupervised learning", "author": ["Z.H. Zhou", "J.M. Xu"], "venue": "Proceedings of the 24th International Conference on Machine Learning. pp. 1167\u20131174. ACM, New York, NY, USA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "Multiple instance learning (MIL) [8] is a learning problem with bags and instances.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 94, "endOffset": 100}, {"referenceID": 14, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 94, "endOffset": 100}, {"referenceID": 7, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 146, "endOffset": 149}, {"referenceID": 16, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 181, "endOffset": 191}, {"referenceID": 12, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 181, "endOffset": 191}, {"referenceID": 24, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 181, "endOffset": 191}, {"referenceID": 1, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 209, "endOffset": 212}, {"referenceID": 17, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 231, "endOffset": 238}, {"referenceID": 11, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 231, "endOffset": 238}, {"referenceID": 0, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 260, "endOffset": 263}, {"referenceID": 8, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 286, "endOffset": 292}, {"referenceID": 21, "context": "MIL is applicable to a wide range of real-world problems such as molecule behavior prediction [3,15], drug activity prediction [8], domain theory [7], content-based image retrieval [17,13,25], visual tracking [2], object detection [18,12], text categorization [1] and medical diagnosis [9,22].", "startOffset": 286, "endOffset": 292}, {"referenceID": 7, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 15, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 25, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 0, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 10, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 22, "context": "A lot of approaches for MIL have been developed [8,16,26,1,11,23], which are classified into two groups in general: (1) The methods in the first group are based on generative modeling, including", "startOffset": 48, "endOffset": 65}, {"referenceID": 15, "context": "the diverse density [16] and its extension, the expectation-maximization diverse density (EM-DD) [26].", "startOffset": 20, "endOffset": 24}, {"referenceID": 25, "context": "the diverse density [16] and its extension, the expectation-maximization diverse density (EM-DD) [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The multiple-instance support vector machine (MI-SVM) [1] is an approach based on SVMs.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "The key-instance support vector machine (KI-SVM) [14] converts MI-SVM into a convex problem by using a discrete vector, but it is still difficult to optimize due to a large number of optimization variables.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "[11] introduced set kernels (a.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Our contribution in this paper is to propose a novel PU-MIL method based on the risk minimization framework [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Through experiments, we demonstrate that the proposed method combined with the minimax kernel [1] compares favorably with an existing method.", "startOffset": 94, "endOffset": 97}, {"referenceID": 18, "context": "2, we review existing methods for PU learning [19,20] and MIL [11], on which our proposed method 4 The fact that the consumer did not buy an album does not directly mean that the album is not attracting (negative).", "startOffset": 46, "endOffset": 53}, {"referenceID": 19, "context": "2, we review existing methods for PU learning [19,20] and MIL [11], on which our proposed method 4 The fact that the consumer did not buy an album does not directly mean that the album is not attracting (negative).", "startOffset": 46, "endOffset": 53}, {"referenceID": 10, "context": "2, we review existing methods for PU learning [19,20] and MIL [11], on which our proposed method 4 The fact that the consumer did not buy an album does not directly mean that the album is not attracting (negative).", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 80, "endOffset": 83}, {"referenceID": 27, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "Positive and Negative set kernels [11] sMIL [4] KI-SVM [14] miGraph [27] MI-SVM [1] MissSVM [28] soft-bag SVM [13] dMIL [25]", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "3) puMIL [24]", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "4, we experimentally compare the proposed method (PU-SKC) with the existing method (puMIL) [24].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "The support vector machine (SVM) [6] is one of the standard methods for training a binary classifier.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "[11] proposed set kernels (multiple instance kernels).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] showed that the statistic kernel with the minimax statistics (4) for s and the polynomial kernel for k outperforms other set kernels in the standard MIL:", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19,20] proposed methods based on the empirical risk minimization framework to learn only from positive and unlabeled samples.", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[19,20] proposed methods based on the empirical risk minimization framework to learn only from positive and unlabeled samples.", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[19,20] formulated the PU learning problem in the empirical risk minimization framework.", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[19,20] formulated the PU learning problem in the empirical risk minimization framework.", "startOffset": 0, "endOffset": 7}, {"referenceID": 19, "context": "[20] reported that it achieved the best performance in the ordinary PU classification setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 186, "endOffset": 195}, {"referenceID": 1, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 186, "endOffset": 195}, {"referenceID": 2, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 186, "endOffset": 195}, {"referenceID": 9, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 204, "endOffset": 216}, {"referenceID": 9, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 204, "endOffset": 216}, {"referenceID": 9, "context": "Hyperparameters (the degree parameter \u03c1 in the polynomial kernel (5) and the regularization parameter \u03bb in the objective function (17)) are selected via 5-fold cross-validation from \u03c1 \u2208 [1, 2, 3] and \u03bb \u2208 [10, 10, 10].", "startOffset": 204, "endOffset": 216}, {"referenceID": 4, "context": "We use the area under the receiver operating characteristics curve (AUC) [5] as the evaluation metric, which is robust to the class imbalance, i.", "startOffset": 73, "endOffset": 76}, {"referenceID": 23, "context": "As it can be seen from Table 3, PU-SKC outperforms the existing method puMIL [24] under the different class priors.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "dataset true class prior PU-SKC puMIL [24] Musk1 0.", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "This method is based on the convex formulation of PU learning [20] and the set kernel [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "This method is based on the convex formulation of PU learning [20] and the set kernel [11].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "PU-SKC performed better than the existing PU multiple instance classification method [24] in the classifications of the benchmark MIL datasets (drug activity prediction and image annotation).", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "Multiple instance learning (MIL) is a variation of traditional supervised learning problems where data (referred to as bags) are composed of sub-elements (referred to as instances) and only bag labels are available. MIL has a variety of applications such as content-based image retrieval, text categorization and medical diagnosis. Most of the previous work for MIL assume that the training bags are fully labeled. However, it is often difficult to obtain an enough number of labeled bags in practical situations, while many unlabeled bags are available. A learning framework called PU learning (positive and unlabeled learning) can address this problem. In this paper, we propose a convex PU learning method to solve an MIL problem. We experimentally show that the proposed method achieves better performance with significantly lower computational costs than an existing method for PU-MIL.", "creator": "LaTeX with hyperref package"}}}