{"id": "1704.07489", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Multi-Task Video Captioning with Video and Entailment Generation", "abstract": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks: a sequence to read a single video, and a sequence to read the corresponding text. We demonstrate the three new methods for visualizing multiple transcriptions of transcripts with the two new techniques.\n\n\n\n\n\n\n\n\n\n\n\n\nThe task of annotating a text with the two new methods (COD) presents a problem when both encoding and decoding an encoded text are encoded. The two new methods help researchers decode audio encoded text from all segments of the text with the two new methods. It is thus possible to model the transcriptions independently by comparing two previously described processes by encoding and decoding one previously described process in which decoding and decoding a text were encoded, and, for the same reason, comparing each other before each other. One task is then used to decode both the two new methods to be executed at the same time, in both cases, with the same parameters. The first method is used to decode each text, while the second is used to decode one previously described process in which decoding and decoding both text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded text and encoded", "histories": [["v1", "Mon, 24 Apr 2017 23:07:32 GMT  (4376kb)", "https://arxiv.org/abs/1704.07489v1", "Accepted at ACL 2017 (13 pages w/ supplementary)"], ["v2", "Tue, 8 Aug 2017 17:08:58 GMT  (5518kb)", "http://arxiv.org/abs/1704.07489v2", "ACL 2017 (14 pages w/ supplementary)"]], "COMMENTS": "Accepted at ACL 2017 (13 pages w/ supplementary)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["ramakanth pasunuru", "mohit bansal"], "accepted": true, "id": "1704.07489"}, "pdf": {"name": "1704.07489.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mbansal}@cs.unc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n07 48\n9v 2\n[ cs\n.C L\n] 8\nA ug\n2 01\n7\nVideo captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-ofthe-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task."}, {"heading": "1 Introduction", "text": "Video captioning is the task of automatically generating a natural language description of the content of a video, as shown in Fig. 1. It has various applications such as assistance to a visually impaired person and improving the quality of online video search or retrieval. This task has gained recent momentum in the natural language processing and computer vision communities, esp. with the advent of powerful image processing features as well as sequence-to-sequence LSTMmodels. It\nis also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics.\nPrevious work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to \u2018translate\u2019 the video to a caption. Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models. Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a). More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).\nDespite these recent improvements, video captioning models still suffer from the lack of sufficient temporal and logical supervision to be able to correctly capture the action sequence and storydynamic language in videos, esp. in the case of short clips. Hence, they would benefit from incorporating such complementary directed knowledge, both visual and textual. We address this by jointly training the task of video captioning with two related directed-generation tasks: a temporally-\ndirected unsupervised video prediction task and a logically-directed language entailment generation task. We model this via many-to-many multi-task learning based sequence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.\nThe unsupervised video prediction task, i.e., video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task\u2019s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence. The entailment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailed caption representations, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by (or follow from) the full video content. The overall many-tomany multi-task model combines all three tasks.\nOur three novel multi-task models show statistically significant improvements over the state-ofthe-art, and achieve the best-reported results (and rank) on multiple datasets, based on several automatic and human evaluations. We also demonstrate that video captioning, in turn, gives mutual improvements on the new multi-reference entailment generation task."}, {"heading": "2 Related Work", "text": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder. To harness the important frame sequence temporal ordering, Venugopalan et al. (2015a) proposed a sequence-to-sequence model with video encoder and language decoder RNNs.\nMore recently, Venugopalan et al. (2016) explored linguistic improvements to the caption decoder by fusing it with external language models. Moreover, an attention or alignment mechanism was added between the encoder and the decoder\nto learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a). In contrast to static visual features, Yao et al. (2015) also considered temporal video features from a 3D-CNN model pretrained on an action recognition task.\nTo explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments. Yu et al. (2016)\u2019s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph. Pan et al. (2016b) proposed to simultaneously learn the RNN word probabilities and a visual-semantic joint embedding space that enforces the relationship between the semantics of the entire sentence and the visual content. Despite these useful recent improvements, video captioning still suffers from limited supervision and generalization capabilities, esp. given the complex action-based temporal and story-based logical dynamics that need to be captured from short video clips. Our work addresses this issue by bringing in complementary temporal and logical knowledge from video prediction and textual entailment generation tasks (respectively), and training them together via many-to-many multi-task learning.\nMulti-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daume\u0301 III, 2012). Recently, Luong et al. (2016) combined multi-task learning with sequence-to-sequence models, sharing parameters across the tasks\u2019 encoders and decoders. They showed improvements on machine translation using parsing and image captioning. We additionally incorporate an attention mechanism to this many-to-many multi-task learning approach and improve the multimodal, temporal-logical video captioning task by sharing its video encoder with the encoder of a video-to-video prediction task and by sharing its caption decoder with the decoder of a linguistic premise-to-entailment generation task.\nImage representation learning has been successful via supervision from very large object-labeled datasets. However, similar amounts of supervision are lacking for video representation learning. Srivastava et al. (2015) address this by propos-\ning unsupervised video representation learning via sequence-to-sequence RNN models, where they reconstruct the input video sequence or predict the future sequence. We model video generation with an attention-enhanced encoder-decoder and harness it to improve video captioning.\nThe task of recognizing textual entailment (RTE) is to classify whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (neutral), which is helpful for several downstream NLP tasks. The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014). However, directly generating the entailed hypothesis sentences given a premise sentence would be even more beneficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs. Recently, Kolesnyk et al. (2016) tried a sequenceto-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation. We modify the SNLI corpus to a new multi-reference (and a more challenging zero train-test premise overlap) setting, and present a novel multi-task training setup with the related video captioning task (where the caption is also entailed by the video), showing mutual improvements on both the tasks."}, {"heading": "3 Models", "text": "We first discuss a simple encoder-decoder model as a baseline reference for video captioning. Next, we improve this via an attention mechanism. Finally, we present similar models for the unsupervised video prediction and entailment generation tasks, and then combine them with video captioning via the many-to-many multi-task approach."}, {"heading": "3.1 Baseline Sequence-to-Sequence Model", "text": "Our baseline model is similar to the standard machine translation encoder-decoder RNN\nmodel (Sutskever et al., 2014) where the final state of the encoder RNN is input as an initial state to the decoder RNN, as shown in Fig. 2. The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long sequences due to forget-style gates (Hochreiter and Schmidhuber, 1997). For video captioning, our input to the encoder is the video frame features1 {f1, f2, ..., fn} of length n, and the caption word sequence {w1, w2, ..., wm} of length m is generated during the decoding phase. The distribution of the output sequence w.r.t. the input sequence is:\np(w1, ..., wm|f1, ..., fn) = m\u220f\nt=1\np(wt|h d t ) (1)\nwhere hdt is the hidden state at the t th time step of the decoder RNN, obtained from hdt\u22121 and wt\u22121 via the standard LSTM-RNN equations. The distribution p(wt|h d t ) is given by softmax over all the words in the vocabulary."}, {"heading": "3.2 Attention-based Model", "text": "Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as the encoder and a unidirectional LSTMRNN as the decoder, see Fig. 3. At each time step t, the decoder LSTM hidden state hdt is a nonlinear recurrent function of the previous decoder hidden state hdt\u22121, the previous time-step\u2019s generated word wt\u22121, and the context vector ct:\nhdt = S(h d t\u22121, wt\u22121, ct) (2)\n1We use several popular image features such as VGGNet, GoogLeNet and Inception-v4. Details in Sec. 4.1.\nwhere ct is a weighted sum of encoder hidden states {hei}:\nct =\nn\u2211\ni=1\n\u03b1t,ih e i (3)\nThese attention weights {\u03b1t,i} act as an alignment mechanism by giving higher weights to certain encoder hidden states which match that decoder time step better, and are computed as:\n\u03b1t,i = exp(et,i)\u2211n\nk=1 exp(et,k) (4)\nwhere the attention function et,i is defined as:\net,i = w T tanh(W eah e i +W d ah d t\u22121 + ba) (5)\nwhere w,W ea ,W d a , and ba are learned parameters. This attention-based sequence-to-sequence model (Fig. 3) is our enhanced baseline for video captioning. We next discuss similar models for the new tasks of unsupervised video prediction and entailment generation and then finally share them via multi-task learning."}, {"heading": "3.3 Unsupervised Video Prediction", "text": "We model unsupervised video representation by predicting the sequence of future video frames given the current frame sequence. Similar to Sec. 3.2, a bidirectional LSTM-RNN encoder and an LSTM-RNN decoder is used, along with attention. If the frame level features of a video of length n are {f1, f2, ..., fn}, these are divided into two sets such that given the current frames {f1, f2, .., fk} (in its encoder), the model has to predict (decode) the rest of the frames {fk+1, fk+2, .., fn}. The motivation is that this\nhelps the video encoder learn rich temporal representations that are aware of their action-based context and are also robust to missing frames and varying frame lengths or motion speeds. The optimization function is defined as:\nminimize \u03c6\nn\u2212k\u2211\nt=1\n||fdt \u2212 ft+k|| 2 2 (6)\nwhere \u03c6 are the model parameters, ft+k is the true future frame feature at decoder time step t and fdt is the decoder\u2019s predicted future frame feature at decoder time step t, defined as:\nfdt = S(h d t\u22121, f d t\u22121, ct) (7)\nsimilar to Eqn. 2, with hdt\u22121 and f d t\u22121 as the previous time step\u2019s hidden state and predicted frame feature respectively, and ct as the attentionweighted context vector."}, {"heading": "3.4 Entailment Generation", "text": "Given a sentence (premise), the task of entailment generation is to generate a sentence (hypothesis) which is a logical deduction or implication of the premise. Our entailment generation model again uses a bidirectional LSTM-RNN encoder and LSTM-RNN decoder with an attention mechanism (similar to Sec. 3.2). If the premise sp is a sequence of words {wp1 , w p 2, ..., w p n} and the hypothesis sh is {wh1 , w h 2 , ..., w h m}, the distribution of the entailed hypothesis w.r.t. the premise is:\np(wh1 , ..., w h m|w p 1, ..., w p n) =\nm\u220f\nt=1\np(wht |h d t ) (8)\nwhere the distribution p(wht |h d t ) is again obtained via softmax over all the words in the vocabulary and the decoder state hdt is similar to Eqn. 2."}, {"heading": "3.5 Multi-Task Learning", "text": "Multi-task learning helps in sharing information between different tasks and across domains. Our primary aim is to improve the video captioning model, where visual content translates to a textual form in a directed (entailed) generation way. Hence, this presents an interesting opportunity to share temporally and logically directed knowledge with both visual and linguistic generation tasks. Fig. 4 shows our overall many-to-many multi-task model for jointly learning video captioning, unsupervised video prediction, and textual entailment generation. Here, the video captioning task shares its video encoder (parameters) with the encoder of the video prediction task (one-to-many setting) so as to learn context-aware and temporally-directed visual representations (see Sec. 3.3).\nMoreover, the decoder of the video captioning task is shared with the decoder of the textual entailment generation task (many-to-one setting), thus helping generate captions that can be \u2018entailed\u2019 by, i.e., are logically implied by or follow from the video content (see Sec. 3.4).2 In both the one-to-many and the many-to-one settings, we also allow the attention parameters to be shared or separated. The overall many-to-many setting thus improves both the visual and language representations of the video captioning model.\nWe train the multi-task model by alternately optimizing each task in mini-batches based on a mixing ratio. Let \u03b1v, \u03b1f , and \u03b1e be the number of mini-batches optimized alternately from each of these three tasks \u2013 video captioning, unsupervised video future frames prediction, and entailment generation, resp. Then the mixing ratio is defined as \u03b1v(\u03b1v+\u03b1f+\u03b1e) : \u03b1f (\u03b1v+\u03b1f+\u03b1e) : \u03b1e(\u03b1v+\u03b1f+\u03b1e) ."}, {"heading": "4 Experimental Setup", "text": ""}, {"heading": "4.1 Datasets", "text": "Video Captioning Datasets We report results on three popular video captioning datasets. First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con-\n2Empirically, logical entailment helped captioning more than simple fusion with language modeling (i.e., partial sentence completion with no logical implication), because a caption is also \u2018entailed\u2019 by a video in a logically-directed sense and hence the entailment generation task matches the video captioning task better than language modeling. Moreover, a multi-task setup is more suitable to add directed information such as entailment (as opposed to pretraining or fusion with only the decoder). Details in Sec. 5.1.\ntains 1970 YouTube videos in the wild with several different reference captions per video (40 on average). We also use MSR-VTT (Xu et al., 2016) with 10, 000 diverse video clips (from a video search engine) \u2013 it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al., 2015) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible. We use the standard splits for all three datasets. Further details about all these datasets are provided in the supplementary.\nVideo Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions. We use the standard splits \u2013 further details in supplementary.\nEntailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral. It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entailment pairs, and we use this subset in our multi-task video captioning model. For improving video captioning, we use the same training/validation/test splits as provided by Bowman et al. (2015), which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).\nHowever, for the entailment generation multitask results (see results in Sec. 5.3), we modify the splits so as to create a multi-reference setup which can afford evaluation with automatic metrics. A given premise usually has multiple entailed hypotheses but the original SNLI corpus is set up as single-reference (for classification). Due to this, the different entailed hypotheses of the same premise land up in different splits of the dataset (e.g., one in train and one in test/validation) in many cases. Therefore, we regroup the premiseentailment pairs and modify the split as follows: among the 190, 113 premise-entailment pairs subset of the SNLI corpus, there are 155, 898 unique premises; out of which 145, 822 have only one hy-\npothesis and we make this the training set, and the rest of them (10, 076) have more than one hypothesis, which we randomly shuffle and divide equally into test and validation sets, so that each of these two sets has approximately the same distribution of the number of reference hypotheses per premise.\nThese new validation and test sets hence contain premises with multiple entailed hypotheses as ground truth references, thus allowing for automatic metric evaluation, where differing generations still get positive scores by matching one of the multiple references. Also, this creates a more challenging dataset for entailment generation because of zero premise overlap between the training and val/test sets. We will make these split details publicly available.\nPre-trained Visual Frame Features For the three video captioning and UCF-101 datasets, we fix our sampling rate to 3fps to bring uniformity in the temporal representation of actions across all videos. These sampled frames are then converted into features using several stateof-the-art pre-trained models on ImageNet (Deng et al., 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016). Details of these feature dimensions and layer positions are in the supplementary."}, {"heading": "4.2 Evaluation (Automatic and Human)", "text": "For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004). Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms. We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.3\nWe also present human evaluation results based\n3We use avg. of these four metrics on validation set to choose the best model, except for single-reference M-VAD dataset where we only report and choose based on METEOR.\non relevance (i.e., how related is the generated caption w.r.t. the video contents such as actions, objects, and events; or is the generated hypothesis entailed or implied by the premise) and coherence (i.e., a score on the logic, readability, and fluency of the generated sentence)."}, {"heading": "4.3 Training Details", "text": "We tune all hyperparameters on the dev splits: LSTM-RNN hidden state size, learning rate, weight initializations, and mini-batch mixing ratios (tuning ranges in supplementary). We use the following settings in all of our models (unless otherwise specified): we unroll video encoder/decoder RNNs to 50 time steps and language encoder/decoder RNNs to 30 time steps. We use a 1024-dimension RNN hidden state size and 512-dim vectors to embed visual features and word vectors. We use Adam optimizer (Kingma and Ba, 2015). We apply a dropout of 0.5. See subsections below and supp for full details."}, {"heading": "5 Results and Analysis", "text": ""}, {"heading": "5.1 Video Captioning on YouTube2Text", "text": "Table 1 presents our primary results on the YouTube2Text (MSVD) dataset, reporting several previous works, all our baselines and attention model ablations, and our three multi-task models, using the four automated evaluation metrics. For each subsection below, we have reported the important training details inline, and refer to the supplementary for full details (e.g., learning rates and initialization).\nBaseline Performance We first present all our baseline model choices (ablations) in Table 1. Our baselines represent the standard sequence-tosequence model with three different visual feature types as well as those with attention mechanisms. Each baseline model is trained with three random seed initializations and the average is reported (for stable results). The final baseline model \u2297 instead uses an ensemble (E), which is a standard denoising method (Sutskever et al., 2014) that performs inference over ten randomly initialized models, i.e., at each time step t of the decoder, we generate a word based on the avg. of the likelihood probabilities from the ten models. Moreover, we use beam search with size 5 for all baseline models. Overall, the final baseline model with Inceptionv4 features, attention, and 10-ensemble performs\nwell (and is better than all previous state-of-theart), and so we next add all our novel multi-task models on top of this final baseline.\nMulti-Task with Video Prediction (1-to-M) Here, the video captioning and unsupervised video prediction tasks share their encoder LSTM-RNN weights and image embeddings in a one-to-many multi-task setting. Two important hyperparameters tuned (on the validation set of captioning datasets) are the ratio of encoder vs decoder frames for video prediction on UCF-101 (where we found that 80% of frames as input and 20% for prediction performs best); and the mini-batch mixing ratio between the captioning and video prediction tasks (where we found 100 : 200 works well). Table 1 shows a statistically significant improvement4 in all metrics in comparison to the best baseline (non-multitask) model as well as w.r.t. all previous works, demonstrating the effectiveness of multi-task learning for video captioning with video prediction, even with unsupervised signals.\nMulti-Task with Entailment Generation (Mto-1) Here, the video captioning and entailment generation tasks share their language decoder LSTM-RNN weights and word embeddings in a many-to-one multi-task setting. We observe\n4Statistical significance of p < 0.01 for CIDEr-D and ROUGE-L, p < 0.02 for BLEU-4, p < 0.03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.\nthat a mixing ratio of 100 : 50 alternating minibatches (between the captioning and entailment tasks) works well here. Again, Table 1 shows statistically significant improvements5 in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task setting. Note that in our initial experiments, our entailment generation model helped the video captioning task significantly more than the alternative approach of simply improving fluency by adding (or deep-fusing) an external language model (or pretrained word embeddings) to the decoder (using both in-domain and out-of-domain language models), again because a caption is also \u2018entailed\u2019 by a video in a logically-directed sense and hence this matches our captioning task better (also see results of Venugopalan et al. (2016) in Table 1).\nMulti-Task with Video and Entailment Generation (M-to-M) Combining the above one-tomany and many-to-one multi-task learning models, our full model is the 3-task, many-to-many model (Fig. 4) where both the video encoder and the language decoder of the video captioning model are shared (and hence improved) with that of the unsupervised video prediction and entailment generation models, respectively.6 A mixing ratio of 100 : 100 : 50 alternate mini-batches\n5Statistical significance of p < 0.01 for all four metrics. 6We found the setting with unshared attention parameters to work best, likely because video captioning and video prediction prefer very different alignment distributions.\nof video captioning, unsupervised video prediction, and entailment generation, resp. works well. Table 1 shows that our many-to-many multi-task model again outperforms our strongest baseline (with statistical significance of p < 0.01 on all metrics), as well as all the previous state-of-theart results by large absolute margins on all metrics. It also achieves significant improvements on some metrics over the one-to-many and many-toone models.7 Overall, we achieve the best results to date on YouTube2Text (MSVD) on all metrics."}, {"heading": "5.2 Video Captioning on MSR-VTT, M-VAD", "text": "In Table 2, we also train and evaluate our final many-to-many multi-task model on two other video captioning datasets (using their standard splits; details in supplementary). First, we evaluate on the new MSR-VTT dataset (Xu et al., 2016). Since this is a recent dataset, we list previous works\u2019 results as reported by the MSR-VTT dataset paper itself.8 We improve over all of these significantly. Moreover, they maintain a leaderboard9 on this dataset and we also report the top 3 systems from it. Based on their ranking method, our multi-task model achieves the new rank 1 on this leaderboard. In Table 3, we further evaluate our model on the challenging movie-based M-VAD dataset, and again achieve improvements over all previous work (Venugopalan et al., 2015a;\n7Many-to-many model\u2019s improvements have a statistical significance of p < 0.01 on all metrics w.r.t. baseline, and p < 0.01 on CIDEr-D w.r.t. both one-to-many and many-toone models, and p < 0.04 on METEOR w.r.t. one-to-many.\n8In their updated supplementary at https: //www.microsoft.com/en-us/research/wp-content/ uploads/2016/10/cvpr16.supplementary.pdf 9 http://ms-multimedia-challenge.com/leaderboard\nPan et al., 2016a; Yao et al., 2015).10"}, {"heading": "5.3 Entailment Generation Results", "text": "Above, we showed that the new entailment generation task helps improve video captioning. Next, we show that the video captioning task also inversely helps the entailment generation task. Given a premise, the task of entailment generation is to generate an entailed hypothesis. We use only the entailment pairs subset of the SNLI corpus for this, but with a multi-reference split setup to allow automatic metric evaluation and a zero traintest premise overlap (see Sec. 4.1). All the hyperparameter details (again tuned on the validation set) are presented in the supplementary. Table 4 presents the entailment generation results for the baseline (sequence-to-sequence with attention, 3- ensemble, beam search) and the multi-task model which uses video captioning (shared decoder) on top of the baseline. A mixing ratio of 100 : 20 alternate mini-batches of entailment generation and video captioning (resp.) works well.11 The multitask model achieves stat. significant (p < 0.01) improvements over the baseline on all metrics, thus demonstrating that video captioning and entailment generation both mutually help each other."}, {"heading": "5.4 Human Evaluation", "text": "In addition to the automated evaluation metrics, we present pilot-scale human evaluations on the YouTube2Text (Table 1) and entailment generation (Table 4) results. In each case, we compare our strongest baseline with our final multi-task model (M-to-M in case of video captioning and M-to-1 in case of entailment generation). We evaluate a random sample of 300 generated captions (or entailed hypotheses) from the test set, across three human evaluators. We remove the model identity to anonymize the two models, and ask the human evaluators to choose the better model based on relevance and coherence (described in Sec. 4.2). As shown in Table 5 and Table 6,\n10Following previous work, we only use METEOR because M-VAD only has a single reference caption per video.\n11Note that this many-to-one model prefers a different mixing ratio and learning rate than the many-to-one model for improving video captioning (Sec. 5.1), because these hyperparameters depend on the primary task being improved, as also discussed in previous work (Luong et al., 2016).\nthe multi-task models are always better than the strongest baseline for both video captioning and entailment generation, on both relevance and coherence, and with similar improvements (2-7%) as the automatic metrics (shown in Table 1)."}, {"heading": "5.5 Analysis", "text": "Fig. 5 shows video captioning generation results on the YouTube2Text dataset where our final M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than\nthe baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly. Overall, we find that the multi-task model generates captions that are better at both temporal action prediction and logical entailment (i.e., correct subset of full video premise) w.r.t. the ground truth captions. The supplementary also provides ablation examples of improvements by the 1-to-M video prediction based multi-task model alone, as well as by the M-to-1 entailment based multi-task model alone (over the baseline).\nOn analyzing the cases where the baseline is better than the final M-to-M multi-task model, we find that these are often scenarios where the multitask model\u2019s caption is also correct but the baseline caption is a bit more specific, e.g., \u201ca man is holding a gun\u201d vs \u201ca man is shooting a gun\u201d.\nFinally, Table 7 presents output examples of our entailment generation multi-task model (Sec. 5.3), showing how the model accurately learns to produce logically implied subsets of the premise."}, {"heading": "6 Conclusion", "text": "We presented a multimodal, multi-task learning approach to improve video captioning by incorporating temporally and logically directed knowledge via video prediction and entailment generation tasks. We achieve the best reported results (and rank) on three datasets, based on multiple automatic and human evaluations. We also showmutual multi-task improvements on the new entailment generation task. In future work, we are applying our entailment-based multi-task paradigm\nto other directed language generation tasks such as image captioning and document summarization."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This work was partially supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Research Grant, and NVidia GPU awards.\nSupplementary Material"}, {"heading": "A Experimental Setup", "text": "A.1 Datasets\nA.1.1 Video Captioning Datasets\nYouTube2Text or MSVD The Microsoft Research Video Description Corpus (MSVD) or YouTube2Text (Chen and Dolan, 2011) is used for our primary video captioning experiments. It has 1970 YouTube videos in the wild with many diverse captions in multiple languages for each video. Caption annotations to these videos are collected using Amazon Mechanical Turk (AMT). All our experiments use only English captions. On average, each video has 40 captions, and the overall dataset has about 80, 000 unique video-caption pairs. The average clip duration is roughly 10 seconds. We used the standard split as stated in Venugopalan et al. (2015a), i.e., 1200 videos for training, 100 videos for validation, and 670 for testing.\nMSR-VTT MSR-VTT is a recent collection of 10, 000 video clips of 41.2 hours duration (i.e., average duration of 15 seconds), which are annotated by AMT workers. It has 200, 000 video clip-sentence pairs covering diverse content from a commercial video search engine. On average, each clip is annotated with 20 natural language captions. We used the standard split as provided in (Xu et al., 2016), i.e., 6, 513 video clips for training, 497 for validation, and 2, 990 for testing.\nM-VAD M-VAD is a movie description dataset with 49, 000 video clips collected from 92movies, with the average clip duration being 6 seconds. Alignment of descriptions to video clips is done through an automatic procedure using Descriptive Video Service (DVS) provided for the movies. Each video clip description has only 1 or 2 sentences, making most evaluation metrics (except paraphrase-based METEOR) infeasible. Again,\nwe used the standard train/val/test split as provided in Torabi et al. (2015).\nA.1.2 Video Prediction Dataset\nFor our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories and with an average clip length of 7.21 seconds each. This dataset suits our video captioning task well because both contain short video clips of a single action or few actions, and hence using future frame prediction on UCF-101 helps learn more robust and context-aware video representations for our short clip video captioning task. We use the standard split of 9, 500 videos for training (we don\u2019t need any validation set in our setup because we directly tune on the validation set of the video captioning task).\nA.2 Pre-trained Visual Frame Features\nFor the three video captioning datasets (Youtube2Text, MSR-VTT, M-VAD) and the unsupervised video prediction dataset (UCF-101), we fix our sampling rate to 3fps to bring uniformity in the temporal representation of actions across all videos. These sampled frames are then converted into features using several state-of-theart pre-trained models on ImageNet (Deng et al., 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016). For VGGNet, we use its fc7 layer features with dimension 4096. For GoogLeNet and Inception-v4, we use the layer before the fully connected layer with dimensions 1024 and 1536, respectively. We follow standard preprocessing and convert all the natural language descriptions to lower case and tokenize the sentences and remove punctuations."}, {"heading": "B Training Details", "text": "In all of our experiments, we tune all the model hyperparameters on validation (development) set of the corresponding dataset. We consider the following short hyperparameters ranges and tune lightly on: LSTM-RNN hidden state size - {256, 512, 1024}; learning rate in the range [10\u22125, 10\u22122]with uniform intervals on a log-scale; weight initializations in the range [\u22120.1, 0.1] and mixing ratios in the range 1:[0.01, 3] with uniform intervals on a log-scale. We use the follow-\ning settings in all of our models (unless otherwise specified in a subsection below): we unroll video encoder/decoder LSTM-RNNs to 50 time steps and language encoder/decoder LSTM-RNNs to 30 time steps. We use a 1024-dimension LSTM-RNN hidden state size. We use 512-dimension vectors to embed frame level visual features and word vectors. These embedding weights are learned during the training. We use the Adam optimizer (Kingma and Ba, 2015) with default coefficients and a batch size of 32. We apply a dropout with probability 0.5 to the vertical connections of LSTM (Zaremba et al., 2014) to reduce overfitting.\nB.1 Video Captioning on YouTube2Text\nB.1.1 Baseline and Attention Models\nOur primary baseline model (Inception-v4, attention, ensemble) uses a learning rate of 0.0001 and initializes all its weights with a uniform distribution in the range [\u22120.05, 0.05].\nB.1.2 Multi-Task with Video Prediction\n(1-to-M)\nIn this model, the video captioning and unsupervised video prediction tasks share their encoder LSTM-RNN weights and image embeddings in a one-to-many multi-task setting. We again use a learning rate of 0.0001 and initialize all the learnable weights with a uniform distribution in the range [\u22120.05, 0.05]. Two important hyperparameters tuned (on the validation set of captioning datasets) are the ratio of encoder vs decoder frames for video prediction on UCF-101 (where we found that 80% of frames as input and 20% for prediction performs best); and the mini-batch mixing ratio between the captioning and video prediction tasks (where we found 100 : 200 works well).\nB.1.3 Multi-Task with Entailment\nGeneration (M-to-1)\nIn this model, the video captioning and entailment generation tasks share their language decoder LSTM-RNN weights and word embeddings in a many-to-one multi-task setting. We again use a learning rate of 0.0001. All the trainable weights are initialized with a uniform distribution in the range [\u22120.08, 0.08]. We observe that a mixing ratio of 100 : 50 (between the captioning and entailment generation tasks) alternating minibatches works well here.\nB.1.4 Multi-Task with Video and Entailment\nGeneration (M-to-M)\nIn this many-to-many, three-task model, the video encoder is shared between the video captioning and unsupervised video prediction tasks, and the language decoder is shared between the video captioning and entailment generation tasks. We again use a learning rate of 0.0001. All the trainable weights are initialized with a uniform distribution in the range [\u22120.08, 0.08]. We found that a mixing ratio of 100 : 100 : 50 alternative mini-batches of video captioning, unsupervised video prediction, and entailment generation works best.\nB.2 Video Captioning on MSR-VTT\nWe also evaluate our many-to-many multi-task model on other video captioning datasets. For MSR-VTT, we train the model again using a learning rate of 0.0001. All the trainable weights are initialized with a uniform distribution in the range [\u22120.05, 0.05]. We found that a mixing ratio of 100 : 20 : 20 alternative mini-batches of video captioning, unsupervised video prediction, and entailment generation works best.\nB.3 Video Captioning on M-VAD\nFor the M-VAD dataset, we use 512 dimension hidden vectors for the LSTMs to reduce overfitting. We initialize the LSTM weights with a uniform distribution in the range [\u22120.1, 0.1] and all other weights with a uniform distribution in the range [\u22120.05, 0.05]. We use a learning rate of 0.001. We found a mixing ratio of 100 : 5 : 5 alternative mini-batches of video captioning, unsupervised video prediction, and entailment generation works best.\nB.4 Entailment Generation\nHere, we use video captioning to in turn help improve entailment generation results. We use the same hyperparameters for both the baseline and the multi-task model (Sec. 5.3 and Table 4). We use a learning rate of 0.001. All the trainable weights are initialized with a uniform distribution in the range [\u22120.08, 0.08]. We found a mixing ratio of 100 : 20 alternate mini-batches training of entailment generation and video captioning to perform best."}, {"heading": "C Analysis", "text": "In Sec. 5.5 of the main paper, we discussed examples comparing the generated captions of the final many-to-many multi-task model with those of the baseline. Here, we also separately compare our one-to-many (video prediction based) and many-to-one (entailment generation based) multitask models with the baseline. As shown in Table 8, our one-to-many multi-task model better identifies the actions and objects in comparison to the baseline, because the video prediction task helps it learn better context-aware visual representations, e.g., \u201ca man is eating something\u201d vs. \u201ca man is drinking something\u201d and \u201ca woman is slicing a vegetable\u201d vs. \u201ca woman is slicing an onion\u201d.\nOn the other hand, the many-to-one multi-task (with entailment generation) seems to be stronger at generating a caption which is a logicallyimplied entailment of a ground-truth caption, e.g., \u201ca cat is playing with a cat\u201d vs. \u201ca cat is playing\u201d and \u201ca woman is talking\u201d vs \u201ca woman is doing makeup\u201d (see Table 8)."}], "references": [{"title": "Multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil."], "venue": "NIPS.", "citeRegEx": "Argyriou et al\\.,? 2007", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Com-", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325 .", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "EACL.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A multi-modal clustering method for web videos", "author": ["Haiqi Huang", "Yueming Lu", "Fangwei Zhang", "Songlin Sun."], "venue": "International Conference on Trustworthy Computing and Services. pages 163\u2013 169.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "ICML.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "AvMendiz\u00e1bal."], "venue": "In SemEval. pages", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Generating natural language inference chains", "author": ["Vladyslav Kolesnyk", "Tim Rockt\u00e4schel", "Sebastian Riedel."], "venue": "arXiv preprint arXiv:1606.01404 .", "citeRegEx": "Kolesnyk et al\\.,? 2016", "shortCiteRegEx": "Kolesnyk et al\\.", "year": 2016}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum\u00e9 III."], "venue": "ICML.", "citeRegEx": "Kumar and III.,? 2012", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval 2:5.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 workshop. volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Computer-intensive methods for testing hypotheses", "author": ["Eric W Noreen."], "venue": "Wiley New York.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "ZhongwenXu", "Yi Yang", "FeiWu", "Yueting Zhuang."], "venue": "CVPR. pages 1029\u20131038.", "citeRegEx": "Pan et al\\.,? 2016a", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "CVPR. pages 4594\u20134602.", "citeRegEx": "Pan et al\\.,? 2016b", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "andWeiJing Zhu"], "venue": "In ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "ICLR.", "citeRegEx": "Simonyan and Zisserman.,? 2015", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah."], "venue": "arXiv preprint arXiv:1212.0402 .", "citeRegEx": "Soomro et al\\.,? 2012", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov."], "venue": "ICML. pages 843\u2013852.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke."], "venue": "CoRR.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "CVPR. pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Jesse Thomason", "Subhashini Venugopalan", "Sergio Guadarrama", "Kate Saenko", "Raymond J Mooney."], "venue": "COLING.", "citeRegEx": "Thomason et al\\.,? 2014", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["Atousa Torabi", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "arXiv preprint arXiv:1503.01070 .", "citeRegEx": "Torabi et al\\.,? 2015", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR. pages 4566\u20134575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko."], "venue": "EMNLP.", "citeRegEx": "Venugopalan et al\\.,? 2016", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "CVPR. pages 4534\u20134542.", "citeRegEx": "Venugopalan et al\\.,? 2015a", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "NAACL HLT.", "citeRegEx": "Venugopalan et al\\.,? 2015b", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Msrvtt: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui."], "venue": "CVPR. pages 5288\u20135296.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "CVPR. pages 4507\u20134515.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "CVPR.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 33, "context": "Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.", "startOffset": 34, "endOffset": 80}, {"referenceID": 21, "context": "Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.", "startOffset": 34, "endOffset": 80}, {"referenceID": 36, "context": "Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 139, "endOffset": 176}, {"referenceID": 20, "context": "Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 139, "endOffset": 176}, {"referenceID": 20, "context": "More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).", "startOffset": 153, "endOffset": 189}, {"referenceID": 37, "context": "More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).", "startOffset": 153, "endOffset": 189}, {"referenceID": 17, "context": ", 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video. A sequence-to-sequence model is then used to \u2018translate\u2019 the video to a caption. Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models.", "startOffset": 83, "endOffset": 234}, {"referenceID": 18, "context": "We model this via many-to-many multi-task learning based sequence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.", "startOffset": 85, "endOffset": 105}, {"referenceID": 2, "context": ", premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailed caption representations, since the caption is essentially an entailment of the video, i.", "startOffset": 82, "endOffset": 103}, {"referenceID": 24, "context": ", video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task\u2019s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence.", "startOffset": 42, "endOffset": 67}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 29, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 10, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.", "startOffset": 28, "endOffset": 96}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder.", "startOffset": 29, "endOffset": 252}, {"referenceID": 8, "context": "Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it. Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder. To harness the important frame sequence temporal ordering, Venugopalan et al. (2015a) proposed a sequence-to-sequence model with video encoder and language decoder RNNs.", "startOffset": 29, "endOffset": 504}, {"referenceID": 36, "context": "Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 182, "endOffset": 219}, {"referenceID": 20, "context": "Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a).", "startOffset": 182, "endOffset": 219}, {"referenceID": 29, "context": "More recently, Venugopalan et al. (2016) explored linguistic improvements to the caption decoder by fusing it with external language models.", "startOffset": 15, "endOffset": 41}, {"referenceID": 17, "context": "(2016) explored linguistic improvements to the caption decoder by fusing it with external language models. Moreover, an attention or alignment mechanism was added between the encoder and the decoder to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a). In contrast to static visual features, Yao et al. (2015) also considered temporal video features from a 3D-CNN model pretrained on an action recognition task.", "startOffset": 16, "endOffset": 385}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments.", "startOffset": 42, "endOffset": 61}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments. Yu et al. (2016)\u2019s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph.", "startOffset": 42, "endOffset": 219}, {"referenceID": 20, "context": "To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments. Yu et al. (2016)\u2019s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph. Pan et al. (2016b) proposed to simultaneously learn the RNN word probabilities and a visual-semantic joint embedding space that enforces the relationship between the semantics of the entire sentence and the visual content.", "startOffset": 42, "endOffset": 370}, {"referenceID": 3, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012).", "startOffset": 164, "endOffset": 229}, {"referenceID": 0, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012).", "startOffset": 164, "endOffset": 229}, {"referenceID": 0, "context": "Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daum\u00e9 III, 2012). Recently, Luong et al. (2016) combined", "startOffset": 180, "endOffset": 261}, {"referenceID": 25, "context": "Srivastava et al. (2015) address this by propos-", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "(2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014).", "startOffset": 100, "endOffset": 149}, {"referenceID": 12, "context": "(2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014).", "startOffset": 100, "endOffset": 149}, {"referenceID": 2, "context": "The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 2, "context": "The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014). However, directly generating the entailed hypothesis sentences given a premise sentence would be even more beneficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs. Recently, Kolesnyk et al. (2016) tried a sequenceto-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation.", "startOffset": 64, "endOffset": 516}, {"referenceID": 26, "context": "model (Sutskever et al., 2014) where the final state of the encoder RNN is input as an initial state to the decoder RNN, as shown in Fig.", "startOffset": 6, "endOffset": 30}, {"referenceID": 9, "context": "The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long sequences due to forget-style gates (Hochreiter and Schmidhuber, 1997).", "startOffset": 127, "endOffset": 161}, {"referenceID": 1, "context": "Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as the encoder and a unidirectional LSTMRNN as the decoder, see Fig.", "startOffset": 47, "endOffset": 70}, {"referenceID": 4, "context": "First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con-", "startOffset": 39, "endOffset": 61}, {"referenceID": 35, "context": "We also use MSR-VTT (Xu et al., 2016) with 10, 000 diverse video clips (from a video search engine) \u2013 it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al.", "startOffset": 20, "endOffset": 37}, {"referenceID": 30, "context": ", 2016) with 10, 000 diverse video clips (from a video search engine) \u2013 it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al., 2015) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible.", "startOffset": 157, "endOffset": 178}, {"referenceID": 24, "context": "Video Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions.", "startOffset": 123, "endOffset": 144}, {"referenceID": 2, "context": "Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral.", "startOffset": 144, "endOffset": 165}, {"referenceID": 2, "context": "Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral. It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entailment pairs, and we use this subset in our multi-task video captioning model. For improving video captioning, we use the same training/validation/test splits as provided by Bowman et al. (2015), which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).", "startOffset": 145, "endOffset": 572}, {"referenceID": 23, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 47}, {"referenceID": 28, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 11, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 27, "context": ", 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016).", "startOffset": 51, "endOffset": 73}, {"referenceID": 6, "context": "matic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al.", "startOffset": 112, "endOffset": 139}, {"referenceID": 22, "context": "matic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 31, "context": ", 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004).", "startOffset": 17, "endOffset": 40}, {"referenceID": 17, "context": ", 2015), and ROUGE-L (Lin, 2004).", "startOffset": 21, "endOffset": 32}, {"referenceID": 5, "context": "We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.", "startOffset": 67, "endOffset": 86}, {"referenceID": 13, "context": "We use Adam optimizer (Kingma and Ba, 2015).", "startOffset": 22, "endOffset": 43}, {"referenceID": 26, "context": "The final baseline model \u2297 instead uses an ensemble (E), which is a standard denoising method (Sutskever et al., 2014) that performs inference over ten randomly initialized models, i.", "startOffset": 94, "endOffset": 118}, {"referenceID": 34, "context": "Models METEOR CIDEr-D ROUGE-L BLEU-4 PREVIOUS WORK LSTM-YT (V) (Venugopalan et al., 2015b) 26.", "startOffset": 63, "endOffset": 90}, {"referenceID": 33, "context": "2 S2VT (V + A) (Venugopalan et al., 2015a) 29.", "startOffset": 15, "endOffset": 42}, {"referenceID": 36, "context": "8 Temporal Attention (G + C) (Yao et al., 2015) 29.", "startOffset": 29, "endOffset": 47}, {"referenceID": 21, "context": "9 LSTM-E (V + C) (Pan et al., 2016b) 31.", "startOffset": 17, "endOffset": 36}, {"referenceID": 32, "context": "3 Glove + DeepFusion (V) (E) (Venugopalan et al., 2016) 31.", "startOffset": 29, "endOffset": 55}, {"referenceID": 37, "context": "1 p-RNN (V + C) (Yu et al., 2016) 32.", "startOffset": 16, "endOffset": 33}, {"referenceID": 20, "context": "9 HNRE + Attention (G + C) (Pan et al., 2016a) 33.", "startOffset": 27, "endOffset": 46}, {"referenceID": 19, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.", "startOffset": 43, "endOffset": 85}, {"referenceID": 7, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.", "startOffset": 43, "endOffset": 85}, {"referenceID": 7, "context": "03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples. that a mixing ratio of 100 : 50 alternating minibatches (between the captioning and entailment tasks) works well here. Again, Table 1 shows statistically significant improvements in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task setting. Note that in our initial experiments, our entailment generation model helped the video captioning task significantly more than the alternative approach of simply improving fluency by adding (or deep-fusing) an external language model (or pretrained word embeddings) to the decoder (using both in-domain and out-of-domain language models), again because a caption is also \u2018entailed\u2019 by a video in a logically-directed sense and hence this matches our captioning task better (also see results of Venugopalan et al. (2016) in Table 1).", "startOffset": 58, "endOffset": 920}, {"referenceID": 35, "context": "3 Yao et al. (2015) 25.", "startOffset": 2, "endOffset": 20}, {"referenceID": 35, "context": "2 Xu et al. (2016) 25.", "startOffset": 2, "endOffset": 19}, {"referenceID": 35, "context": "Results are reimplementations as per Xu et al. (2016). We also report the top 3 leaderboard systems \u2013 our model achieves the new rank 1 based on their ranking method.", "startOffset": 37, "endOffset": 54}, {"referenceID": 31, "context": "Models METEOR Yao et al. (2015) 5.", "startOffset": 14, "endOffset": 32}, {"referenceID": 30, "context": "7 Venugopalan et al. (2015a) 6.", "startOffset": 2, "endOffset": 29}, {"referenceID": 20, "context": "7 Pan et al. (2016a) 6.", "startOffset": 2, "endOffset": 21}, {"referenceID": 18, "context": "1), because these hyperparameters depend on the primary task being improved, as also discussed in previous work (Luong et al., 2016).", "startOffset": 112, "endOffset": 132}, {"referenceID": 4, "context": "YouTube2Text or MSVD The Microsoft Research Video Description Corpus (MSVD) or YouTube2Text (Chen and Dolan, 2011) is used for our primary video captioning experiments.", "startOffset": 92, "endOffset": 114}, {"referenceID": 4, "context": "YouTube2Text or MSVD The Microsoft Research Video Description Corpus (MSVD) or YouTube2Text (Chen and Dolan, 2011) is used for our primary video captioning experiments. It has 1970 YouTube videos in the wild with many diverse captions in multiple languages for each video. Caption annotations to these videos are collected using Amazon Mechanical Turk (AMT). All our experiments use only English captions. On average, each video has 40 captions, and the overall dataset has about 80, 000 unique video-caption pairs. The average clip duration is roughly 10 seconds. We used the standard split as stated in Venugopalan et al. (2015a), i.", "startOffset": 93, "endOffset": 632}, {"referenceID": 35, "context": "We used the standard split as provided in (Xu et al., 2016), i.", "startOffset": 42, "endOffset": 59}, {"referenceID": 30, "context": "Again, we used the standard train/val/test split as provided in Torabi et al. (2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 24, "context": "For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories and with an average clip length of 7.", "startOffset": 98, "endOffset": 119}, {"referenceID": 23, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al.", "startOffset": 17, "endOffset": 47}, {"referenceID": 28, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 11, "context": ", 2009) \u2013 VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al.", "startOffset": 59, "endOffset": 106}, {"referenceID": 27, "context": ", 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016).", "startOffset": 51, "endOffset": 73}, {"referenceID": 13, "context": "We use the Adam optimizer (Kingma and Ba, 2015) with default coefficients and a batch size of 32.", "startOffset": 26, "endOffset": 47}, {"referenceID": 38, "context": "5 to the vertical connections of LSTM (Zaremba et al., 2014) to reduce overfitting.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data. We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailed caption decoder representations. For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks. We achieve significant improvements and the new state-ofthe-art on several standard video captioning datasets using diverse automatic and human evaluations. We also show mutual multi-task improvements on the entailment generation task.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}