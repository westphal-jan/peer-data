{"id": "1704.07820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Introspective Generative Modeling: Decide Discriminatively", "abstract": "We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. When training data, a randomised discriminative learning is achieved by employing a series of discriminative classification procedures.\n\n\n\nIn addition to being able to construct discriminative classifiers, these discriminative classifiers are more important to the learning and the learning. The resulting discriminative classifiers are generated at the start of a discriminative learning, as is the initial choice.\nThe resulting discriminative classifier is then set to the following level:\nAs we can see in this example, this discriminative classifier is not a single discriminative classifier. Rather, it is a combination of a combination of a subset of the discriminative classes and a subset of the discriminative classes.\nHowever, it is also possible to construct discriminative classifiers in terms of generative classifiers.\nFor example, a function can be inferred from a single classifier and can be inferred from a subset of the discriminative classes.\nAnother way to generate discriminative classifiers is to construct a single discriminative classifier:\nEach discriminative classifier is based on its own discriminative classifier, but in the same way, its own discriminative classifier.\nIn other words, this discriminative classifier is based on its own discriminative classifier, but in the same way, its own discriminative classifier.\nSo, in this example, the same discriminative classifier is generated by the discriminative classifier.\nThe problem is that it doesn't have to create discriminative classifiers for each group.\nThe first discriminative classifier can be generated by a single discriminative classifier and only by a given discriminative classifier, in the form of a single discriminative classifier and only by a given discriminative classifier.\nThe discriminative classifier is derived from a single discriminative classifier, but not by a single discriminative classifier.\nIt will be difficult to distinguish between discrim", "histories": [["v1", "Tue, 25 Apr 2017 17:57:33 GMT  (3380kb,D)", "http://arxiv.org/abs/1704.07820v1", "10 pages, 9 figures"]], "COMMENTS": "10 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["justin lazarow", "long jin", "zhuowen tu"], "accepted": false, "id": "1704.07820"}, "pdf": {"name": "1704.07820.pdf", "metadata": {"source": "CRF", "title": "Introspective Generative Modeling: Decide Discriminatively", "authors": ["Justin Lazarow", "Long Jin", "Zhuowen Tu"], "emails": ["jlazarow@ucsd.edu", "lojin@ucsd.edu", "ztu@ucsd.edu"], "sections": [{"heading": "1. Introduction", "text": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25]. Unsupervised learning, where no task-specific labeling/feedback is provided on top of the input data, still remains one of the most difficult problems in machine learning but holds a bright future since a large number of tasks have little to no supervision.\nPopular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2]. In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) [34] to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling. Unsupervised learning models are often generative and supervised classifiers are often discriminative; generative model learning has been traditionally considered to be\n1\u2217 equal contribution.\na much harder task than discriminative learning [13], due to its intrinsic learning complexity, as well many assumptions and simplifications made about the underlying models.\nGenerative and discriminative models have traditionally been considered distinct and complementary to each other. In the past, connections have been built to combine the two families [13, 26, 40, 20]. In the presence of supervised information with a large amount of data, a discriminative classifier [24] exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity. Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39]. There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples. We will discuss in detail the relations and connections of our IGM\n1\nar X\niv :1\n70 4.\n07 82\n0v 1\n[ cs\n.C V\n] 2\n5 A\npr 2\n01 7\nwith these existing literature in the next section. In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.\nInspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating. There are a number of interesting properties about IGM that are worth highlighting:\n\u2022 CNN classifier as generator: No special condition on CNN architecture is needed in IGM and many existing CNN classifiers can be directly made into generators, if trained properly.\n\u2022 End-to-end self-evaluation and learning: Perform end-toend \u201cintrospective learning\u201d to self-classify between synthesized samples (pseudo-negatives) and the training data, followed by direct discriminative learning, to approach the target distribution.\n\u2022 Integrated unsupervised/supervised learning: Unsupervised and supervised learning can be carried out under similar pipelines, differing in the absence or presence of initial negative samples.\n\u2022 All backpropagation: Our synthesis-by-classification algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the classification stage for the CNN parameters.\n\u2022 Model-based anysize-image-generation: Since our generative modeling models the input image, we are able to train on images of a size and generate an image of a larger size while maintaining the coherence for the entire image.\n\u2022 Agnosticity to various vision applications: Due to its intrinsic modeling power being at the same time generative and discriminative, IGM can be adopted to many applications; under the same pipeline, we show a number of vision tasks in the experiments, including texture modeling, artistic style transference, face modeling, and semi-supervised image classification in this paper. Supervised classification cases of an algorithm in the same introspective learning family can been seen in [21]."}, {"heading": "2. Significance and related work", "text": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44]. It builds on top of convolutional neural networks\n[25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14]. The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM. More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d). Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].\nThe history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19]. Each of these pioneering works points to some promising direction to unsupervised generative modeling. However the modeling power of these existing frameworks is still somewhat limited in computational and/or representational aspects. In addition, not too many of them sufficiently explore the power of discriminative modeling. Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41]. The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework. The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.\nRelationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution. Our IGM algorithm takes inspiration from GDL, but we also observe a number of limitations in GDL that will be overcome by IGM: GDL uses manually specified feature types (histograms and Haar filters), which are fairly limited by today\u2019s standard; the sampling process in GDL, based on Markov chain Monte Carlo (MCMC), is a big computational bottleneck; the experimental results for image modeling and classification were not satisfactory. To summarize, the main differences between GDL and IGM include: \u2022 The adoption of convolutional networks in IGM results in a\nsignificant boost to feature learning.\n\u2022 Introducing backpropagation to the synthesis/sampling process in IGM makes a fundamental improvement to the sam-\npling process in GDL that is otherwise slow and impractical.\n\u2022 An alternative algorithm, namely IGM-single (see Fig. 4), is additionally proposed to maintain a single classifier for IGM.\n\u2022 Higher quality results for image modeling are demonstrated in IGM.\nComparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM. We summarize the key differences between IGM and GAN. Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.\n\u2022 Unified generator/discriminator vs. separate generator and discriminator. IGM maintains a single model that is simultaneously a generator and a discriminator. The IGM generator therefore has self-awareness \u2014 being able to self-evaluate the difference between its generated samples (called pseudonegatives) w.r.t. the training data, followed by a direct CNN classifier training. GAN instead creates two convolutional networks, a generator and a discriminator.\n\u2022 Training. Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train [1]. IGM carries out a straightforward use of backpropagation in both the sampling and the classifier training stage, making the learning process direct. For example, for the textures shown in the experiments Fig. 2 and Fig. 6, all results by IGM are obtained under the identical setting without hyper-parameter tuning.\n\u2022 Modeling. The generator in GAN is a mapping from the features to the images. IGM directly models the underlying statistics of an image with an efficient sampling/inference process, which makes IGM flexible. For example, we are able to conduct model-based-anysize-generation in the texture modeling task by directly maintaining the underlying statistics of the entire image.\n\u2022 Speed. GAN performs a forward pass to reconstruct an image, which is generally faster than IGM where synthesis is carried out using backpropagation. IGM is still practically feasible since it takes only about 1\u2212 2 seconds to synthesize an image of size 64 \u00d7 64 and around 30 seconds to synthesize a texture image of size 320\u00d7 200 excluding the time to load the models.\n\u2022 Model size. Since a cascade of CNN classifiers (60 \u2212 200) are included in a single IGM model, IGM has a much larger model complexity than GAN. This is an advantage of GAN over IGM. Our alternative IGM-single model maintains a single CNN classifier but its generative power is worse than those of IGM and GAN.\nRelationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power. a). IGM focuses on the generator side studying its image construction capability. b). IGM consists of a sequence of cascading classifiers, whereas ICL is only composed of a single\nclassifier. Essentially, ICL is similar to IGM-single, with a small difference in the absence/presence of given negative samples. The generative modeling aspect of ICL/IGMsingle is not as competitive as IGM though. c). In ICL, a formulation for training a softmax multi-class classification was proposed, which is not in IGM. d). In addition, ICL focuses on single image patch, whereas IGM is able to model/synthesize an arbitrary size of image. A number of important image modeling tasks, including texture modeling, style transferring, face modeling, and semi-supervised learning are demonstrated here, which are not covered in ICL [21]."}, {"heading": "3. Method", "text": "We describe below the introspective generative modeling (IGM) algorithm. We discuss the main formulation first, which bears some level of similarity to GDL [39]. However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power. The enhanced modeling power comes mainly from CNNs due to its end-to-end learning with automatic feature learning and tuning when backpropagating on the network parameters; enhanced computational power also largely from CNNs due to a natural implementation of sampling by backpropagating on the input image. GDL is similar to IGM (see Fig. 3), but IGM-single (see Fig. 4) maintains a single CNN as opposed to having a sequence of classifiers in both GDL and IGM. We motivate the formulation of IGM from the Bayes theory, similar to GDL."}, {"heading": "3.1. Formulation", "text": "We start the discussion by borrowing notation from [39]. Suppose we are given a set of training images (patches): S = {xi, i = 1..n}. We focus on patch-based input first and let x \u2208 Rm be a data sample (an image patch of size say m = 64 \u00d7 64). We adopt the pseudo-negative concept defined in [39] and define class labels y \u2208 {\u22121,+1}, indicating x being a negative or a positive sample. Here we assume the positive samples with label y = +1 are the patterns/targets we want to study. A generative model computes for p(y,x) = p(x|y)p(y), which captures the underlying generation process of x for class y. A discriminative classifier instead computes p(y|x). Under the Bayes rule, similar to the motivation in [39]:\np(x|y = +1) = p(y = +1|x)p(y = \u22121) p(y = \u22121|x)p(y = +1) p(x|y = \u22121),\n(1) which can be further simplified when assuming equal priors p(y = +1) = p(y = \u22121):\np(x|y = +1) = p(y = +1|x) 1\u2212 p(y = +1|x) p(x|y = \u22121). (2)\nBased on Eq. (2), a generative model for the positive samples (patterns of interest) p(x|y = +1) can be fully represented by a generative model for the negatives p(x|y = \u22121) and a discriminative classifier p(y = +1|x), if both p(x|y = \u22121) and p(y = +1|x) can be accurately obtained/learned. However, this seemingly intriguing property is, in a way, a chicken-and-egg problem. To faithfully learn the positive patterns p(x|y = +1), we need to have a representative p(x|y = \u22121), which is equally difficult, if not more. For clarity, we now use p\u2212(x) to represent p(x|y = \u22121). In the GDL algorithm [39], a solution was given to learning p(x|y = +1) by using an iterative process starting from an initial reference distribution of the negatives p\u22120 (x), e.g. a Gaussian distribution U(x) on the entire space of x \u2208 Rm:\np\u22120 (x) = U(x)\np\u2212t (x) = 1\nZt qt(y = +1|x) qt(y = \u22121|x) \u00b7 p\u2212t\u22121(x), t = 1..T (3)\nwhere Zt = \u222b qt(y=+1|x)\nqt(y=\u22121|x)p \u2212 t\u22121(x)dx. Our hope is to grad-\nually learn p\u2212t (x) by following this iterative process of Eq. 3:\np\u2212t (x) t=\u221e\u2192 p(x|y = +1), (4)\nsuch that the samples drawn x \u223c p\u2212t (x) become indistinguishable from the given training samples. The samples drawn from x \u223c p\u2212t (x) are called pseudo-negatives, following a definition in [39]. Next, we present the realization of Eq. 3, namely IGM (consisting of a sequence of CNN classifiers and see Fig. 3) and additionally IGM-single (maintaining a single CNN classifier and see Fig. 4). 3.2. IGM Training\nNext, we present our introspective generative modeling algorithm using a sequence of classifiers, called IGM. The given (unlabeled) training set is defined as S = {xi, i = 1..n}, which is turned into S+ = {(xi, yi = +1), i = 1..n} within the discriminative setting. We start from an initial pseudo-negative set\nS0\u2212 = {(xi,\u22121), i = 1, ..., l}\nwhere xi \u223c p\u22120 (x) = U(x) which is a Gaussian distribution. A working set for t = 1..T\nSt\u22121\u2212 = {(xi,\u22121), i = 1, ..., l}.\nthen includes the pseudo-negative samples self-generated from each round. l indicates the number of pseudonegatives generated at each round. We carry out learning with t = 1...T to iteratively obtain\nqt(y = +1|x), qt(y = \u22121|x) (5)\nby updating classifier Ct on S+ \u222a St\u22121\u2212 . The reason for using q is because it is an approximation to the true p due\nAlgorithm 1 Outline of the IGM algorithm. Input: Given a set of training data S+ = {(xi, yi = +1), i = 1..n} with x \u2208 <m. Initialization: obtain an initial distribution e.g. Gaussian for the pseudo-negative samples: p\u22120 (x) = U(x). Create S 0 \u2212 =\n{(xi,\u22121), i = 1, ..., l} with xi \u223c p\u22120 (x) For t=1..T 1. Classification-step: Train CNN classifier Ct on S+ \u222a St\u22121\u2212 , resulting in qt(y = +1|x). 2. Update the model: p\u2212t (x) = 1Zt qt(y=+1|x) qt(y=\u22121|x)p \u2212 t\u22121(x). 3. Synthesis-step: sample l pseudo-negative samples xi \u223c p\u2212t (x), i = 1, ..., l from the current model p \u2212 t (x) using a variational sampling procedure (backpropagation on the input) to obtain St\u2212 = {(xi,\u22121), i = 1, ..., l}. 4. t \u2190 t + 1 and go back to step 1 until convergence (e.g. indistinguishable to the given training samples). End\nto limited samples drawn in <m. At each time t, we then compute\np\u2212t (x) = 1\nZt qt(y = +1|x) qt(y = \u22121|x) p\u2212t\u22121(x), (6)\nwhere Zt = \u222b qt(y=+1|x)\nqt(y=\u22121|x)p \u2212 t\u22121(x)dx. We draw new sam-ples\nxi \u223c p\u2212t (x) to have the pseudo-negative set:\nSt\u2212 = {(xi,\u22121), i = 1, ..., l}. (7)\nAlgorithm 1 describes the learning process. The pipeline of IGM is shown in Fig. 3, which consists of (1) a synthesis step and (2) a classification step. A sequence of CNN classifiers is progressively learned. With the pseudo-negatives being gradually generated, the classification boundary gets tightened and approaches the target distribution."}, {"heading": "3.2.1 Classification-step", "text": "The classification-step can be viewed as training a normal classifier on the training set S+ \u222a St\u2212 where S+ = {(xi, yi = +1), i = 1..n}. St\u2212 = {(xi,\u22121), i = 1, ..., l} for t \u2265 1. We use a CNN as our base classifier. When training a classifier Ct on S+ \u222a St\u2212, we denote the parameters to be learned in Ct by a high-dimensional vector Wt = (w (0) t ,w (1) t ) which might consist of millions of parameters. w(1)t denotes the weights on the top layer combining the features \u03c6(x;w(0)t ) and w (0) t carries all the internal representations. Without loss of generality, we assume a sigmoid function for the discriminative probability\nqt(y|x;Wt) = 1/(1 + exp{\u2212y < w(1)t , \u03c6(x;w (0) t ) >}).\n(8) Both w(1)t and w (0) t can be learned by the standard stochastic gradient descent algorithm via backpropagation to minimize a cross-entropy loss with an additional term on the\npseudo-negatives: L(Wt) = \u2212 i=1..n\u2211\n(xi,+1)\u2208S+\nln qt(+1|xi;Wt)\u2212 i=1..l\u2211\n(xi,\u22121)\u2208St\u2212\nln qt(\u22121|xi;Wt)"}, {"heading": "3.2.2 Synthesis-step", "text": "In the classification step, we obtain qt(y|x;Wt) which is then used to update p\u2212t (x) according to Eq. (6):\np\u2212t (x) = t\u220f a=1 1 Za qa(y = +1|x;Wa) qa(y = \u22121|x;Wa) p\u22120 (x). (9)\nIn the synthesis-step, our goal is to draw fair samples from p\u2212t (x). The sampling process is carried out by backpropagation, but now we need to go through a sequence classifiers by using 1Za qa(y=+1|x;Wa) qa(y=\u22121|x;Wa) , a = 1..t. This can be timeconsuming. In practice, we can simply perform backpropagation on the previous set St\u22121\u2212 by taking 1 Zt qt(y=+1|x;Wt) qt(y=\u22121|x;Wt) . Therefore, generating pseudo-negative samples when training IGM does not need a large overhead. Additional Gaussian noise can be added to the stochastic gradient as in [43] but we did not observe a big difference in the quality of samples in practice. This is probably due to the equivalent class [45] where the probability mass is widely distributed over an extremely large image space. Sampling strategies\nIn [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow. Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis. Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29]. When conducting experiments, some alternative sampling schemes using SGD can be applied: i) earlystopping once x becomes positive (or after a small fixed number of steps); or ii) sampling with equilibrium after long steps. We found early-stopping effective and efficient, which can be viewed as contrastive divergence [7] where a short Markov chain is simulated.\nNote that the partition function (normalization) Za is a constant that is not dependent on the sample x. Let\ngt(x) = qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) = exp{< w(1)t , \u03c6(x;w (0) t ) >}, (10) and take its ln, which is nicely turned into the logit of qt(y = +1|x;Wt)\nln gt(x) =< w (1) t , \u03c6(x;w (0) t ) > . (11)\nStarting from a x drawn from p\u2212t\u22121(x), we directly increase < w\n(1) t , \u03c6(x;w (0) t ) > using stochastic gradient ascent on x via backpropagation which allows us to obtain fair samples\nsubject to Eq. (9). A noise can be injected as in [43] when performing SGD sampling. Overall model The overall IGM model after T stages of training becomes:\np\u2212T (x) = 1\nZ T\u220f t=1 qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) p\u22120 (x)\n= 1\nZ T\u220f t=1 exp{< w(1)t , \u03c6(x;w (0) t ) >}p\u22120 (x),\n(12) where Z = \u222b \u220fT\nt=1 exp{< w (1) t , \u03c6(x;w (0) t ) >}p\u22120 (x)dx.\nIGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39]. Theorem 1 KL[p(x|y = +1)||p\u2212t+1(x)] \u2264 KL[p(x|y = +1)||p\u2212t (x)] whereKL denotes the Kullback-Leibler divergences, and p(x|y = +1) \u2261 p+(x)."}, {"heading": "3.3. An alternative: IGM-single", "text": "We briefly present the IGM-single algorithm, which is similar to the introspective classifier learning algorithm [21] with the difference without the presence of input negative samples. The pipeline of IGM-single is shown in Fig. 4. A key aspect here is that we maintain a single CNN classifier throughout the entire learning process.\nIn the classification step, we obtain qt(y|x;Wt) (similar as Eq. 8) which is then used to update p\u2212t (x) according to Eq. (13):\np\u2212t (x) = 1\nZt qt(y = +1|x;Wt) qt(y = \u22121|x;Wt) p\u22120 (x). (13)\nIn the synthesis-step, we draw samples from p\u2212t (x). Overall model The overall IGM-single model after T stages of training becomes:\np\u2212T (x) = 1\nZT exp{< w(1)T , \u03c6(x;w (0) T ) >}p \u2212 0 (x), (14) where ZT = \u222b exp{< w(1)T , \u03c6(x;w (0) T ) >}p \u2212 0 (x)dx."}, {"heading": "3.4. Model-based anysize-image-generation", "text": "pT (I) \u221d T\u220f\nt=1 m1\u220f i=1 m2\u220f j=1 gt(x(i, j))p \u2212 0 (x(i, j)) (15)\nwhere gt(x(i, j)) (see Eq. 10) denotes the score of the patch of size e.g. 64 \u00d7 64 for x(i, j) under the discriminator at round t. Fig. 5 gives an illustration for one round of sampling. This allows us to synthesize much larger images by being able to enforce the coherence and interactions surrounding a particular pixel. In practice, we add stochasticity and efficiency to the synthesis process by randomly sampling these set of patches."}, {"heading": "4. Experiments", "text": "We evaluate both IGM and IGM-single. In each method, we adopt the discriminator architecture of [33] which involves an input size of 64x64x3 in the RGB colorspace, four convolutional layers using 5 \u00d7 5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively. We include batch normalization after each convolutional layer (excluding the first) and use leaky ReLU activations with leak slope 0.2. The classification layer flattens the input and finally feeds it into a sigmoid activation.\nThis serves as the discriminator for the 64 \u00d7 64 patches we extract from the training image(s). Note that is is a gen-\neral purpose architecture with no modifications made for a specific task in mind.\nIn texture synthesis and artistic style, we make use of the \u201canysize-image-generation\u201d architecture by adding a \u201chead\u201d to the network that, at each forward pass of the network, randomly selects some number (equal to the desired batch size) of 64 \u00d7 64 random patches (possibly overlapping) from the full sized images and passes them to the discriminator. This allows us to retain the whole space of patches within a training image rather than select some subset of them in advance to use during training."}, {"heading": "4.1. Texture synthesis", "text": "Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32]. Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11]. We train similar textures to [41]. Each source texture is resized to 256 \u00d7 256, used as the single \u201cpositive\u201d example in the training set and a set of 200 negative examples are initially sampled from a normal distribution with \u03c3 = 0.3 of size 320 \u00d7 320 after adding padding of 32 pixels to each spatial dimension of the image to ensure each pixel of the 256\u00d7256 center has equal probability of being extracted in some patch. 1000 patches are extracted randomly across the training images and fed to the discriminator at each forward pass of the network (during training and synthesis stages) from a batch size of 100 images \u2014 50 random positives and negatives when training and 100 pseudo-negatives during synthesis. At each round, our classifier is finetuned using stochastic gradient descent with learning rate 0.01 from the previous round\u2019s classifier after the augmentation of the negative set with the 200\n320\u00d7 320 synthesized pseudo-negatives. Pseudo-negatives from more recent rounds are chosen in mini-batches with higher probability than those of earlier rounds in order to ensure the discriminator learns from its most recent mistakes as well as provide for more efficient training when the set of accumulated negatives has grown large in later rounds. During the synthesis stage, pseudo-negatives are synthesized using the previous round\u2019s pseudo-negatives as their initialization. Adam is used with a learning rate of 0.1 and \u03b2 = 0.5 and stops early when the average probability of the patches under the discriminator is more likely than not to be a positive across some window of previous steps, usually 20, in order to reduce variance. This allows us to, on average, cross the decision boundary of the current iteration of the discriminator. We find this sampling strategy to attain a good balance in effectiveness and efficiency. Empirically, we find training the networks for 70 rounds to provide good results in terms of synthesis and distillation of the model\u2019s knowledge.\nNew textures are synthesized under IGM by: sampling from the same distribution used initially during training (in our case, normally distributed with \u03c3 = 0.3), performing backpropagation of the synthesis using the saved parameters of the networks for each round, and feeding the resulting partial synthesis to the next round. The same early stopping criterion is used as outlined during training, however, the number of patches is dialed down to match the number being synthesized. We use about 10 patches per image when synthesizing a 256 \u00d7 256 image since this matches the average number of patches extracted per image during training. Making the number of patches much larger than corresponding ratio used in the training process has shown to generate images of lower quality and diversity.\nUnder IGM-single there is a single network, and thus only a single round of synthesis takes place to transform the initial noise to a high probability texture.\nConsidering the results in Fig. 2 and 6, we see that IGM generates images of similar quality to [41], however, it is usually more faithful to the structure of the input images. In Fig. 2, the \u201cbricks\u201d texture synthesized by IGM is very strict about the grout lines being straight to ensure the bricks are rectilinear. Similarly, in Fig. 6, the \u201cforest\u201d texture preserves continuity but allows for some of the variation in angle and path that the tree trunks take. The \u201cdiamond\u201d texture is reflective of the grid-like pattern seen from the input image and does not allow for overlap or differently sized diamonds. In the bottom row of \u201cpebbles\u201d, the resulting synthesis captures the size of the pebbles seen in the input image as well as the variation in color and shading."}, {"heading": "4.2. Artistic style transfer", "text": "We also attempt to transfer artistic style as shown in [14]. However, our architecture makes no use of additional networks for content and texture transferring task uses a loss\nfunctions during synthesis to minimize\n\u2212 ln p(Istyle | I) \u221d \u03b1\u00b7||Istyle\u2212I||2\u2212(1\u2212\u03b1)\u00b7ln p\u2212style(Istyle),\nwhere I is an input image and Istyle is its stylized version, and p\u2212style(I) denotes the model learned from the training style image. We include a L2 fidelity term during synthesis, weighted by a parameter \u03b1, making Istyle not too far away from the input image I. We choose \u03b1 = 0.3 and average the L2 difference between the original content image and the current stylized image at each step of synthesis. Two examples of the artistic style transfer are shown in Fig. 7."}, {"heading": "4.3. Face modeling", "text": "The CelebA dataset [28] is used in our face modeling experiment, which consists of 202, 599 face images. We crop the center 64 \u00d7 64 patches in these images as our positive examples. For the classification step, we use stochastic gradient descent with learning rate 0.01 and a batch size of 100 images, which contains 50 random positives and 50 random negatives. For the synthesis step, we use the Adam optimizer with learning rate 0.01 and \u03b2 = 0.5 and stop early when the pseudo-negatives cross the decision boundary. In Fig. 8, we show some face examples generated by our model and the DCGAN model."}, {"heading": "4.4. SVHN unsupervised learning", "text": "The SVHN [31] dataset consists of color images of house numbers collected by Google Street View. The training set consists of 73, 257 images, the extra set consists of 531, 131 images, and the test set has 26, 032 images. The images are of the size 32 \u00d7 32. We combine the training and extra set as our positive examples for unsupervised learning. Following the same settings in the face modeling experiments, our IGM model can generate examples as shown in Fig. 9."}, {"heading": "4.5. SVHN semi-supervised classification", "text": "We perform the semi-supervised classification experiment by following the procedure outlined in [33]. We first train a model on the SVHN training and extra set in an unsupervised way, as in Section 4.4. Then, we train an L2-SVM on the learned representations of this model. The features from the last three convolutional layers are concatenated to form a 14336-dimensional feature vector. A 10, 000 example held-out validation set is taken from the training set and is used for model selection. The SVM classifier is trained on 1000 examples taken at random from the remainder of the training set. The test error rate is averaged over 100 different SVMs trained on random 1000-example training sets. Within the same setting, our IGM model achieves the test error rate of 36.44 \u00b1 0.72% and the DCGAN model achieves 33.13 \u00b1 0.83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset)."}, {"heading": "5. Conclusion", "text": "Introspective generative modeling points to an encouraging direction for unsupervised image modeling that capitalizes on the power of discriminative deep convolutional neural networks. It can be adopted for a wide range of problems in computer vision and machine learning."}, {"heading": "6. Acknowledgement", "text": "This work is supported by NSF IIS-1618477 and a Northrop Grumman Contextual Robotics grant. We thank Saining Xie, Jun-Yan Zhu, Jiajun Wu, Stella Yu, and Alexei Efros for helpful discussions."}], "references": [{"title": "Wasserstein gan", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "ICML unsupervised and transfer learning, 27,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large-scale kernel machines, 34(5),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, 3(Jan):993\u2013 1022,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural photo editing with introspective adversarial networks", "author": ["A. Brock", "T. Lim", "J. Ritchie", "N. Weston"], "venue": "arXiv preprint arXiv:1609.07093,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G. Hinton"], "venue": "AISTATS, volume 10, pages 33\u201340. Citeseer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Stochastic gradient hamiltonian monte carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 19(4):380\u2013393,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "2nd edition,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Texture synthesis by nonparametric sampling", "author": ["A.A. Efros", "T.K. Leung"], "venue": "ICCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. of Comp. and Sys. Sci., 55(1),", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "The elements of statistical learning, volume 1", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "NIPS,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "General pattern theory-A mathematical study of regular structures", "author": ["U. Grenander"], "venue": "Clarendon Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Pyramid-based texture analysis/synthesis", "author": ["D.J. Heeger", "J.R. Bergen"], "venue": "SIGGRAPH, pages 229\u2013238,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "The\u201d wake-sleep\u201d algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": "Science, 268(5214):1158,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation, 18:1527\u2013 1554,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning: discriminative and generative", "author": ["T. Jebara"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Introspective classifier learning: Empower generatively", "author": ["L. Jin", "J. Lazarow", "Z. Tu"], "venue": "arXiv preprint arXiv:,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "DCGAN-tensorflow", "author": ["T. Kim"], "venue": "https://github.com/ carpedm20/DCGAN-tensorflow,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R. Howard", "W. Hubbard", "L. Jackel"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Monte Carlo strategies in scientific computing", "author": ["J.S. Liu"], "venue": "Springer Science & Business Media,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent as approximate bayesian inference", "author": ["S. Mandt", "M.D. Hoffman", "D.M. Blei"], "venue": "arXiv preprint arXiv:1704.04289,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepdream - a code example for visualizing neural networks", "author": ["A. Mordvintsev", "C. Olah", "M. Tyka"], "venue": "Google Research,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["J. Portilla", "E.P. Simoncelli"], "venue": "Int\u2019l j. of computer vision, 40(1):49\u201370,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv:1511.06434,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling by shortest data description", "author": ["J. Rissanen"], "venue": "Automatica, 14(5):465\u2013471,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1978}, {"title": "Fields of experts: A framework for learning image priors", "author": ["S. Roth", "M.J. Black"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, 22(8):888\u2013905,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "Adagan: Boosting generative models", "author": ["I. Tolstikhin", "S. Gelly", "O. Bousquet", "C.-J. Simon-Gabriel", "B. Sch\u00f6lkopf"], "venue": "arXiv:1701.02386,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning generative models via discriminative approaches", "author": ["Z. Tu"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Brain anatomical structure segmentation by hybrid discriminative/generative models", "author": ["Z. Tu", "K.L. Narr", "P. Doll\u00e1r", "I. Dinov", "P.M. Thompson", "A.W. Toga"], "venue": "IEEE Tran. on Medical Imag.,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Texture networks: Feed-forward synthesis of textures and stylized images", "author": ["D. Ulyanov", "V. Lebedev", "A. Vedaldi", "V. Lempitsky"], "venue": "ICML,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag New York, Inc.,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1995}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "ICML,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Self supervised boosting", "author": ["M. Welling", "R.S. Zemel", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Equivalence of julesz ensembles and frame models", "author": ["Y.N. Wu", "S.C. Zhu", "X. Liu"], "venue": "International Journal of Computer Vision, 38(3),", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}, {"title": "Cooperative training of descriptor and generator networks", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "arXiv:1609.09408,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "A theory of generative convnet", "author": ["J. Xie", "Y. Lu", "S.-C. Zhu", "Y.N. Wu"], "venue": "ICML,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature extraction from faces using deformable templates", "author": ["A.L. Yuille", "P.W. Hallinan", "D.S. Cohen"], "venue": "International journal of computer vision, 8(2):99\u2013111,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1992}, {"title": "Vision as bayesian inference: analysis by synthesis", "author": ["A.L. Yuille", "D. Kersten"], "venue": "Trends in cognitive sciences,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Energy-based generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv:1609.03126,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimax entropy principle and its application to texture modeling", "author": ["S.C. Zhu", "Y.N. Wu", "D. Mumford"], "venue": "Neural Computation, 9(8):1627\u20131660,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 41, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 187, "endOffset": 191}, {"referenceID": 4, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 208, "endOffset": 211}, {"referenceID": 24, "context": "Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM [42], boosting [12], random forests [5], and convolutional neural networks [25].", "startOffset": 247, "endOffset": 251}, {"referenceID": 9, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 102, "endOffset": 106}, {"referenceID": 36, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 128, "endOffset": 132}, {"referenceID": 3, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "Popular unsupervised learning methods include mixture models [10], principal component analysis (PCA) [22], spectral clustering [37], topic modeling [4], and autoencoders [3, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 33, "context": "In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) [34] to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "a much harder task than discriminative learning [13], due to its intrinsic learning complexity, as well many assumptions and simplifications made about the underlying models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 25, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 39, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 19, "context": "In the past, connections have been built to combine the two families [13, 26, 40, 20].", "startOffset": 69, "endOffset": 85}, {"referenceID": 23, "context": "In the presence of supervised information with a large amount of data, a discriminative classifier [24] exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity.", "startOffset": 99, "endOffset": 103}, {"referenceID": 43, "context": "Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39].", "startOffset": 127, "endOffset": 131}, {"referenceID": 38, "context": "Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation [44] and image modeling [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 32, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 35, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 0, "context": "There is also a wave of recent development in generative adversarial networks (GAN) [15, 33, 36, 1] in which a discriminator helps a generator try not to be fooled by \u201cfake\u201d samples.", "startOffset": 84, "endOffset": 99}, {"referenceID": 43, "context": "In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.", "startOffset": 3, "endOffset": 7}, {"referenceID": 38, "context": "In [44], a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in [39] generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.", "startOffset": 243, "endOffset": 247}, {"referenceID": 50, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 43, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 38, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 50, "endOffset": 62}, {"referenceID": 24, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 23, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 13, "context": "Inspired by the prior work on generative modeling [51, 44, 39] and development of convolutional neural networks [25, 24, 14], we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating.", "startOffset": 112, "endOffset": 124}, {"referenceID": 20, "context": "Supervised classification cases of an algorithm in the same introspective learning family can been seen in [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 50, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 153, "endOffset": 157}, {"referenceID": 12, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 184, "endOffset": 188}, {"referenceID": 43, "context": "Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling [51], the hybrid modeling work [13], and the self-supervised boosting algorithm [44].", "startOffset": 233, "endOffset": 237}, {"referenceID": 24, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 38, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "It builds on top of convolutional neural networks [25] and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) [39], and the DeepDream code [30] and the neural artistic style work [14].", "startOffset": 256, "endOffset": 260}, {"referenceID": 38, "context": "The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 38, "context": "The general pipeline of IGM is similar to that of GDL [39], with the boosting algorithm used in [39] is replaced by a CNN in IGM.", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 30, "endOffset": 38}, {"referenceID": 13, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 30, "endOffset": 38}, {"referenceID": 38, "context": "More importantly, the work of [30, 14] motives us to significantly improve the time-consuming sampling process in [39] by an efficient SGD process via backpropagation (the reason for us to say \u201call backpropagation\u201d).", "startOffset": 114, "endOffset": 118}, {"referenceID": 38, "context": "Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL [39] and the recent development of generative adversarial networks (GAN) [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 47, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 8, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 174, "endOffset": 177}, {"referenceID": 17, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 190, "endOffset": 194}, {"referenceID": 50, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 223, "endOffset": 227}, {"referenceID": 34, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 250, "endOffset": 254}, {"referenceID": 48, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 272, "endOffset": 276}, {"referenceID": 18, "context": "The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory [16], deformable models [48], inducing features [9], wake-sleep [18], the MiniMax entropy theory [51], the field of experts [35], Bayesian models [49], and deep belief nets [19].", "startOffset": 299, "endOffset": 303}, {"referenceID": 46, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 147, "endOffset": 155}, {"referenceID": 40, "context": "Recent works that adopt convolutional neural networks for generative modeling [47] either use CNNs as a feature extractor or create separate paths [46, 41].", "startOffset": 147, "endOffset": 155}, {"referenceID": 13, "context": "The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "The neural artistic transferring work [14] has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused [14] on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework.", "startOffset": 151, "endOffset": 155}, {"referenceID": 43, "context": "The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "The self-supervised boosting work [44] sequentially learns weak classifiers under boosting [12] for density estimation, but its modeling power was not adequately demonstrated.", "startOffset": 91, "endOffset": 95}, {"referenceID": 38, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 22, "endOffset": 26}, {"referenceID": 38, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Relationship with GDL [39] The generative via discriminative learning framework (GDL) [39] learns a generator through a sequence of boosting classifiers [12] using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "Comparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "Comparison with GAN [15] The recent development of generative adversarial neural networks [15] is very interesting and also highly related to IGM.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 32, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 49, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 37, "context": "Other recent algorithms alongside GAN [15, 33, 50, 6, 38] share similar properties with it.", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 20, "context": "Relationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "Relationship with ICL [21] The Introspective Classifier Learning work (ICL) [21] is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "A number of important image modeling tasks, including texture modeling, style transferring, face modeling, and semi-supervised learning are demonstrated here, which are not covered in ICL [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 38, "context": "We discuss the main formulation first, which bears some level of similarity to GDL [39].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power.", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "However, with the replacement of the boosting algorithm [12] by convolutional neural networks [25], IGM demonstrates significant improvement over GDL in terms of both modeling and computational power.", "startOffset": 94, "endOffset": 98}, {"referenceID": 38, "context": "Formulation We start the discussion by borrowing notation from [39].", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "We adopt the pseudo-negative concept defined in [39] and define class labels y \u2208 {\u22121,+1}, indicating x being a negative or a positive sample.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "Under the Bayes rule, similar to the motivation in [39]:", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "[14], Texture Nets [41], Portilla & Simoncelli [32], and DCGAN [33] results are from [41].", "startOffset": 85, "endOffset": 89}, {"referenceID": 38, "context": "In the GDL algorithm [39], a solution was given to learning p(x|y = +1) by using an iterative process starting from an initial reference distribution of the negatives p0 (x), e.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "The samples drawn from x \u223c pt (x) are called pseudo-negatives, following a definition in [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 42, "context": "Additional Gaussian noise can be added to the stochastic gradient as in [43] but we did not observe a big difference in the quality of samples in practice.", "startOffset": 72, "endOffset": 76}, {"referenceID": 44, "context": "This is probably due to the equivalent class [45] where the probability mass is widely distributed over an extremely large image space.", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "Sampling strategies In [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "Sampling strategies In [39], various Markov chain Monte Carlo techniques [27] including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow.", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Motivated by the DeepDream code [30] and Neural Artistic Style work [14], we perform stochastic gradient descent via backpropagation in synthesis.", "startOffset": 68, "endOffset": 72}, {"referenceID": 42, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 7, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 28, "context": "Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling [43, 8, 29].", "startOffset": 130, "endOffset": 141}, {"referenceID": 6, "context": "We found early-stopping effective and efficient, which can be viewed as contrastive divergence [7] where a short Markov chain is simulated.", "startOffset": 95, "endOffset": 98}, {"referenceID": 42, "context": "A noise can be injected as in [43] when performing SGD sampling.", "startOffset": 30, "endOffset": 34}, {"referenceID": 38, "context": "IGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39].", "startOffset": 45, "endOffset": 49}, {"referenceID": 38, "context": "IGM shares a similar cascade aspect with GDL [39] where the convergence of this iterative learning process to the target distribution was shown by the following theorem in [39].", "startOffset": 172, "endOffset": 176}, {"referenceID": 20, "context": "We briefly present the IGM-single algorithm, which is similar to the introspective classifier learning algorithm [21] with the difference without the presence of input negative samples.", "startOffset": 113, "endOffset": 117}, {"referenceID": 32, "context": "In each method, we adopt the discriminator architecture of [33] which involves an input size of 64x64x3 in the RGB colorspace, four convolutional layers using 5 \u00d7 5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "[14] and Texture Nets [41] results are from [41].", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 50, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 10, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 31, "context": "Texture synthesis Texture modeling/rendering is a long standing problem in computer vision and graphics [17, 51, 11, 32].", "startOffset": 104, "endOffset": 120}, {"referenceID": 50, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 55, "endOffset": 63}, {"referenceID": 45, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 55, "endOffset": 63}, {"referenceID": 10, "context": "Here we are interested in statistical texture modeling [51, 46], instead of just texture rendering [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "We train similar textures to [41].", "startOffset": 29, "endOffset": 33}, {"referenceID": 40, "context": "2 and 6, we see that IGM generates images of similar quality to [41], however, it is usually more faithful to the structure of the input images.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Artistic style transfer We also attempt to transfer artistic style as shown in [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "The CelebA dataset [28] is used in our face modeling experiment, which consists of 202, 599 face images.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The first, the second, and the third column are respectively results by DCGAN [33] using tensorflow implementation [23], IGM-single, and IGM.", "startOffset": 115, "endOffset": 119}, {"referenceID": 30, "context": "The SVHN [31] dataset consists of color images of house numbers collected by Google Street View.", "startOffset": 9, "endOffset": 13}, {"referenceID": 32, "context": "We perform the semi-supervised classification experiment by following the procedure outlined in [33].", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset).", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "83% (we ran the DCGAN code [23] in an identical setting as IGM for a fair comparison since the result reported in [33] was achieved by training on the ImageNet dataset).", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semisupervised learning. 1", "creator": "LaTeX with hyperref package"}}}