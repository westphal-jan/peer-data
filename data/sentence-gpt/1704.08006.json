{"id": "1704.08006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Deep Text Classification Can be Fooled", "abstract": "Deep neural networks (DNNs) play a key role in many applications. Current studies focus on crafting adversarial samples against DNN-based image classifiers by introducing some imperceptible perturbations to the input. However, DNNs for natural language processing have not got the attention they deserve, and they have been neglected.\n\n\n\n\n\nThe most frequently used DNN algorithm uses the DNN algorithm to classify (decay, then recast) images on a given set of images. For example, a DNN using the DNN algorithm may have an image-classifier that generates a new picture, but that is often referred to as \"the average image\". For example, a DNN with the same name may not be a better algorithm for a natural language. For example, a DNN with a very high-quality image that has the same name would not be a better DNN for a natural language. The DNN algorithm, instead, is a much more accurate DNN than a DNN for a natural language.\nThe DNN algorithm is designed to avoid arbitrary inputs, such as image and text, that will be generated by the input of each image. The algorithm provides an input-based model that produces image-classifiers. As a result, it can be used to evaluate the original picture with the input of each image.\nIf images have been used to represent a model (like the following), a DNN algorithm would have the most complex model that could be applied to the model as input. The image representation in an image would look similar to that in the model described. The model of a representation would look similar to that in the model described. The model of an image would look similar to that in the model described. The model of an image would look similar to that in the model described. The model of a model would look similar to that in the model described. The model of an image would look similar to that in the model described. The model of an image would look similar to that in the model described. The model of an image would look similar to that in the model described. The model of a image would look similar to that in the model described. The model of a image would look similar to that in the model described. The model of a model would look similar to that in the model described. The model of a image would look similar to that in the model described. The model of a image would look similar to that in the model described. The model of", "histories": [["v1", "Wed, 26 Apr 2017 08:17:34 GMT  (639kb)", "http://arxiv.org/abs/1704.08006v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["bin liang", "hongcheng li", "miaoqiang su", "pan bian", "xirong li", "wenchang shi"], "accepted": false, "id": "1704.08006"}, "pdf": {"name": "1704.08006.pdf", "metadata": {"source": "CRF", "title": "Deep Text Classification Can be Fooled", "authors": ["Bin Liang", "Hongcheng Li", "Xirong Li", "Wenchang Shi"], "emails": ["wenchang}@ruc.edu.cn"], "sections": [{"heading": "Introduction", "text": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision (Krizhevsky et al. 2012), speech recognition (Dahl et al. 2012), and natural language processing (Collobert and Weston 2008). DNNs have exhibited state-of-the-art performance in these tasks. Meanwhile, their robustness has also raised concerns.\nSome recent studies (Goodfellow et al. 2015; Kereliuk et al. 2015) demonstrate that DNN-based image or audio classifiers can be fooled by adversarial samples, which are well-crafted to cause a trained model to misclassify. As shown in Figure 1, an adversarial image can be generated by adding some imperceptible perturbations in a given image (Goodfellow et al. 2015). Consequently, the famous GoogLeNet (Szegedy et al. 2015) will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations. These studies reveal an important fact that the adversaries could potentially use the crafted image to cause dangerous conditions. As shown in (Papernot et al.\n2016a), a stop sign, after being crafted, will be incorrectly classified as a yield sign. As a result, a self-driving car equipped with the DNN classifier may behave dangerously.\nTo the best of our knowledge, all existing related studies have focused on attacking image or audio classification\nDNNs. Other types of DNNs, however, have not got the attention they deserve, especially the DNNs for natural language processing. Text processing also plays an important role in modern information analysis. For instance, many spam or phishing webpage detection systems are primarily based on text classification (Whittaker et al. 2010). Google has begun to detect web spams by using a deep network1. The content of a target page is fed to a DNN to determine if the page is a spam. Moreover, by treating text as a kind of raw signal at character level, text data can be classified by a convolutional DNN as for images (Zhang et al. 2015). Naturally, the question arises as whether the text classification DNNs can also be attacked as already done to image or audio classification DNNs.\nIn essence, text is a kind of discrete data, while the multimedia data such as images is continuous. When adding a relatively small perturbation to the pixels of an image, the resultant change can be difficult to be detected by a human observer. By contrast, such a perturbation will turn a character or a word to a completely different one. A sentence may be changed out of all recognition given a small amount of perturbations. As a result, when directly adopting existing perturbation algorithms, the resulting text\n1 http://www.seobythesea.com/2014/09/google-turns-deep-learningclassification-fight-web-spam/\nFigure 1: An adversarial sample generated with the Method presented in (Goodfellow et al. 2015).\nsign( x J (F, x, c))\n+ 0.6 \nZebra 75.07% confidence\nLionfish 88.81% confidence\n=\nsample will lose its original meaning or even become meaningless for human observers.\nBased on the above discussions, we argue that there are two particular challenges when crafting effective adversarial examples to fool a text DNN classifier.\nFirst, an adversarial text sample should be utilitypreserving. It must keep its original meaning for a human observer, while at the same time trick the target classifier. Consider for instance a spam message advertising something. Its adversarial version should not only fool a spam filter, but also effectively deliver the advertisement.\nSecond, the perturbation should be imperceptible. The change to the original sample cannot be easily spotted by a human. In practice, the original input is often a free-text document (e.g., a cellphone message or a tweet) rather than a rich text such as an HTML page. Hence, the adversary cannot hide the perturbation by leveraging various rich-text style properties, e.g., setting the font color or size. The perturbation in a plain text should be properly blended with the original text to prevent them from attracting the human observers\u2019 attention as far as possible.\nIn this paper, we present a simple but effective method for crafting adversarial samples against DNN-based natural language text classifiers. Instead of simply overlapping the perturbation and the original input, we design three perturbation strategies, namely insertion, modification, and removal, to elaborately dress up a given text to generate an adversarial sample. By computing the cost gradients for the original text and training samples, we determine what should be inserted, modified or removed, where to insert and how to modify. With our method, we can launch a source/target misclassification attack to force the prediction of a specific input to be an arbitrary class of interest. Quantitative experiments show that the generated adversarial samples successfully trick a state-of-the-art model (Zhang et al. 2015). Their output classification can be altered to any desirable classes. Meanwhile, the introduced perturbations are difficult to be perceived by humans."}, {"heading": "Target Model and Dataset", "text": "A recent work (Zhang et al. 2015) presents a convolutional network model for text classification at character level. For model training and evaluation, the authors employ a DBpedia ontology dataset (Lehmann et al. 2014), which contains 560,000 training samples and 70,000 test samples of 14 high-level classes such as Company, Building and Film. Before feeding an input text into the network, every character of the text is quantized as a vector using one-hot representation per alphabet for encoding. For example, the character \u2018c\u2019 is encoded as the vector (0, 0, 1, 0, \u2026, 0), in which only the third dimension is set to one. Through six convolutional layers and three fully-connected\nlayers, the input is finally projected into a vector indicating the classification confidences of the 14 classes. We choose this state-of-the-art model and the DBpedia dataset as the experiment targets to evaluate the effectiveness of the proposed method.\nMethodology"}, {"heading": "Overview", "text": "For a given sample x and a trained DNN classifier model F, the attacker aims to craft an adversarial sample x* = x + x by adding a perturbation x to x, such that F(x*)  F(x). To generate the desirable x, using the knowledge of the network sensitivity extracted from F, such as the gradient of the cost function with respect to the original sample x (Goodfellow et al. 2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b). We also leverage the cost gradient to craft adversarial samples, but in the text domain.\nThe gradient-based adversarial sample generation technique is proposed by Goodfellow et al (Goodfellow et al. 2015). They present the fast gradient sign method (FGSM for short) to craft adversarial samples against image DNNs. Let c be the true class of x and J (F, x, c) be the cost function used to train the DNN F. The perturbation is computed as the sign of the model\u2019s cost function gradient, i.e.,\nx =  sign(x J (F, x, c)) (1)\nwhere  is set to be small enough to make x undetectable.\nWe are inspired by (Goodfellow et al. 2015), but leverage the cost gradient in a completely different way due to distinct requirements of adversarial text samples. In the FGSM algorithm, all input pixels are applied either a positive or negative change in the same degree according to the direction (sign) of corresponding cost gradients regardless of their magnitude. In fact, the magnitude of the gradient also implies which part of the input image most should be manipulated to make it be misclassified. A pixel with larger gradient magnitude contributes more to the current classification result. As illustrated in Figure 2(a), only manipulating the 4,000 (2.66%) input pixels with the highest gradient magnitude can also generate an effective adversarial sample. By contrast, as shown in Figure 2(b), even manipulating the 40,000 (26.6%) pixels with the lowest magnitude, the classification result remains unchanged.\nHowever, the technique is not directly applicable for text classification. As illustrated in Figure 3(a), the original text is a short description of a motorcycle and correctly classified as Means of Transportation by the DNN model (Zhang et al. 2015). Although the prediction can be changed to Company by FGSM, the new text, see Figure 3(b), is unreadable. One might consider manipulating only\nthe characters with the highest, see Figure 3(c). Still, the generated text is unnatural with noticeable perturbation.\nIt is clear that more sophisticated strategies are needed to craft adversarial samples for the text data. In order to maintain the utility of a text sample, we perturb the sample not only by directly modifying its words, but also inserting new items (words or sentences) or removing some original ones from it. To increase the classification confidence of a target class, we insert some new items that can bring significant contribution to the class. If the inserted items are semantically compatible with the original text, they can avoid drawing the observer\u2019s attention. Similarly, by removing the original items with significant contribution to the genuine class (but without affecting the meaning of the text), the classification confidence of the original class will be decreased without compromising the text utility.\nBased on the above analysis, we present a method to craft the adversarial sample against the DNN-based natural language text classifier (Zhang et al. 2015). The fundamental of our method lies in identifying the text items that possess significant contribution to the classification by leveraging the cost gradient. Based on the identified items, three perturbation strategies, i.e., insertion, modification, and\nremoval, are used alone or in combination to craft an adversarial sample for a given text,\nIt is worth pointing out that the proposed method allows us to arbitrarily choose a class of interest as the target class to launch a source/target misclassification attack rather than just alter the output classification to any class different from the original class as performed in FGSM (Goodfellow et al. 2015). In what follows we describe in more details the three strategies."}, {"heading": "Insertion Strategy", "text": "For a given text t, if F(t) = c, the goal of the strategy is to insert some new text items (attack payload) in t, which can effectually upgrade the classification confidence of t for the class c\u2019 of interest (c\u2019  c). In general, this will also make the confidence of the original class c to be downgraded accordingly. The key elements to construct the attack payload are identified by leveraging the cost gradient as follows.\nFirst, the backpropagation algorithm is employed to compute the cost gradients for the samples in the training set. The cost gradient x J (F, x, c\u2019) is computed for every training sample x. Consequently, we can get the cost gradients of every dimension in all character vectors of these samples. We term characters containing the dimensions with highest gradient magnitude hot characters. Per sample we empirically select the top 100 hot characters. Subsequently, the phrases containing enough hot characters are identified via a simple scan.\nFinally, for all training samples of the target class, the phrases obtained previously will be collected together. From them, we determine the most frequent phrases, called Hot Training Phrases (HTPs). For example, \u201chistoric\u201d is an HTP of the Building class, appearing in 7,279 samples.\nHTPs are used as the key elements to construct our attack payload. For a given text t and the target class c\u2019, an adversarial sample t* can sometime be crafted by inserting just a few HTPs of c\u2019 into t. Figure 4 shows a sample classified as the Company class with 99.7% confidence. Inserting just one HTP, i.e., \u201chistoric\u201d, can successfully let it be misclassified as the Building class with 88.6% confidence.\nThe insertion position is also crucial. For example, if inserting the HTP \u201chistoric\u201d before \u201cexchange had just \u2026\u201d in the above sample, the resulting text is still correctly\nThe Uganda Securities Exchange (USE) is the historic principal stock exchange of Uganda. It was founded in June 1997. The USE is operated under the jurisdiction of Uganda's Capital Markets Authority which in turn reports to the Bank of Uganda, Uganda's central bank. The exchange's doors opened to trading in January 1998. At the time, the exchange had just one listing, a bond issued by the East African Development Bank. Uganda Securities Exchange\nFigure 4: An adversarial text sample generated by inserting just one hot word (shown in red italic).\nThe Suzuki DR-Z400 is a dual purpose motorcycle manufactured by Suzuki. Suzuki DR-Z400\n(a) original text sample.\nbgaaeaTheaSuzukicDR-Z400aisaaadualepurposecmotorcyclecmanufacturedabyaSuzuki.bSuzukiaDR-Z400\nFigure 3: Adversarial text samples generated with FGSM.\nThe Suzuki DR-Z400 is afdual purboce matorcyclebmaaufacturea by Suzuki. Suzuki DR-Z400\n(b) adversarial sample generated with FGSM directly.\n(c) only manipulate top characters (shown in red italic.).\nLionfish 72.8% confidence\nZebra 61.76% confidence\nZebra 75.07% confidence\nZebra 75.07% confidence sign( x J (F, x, c)) of\nbottom 40,000 pixels\nsign( x J (F, x, c)) of\ntop 4,000 pixels\n+ 0.6  =\n+ 0.6  =\n(a) only perturb top 4,000 pixels (success)\n(b) perturb bottom 40,000 pixels (fail)\nFigure 2: Adversarial samples generated with FGSM.\nHot Issue is the second Korean EP by South Korean boy band Big Bang, released under YG Entertainment, an entertainment company founded in 1996 in Seoul, South Korea. Big Bang's first EP Always further established the group's popularity in South Korea, with the single Last Farewell topping online charts for 8 consecutive weeks, sold over 5 million digital downloads. The group's leader, the then 20- year-old G-Dragon produced and wrote the lyrics for all tracks on Hot Issue. The song is a blend of trance hip-hop beats and pop melodies. Hot Issue (EP)\nFigure 5: An adversarial text sample generated by inserting a parenthesis based on a fact (shown in red italic).\nclassified as the Company class with 99.6% confidence. We find that the attack payload should be inserted nearby the phrases with significant contribution to the original class, such as \u201cprincipal stock exchange of Uganda. It was founded\u201d in the above sample. Similar to HTPs, this kind of phrases can also be identified by computing t J (F, t, c) for the given sample t with respect to the original class c.\nWhile multiple insertions are needed sometimes. Directly inserting multiple HTPs into a text often hurts its utility and readability. Therefore, we consider to introduce multiple HTPs by assembling them into a syntax unit and inserting the new unit in a proper position. To avoid being spotted by the human observer, the inserted text should be semantically compatible with the context of the insertion point. In practice, by searching Internet or some databases of facts, e.g., (Suchanek et al. 2007), we can get some facts that are closely related to the insertion point and embody some desirable HTPs of the target class as well. They can be transformed to a descriptive clause or a parenthesis for the insertion point to be easily inserted in a natural way.\nFor example, as shown in Figure 5, there is a text classified as the Album class with 99.9% confidence. To make it misclassified as the Company class, we can choose \u201cYG Entertainment\u201d, a company name appeared in the sample, as the insertion point. Within a few tries, a related fact that carries three HTPs of the Company class (i.e., \u201ccompany\u201d, \u201cfounded\u201d, and \u201centertainment\u201d) can be easily found from Wikipedia by googling the name and some combinations of HTPs. The fact can be represented as a parenthesis and inserted behind the company name. The resulting text is classified as the Company class with 94.0% confidence, without drawing the observer\u2019s attention.\nNote that we cannot always find a proper fact to wrap HTPs. To resolve the issue, we present a new concept, the forged fact, which is an artificial fact that can bring the desirable HTPs as the attack payload and should be difficult to be disproved. The forged fact can be created by reforming some real things related to the HTPs to make people believe it really happened. Furthermore, we exclude forged facts that can be disproved by retrieving their opposing evidences on Internet. Figure 6 presents a forged fact carrying desirable HTPs (i.e., \u201caction\u201d, \u201cmovie\u201d, \u201cdi-\nrected by\u201d, and \u201cAmerican\u201d) to alter the class from Means of Transportation to Film with 82.4% confidence."}, {"heading": "Modification Strategy", "text": "The modification strategy is to affect the model output by slightly manipulating some phrases in the input. Again, we employ the cost gradient to choose what should be modified. For a given sample t classified as the class c, we compute the cost gradient t J (F, t, c). Similar to identifying the HTPs, we identify phrases with significant contribution to the current classification by locating hot characters with the highest gradient magnitude. We term this kind of phrases as Hot Sample Phrase (HSP). Modifying the HSPs of an input is likely to change the current classification.\nFurther we consider how to modify the HSPs to let the output classification be a particular class c\u2019. In theory, the modification should increase the loss function J (F, t, c) and meanwhile decrease J (F, t, c\u2019). In other words, the modification should follow the direction of the cost gradient t J (F, t, c), yet against the direction of t J (F, t, c\u2019).\nIn order to perform the modification without getting the human observer\u2019s attention, an HSP is modified in two ways: (1) choose the common misspellings of it from related corpora to replace it, and (2) alter some of its characters to ones in similar visual appearance, e.g., replacing the lower-case letter \u2018l\u2019 (el) with the numeral \u20181\u2019 (one). A number of modification candidates can be generated. To verify whether they follow the right gradient directions, the candidates are checked one by one based on the two kinds of cost gradients mentioned above. In this way, the final modification is determined.\nAs shown in Figure 7, the phrase \u201ccomedy film property\u201d\nis identified as an HSP for the original class (Film, 99.6% confidence). We want to change its output to the Company class. From a misspelling corpus2, we find a common misspelling \u201cflim\u201d. As illustrated in Table 1, using it to replace \u201cfilm\u201d does follow the desirable gradient directions. Interestingly, such a slight modification dramatically changes the output to the Company class with 99.0% confidence.\n2 http://www.dcs.bbk.ac.uk/~ROGER/corpora.html\nThe APM 20 Lionceau is a two-seat very light aircraft manufactured by the French manufacturer Issoire Aviation. Despite its classic appearance it is entirely built from composite materials especially carbon fibers. Designed by Philippe Moniot and certified in 1999 (see EASA CS-VLA) this very light (400 kg empty 634 kg loaded) and economical (80 PS engine) aircraft is primarily intended to be used to learn to fly but also to travel with a relatively high cruise speed (113 knots). Lionceau has appeared in an American action movie directed by Ron Howard. A three-seat version the APM 30 Lion was presented at the 2005 Paris Air Show. Issoire APM 20 Lionceau\nFigure 6: An adversarial text sample generated by inserting a forged fact (shown in red italic)."}, {"heading": "Removal Strategy", "text": "The removal strategy is to downgrade the confidence of the original class by removing its HSPs. Since arbitrarily eliminating HSPs from the input often compromises its meaning, only words of the HSPs that play as a supplementary role, e.g., an inessential adjective or adverb, can be removed. For instance, the sample in Figure 8 is classified as the Film class with 95.5% confidence. By computing the cost gradient, we identify only one HSP \u201cseven-part British television series\u201d. Removing \u201cBritish\u201d has not much impact on the meaning, but results in a drop of 35.0% in the confidence of the original class (from 95.5% to 60.5%)."}, {"heading": "Combination of Three Strategies", "text": "In practice, we need to combine the above three strategies to craft adversarial samples. For example, changing the output classification by the removal strategy alone is difficult. However, by combining it with the other strategies, we can avoid introducing excessive modification or insertion to the original text. As shown in Figure 9, by inserting a forged fact, removing an HTP, and modifying an HSP, the output classification can be successfully changed (from 83.7% Building to 88.7% Means of Transportation), but failed when only using any strategy alone.\nEvaluation\nWe evaluate the effectiveness of our method by answering the following questions.\nQ1: Can the classification of a given sample be altered to an arbitrary target class? We randomly select a sample from the test set, originally classified as Means of Transportation (class 6#). Using the three strategies, 13 adversarial samples targeting the other classes are crafted respectively. The experiments (Table 2, No.1~13) show that every adversarial sample successfully fools the model.\nQ2: Can we craft adversarial samples for the samples from arbitrary source classes? We randomly select one sample from the testing sets of the 13 classes (except Means of Transportation) respectively, and arbitrarily designated a target class for each selected sample. Accordingly, 13 adversarial samples are crafted. The experiments (Table 2, No.14~26) show that all these adversarial samples can result in an expected misclassification no matter what their original classes are.\nQ3: Can the adversarial sample avoid being distinguished by human observers and still keep the utility? We perform a user study with 23 students from our university as subjects. They have no prior knowledge about this project. The study is performed in a single-blind manner, i.e., each subject is provided with 20 text samples, half of which are with perturbations, but the subject does not know which sample is affected. They are asked to manually classify each sample into one of the 14 classes. Further, if they consider a sample is artificially modified, they are asked to pinpoint where the modification is performed.\nFor the ten original samples our subjects classify them with an averaged accuracy of 94.2%, while for the ten affected samples the accuracy is 94.8%. The comparable performance shows that utility is indeed preserved in the adversarial samples. In total, there are 240 places marked as modified, with 12 successful matches. Recall that our method yields 594 changes. Our modifications can be detected by the human observers with an accuracy of 12/240=5.0% and a recall of 12/594=2.0%. The result suggests that the adversarial samples crafted by the proposed method are difficult to be perceived.\nMaisie is a comedy flim property MGM originally purchased for Jean Harlow but before a shooting script could be completed Harlow died in 1937. It was put on hold until 1939 when Ann Sothern was hired to star in the project with Robert Young as leading man. It is based on the novel Dark Dame by Wilson Collison. It was the first of ten films starring Sothern as Maisie Ravier. In Mary C. Maisie (film)\nFigure 7: An adversarial text sample generated by introducing a common misspelling (shown in red italic).\nEdward & Mrs. Simpson is a seven-part British television series that dramatises the events leading to the 1936 abdication of King Edward VIII of the United Kingdom who gave up his throne to marry the twice-divorced American Wallis Simpson. The series made by Thames Television for ITV was originally broadcast in 1978. Edward Fox played Edward and Cynthia Harris portrayed Mrs. Simpson. Edward & Mrs. Simpson\nFigure 8: Lower the confidence of the original class by removing a word (shown in blue italic) from an HSP.\nQ4: Is our method efficient enough? It took 116 hours in total to compute the cost gradient and identified HTPs for all the 14 classes (8.29 hours per class) on a desktop computer. The computation is performed only once for all the adversarial samples. Crafting one adversarial sample takes about 10 to 20 minutes. The cost is acceptable for an adversary to get a desirable adversarial sample. In practice, she or he is willing to spend more time to do this business."}, {"heading": "Related Work", "text": "Many studies have investigated the security of traditional machine learning methods (Barreno et al. 2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al. 2016). Researchers also tried to bypass DNN-based classifiers through crafting adversarial samples. There are various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al. 2016b; Moosavi-Dezfooli et al. 2016), and evolution-based (Nguyen et al. 2015). The gradient-based method is most efficient and easy to use.\nPapernot et al. introduced a defense named defensive distillation to adversarial samples (Papernot et al. 2016c).\nTwo networks were trained as a distillation, where the first network produced probability vectors used to label the original dataset, while the other was trained using the newly labelled dataset. As a result, the effectiveness of adversarial samples can be substantially reduced. However, as pointed in (Carlini and Wagner 2016), how to construct defenses that are robust to adversarial examples remains open. A feasible solution is to improve the robustness of DNNs by making it harder for attackers to craft adversarial samples rather than eliminating them thoroughly. The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b). This can make it harder for attackers to generate new adversarial samples. A study (Shaham et al. 2015) also shows that using worst-case examples which are close to the original training points to update network parameters can improve both the performance and robustness of target model.\nUnfortunately, none of the above studies pay any attention to DNN-based text classification. Text as discrete data is sensitive to perturbation. The existing attack methods cannot be directly applicable for text. We need to design new attack techniques and corresponding defenses.\nConclusion and Discussion\nOur research has revealed that DNN-based text classification is also vulnerable to the adversarial sample attack. So its robustness should be seriously considered.\nIn our study, the pre-trained target DNN and its training set are required to craft adversarial samples. In practice, the hypothesis is not always satisfied. However, Papernot et al. (Papernot et al. 2016a) proposed a Jacobian-based dataset augmentation technique to train a substitute model for a target DNN-based on limited pairs of inputs-outputs, without accessing its model, parameters, or training data. The authors also demonstrated that using the substitute model can also effectively craft the adversarial samples to attack the target DNN. Besides, according to some studies (Szegedy et al. 2014; Goodfellow et al. 2015), adversarial samples can transfer from one model to another, even if the second model has a different architecture or was trained on a different set. We have reason to believe that combining the studies with our method can launch a black-box attack to a close-source DNN-based text classifier, even it has a different architecture, e.g., word2vec (Mikolov et al. 2013).\nSo far, some human efforts are also required when crafting an adversarial sample in our method. Such amounts of human efforts are acceptable for common attack scenarios. To support adversarial training and investigate the possibility of large-scale attacks, in the future, we will address how to automatically craft adversarial text samples.\nNo. Source\nClass No.\nTarget\nClass No.\nInserted\nHTPs\nModified\nHIPs\nRemoved\nHTPs 1\n6# (99.9%)\n1# (94.4%) 3 0 0\n2 2# (76.3%) 5 0 1 3 3# (68.2%) 5 3 1 4 4# (84.7%) 3 3 1 5 5# (82.4%) 4 0 1 6 7# (86.3%) 3 2 1 7 8# (70.0%) 2 3 1 8 9# (89.7%) 3 2 1 9 10# (81.0%) 3 2 1\n10 11# (82.4%) 3 0 0 11 12# (66.3%) 3 2 1 12 13# (82.4%) 4 0 0 13 14# (80.2%) 4 2 1 14 1# (99.7%) 7# (88.6%) 1 0 0 15 2# (99.3%) 13# (72.0%) 2 1 0 16 3# (99.8%) 12# (84.2%) 2 1 0 17 4# (99.9%) 3# (77.5%) 2 1 1 18 5# (99.8%) 4# (95.9%) 3 1 0 19 7# (83.7%) 6# (88.7%) 2 1 1 20 8# (99.5%) 7# (96.3%) 1 0 0 21 9# (99.9%) 7# (98.2%) 1 1 0 22 10# (84.6%) 6# (94.2%) 0 0 1 23 11# (85.3%) 8# (81.7%) 1 0 1 24 12# (99.9%) 1# (94.0%) 2 0 0 25 13# (99.6%) 1# (99.0%) 0 1 0 26 14# (99.9%) 13# (87.1%) 3 0 1\nAvg 98.1% 84.7% 2.5 1.0 0.6\nTable 2: Experiments for Q1 and Q2."}], "references": [{"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A. Joseph", "J. Tygar"], "venue": "ASIACCS", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "Towards evaluating the robustness of neural networks", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv: 1608.04644.", "citeRegEx": "Carlini and Wagner,? 2016", "shortCiteRegEx": "Carlini and Wagner", "year": 2016}, {"title": "A unified architecture for natural language processing: Deep neural networks with task learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML 2008.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Contextdependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE TASLP, 20(1): 30\u201342.", "citeRegEx": "Dahl et al\\.,? 2012", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "ICLR 2015.", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Deep learning and music adversaries", "author": ["C. Kereliuk", "B. Sturm", "J. Larsen"], "venue": "IEEE TMM, 17(11): 2059-2071.", "citeRegEx": "Kereliuk et al\\.,? 2015", "shortCiteRegEx": "Kereliuk et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS 2012.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. Kleef", "S. Auer", "C. Bizer"], "venue": "Semantic Web Journal.", "citeRegEx": "Lehmann et al\\.,? 2014", "shortCiteRegEx": "Lehmann et al\\.", "year": 2014}, {"title": "Cracking classifiers for evasion: a case study on the Google\u2019s phishing pages filter", "author": ["B. Liang", "M. Su", "W. You", "W. Shi", "G. Yang"], "venue": "WWW 2016.", "citeRegEx": "Liang et al\\.,? 2016", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS 2013.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "DeepFool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": null, "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "CVPR 2015.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697.", "citeRegEx": "Papernot et al\\.,? 2016a", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "IEEE EuroS&P 2016.", "citeRegEx": "Papernot et al\\.,? 2016b", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "IEEE S&P 2016.", "citeRegEx": "Papernot et al\\.,? 2016c", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Understanding adversarial training: increasing local stability of neural nets through robust optimization", "author": ["U. Shaham", "Y. Yamada", "S. Negahban"], "venue": "arXiv preprint arXiv:1511.05432.", "citeRegEx": "Shaham et al\\.,? 2015", "shortCiteRegEx": "Shaham et al\\.", "year": 2015}, {"title": "Practical evasion of a learningbased classifier: A case study", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "IEEE S&P 2014.", "citeRegEx": "\u0160rndi\u0107 and Laskov,? 2014", "shortCiteRegEx": "\u0160rndi\u0107 and Laskov", "year": 2014}, {"title": "YAGO: a core of semantic knowledge unifying wordnet and Wikipedia", "author": ["F. Suchanek", "G Kasneci", "G. Weikum"], "venue": null, "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "ICLR 2014.", "citeRegEx": "Szegedy et al\\.,? 2014", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Large-scale automatic classification of phishing pages", "author": ["C. Whittaker", "B. Ryner", "M. Nazif"], "venue": "NDSS 2010. Zhang, X.; Zhao, J.; and LeCun, Y. 2015. Character-level convolutional networks for text classification. In NIPS 2015.", "citeRegEx": "Whittaker et al\\.,? 2010", "shortCiteRegEx": "Whittaker et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Deep neural networks (DNNs) have been widely adopted in many applications such as computer vision (Krizhevsky et al. 2012), speech recognition (Dahl et al.", "startOffset": 98, "endOffset": 122}, {"referenceID": 3, "context": "2012), speech recognition (Dahl et al. 2012), and natural language processing (Collobert and Weston 2008).", "startOffset": 26, "endOffset": 44}, {"referenceID": 2, "context": "2012), and natural language processing (Collobert and Weston 2008).", "startOffset": 39, "endOffset": 66}, {"referenceID": 4, "context": "Some recent studies (Goodfellow et al. 2015; Kereliuk et al. 2015) demonstrate that DNN-based image or audio classifiers can be fooled by adversarial samples, which are well-crafted to cause a trained model to misclassify.", "startOffset": 20, "endOffset": 66}, {"referenceID": 5, "context": "Some recent studies (Goodfellow et al. 2015; Kereliuk et al. 2015) demonstrate that DNN-based image or audio classifiers can be fooled by adversarial samples, which are well-crafted to cause a trained model to misclassify.", "startOffset": 20, "endOffset": 66}, {"referenceID": 4, "context": "by adding some imperceptible perturbations in a given image (Goodfellow et al. 2015).", "startOffset": 60, "endOffset": 84}, {"referenceID": 18, "context": "Consequently, the famous GoogLeNet (Szegedy et al. 2015) will misclassify the resultant image, while a human observer can still correctly classify it and without noticing the existence of the introduced perturbations.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "As shown in (Papernot et al. 2016a), a stop sign, after being crafted, will be incorrectly classified as a yield sign.", "startOffset": 12, "endOffset": 35}, {"referenceID": 20, "context": "many spam or phishing webpage detection systems are primarily based on text classification (Whittaker et al. 2010).", "startOffset": 91, "endOffset": 114}, {"referenceID": 4, "context": "presented in (Goodfellow et al. 2015).", "startOffset": 13, "endOffset": 37}, {"referenceID": 7, "context": "For model training and evaluation, the authors employ a DBpedia ontology dataset (Lehmann et al. 2014),", "startOffset": 81, "endOffset": 102}, {"referenceID": 4, "context": "(Goodfellow et al. 2015), has proven to be effective (Kereliuk et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 10, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 13, "context": "2015), has proven to be effective (Kereliuk et al. 2015; Moosavi-Dezfooli et al. 2016; Papernot et al. 2016b).", "startOffset": 34, "endOffset": 109}, {"referenceID": 4, "context": "The gradient-based adversarial sample generation technique is proposed by Goodfellow et al (Goodfellow et al. 2015).", "startOffset": 91, "endOffset": 115}, {"referenceID": 4, "context": "We are inspired by (Goodfellow et al. 2015), but leverage the cost gradient in a completely different way due to", "startOffset": 19, "endOffset": 43}, {"referenceID": 17, "context": ", (Suchanek et al. 2007), we can get some facts that are closely related to the insertion point and embody some desirable HTPs of the target class as well.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": "Many studies have investigated the security of traditional machine learning methods (Barreno et al. 2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 16, "context": "2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "2006) and proposed some attack methods, including the black-box attack (\u0160rndi\u0107 and Laskov 2014) and the white-box attack (Liang et al. 2016).", "startOffset": 121, "endOffset": 140}, {"referenceID": 4, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 5, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 11, "context": "various methods to generate adversarial samples against DNNs, including gradient-based (Goodfellow et al. 2015; Kereliuk et al. 2015; Nguyen et al. 2015), decision function-based (Papernot et al.", "startOffset": 87, "endOffset": 153}, {"referenceID": 13, "context": "2015), decision function-based (Papernot et al. 2016b; Moosavi-Dezfooli et al. 2016), and evolution-based (Nguyen et al.", "startOffset": 31, "endOffset": 84}, {"referenceID": 10, "context": "2015), decision function-based (Papernot et al. 2016b; Moosavi-Dezfooli et al. 2016), and evolution-based (Nguyen et al.", "startOffset": 31, "endOffset": 84}, {"referenceID": 11, "context": "2016), and evolution-based (Nguyen et al. 2015).", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "introduced a defense named defensive distillation to adversarial samples (Papernot et al. 2016c).", "startOffset": 73, "endOffset": 96}, {"referenceID": 1, "context": "However, as pointed in (Carlini and Wagner 2016), how to construct defenses that are robust to adversarial examples remains open.", "startOffset": 23, "endOffset": 48}, {"referenceID": 4, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 5, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 13, "context": "The adversarial training is a straightforward defense technique which uses as many as possible adversarial samples during training process as a kind of regularization (Goodfellow et al. 2015; Kereliuk et al. 2015; Papernot et al. 2016b).", "startOffset": 167, "endOffset": 236}, {"referenceID": 15, "context": "A study (Shaham et al. 2015) also shows that", "startOffset": 8, "endOffset": 28}, {"referenceID": 12, "context": "(Papernot et al. 2016a) proposed a Jacobian-based dataset augmentation technique to train a substitute model for a target DNN-based on limited pairs of inputs-outputs, without accessing its model, parameters, or training data.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Besides, according to some studies (Szegedy et al. 2014; Goodfellow et al. 2015), adversarial samples can transfer from one model to another, even if the second model has a different architecture or was trained on a different set.", "startOffset": 35, "endOffset": 80}, {"referenceID": 4, "context": "Besides, according to some studies (Szegedy et al. 2014; Goodfellow et al. 2015), adversarial samples can transfer from one model to another, even if the second model has a different architecture or was trained on a different set.", "startOffset": 35, "endOffset": 80}, {"referenceID": 9, "context": ", word2vec (Mikolov et al. 2013).", "startOffset": 11, "endOffset": 32}], "year": 2017, "abstractText": "Deep neural networks (DNNs) play a key role in many applications. Current studies focus on crafting adversarial samples against DNN-based image classifiers by introducing some imperceptible perturbations to the input. However, DNNs for natural language processing have not got the attention they deserve. In fact, the existing perturbation algorithms for images cannot be directly applied to text. This paper presents a simple but effective method to attack DNN-based text classifiers. Three perturbation strategies, namely insertion, modification, and removal, are designed to generate an adversarial sample for a given text. By computing the cost gradients, what should be inserted, modified or removed, where to insert and how to modify are determined effectively. The experimental results show that the adversarial samples generated by our method can successfully fool a state-of-the-art model to misclassify them as any desirable classes without compromising their utilities. At the same time, the introduced perturbations are difficult to be perceived. Our study demonstrates that DNN-based text classifiers are also prone to the adversarial sample attack.", "creator": "Microsoft\u00ae Word 2016"}}}