{"id": "1704.08092", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations", "abstract": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence. Importantly, it also incorporates the principle that both an input and a new input represent a specific point in a given input sequence. Our model uses this knowledge to make the model more flexible for addressing an issue such as the intergenerational influence of language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 26 Apr 2017 13:10:12 GMT  (198kb,D)", "http://arxiv.org/abs/1704.08092v1", "To appear at ACL2017, code available atthis https URL"]], "COMMENTS": "To appear at ACL2017, code available atthis https URL", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["samuel r\u00f6nnqvist", "niko schenk", "christian chiarcos"], "accepted": true, "id": "1704.08092"}, "pdf": {"name": "1704.08092.pdf", "metadata": {"source": "CRF", "title": "A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations", "authors": ["Samuel R\u00f6nnqvist", "Niko Schenk", "Christian Chiarcos"], "emails": ["sronnqvi@abo.fi", "schenk@informatik.uni-frankfurt.de", "chiarcos@informatik.uni-frankfurt.de"], "sections": [{"heading": "1 Introduction", "text": "True text understanding is one of the key goals in Natural Language Processing and requires capabilities beyond the lexical semantics of individual words or phrases. Natural language descriptions are typically driven by an inter-sentential coherent structure, exhibiting specific discourse properties, which in turn contribute significantly to the global meaning of a text. Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013).\nVarious formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004). The annotation schemata of the Penn Discourse Treebank (Prasad et al., 2008, PDTB) and the Chinese Discourse Treebank (Zhou and Xue, 2012, CDTB), for instance, define\n\u2217Both first authors contributed equally to this work.\ndiscourse units as syntactically motivated character spans in the text, augmented with relations pointing from the second argument (Arg2, prototypically, a discourse unit associated with an explicit discourse marker) to its antecedent, i.e., the discourse unit Arg1. Relations are labeled with a relation type (its sense) and the associated discourse marker. Both, PDTB and CDTB, distinguish explicit from implicit relations depending on the presence of such a marker (e.g., because/\u56e0).1 Sense classification for implicit relations is by far more challenging because the argument pairs lack the marker as an important feature. Consider, for instance, the following example from the CDTB as implicit CONJUNCTION:\nArg1: \u4f1a\u8c08\u5c31\u4e00\u4e9b\u539f\u5219\u548c\u5177\u4f53\u95ee\u9898\u8fdb\u884c\u4e86 \u6df1\u5165\u8ba8\u8bba\uff0c\u8fbe\u6210\u4e86\u4e00\u4e9b\u8c05\u89e3 In the talks, they discussed some principles and specific questions in depth, and reached some understandings\nArg2: \u53cc\u65b9\u4e00\u81f4\u8ba4\u4e3a\u4f1a\u8c08\u5177\u6709\u79ef\u6781\u6210\u679c Both sides agree that the talks have positive results\nMotivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011). Only recently, resource-lean architectures have been proposed. These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016). So far, unfortunately, these models have been evaluated only on four top-level senses\u2014sometimes even with inconsistent evaluation setups.2 Furthermore, most systems have initially been designed for the English PDTB and involve complex, task-\n1The set of relation types and senses is completed by alternative lexicalizations (ALTLEX/discourse marker rephrased), and entity relations (ENTREL/anaphoric coherence).\n2E.g., four binary classifiers vs. four-way classification.\nar X\niv :1\n70 4.\n08 09\n2v 1\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\nspecific architectures (Liu and Li, 2016), while discourse modeling techniques for Chinese have received very little attention in the literature and are still seriously underrepresented in terms of publicly available systems. What is more, over 80% of all words in Chinese discourse relations are implicit\u2014compared to only 52% in English (Zhou and Xue, 2012).\nRecently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al. (2016). Even though these specific models completely ignore word order within arguments, such feedforward architectures have been claimed by Rutherford et al. (2016) to generally outperform any thoroughly-tuned recurrent architecture.\nOur Contribution: In this work, we release the first attention-based recurrent neural sense classifier, specifically developed for Chinese implicit discourse relations. Inspired by Zhou et al. (2016), our system is a practical adaptation of the recent advances in relation modeling extended by a novel sampling scheme.\nContrary to previous assertions by Rutherford et al. (2016), our model demonstrates superior performance over traditional bag-of-words approaches with feedfoward networks by treating discourse arguments as a joint sequence. We evaluate our method within an independent framework and show that it performs very well beyond standard class-level predictions, achieving stateof-the-art accuracy on the CDTB test set.\nWe illustrate how our model\u2019s attention mechanism provides means to highlight those parts of an input sequence that are relevant for the classification decision, and thus, it may enable a better understanding of the implicit discourse parsing problem. Our proposed network architecture is flexible and largely language-independent as it operates only on word embeddings. It stands out due to its structural simplicity and builds a solid ground for further development towards other textual domains."}, {"heading": "2 Approach", "text": "We propose the use of an attention-based bidirectional Long Short-Term Memory (Hochreiter\nand Schmidhuber, 1997, LSTM) network to predict senses of discourse relations. The model draws upon previous work on LSTM, in particular its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al., 2014; Hermann et al., 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al., 2016). More specifically, our model is a flexible recurrent neural network with capabilities to sequentially inspect tokens and to highlight which parts of the input sequence are most informative for the discourse relation recognition task, using the weighting provided by the attention mechanism. Furthermore, the model benefits from a novel sampling scheme for arguments, as elaborated below. The system is learned in an end-to-end manner and consists of multiple layers, which are illustrated in Figure 1.\nFirst, token sequences are taken as input and special markers (<ARG1>, </ARG1>, etc.) are inserted into the corresponding positions to inform the model on the start and end points of argument spans. This way, we can ensure a general flexibility in modeling discourse units and could easily extend them with additional context, for instance. In our experiments on implicit arguments,\nonly the tokens in the respective spans are considered. Note that, unlike previous works, our approach models Arg1-Arg2 pairs as a joint sequence and does not first compute intermediate representations of arguments separately.\nSecond, an input layer encodes tokens using one-hot vector representations (ti for tokens at positions i \u2208 [1, k]), and a subsequent embedding layer provides a dense representation (ei) to serve as input for the recurrent layers. The embedding layer is initialized using pre-trained word vectors, in our case 300-dimensional Chinese Gigaword vectors (Graff and Chen, 2005).3 These embeddings are further tuned as the network is trained towards the prediction task. Embeddings for unknown tokens, e.g., markers, are trained by backpropagation only. Note that, tokens, markers and the pre-trained vectors represent the only source of information for the prediction task.\nFor the recurrent setup, we use a layer of LSTM networks in a bidirectional manner, in order to better capture dependencies between parts of the input sequence by inspection of both left and righthand-side contexts at each time step. The LSTM holds a state representation as a continuous vector passed to the subsequent time step, and it is capable of modeling long-range dependencies due to its gated memory. The forward (A\u2032) and backward (A\u2032\u2032) LSTMs traverse the sequence ei, producing sequences of vectors h\u2032i and h \u2032\u2032 i respectively, which are then summed together (indicated by \u2295 in Figure 1).\nThe resulting sequence of vectors hi is reduced into a single vector and fed to the final softmax output layer in order to classify the sense label y of the discourse relation. This vector may be obtained either as the final vector h produced by an LSTM, or through pooling of all hi, or by using attention, i.e., as a weighted sum over hi. While the model may be somewhat more difficult to optimize using attention, it provides the added benefit of interpretability, as the weights highlight to what extent the classifier considers the LSTM state vectors at each token during modeling. This is particularly interesting for discourse parsing, as most previous approaches have provided little support for pinpointing the driving features in each argument span.\nFinally, the attention layer contains the trainable\n3http://www.cs.brandeis.edu/\u02dcclp/ conll16st/dataset.html\nvector w (of the same dimensionality as vectors hi) which is used to dynamically produce a weight vector \u03b1 over time steps i by:\n\u03b1 = softmax(wT tanh(H))\nwhere H is a matrix consisting of vectors hi. The output layer r is the weighted sum of vectors inH:\nr = H\u03b1T\nPartial Argument Sampling: For the purpose of enlarging the instance space of training items in the CDTB, and thus, in order to improve the predictive performance of the model, we propose a novel partial sampling scheme of arguments, whereby the model is trained and validated on sequences containing both arguments, as well as single arguments. A data point (a1, a2, y), with ai being the token sequence of argument i, is expanded into {(a1, a2, y), (a1, a2, y), (a1, y), (a2, y)}. We duplicate bi-argument samples (a1, a2, y) (in training and development data only) to balance their frequencies against single-argument samples.\nTwo lines of motivation support the inclusion of single argument training examples, grounded in linguistics and machine learning, respectively. First, it has been shown that single arguments in isolation can evoke a strong expectation towards a certain implicit discourse relation, cf. Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs. Second, the procedure may encourage the model to learn better representations of individual argument spans in support of modeling of arguments in composition, cf. LeCun et al. (2015). Due to these aspects, we believe this data augmentation technique to be effective in reinforcing the overall robustness of our model.\nImplementational Details: We train the model using fixed-length sequences of 256 tokens with zero padding at the beginning of shorter sequences and truncate longer ones. Each LSTM has a vector dimensionality of 300, matching the embedding size. The model is regularized by 0.5 dropout rate between the layers and weight decay (2.5e\u22126) on the LSTM inputs. We employ Adam optimization (Kingma and Ba, 2014) using the cross-entropy loss function with mini batch size of 80.4\n4The model is implemented in Keras https:// keras.io/."}, {"heading": "3 Evaluation", "text": "We evaluate our recurrent model on the CoNLL 2016 shared task data5 which include the official training, development and test sets of the CDTB; cf. Table 2 for an overview of the implicit sense distribution.6\nIn accordance with previous setups (Rutherford et al., 2016), we treat entity relations (ENTREL) as implicit and exclude ALTLEX relations. In the evaluation, we focus on the sense-only track, the subtask for which gold arguments are provided and a system is supposed to label a given argument pair with the correct sense. The results are shown in Table 1.\nWith our proposed architecture it is possible to correctly label 257/352 (73.01%) of implicit rela-\n5http://www.cs.brandeis.edu/\u02dcclp/ conll16st/\n6Note that, in the CDTB, implicit relations appear almost three times more often than explicit relations. Out of these, 65% appear within the same sentence. Finally, 25 relations in the training set have two labels.\ntions on the test set, outperforming the best feedforward system of Wang and Lan (2016) and all other word order-agnostic approaches. Development and test set performances suggest the robustness of our approach and its ability to generalize to unseen data.\nAblation Study: We perform an ablation study to quantitatively assess the contribution of two of the characteristic aspects of our model. First, we compare the use of the attention mechanism against the simpler alternative of feeding the final LSTM hidden vectors (h\u2032k and h \u2032\u2032 1) directly to the output layer. When attention is turned off, this yields an absolute decrease in performance of 2.70% on the test set, which is substantial and significant according to a Welch two-sample t-test (p < .001). Second, we independently compare the use of the partial sampling scheme against training on the standard argument pairs in the CDTB. Here, the absence of the partial sampling scheme yields an absolute decrease in accuracy of 5.74% (p < .001), which demonstrates its importance for achieving competitive performance on the task.\nPerformance on the PDTB: As a side experiment, we investigate the model\u2019s language independence by applying it to the implicit argument pairs of the English PDTB. Due to computational time constraints we do not optimize hyperparameters, but instead train the model using identical settings as for Chinese, which is expected to lead to suboptimal performance on the evaluation data. Nevertheless, we measure 27.09% accuracy on the PDTB test set (surpassing the majority class baseline of 22.01%), which shows that the model has potential to generalize across implicit discourse relations in a different language.\nVisualizing Attention Weights: Finally, in Figure 2, we illustrate the learned attention weights which pinpoint important subcomponents within a given implicit discourse relation. For the implicit CONJUNCTION relation the weights indicate a peak on the transition between the argument boundary, establishing a connection between the semantically related terms understandings\u2013agree. Most ENTRELs show an opposite trend: here second arguments exhibit larger intensities than Arg1, as most entity relations follow the characteristic writing style of newspapers by adding additional information by reference to the same entity."}, {"heading": "4 Summary & Outlook", "text": "In this work, we have presented the first attentionbased recurrent neural sense labeler specifically developed for Chinese implicit discourse relations. Its ability to model discourse units sequentially and jointly has been shown to be highly beneficial, both in terms of state-of-the-art performance on the CDTB (outperforming word order-agnostic feedforward approaches), and also in terms of insightful observations into the inner workings of the model through its attention mechanism. The architecture is structurally simple, benefits from partial argument sampling, and can be eas-\nily adapted to similar relation recognition tasks. In future work, we intend to extend our approach to different languages and domains, e.g., to the recent data sets on narrative story understanding or question answering (Mostafazadeh et al., 2016; Feng et al., 2015). We believe that recurrent modeling of implicit discourse information can be a driving force in successfully handling such complex semantic processing tasks.7"}, {"heading": "Acknowledgments", "text": "The authors would like to thank Ayah Zirikly, Philip Schulz and Wei Ding for their very helpful suggestions on an early draft version of the paper, and also thank the anonymous reviewers for their valuable feedback and insightful comments. We are grateful to Farrokh Mehryary for technical support with the attention layer implementation. Computational resources were provided by CSC \u2013 IT Centre for Science, Finland, and Arcada University of Applied Sciences, Helsinki, Finland. Our research at Goethe University Frankfurt was supported by the project \u2018Linked Open Dictionaries (LiODi, 2015-2020)\u2019, funded by the German Ministry for Education and Research (BMBF).\n7The code involved in this study is publicly available at http://www.acoli.informatik. uni-frankfurt.de/resources/."}], "references": [{"title": "Uniform Information Density at the Level of Discourse Relations: Negation Markers and Discourse Connective Omission", "author": ["Fatemeh Torabi Asr", "Vera Demberg."], "venue": "11th International Conference on Computational Semantics (IWCS).", "citeRegEx": "Asr and Demberg.,? 2015", "shortCiteRegEx": "Asr and Demberg.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network", "author": ["Jifan Chen", "Qi Zhang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Com-", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Minwei Feng", "Bing Xiang", "Michael R. Glass", "Lidan Wang", "Bowen Zhou."], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and", "citeRegEx": "Feng et al\\.,? 2015", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Text-level Discourse Parsing with Rich Linguistic Features", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1. Association for Computational Linguis-", "citeRegEx": "Feng and Hirst.,? 2012", "shortCiteRegEx": "Feng and Hirst.", "year": 2012}, {"title": "Chinese Gigaword", "author": ["David Graff", "Ke Chen."], "venue": "LDC Catalog No.: LDC2003T09, ISBN, 1:58563\u2013 58230.", "citeRegEx": "Graff and Chen.,? 2005", "shortCiteRegEx": "Graff and Chen.", "year": 2005}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5-6):602\u2013610. https://doi.org/10.1016/j.neunet.2005.06.042.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Using Discourse Commitments to Recognize Textual Entailment", "author": ["Andrew Hickl."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1. Association for Computational Linguistics, Strouds-", "citeRegEx": "Hickl.,? 2008", "shortCiteRegEx": "Hickl.", "year": 2008}, {"title": "Single-Document Summarization as a Tree Knapsack Problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Chinese Discourse Relation Recognition", "author": ["Hen-Hsen Huang", "Hsin-Hsi Chen."], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing, Chiang Mai, Thailand, pages", "citeRegEx": "Huang and Chen.,? 2011", "shortCiteRegEx": "Huang and Chen.", "year": 2011}, {"title": "A Latent Variable Recurrent Neural Network for Discourse-Driven Language Models", "author": ["Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Discourse Relation Sense Classification Systems for CoNLL-2016 Shared Task", "author": ["Ping Jian", "Xiaohan She", "Chenwei Zhang", "Pengcheng Zhang", "Jian Feng."], "venue": "Proceedings of the CoNLL-16 shared task. Association for", "citeRegEx": "Jian et al\\.,? 2016", "shortCiteRegEx": "Jian et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Temporal Interpretation, Discourse Relations and Commonsense entailment", "author": ["Alex Lascarides", "Nicholas Asher."], "venue": "Linguistics and Philosophy 16(5):437\u2013493.", "citeRegEx": "Lascarides and Asher.,? 1993", "shortCiteRegEx": "Lascarides and Asher.", "year": 1993}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention", "author": ["Yang Liu", "Sujian Li."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Liu and Li.,? 2016", "shortCiteRegEx": "Liu and Li.", "year": 2016}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["William C. Mann", "Sandra A. Thompson."], "venue": "Text 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "The Penn Discourse TreeBank", "author": ["nie Webber"], "venue": null, "citeRegEx": "Webber.,? \\Q2008\\E", "shortCiteRegEx": "Webber.", "year": 2008}, {"title": "Neural Network Models for Im", "author": ["wen Xue"], "venue": null, "citeRegEx": "Xue.,? \\Q2016\\E", "shortCiteRegEx": "Xue.", "year": 2016}, {"title": "Discourse processing for context question answering based on linguistic knowledge", "author": ["Mingyu Sun", "Joyce Y Chai."], "venue": "Knowledge-Based Systems 20(6):511\u2013526.", "citeRegEx": "Sun and Chai.,? 2007", "shortCiteRegEx": "Sun and Chai.", "year": 2007}, {"title": "Discourse connectors for latent subjectivity in sentiment analysis", "author": ["Rakshit S. Trivedi", "Jacob Eisenstein."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguis-", "citeRegEx": "Trivedi and Eisenstein.,? 2013", "shortCiteRegEx": "Trivedi and Eisenstein.", "year": 2013}, {"title": "Two Endto-end Shallow Discourse Parsers for English and Chinese in CoNLL-2016 Shared Task", "author": ["Jianxiang Wang", "Man Lan."], "venue": "Proceedings of the CoNLL-16 shared task. Association for Computational Linguistics, pages 33\u201340.", "citeRegEx": "Wang and Lan.,? 2016", "shortCiteRegEx": "Wang and Lan.", "year": 2016}, {"title": "D-LTAG: extending lexicalized TAG to discourse", "author": ["Bonnie L. Webber."], "venue": "Cognitive Science 28(5):751\u2013779. http://dblp.unitrier.de/db/journals/cogsci/cogsci28.html.", "citeRegEx": "Webber.,? 2004", "shortCiteRegEx": "Webber.", "year": 2004}, {"title": "Discourse Sense Classification from Scratch using Focused RNNs", "author": ["Gregor Weiss", "Marko Bajec."], "venue": "Proceedings of the CoNLL-16 shared task. Association for Computational Linguistics, pages 50\u201354. https://doi.org/10.18653/v1/K16-2006.", "citeRegEx": "Weiss and Bajec.,? 2016", "shortCiteRegEx": "Weiss and Bajec.", "year": 2016}, {"title": "The CoNLL-2016 Shared Task on Shallow Discourse Parsing", "author": ["Nianwen Xue", "Hwee Tou Ng", "Sameer Pradhan", "Bonnie Webber", "Attapol Rutherford", "Chuan Wang", "Hongmin Wang."], "venue": "Proceedings of the Twentieth Conference on Computational Nat-", "citeRegEx": "Xue et al\\.,? 2016", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Shallow Convolutional Neural Network for Implicit Discourse Relation Recognition", "author": ["Biao Zhang", "Jinsong Su", "Deyi Xiong", "Yaojie Lu", "Hong Duan", "Junfeng Yao."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "AttentionBased Bidirectional Long Short-Term Memory Networks for Relation Classification", "author": ["Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "PDTBstyle Discourse Annotation of Chinese Text", "author": ["Yuping Zhou", "Nianwen Xue."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for", "citeRegEx": "Zhou and Xue.,? 2012", "shortCiteRegEx": "Zhou and Xue.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 127, "endOffset": 147}, {"referenceID": 8, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 180, "endOffset": 193}, {"referenceID": 22, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al.", "startOffset": 214, "endOffset": 244}, {"referenceID": 9, "context": "Automatically detecting how meaning units are organized benefits practical downstream applications, such as question answering (Sun and Chai, 2007), recognizing textual entailment (Hickl, 2008), sentiment analysis (Trivedi and Eisenstein, 2013), or text summarization (Hirao et al., 2013).", "startOffset": 268, "endOffset": 288}, {"referenceID": 18, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 15, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 24, "context": "Various formalisms in terms of semantic coherence frameworks have been proposed to account for these contextual assumptions (Mann and Thompson, 1988; Lascarides and Asher, 1993; Webber, 2004).", "startOffset": 124, "endOffset": 191}, {"referenceID": 4, "context": "Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011).", "startOffset": 124, "endOffset": 189}, {"referenceID": 11, "context": "Motivation: Previous work on implicit sense labeling is heavily feature-rich and requires domainspecific, semantic lexicons (Pitler et al., 2009; Feng and Hirst, 2012; Huang and Chen, 2011).", "startOffset": 124, "endOffset": 189}, {"referenceID": 27, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 12, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 2, "context": "These promising neural methods attempt to infer latent representations appropriate for implicit relation classification (Zhang et al., 2015; Ji et al., 2016; Chen et al., 2016).", "startOffset": 120, "endOffset": 176}, {"referenceID": 17, "context": "specific architectures (Liu and Li, 2016), while discourse modeling techniques for Chinese have received very little attention in the literature and are still seriously underrepresented in terms of publicly available systems.", "startOffset": 23, "endOffset": 41}, {"referenceID": 29, "context": "What is more, over 80% of all words in Chinese discourse relations are implicit\u2014compared to only 52% in English (Zhou and Xue, 2012).", "startOffset": 112, "endOffset": 132}, {"referenceID": 26, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established.", "startOffset": 55, "endOffset": 73}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al.", "startOffset": 56, "endOffset": 278}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al.", "startOffset": 56, "endOffset": 300}, {"referenceID": 20, "context": "Recently, in the context of the CoNLL 2016 shared task (Xue et al., 2016), a first independent evaluation platform beyond class level has been established. Surprisingly, the best performing neural architectures to date are standard feedforward networks, cf. Wang and Lan (2016); Schenk et al. (2016); Qin et al. (2016). Even though these specific models completely ignore word order within", "startOffset": 56, "endOffset": 319}, {"referenceID": 28, "context": "Inspired by Zhou et al. (2016), our system is a practical adaptation of the recent", "startOffset": 12, "endOffset": 31}, {"referenceID": 6, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al.", "startOffset": 40, "endOffset": 70}, {"referenceID": 1, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al., 2014; Hermann et al., 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al.", "startOffset": 114, "endOffset": 159}, {"referenceID": 7, "context": "lar its bidirectional mode of operation (Graves and Schmidhuber, 2005), attention mechanisms for recurrent models (Bahdanau et al., 2014; Hermann et al., 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al.", "startOffset": 114, "endOffset": 159}, {"referenceID": 28, "context": ", 2015), and the combined use of these techniques for entity relation recognition in annotated sequences (Zhou et al., 2016).", "startOffset": 105, "endOffset": 124}, {"referenceID": 5, "context": "The embedding layer is initialized using pre-trained word vectors, in our case 300-dimensional Chinese Gigaword vectors (Graff and Chen, 2005).", "startOffset": 120, "endOffset": 142}, {"referenceID": 0, "context": "Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Asr and Demberg (2015) and, in particular, Rohde and Horton (2010) in their psycholinguistic study on implicit causality verbs.", "startOffset": 0, "endOffset": 67}, {"referenceID": 16, "context": "LeCun et al. (2015). Due to these aspects, we believe this data augmentation technique to be effective in reinforcing the overall robustness of our model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "We employ Adam optimization (Kingma and Ba, 2014) using the cross-entropy loss function with mini batch size of 80.", "startOffset": 28, "endOffset": 49}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.", "startOffset": 2, "endOffset": 22}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.", "startOffset": 2, "endOffset": 50}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.", "startOffset": 2, "endOffset": 76}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.", "startOffset": 2, "endOffset": 105}, {"referenceID": 21, "context": "1 Wang and Lan (2016) 73.53 1 Wang and Lan (2016) 72.42 2 Qin et al. (2016) 71.57 2 Schenk et al. (2016) 71.87 3 Schenk et al. (2016) 70.", "startOffset": 2, "endOffset": 134}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.", "startOffset": 20, "endOffset": 31}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.", "startOffset": 20, "endOffset": 65}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.", "startOffset": 20, "endOffset": 91}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.", "startOffset": 20, "endOffset": 122}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.", "startOffset": 20, "endOffset": 153}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07 6 Weiss and Bajec (2016) 61.", "startOffset": 20, "endOffset": 184}, {"referenceID": 19, "context": "59 3 Rutherford and Xue (2016) 70.47 4 Rutherford and Xue (2016) 68.30 4 Qin et al. (2016) 67.41 5 Weiss and Bajec (2016) 66.67 5 Weiss and Bajec (2016) 64.07 6 Weiss and Bajec (2016) 61.44 6 Weiss and Bajec (2016) 63.", "startOffset": 20, "endOffset": 215}, {"referenceID": 13, "context": "51 7 Jian et al. (2016) 21.", "startOffset": 5, "endOffset": 24}, {"referenceID": 13, "context": "51 7 Jian et al. (2016) 21.90 7 Jian et al. (2016) 21.", "startOffset": 5, "endOffset": 51}, {"referenceID": 23, "context": "forward system of Wang and Lan (2016) and all other word order-agnostic approaches.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "tion answering (Mostafazadeh et al., 2016; Feng et al., 2015).", "startOffset": 15, "endOffset": 61}], "year": 2017, "abstractText": "We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model\u2019s ability to selectively focus on the relevant parts of an input sequence.", "creator": "LaTeX with hyperref package"}}}