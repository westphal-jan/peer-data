{"id": "1704.08350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "The MacGyver Test - A Framework for Evaluating Machine Resourcefulness and Creative Problem Solving", "abstract": "Current measures of machine intelligence are either difficult to evaluate or lack the ability to test a robot's problem-solving capacity in open worlds. We propose a novel evaluation framework based on the formal notion of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. The approach uses machine learning to investigate the nature and accuracy of an agent's performance and their impact on an agent's ability to test a problem-solving capacity. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. The approach uses machine learning to investigate the nature and accuracy of an agent's performance and their impact on an agent's ability to test a problem-solving capacity. We propose a novel evaluation framework based on the formal notion of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. The approach uses machine learning to investigate the nature and accuracy of an agent's performance and their impact on an agent's ability to test a problem-solving capacity. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. The approach uses machine learning to investigate the nature and accuracy of an agent's performance and their impact on an agent's ability to test a problem-solving capacity. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents. We propose a novel evaluation framework based on the formal idea of MacGyver Test which provides a practical way for assessing the resilience and resourcefulness of artificial agents.", "histories": [["v1", "Wed, 26 Apr 2017 21:05:27 GMT  (25kb)", "http://arxiv.org/abs/1704.08350v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vasanth sarathy", "matthias scheutz"], "accepted": false, "id": "1704.08350"}, "pdf": {"name": "1704.08350.pdf", "metadata": {"source": "CRF", "title": "The MacGyver Test - A Framework for Evaluating Machine Resourcefulness and Creative Problem Solving", "authors": ["Vasanth Sarathy", "Matthias Scheutz"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n08 35\n0v 1\n[ cs\n.A I]\n2 6\nA pr\n2 01"}, {"heading": "1 Introduction", "text": "Consider a situation when your only suit is covered in lint and you do not own a lint remover. Being resourceful, you reason that a roll of duct tape might be a good substitute. You then solve the problem of lint removal by peeling a full turn\u2019s worth of tape and re-attaching it backwards onto the roll to expose the sticky side all around the roll. By rolling it over your suit, you can now pick up all the lint. This type of everyday creativity and resourcefulness is a hallmark of human intelligence and best embodied in the 1980s television series MacGyver which featured a clever secret service agent who used common objects around him like paper clips and rubber bands in inventive ways to escape difficult life-or-death situations.1\nYet, current proposals for tests of machine intelligence do not measure abilities like resourcefulness or creativity, even though this is exactly what is needed for artificial agents such as space-exploration robots, search-and-rescue agents, or even home and elder-care helpers to be more robust, resilient, and ultimately autonomous.\nIn this paper we thus propose an evaluation framework for machine intelligence and capability consisting of practical tests for inventiveness, resourcefulness, and resilience. Specifically, we introduce the notion ofMacGyver Test (MT) as a practical alternative to the Turing Test intended to advance research.\n1As a society, we place a high value on our human ability to solve novel problems and remain resilient while doing so. Beyond the media, our patent system and peer-reviewed publication systems are additional examples of us rewarding creative problem solving and elegance of solution."}, {"heading": "2 Background: Turing Test and its Progeny", "text": "Alan Turing asked whether machines could produce observable behavior (e.g., natural language) that we (humans)would say required thought in people [Turing, 1950]. He suggested that if an interrogator was unable to tell, after having a long free-flowing conversation with a machine whether she was dealing with a machine or a person, then we can conclude that the machine was \u201cthinking\u201d. Turing did not intend for this to be a test, but rather a prediction of sorts [Cooper and Van Leeuwen, 2013]. Nevertheless, since Turing, others have developed tests for machine intelligence that were variations of the so-called Turing Test to address a common criticism that it was easy to deceive the interrogator. Levesque et al. designed a reading comprehension test, entitled the Winograd Schema Challenge, in which the agent is presented a question having some ambiguity in the referent of a pronoun or possessive adjective. The question asks to determine the referent of this ambiguous pronoun or possessive adjective, by selecting one of two choices [Levesque et al., 2012]. Feigenbaum proposed a variation of the Turing Test in which a machine can be tested against a team of subject matter specialists through natural language conversation [Feigenbaum, 2003]. Other tests attempted to study a machine\u2019s ability to produce creative artifacts and solve novel problems [Boden, 2010; Bringsjord et al., 2001; Bringsjord and Sen, 2016; Riedl, 2014]. Extending capabilities beyond linguistic and creative, Harnad\u2019s Total Turing Test (T3) suggested that the range of capabilities must be expanded to a full set of robotic capacities found in embodied systems [Harnad, 1991]. Schweizer extended the T3 to incorporate species evolution and development over time and proposed the Truly Total Turing Test (T4) to test not only individual cognitive systems but whether as a species the candidate cognitive architecture in question is capable of long-term evolutionary achievement [Schweizer, 2012]. Finding that the Turing Test and its above-mentioned variants were not helping guide research and development, many proposed a task-based approach. Specific task-based goals were designed couched as toy problems that were representative of a real-world task [Cohen, 2005]. The research communities benefited greatly from this approach and focused their efforts towards specific machine capabilities like object recognition, automatic scheduling and planning, scene under-\nstanding, localization and mapping, and even game-playing. Many public competitions and challenges emerged that tested the machine\u2019s performance in applying these capabilities \u2013 from image recognition contests and machine learning contests. Some of these competitions even tested embodiment and robotic capacities, while combining multiple tasks. For example, the DARPA Robotics Challenge tested a robot\u2019s ability to conduct tasks relevant to remote operation including turning valves, using a tool to break through a concrete panel, opening doors, remove debris blocking entryways. Unfortunately, the Turing Test variants as well as the taskbased challenges are not sufficient as true measures of autonomy in the real-world. Autonomy requires a multi-modal ability and an integrated embodied system to interact with the environment, and achieve goals while solving open-world problems with the limited resources available. None of these tests are interested in measuring this sort of intelligence and capability, the sort that is most relevant from a practical standpoint."}, {"heading": "3 The MacGyver Evaluation Framework", "text": "The proposed evaluation framework, based on the idea of MacGyver-esque creativity, is intended to answer the question whether embodied machines can generate, execute and learn strategies for identifying and solving seeminglyunsolvable real-world problems. The idea is to present an agent with a problem that is unsolvable with the agent\u2019s initial knowledge and observing the agent\u2019s problem solving processes to estimate the probability that the agent is being creative: if the agent can think outside of its current context, take some exploratory actions, and incorporate relevant environmental cues and learned knowledge to make the problem tractable (or at least computable) then the agent has the general ability to solve open-world problems more effectively.2\nThis type of problem solving framework is typically used in the area of automated planning for describing various sorts of problems and solution plans and is naturally suited for defining a MacGyver-esque problem and a creative solution strategy. We are now ready to formalize various notions of the MacGyver evaluation framework."}, {"heading": "3.1 Preliminaries - Classical Planning", "text": "We define L to be a first order language with predicates p(t1, . . . , tn) and their negations \u00acp(t1, . . . , tn) , where ti represents terms that can be variables or constants. A predicate is grounded if and only if all of its terms are constants. We will use classical planning notions of a planning domain in L that can be represented as \u03a3 = (S,A, \u03b3), where S represents the set of states, A is the set of actions and \u03b3 are the transition functions. A classical planning problem is a triple P = (\u03a3, s0, g), where s0 is the initial state and g is the goal state. A plan \u03c0 is any sequence of actions and a plan \u03c0 is a solution to the planning problem if g \u2286 \u03b3(s0, \u03c0). We also\n2Note that the proposed MT is a subset of Harnad\u2019s T3, but instead of requiring robots to do \u201ceverything real people do\u201d, MT is focused on requiring robots to exhibit resourcefulness and resilience. MT is also a subset of Schweizer\u2019s T4 which expands T3 with the notion of species-level intelligence.\nconsider the notion of state reachability and the set of all suc-\ncessor states \u0393\u0302(s), which defines the set of states reachable from s."}, {"heading": "3.2 A MacGyver Problem", "text": "To formalize a MacGyver Problem (MGP), we define a universe and then a world within this universe. The world describes the full set of abilities of an agent and includes those abilities that the agent knows about and those of which it is unaware. We can then define an agent subdomain as representing a proper subset of the world that is within the awareness of the agent. An MGP then becomes a planning problem defined in the world, but outside the agent\u2019s current subdomain.\nDefinition 1 (Universe). We first define a Universe U = (S,A, \u03b3) as a classical planning domain representing all aspects of the physical world perceivable and actionable by any and all agents, regardless of capabilities. This includes all the allowable states, actions and transitions in the physical universe.\nDefinition 2 (World). We define a world Wt = (St, At, \u03b3t) as a portion of the Universe U corresponding to those aspects that are perceivable and actionable by a particular species t of agent. Each agent species t \u2208 T has a particular set of sensors and actuators allowing agents in that species to perceive a proper subset of states, actions or transition functions. Thus, a world can be defined as follows:\nW t = {(St, At, \u03b3t) | ((St \u2286 S) \u2228 (At \u2286 A) \u2228 (\u03b3t \u2286 \u03b3))\n\u2227 \u00ac((St = S) \u2227 (At = A) \u2227 (\u03b3t = \u03b3))}\nDefinition 3 (Agent Subdomain). We next define an agent \u03a3ti = (S t i , A t i, \u03b3 t i ) of type t, as a planning subdomain corresponding to the agent\u2019s perception and action within its world Wt. In other words, the agent is not fully aware of all of its capabilities at all times, and the agent domain \u03a3ti corresponds to the portion of the world that the agent is perceiving and acting at time i.\n\u03a3ti = {(S t i , A t i, \u03b3 t i ) | ((S t i \u2282 S t)\u2228(Ati \u2282 A t)\u2228(\u03b3ti \u2282 \u03b3 t))\n\u2227 \u00ac((Sti = S t) \u2227 (Ati = A t) \u2227 (\u03b3ti = \u03b3 t))}\nDefinition 4 (MacGyver Problem). We define a MacGyver Problem (MGP) with respect to an agent t, as a planning problem in the agent\u2019s world Wt that has a goal state g that is currently unreachable by the agent. Formally, an MGP PM = (Wt, s0, g), where:\n\u2022 s0 \u2208 Sti is the initial state of the agent\n\u2022 g is a set of ground predicates\n\u2022 Sg = {s \u2208 S|g \u2286 s}\nWhere g \u2286 s\u2032, \u2200s\u2032 \u2208 \u0393\u0302Wt(s0) \\ \u0393\u0302\u03a3t i (s0)\nIt naturally follows that in the context of a world Wt, the MGP PM is a classical planning problem which from the agent\u2019s current perspective is unsolvable. We can reformulate the MGP as a language recognition problem to be able to do a brief complexity analysis.\nDefinition 5 (MGP-EXISTENCE).Given a set of statements D of planning problems, let MGP-EXISTENCE(D) be the set of all statements P \u2208 D such that P represents a MacGyver Problem PM , without any syntactical restrictions.\nTheorem 1 MGP-EXISTENCE is decidable.\nProof. The proof is simple. The number of possible states in the agent\u2019s subdomain\u03a3ti and the agent\u2019s worldW\nt are finite. So, it is possible to do a brute-force search to see whether a solution exists in the agent\u2019s world but not in the agent\u2019s initial domain.\nTheorem 2 MGP-EXISTENCE is EXPSPACE-complete.\nProof. (Membership). An MGP amounts to looking to see if the problem is a solvable problem in the agent-domain. Upon concluding it is not solvable, the problem then becomes one of determining if it is a solvable problem in the world corresponding to the agent\u2019s species. Each of these problems are PLAN-EXISTENCE problems, which are in EXPSPACE for the unrestricted case [Ghallab et al., 2004]. Thus, MGPEXISTENCE is in EXPSPACE. (Hardness). We can reduce the classical planning problem P (\u03a3, s0, g) to an MGP (PLAN-EXISTENCE\u2264pm MGPEXISTENCE), by defining a new world W. To define a new world, we extend the classical domain by one state, defining the new state as a goal state, and adding actions and transitions from every state to the new goal state. We also set the agent domain to be the same as the classical planning domain. Now, P (\u03a3, s0, g) \u2208 PLAN-EXISTENCE iff P (W, s0, g) \u2208 MGP-EXISTENCE for agent with domain \u03a3. Thus, MGPEXISTENCE is EXPSPACE-hard."}, {"heading": "3.3 Solving a MacGyver Problem", "text": "From Theorems 1 and 2, we know that, while possible, it is intractable for an agent to know whether a given problem is an MGP. From an agent\u2019s perspective, solving an MGP is like solving any planning problemwith the additional requirement to sense or learn some previously unknown state, transition function or action. Specifically, solving an MGP will involve performing some actions in the environment, making observations, extending and contracting the agent\u2019s subdomain and exploring different contexts.\nSolution Strategies Definition 6 (Agent Domain Modification). A domain modification \u03a3t\u2217j involves either a domain extension or a domain contraction3. A domain extension \u03a3t+j of an agent is an Agent-subdomain at time j that is in the agent\u2019s world Wt but not in the agent\u2019s subdomain \u03a3ti in the previous time i, such that \u03a3ti \u03a3 t j . The agent extends its subdomain through sensing and perceiving its environment and its own self - e.g., the agent can extend its domain by making an observation, receiving advice or an instruction or performing introspection. Formally,\n\u03a3t+j = {(S t+ j , A t+ j , \u03b3 t+ j ) | (S t+ j \u2282 S t \\ Sti )\n\u2228 (At+j \u2282 A t \\Ati) \u2228 (\u03b3 t+ j \u2282 \u03b3 t \\ \u03b3ti )}\n3In the interest of brevity we will only consider domain extensions for now.\nThe agent subdomain that results from a domain extension is \u03a3tj = \u03a3 t i \u222a \u03a3 t+ j\nA domain modification set \u2206\u03a3t i = {\u03a3t\u22171 ,\u03a3 t\u2217 2 , . . . ,\u03a3 t\u2217 n } is a set of n domain modifications on subdomain\u03a3ti. Let \u03a3 t \u2206 be the subdomain resulting from applying\u2206\u03a3t i on \u03a3ti\nDefinition 7 (Strategy and Domain-Modifying Strategy). A strategy is a tuple \u03c9 = (\u03c0,\u2206) of a plan \u03c0 and a set \u2206 of domain modifications. A domain-modifying strategy \u03c9C involves at least one domain modification, i.e.,\u2206 6= \u2205.\nDefinition 8 (Context). A context is a tuple Ci = (\u03a3i, si) representing the agent\u2019s subdomain and state at time i.\nWe are now ready to define an insightful strategy as a set of actions and domain modifications that the agent needs to perform to allow for the goal state of the problem to be reachable by the agent.\nDefinition 9 (Insightful Strategy). Let Ci = (\u03a3 t i, s0) be the agent\u2019s current context. Let PM = (Wt, s0, g) be an MGP for the agent in this context. An insightful strategy is a domain-modifying strategy \u03c9I = (\u03c0I ,\u2206I) which when applied in Ci results in a context Cj = (\u03a3 t j , sj), where \u03a3tj = \u03a3 t \u2206I such that g \u2286 s \u2032, \u2200s\u2032 \u2208 \u0393\u0302\u03a3tj (sj).\nFormalizing the insightful strategy in this way is somewhat analogous to the moment of insight that is reached when a problem becomes tractable (or in our definition computable) or when solution plan becomes feasible. Specifically, solving a problem involves some amount of creative exploration and domain extensions and contractions until the point when the agent has all the information it needs within its subdomain to solve the problem as a classical planning problem, and does not need any further domain extensions. We can alternatively define an insightful strategy in terms of when the goal state is not only reachable, but a solution can be discovered in polynomial time. We will next review simple toy examples to illustrate the concepts discussed thus far."}, {"heading": "4 Operationalizing a MacGyver Problem", "text": "We will consider two examples that will help operationalize the formalism presented thus far. The first is a modification of the popular Blocks World planning problem. The second is a more practical task of tightening screws, however, with the caveat that certain common tools are unavailable and the problem solver must improvise. We specifically discuss various capabilities that an agent must possess in order to overcome the challenges posed by the examples."}, {"heading": "4.1 Toy Example: Block-and-Towel World", "text": "Consider an agent tasked with moving a block from one location to another which the agent will not be able to execute without first discovering some new domain information. Let the agent subdomain \u03a3 consist of a set of locations l = {L1, L2, L3}, two objects o = {T,B} a towel and a block, and a function locationOf : o \u2192 l representing the location of object o. Suppose the agent is aware of the following predicates and their negations:\n\u2022 at(o, l): an object o is at location l\n\u2022 near(l): the agent is near location l \u2022 touching(o): the agent is touching object o \u2022 holding(o): the agent is holding the object o\nWe define a set of actions A in the agent domain as follows:\n\u2022 reach(o, l): Move the robot arm to near the object o precond: {at(o, l)} effect: {near(l)} \u2022 grasp(o, l): Grasp object o at l precond: {near(l), at(o, l)} effect: {touching(o)} \u2022 lift(o, l): Lift object o from l precond: {touching(o), at(o, l)} effect: {holding(o),\u00acat(o, l),\u00acnear(l)} \u2022 carryTo(o, l): Carry object o to l precond: {holding(o)} effect: {\u00acholding(o), at(o, l)} \u2022 release(o, l): Release object o at l precond: {touching(o), at(o, l)} effect: {\u00actouching(o), at(o, l)}\nGiven an agent domain \u03a3, and a start state s0 as defined below, we can define the agent contextC = (\u03a3, s0) as a tuple with the agent domain and the start state.\ns0 = {at(T, L1), at(B,L2),\u00acholding(T ),\u00acholding(B),\n\u00acnear(L1),\u00acnear(L2),\u00acnear(L3),\u00actouching(T ),\n\u00actouching(B), locationOf (B) = L2,\nlocationOf (T ) = L1}\nConsider a simple planning problem for the Block-andTowel World in which the agent must move the blockB from location L2 to L3. The agent could execute a simple plan as follows to solve the problem:\n\u03c01 = {reach(B, locationOf (B)),\ngrasp(B, locationOf (B)),\nlift(B, locationOf (B)), carryTo(B,L3), release(B,L3)}\nDuring the course of the plan, the agent touches and holds the block as it moves it from location L2 to L3. Using a similar plan, the agent could move the towel to any location, as well. Now, consider a more difficult planning problem in which the agent is asked to move the block from L2 to L3 without touching it. Given the constraints imposed by the problem, the goal state, is not reachable and the agent must discover an alternative way to move the block. To do so, the agent must uncover states in the worldWt that were previously not in its subdomain \u03a3. For example, the agent may learn that by moving the towel to location L2, the towel \u201ccovers\u201d the block, so it might discover a new predicate covered(o1, o2) that would prevent it from touching the block. The agent may also uncover a new action push(o, l1, l2) which would allow it to push the object along the surface. To uncover new predicates and actions, the agent may have to execute an insightful strategy \u03c9I . Once the agent\u2019s domain has been extended, the problem becomes a standard planning problem for which the agent can discover a solution plan for covering the block with the towel and then pushing both the towel and the block\nfrom location L2 to L3. In order to autonomously resolve this problem, the agent must be able to recognize when it is stuck, discover new insights, and build new plans. Additionally, the agent must be able to actually execute this operation in the real-world. That is, the agent must have suitable robotic sensory and action mechanisms to locate and grasp and manipulate the objects."}, {"heading": "4.2 Practical Example: Makeshift Screwdriver", "text": "Consider the practical example of attaching or fastening things together, a critical task in many domains, which, depending on the situation, can require resilience to unexpected events and resourcefulness in finding solutions. Suppose an agent must affix two blocks from a set of blocks b = {B1, B2}. In order to do so, the agent has a tool box containing a set of tools t = {screwdriver, plier, hammer} and a set of fasteners f = {screw, nail}. In addition, there are other objects in the agent\u2019s environment o = {towel, coin,mug, ducttape}. Assume the agent can sense the following relations (i.e., predicates and their negations) with respect to the objects 4:\n\u2022 isAvailable(t): tool t is available to use \u2022 fastenWith(t, f): tool t is designed for fastener f \u2022 grabWith(t): tool t is designed to grab a fastener f \u2022 isHolding(t): agent is holding tool t \u2022 isReachable(t, f): tool t can reach fastener f \u2022 isCoupled(t, f): tool t is coupled to fastener f \u2022 isAttachedTo(f, b1, b2): fastener f is attached to or inserted into blocks b1 and b2 \u2022 isSecured(f, b1, b2): fastener f is tightly secured into blocks b1 and b2.\nWe can also define a set of actions in the agent subdomain as follows:\n\u2022 select(t, f): select/grasp a tool t to use with fastener f precond: {isAvailable(t), fastenWith(t, f)} effect: {isHolding(t)} \u2022 grab(t, f): Grab a fastener f with tool t. precond: {isHolding(t), grabWith(t)} effect: {isCoupled(t, f)} \u2022 placeAndAlign(f, b1, b2): Place and align fastener f , and blocks b1 and b2 effect: {isAttachedTo(f, b1, b2)} \u2022 reachAndEngage(t, f): Reach and engage the tool t with fastener f precond: {isHolding(t), fastenWith(t, f), isReachable(t, f)} effect: {isCoupled(t, f)} \u2022 install(f, t, b1, b2): Install the fastener f with tool t precond: {isCoupled(t, f), isAttachedTo(f, b1, b2)} effect: {isSecured(f, b1, b2}\nNow suppose a screw has been loosely inserted into two blocks (isAttachedTo(screw,B1, B2)) and needs\n4Given space limitations, we have not presented the entire domain represented by this example. Nevertheless, our analysis of the MacGyver-esque properties should still hold.\nto be tightened (\u00acisSecured(screw,B1, B2)). Tightening a screw would be quite straightforward by performing actions select(), reachAndEngage(), install(). But for some reason the screwdriver has gone missing (\u00acisAvailable(screwdriver)). This is a MacGyver problem because there is no way for the agent, given its current subdomain of knowledge, to tighten the screw as the goal state of isSecured(screw,B1, B2) is unreachable from the agent\u2019s current context. Hence, the agent must extend its domain. One approach is to consider one of the non-tool objects, e.g., a coin could be used as a screwdriver as well, while a mug or towel might not.\nThe agent must be able to switch around variables in its existing knowledge to expose previously unknown capabilities of tools. For example, by switching grab(t, f) to grab(t, o) the agent can now explore the possibility of grabbing a coin with a plier. Similarly, by relaxing constraints on variables in other relations, the agent can perform a reachAndEngage(o, f) action whereby it can couple a makeshift tool, namely the coin, with the screw.\nWhat if the screw was in a recessed location and therefore difficult to access without an elongate arm? While the coin might fit on the head of the screw, it does not have the necessary elongation and would not be able to reach the screw. An approach here might be to grab the coin with the plier and use that assembly to tighten the screw, maybe even with some additionally duct tape for extra support. As noted earlier, generally, the agent must be able to relax some of the preexisting constraints and generate new actions. By exploring and hypothesizing and then testing each variation, the agent can expand its domain.\nThis example, while still relatively simple for humans, helps us highlight the complexity of resources needed for an agent to perform the task. Successfully identifying and building a makeshift screwdriver when a standard screwdriver is not available shows a degree of resilience to events and autonomy and resourcefulness that we believe to be an important component of everyday creativity and intelligence. By formulating the notion of resourcefulness in this manner, we can better study the complexity of the cognitive processes and also computationalize these abilities and even formally measure them.\nAgent Requirements: Intelligence and Physical Embodiment\nWhen humans solve problems, particularly creative insight problems, they tend to use various heuristics to simplify the search space and to identify invariants in the environment that may or may not be relevant [Knoblich, 2009]. An agent solving an MGP must possess the ability to execute these types of heuristics and cognitive strategies. Moreover, MGPs are not merely problems in the classical planning sense, but require the ability to discover when a problem is unsolvable from a planning standpoint and then discover, through environmental exploration, relevant aspects of its surroundings in order to extend its domain of knowledge. Both these discoveries in turn are likely to require additional cognitive resources and heuristics that allow the agent to make these discoveries ef-\nficiently. Finally, the agent must also be able to remember this knowledge and be able to, more efficiently, solve future instances of similar problems. From a real-world capabilities standpoint, the agent must possess the sensory and action capabilities to be able to execute this exploration and discovery process, including grasping and manipulating unfamiliar objects. These practical capabilities are not trivial, but in combination with intelligent reasoning, will provide a clear demonstration of agent autonomy while solving practical real-world problems. These examples provide a sense for the types of planning problems that might qualify as an MGP. Certain MGPs are more challenging than others and we will next present a theoretical measure for the difficulty of an MGP."}, {"heading": "4.3 Optimal Solution and M-Number", "text": "Generally, we can assume that a solvable MGP has a best solution that involves an agent taking the most effective actions, making the required observations as and when needed and uncovering a solution using the most elegant strategy. We formalize these notions by first defining optimal solutions and then the M-Number, which is the measure of the complexity of an insightful strategy in the optimal solution.\nDefinition 10 (Optimal Solutions). Let PM = (Wt, s0, g) be an MGP for the agent. Let \u03c0\u0302 be an optimal solution plan to PM . A set of optimal domain modifications is a set of domain modifications \u2206\u0302 is the minimum set of domain modifications needed for the inclusion of actions in the optimal solution plan \u03c0\u0302. An optimal solution strategy is a solution\nstrategy \u03c9\u0302 = (\u03c0\u0302, \u2206\u0302), where \u2206\u0302 is a set of optimal domain modifications.\nDefinition 11 (M-Number). Let PM = (Wt, s0, g) be an MGP for the agent. Let \u2126\u0302 = {\u03c9\u03021, . . . , \u03c9\u0302n} be the set of n optimal solution strategies. For each \u03c9\u0302i \u2208 \u2126\u0302, there exists an insightful strategy \u03c9\u0302Ii \u2286 \u03c9\u0302i. Let \u2126\u0302 I = {\u03c9\u0302I1 , . . . , \u03c9\u0302 I n} be the set of optimal insightful strategies. The set \u2126\u0302I can be represented by a program p on some prefix universal Turing machine capable of listing elements of \u2126\u0302I and halting. We can then use Kolmogorov complexity of the set of these insightful strategies, K(\u2126\u0302I) := minp\u2208B\u2217{|p| : U(p) computes \u2126\u0302I} [Li and Vita\u0301nyi, 1997]. We define the intrinsic difficulty of the MGP (M-Number orM) as the Kolmogorov complexity of the set of optimal insightful strategies \u2126\u0302I , M = K(\u2126\u0302I).\nAs we have shown MGP-EXISTENCE is intractable and measuring the intrinsic difficulty of an MGP is not computable if we use Kolmogorov complexity. Even if we instead choose to use an alternative and computable approximation to Kolmogorov complexity (e.g., Normalized Compression Distance), determining the M-Number is difficult to do as we must consult an oracle to determine the optimal solution. In reality, an agent does not know that the problem it is facing is an MGP and even if it did know this, the agent would have a tough time determining how well it is doing."}, {"heading": "4.4 Measuring Progress and Agent Success", "text": "When we challenge each other with creative problems,we often know if the problem-solver is getting closer (\u201cwarmer\u201d) to\nthe solution. We formalize this idea using Solomonoff Induction. To do so, we will first designate a \u201cjudge\u201d who, based on a strategy currently executed by the agent, guesses the probability that, in some finite number of steps, the agent is likely to have completed an insightful strategy.\nConsider an agent performing a strategy \u03c9 to attempt to solve an MGP PM and a judge evaluating the performance of the agent. The judgemust first attempt to understandwhat the agent is trying to do. Thus, the judge must first hypothesize an agent model that is capable of generating \u03c9.\nLet the agent be defined by the probability measure \u00b5(\u03c9 | PM ,C), where this measure represents the probability that an agent generates a strategy\u03c9 given anMGPPM when in a particular contextC. The judge does not know \u00b5 in advance and the measure could change depending on the type of agent. For example, a random agent could have \u00b5(\u03c9) = 2\u2212|\u03c9|, whereas a MacGyver agent could be represented by a different probability measure. Not knowing the type of agent, we want the judge to be able to evaluate as many different types of agents as possible. There are infinitely many different types of agents and accordingly infinitely many different hypotheses \u00b5 for an agent. Thus, we cannot simply take an expected value with respect to a uniform distribution, as some hypotheses must be weighed more heavily than others.\nSolomonoff devised a universal distribution over a set of computable hypotheses from the perspective of computability theory [Solomonoff, 1960]. The universal prior of a hypothesis was defined:\nP (\u00b5) \u2261 \u2211\np:U(p,\u03c9)=\u00b5(\u03c9)\n2\u2212|p|\nThe judge applies the principle of Occam\u2019s razor - given many explanations (in our case hypotheses), the simplest is the most likely, and we can approximate P (\u00b5) \u2261 2\u2212K(\u00b5), whereK(\u00b5) is the Kolmogorov complexity of measure \u00b5. To be able to measure the progress of an agent solving an MGP, we must be able to define a performance metric R\u00b5. In this paper, we do not develop any particular performance metric, but suggest that a performance metric be proportional to the level of resourcefulness and creativity of the agent. Generally, measuring progress may depend on problem scope, control variables, length and elegance of the solution and other factors. Nevertheless, a simple measure of this sort can serve as a placeholder to develop our theory.\nWe are now ready to define the performance or progress of an agent solving an MGP.\nDefinition 12 (Expected Progress). Consider an agent in context C = (\u03a3ti, s0) solving an MGP PM = (W\nt, s0, g). The agent has executed strategy \u03c9 comprising actions and domain modifications. Let U be the space of all programs that compute a measure of agent resourcefulness. Consider a judge observing the agent and fully aware of the agent\u2019s context and knowledge and the MGP itself. Let the judge be prefix universal Turing machine U and letK be the Kolmogorov complexity function. Let the performance metric, which is an interpretation of the cumulative state of the agent resourcefulness in solving the MGP, be R\u00b5. The expected progress of\nthis agent as adjudicated by the judge is:\nM(\u03c9) \u2261 \u2211\n\u00b5\u2208U\n2\u2212K(\u00b5) \u00b7R\u00b5\nNow, we are also interested in seeing whether the agent, given this strategy \u03c9 is likely to improve its performance over the next k actions. The judge will need to predict the continuation of this agent\u2019s strategy taking all possible hypotheses of the agent\u2019s behavior into account. Let \u03c9+ be a possible continuation and let a represent concatenation.\nM(\u03c9a\u03c9+ | \u03c9) = M(\u03c9a\u03c9+)\nM(\u03c9)\nThe judge is a Solomonoff predictor such that the predicted finite continuation \u03c9+ is likely to be one in which \u03c9a\u03c9+ is less complex in the Kolmogorov sense. The judge measures the state of the agent\u2019s attempts at solving the MGP and can also predict how the agent is likely to perform in the future."}, {"heading": "5 Conclusion and Future Work", "text": "In the Apollo 13 space mission, astronauts together with ground control had to overcome several challenges to bring the team safely back to Earth [Lovell and Kluger, 2006]. One of these challenges was controlling carbon dioxide levels onboard the space craft: \u201cFor two days straight [they] had worked on how to jury-rig the Odysseys canisters to the Aquariuss life support system. Now, using materials known to be available onboard the spacecraft \u2013 a sock, a plastic bag, the cover of a flight manual, lots of duct tape, and so on \u2013 the crew assembled a strange contraption and taped it into place. Carbon dioxide levels immediately began to fall into the safe range.\u201d [Cass, 2005; Team, 1970]. We proposed the MacGyver Test as a practical alternative to the Turing Test and as a formal alternative to robotic and machine learning challenges. The MT does not require any specific internal mechanism for the agent, but instead focuses on observed problem-solving behavior akin to the Apollo 13 team. It is flexible and dynamic allowing for measuring a wide range of agents across various types of problems. It is based on fundamental notions of set theory, automated planning, Turing computation, and complexity theory that allow for a formal measure of task difficulty. AlthoughKolmogorov complexity and the Solomonoff Induction measures are not computable, they are formally rigorous and can be substituted with computable approximations for practical applications. In future work, we plan to develop more examples of MGPs and also begin to unpack any interesting aspects of the problem\u2019s structure, study its complexity and draw comparisons between problems. We believe that the MT formally captures the concept of practical intelligence and everyday creativity that is quintessentially human and practically helpful when designing autonomous agents. Most importantly, the intent of the MT and the accompanying MGP formalism is to help guide research by providing a set of mathematically formal specifications for measuring AI progress based on an agent\u2019s ability to solve increasingly difficult MGPs. We thus invite researchers to develop MGPs of varying difficulty and design agents that can solve them."}], "references": [{"title": "Kybernetes", "author": ["Margaret A. Boden. The Turing test", "artistic creativity"], "venue": "39(3):409\u2013413,", "citeRegEx": "Boden. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "On Creative Self-Driving Cars: Hire the Computational Logicians", "author": ["Selmer Bringsjord", "Atriya Sen"], "venue": "Fast. page Forthcoming,", "citeRegEx": "Bringsjord and Sen. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "the Turing test", "author": ["Selmer Bringsjord", "Paul Bello", "David Ferrucci. Creativity"], "venue": "and the (better) Lovelace Test. Minds and Machines, 11(1):3\u201327,", "citeRegEx": "Bringsjord et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Apollo 13", "author": ["Stephen Cass"], "venue": "we have a solution. IEEE Spectrum On-line, 04, 1,", "citeRegEx": "Cass. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "If not turing\u2019s test", "author": ["Paul R Cohen"], "venue": "then what? AI Magazine, 26(4):61,", "citeRegEx": "Cohen. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Journal of the ACM", "author": ["Edward A. Feigenbaum. Some challenges", "grand challenges for computational intelligence"], "venue": "50(1):32\u201340,", "citeRegEx": "Feigenbaum. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Paolo Traverso", "author": ["Malik Ghallab", "Dana Nau"], "venue": "Automated Planning: Theory and Practice.", "citeRegEx": "Ghallab et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "other minds: A machine incarnation of an old philosophical problem", "author": ["Stevan Harnad. Other bodies"], "venue": "Minds and Machines, 1(1):43\u201354,", "citeRegEx": "Harnad. 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "pages 275\u2013300", "author": ["G\u00fcnther Knoblich. Psychological research on insight problem solving. In Recasting reality"], "venue": "Springer,", "citeRegEx": "Knoblich. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "The Winograd Schema Challenge", "author": ["Hector Levesque", "Ernest Davis", "Leora Morgenstern"], "venue": "Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Levesque et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Second edition)", "author": ["M Li", "Paul Vit\u00e1nyi. An introduction to Kolmogorov complexity", "its applications"], "venue": "Computers & Mathematics with Applications, 34:137,", "citeRegEx": "Li and Vit\u00e1nyi. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Apollo 13", "author": ["Jim Lovell", "Jeffrey Kluger"], "venue": "Houghton Mifflin Harcourt,", "citeRegEx": "Lovell and Kluger. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Riedl", "author": ["O Mark"], "venue": "The Lovelace 2.0 Test of Artificial Creativity and Intelligence. arXiv preprint arXiv:1410.6142v3, page 2,", "citeRegEx": "Riedl. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Minds and Machines", "author": ["Paul Schweizer. The externalist foundations of a truly total turing test"], "venue": "22(3):191\u2013212,", "citeRegEx": "Schweizer. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A preliminary report on a general theory of inductive inference", "author": ["RJ Solomonoff"], "venue": "Zator Technical Bulletin, (138)", "citeRegEx": "Solomonoff. 1960", "shortCiteRegEx": null, "year": 1960}, {"title": "MIND", "author": ["Alan M Turing. Computing Machine", "Intelligence"], "venue": "LIX(236):433\u2013460,", "citeRegEx": "Turing. 1950", "shortCiteRegEx": null, "year": 1950}], "referenceMentions": [{"referenceID": 15, "context": ", natural language) that we (humans)would say required thought in people [Turing, 1950].", "startOffset": 73, "endOffset": 87}, {"referenceID": 9, "context": "The question asks to determine the referent of this ambiguous pronoun or possessive adjective, by selecting one of two choices [Levesque et al., 2012].", "startOffset": 127, "endOffset": 150}, {"referenceID": 5, "context": "Feigenbaum proposed a variation of the Turing Test in which a machine can be tested against a team of subject matter specialists through natural language conversation [Feigenbaum, 2003].", "startOffset": 167, "endOffset": 185}, {"referenceID": 0, "context": "Other tests attempted to study a machine\u2019s ability to produce creative artifacts and solve novel problems [Boden, 2010; Bringsjord et al., 2001; Bringsjord and Sen, 2016; Riedl, 2014].", "startOffset": 106, "endOffset": 183}, {"referenceID": 2, "context": "Other tests attempted to study a machine\u2019s ability to produce creative artifacts and solve novel problems [Boden, 2010; Bringsjord et al., 2001; Bringsjord and Sen, 2016; Riedl, 2014].", "startOffset": 106, "endOffset": 183}, {"referenceID": 1, "context": "Other tests attempted to study a machine\u2019s ability to produce creative artifacts and solve novel problems [Boden, 2010; Bringsjord et al., 2001; Bringsjord and Sen, 2016; Riedl, 2014].", "startOffset": 106, "endOffset": 183}, {"referenceID": 12, "context": "Other tests attempted to study a machine\u2019s ability to produce creative artifacts and solve novel problems [Boden, 2010; Bringsjord et al., 2001; Bringsjord and Sen, 2016; Riedl, 2014].", "startOffset": 106, "endOffset": 183}, {"referenceID": 7, "context": "Extending capabilities beyond linguistic and creative, Harnad\u2019s Total Turing Test (T3) suggested that the range of capabilities must be expanded to a full set of robotic capacities found in embodied systems [Harnad, 1991].", "startOffset": 207, "endOffset": 221}, {"referenceID": 13, "context": "Schweizer extended the T3 to incorporate species evolution and development over time and proposed the Truly Total Turing Test (T4) to test not only individual cognitive systems but whether as a species the candidate cognitive architecture in question is capable of long-term evolutionary achievement [Schweizer, 2012].", "startOffset": 300, "endOffset": 317}, {"referenceID": 4, "context": "Specific task-based goals were designed couched as toy problems that were representative of a real-world task [Cohen, 2005].", "startOffset": 110, "endOffset": 123}, {"referenceID": 6, "context": "Each of these problems are PLAN-EXISTENCE problems, which are in EXPSPACE for the unrestricted case [Ghallab et al., 2004].", "startOffset": 100, "endOffset": 122}, {"referenceID": 8, "context": "When humans solve problems, particularly creative insight problems, they tend to use various heuristics to simplify the search space and to identify invariants in the environment that may or may not be relevant [Knoblich, 2009].", "startOffset": 211, "endOffset": 227}, {"referenceID": 10, "context": "then use Kolmogorov complexity of the set of these insightful strategies, K(\u03a9\u0302) := minp\u2208B\u2217{|p| : U(p) computes \u03a9\u0302} [Li and Vit\u00e1nyi, 1997].", "startOffset": 115, "endOffset": 137}, {"referenceID": 14, "context": "Solomonoff devised a universal distribution over a set of computable hypotheses from the perspective of computability theory [Solomonoff, 1960].", "startOffset": 125, "endOffset": 143}, {"referenceID": 11, "context": "In the Apollo 13 space mission, astronauts together with ground control had to overcome several challenges to bring the team safely back to Earth [Lovell and Kluger, 2006].", "startOffset": 146, "endOffset": 171}, {"referenceID": 3, "context": "\u201d [Cass, 2005; Team, 1970].", "startOffset": 2, "endOffset": 26}], "year": 2017, "abstractText": "Current measures of machine intelligence are either difficult to evaluate or lack the ability to test a robot\u2019s problem-solving capacity in open worlds. We propose a novel evaluation framework based on the formal notion ofMacGyver Testwhich provides a practical way for assessing the resilience and resourcefulness of artificial agents.", "creator": "LaTeX with hyperref package"}}}