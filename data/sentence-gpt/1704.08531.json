{"id": "1704.08531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "abstract": "This paper aims to catalyze the discussions about text feature extraction techniques using neural network architectures. The research questions discussed in the paper focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 27 Apr 2017 12:27:25 GMT  (118kb,D)", "http://arxiv.org/abs/1704.08531v1", "9 pages, 2 figures"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vineet john"], "accepted": false, "id": "1704.08531"}, "pdf": {"name": "1704.08531.pdf", "metadata": {"source": "CRF", "title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "authors": ["Vineet John"], "emails": ["v2john@uwaterloo.ca"], "sections": [{"heading": "1 Motivation", "text": "A majority of the methods currently in use for textbased feature extraction rely on relatively simple statistical techniques. For instance, a word cooccurrence model like n-grams or a bag-of-words model like TF-IDF.\nThe motivation of this research project is to identify and survey the techniques that use neural networks and study them in juxtaposition with the traditional text feature extraction models to show their differences in approach.\nFeature extraction of text can be used for a multitude of applications including - but not limited to - unsupervised semantic similarity detection, article classification and sentiment analysis.\nThe goal of this project is to document of the differences, advantages and drawbacks in the domain of feature extraction from text data using neural networks. It also sketches the evolution of such techniques over time.\nThis report could serve as a quick cheat-sheet for engineers looking to build a text classification or regression pipeline, as the discussion (Section 15) would serve to map a use-cases to feature extraction implementation specifics."}, {"heading": "2 Research Questions", "text": "RQ1 What are the relatively simple statistical techniques to extract features from text?\nRQ2 Is there any inherent benefit to using neural networks as opposed to the simple methods?\nRQ3 What are the trade-offs that neural networks incur as opposed to the simple methods?\nRQ4 How do the different techniques compare to each other in terms of performance and accuracy?\nRQ5 In what use-cases do the trade-offs outweigh the benefits of neural networks?"}, {"heading": "3 Methodology", "text": "The research questions listed in Section 2 will be tackled by surveying a few of the important overview papers on the topic(Goldberg, 2016)(Bengio et al., 2003)(Morin and Bengio, 2005). A few of the groundbreaking research papers in this area will also be studied, including word embeddings(Mikolov et al., 2013a)(Mikolov et al., 2013b)(Mikolov et al., 2013c).\nIn addition to this, other less-obvious methods of features extraction will be surveyed, including tasks like part-of-speech tagging, chunking, named entity recognition, and semantic role labeling(Socher et al., 2011)(Luong et al., 2013)(Maas et al., 2015)(Li et al., 2015)(Collobert et al., 2011)(Pennington et al., 2014)."}, {"heading": "4 Background", "text": "This section provides a high level background of the tasks within Computational Linguistics."}, {"heading": "4.1 Part-of-Speech Tagging", "text": "\u2022 POS tagging aims to label each word with a unique tag that indicates its syntactic role,\nar X\niv :1\n70 4.\n08 53\n1v 1\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 7\nlike noun, verb, adjective etc.\n\u2022 The best POS taggers are based on classifiers trained on windows of text, which are then fed to a bidirectional decoding algorithm during inference.\n\u2022 In general, models resemble a bi-directional dependency network, and can be trained using a variety of methods including support vector machines and bi-directional Viterbi decoders."}, {"heading": "4.2 Chunking", "text": "\u2022 Chunking aims to label segments of a sentence with syntactic constituents such as noun or verb phrases. It is also called shallow parsing and can be viewed as a generalization of part-of-speech tagging to phrases instead of words.\n\u2022 Implementations of chunking usually require an underlying POS implementation, after which the words are compounded or chunked by concatenation."}, {"heading": "4.3 Named Entity Recognition", "text": "\u2022 NER labels atomic elements in a sentence into categories such as PERSON or LOCATION.\n\u2022 Features to train NER classifiers include POS tags, CHUNK tags, prefixes and suffixes, and large lexicons of the labeled entities."}, {"heading": "4.4 Semantic Role Labeling", "text": "\u2022 SRL aims to assign a semantic role to a syntactic constituent of a sentence.\n\u2022 State-of-the-art SRL systems consist of several stages: producing a parse tree, identifying which parse tree nodes represent the arguments of a given verb, and finally classifying these nodes to compute the corresponding SRL tags.\n\u2022 SRL systems usually entail numerous features like the parts of speech and syntactic labels of words and nodes in the tree, the syntactic path to the verb in the parse tree, whether a node in the parse tree is part of a noun or verb phrase etc."}, {"heading": "5 Document Vectorization", "text": "Document vectorization is needed to convert text content into a numeric vector representation that can be utilized as features, which can then be used to train a machine learning model on. This section talks about a few different statistical methods for computing this feature vector(John and Vechtomova, 2017)."}, {"heading": "5.1 N-gram Model", "text": "N-grams are contiguous sequences of \u2018n\u2019 items from a given sequence of text or speech. Given a complete corpus of documents, each tuple of \u2018n\u2019 grams, either characters or words are represented by a unique bit in a bit vector, which, when aggregated for a body of text, form a sparse vectorized representation of the text in the form of n-gram occurrences."}, {"heading": "5.2 TF-IDF Model", "text": "Term frequency - inverse document frequency (TF-IDF), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus (Sparck Jones, 1972). The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. It is a bag-ofwords model, and doesn\u2019t preserve word ordering."}, {"heading": "5.3 Paragraph Vector Model", "text": "A Paragraph Vector model is comprised of an unsupervised learning algorithm that learns fixedsize vector representations for variable-length pieces of texts such as sentences and documents (Le and Mikolov, 2014). The vector representations are learned to predict the surrounding words in contexts sampled from the paragraph.\nTwo distinct implementations have gained prominence in the community.\n\u2022 Doc2Vec: A Python library implementation in Gensim. 1.\n\u2022 FastText: A standalone implementation in C++. (Bojanowski et al., 2016) (Joulin et al., 2016).\n1https://radimrehurek.com/gensim/models/doc2vec.html"}, {"heading": "6 A Primer of Neural Net Models for", "text": "NLP(Goldberg, 2016)\n\u2022 Fully connected feed-forward neural networks are non-linear learners that can be used as a drop-in replacement wherever a linear learner is used.\n\u2022 The high accuracy observed in experimental results is a consequence of this nonlinearity along with the availability of pretrained word embeddings.\n\u2022 Multi-layer feed-forward networks can provide competitive results on sentiment classification and factoid question answering\n\u2022 Convolutional and pooling architecture show promising results on many tasks, including document classification, short-text categorization, sentiment classification, relation type classification between entities, event detection, paraphrase identification, semantic role labeling, question answering, predicting box-office revenues of movies based on critic reviews, modeling text interestingness, and modeling the relation between charactersequences and part-of-speech tags.\n\u2022 Convolutional and pooling architectures allow us to encode arbitrarily large items as fixed size vectors capturing their most salient features, but, they do so by sacrificing most of the structural information.\n\u2022 Recurrent and recursive networks allows using sequences and trees and preserve the structural information.\n\u2022 Recurrent models have been shown to produce very strong results for language modeling as well as for sequence tagging, machine translation, dependency parsing, sentiment analysis, noisy text normalization, dialog state tracking, response generation, and modeling the relation between character sequences and part-of-speech tags.\n\u2022 Recursive models were shown to produce state-of-the-art or near state-of-the-art results for constituency and dependency parse reranking, discourse parsing, semantic relation classification, political ideology detection based on parse trees, sentiment classification, target-dependent sentiment classification and question answering.\n\u2022 Convolutional nets are observed to to work\nwell for summarization related tasks, just as recurrent/recursive nets work well for language modeling tasks."}, {"heading": "7 A Neural Probabilistic Language Model", "text": "Goal: Knowing the basic structure of a sentence, one should be able to create a new sentence by replacing parts of the old sentence with interchangeable entities(Bengio et al., 2003).\nChallenge: The main bottleneck is computing the activations of the output layer, since it is a fully-connected softmax activation layer.\nDescription: \u2022 One of the major contributions of this paper\nin terms of optimizations was data parallel processing (different processors working on a different subsets of data) and asynchronous processor usage of shared memory.\n\u2022 The authors propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences.\n\u2022 A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality. It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variables (such as words in a sentence, or discrete attributes in a datamining task).\n\u2022 State-of-the art results are typically obtained using trigrams.\n\u2022 Language generation via substitution of semantically similar language constructs of existing sentences can be done via sharedparameter multi-layer neural networks.\n\u2022 The objective of this paper is to obtain realvalued vector sequences of words and learn a joint probability function for those sequences of words alongside the feature vector, and hence, jointly learn both the real-valued vector representation and the parameters of the probability distribution.\n\u2022 This probability function can be tuned in or-\nder to maximize log-likelihood of the training data, while penalizing the cost function, similar to the penalty term one used in Ridge regression.\n\u2022 This will ensure that semantically similar words end up with an almost equivalent feature vectors, called learned distributed feature vectors.\n\u2022 A challenge with modeling discrete variables like a sentence structure as opposed to a continuous value is that the continuous valued function can be assumed to have some form of locality, but the same assumption cannot be made in case of discrete functions.\n\u2022 N-gram models try to achieve a statistical modeling of languages by calculating the conditional probabilities of each possible word that can follow a set of n preceding words.\n\u2022 New sequences of words can be generated by effectively gluing together the popular combinations i.e. n-grams with very high frequency counts."}, {"heading": "8 Hierarchical Probabilistic Neural Network Language Model", "text": "Goal: Implementing a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet2 semantic hierarchy(Morin and Bengio, 2005).\nDescription: \u2022 Similar to the previous paper, attempts to\ntackle the \u2018curse of dimensionality\u2019 (Section 7) and attempts to produce a much faster variant.\n\u2022 Back-off n-grams are used to learn a realvalued vector representation of each word.\n\u2022 The word embeddings learned are shared across all the participating nodes in the distributed architecture.\n\u2022 A very important component of the whole model is the choice of the words binary encoding, i.e. of the hierarchical word cluster-\n2https://wordnet.princeton.edu/\ning. In this paper the authors combine empirical statistics with prior knowledge from the WordNet resource."}, {"heading": "9 A Hierarchical Neural Autoencoder for Paragraphs and Documents", "text": "Goal: Attempts to build a paragraph embedding from the underlying word and sentence embeddings, and then proceeds to encode the paragraph embedding in an attempt to reconstruct the original paragraph(Li et al., 2015).\nDescription: \u2022 The implementation uses an LSTM layer to\nconvert words into a vector representation of a sentence. A subsequent LSTM layer converts multiple sentences into a paragraph.\n\u2022 For this to happen, we need to preserve, syntactic, semantic and discourse related properties while creating the embedded representation.\n\u2022 Hierarchical LSTM utilized to preserve sentence structure.\n\u2022 Parameters are estimated by maximizing likelihood of outputs given inputs, similar to standard sequence-to-sequence models.\n\u2022 Estimates are calculated using softmax functions to maximize the likelihood of the constituent words.\n\u2022 Attention models using the hierarchical autoencoder could be utilized for dialog systems, since it explicitly models for discourse."}, {"heading": "10 Linguistic Regularities in Continuous Space Word Representations", "text": "Goal: In this paper, the authors examine the vector-space word representations that are implicitly learned by the input-layer weights. These representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words(Mikolov et al., 2013c). This is one of the seminal papers that led to the creation of Word2Vec, which is a state-of-the-art word embedding tool(Mikolov et al., 2013a).\nDescription: \u2022 A defining feature of neural network lan-\nguage models is their representation of words as high dimensional real-valued vectors.\n\u2022 In this model, words are converted via a learned lookup-table into real valued vectors which are used as the inputs to a neural network.\n\u2022 One of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models.\n\u2022 The word representations in this paper are learned by a recurrent neural network language model.\n\u2022 The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words. The hidden layer s(t) maintains a representation of the sentence history. The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary.\n\u2022 The values in the hidden and output layers are computed as follows:\ns(t) = f(Uw(t) +Ws(t\u2212 1))\ny(t) = g(V s(t))\nwhere f(z) = 11+e\u2212z and g(zm) = ezm\u2211 k ezk\n\u2022 One of the biggest features of having realvalued feature representations is the ability\nto compute the answer to an analogy question a : b; c : d where d is unknown. With continuous space word representations, this becomes as simple as calculating\ny = xb \u2212 xa + xc\ny is the best estimate of d that the model could compute. If there is no vector amongst the trained words such that y == xw, the nearest vector representation can be estimated using cosine similarity.\nw\u2217 = argmaxw xwy\n||xw||||y||"}, {"heading": "11 Better Word Representations with Recursive Neural Networks for Morphology", "text": "Goal: The paper aims to address the inaccuracy in vector representations of complex and rare words, supposedly caused by the lack of relation between morphologically related words(Luong et al., 2013).\nDescription: \u2022 The authors treat each morpheme as a basic\nunit in the RNNs and construct representations for morphologically complex words on the fly from their morphemes. By training a neural language model (NLM) and integrating RNN structures for complex words, they utilize contextual information to learn morphemic semantics and their compositional properties.\n\u2022 Discusses a problem that the Word2Vec syntactic relations like\nxapples \u2212 xapple \u2248 xcars \u2212 xcar\nmight not hold true if the vector representation of a rare word is inaccurate to begin with.\n\u2022 morphoRNN operates at the morpheme level rather than the word level. An example of the this is illustrated in Figure 2.\n\u2022 Parent words are created by combining a stem vector and an affix vector, as shown in Equation 1.\np = f(Wm[xstem;xaffix] + bm) (1)\n\u2022 The cost function is expression in terms of the squared Euclidean loss between the newly constructed representation pc(xi) and the reference representation pr(xi). The cost function is given in Equation 2.\nJ(\u03b8) = N\u2211 i=1 (||pr(xi)\u2212 pc(xi)||22) + \u03bb 2 ||\u03b8||22\n(2)\n\u2022 The paper describes both context sensitive and insensitive versions of the Morphological RNN.\n\u2022 Similar to a typical RNN, the network is trained by computing the activation functions and propagating the errors backward in a forward-backward pass architecture.\n\u2022 This RNN model performs better than most of the other neural language models, and could be used to supplement word vectors."}, {"heading": "12 Efficient Estimation of Word Representations in Vector Space", "text": "Goal: The main goal of this paper is to introduce techniques that can be used for learning highquality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary(Mikolov et al., 2013a).\nChallenge: The complexity that arises at the fully-connected output layer of the neural network is the dominant part of the computation. A couple of methods suggested to mitigate this is to use hierarchical versions of the softmax output activation units, or to refrain from per-\nforming normalization at the final layer altogether.\nDescription: \u2022 The ideas presented in this paper build on the\nprevious ideas presented by (Bengio et al., 2003).\n\u2022 The objective was to obtain high-quality word embeddings that capture the syntactic and semantic characteristics of words in a manner that allows algebraic operations to proxy the distances in vector space.\nman\u2212 woman = king \u2212 queen\nor tell \u2212 told = walk \u2212 walked\n\u2022 The training time here scales with the dimensionality of the learned feature vectors and not on the volume of training data.\n\u2022 The approach attempts to find a distributed vector representation of values as opposed to a continuous representation of values as computed by methods like LSA and LDA.\n\u2022 The models are trained using stochastic gradient descent and backpropagation.\n\u2022 The RNN models are touted to have an inherently better representation of sentence structure for complex patterns, without the need to specify context length.\n\u2022 To allow for the distributed training of the data, the framework DistBelief was used with multiple replicas of the model. Adagrad was utilized for asynchronous gradient descent.\n\u2022 Two distinct models were conceptualized for the training of the word vectors based on context, both of which are continuous and distributed representations of words. These are illustrated in Figure 3.\n\u2013 Continuous Bag-of-Words model: This model uses the context of a word i.e. the words that precede and follow it, to predict the current word.\n\u2013 Skip-gram model: This model uses the current word to predict the context it appeared in.\nThe experimental results show that the CBOW and skip-gram models consistently out-perform the then state-of-the-art models. It was also observed that after a point, increasing the dimensions\nand the size of the data began providing diminishing returns."}, {"heading": "13 Distributed Representations of Words and Phrases and their Compositionality", "text": "Goal: This paper builds upon the idea of the Word2Vec skip-gram model, and presents optimizations in terms of quality of the word embeddings as well as speed-ups while training. It also proposes an alternative to the hierarchical softmax final layer, called negative sampling(Mikolov et al., 2013b).\nDescription: \u2022 One of the optimizations suggested is to sub-\nsample the training set words to achieve a speed-up in model training.\n\u2022 Given a sequence of training words [w1, w2, w3, ..., wT ], the objective of the skip-gram model is to maximize the average log probability shown in Equation 3\n1\nT T\u2211 t=1 \u2211 \u2212c\u2264j\u2264c;j 6=0 logP (wt+j , wt) (3)\nwhere c is the window or context surrounding the current word being trained on.\n\u2022 As introduced by (Morin and Bengio, 2005), a computationally efficient approximation of the full softmax is the hierarchical softmax. The hierarchical softmax uses a binary tree representation of the output layer with the W words as its leaves and, for each node, explicitly represents the relative probabilities of its\nchild nodes. These define a random walk that assigns probabilities to words.\n\u2022 The authors use a binary Huffman tree, as it assigns short codes to the frequent words which results in fast training. It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models.\n\u2022 Noise Contrastive Estimation (NCE), which is an alternative to hierarchical softmax, posits that a good model should be able to differentiate data from noise by means of logistic regression.\n\u2022 To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word within the training set is discarded with probability computed by the below formula.\nP (wi) = 1\u2212 \u221a t\nf(wi)\nThis is similar to a dropout of neurons from the network, except that it is statistically more likely that frequent words are removed from the corpus by virtue of this method.\n\u2022 Discarding the frequently occurring words allows for a reduction in computational and memory cost.\n\u2022 The individual words can easily be coalesced into phrases using unigram and bigram frequency counts, as shown below.\nscore(wi, wj) = count(wiwj)\u2212 \u03b4\ncount(wi) \u2217 count(wj)\n\u2022 Another interesting property of learning these distributed representations is that the word and phrase representations learned by the skip-gram model exhibit a linear structure that makes it possible to perform precise analogical reasoning using simple vector arithmetic."}, {"heading": "14 Glove: Global Vectors for Word Representation", "text": "Goal: This paper proposes a global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local\ncontext window methods(Pennington et al., 2014).\nDescription: \u2022 While methods like LSA efficiently lever-\nage statistical information, they do relatively poorly on word analogy tasks, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on analogy tasks, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts.\n\u2022 The relationship between any arbitrary words can be examined by studying the ratio of their co-occurrence probabilities with various probe words.\n\u2022 The authors suggest that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves.\n\u2022 We can express this co-occurrence relation as shown below\nF ((wi \u2212 wj)Twk) = Pik Pjk\nThis makes the feature matrix interchangeable with its transpose.\n\u2022 An additive shift is included in the logarithm,\nlog(Xik)\u21d2 log(1 +Xik)\nwhich maintains the sparsity of X while avoiding the divergences while computing the co-occurrences matrix.\n\u2022 The model obtained in the paper could be compared to a global skip-gram model as opposed to a fixed window-size skipgram model as proposed by (Mikolov et al., 2013a).\n\u2022 The performance seems to increase monotonically with an increase in training data."}, {"heading": "15 Discussion", "text": "Following the literature survey, this section revisits the original research questions and provides a succinct summary that can be inferred from the experimental results and conclusions drawn from the original papers.\nRQ1 What are the relatively simple statistical techniques to extract features from text? Word count frequency models like n-gram and simple bag-of-words models such as TFIDF are still the easiest tools to obtain an numeric vector representation of text.\nRQ2 Is there any inherent benefit to using neural networks as opposed to the simple methods? The benefit of using neural nets primarily is their ability to identify obscure patterns, and remain flexible enough for a varied set of application areas from topic classification to syntax parse-tree generation.\nRQ3 What are the trade-offs that neural networks incur as opposed to the simple methods? The trade-offs are typically expressed in terms of computational cost and memory usage, although model complexity is a factor too, given that neural nets can be trained to learn arbitrarily complex generative models.\nRQ4 How do the different techniques compare to each other in terms of performance and accuracy? This question can only be answered subjectively as it varies from application to application. Typically, document similarity can be tackled with a simple statistical approach like TF-IDF. CNNs inherently model input data in a manner that iteratively reduces the dimensionality, making it a great fit for topic classification and document summarization. RNNs are great at modeling sequences of text, which make them apt for language syntax modeling. Amongst the frameworks, GloVe\u2019s pre-trained word-embeddings perform better than vanilla Word2Vec, which is considered state-of-the-art.\nRQ5 In what use-cases do the trade-offs outweigh the benefits of neural networks? As explained for the previous question, for a simple information retrieval use case such as document ranking, models such as TFIDF, and word PMI (pointwise mutual information) are sufficient, and neural networks would be overkill in such use-cases."}, {"heading": "16 Conclusion", "text": "This paper has summarized the important aspects of the state-of-the-art neural network techniques that have emerged in recent years. The field of machine translation, natural language understanding and natural language generation are important areas of research when it comes to developing a range of applications from a simple chatbot, to the conceptualization of a general AI entity.\nThe discussion section aggregates the results of the surveyed papers and offers a ready reference for new-comers to the field.\nFor future work, it is intended to experimentally compare different word-embedding approaches to act as a bootstrapping method to iteratively build high quality datasets for future machine learning model usage."}, {"heading": "17 Acknowledgments", "text": "The author would like to thank Dr. Pascal Poupart for his constructive feedback on the survey proposal."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of machine learning research 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Enriching word vectors with subword information", "author": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.04606 .", "citeRegEx": "Bojanowski et al\\.,? 2016", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12(Aug):2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg."], "venue": "Journal of Artificial Intelligence Research 57:345\u2013420.", "citeRegEx": "Goldberg.,? 2016", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Uw-finsent at semeval-2017 task 5: Fine-grained sentiment analysis on financial news headlines", "author": ["Vineet John", "Olga Vechtomova."], "venue": "Proceedings of the 11th international workshop on semantic evaluation .", "citeRegEx": "John and Vechtomova.,? 2017", "shortCiteRegEx": "John and Vechtomova.", "year": 2017}, {"title": "Bag of tricks for efficient text classification", "author": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1607.01759 .", "citeRegEx": "Joulin et al\\.,? 2016", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML. volume 14, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL. pages 104\u2013113.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Andrew L Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y Ng."], "venue": "HLT-NAACL. pages 345\u2013354.", "citeRegEx": "Maas et al\\.,? 2015", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Hlt-naacl. volume 13, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Aistats. Citeseer, volume 5, pages 246\u2013252.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng."], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11). pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["Karen Sparck Jones."], "venue": "Journal of documentation 28(1):11\u201321.", "citeRegEx": "Jones.,? 1972", "shortCiteRegEx": "Jones.", "year": 1972}], "referenceMentions": [{"referenceID": 3, "context": "The research questions listed in Section 2 will be tackled by surveying a few of the important overview papers on the topic(Goldberg, 2016)(Bengio et al.", "startOffset": 123, "endOffset": 139}, {"referenceID": 0, "context": "The research questions listed in Section 2 will be tackled by surveying a few of the important overview papers on the topic(Goldberg, 2016)(Bengio et al., 2003)(Morin and Bengio, 2005).", "startOffset": 139, "endOffset": 160}, {"referenceID": 12, "context": ", 2003)(Morin and Bengio, 2005).", "startOffset": 7, "endOffset": 31}, {"referenceID": 9, "context": "A few of the groundbreaking research papers in this area will also be studied, including word embeddings(Mikolov et al., 2013a)(Mikolov et al.", "startOffset": 104, "endOffset": 127}, {"referenceID": 10, "context": ", 2013a)(Mikolov et al., 2013b)(Mikolov et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 11, "context": ", 2013b)(Mikolov et al., 2013c).", "startOffset": 8, "endOffset": 31}, {"referenceID": 14, "context": "In addition to this, other less-obvious methods of features extraction will be surveyed, including tasks like part-of-speech tagging, chunking, named entity recognition, and semantic role labeling(Socher et al., 2011)(Luong et al.", "startOffset": 196, "endOffset": 217}, {"referenceID": 7, "context": ", 2011)(Luong et al., 2013)(Maas et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 8, "context": ", 2013)(Maas et al., 2015)(Li et al.", "startOffset": 7, "endOffset": 26}, {"referenceID": 2, "context": ", 2015)(Collobert et al., 2011)(Pennington et al.", "startOffset": 7, "endOffset": 31}, {"referenceID": 13, "context": ", 2011)(Pennington et al., 2014).", "startOffset": 7, "endOffset": 32}, {"referenceID": 4, "context": "This section talks about a few different statistical methods for computing this feature vector(John and Vechtomova, 2017).", "startOffset": 94, "endOffset": 121}, {"referenceID": 6, "context": "A Paragraph Vector model is comprised of an unsupervised learning algorithm that learns fixedsize vector representations for variable-length pieces of texts such as sentences and documents (Le and Mikolov, 2014).", "startOffset": 189, "endOffset": 211}, {"referenceID": 1, "context": "(Bojanowski et al., 2016) (Joulin et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": ", 2016) (Joulin et al., 2016).", "startOffset": 8, "endOffset": 29}, {"referenceID": 3, "context": "6 A Primer of Neural Net Models for NLP(Goldberg, 2016)", "startOffset": 39, "endOffset": 55}, {"referenceID": 0, "context": "Goal: Knowing the basic structure of a sentence, one should be able to create a new sentence by replacing parts of the old sentence with interchangeable entities(Bengio et al., 2003).", "startOffset": 161, "endOffset": 182}, {"referenceID": 12, "context": "The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet2 semantic hierarchy(Morin and Bengio, 2005).", "startOffset": 148, "endOffset": 172}, {"referenceID": 11, "context": "This allows vector-oriented reasoning based on the offsets between words(Mikolov et al., 2013c).", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "This is one of the seminal papers that led to the creation of Word2Vec, which is a state-of-the-art word embedding tool(Mikolov et al., 2013a).", "startOffset": 119, "endOffset": 142}, {"referenceID": 7, "context": "Goal: The paper aims to address the inaccuracy in vector representations of complex and rare words, supposedly caused by the lack of relation between morphologically related words(Luong et al., 2013).", "startOffset": 179, "endOffset": 199}, {"referenceID": 9, "context": "Goal: The main goal of this paper is to introduce techniques that can be used for learning highquality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary(Mikolov et al., 2013a).", "startOffset": 204, "endOffset": 227}, {"referenceID": 0, "context": "Description: \u2022 The ideas presented in this paper build on the previous ideas presented by (Bengio et al., 2003).", "startOffset": 90, "endOffset": 111}, {"referenceID": 10, "context": "It also proposes an alternative to the hierarchical softmax final layer, called negative sampling(Mikolov et al., 2013b).", "startOffset": 97, "endOffset": 120}, {"referenceID": 12, "context": "\u2022 As introduced by (Morin and Bengio, 2005), a computationally efficient approximation of the full softmax is the hierarchical softmax.", "startOffset": 19, "endOffset": 43}, {"referenceID": 13, "context": "context window methods(Pennington et al., 2014).", "startOffset": 22, "endOffset": 47}, {"referenceID": 9, "context": "\u2022 The model obtained in the paper could be compared to a global skip-gram model as opposed to a fixed window-size skipgram model as proposed by (Mikolov et al., 2013a).", "startOffset": 144, "endOffset": 167}], "year": 2017, "abstractText": "This paper aims to catalyze research discussions about text feature extraction techniques using neural network architectures. The research questions discussed here focus on the state-of-the-art neural network techniques that have proven to be useful tools for language processing, language generation, text classification and other computational linguistics tasks.", "creator": "LaTeX with hyperref package"}}}