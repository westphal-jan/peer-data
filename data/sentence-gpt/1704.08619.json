{"id": "1704.08619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "abstract": "Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states, such as social interactions, emotion and perception, including emotional states of the brain, and in some cases, neural networks.\n\n\n\n\nThe present work focused on the following topics. First, a new model of emotion recognition is required, and the computational model to predict the most emotion using the most sophisticated and relevant models and models. Second, the model model to predict the most emotional states will require a large amount of time and have the highest number of parameters and complexity. Third, the model to predict most emotion detection should rely on the human brain. The model to predict most emotion detection should be able to estimate the most emotional states over a period of time. Fourth, the model to predict most emotion detection should focus on the most emotion detection, such as empathy, emotion and perception. Finally, a single model to predict most emotion detection should be able to predict the most emotion detection, such as empathy, emotion and perception.", "histories": [["v1", "Thu, 27 Apr 2017 15:14:33 GMT  (1324kb)", "http://arxiv.org/abs/1704.08619v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["panagiotis tzirakis", "george trigeorgis", "mihalis a nicolaou", "bj\\\"orn schuller", "stefanos zafeiriou"], "accepted": false, "id": "1704.08619"}, "pdf": {"name": "1704.08619.pdf", "metadata": {"source": "CRF", "title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "authors": ["Panagiotis Tzirakis", "George Trigeorgis", "Mihalis A. Nicolaou", "Bj\u00f6rn Schuller", "Stefanos Zafeiriou"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n08 61\n9v 1\n[ cs\n.C V\n] 2\n7 A\npr 2\n01 7\nIndex Terms\u2014end-to-end learning, emotion recognition, deep learning\nI. INTRODUCTION\nE MOTION recognition is an essential component towards complete interaction between human and machine, as affective information is fundamental to human communication. Applications of emotion recognition can be found in different domains. For instance, emotion states can be used to monitor and predict fatigue state [1]. In speech recognition, emotion recognition can be used in call centres, where the goal is to detect the emotional state of the caller and provide feedback for the quality of the service [2].\nThe task of recognising emotions is challenging because human emotions lack of temporal boundaries and different individuals express emotions in different ways [3]. Although current work around emotion recognition was concentrated around infering the emotion of a subject out of its speech other modalities such as visual information (facial gestures) have also been used. With the advent of deep neural networks in the last decade a number of groundbreaking improvements have been observed in several established pattern recognition areas such as object, speech and speaker recognition, as well as in combined problem solving approaches, e.g. in audio-visual recognition, and in the rather recent field of paralinguistics.\nNumerous studies have shown the favourable property of these network variants to model inherent structure contained in the speech signal [4], with more recent research attempting end-to-end optimisation utilising as little human a-priori\nknowledge as possible [5]. Nevertheless, the majority of these works make use of commonly hand-engineered features have been used as input features, such as Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) coefficients, and supra-segmental features such as those used in the series of ComParE [6] and AVEC challenges [7], which build upon knowledge gained in decades of auditory research and have shown to be robust for many speech domains.\nRecently, however, a trend in the machine learning community has emerged towards deriving a representation of the input signal directly from raw, unprocessed data. The motivation behind this idea is that, ultimately, the network learns an intermediate representation of the raw input signal automatically that better suits the task at hand and hence leads to improved performance.\nIn this paper, we study automatic affect sensing using both speech and visual information in an end-to-end manner. Features are extracted from the speech signal using a CNN architecture designed for the audio channel and from the visual information using a ResNet-50 network architecture [8]. The output of these networks are fused together and fed to an LSTM to find the affective state of individuals. Contrary to the current practises, where each network is trained individually and the results are simply fed to a classifier our system is trained in an end-to-end manner. To our knowledge this is the first work in literature that applies such an end-to-end model for audio visual emotion recognition task. Furthermore, we suggest using explicit maximisation of the concordance correlation coefficient (\u03c1c) [9] in our model and show that this improves performance in terms of emotion prediction compared to optimising the mean square error objective, which is traditionally used. Finally, by further studying the activations of different cells in the recurrent layers, we find the existence of interpretable cells, which are highly correlated with several prosodic and acoustic features that were always assumed to convey affective information in speech, such as the loudness and the fundamental frequency. A preliminary version of this work was presented in [10], where only the raw speech waveform was used. We extend this work by considering also the visual modality in an end-to-end manner.\nTo show the benefit of our proposed multimodal model we evaluated it in the REmote COLlaborative and Affective (RECOLA) database. A part of this database was used for the Audio/Visual Emotion Challenge and Workshop (AVEC) 2016. Our model is trained using the whole database. Results show that the multimodal model benefits from the two modalities by producing equal results for arousal and valence as the speech and visual networks, respectively. We compare the\nunimodal and the multimodal models using results obtained in the AVEC 2016 challenge. Only the papers that used the audio, visual or audiovisual modalities are considered. In order to perform a fair comparison we apply the proposed method on the test set of the AVEC challenge. As shown by our experiments our unimodal models produce the best results for both the speech and visual modalities.\nThe remainder of the paper is organized as follows. Section II reports related studies on emotion recognition using multiple modalities with DNNs. Section III introduces the multimodal model architecture. Section IV describes the dataset used for the experiments. Section V presents the experiments performed and reports the results. Finally, section VI concludes this paper."}, {"heading": "II. RELATED WORK", "text": "The performance of pattern recognition models have been greatly improved with DNNs. Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.\nThese models have been used in various ways for multimodal recognition tasks such as in speech recognition. For instance, Ngiam et al. [15] proposed a Multimodal Deep Autoencoder (MDAE) network to extract features from audio and video modalities. First, a bimodal DBN was trained to initialize the deep autoencoder and then the MDAE was finetuned to minimize the reconstruction error of both modalities. In another study, Hu et al. [16] proposed a temporal multimodal network named Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRBM) to model audiovisual sequence of data. Another task that DNNs have also been used is gesture recognition. In [17] the authors use skeletal information and RGB-D images to recognize gestures. More particularly, they use DBNs to process skeleton features and a 3D CNN for the RGB-D data. Temporal information is considered by stacking a Hidden Markov Model (HMM) on top.\nThe emotion recognition domain has highly benefited with the advent of DNNs. Some works explore deep learning approaches for speech emotion recognition. For instance, Han et al. [18] uses hand-crafted features to feed a DNN that produces a probability distribution over categorical emotion states. From these probabilities they compute statistics from the whole utterance and finally, they perform classification by training an extreme learning machine. Lim et al [19] after transforming the data using short time fourier transform, they used CNNs to extract high-level features. In order to capture the temporal structure LSTMs were used. In a similar work, Trigeorgis et al.[10] proposed an end-to-end model that uses a CNN to extract features from the raw signal and then an LSTM network to capture the contextual information in the data.\nOther works try to solve the emotion recognition task by using facial information with DNNs. For example, Huang\net al. [20] proposed a transductive learning framework for image-based emotion recognition by combining DNNs and hypergraphs. More particularly, after the DNN was trained for emotion classification task, each node in the last fully connected layer was considered as an attribute and used to form a hyperedge in a hypergraph. In another study, Ebrahimi et al. [21] combined CNNs and RNNs to recognise categorical emotions in videos. A CNN was first trained to classify static images containing emotion. Then, the extracted features from the CNN were used to train an RNN to produce an emotion for the whole video.\nRecently, combining audio and visual modalities have great success for recognizing emotions. Some studies exploited the beneficial features DNNs can extract [22], [23], [24]. Kim et al. [23] proposed four different DBN architectures with one of them being a basic 2-layer DBN, and the others variation of it. The basic architecture first learns the features of the audio and video separately. After which it concatenates these features from the two modalities it uses them to learn the second layer. The features were evaluated using a Support Vector Machine (SVM). In another study, Kahou et al. [24] proposed to combine modality-specific DNNs to recognize categorical emotions in video. A CNN was used to analyze the video frames, a DBN to capture audio information, a deep autoencoder to model human actions depicted within the entire scene, and finally a CNN network to extract features from the mouth of the human. To output a final prediction they used two techniques that gave similar results. The first is to take the average of modality-specific predictions and in the second they learned an SVM with an RBF kernel using the concatenation features. In another study [25] compared handcrafted features extracted from faces using multi-scale Dense SIFT features (MSDF), and features extracted from CNNs to train linear Support Vector Regression (SVR). The extracted audio features were the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS). The combination of the features were used to learn a Support Vector Regression (SVR).\nZhang et al. [26] used a multimodal CNN for classifying emotions with audio and visual modalities. The model is trained in two phases. In the first phase the two CNNs are pretrained on large image datasets and fine-tuned to perform emotion recognition. The audio CNN takes as input the melspectogram segment of the audio signal and the video CNN takes the face. In the second phase a DNN was trained that comprised of a number of fully-connected layers. The concatenation of the features extracted by the two CNNs were inputted. In another study, Ringeval et al. [27] uses an BLSTM-RNN to capture the contextual information that exists in the multimodal features (audio, video, physiological) extracted from the data. In a more recent work, Han et al. [28] proposes the strength modeling framework which can be implemented as feature-level and decision-level fusion strategy and comprises of two regression models. The first model\u2019s predictions are concatenated with the original feature vector and fed to the second regression model for the final prediction.\nThe importance of recognizing emotions motivated the creation of the Audio/Visual Emotion Challenge and Workshop (AVEC) [29]. In 2016 challenge audio, video and physiological\nmodalities were considered. In one of the submitted models for the challenge, Huang et al. [30] proposed to use variants of Relevance Vector Machine (RVM) for modeling audio, video and audiovisual data. In another work model by Weber et al. [31] used high-level geometry features for predicting dimensional features. Brady et al. [32] also used low- and high-level features for modeling emotions. In a different study Povolny et al. [33] complemented original baseline features for both audio and video to perform emotion recognition. Somandepalli et al. [34] also used additional features but only for the audio modality.\nAll of the works in the literature make use of commonly hand-crafted features in audio or visual modality or in some cases in both of them. Moreover, they do not always consider temporal information in the data. In this study we propose a multimodal model trained end-to-end that also considers the contextual temporal information."}, {"heading": "III. PROPOSED METHOD", "text": "One of the first steps in a traditional machine learning algorithms is to extract features from the data. To extract features in audio, finite impulse response filters can be used which perform time-frequency decomposition to reduce the influence of background noise [35]. More complicated handengineered kernels, such as gammatone filters [36], which were formulated by studying the frequency responses of the receptive fields of auditory neurons of grassfrogs, can be used as well.\nA key component of our model is the convolution operation. For the audio and visual signals, 1-d and 2-d convolution is used, respectively.\n(f \u22c6 h)(i, j) = T\u2211\nk=\u2212T\nT\u2211\nm=\u2212T\nf(k,m) \u00b7 h(i\u2212 k, j \u2212m) (1)\nwhere f(x) is a kernel function whose parameters are learnt from the data of the task in hand. After the spatial-modeling of the signals, which removes background noise and enhances specific parts of the signals for the task in hand, we model the temporal structure of both speech and video by using a recurrent network with LSTM cells. We use LSTM for (i) simplicity, and (ii) to fairly compare against existing approaches which concentrated in the combination of handengineered features and LSTM networks. Finally, our model is subsequently trained with backpropagation using the objective function, cf. Equation 3.\nA. Visual Network\nNone of the first steps in the traditional face recognition pipeline is feature extraction utilizing hand-crafted representations such as Scale Invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG). Recently, deep convolutional networks have been used to extract features from faces [26].\nIn this study we use a deep residual network (ResNet) of 50 layers [8]. As input to the network we used the pixel intensities from the cropped faces of the subject\u2019s video. Deep residual\nnetworks adopt residual learning by stacking building blocks of the form:\nyk = F(xk, {Wk}) + h(xk) (2)\nwhere x and y are the input and output of the layer k, F(xk, {Wk}) is the residual function to be learned and h(xk) can be either an identity mapping or a linear projection to match the dimensions of function F and the input x. The first layer of ResNet-50 is a 7x7 convolutional layer with 64 feature maps, followed by a max pooling layer of size 3x3. The rest of the network comprises of 4 bottleneck architectures, where after these architectures a shorcut connection is added. These architectures contain 3 convolutional layers of sizes 1x1, 3x3, and 1x1, for each residual function. Table I shows the replication and the sizes of the feature maps for each bottleneck architecture. After the last bottleneck architecture an average pooling layer is inserted."}, {"heading": "B. Speech Network", "text": "In contrast to previous work done in the field of paralinguistics, where acoustic features are first extracted and then passed to a machine learning algorithm, we aim at learning the feature extraction and regression steps in one jointly trained model for predicting the emotion.\nInput. We segment the raw waveform to 6 s long sequences after we preprocess the time-sequences to have zero mean and unit variance to account for variations in different levels of loudness between the speakers. At 16 kHz sampling rate, this corresponds to a 96000-dimensional input vector.\nTemporal Convolution. We use F = 20 space time finite impulse filters with a 5ms window in order to extract finescale spectral information from the high sampling rate signal.\nPooling across time. The impulse response of each filter is passed through a half-wave rectifier (analogous to the cochlear transduction step in the human ear) and then downsampled to 8 kHz by pooling each impulse response with a pool size = 2.\nTemporal Convolution. We use M = 40 space time finite impulse filters of 500ms window. These are used to extract more long-term characteristics of the speech and the roughness of the speech signal.\nMax pooling across channels. We perform max-pooling across the channel domain with a pool size of 10. This reduces the dimensionality of the signal while preserving the necessary statistics of the convolved signal.\nDropout. Due to the large number of parameters (1.1 mil.) compared to the number of training examples we need to\nperform some regularisation in order for the model not to overfit on the training data. We opt to use dropout with a probability of 0.5."}, {"heading": "C. Objective function", "text": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7]. Nonetheless, previous work minimized the MSE during the training of the networks, but evaluated the models with respect to \u03c1c [37], [7]. Instead, we propose to include the metric used to evaluate the performance in the objective function (Lc) used to train the networks. Since the objective function is a cost function, we define Lc as follow:\nLc = 1\u2212 \u03c1c = 1\u2212 2\u03c32xy\n\u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y) 2\n= 1\u2212 2\u03c32xy\u03c8 \u22121 (3)\nwhere \u03c8 = \u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y) 2 and \u00b5x = E(x), \u00b5y = E(y), \u03c3 2 x = var(x), \u03c3 2 y = var(y) and \u03c3 2\nxy = cov(x,y) Thus, to minimise Lc (or maximise \u03c1c), we backpropagate the gradient of the last layer weights with respect to Lc,\n\u2202Lc \u2202x \u221d 2 \u03c32xy(x\u2212 \u00b5y) \u03c82 + \u00b5y \u2212 y \u03c8 , (4)\nwhere all vector operations are done element-wise."}, {"heading": "D. Network Training", "text": "Before training the multimodal network, each modalityspecific network is trained separately to speed up the training procedure.\nVisual Network. For the visual network, we chose to finetune the pretrained ResNet-50 on the database used in this work. This model was trained on the ImageNet 2012 [38] classification dataset that consists of 1000 classes. The pretrained model was preferred than training the network from scratch in order to be benefited by the features already learned by the model. To train the network a 2-layer LSTM, with 256 cells each is stack on top of it to capture temporal information.\nSpeech Network. The CNN network operates on raw signal to extract features from. In order to consider the temporal structure of speech, we use two LSTM layers with 256 cells each on top of the CNN.\nMultimodal Network. After training the visual and speech networks the LSTM layers are discarded and only the extracted features are considered. The speech network extracts 1280 features while the visual netowork 640 features. These are concatenated to form a 1920 dimensional feature vector and used to feed a 2-layer LSTM with 256 cells each. The LSTM layers are trained and the visual and speech networks are finetuned. Figure 1 shows the multimodal network.\nThe goal for each unimodal and the multimodal network is\nto minimize:\nLc = Lac + L v c\n2 (5)\nwhere Lac and L v c are the concordance of the arousal and\nvalence, respectively.\nFor the recurrent layers of speech, visual and multimodal networks, we segment the 6 s sequences to 150 smaller subsequences to match the granularity of the annotation frequency of 40ms."}, {"heading": "IV. DATASET", "text": "Time-continuous prediction of spontaneous and natural emotions (arousal and valence) is investigated on speech and visual data by using the REmote COLlaborative and Affective (RECOLA) database introduced by Ringeval et al. [39]; the full dataset for which participants gave their consent to share their data is used for the purpose of this study. Four modalities are included in the corpus: audio, video, electro-cardiogram (ECG) and electro-dermal activity (EDA). In total, 9.5h of multimodal recordings from 46 French-speaking participants were recorded and annotated for 5minutes each, performing a collaboration task in dyads during a video conference. Among the participants, 17 were French, three German and three Italian. The dataset is split in three partitions \u2013 train (16 subjects), validation (15 subjects) and test (15 subjects) \u2013 by stratifying (i.e., balancing) the gender and the age of the speakers. Finally, 6 French -speaking annotators (three male, three female) annotated all the recordings."}, {"heading": "V. EXPERIMENTS & RESULTS", "text": "For training the models we utilised the Adam optimisation method [40], and a fixed learning rate of 10\u22124 throughout all experiments. For the audio model we used a mini-batch of 25 samples. Also, for regularisation of the network, we used dropout [41] with p = 0.5 for all layers except the recurrent ones. This step is important as our models have a large amount of parameters (\u2248 1.5M ) and not regularising the network makes it prone on overfitting on the training data.\nFor the video model, the image size used was 96\u00d7 96 with mini-batch of size 2. Small mini-batch is selected because of hardware limitations. The data were augmented by resizing the image to size 110 \u00d7 110 and randomly cropping it to equal its original size. This produces a scale invariant model. In addition, color augmentation is used by introducing random brightness and saturation to the image.\nFinally, for all investigated methods, a chain of postprocessing is applied to the predictions obtained on the development set: (i) median filtering (with size of window ranging from 0.4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.04 s to 10 s), to compensate for delays in the ratings [43]. Any of these post-processing steps is kept when an improvement is observed on the \u03c1c of the validation set, and applied then with the same configuration on the test partition."}, {"heading": "A. Ablation study", "text": "Due to memory and training instability concerns [44] its not always optimal to use very large sequences in recurrent networks. The justification for this can be either the overblowing of gradients or the very deep unrolled graph which makes training of such big networks harder.\nIn order to choose the best sequence length to feed our LSTM layers, we conducted experiments using sequence lengths 75, 150, and 300 for both speech and visual models. Table II shows the results on the development set. For all the experiments a total of 60 epochs were run for our models.\nFor the visual network we expect to get the highest value in the valence dimension, while for the speech model in the arousal dimension. Results indicate that the best value for the speech model is 150 while for the visual model is 300. Due to the fact that the difference in performance for the visual network is small when sequence length of 150 or 300 is used, we chose to train the mutlimodal network with 150 sequence length."}, {"heading": "B. Speech Modality", "text": "Results obtained for each method, using all 46 participants, are shown in Table III. In all of the experiments, our model outperforms the designed features in terms of \u03c1c. One may note, however, that the eGEMAPS feature set provides close performance on valence, which is much more difficult to predict from speech compared to arousal. Furthermore, we show that by incorporating \u03c1c directly in the optimisation function of all networks allows us to optimise the models on the metric (\u03c1c) on which we evaluate the models. This\nprovides us with i) a more elegant way to optimise models, and ii) gives consistently better results across all test-runs as seen in Table III.\nIn addition, we compare the performance on the results obtained for methods that exist in the literature. Most of them have been submitted to the AVEC 2016 challenge, using 27 participants Table IV. In case performance on the test or validation set was not reported in the paper a dash is inserted. Results show that our model outperforms the other models in the test set when predicting arousal dimension. It is important to notice that although our model gets lower \u03c1c on the arousal dimension for the validation set compared to the baseline of the challenge, its performance is better on the test set.\n1) Relation to existing acoustic and prosodic features: The speech signals convey information about the affective state either explicitly, i.e., by linguistic means, or implicitly, i.e.,\nby acoustic or prosodic cues. It is well accepted amongst the research community that certain acoustic and prosodic features play an important role in recognising the affective state [45]. Some of these features, such as the mean of the fundamental frequency (F0), mean speech intensity, loudness, as well as pitch range [46], should thus be captured by our model.\nTo gain a better understanding of what our speech model learns, and how this relates to existing literature, we study the statistics of gate activations in the network applied on an unseen speech recording; a visualisation of the hidden-tooutput connections of different cells in the recurrent layers of the network is given in Figure 2. This plot shows that certain\ncells of the model are very sensitive to different features conveyed in the original speech wave form.\nC. Visual Modality\nVisual modality has been shown to more easily predict the valence dimension rather than the arousal. Table V presents the best results on the RECOLA dataset for the valence dimension. Only the work from Han et al. [28] was not submitted to the AVEC 2016 challenge. The features used for all of the models are appearance and geometric. For appearance Local Gabor Binary Patterns from Three Orthogonal Planes (LGBPTOP) features were extracted, whereas facial landmarks were extracted for the geometric features. The input to our network are the raw pixel intensities from the face extracted from the frames of the videos using the Multi-Domain Convolutional Neural Network Tracker (MDNet) [47] tracking algorithm. This algorithm takes the bounding box of the face in the first frame of the video and tracks it in all frames.\nAs expected visual modality benefits the models in the valence dimension. The only exception is the Video CNN-L4 model which performs better in the arousal dimension when appearance features are used. Our model outperforms all the other models in the valence dimension for the test set."}, {"heading": "D. Multimodal Analysis", "text": "Only two other models found in the literature to use both speech and visual modalities on the RECOLA database. These are the Output-Associative Relevance Vector Machine Staircase Regression (OA RVM-SR) [30] and the strength modeling system proposed by Han et al. [28]. Results are shown in Table VI. Our model outperforms the other two models in the valence dimension with high magnitude. For the arousal dimension the OA RVM-SR produces the best results. However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.g., 2D/3D facial landmarks etc.) which require the presence of an accurate facial landmark tracking methodology (ours was applied on the results of a conventional face detector only). We expect that our results would be improved further\nby applying similar strategies. Finally, to further demonstrate the benefits of our model for automatic prediction of arousal and valence Figure 3 illustrates results for single test subject from RECOLA."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we propose a multimodal system that operates on the raw signal, to perform an end-to-end spontaneous emotion prediction task from speech and visual data. To consider the contextual information in the data LSTM network was used. To speed up the training of the model we pretrained the speech and visual networks, separately. In addition, we study the gate activations of the recurrent layers in the speech modality and find cells that are highly correlated with prosodic features that were always assumed to cause arousal. Our experiments on the unimodal modality show that our models achieve significantly better performance in the test set in comparison to other models using the RECOLA database including those submitted to the AVEC2016 challenge, thus demonstrating the efficacy of learning features that better suit the task-at-hand. On the other hand, our multimodal model performs better in the valence dimension rather than the arousal compared to other models."}, {"heading": "ACKNOWLEDGMENT", "text": "The support of the EPSRC Centre for Doctoral Training in High Performance Embedded and Distributed Systems (HiPEDS, Grant Reference EP/L016796/1) is gratefully acknowledged."}], "references": [{"title": "Real-time nonintrusive monitoring and prediction of driver fatigue", "author": ["Q. Ji", "Z. Zhu", "P. Lan"], "venue": "IEEE Transactions on Vehicular Technology, vol. 53, no. 4, pp. 1052\u20131068, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting anger in automated voice portal dialogs", "author": ["F. Burkhardt", "J. Ajmera", "R. Englert", "J. Stegmann", "W. Burleson"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, Pittsburgh, United States, September 2006, pp. 1053\u20131056.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011", "author": ["C.-N. Anagnostopoulos", "T. Iliou", "I. Giannoukos"], "venue": "Artificial Intelligence Review, vol. 43, no. 2, pp. 155\u2013177, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "Y. Dong", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, November 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proceedings of International Conference on Machine Learning, Beijing, China, June 2014, pp. 1764\u20131772.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The INTER- SPEECH 2013 Computational Paralinguistics Challenge: Social signals, conflict, emotion, autism", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "A. Vinciarelli", "K. Scherer", "F. Ringeval", "M. Chetouani", "F. Weninger", "F. Eyben", "E. Marchi", "M. Mortillaro", "H. Salamin", "A. Polychroniou", "F. Valente", "S. Kim"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, Lyon, France, August 2013, pp. 148\u2013152.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "AV+EC 2015 \u2013 The First Affect Recognition Challenge Bridging Across Audio, Video, and Physiological Data", "author": ["F. Ringeval", "B. Schuller", "M. Valstar", "S. Jaiswal", "E. Marchi", "D. Lalanne", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, Brisbane, Australia, October 2015, pp. 3\u20138.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition, Las Vegas, United States, June-July 2016, pp. 770\u2013 778.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A concordance correlation coefficient to evaluate reproducibility", "author": ["L.I.-K. Lin"], "venue": "Biometrics, vol. 45, no. 1, pp. 255\u2013268, March 1989.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Shanghai, China, March 2016, pp. 5200\u20135204.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Autoencoders, minimum description length and helmholtz free energy", "author": ["R.S. Zemel"], "venue": "Proceedings of the Neural Information Processing Systems, 1994.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Generalization and network design strategies", "author": ["Y. LeCun"], "venue": "Connectionism in Perspective, pp. 143\u2013155, 1989.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1989}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the International Conference on Machine Learning, Washington, United States, June-July 2011, pp. 689\u2013696.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal multimodal learning in audiovisual speech recognition", "author": ["D. Hu", "X. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, United States, June-July 2016, pp. 3574\u20133582.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep dynamic neural networks for multimodal gesture segmentation and recognition", "author": ["D. Wu", "L. Pigou", "P.-J. Kindermans", "N.D.-H. Le", "L. Shao", "J. Dambre", "J.-M. Odobez"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 8, pp. 1583\u20131597, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech emotion recognition using deep neural network and extreme learning machine.", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Speech emotion recognition using convolutional and recurrent neural networks", "author": ["W. Lim", "D. Jang", "T. Lee"], "venue": "Proceedings of the Signal and Information Processing Association Annual Summit and Conference, Jeju, Korea, December 2016, pp. 1\u20134.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning driven hypergraph representation for image-based emotion recognition", "author": ["Y. Huang", "H. Lu"], "venue": "Proceedings of the International Conference on Multimodal Interaction, Tokyo, Japan, November 2016, pp. 243\u2013247.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural networks for emotion recognition in video", "author": ["S. Ebrahimi Kahou", "V. Michalski", "K. Konda", "R. Memisevic", "C. Pal"], "venue": "Proceedings of the International Conference on Multimodal Interaction, Seattle, United States, November 2015, pp. 467\u2013474.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Emotion recognition using multimodal deep learning", "author": ["W. Liu", "W.-L. Zheng", "B.-L. Lu"], "venue": "International Conference on Neural Information Processing, Kyoto, Japan, October 2016, pp. 521\u2013529.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning for robust feature generation in audiovisual emotion recognition", "author": ["Y. Kim", "H. Lee", "E.M. Provost"], "venue": "Proceedings of the International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, May 2013, pp. 3687\u20133691.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Emonets: Multimodal deep learning approaches for emotion recognition in video", "author": ["S.E. Kahou", "X. Bouthillier", "P. Lamblin", "C. Gulcehre", "V. Michalski", "K. Konda", "S. Jean", "P. Froumenty", "Y. Dauphin", "N. Boulanger- Lewandowski"], "venue": "Journal on Multimodal User Interfaces, vol. 10, no. 2, pp. 99\u2013111, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal deep convolutional neural network for audio-visual emotion recognition", "author": ["S. Zhang", "S. Zhang", "T. Huang", "W. Gao"], "venue": "Proceedings of the International Conference on Multimedia Retrieval, New York, United States, June 2016, pp. 281\u2013284.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data", "author": ["F. Ringeval", "F. Eyben", "E. Kroupi", "A. Yuce", "J.-P. Thiran", "T. Ebrahimi", "D. Lalanne", "B. Schuller"], "venue": "Pattern Recognition Letters, vol. 66, pp. 22\u201330, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals", "author": ["J. Han", "Z. Zhang", "N. Cummins", "F. Ringeval", "B. Schuller"], "venue": "Image and Vision Computing, pp. \u2013, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge", "author": ["M. Valstar", "J. Gratch", "B. Schuller", "F. Ringeval", "D. Lalanne", "M. Torres Torres", "S. Scherer", "G. Stratou", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 3\u201310.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Staircase regression in oa rvm, data selection and gender dependency in avec 2016", "author": ["Z. Huang", "B. Stasak", "T. Dang", "K. Wataraka Gamage", "P. Le", "V. Sethu", "J. Epps"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 19\u201326.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "High-level geometrybased features of video modality for emotion prediction", "author": ["R. Weber", "V. Barrielle", "C. Soladi\u00e9", "R. S\u00e9guier"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 51\u201358.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-modal audio, video and physiological sensor learning for continuous emotion prediction", "author": ["K. Brady", "Y. Gwon", "P. Khorrami", "E. Godoy", "W. Campbell", "C. Dagli", "T.S. Huang"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 97\u2013104.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal emotion recognition for avec 2016 challenge", "author": ["F. Povolny", "P. Matejka", "M. Hradis", "A. Popkov\u00e1", "L. Otrusina", "P. Smrz", "I. Wood", "C. Robin", "L. Lamel"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge, Amsterdam, The Netherlands, October 2016, pp. 75\u201382.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Online affect tracking with multimodal kalman filters", "author": ["K. Somandepalli", "R. Gupta", "M. Nasir", "B.M. Booth", "S. Lee", "S.S. Narayanan"], "venue": "Proceedings of the International Workshop on Audio/Visual Emotion Challenge, 2016, pp. 59\u201366.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved speech recognition using high-pass filtering of subband envelopes", "author": ["H. Hirsch", "P. Meyer", "H. Ruehl"], "venue": "Proceedings of the European Conference on Speech Technology, Genoa, Italy, September 1991, pp. 413\u2013416.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1991}, {"title": "Gammatone features and feature combination for large vocabulary speech recognition", "author": ["R. Schl\u00fcter", "L. Bezrukov", "H. Wagner", "H. Ney"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Honolulu, United States, April 2007, pp. 649\u2013 652.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data", "author": ["F. Ringeval", "F. Eyben", "E. Kroupi", "A. Yuce", "J.-P. Thiran", "T. Ebrahimi", "D. Lalanne", "B. Schuller"], "venue": "Pattern Recognition Letters, vol. 66, pp. 22\u201330, November 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions", "author": ["S.-A.S.J. Ringeval", "Fabien", "D. Lalanne"], "venue": "Proceedings of the IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, Shanghai, China, April 2013, pp. 1\u20138.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u2013 1958, January 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1929}, {"title": "Ensemble methods for continuous affect recognition: Multimodality, temporality, and challenges", "author": ["M. K\u00e4chele", "P. Thiam", "G. Palm", "F. Schwenker", "M. Schels"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, Brisbane, Australia, October 2015, pp. 9\u201316.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Correcting time-continuous emotional labels by modeling the reaction lag of evaluators", "author": ["S. Mariooryad", "C. Busso"], "venue": "IEEE Transactions on Affective Computing, vol. 6, no. 2, pp. 97\u2013108, April-June 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Vocal communication of emotion: A review of research paradigms", "author": ["K. Scherer"], "venue": "Speech Communication, vol. 40, no. 1-2, pp. 227\u2013256, April 2003.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2003}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  9 affective computing", "author": ["F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. Andr\u00e9", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan"], "venue": "IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For instance, emotion states can be used to monitor and predict fatigue state [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "In speech recognition, emotion recognition can be used in call centres, where the goal is to detect the emotional state of the caller and provide feedback for the quality of the service [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 2, "context": "The task of recognising emotions is challenging because human emotions lack of temporal boundaries and different individuals express emotions in different ways [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Numerous studies have shown the favourable property of these network variants to model inherent structure contained in the speech signal [4], with more recent research attempting end-to-end optimisation utilising as little human a-priori knowledge as possible [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "Numerous studies have shown the favourable property of these network variants to model inherent structure contained in the speech signal [4], with more recent research attempting end-to-end optimisation utilising as little human a-priori knowledge as possible [5].", "startOffset": 260, "endOffset": 263}, {"referenceID": 5, "context": "Nevertheless, the majority of these works make use of commonly hand-engineered features have been used as input features, such as Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) coefficients, and supra-segmental features such as those used in the series of ComParE [6] and AVEC challenges [7], which build upon knowledge gained in decades of auditory research and have shown to be robust for many speech domains.", "startOffset": 296, "endOffset": 299}, {"referenceID": 6, "context": "Nevertheless, the majority of these works make use of commonly hand-engineered features have been used as input features, such as Mel-Frequency Cepstral Coefficients (MFCC), Perceptual Linear Prediction (PLP) coefficients, and supra-segmental features such as those used in the series of ComParE [6] and AVEC challenges [7], which build upon knowledge gained in decades of auditory research and have shown to be robust for many speech domains.", "startOffset": 320, "endOffset": 323}, {"referenceID": 7, "context": "Features are extracted from the speech signal using a CNN architecture designed for the audio channel and from the visual information using a ResNet-50 network architecture [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "correlation coefficient (\u03c1c) [9] in our model and show that this improves performance in terms of emotion prediction compared to optimising the mean square error objective, which is traditionally used.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "A preliminary version of this work was presented in [10], where only the raw speech waveform was used.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Recently, a series of new neural network architectures have been revitalised, such as autoencoder networks [11], Convolutional Neural Networks (CNNs) [12], Deep Belief Networks (DBNs) [13] or memory enhanced neural network models such as Long Short-Term Memory (LSTM) [14] models.", "startOffset": 268, "endOffset": 272}, {"referenceID": 14, "context": "[15] proposed a Multimodal Deep Autoencoder (MDAE) network to extract features from audio and video modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] proposed a temporal multimodal network named Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRBM) to model audiovisual sequence of data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17] the authors use skeletal information and RGB-D images to recognize gestures.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "[18] uses hand-crafted features to feed a DNN that produces a probability distribution over categorical emotion states.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Lim et al [19] after transforming the data using short time fourier transform, they used CNNs to extract high-level features.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "[10] proposed an end-to-end model that uses a CNN to extract features from the raw signal and then an LSTM network to capture the contextual information in the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] proposed a transductive learning framework for image-based emotion recognition by combining DNNs and hypergraphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] combined CNNs and RNNs to recognise categorical emotions in videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "Some studies exploited the beneficial features DNNs can extract [22], [23], [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "[23] proposed four different DBN architectures with one of them being a basic 2-layer DBN, and the others variation of it.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] proposed to combine modality-specific DNNs to recognize categorical emotions in video.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] used a multimodal CNN for classifying emotions with audio and visual modalities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] uses an BLSTM-RNN to capture the contextual information that exists in the multimodal features (audio, video, physiological) extracted from the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] proposes the strength modeling framework which can be implemented as feature-level and decision-level fusion strategy and comprises of two regression models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The importance of recognizing emotions motivated the creation of the Audio/Visual Emotion Challenge and Workshop (AVEC) [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "[30] proposed to use variants of Relevance Vector Machine (RVM) for modeling audio, video and audiovisual data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] used high-level geometry features for predicting dimensional features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] also used low- and high-level features for modeling emotions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] complemented original baseline features for both audio and video to perform emotion recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] also used additional features but only for the audio modality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "To extract features in audio, finite impulse response filters can be used which perform time-frequency decomposition to reduce the influence of background noise [35].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "More complicated handengineered kernels, such as gammatone filters [36], which were formulated by studying the frequency responses of the receptive fields of auditory neurons of grassfrogs, can be used as well.", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "Recently, deep convolutional networks have been used to extract features from faces [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "In this study we use a deep residual network (ResNet) of 50 layers [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 35, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "To evaluate the agreement level between the predictions of the network and the gold-standard derived from the annotations, the concordance correlation coefficient (\u03c1c) [9] has recently been proposed [37], [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 35, "context": "Nonetheless, previous work minimized the MSE during the training of the networks, but evaluated the models with respect to \u03c1c [37], [7].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Nonetheless, previous work minimized the MSE during the training of the networks, but evaluated the models with respect to \u03c1c [37], [7].", "startOffset": 132, "endOffset": 135}, {"referenceID": 36, "context": "This model was trained on the ImageNet 2012 [38] classification dataset that consists of 1000 classes.", "startOffset": 44, "endOffset": 48}, {"referenceID": 37, "context": "[39]; the full dataset for which participants gave their consent to share their data is used for the purpose of this study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "For training the models we utilised the Adam optimisation method [40], and a fixed learning rate of 10 throughout all experiments.", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "Also, for regularisation of the network, we used dropout [41] with p = 0.", "startOffset": 57, "endOffset": 61}, {"referenceID": 6, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 40, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "4 s to 20 s) [7], (ii) centring (by computing the bias between gold-standard and prediction) [42], (iii) scaling (using the ratio of standard-deviation of gold-standard and prediction as scaling factor) [42] and (iv) time-shifting (by shifting the prediction forward in time with values ranging from 0.", "startOffset": 203, "endOffset": 207}, {"referenceID": 41, "context": "04 s to 10 s), to compensate for delays in the ratings [43].", "startOffset": 55, "endOffset": 59}, {"referenceID": 42, "context": "Due to memory and training instability concerns [44] its not always optimal to use very large sequences in recurrent networks.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "Baseline [29] eGeMAPS .", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "455) RVM [30] eGeMAPS - (.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "396) Audio BN-Multi [33] Mixed - (.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "503) Brady et al [32] MFCC - (.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "[31] eGeMAPS - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Mixed - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] 13 LLDs .", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "It is well accepted amongst the research community that certain acoustic and prosodic features play an important role in recognising the affective state [45].", "startOffset": 153, "endOffset": 157}, {"referenceID": 44, "context": "Some of these features, such as the mean of the fundamental frequency (F0), mean speech intensity, loudness, as well as pitch range [46], should thus be captured by our model.", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "[28] was not submitted to the AVEC 2016 challenge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Baseline [29] Geometric .", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "612) RVM [30] Geometric - (.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "571) Video CNN-L4 [33] Mixed - (.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "497) Brady et al [32] Appearance - (.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "[31] Geometric - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Geometric - (.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Geometric .", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "These are the Output-Associative Relevance Vector Machine Staircase Regression (OA RVM-SR) [30] and the strength modeling system proposed by Han et al.", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "However, we would like to note here that there are two main differences between our system and the system in [30] (a) the system in [30] was trained using both training and validation set, whereas our model was trained using only the training set and (b) our system operates directly on the raw pixel domain, while the system in [30] made use of a number of geometric features (e.", "startOffset": 329, "endOffset": 333}, {"referenceID": 28, "context": "OA RVMSR [30] eGeMAPS ComParE Geometric Appearance .", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "[28] 13 LLDs Geometric .", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to extract features from the speech, while for the visual modality a deep residual network (ResNet) of 50 layers. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, Long Short-TermMemory (LSTM) networks are utilized. The system is then trained in an end-to-end fashion where \u2013 by also taking advantage of the correlations of the each of the streams \u2013 we manage to significantly outperform the traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.", "creator": "LaTeX with hyperref package"}}}