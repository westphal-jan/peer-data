{"id": "1704.08795", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "abstract": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent for processing input and a model to model spatial relationships, including a recurrent network based on an inference-driven model of a recurrent task. Using a new paradigm, we use our model to perform model-based reinforcement learning in a context based on a recurrent task, such as a train model. When neural networks are applied to tasks with non-cognitive tasks, we identify the optimal training parameters that represent individual and contextual input. We use a combination of neural networks and reinforcement learning in a context based on training of a recurrent task. A model for associative and implicit information-based reinforcement learning (ASM) is used to map data from recurrent learning models to a particular task in the context of a recurrent task. We show that our models predict the exact neural network input to each task in a given context.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 28 Apr 2017 03:12:57 GMT  (2383kb,D)", "http://arxiv.org/abs/1704.08795v1", null], ["v2", "Sat, 22 Jul 2017 15:10:11 GMT  (4607kb,D)", "http://arxiv.org/abs/1704.08795v2", "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dipendra kumar misra", "john langford", "yoav artzi"], "accepted": true, "id": "1704.08795"}, "pdf": {"name": "1704.08795.pdf", "metadata": {"source": "CRF", "title": "Mapping Instructions and Visual Observations to Actions with Reinforcement Learning", "authors": ["Dipendra K. Misra", "John Langford", "Yoav Artzi"], "emails": ["yoav}@cs.cornell.edu", "jcl@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "An agent executing natural language instructions requires robust understanding of language and its environment. Existing approaches addressing this problem assume structured environment representations (e.g.,. Chen and Mooney, 2011; Mei et al., 2016), or combine separately trained models (e.g., Matuszek et al., 2010; Tellex et al., 2011), including for language understanding and visual reasoning. We propose to directly map text and raw image input to actions with a single learned model. Our approach offers multiple benefits, such as not requiring intermediate representations, planning procedures, or training multiple models.\nConsider an agent observing its surroundings with a camera sensor. For example, Figure 1 shows an RGB image observed by an agent in a block environment. Given the RGB input, the agent must identify the blocks and their layout. To\nunderstand the instruction, the agent must identify the block to move (Toyota block) and the destination (just right of the SRI block). Finally, the agent needs to generate actions, for example moving the Toyota block around obstructing blocks.\nTo address these challenges with a single model, we design a neural network agent. The agent executes instructions by generating a sequence of actions. At each step, the agent takes as input the instruction text, observes the world as an RGB image, and selects the next action. Action execution changes the state of the world. Given an observation of the new world state, the agent selects the next action. This process continues until the agent indicates execution completion.\nar X\niv :1\n70 4.\n08 79\n5v 1\n[ cs\n.C L\n] 2\n8 A\npr 2\n01 7\nWe train the agent with different levels of supervision, including complete demonstrations of the desired behavior and annotations of the goal state only. While the learning problem can be easily cast as a supervised learning problem, learning only from the states observed in the training data results in poor generalization and failure to recover from test errors. We use reinforcement learning (Sutton and Barto, 1998) to observe a broader set of states through exploration. Following recent work in robotics (e.g., Levine et al., 2016; Rusu et al., 2016), we assume the training environment, in contrast to the test environment, is instrumented and provides access to the state. This enables a simple problem reward function that uses the state and provides positive reward on task completion only. This type of reward offers two important advantages: (a) it is a simple way to express the ideal agent behavior we wish to achieve, and (b) it creates a platform to add training data information.\nWe use reward shaping (Ng et al., 1999) to exploit the training data to add to the reward additional information. The modularity of shaping allows varying the amount of supervision, for example by using complete demonstrations for only a fraction of the training examples. Shaping also naturally associates actions with immediate reward. This enables learning in a contextual bandit setting (Langford and Zhang, 2007), where optimizing the immediate reward is sufficient and has better sample complexity than unconstrained reinforcement learning (Agarwal et al., 2014).\nWe evaluate with the block world environment and data of Bisk et al. (2016), where each instruction moves one block (Figure 1). While the original task focused on source and target prediction only, we build an interactive simulator and formulate the task of predicting the complete sequence of actions. At each step, the agent must select between 81 actions with 15.4 steps required to complete a task on average, significantly more than existing environments (e.g., Chen and Mooney, 2011). Our experiments demonstrate that our reinforcement learning approach effectively reduces execution error by 24% over standard supervised learning and 34- 39% over common reinforcement learning techniques. We will release our open source simulator, and provide video execution samples at: https://youtu.be/S_PG9e2jcac."}, {"heading": "2 Technical Overview", "text": "Task Let X be the set of all instructions, S the set of all world states, and A the set of all actions. An instruction x\u0304 \u2208 X is a sequence \u3008x1, . . . , xn\u3009, where each xi is a token. The agent executes instructions by generating a sequence of actions, and indicates execution completion with the special action STOP. Action execution modifies the world state following a transition function T : S \u00d7 A \u2192 S. The execution e\u0304 of an instruction x\u0304 starting from s1 is an m-length sequence \u3008(s1, a1), . . . , (sm, am)\u3009, where sj \u2208 S , aj \u2208 A, T (si, ai) = si+1 and am = STOP. In the block world (Figure 1), a state specifies the positions of all blocks. For each action, the agent moves a single block on the plane in one of four directions (north, south, east, or west). There are 20 blocks, and 81 possible actions at each step, including STOP. For example, the action TOYOTA-SOUTH moves the Toyota block one step south. Blocks can not move over or through other blocks. Model The agent observes the world state via a visual sensor (i.e., a camera). Given a world state s, the agent observes an RGB image I generated by the function IMG(s). We distinguish between the world state s and the agent state s\u0303, which includes the image observation IMG(s), images of previous states, and the previous action. To map instructions to actions, the agent reasons about the agent state s\u0303 and instruction x\u0304 to generate a sequence of actions. At each step, the agent generates a single action. We model the agent with a neural network policy. At each step j, the network takes as input the current agent state s\u0303j , and predicts the next action to execute aj . We formally define the agent state and model in Section 4. Learning We assume access to training data with N examples {(x\u0304(i), s(i)1 , e\u0304(i))}Ni=1, where x\u0304(i) is an instruction, s(i)1 is a start state, and e\u0304 (i) is an execution demonstration of x\u0304(i) starting at s(i)1 . We use policy gradient (Section 5) with reward shaping (Section 6) derived from the training data to increase learning speed and exploration effectiveness. Following work in robotics (e.g., Levine et al., 2016), we assume an instrumented environment with access to the world state to compute the reward during training only. We define our approach in general terms with demonstrations, but also experiment with training using goal states. Evaluation We evaluate task completion error on a test set {(x\u0304(i), s(i)1 , s (i) g )}Mi=1, where x\u0304(i) is an\ninstruction, s(i)1 is a start state, and s (i) g is the goal state. We measure execution error as the distance between the final execution state and s(i)g ."}, {"heading": "3 Related Work", "text": "Learning to follow instructions was studied extensively with structured environment representations, including with semantic parsing (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a,b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al., 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al., 2016). In contrast, we study the problem of an agent that takes as input instructions and raw visual input. Instruction following with visual input was studied with pipeline approaches that use separately learned models for visual reasoning (Matuszek et al., 2010, 2012; Tellex et al., 2011; Paul et al., 2016). Rather than decomposing the problem, we adopt a single-model approach and learn from instructions paired with demonstrations or goal states. Our work is related to Sung et al. (2015). While they use sensory input to select and adjust a trajectory observed during training, we are not restricted to training sequences. Executing instructions in non-learning settings has also received significant attention (e.g., Winograd, 1972; Webber et al., 1995; MacMahon et al., 2006).\nOur work is related to a growing interest in problems that combine language and vision, including visual question answering (e.g., Antol et al., 2015; Andreas et al., 2016b,a), caption generation (e.g., Chen et al., 2015, 2016; Xu et al., 2015), and visual reasoning (Johnson et al., 2016; Suhr et al., 2017). We address the prediction of the next action given a world image and an instruction.\nReinforcement learning with neural networks has been used for various NLP tasks, including text-based games (Narasimhan et al., 2015; He et al., 2016), information extraction (Narasimhan et al., 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al., 2016).\nNeural network reinforcement learning techniques have been recently studied for behavior learning tasks, including playing games (Mnih et al., 2013, 2015, 2016; Silver et al., 2016) and solving memory puzzles (Oh et al., 2016). In contrast to this line of work, our data is limited. Observing new states in a computer game simply requires playing it. In contrast, our agent also con-\nsiders natural language instructions. As the set of instructions is limited to the training data, the set of states the agent can observe is constrained. We address the data efficiency problem by learning in a contextual bandit setting, which is known to be more tractable (Agarwal et al., 2014), and using reward shaping to increase exploration effectiveness. Zhu et al. (2017) address generalization of reinforcement learning to new target goals in visual search by providing the agent an image of the goal state. We address a related problem. However, we provide natural language and the agent must learn to recognize the goal state.\nReinforcement learning is extensively used in robotics (Kober et al., 2013). Similar to recent work on learning neural network policies for robot control (Levine et al., 2016; Schulman et al., 2015; Rusu et al., 2016), we assume an instrumented training environment and use the state to compute rewards during learning. Our approach adds the ability to specify tasks using natural language."}, {"heading": "4 Model", "text": "We model the agent policy \u03c0 with a neural network. The agent observes the instruction and an RGB image of the world. Given a world state s, the image I is generated using the function IMG(s). The instruction execution is generated one step at a time. At each step j, the agent observes an image Ij of the current world state sj and the instruction x\u0304, predicts the action aj , and executes it to transition to the next state sj+1. This process continues until STOP is predicted and the agent stops, indicating instruction completion. The agent also has access to K images of previous states and the previous action to distinguish between different stages of the execution (Mnih et al., 2015). Figure 2 illustrates our architecture.\nFormally,1 at step j, the agent considers an agent state s\u0303j , which is a tuple (x\u0304, Ij , Ij\u22121, . . . , Ij\u2212K , aj\u22121), where x\u0304 is the natural language instruction, Ij is an image of the current world state, the images Ij\u22121, . . . , Ij\u2212K represent K previous states, and aj\u22121 is the previous action. The agent state includes information about the current world state and the execution. Considering the previous action aj\u22121 allows the agent to avoid repeating failed actions, for example when\n1We use bold-face capital letters for matrices and boldface lowercase letters for vectors. Computed input and state representations use bold versions of the symbols. For example, x\u0304 is the computed representation of an instruction x\u0304.\ntrying to move in the direction of an obstacle. In Figure 2, the agent is given the instruction Place the Toyota east of SRI, is at the 10-th step of the execution, and considers K = 2 previous images.\nWe generate continuous vector representations for all inputs. We use a recurrent neural network (RNN; Elman, 1990) with a long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to map the instruction x\u0304 = \u3008x1, . . . , xn\u3009 to a vector representation x\u0304. Each token xi is mapped to a fixed dimensional vector with the learned embedding function \u03c8(xi). The instruction representation x\u0304 is computed by applying the LSTM recurrence to generate a sequence of hidden states li = LSTM(\u03c8(xi), li\u22121), and computing the mean x\u0304 = 1n \u2211n i=1 li (Narasimhan et al., 2015). The current image Ij and previous images Ij\u22121,. . . ,Ij\u2212K are concatenated along the channel dimension and embedded with a convolutional neural network (CNN) to generate the visual state v (Mnih et al., 2013). The last action aj\u22121 is embedded with the embedding function \u03c8a(aj\u22121). The vectors vj , x\u0304, and \u03c8a(aj\u22121) are concatenated to create the agent state vector representation s\u0303j = [vj , x\u0304, \u03c8a(aj\u22121)].\nTo compute the action to execute, we use a feedforward perceptron that decomposes according to the domain actions. In the block world domain, where actions decompose to selecting the block to move and the direction, the network computes block and direction probabilities. Formally, we decompose an action a to direction aD and block aB . We compute the feedforward network:\nh1 = max(W(1)s\u0303j + b (1), 0)\nhD = W(D)h1 + b(D) hB = W(B)h1 + b(B) ,\nand the action probability is a product of the component probabilities:\nP (aDj = d | x\u0304, s1, aj\u22121) \u221d exp(hDd ) P (aBj = b | x\u0304, s1, aj\u22121) \u221d exp(hBb ) .\nAt the beginning of execution, the first action a0 is set to the special value NONE, and previous images are zero matrices. The embedding function \u03c8 is a learned matrix. The function \u03c8a concatenates the embeddings of aDj\u22121 and a B j\u22121, which are obtained from learned matrices, to compute the embedding of aj\u22121. The model parameters \u03b8 include W(1), b(1), W(D), b(D), W(B), b(B), the parameters of the LSTM recurrence, the parameters of the convolutional network CNN, and the embedding matrices. In our experiments (Section 7), all parameters are learned without external resources."}, {"heading": "5 Learning", "text": "We use policy gradient for reinforcement learning (Williams, 1992) to estimate the parameters \u03b8 of the agent policy. We assume access to a training set of N examples {(x\u0304(i), s(i)1 , e\u0304(i))}Ni=1, where x\u0304(i) is an instruction, s(i)1 is a start state, and e\u0304(i) is an execution demonstration starting from s (i) 1 of instruction x\u0304\n(i). The main learning challenge is learning how to execute instructions given raw visual input from relatively limited data. We learn in a contextual bandit setting, which provides theoretical advantages over general reinforcement learning. In Section 8, we verify this empirically. Reward Function The instruction execution problem defines a simple problem reward to measure task completion. The agent receives a positive reward when the task is completed, a negative reward for incorrect completion (i.e., STOP in the wrong state) and actions that fail to execute (e.g., when the direction is blocked), and a small penalty otherwise. This reward is indifferent to the exact execution details, but prefers shorter trajectories. To compute the reward, we assume access to the world state. This learning setup is inspired by work in robotics (Section 3), where it is achieved by instrumenting the training environment. The agent, on the other hand, only observes the agent state (Section 4). When deployed, the system re-\nlies on visual observations only. The reward functionR(i) : S \u00d7A \u2192 R is defined for each training example (x\u0304(i), s(i)1 , e\u0304 (i)), i = 1 . . . N :\nR(i)(s, a) =  1.0 if s = sm(i) and a = STOP \u22121.0 s 6= sm(i) and a = STOP \u22121.0 a fails to execute \u2212\u03b4 else ,\nwhere m(i) is the length of e\u0304(i). The reward function does not provide intermediate positive feedback to the agent for actions that bring it closer to its goal. As a result, when the agent explores randomly early during learning, it is unlikely to encounter the goal state and receive positive reward due to the large number of steps required to execute tasks. In Section 6, we describe how reward shaping, a method to augment the reward with additional information, is used to take advantage of the training data and address this challenge.\nPolicy Gradient Objective The shaped reward allows for a contextual bandit setting, where immediate reward optimization is sufficient to optimize policy parameters \u03b8. The objective is defined to uses the immediate reward:\nJ = 1 N N\u2211 i=1 E[R(i)(s, a)] ,\nwhere the expectation is taken over the state-action pairs visited by the agent given the policy \u03c0 starting at s(i)1 following instruction x\u0304\n(i). While the objective is designed to work best with the shaped reward, the reward may be the original problem reward defined above or one of several rewards defined via reward shaping (Section 6). The algorithm remains the same in either case. The objective is an adaptation of the policy gradient objective defined by Sutton et al. (1999) to multiple starting states, one for each example. In contrast to most policy gradient approaches, we apply the objective to a contextual bandit setting where immediate reward is optimized rather than total expected reward. The primary theoretical advantage of a contextual bandit setting is much tighter sample complexity bounds when comparing upper bounds for contextual bandits (Langford and Zhang, 2007) to lower bounds (Krishnamurthy et al., 2016) or upper bounds (Kearns et al., 1999) for total reward maximization. Empirically, we observe poor results when optimizing the total reward (REINFORCE baseline in Section 8) in accordance with the theory. To derive the gradi-\nent, we use the likelihood ratio method:\n\u2207\u03b8J = 1\nN N\u2211 i=1 E[\u2207\u03b8 log \u03c0(s\u0303, a)R(i)(s, a)] ,\nwhere reward is computed from the world state but policy is learned on the agent state. We approximate the gradient using sampling. Entropy Penalty We observe that early in training, the agent is overwhelmed with negative reward and rarely completes the task. This results in the policy \u03c0 rapidly converging towards a suboptimal deterministic policy with an entropy of 0. To delay premature convergence we add an entropy term to the objective (Williams and Peng, 1991; Mnih et al., 2016). The entropy term encourages a uniform distribution policy, and in practice encourages exploration early during training. The regularized objective is:\nJ = 1 N N\u2211 i=1 E[R(i)(s, a) + \u03bbH(\u03c0(s\u0303, \u00b7))] ,\nwhere H(\u03c0(s\u0303, \u00b7)) is the entropy of \u03c0 given the agent state s\u0303, \u03bb is a hyperparameter that controls the strength of the regularization. The gradient is:\n\u2207\u03b8J =\n1 N N\u2211 i=1 E[\u2207\u03b8 log \u03c0(s\u0303, a)R(i)(s, a) + \u03bb\u2207\u03b8H(\u03c0(s\u0303, \u00b7))] .\nWhile the entropy term delays premature convergence, it does not eliminate it completely. Similar issues with vanilla policy gradient were recently reported for other tasks (Mnih et al., 2016). Algorithm Algorithm 1 shows our learning algorithm. We iterate over the data T times, for each training example (x\u0304(i), s(i)1 , e\u0304\n(i)), i = 1 . . . N in an epoch, we perform a rollout using our policy to generate an execution (lines 7 - 16). The length of the rollout is bound by J , but may be shorter if the agent selected the STOP action. At each step j, the agent updates the agent state s\u0303j (lines 9 - 10), samples an action from the policy \u03c0 (line 12), and executes it to generate the new world state sj+1 (line 13). The gradient is approximated using the sampled action with the computed reward R(i)(sj , aj) (line 15). Following each rollout, we update the parameters \u03b8 with the mean of the gradients using ADAM (Kingma and Ba, 2014)."}, {"heading": "6 Reward Shaping", "text": "Reward shaping is a method for transforming a reward function by adding a shaping term to the problem reward. We use shaping to add additional\nAlgorithm 1 Policy gradient learning\nInput: Training set {(x\u0304(i), s(i)1 , e\u0304(i))}Ni=1, learning rate \u00b5, epochs T , horizon J , and entropy regularization term \u03bb. Definitions: IMG(s) is a camera sensor that reports an RGB image of state s. \u03c0 is a probabilistic neural network policy parameterized by \u03b8, as described in Section 4. EXECUTE(s, a) executes the action a at the state s, and returns the new state. R(i) is the reward function for example i. ADAM(\u2206) applies a per-feature learning rate to the gradient \u2206 (Kingma and Ba, 2014). Output: Policy parameters \u03b8. 1: \u00bb Iterate over the training data. 2: for t = 1 to T , i = 1 to N do 3: I1\u2212K , . . . , I0 = ~0 4: a0 = NONE, s1 = s (i) 1\n5: j = 1 6: \u00bb Rollout up to episode limit. 7: while j \u2264 J and aj 6= STOP do 8: \u00bb Observe world and construct agent state. 9: Ij = IMG(sj)\n10: s\u0303j = (x\u0304(i), Ij , Ij\u22121, . . . , Ij\u2212K , adj\u22121) 11: \u00bb Sample an action from the policy. 12: aj \u223c \u03c0(s\u0303j , a) 13: sj+1 = EXECUTE(sj , aj) 14: \u00bb Compute the approximate gradient. 15: \u2206j \u2190 \u2207\u03b8 log \u03c0(s\u0303j , aj)R(i)(sj , aj) +\u03bb\u2207\u03b8H(\u03c0(s\u0303j , \u00b7)) 16: j+ = 1 17: \u03b8 \u2190 \u03b8 + \u00b5ADAM( 1\nj \u2211j j\u2032=1 \u2206j\u2032)\n18: return \u03b8\ninformation to the problem reward function (Section 5). Adding an arbitrary shaping term can change the optimality of policies and modify the original problem, for example by making bad policies according to the problem reward optimal according to the shaped function.2 Ng et al. (1999) and Wiewiora et al. (2003) outline potential-based terms that realize sufficient conditions for safe shaping.3 Adding a shaping term is safe if the order of policies according to the shaped reward is identical the order according to the original problem reward. While safe shaping only applies to optimizing the total reward, we show empirically the effectiveness of the safe shaping terms we design in a contextual bandit setting. We introduce two shaping terms. The final shaped reward is an additive combination of them and the problem reward. Similar to the problem reward, we define example-specific shaping terms to consider. We modify the reward function signature as required. Distance-based Shaping (F1) The first shaping term measures if the agent moved closer to the\n2For example, adding a shaping term F = \u2212R will result in a shaped reward that is always 0, and any policy will be trivially optimal with respect to it.\n3For convenience, we briefly overview the theorems of Ng et al. (1999) and Wiewiora et al. (2003) in Appendix A.\ngoal state. We design it to be a safe potential-based term (Ng et al., 1999):\nF (i) 1 (sj , aj , sj+1) = \u03c6 (i) 1 (sj+1)\u2212 \u03c6 (i) 1 (sj) .\nThe potential \u03c6(i)1 (s) is proportional to the negative distance from the goal state s(i)g . Formally, \u03c6 (i) 1 (s) = \u2212\u03b7\u2016s\u2212 s (i) g \u2016, where \u03b7 is a constant scaling factor, and \u2016.\u2016 is a distance metric. In the block world, the distance between two states is the sum of the Euclidean distances between the positions of each block in the two states, and \u03b7 is the inverse of block width. The middle column in Figure 3 visualizes the potential \u03c6(i)1 . Trajectory-based Shaping (F2) Distancebased shaping may lead the agent to sub-optimal states, for example when an obstacle blocks the direct path to the goal state, and the agent must temporarily increase its distance from the goal to bypass it. We incorporate complete trajectories by using a simplification of the shaping term introduced by Brys et al. (2015). Unlike F1, it requires access to the previous state and action. It is based on the look-back advice shaping term of Wiewiora et al. (2003), who introduced safe potential-based shaping that considers the previous state and action. The second term is: F\n(i) 2 (sj\u22121, aj\u22121, sj , aj) = \u03c6 (i) 2 (sj , aj)\u2212\u03c6 (i) 2 (sj\u22121, aj\u22121) .\nGiven e\u0304(i) = \u3008(s1, a1), . . . , (sm, am)\u3009, to compute the potential \u03c6(i)2 (s, a), we identify the closest state sj in e\u0304(i) to s. If \u03b7\u2016sj \u2212 s\u2016 < 1 and aj = a, \u03c6 (i) 2 (s, a) = 1.0, else \u03c6 (i) 2 (s, a) = \u2212\u03b4f , where \u03b4f is a penalty parameter. We use the same distance computation and parameter \u03b7 as in F1. When the agent is in a state close to a demonstration state,\nthis term encourages taking the action taken in the related demonstration state. The right column in Figure 3 visualizes the effect of the potential \u03c6(i)2 ."}, {"heading": "7 Experimental Setup", "text": "Environment We use the environment of Bisk et al. (2016). The original task required predicting the source and target positions for a single block given an instruction. In contrast, we address the task of moving blocks on the plane to execute instructions given visual input. This requires generating the complete sequence of actions needed to complete the instruction. The environment contains up to 20 blocks marked with logos or digits. Each block can be moved in four directions. Including the STOP action, in each step, the agent selects between 81 actions. The set of actions is constant and is not limited to the blocks present. The transition function is deterministic. The size of each block step is 0.04 of the board size. The agent observes the board from above. Data Bisk et al. (2016) collected a corpus of instructions paired with start and goal states. Figure 1 shows example instructions. To create demonstrations, we compute the shortest paths. While this process may introduce noise for instructions that specify specific trajectories (e.g., move SRI two steps north and then . . . ) rather than only describing the goal state, analysis of the data shows this issue is limited. Out of 100 sampled instructions, 92 describe the goal state rather than the trajectory. A secondary source of noise is due to discretization of the state space. As a result, the agent often can not reach the exact target position. The demonstrations error illustrates this problem (Table 3). To provide task completion reward during learning, we relax the state comparison, and consider states to be equal if the sum of block distances is under the size of one block. While the original data includes instructions for moving one block or multiple blocks, our demonstration generation procedure is unable to disambiguate action ordering when multiple blocks are moved. Therefore, we focus on instructions where a single block changes its position between the start and goal states, and restrict demonstration generation to move the changed block. However, the remaining data, and the complexity it introduces, provide an important direction for future work. The corpus includes 11,871/1,719/3,177 instructions for training/development/testing. Table 1 shows corpus statistic compared to the commonly\nused SAIL navigation corpus (MacMahon et al., 2006; Chen and Mooney, 2011). While the SAIL agent only observes its immediate surroundings, overall the blocks domain provides more complex instructions. Furthermore, the SAIL environment includes only 400 states, which is insufficient for generalization with vision input. We compare to other data sets in Appendix D. Evaluation We evaluate task completion error as the sum of Euclidean distances for each block between its position at the end of the execution and in the gold goal state. We divide distances by block size to normalize for the image size. In contrast, Bisk et al. (2016) evaluate the selection of the source and target positions independently. Systems We report ablations, the upper bound of following the demonstrations (Demonstrations), and five baselines: (a) STOP: the agent immediately stops, (b) RANDOM: the agent takes random actions, (c) SUPERVISED: supervised learning with maximum-likelihood estimate using stateaction pairs from the demonstrations, (d) DQN: deep Q-learning with both shaping terms (Mnih et al., 2015), and (e) REINFORCE: policy gradient with cumulative episodic reward with both shaping terms (Sutton et al., 1999). Full baseline details are given in Appendix B. Parameters and Initialization Full details are in Appendix C. We consider K = 4 previous images, and horizon length J = 40. We initialize our model with the SUPERVISED model."}, {"heading": "8 Results", "text": "Table 2 shows development results. We run each experiment three times and report the best result. The RANDOM and STOP baselines illustrate the complexity of the task. Our approach, including both shaping terms in a contextual bandit setting, significantly outperforms the other methods. SUPERVISED learning demonstrates lower performance. A likely explanation is test-time execution errors leading to unfamiliar states with poor later performance (Kakade and Langford, 2002), a form of the covariate shift problem. The low performance of REINFORCE and DQN illustrates the challenge of general reinforcement learning with\nlimited data due to relatively high sample complexity (Kearns et al., 1999; Krishnamurthy et al., 2016). We also report results using ensembles of the three models.\nWe ablate different parts of our approach. Ablations of supervised initialization (our approach w/o sup. init) or the previous action (our approach w/o prev. action) result in increase in error. While the contribution of initialization is modest, it provides faster learning (Li et al., 2016). On average, after two epochs, we observe an error of 3.94 with initialization and 6.01 without. We hypothesize that the F2 shaping term, which uses full demonstrations, helps to narrow the gap at the end of learning. Without supervised initialization and F2, the error increases to 5.45 (the 0% point in Figure 4). We observe the contribution of each of the shaping terms and their combination. To study the benefit of potential-based shaping, we experiment with a negative distance-to-goal reward. This alternative reward replaces our reward and encourages the agent to get closer to the goal (our approach w/distance reward). With this reward, learning fails to converge, leading to a rela-\ntively high error. To understand performance better, we also measure minimal distance (min. distance), the closest the agent got to the goal. We observe a strong trend: the agent often gets close to the goal and fails to stop. This illustrates the challenge of learning how to be behave at an absorbing state, which is observed relatively rarely during training. This is also illustrated in our video: https://youtu.be/S_PG9e2jcac.\nFigure 4 shows our approach with varying amount of supervision. We remove demonstrations from both supervised initialization and the F2 shaping term. For example, when only 25% are available, only 25% of the data is available for initialization and the F2 term is only present for this part of the data. While some demonstrations are necessary for effective learning, we get most of the benefit with only 12.5%.\nTable 3 provides test results, using the ensembles to decrease the risk of overfitting the development. We observe similar trends to development result with our approach outperforming all baselines. The remaining gap to the demonstrations upper bound illustrates the need for future work."}, {"heading": "9 Conclusions", "text": "We study the problem of learning to execute instructions in a situated environment given only raw visual observations. Supervised approaches do not explore adequately to handle test time errors, and reinforcement learning approaches require a large number of samples for good convergence. Our solution provides an effective combination of both approaches: reward shaping to create relatively stable optimization in a contextual bandit setting, which takes advantage of a signal similar to supervised learning, with a reinforcement basis that admits substantial exploration and easy avenues for smart initialization. This combination is designed for a few-samples regime, as we address. When the number of samples is unbounded, the drawbacks observed in this scenario for optimizing longer term reward do not hold."}, {"heading": "Acknowledgments", "text": "This research was supported by a Google Faculty Award and an Amazon Web Services Research Grant. We thank the Cornell NLP group and the Microsoft Research Machine Learning NYC group for their support and insightful comments."}, {"heading": "A Reward Shaping Theorems", "text": "In Section 6, we introduce two reward shaping terms. We follow the safe-shaping theorems of Ng et al. (1999) and Wiewiora et al. (2003). The theorems outline potential-based terms that realize sufficient conditions for safe shaping. Applying safe terms guarantees the order of policies according to the original problem reward does not change. While the theory only applies when optimizing the total reward, we show empirically the effectiveness of the safe shaping terms in a contextual bandit setting. For convenience, we provide the definitions of potential-based shaping terms and the theorems introduced by Ng et al. (1999) and Wiewiora et al. (2003) using our notation. We refer the reader to the original papers for the full details and proofs.\nThe distance-based shaping term F1 is defined based on the safeshaping theorem of Ng et al. (1999):\nDefinition. A shaping term F : S \u00d7 A \u00d7 S \u2192 R is potential-based if there exists a function \u03c6 : S \u2192 R such that, at time j, F (sj , aj , sj+1) = \u03b3\u03c6(sj+1)\u2212\u03c6(sj), \u2200sj , sj+1 \u2208 S and aj \u2208 A, where \u03b3 \u2208 [0, 1] is a future reward discounting factor. The function \u03c6 is the potential function of the shaping term F .\nTheorem. Given a reward function R(sj , aj), if the shaping term is potential-based, the shaped reward RF (sj , aj , sj+1) = R(sj , aj)+F (sj , aj , sj+1) does not modify the total order of policies.\nIn the definition of F1, we set the discounting term \u03b3 to 1.0 and omit it.\nThe trajectory-based shaping term F2 follows the shaping term introduced by Brys et al. (2015). To define it, we use the lookback advice shaping term of Wiewiora et al. (2003), who extended the potential-based term of Ng et al. (1999) for safe shaping terms that consider the previous state and action:\nDefinition. A shaping term F : S \u00d7 A \u00d7 S \u00d7 A \u2192 R is potential-based if there exists a function \u03c6 : S \u00d7 A \u2192 R such that, at time j, F (sj\u22121, aj\u22121, sj , aj) = \u03b3\u03c6(sj , aj) \u2212 \u03c6(sj\u22121, aj\u22121), \u2200sj , sj\u22121 \u2208 S and aj , aj\u22121 \u2208 A, where \u03b3 \u2208 [0, 1] is a future reward discounting factor. The function \u03c6 is the potential function of the shaping term F .\nTheorem. Given a reward function R(sj , aj), if the shaping term is potential-based, the shaped reward RF (sj\u22121, aj\u22121, sj , aj) = R(sj , aj) + F (sj\u22121, aj\u22121, sj , aj) does not modify the total order of policies.\nIn the definition of F2 as well, we set the discounting term \u03b3 to 1.0 and omit it."}, {"heading": "B Baseline Systems", "text": "We implement multiple systems to evaluate our approach. The demonstrations system provides an upper-bound, while the rest are baselines and existing methods. STOP The agent performs the STOP action immediately at the beginning of execution. RANDOM The agent samples actions uniformly until STOP is sampled or J actions were sampled, where J is the execution horizon. SUPERVISED Given the training data with N instruction-state-execution triplets, we generate training data of state-action pairs and optimize the log-likelihood of the data. Formally, we optimize the objective:\nJ = 1\nN N\u2211 i=1 m(i)\u2211 j=1 log \u03c0(s\u0303 (i) j , a (i) j ) ,\nwhere m(i) is the length of the execution e\u0304(i), s\u0303(i)j is the observed state at step j in sample i, and a(i)j is the demonstration action of step j in demonstration execution e\u0304(i). Agent states are generated with the annotated previous actions (i.e., to generate previous images and the previous action). We use minibatch gradient descent with ADAM updates (Kingma and Ba, 2014). DQN We use deep Q-learning (Mnih et al., 2015) to train a Q-network. We use the architecture described in Section 4, except replacing the task specific part with a single 81-dimension layer. In contrast to our probabilistic model, we do not decompose block and direction selection. We use the shaped reward function, including both F1 and F2. We use a replay memory of size 2,000 and an -greedy behavior policy to generate rollouts. We attenuate the value of from 1 to 0.1 in 100,000 steps and use prioritized sweeping for sampling. We also use a target network that is synchronized after every epoch. REINFORCE We use the REINFORCE algorithm (Sutton et al., 1999) to train our agent. REINFORCE performs policy gradient learning with total reward accumulated over the roll-out as opposed to using immediate rewards in our main approach. REINFORCE samples the total reward using monte-carlo sampling by performing a rollout. We use the shaped reward function when using REINFORCE, including both F1 and F2 shaping terms. Similar to our approach, we initialize with a SUPERVISED model and regularize the objective with the entropy of the policy."}, {"heading": "C Parameters and Initialization", "text": "C.1 Architecture Parameters\nWe use an RGB image of 120x120 pixels, and a convolutional neural network (CNN) with 4 layers. The first two layers apply 32 8 \u00d7 8 filters with a stride of 4, the third applies 32 4 \u00d7 4 filters with a stride of 2. The last layer performs an affine transformation to create a 200-dimension vector. We linearly scale all images to have zero mean and unit norm. We use a single layer RNN with 150-dimensional word embeddings and 250 LSTM units. The dimension of the action embedding \u03c8a is 56, including 32 for embedding the block and 24 for embedding the directions. W(1) is a 506\u00d7 120 matrix and b(1) is a 120-dimension vector. W(D) is 120\u00d720 for 20 blocks, and W(B) is 120\u00d75 for the four directions (north, south, east, west) and the STOP action. We consider K = 4 previous images, and use horizon length J = 40.\nC.2 Initialization\nEmbedding matrices are initialized with a zeromean unit-variance Gaussian distribution. All biases are initialized to 0. We use a zero-mean truncated normal distribution to initialize the CNN filters (0.005 variance) and CNN weights matrices (0.004 variance). All other weight matrices are initialized with a normal distribution (mean=0.0, standard deviation=0.01). The matrices used in the word embedding function \u03c8 are initialized with a zero-mean normal distribution with standard deviation of 1.0. Action embeddings matrices, which are used \u03c8a, are initialized with a zeromean normal distribution with 0.001 standard deviation. We initialize policy gradient learning, including our approach, with parameters estimated using supervised learning for two epochs, except the direction parameters W(D) and b(D), which we learn from scratch. We found this initialization method to provide a good balance between strong initialization and not biasing the learning too much, which can result in limiting exploration.\nC.3 Learning Parameters\nWe use the distance error on a small validation set as stopping criteria. After each epoch, we save the model, and select the final model based on development set performance. While this method overfits the development set, we found it more reliable then using the small validation set alone. Our relatively modest performance degradation on\nthe held-out set illustrates that our models generalize well. We set the reward and shaping penalties \u03b4 = \u03b4f = 0.02. The entropy regularization coefficient is \u03bb = 0.1. The learning rate is \u00b5 = 0.001 for supervised learning and \u00b5 = 0.00025 for policy gradient. We clip the gradient at a norm of 5.0. All learning algorithms use a mini-batch of size 32 during training."}, {"heading": "D Dataset Comparisons", "text": "We briefly review instruction following datasets in Table 4, including: Blocks (Bisk et al., 2016), SAIL (MacMahon et al., 2006; Chen and Mooney, 2011), Matuszek (Matuszek et al., 2012), and Misra (Misra et al., 2015). Overall, Blocks provides the largest training set and a relatively complex environment with well over 2.4318 possible states.4 The most similar dataset is SAIL, which provides only partial observability of the environment (i.e., the agent observes what is around it only). However, SAIL is less complex on other dimensions related to the instructions, trajectories, and action space. In addition, while Blocks has a large number of possible states, SAIL includes only 400 states. The limited number of different states in SAIL makes it difficult to learn vision models that generalize well. Misra (Misra et al., 2015) provides a parameterized action space (e.g., grasp(cup)), which leads to a large number of potential actions. However, it has a relatively small corpus."}, {"heading": "E Common Questions", "text": "This is a list of potential questions following various decisions that we made. While we ablated and discussed all the crucial decisions in the paper, we decided to include this appendix to provide as much information as possible. Is it possible to manually engineer a competitive reward function without shaping? Shaping is a principled approach to add information to a problem reward with relatively intuitive potential functions. Our experiments demonstrate its effectiveness empirically. Investing engineering effort in designing an alternative reward function specifically designed to the task is a potential alternative approach.\n4We compute this loose lower bound on the number of states in the block world as 20! = 2.4318 (giving positions, the number of block permutations). This is a very loose lower bound.\nAre you using beam search? Why not? While using beam search can probably increase our performance, we chose to avoid it. We are motivated by robotic scenarios, where implementing beam search is a challenging task and often not possible. We distinguish between beam search and backtracking. Beam search is also incompatible with common assumptions of reinforcement learning, although it is often used during test with reinforcement learning systems. Why are you using the mean of the LSTM hidden states instead of just the final state? We empirically tested both options. Using the mean worked better. This was also observed by Narasimhan et al. (2015). Understanding in which scenarios one technique is better than the other is an important question for future work. Can you provide more details about initialization? Please see Appendix C. Does the agent in the block world learn to move obstacles and other blocks? While the agent can move any block at any step, in practice, it rarely happens. The agent prefers to move blocks around obstacles rather than moving other blocks and moving them back into place afterwards. This behavior is learned form the data and shows even when we use only very limited amount of demonstrations. We hypothesize that in other tasks the agent is likely to learn that moving obstacles is advantageous, for example when demonstrations include moving obstacles. Does the agent explicitly mark where it is in the instruction? We estimate that over 90% of the instructions describe the target position. Therefore, it is often not clear how much of the instruction was completed during the execution. The agent does not have an explicit mechanism to mark portions of the instruction that are complete. We briefly experimented with attention, but found that empirically it does not help in our domain. Designing an architecture to allows such considerations is an important direction for future work. Does the agent know which blocks are present? Not all blocks are included in each task. The agent\nmust infer which blocks are present from the image and instruction. The set of possible actions, which includes moving all possible blocks, does not change between tasks. If the agent chooses to move a block that is not present, the state remains the same. Did you experiment with executing sequences of instruction? The Bisk et al. (2016) includes such instructions, right? The majority of existing corpora, including SAIL (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Mei et al., 2016) and the maps domain of Vogel and Jurafsky (2010), provide segmented sequences of instructions. Existing approaches take advantage of this segmentation during training. For example, Chen and Mooney (2011), Artzi and Zettlemoyer (2013), and Mei et al. (2016) all train on segmented data and test on sequences of instructions by doing inference on one sentence at a time. We are also able to do this. Similar to these approaches, we will likely suffer from cascading errors. The multi-instruction paragraphs in the Bisk et al. (2016) data are an open problem and present new challenges beyond simply longer instructions. For example, they often merge multiple block placements in one instruction (e.g, put the SRI, HP, and Dell blocks in a row). Since the original corpus does not provide trajectories and our automatic generation procedure is not able to resolve which block to move first, we do not have demonstrations for this data. The instructions also present a significantly more complex task. This is an important direction for future work, which illustrates the complexity and potential of the domain. Potential-based shaping was proven to be safe when maximizing the total expected reward. Does this apply for the contextual bandit setting, where you maximize the immediate reward? The safe shaping theorems (Appendix A) do not hold in our contextual bandit setting. We show empirically that shaping works in practice. However, how and if it changes the order of policies is an open question.\nHow long does it take to train? How many frames the agent observes? The agent observes about 2.5 million frames. It takes about 16 hours using 50% capacity of Nvidia Pascal Titan X GPU to train our contextual bandit agent. DQN takes more than twice the time for the same number of epochs while supervised learning agent takes about 9 hours to converge. We tried to train DQN for around four days, but did not observe improvement. Did you consider initializing DQN with supervised learning? Initializing DQN with the probabilistic supervised model is challenging. Since DQN is not probabilistic it is not clear what this initialization means. Smart initialization of DQN is an important problem for future work."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Agarwal et al\\.,? 2014", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Alignmentbased compositional semantics for instruction following", "author": ["Jacob Andreas", "Dan Klein."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/D15-1138.", "citeRegEx": "Andreas and Klein.,? 2015", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Andreas et al\\.,? 2016a", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "Andreas et al\\.,? 2016b", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Journal of Computer Vision.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Learning compact lexicons for CCG semantic parsing", "author": ["Yoav Artzi", "Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.3115/v1/D14-1134.", "citeRegEx": "Artzi et al\\.,? 2014a", "shortCiteRegEx": "Artzi et al\\.", "year": 2014}, {"title": "Programming by demonstration with situated semantic parsing", "author": ["Yoav Artzi", "Maxwell Forbes", "Kenton Lee", "Maya Cakmak."], "venue": "AAAI Fall Symposium Series.", "citeRegEx": "Artzi et al\\.,? 2014b", "shortCiteRegEx": "Artzi et al\\.", "year": 2014}, {"title": "Weakly supervised learning of semantic parsers for mapping instructions to actions", "author": ["Yoav Artzi", "Luke Zettlemoyer."], "venue": "Transactions of the Association of Computational Linguistics 1:49\u201362. http://aclweb.org/anthology/Q13-1005.", "citeRegEx": "Artzi and Zettlemoyer.,? 2013", "shortCiteRegEx": "Artzi and Zettlemoyer.", "year": 2013}, {"title": "Natural language communication with robots", "author": ["Yonatan Bisk", "Deniz Yuret", "Daniel Marcu."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Bisk et al\\.,? 2016", "shortCiteRegEx": "Bisk et al\\.", "year": 2016}, {"title": "Reinforcement learning for mapping instructions to actions", "author": ["S.R.K. Branavan", "Harr Chen", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Branavan et al\\.,? 2009", "shortCiteRegEx": "Branavan et al\\.", "year": 2009}, {"title": "Reading between the lines: Learning to map high-level instructions to commands", "author": ["S.R.K. Branavan", "Luke Zettlemoyer", "Regina Barzilay."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Branavan et al\\.,? 2010", "shortCiteRegEx": "Branavan et al\\.", "year": 2010}, {"title": "Reinforcement learning from demonstration through shaping", "author": ["Tim Brys", "Anna Harutyunyan", "Halit Bener Suay", "Sonia Chernova", "Matthew E. Taylor", "Ann Now\u00e9."], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence.", "citeRegEx": "Brys et al\\.,? 2015", "shortCiteRegEx": "Brys et al\\.", "year": 2015}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["David L. Chen", "Raymond J. Mooney."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "Chen and Mooney.,? 2011", "shortCiteRegEx": "Chen and Mooney.", "year": 2011}, {"title": "Bootstrap, review, decode: Using out-ofdomain textual data to improve image captioning", "author": ["Wenhu Chen", "Aur\u00e9lien Lucchi", "Thomas Hofmann."], "venue": "CoRR abs/1611.05321.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "CoRR abs/1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "D. Christopher Manning."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. http://aclweb.org/anthology/D16-1245.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science 14:179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Deep reinforcement learning with a natural language action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Compu-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross B. Girshick."], "venue": "CoRR abs/1612.06890.", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Sham Kakade", "John Langford."], "venue": "Machine Learning, Proceedings of the Nineteenth", "citeRegEx": "Kakade and Langford.,? 2002", "shortCiteRegEx": "Kakade and Langford.", "year": 2002}, {"title": "A sparse sampling algorithm for near-optimal planning in large markov decision processes", "author": ["Michael Kearns", "Yishay Mansour", "Andrew Y. Ng."], "venue": "Proeceediings of the International Joint Conference on Artificial Intelligence.", "citeRegEx": "Kearns et al\\.,? 1999", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Unsupervised PCFG induction for grounded language learning with highly ambiguous supervision", "author": ["Joohyun Kim", "Raymond Mooney."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kim and Mooney.,? 2012", "shortCiteRegEx": "Kim and Mooney.", "year": 2012}, {"title": "Adapting discriminative reranking to grounded language learning", "author": ["Joohyun Kim", "Raymond Mooney."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Kim and Mooney.,? 2013", "shortCiteRegEx": "Kim and Mooney.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Reinforcement learning in robotics: A survey", "author": ["Jens Kober", "J. Andrew Bagnell", "Jan Peters."], "venue": "International Journal of Robotics Research 32:1238\u2013 1274.", "citeRegEx": "Kober et al\\.,? 2013", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "PAC reinforcement learning with rich observations", "author": ["Akshay Krishnamurthy", "Alekh Agarwal", "John Langford."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Krishnamurthy et al\\.,? 2016", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2016}, {"title": "The epochgreedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang."], "venue": "Advances in Neural Information Processing Systems 20, Proceedings of the TwentyFirst Annual Conference on Neural Information", "citeRegEx": "Langford and Zhang.,? 2007", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel."], "venue": "Journal of Machine Learning Research 17.", "citeRegEx": "Levine et al\\.,? 2016", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Walk the talk: Connecting language, knowledge, action in route instructions", "author": ["Matthew MacMahon", "Brian Stankiewics", "Benjamin Kuipers."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "MacMahon et al\\.,? 2006", "shortCiteRegEx": "MacMahon et al\\.", "year": 2006}, {"title": "Following directions using statistical machine translation", "author": ["C. Matuszek", "D. Fox", "K. Koscher."], "venue": "Proceedings of the international conference on Human-robot interaction.", "citeRegEx": "Matuszek et al\\.,? 2010", "shortCiteRegEx": "Matuszek et al\\.", "year": 2010}, {"title": "Learning to parse natural language commands to a robot control system", "author": ["C. Matuszek", "E. Herbst", "Luke S. Zettlemoyer", "D. Fox."], "venue": "Proceedings of the International Symposium on Experimental Robotics.", "citeRegEx": "Matuszek et al\\.,? 2012", "shortCiteRegEx": "Matuszek et al\\.", "year": 2012}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "R. Matthew Walter."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Tell me dave: Contextsensitive grounding of natural language to manipulation instructions", "author": ["Dipendra K. Misra", "Jaeyong Sung", "Kevin Lee", "Ashutosh Saxena."], "venue": "The International Journal of Robotics Research 35(1-3):281\u2013300.", "citeRegEx": "Misra et al\\.,? 2016", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Environment-driven lexicon induction for high-level instructions", "author": ["Kumar Dipendra Misra", "Kejia Tao", "Percy Liang", "Ashutosh Saxena."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Misra et al\\.,? 2015", "shortCiteRegEx": "Misra et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu."], "venue": "Proceedings of the International", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski."], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? 2015", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Language understanding for textbased games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y. Ng", "Daishi Harada", "Stuart J. Russell."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Ng et al\\.,? 1999", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Oh et al\\.,? 2016", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators", "author": ["Rohan Paul", "Jacob Arkin", "Nicholas Roy", "Thomas M. Howard."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Paul et al\\.,? 2016", "shortCiteRegEx": "Paul et al\\.", "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A. Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell."], "venue": "CoRR .", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529 7587:484\u2013", "citeRegEx": "Sutskever et al\\.,? 2016", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "A corpus of compositional language for visual reasoning", "author": ["Alane Suhr", "Mike Lewis", "James Yeh", "Yoav Artzi."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.", "citeRegEx": "Suhr et al\\.,? 2017", "shortCiteRegEx": "Suhr et al\\.", "year": 2017}, {"title": "Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["Jaeyong Sung", "Seok Hyun Jin", "Ashutosh Saxena."], "venue": "International Symposium on Robotics Research.", "citeRegEx": "Sung et al\\.,? 2015", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto."], "venue": "IEEE Trans. Neural Networks 9:1054\u20131054.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Understanding natural language commands for robotic navigation and mobile manipulation", "author": ["S. Tellex", "T. Kollar", "S. Dickerson", "M. Walter", "A.G. Banerjee", "S. Teller", "N. Roy."], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "Tellex et al\\.,? 2011", "shortCiteRegEx": "Tellex et al\\.", "year": 2011}, {"title": "Learning to follow navigational directions", "author": ["Adam Vogel", "Daniel Jurafsky."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P10-1083.", "citeRegEx": "Vogel and Jurafsky.,? 2010", "shortCiteRegEx": "Vogel and Jurafsky.", "year": 2010}, {"title": "Instructions, intentions and expectations", "author": ["B. Webber", "N. Badler", "B. Di Eugenio", "C. Geib", "L. Levison", "M. Moore."], "venue": "Artificial Intelligence 73(1):253\u2013 269.", "citeRegEx": "Webber et al\\.,? 1995", "shortCiteRegEx": "Webber et al\\.", "year": 1995}, {"title": "Principled methods for advising reinforcement learning agents", "author": ["Eric Wiewiora", "Garrison W. Cottrell", "Charles Elkan."], "venue": "Proceedings of the International Conference on Machine Learning.", "citeRegEx": "Wiewiora et al\\.,? 2003", "shortCiteRegEx": "Wiewiora et al\\.", "year": 2003}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine Learning 8.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng."], "venue": "Connection Science 3(3):241\u2013268.", "citeRegEx": "Williams and Peng.,? 1991", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Understanding natural language", "author": ["Terry Winograd."], "venue": "Cognitive Psychology 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Jamie Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "Proceedings of the Inter-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Yuke Zhu", "Roozbeh Mottaghi", "Eric Kolve", "Joseph J. Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2017}, {"title": "Understanding in which scenarios one technique is better than the other is an important question for future", "author": ["Narasimhan"], "venue": null, "citeRegEx": "Narasimhan,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan", "year": 2015}, {"title": "2016) and the maps domain of Vogel and Jurafsky (2010), provide segmented sequences of instructions. Existing approaches take advantage of this segmentation during training", "author": ["Artzi", "Zettlemoyer", "Mei"], "venue": null, "citeRegEx": "Artzi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2010}, {"title": "2016) all train on segmented data and test on sequences of instructions by doing inference on one sentence at a time", "author": ["ple", "Chen", "Mooney", "Artzi", "Zettlemoyer", "Mei"], "venue": null, "citeRegEx": "ple et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ple et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Existing approaches addressing this problem assume structured environment representations (e.g.,. Chen and Mooney, 2011; Mei et al., 2016), or combine separately trained models (e.", "startOffset": 90, "endOffset": 138}, {"referenceID": 49, "context": "We use reinforcement learning (Sutton and Barto, 1998) to observe a broader set of states through exploration.", "startOffset": 30, "endOffset": 54}, {"referenceID": 44, "context": "Following recent work in robotics (e.g., Levine et al., 2016; Rusu et al., 2016), we assume the training environment, in contrast to the test environment, is instrumented and provides access to the state.", "startOffset": 34, "endOffset": 80}, {"referenceID": 41, "context": "We use reward shaping (Ng et al., 1999) to exploit the training data to add to the reward additional information.", "startOffset": 22, "endOffset": 39}, {"referenceID": 27, "context": "setting (Langford and Zhang, 2007), where optimizing the immediate reward is sufficient and has better sample complexity than unconstrained reinforcement learning (Agarwal et al.", "startOffset": 8, "endOffset": 34}, {"referenceID": 0, "context": "setting (Langford and Zhang, 2007), where optimizing the immediate reward is sufficient and has better sample complexity than unconstrained reinforcement learning (Agarwal et al., 2014).", "startOffset": 163, "endOffset": 185}, {"referenceID": 8, "context": "We evaluate with the block world environment and data of Bisk et al. (2016), where each instruction moves one block (Figure 1).", "startOffset": 57, "endOffset": 76}, {"referenceID": 12, "context": "Learning to follow instructions was studied extensively with structured environment representations, including with semantic parsing (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a,b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 133, "endOffset": 262}, {"referenceID": 7, "context": "Learning to follow instructions was studied extensively with structured environment representations, including with semantic parsing (Chen and Mooney, 2011; Kim and Mooney, 2012, 2013; Artzi and Zettlemoyer, 2013; Artzi et al., 2014a,b; Misra et al., 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 133, "endOffset": 262}, {"referenceID": 1, "context": ", 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 52, "context": ", 2015, 2016), alignment models (Andreas and Klein, 2015), reinforcement learning (Branavan et al., 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al.", "startOffset": 82, "endOffset": 137}, {"referenceID": 33, "context": ", 2009, 2010; Vogel and Jurafsky, 2010), and neural network models (Mei et al., 2016).", "startOffset": 67, "endOffset": 85}, {"referenceID": 58, "context": ", 2016b,a), caption generation (e.g., Chen et al., 2015, 2016; Xu et al., 2015), and visual reasoning (Johnson et al.", "startOffset": 31, "endOffset": 79}, {"referenceID": 19, "context": ", 2015), and visual reasoning (Johnson et al., 2016; Suhr et al., 2017).", "startOffset": 30, "endOffset": 71}, {"referenceID": 47, "context": ", 2015), and visual reasoning (Johnson et al., 2016; Suhr et al., 2017).", "startOffset": 30, "endOffset": 71}, {"referenceID": 39, "context": "Reinforcement learning with neural networks has been used for various NLP tasks, including text-based games (Narasimhan et al., 2015; He et al., 2016), information extraction (Narasimhan et al.", "startOffset": 108, "endOffset": 150}, {"referenceID": 17, "context": "Reinforcement learning with neural networks has been used for various NLP tasks, including text-based games (Narasimhan et al., 2015; He et al., 2016), information extraction (Narasimhan et al.", "startOffset": 108, "endOffset": 150}, {"referenceID": 40, "context": ", 2016), information extraction (Narasimhan et al., 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 15, "context": ", 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 29, "context": ", 2016), co-reference resolution (Clark and Manning, 2016), and chatbots (Li et al., 2016).", "startOffset": 73, "endOffset": 90}, {"referenceID": 42, "context": ", 2016) and solving memory puzzles (Oh et al., 2016).", "startOffset": 35, "endOffset": 52}, {"referenceID": 0, "context": "address the data efficiency problem by learning in a contextual bandit setting, which is known to be more tractable (Agarwal et al., 2014), and using reward shaping to increase exploration effectiveness.", "startOffset": 116, "endOffset": 138}, {"referenceID": 0, "context": "address the data efficiency problem by learning in a contextual bandit setting, which is known to be more tractable (Agarwal et al., 2014), and using reward shaping to increase exploration effectiveness. Zhu et al. (2017) address generalization of reinforcement learning to new target goals in visual search by providing the agent an image of the goal state.", "startOffset": 117, "endOffset": 222}, {"referenceID": 25, "context": "Reinforcement learning is extensively used in robotics (Kober et al., 2013).", "startOffset": 55, "endOffset": 75}, {"referenceID": 38, "context": "The agent also has access to K images of previous states and the previous action to distinguish between different stages of the execution (Mnih et al., 2015).", "startOffset": 138, "endOffset": 157}, {"referenceID": 16, "context": "We use a recurrent neural network (RNN; Elman, 1990) with a long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to map the instruction x\u0304 = \u3008x1, .", "startOffset": 34, "endOffset": 52}, {"referenceID": 18, "context": "We use a recurrent neural network (RNN; Elman, 1990) with a long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to map the instruction x\u0304 = \u3008x1, .", "startOffset": 82, "endOffset": 122}, {"referenceID": 39, "context": "The instruction representation x\u0304 is computed by applying the LSTM recurrence to generate a sequence of hidden states li = LSTM(\u03c8(xi), li\u22121), and computing the mean x\u0304 = 1 n \u2211n i=1 li (Narasimhan et al., 2015).", "startOffset": 184, "endOffset": 209}, {"referenceID": 37, "context": "sual state v (Mnih et al., 2013).", "startOffset": 13, "endOffset": 32}, {"referenceID": 55, "context": "We use policy gradient for reinforcement learning (Williams, 1992) to estimate the parameters", "startOffset": 50, "endOffset": 66}, {"referenceID": 50, "context": "The objective is an adaptation of the policy gradient objective defined by Sutton et al. (1999) to multiple starting states, one for each example.", "startOffset": 75, "endOffset": 96}, {"referenceID": 27, "context": "The primary theoretical advantage of a contextual bandit setting is much tighter sample complexity bounds when comparing upper bounds for contextual bandits (Langford and Zhang, 2007) to lower bounds (Krishnamurthy et al.", "startOffset": 157, "endOffset": 183}, {"referenceID": 26, "context": "The primary theoretical advantage of a contextual bandit setting is much tighter sample complexity bounds when comparing upper bounds for contextual bandits (Langford and Zhang, 2007) to lower bounds (Krishnamurthy et al., 2016) or upper bounds (Kearns et al.", "startOffset": 200, "endOffset": 228}, {"referenceID": 21, "context": ", 2016) or upper bounds (Kearns et al., 1999) for total reward maximization.", "startOffset": 24, "endOffset": 45}, {"referenceID": 56, "context": "To delay premature convergence we add an entropy term to the objective (Williams and Peng, 1991; Mnih et al., 2016).", "startOffset": 71, "endOffset": 115}, {"referenceID": 36, "context": "To delay premature convergence we add an entropy term to the objective (Williams and Peng, 1991; Mnih et al., 2016).", "startOffset": 71, "endOffset": 115}, {"referenceID": 36, "context": "Similar issues with vanilla policy gradient were recently reported for other tasks (Mnih et al., 2016).", "startOffset": 83, "endOffset": 102}, {"referenceID": 24, "context": "dients using ADAM (Kingma and Ba, 2014).", "startOffset": 18, "endOffset": 39}, {"referenceID": 24, "context": "ADAM(\u2206) applies a per-feature learning rate to the gradient \u2206 (Kingma and Ba, 2014).", "startOffset": 62, "endOffset": 83}, {"referenceID": 41, "context": "2 Ng et al. (1999) and Wiewiora et al.", "startOffset": 2, "endOffset": 19}, {"referenceID": 41, "context": "2 Ng et al. (1999) and Wiewiora et al. (2003) outline potential-based terms that realize sufficient conditions for safe shaping.", "startOffset": 2, "endOffset": 46}, {"referenceID": 41, "context": "For convenience, we briefly overview the theorems of Ng et al. (1999) and Wiewiora et al.", "startOffset": 53, "endOffset": 70}, {"referenceID": 41, "context": "For convenience, we briefly overview the theorems of Ng et al. (1999) and Wiewiora et al. (2003) in Appendix A.", "startOffset": 53, "endOffset": 97}, {"referenceID": 41, "context": "We design it to be a safe potential-based term (Ng et al., 1999):", "startOffset": 47, "endOffset": 64}, {"referenceID": 11, "context": "We incorporate complete trajectories by using a simplification of the shaping term introduced by Brys et al. (2015). Unlike F1, it requires access to the previous state and action.", "startOffset": 97, "endOffset": 116}, {"referenceID": 11, "context": "We incorporate complete trajectories by using a simplification of the shaping term introduced by Brys et al. (2015). Unlike F1, it requires access to the previous state and action. It is based on the look-back advice shaping term of Wiewiora et al. (2003), who introduced safe potential-based shaping that considers the previous state and action.", "startOffset": 97, "endOffset": 256}, {"referenceID": 8, "context": "Environment We use the environment of Bisk et al. (2016). The original task required predicting the source and target positions for a single block given an instruction.", "startOffset": 38, "endOffset": 57}, {"referenceID": 8, "context": "Data Bisk et al. (2016) collected a corpus of instructions paired with start and goal states.", "startOffset": 5, "endOffset": 24}, {"referenceID": 30, "context": "used SAIL navigation corpus (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 28, "endOffset": 74}, {"referenceID": 12, "context": "used SAIL navigation corpus (MacMahon et al., 2006; Chen and Mooney, 2011).", "startOffset": 28, "endOffset": 74}, {"referenceID": 8, "context": "In contrast, Bisk et al. (2016) evaluate the selection", "startOffset": 13, "endOffset": 32}, {"referenceID": 38, "context": "actions, (c) SUPERVISED: supervised learning with maximum-likelihood estimate using stateaction pairs from the demonstrations, (d) DQN: deep Q-learning with both shaping terms (Mnih et al., 2015), and (e) REINFORCE: policy gra-", "startOffset": 176, "endOffset": 195}, {"referenceID": 50, "context": "dient with cumulative episodic reward with both shaping terms (Sutton et al., 1999).", "startOffset": 62, "endOffset": 83}, {"referenceID": 20, "context": "A likely explanation is test-time execution errors leading to unfamiliar states with poor later performance (Kakade and Langford, 2002), a form of the covariate shift problem.", "startOffset": 108, "endOffset": 135}, {"referenceID": 21, "context": "limited data due to relatively high sample complexity (Kearns et al., 1999; Krishnamurthy et al., 2016).", "startOffset": 54, "endOffset": 103}, {"referenceID": 26, "context": "limited data due to relatively high sample complexity (Kearns et al., 1999; Krishnamurthy et al., 2016).", "startOffset": 54, "endOffset": 103}, {"referenceID": 29, "context": "While the contribution of initialization is modest, it provides faster learning (Li et al., 2016).", "startOffset": 80, "endOffset": 97}], "year": 2017, "abstractText": "We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent\u2019s exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.", "creator": "LaTeX with hyperref package"}}}