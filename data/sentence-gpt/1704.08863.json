{"id": "1704.08863", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "On weight initialization in deep neural networks", "abstract": "A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations, which I will use to describe their general purpose effects. The following data illustrate an approximate model that employs two input values. First, the weights are ordered in a given number of dimensions, but the weights are selected individually from different dimensions. Second, we find that all of the weights in a neural network are fixed relative to that given number of dimensions. Since these weights are independent of each other, the weights are determined by the two input values that are the weights that were used to evaluate them: The weights are the only constant for the values that are positive for each input. The weights are also computed with a linear activation function called the Linear Activation function. The results show that all of the weights in a neural network are determined by the number of dimensions for each input. Furthermore, we find that the weights are always positive and always negative for the weights that are negative.\n\n\n\n\n\nAn example of this model is in our study, when we start with three inputs in the neural network, the values vary based on the degree of the variance. However, one of these input values is the weight that was applied by a particular input (i.e., the number of the weights is determined by the number of the weights that are positive for each input). The next example is the output of one input:\n\n\nThe data show the output of two input values in this way:\nThe input values in this way are computed by a linear activation function called the Linear Activation function. The weights are determined by the number of dimensions of the weights that are positive for each input. The weights are then computed with a linear activation function called the Linear Activation function. The weights are then computed with a linear activation function called the Linear Activation function.\nAn example of this model is in our study, when we start with three input values in the neural network, the values vary based on the degree of the variance. However, one of these input values is the weight that was applied by a particular input (i.e., the number of the weights is determined by the number of the weights that are negative for each input. The weights are then computed with a linear activation function called the Linear Activation function. The weights are then computed with a linear activation function called the Linear Activation function.", "histories": [["v1", "Fri, 28 Apr 2017 09:57:52 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v1", "9 pages, 4 figures"], ["v2", "Tue, 2 May 2017 22:43:10 GMT  (248kb,D)", "http://arxiv.org/abs/1704.08863v2", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth krishna kumar"], "accepted": false, "id": "1704.08863"}, "pdf": {"name": "1704.08863.pdf", "metadata": {"source": "CRF", "title": "On weight initialization in deep neural networks", "authors": ["Siddharth Krishna Kumar"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In recent years, there have been rapid advances in our understanding of deep neural networks. These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]). These successes have been achieved despite the notorious difficulty in training these deep models.\nPart of the difficulty in training these models lies in determining the proper initialization strategy for the parameters in the model. It is well known [12] that arbitrary initializations can slow down or even completely stall the convergence process. The slowdown arises because arbitrary initializations can result in the deeper layers receiving inputs with small variances, which in turn slows down back propagation, and retards the overall convergence process. Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.\nIn this paper, I revisit the oldest, and most widely used approach to the problem with the goal of resolving some of the unanswered theoretical questions which remain in the literature. The problem can be stated as follows: If the weights in a neural network are initialized using samples from a normal distribution, N (0, v2), how should v2 be chosen to ensure that the variance of the outputs from the different layers are approximately the same?\nThe first systematic analysis of this problem was conducted by Glorot and Bengio [3] who showed that for a linear activation function, the optimal value of v2 = 1/N , where N is the number of nodes feeding into that layer. Although the paper makes several assumptions about the inputs to the model, it works extremely well in many cases and is widely used in the initialization of neural networks to date; this initialization scheme is commonly referred to as the Xavier initialization.\nIn an important follow up paper, He and colleagues [6] argue that the Xavier initialization does not work well with the RELU activation function, and instead propose an initialization of v2 = 2/N (commonly referred to as the He initialization). In support of their initialization, they provide an example of a 30 layer neural network which converges with the He initialization, but not under the Xavier initialization. To the best of my knowledge, the precise reason for the convergence of one method and the non-convergence of the other is not fully understood.\nMy main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6]. For the class of activation functions differentiable at 0, I provide a general initialization strategy. For the class of activation functions that are not differentiable at 0, I focus on the Rectified Linear Unit (RELU) and provide a rigorous proof of the He initialization. I also provide theoretical insights into why the 30 layer neural network converges with the He initialization but not with the Xavier initialization. As a small bonus, I resolve an unanswered question posed in [3] regarding the distributions of activations under the hyperbolic tangent activation.\nar X\niv :1\n70 4.\n08 86\n3v 1\n[ cs\n.L G\n] 2\n8 A\npr 2\n01 7"}, {"heading": "2 The setup", "text": "Consider a deep neural network with M layers. The relationship between the inputs to the mth layer (xm) and m+ 1th layer (xm+1) are described by the recursions\nym(i) = j=N\u2211 j=1 Wm(i, j)xm(j) = j=N\u2211 j=1 pij (1)\nand xm+1(i) = g(ym(i)). (2)\nHere pj = Wm(i, j)xm(j), Wm is a matrix of weights for themth layer, g is the non-linear activation function, and N is the number of nodes in the hidden layers respectively. The weights Wm(i, j) are assumed to be independent identically distributed normal random variables with mean 0 and variance v2. Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with mean 0 and variance 1. For convenience, I use rm and u2m to denote the mean and variance of ym(i) respectively.\nDue to the symmetry in the problem, all inputs to the mth layer will have the same means and variances during the first forward pass (i.e., E(xm(i)) = \u00b5m and V ar(xm(i)) = s2m for all i); the covariances between the inputs to the mth layer need not be 0.\nThe goal is to find the value of v2 which ensures that s21 \u2248 s22... \u2248 s2M = 1 during the first forward pass. To accomplish this, I need to express the central moments of xm+1(i) in terms of the central moments of xm(i) for an arbitrary value of m. I begin by analyzing properties of the neural network that are independent of the activation function considered in the analysis.\nProposition 1. During the first iteration, Wm(i, j) is independent of xm(k) for all values of i, j and k\nUsing the recursions in (1) and (2), xm(k) can be expressed as some non-linear function of the weights in the first m \u2212 1 layers, and the inputs to the first layer. Since the weights in the mth layer are independent of the inputs to the first layer and the weights in all other layers, the weights in the mth layer will also be independent of any non-linear function of these quantities. Therefore, Wm(i, j) is independent of xm(k) for all values of i, j and k. Furthermore, sinceWm(i, j) is independent of xm(k) and xm(l),Wm(i, j) will also be independent of xm(k)xm(l)\nTaking expectations in (1) and using proposition 1, along with the fact that E(Wm(i, j)) = 0 yields\nrm = E(ym(i)) = j=N\u2211 j=1 E(Wm(i, j))E(xm(j)) = 0. (3)\nTherefore, u2m = V ar(ym(i)) = E ( ym(i) 2 ) \u2212 (E(ym(i)))2 = E ( ym(i) 2 ) . Using (1),\nu2m = E j=N\u2211 j=1 Wm(i, j)xm(j) 2\n= j=N\u2211 j=1 E ( Wm(i, j) 2xm(j) 2 ) + 2 \u2211 E (Wm(i, j)xm(j)Wm(k, l)xm(l)) .\n(4)\nFrom proposition 1, Wm(i, j) and Wm(k, l) will (a) be independent of each other, and (b) be independent of xm(j)xm(l). Using these results, along with the fact that E(Wm(i, j)) = 0 gives\nE (Wm(i, j)xm(j)Wm(k, l)xm(l)) = E(Wm(i, j))E(Wm(k, l))E(xm(j)xm(l)) = 0. (5)\nPlugging (5) into (4) gives\nu2m = j=N\u2211 j=1 E ( Wm(i, j) 2xm(j) 2 )\n= j=N\u2211 j=1 E ( Wm(i, j) 2 ) E ( xm(j) 2 )\n= Nv2(s2m + \u00b5 2 m)\n(6)\nfor all i. Interestingly, this result holds for any arbitrary covariance structure of the inputs to the mth layer.\nEquations (3) and (6) provide insights into the central moments of ym(i), but can we derive insights into the distribution of ym(i)? To answer this question, I make the additional assumption that the number of nodes in the hidden layer (N) is \u2018large\u2019; this assumption is reasonable given that most neural networks have several hundred nodes in the hidden layers. Under this assumption, we have the following result\nProposition 2. ym(i) will be approximately normally distributed for all values of m and i.\nFor the first iteration, note that E(pij) = E (Wm(i, j)xm(j)) = E (Wm(i, j))E (xm(j)) = 0. Furthermore, for j 6= k,\nCov(pij , pik) = E(pijpik)\u2212 E(pij)E(pik) = E(pijpik)\n= E (Wm(i, j)xm(j)Wm(i, k)xm(k)) = 0,\n(7)\nwhere the last equality follows from (5). This implies that pi1, pi2 ... piN are independent and identically distributed random variables. Therefore by the Central Limit Theorem, we expect\nym(i) = j=N\u2211 j=1 pij to converge to a normal distribution when N is large [10]. Even when pi1, pi2 ... piN\nare dependent and not identically distributed, the conditions required to ensure that ym(i) = j=N\u2211 j=1 pij converges to a normal distribution are weak (for a list of all the conditions, see Theorem 2.8.2 in [10]). Thus, ym(i) is expected to to be approximately normally distributed during most iterations.\nThe analysis thus far has focused on providing general insights into the distribution of ym(i) resulting from equation (1). In order to analyze the role of the non-linearity induced by (2), assumptions need to be made about the nature of g(x). In particular, my analysis critically hinges on the differentiability of g(x) at 0. Accordingly, I split the analysis into two cases. The first case deals with the general class of activation functions differentiable at 0. In the second case, instead of considering all possible non-differentiable functions, I focus on the Rectified Linear Unit (RELU) which is commonly used in the analysis of neural networks."}, {"heading": "3 Activation functions differentiable at 0", "text": "When g(x) is differentiable at 0, we can perform a Taylor expansion in (2) about E(ym(i)) = 0. Assuming that the higher order terms can be ignored,\nxm+1(i) \u2248 g(0) + (ym \u2212 0)g\u2032(0). (8)\nTaking the expectation in (8) gives\n\u00b5m+1 = E(xm+1(i)) \u2248 g(0). (9)\nThis equation suggests that the expected value of the inputs to the (m + 1)th layer has little dependence on the moments of the inputs to the mth layer. Using this result recursively suggests that for all layers (barring the first),\n\u00b5j = g(0) for all j \u2265 1. (10)\nUsing (6) and (10), the variance of xm+1(i) can be computed from (8) as\ns2m+1 = V ar(xm+1(i)) \u2248 (g\u2032(0))2V ar(ym(i)) = N(g\u2032(0))2v2(s2m + \u00b5 2 m)\n= N(g\u2032(0))2v2(s2m + g(0) 2).\n(11)\nUsing the condition s2m \u2248 s2m\u22121... = s21 = 1 along with (10) and (11) gives\nv2 = 1\nN(g\u2032(0))2(1 + g(0)2) , (12)\nEquation (12) provide a general weight initialization strategy for any arbitrary differentiable activation function. I use the results developed in this section to analyze the optimal value of v2 for two commonly used differentiable activation functions - the hyperbolic tangent and the sigmoid.1"}, {"heading": "3.1 Hyperbolic tangent activation", "text": "For the hyperbolic tangent\ng(x) = tanh(x), (13)\nwe have g(0) = 0 and g\u2032(0) = 1. Plugging these results in (12) yields\nv \u2248 1\u221a N , (14)\nwhich is precisely the Xavier Initialization.\nSequential saturation with the hyperbolic tangent In their analysis of a neural network with the hyperbolic tangent activations, [3] find that the deeper layers in the neural network have a greater proportion of unsaturated nodes than the shallower layers. As is stated in their paper, \u2018why this is happening remains to be understood\u2019.\nTo explain their finding, I begin by noting that in [3], the authors initialize the weights using samples from a uniform distribution ( U [\u22121/ \u221a N, 1/ \u221a N ] ) having a variance of 1/3N . Therefore,\nfrom (10) and (11), with g(0) = 0 and g\u2032(0) = 1, we have \u00b5m = 0 and 1In their calculations, [3] and [6] impose an additional set of constraints to ensure that the variance is maintained even during the backward pass. I believe that this is not required, since the requirement that the variance of the inputs at each layer be the same ensures that the gradient flows through in the backward pass.\ns2m+1 = 1\n3 s2m < s 2 m (15)\nrespectively. From (6), this implies that u2m+1 < u2m for all m (i.e., u2m is a decreasing function of m). Furthermore from proposition 2, ym(i) \u223c N (0, u2m). Therefore, xm+1(i) will be the tanh transformation of a normal random variable. Using results from [4], xm+1(i) = tanh(ym(i)) will have a probability density function (pdf) given by\nf(y) = 1 1\u2212 y2 1\u221a 2\u03c0u2m e\n( \u2212 t2y\n2u2m ) , (16)\nwhere ty = 1\n2 ln\n( 1 + y\n1\u2212 y\n) . (17)\nPlots of this pdf for different values of um (provided in Figure 1) produce trends similar to those observed by the simulation studies of [3] (figure 4 in their paper). A comparison of Figure 1 and figure 4 of [3] suggests that u2m is a decreasing function of m, as is expected.\nFrom the results in [4], we expect the activations to be (a) approximately normally distributed when um is close to 0 and (b) bimodally distributed with local maximas near -1 and +1 when um is close to 1. Accordingly, since u2m is a decreasing function of m, we expect the activations from the shallower layers to be more saturated (i.e., more concentrated near -1 and +1), and the saturation in the activations to reduce as we go to the deeper layers in the network."}, {"heading": "3.2 Sigmoid activation", "text": "For the sigmoid activation defined as\ng(x) = 1\n1 + e\u2212x (18)\nwe have g(0) = 0.5, g\u2032(0) = 1/4. Plugging these values in (12) yields\nv \u2248 3.6\u221a N , (19)\nTo compare the initialization described in (19) with the Xavier initialization, I use a simple 10 layer neural network whose architecture is described in Figure 2. For my experiments, I use the CIFAR 10 dataset [9] comprising 60,000 32 \u00d7 32 color images evenly split over 10 classes. The dataset comprises 50,000 training examples (which forms the training dataset in my analyses) and 10,000 test examples (which forms the validation dataset in my analyses).\nFirst, I train the neural network with the Xavier initialization for 10 iterations and compute the top 5 accuracy on the validation dataset for each iteration. Next, I repeat the process using the initialization stated in (19). A comparison of the validation accuracies for the 2 cases is provided in Figure 3, which shows that the convergence appears to stall with the Xavier initialization, but proceeds rapidly with the initialization proposed in (19).2"}, {"heading": "4 Activation functions not differentiable at 0", "text": "When g(x) is not differentiable at 0, the analysis seems more difficult than in the previous section. Instead of attempting to provide a general solution, I focus on the most important non-differentiable activation function used in the analysis of neural networks - the Rectified Linear Unit (RELU)."}, {"heading": "4.1 RELU activation", "text": "Since the RELU activation is not differentiable at 0, the results from (8 \u2013 11) cannot be used to compute the optimal value of v2. To proceed, I use proposition 2 and (3) which state that\n2Python code (using the package Keras [1]) to replicate Figure 3 can be downloaded from https://github.com/ sidkk86/weight_initialization\nfor the first iteration, ym(i) \u223c N (0, u2m). We are interested in in the mean and variance of xm+1(i) = max(0, ym(i)). The mean will be given by\n\u00b5m+1 = E(ym(i)I(ym(i) > 0)) = 1\num \u221a 2\u03c0 \u222b \u221e 0 xe \u2212 x2 2u2m dx = um\u221a 2\u03c0 \u222b \u221e 0 e\u2212tdt = um\u221a 2\u03c0 . (20)\nSimilarly,\nE(xm+1(i) 2) =\n1\num \u221a 2\u03c0 \u222b \u221e 0 x2e \u2212 x2 2u2m dx = ( 1 2 ) 1 u \u221a 2\u03c0 \u222b \u221e \u2212\u221e x2e\u2212 x2 2u2 dx = u2m 2 . (21)\nUsing (20) and (21),\ns2m+1 = E(xm+1(i) 2)\u2212 \u00b52m+1 = u2m 2 \u2212 u 2 m 2\u03c0 \u2248 0.34u2m. (22)\nFor the variance to be maintained at each iteration, we require s2m+1 \u2248 1 which yields\nu2m \u2248 3. (23)\nPlugging (23) in (20) yields \u00b5m+1 \u2248 0.7. By the symmetry of the problem during the first iteration, we expect \u00b5m+1 \u2248 \u00b5m... \u2248 \u00b52 \u2248 0.7. Using this result in (6) yields\n3 = Nv2(1 + 0.49) (24)\nor v2 \u2248 2/N, (25)\nwhich is consistent with that obtained by [6]."}, {"heading": "To converge or not to converge, that is the question.", "text": "In [6] paper, the authors provide an example of a 22 layer neural network using RELU activations which converges with the Xavier Initialization, and a 30 layer neural network which does not converge with the same initializations and activation functions.\nTo understand why this happens, I compute the central moments of xm+1 in terms of the central moments of xm when v2 = 1/N . From (6) we have\nu2m = s 2 m + \u00b5 2 m. (26)\nPlugging results from (26) into (20) yields the recursion\n\u00b52m+1 = 1\n2\u03c0 (s2m + \u00b5 2 m) \u2248 0.16(s2m + \u00b52m). (27)\nSimilarly, plugging results from (26) and (27) in (22) yields\ns2m+1 \u2248 0.34(s2m + \u00b52m). (28)\nSimple manipulations of equations (26 \u2013 28) gives\n\u00b52m \u2248 0.16\u00d7 (0.51)m\u22121 (29)\nand s2m \u2248 0.34\u00d7 (0.51)m\u22121. (30)\nfor all m \u2265 1. These approximations are remarkably accurate, as is demonstrated by comparisons with the simulation experiments described in Figure 4.\nEquation (30) shows that the variance of the inputs to the deeper layers is exponentially smaller than the variance of the inputs to the shallower layer. Therefore, the deeper the neural network, the worse the performance of the Xavier Initialization will be. From (30), s222 \u2248 1.62\u00d7 10\u22127 and s230 \u2248 6.33 \u00d7 10\u221210. Thus the variance in the input to the 30th layer will be (0.51)8 = 3 \u00d7 10\u22123 times smaller than the variance to the 22nd layer, and explains the possible reason why the 30 layer neural network described in [6] converges, but the 22 layer neural network does not. 3"}, {"heading": "5 Conclusion", "text": "In this paper, I have provided a general framework for weight initialization with non-linear activation functions. First, I provide a general formula for the ideal weight initialization for all activation functions differentiable at 0. I show how the weight initializations change for the hyperbolic tangent and sigmoid activation functions. Second, I focus only on the Rectified Linear Unit (RELU) from the class of functions that are non-differentiable at 0, and I provide a rigorous proof of the He\n3It is surprising that the 22 layer neural network converges!\nInitialization. Finally, I show why the Xavier initialization fails to work with the RELU activation function. Given the sharp increase in non-differentiable activation functions over the years, a more general version of my (largely incomplete) analysis of non-differentiable functions is warranted. My analysis repeatedly illustrates the drastic difference in dynamics which can result from introducing non-linearities in the system."}], "references": [{"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "The tanh transformation", "author": ["Michael D Godfrey"], "venue": "Information Systems Laboratory, Stanford University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Cs 231n: Convolutional neural networks for visual recognition, lecture 5, slide", "author": ["Andrej Karpathy", "Justin Johnson", "Fei Fei Li"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Elements of large-sample theory", "author": ["Erich Leo Lehmann"], "venue": "Springer Science & Business Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "All you need is a good init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "arXiv preprint arXiv:1511.06422,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["David Sussillo", "LF Abbott"], "venue": "arXiv preprint arXiv:1412.6558,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["Christian Szegedy", "Alexander Toshev", "Dumitru Erhan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["Alexander Toshev", "Christian Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 187, "endOffset": 190}, {"referenceID": 13, "context": "These advances have resulted in breakthroughs in several fields, ranging from image recognition ([13],[18],[19]) to speech recognition ([5],[11],[17]) to natural language processing ([2],[8], [15]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 10, "context": "It is well known [12] that arbitrary initializations can slow down or even completely stall the convergence process.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Weight initialization is an area of active research, and numerous methods ([12], [14], [16] to state a few) have been proposed to deal with the problem of the shrinking variance in the deeper layers.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "The problem can be stated as follows: If the weights in a neural network are initialized using samples from a normal distribution, N (0, v), how should v be chosen to ensure that the variance of the outputs from the different layers are approximately the same? The first systematic analysis of this problem was conducted by Glorot and Bengio [3] who showed that for a linear activation function, the optimal value of v = 1/N , where N is the number of nodes feeding into that layer.", "startOffset": 342, "endOffset": 345}, {"referenceID": 4, "context": "In an important follow up paper, He and colleagues [6] argue that the Xavier initialization does not work well with the RELU activation function, and instead propose an initialization of v = 2/N (commonly referred to as the He initialization).", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "My main contributions in this paper are to (a) generalize the results of [3] to the case of non-linear activation functions and (b) to provide a continuum between the results of [3] and [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "As a small bonus, I resolve an unanswered question posed in [3] regarding the distributions of activations under the hyperbolic tangent activation.", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with mean 0 and variance 1.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Consistent with the assumptions in [3] and [6], I assume that the inputs to the first layer are independent and identically distributed random variables with mean 0 and variance 1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "j=1 pij to converge to a normal distribution when N is large [10].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "2 in [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Sequential saturation with the hyperbolic tangent In their analysis of a neural network with the hyperbolic tangent activations, [3] find that the deeper layers in the neural network have a greater proportion of unsaturated nodes than the shallower layers.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "To explain their finding, I begin by noting that in [3], the authors initialize the weights using samples from a uniform distribution ( U [\u22121/ \u221a N, 1/ \u221a N ] ) having a variance of 1/3N .", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "Therefore, from (10) and (11), with g(0) = 0 and g\u2032(0) = 1, we have \u03bcm = 0 and 1In their calculations, [3] and [6] impose an additional set of constraints to ensure that the variance is maintained even during the backward pass.", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "Therefore, from (10) and (11), with g(0) = 0 and g\u2032(0) = 1, we have \u03bcm = 0 and 1In their calculations, [3] and [6] impose an additional set of constraints to ensure that the variance is maintained even during the backward pass.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Using results from [4], xm+1(i) = tanh(ym(i)) will have a probability density function (pdf) given by", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Plots of this pdf for different values of um (provided in Figure 1) produce trends similar to those observed by the simulation studies of [3] (figure 4 in their paper).", "startOffset": 138, "endOffset": 141}, {"referenceID": 1, "context": "A comparison of Figure 1 and figure 4 of [3] suggests that um is a decreasing function of m, as is expected.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "From the results in [4], we expect the activations to be (a) approximately normally distributed when um is close to 0 and (b) bimodally distributed with local maximas near -1 and +1 when um is close to 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "For my experiments, I use the CIFAR 10 dataset [9] comprising 60,000 32 \u00d7 32 color images evenly split over 10 classes.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "or v \u2248 2/N, (25) which is consistent with that obtained by [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "In [6] paper, the authors provide an example of a 22 layer neural network using RELU activations which converges with the Xavier Initialization, and a 30 layer neural network which does not converge with the same initializations and activation functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Figure 4: A comparison of the predicted means and standard errors obtained from (29 \u2013 30) with the simulated values reported in slide 61 of [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "51) = 3 \u00d7 10\u22123 times smaller than the variance to the 22 layer, and explains the possible reason why the 30 layer neural network described in [6] converges, but the 22 layer neural network does not.", "startOffset": 142, "endOffset": 145}], "year": 2017, "abstractText": "A proper initialization of the weights in a neural network is critical to its convergence. Current insights into weight initialization come primarily from linear activation functions. In this paper, I develop a theory for weight initializations with non-linear activations. First, I derive a general weight initialization strategy for any neural network using activation functions differentiable at 0. Next, I derive the weight initialization strategy for the Rectified Linear Unit (RELU), and provide theoretical insights into why the Xavier initialization is a poor choice with RELU activations. My analysis provides a clear demonstration of the role of non-linearities in determining the proper weight initializations.", "creator": "LaTeX with hyperref package"}}}