{"id": "1704.08883", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning", "abstract": "Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process. With the approach described below, a variety of other methods have been successfully demonstrated by several neural networks.\n\n\n\n\nThe model is based on an adaptive agent model, with a high level of performance and safety.\nThe model is based on an adaptive agent model, with a high level of performance and safety.\n\nThe model is based on an adaptive agent model, with a high level of performance and safety.\nThe model is based on an adaptive agent model, with a high level of performance and safety. The agent then select the optimal control action with the highest value. The agent then randomly selects a selected action that will generate a high response to the current state of the simulation. The agent then assigns a value to each decision being made during the training phase.\nThe agent then selects a selected action that will generate a high response to the current state of the simulation. The agent then selects a selected action that will generate a high response to the current state of the simulation. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase. The agent then assigns a value to each decision being made during the training phase", "histories": [["v1", "Fri, 28 Apr 2017 11:44:42 GMT  (128kb,D)", "https://arxiv.org/abs/1704.08883v1", null], ["v2", "Sat, 27 May 2017 14:45:56 GMT  (128kb,D)", "http://arxiv.org/abs/1704.08883v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["seyed sajad mousavi", "michael schukat", "enda howley"], "accepted": false, "id": "1704.08883"}, "pdf": {"name": "1704.08883.pdf", "metadata": {"source": "CRF", "title": "Traffic Light Control Using Deep Policy-Gradient and Value-Function Based Reinforcement Learning", "authors": ["Seyed Sajad Mousavi", "Michael Schukat", "Enda Howley"], "emails": ["s.mousavi1@nuiglaway.ie", "michael.schukat@nuigalway.ie", "enda.howley@nuigalway.ie"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Theory of computation\u2192 Sequential decision making;\nKeywords Traffic control, Reinforcement learning, Deep learning, Policy gradient method, Value-function method, Artificial neural networks"}, {"heading": "1. INTRODUCTION", "text": "With regard to fast growing population around the world, the urban population in the 21st century is expected to increase dramatically. Hence, it is imperative that urban infrastructure is managed effectively to contend with this growth. One of the most critical consideration when designing modern cities is developing smart traffic management systems. The main goal of a traffic management system is reducing traffic congestion which nowadays is one of the major issues of megacities. Efficient urban traffic management results in timea and financial savings as well as reducing CO2 emission into atmosphere. To address this issue, a lot of solutions have been proposed [23, 4, 1, 22]. They can be roughly classified into three groups. The first is pre-timed signal control, where a fixed time is determined for all green phases according to historical traffic demand, without considering possible fluctuations in traffic demand. The second\nis vehicle-actuated signal control where, traffic demand information is used, provided by inductive loop detectors on an equipped intersection to decide to control the signals, e.g. extending or terminating a green phase. The third is adaptive signal control, where the signal timing control is managed and updated automatically according to the current state of the intersection (i.e. traffic demand, queue length of vehicles in each lane of the intersection and traffic flow fluctuation) [13]. In this study, we are interested in the third approach and aim to propose two novel methods for traffic signal control by leveraging recent advances in machine learning and artificial intelligence fields [26, 27].\nReinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver. It does not need to have a perfect knowledge of the environment in advance, for example traffic flow. Instead they are able to gain knowledge and model the dynamics of the environment just by interacting with it. A reinforcement learning agent learns based on trial and error. It receives a scalar reward after taking each action in the environment. The obtained reward is based on how well the taken action is and the agent\u2019s goal is to learn an optimal control policy so the discounted cumulative reward is maximized via repeated interaction with its environment. Aside from traffic control, reinforcement learning has been applied to a number of real world problems such as cloud computing [12, 11].\nTypically the complexity of using reinforcement learning in real world applications such as traffic signal management, grows exponentially as state and action spaces increase. To deal with this problem, function approximation techniques and hierarchical reinforcement learning approaches can be used. Recently, deep learning has gained huge attraction and has been successfully combined with reinforcement learning techniques to deal with complex optimization problems such as playing Atari 2600 games [27], Computer Go program [33], etc., where the classical RL methods could not provide optimal solutions. In this way, the current state of the environment is fed into a deep neural net (e.g. a convolutional neural network [20]) trained by reinforcement learning techniques to predict the next possible optimal action(s).\nInspired by the successes of combining reinforcement learning with deep learning paradigm and with regard to the complex nature of environment of traffic signal control problem, in this paper we aim to use the effectiveness and power of deep reinforcement learning to build adaptive signal control methods in order to optimize the traffic flow. Although a few previous studies have tried to apply deep reinforcement\nar X\niv :1\n70 4.\n08 88\n3v 2\n[ cs\n.L G\n] 2\n7 M\nay 2\n01 7\nlearning in the traffic signal control problem [38, 14], in this research the state representation is different. Also, One of our methods uses policy gradient method which does not suffer from oscillations and instabilities during training process and can take full advantage of the available data of the environment to develop the optimal control policy.\nWe propose adaptive signal controllers by combination two reinforcement learning approaches (i.e. policy gradient and action-value function) and a deep convolution neural network, which perceive embedded camera observations in order to produce control signals in an isolated intersection. We conduct simulated experiments with our proposed methods in SUMO traffic simulator.\nThe rest of the paper is organized as follows. Section 2 provides related work in the area of traffic light control (TLC). Section 3 gives a brief review of reinforcement learning techniques which we have used in this research. Section 4 presents how to formulate the TLC problem as a reinforcement learning task and the proposed methods to solve the task. Then Section 5 provides simulation results and the performance of the proposed approaches. Finally Section 6 concludes the paper and give some directions for future research."}, {"heading": "2. RELATED WORK", "text": "A lot of research has been done in academic and industry communities to build adaptive traffic signal control systems. In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7]. These works have achieved promising results. However, their simulation testbeds have not been mature enough to be comparable with more realistic situations. Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3]. All this these attempts viewed the traffic light control problem as a fully observable Markov decision process (MDP) and investigated whether Q-learning algorithm can be applied to it. However, Richter\u2019s study formulated the traffic problem as a partially observable MDP (POMDP) and applied policy gradient methods to guarantee local convergence under a partial observable environment [31].\nBy utilizing advances in deep learning and its application to different domains [10, 11], deep learning has gained attention in the area of traffic management systems. Previous research has used deep stacked autoencoders (SAE) neural networks to estimate Q-values, where each Q-value is corresponding to each available signal phase [21]. It considered measures of speed and queueing length as its state in each time step of learning process of its proposed method. Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values. Their state representations were a binary matrix of the positions of vehicles on the lanes of an intersection, and a combination of the presence matrix of vehicles, speed and the current traffic signal phase, respectively. However, we use raw visual input data of the traffic simulator snapshots as system states. Moreover, in addition to estimating Q-function, one of the proposed methods directly maps from the input state to a probability distribution over actions (i.e. signal phases) via deep policy gradient\nmethod."}, {"heading": "3. BACKGROUND", "text": "In this section, we will review Reinforcement Learning (RL) approaches and briefly describe how RL is applied to real world problems where the number of states and actions are extremely high so that the regular reinforcement learning techniques cannot deal with them."}, {"heading": "3.1 Reinforcement Learning", "text": "A common reinforcement learning [34] setting is shown in Figure 1 where an RL agent interacts with an environment. The interaction is continued until reaching a terminal state or the agent meets a termination condition. Usually the problems that RL techniques are applied to, are treated as Markov decision processes (MDPs). A MDP is defined as a five-tuple < S,A, T,R, \u03b3 > , where S is the set of states in the state space of the environment, A is the set of actions in the action space that the agent can use in order to interact with the environment, T is the transition function, which is the probability of moving between the environment states, R is the reward function and \u03b3 \u2208 [0, 1] is known as the discount factor, which models the importance of the future and immediate rewards. At each time step t, the agent perceives the state st \u2208 S and, based on its observation, selects an action at. Taking the action, leads to the state of the environment transitions to the next states st+1 \u2208 S regarding the transition function T . Then, the agent receives reward rt which is determined by the reward function R.\nThe goal of the learning agent in reinforcement learning framework is to learn an optimal policy \u03c0 : S \u00d7 A \u2192 [0, 1] which defines the probability of selecting action at in state st, so that with following the underlying policy the expected cumulative discounted reward over time is maximized. The discounted future reward, Rt at time t is defined as follows:\nRt = E[ \u221e\u2211 k=0 \u03b3krt+k], (1)\nwhere the role of the discount factor \u03b3 is to trade off the worth of immediate and future rewards. In most real world problems, there are many states and actions which make it impossible to apply classic reinforcement learning techniques, which consider tabular representations for their state and action spaces. For example, in the problem of traffic light optimization, that we interest in this paper, the state space is continuous. Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].\nDifferent forms of function approximators can be used with reinforcement learning techniques. For example, linear function approximation, a linear combination of feature of state and action spaces f and learned weights w (e.g.\u2211 i fiw) or a non-linear function approximation (e.g. a neural network). Until recently, the majority of work in reinforcement learning has been applying linear function approximatiors. More recently, deep neural networks (DNNs) such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), stacked auto-encoders (SAE), etc. have also been commonly used as function approximators for large reinforcement learning tasks [19, 26]. The interested readers are referred to [29] for a review of using deep neural networks\nwith reinforcement learning framework."}, {"heading": "3.2 Deep learning and Deep Q-learning", "text": "Deep learning techniques are one of the best solutions to address high dimensional data and extract discriminative information from the data. Deep learning algorithms have the capability of automating feature extraction (the extraction of representations) from the data. The representation are learnt through the data which are fed directly into deep nets without using human knowledge (i.e. automated feature extraction). Deep learning models contain multiple layers of representations. Indeed, it is a stack of building blocks such as auto-encoders, Restricted Boltzmann Machines (RBMs) and convolutional layers. During training, the raw data is fed into a network consisting of multiple layers. The output of the each layer which is nonlinear feature transformations, is used as inputs to the next layers of the deep neural network. The output representation of the final layer can be used for constricting classifiers or those applications which can have the better efficiency and performance with abstract representation of the data in a hierarchical manner as inputs. A nonlinear transformation is applied at each layer on its input to try to learn and extract underlying explanatory factors. Consequently, this process learns a hierarchy of abstract representations.\nOne of the main advantages of deep neural networks is the capability of automating feature extraction from row input data. A deep Q-learning Network (DQN) [26] uses this benefit of deep learning in order to represent the agent\u2019s observation as an abstract representation in learning an optimal control policy. The DQN method aggregates a deep neural network function approximator with Q-learning to learn action value function and as a result a policy \u03c0, the behaviour of the agent which tells the agent what action should be selected for each input state. Applying non-linear function approximators such as neural networks with modelfree reinforcement learning algorithms in high-dimensional continuous state and action spaces, has some convergence problems [37]. The reasons for these issues are: 1) Consecutive states in reinforcement learning tasks have correlation. 2) The underlying policy of the agent is changing frequently, because of slight changes in Q-values. To cope with these problems, the DQN provides some solutions which improve the performance of the algorithm significantly. For the prob-\nlem of correlated states, DQN uses the previously proposed experience replay approach [24]. In this way, at each time step, the DQN stores the agent\u2019s experience (st, at, rt, rt+1) into a date set D, where st, at, and rt are the state, chosen action and received reward, respectively and st+1 is the state at the next time step. To update the network, the DQN utilizes stochastic minibatch updates with uniformly random sampling from the experience replay memory (previous observed transitions) at training time. This negates strong correlations between consecutive samples. Another approach to deal with aforementioned convergence issues, which we also examine in this research, is the policy gradient methods. This approach has demonstrated better convergence properties in some RL problems [35]."}, {"heading": "3.3 Policy Gradient Methods", "text": "A Policy Gradient (PG) method tries to optimize a parameterized policy function by gradient descent method. Indeed, policy gradient methods are interested in searching policy space to learn policies directly, instead of estimating state-value or action-value functions. Unlike the traditional reinforcement learning algorithms, PG methods do not suffer from the convergence problems of estimating value functions under nonlinear function approximation or in the environments which might be partially observable MDPs. They can also deal with the complexity of continuous state and action spaces better than purely value-based methods [35]. Policy gradient methods estimate policy gradients using Monte Carlo estimates of the policy gradients [6]. These methods are guaranteed to converge to a local optimum of their parametrized policy function. However, typically PG methods result in high variance in their gradient estimates. Hence, in order to reduce the variance of the gradient estimators, some methods subtract a base line function from the policy gradients. The baseline function can be calculated in different manners [32, 40]. By inspiring these features of PG methods and successes of neural networks in automatic feature abstractions, we use deep neural networks to represent an optimal traffic control policy directly in the traffic signal control problem."}, {"heading": "4. SYSTEM DESCRIPTION", "text": "In this section, we will formulate traffic light control problem as a reinforcement learning task by describing the states, actions and reward function. We then present the policy as a deep neural network and how to train the network."}, {"heading": "4.1 State Representation", "text": "We represent the state of the system as an image st \u2208 Rd or a snapshot of the current state of a graphical simulator (e.g. SUMO-GUI [18]) which is a vector of row pixel values of current view of the intersection at each step of simulation (as shown in Figure 1). This kind of representation is like putting a camera on an intersection which enables it to view the whole intersection. The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14]. Regardless of these states representations that are using a prior knowledge provided, they make as-\nsumptions which are not generalizable for the real world. For instance, they discretize a lane segment of an intersection into cells with a constant length c which is supposed to be the vehicle length to build the vehicle\u2019s speed and presence vectors. However, by feeding the state as an image to a convolutional neural network, the system can detect the location and presence of all vehicles with different lengths and as result the vehicles\u2019 queue on each lane. Furturmore, by stacking a history of consecutive observations as input, the convolutional layers of a deep network are able to estimate velocity and travel direction of vehicles. Hence, the system can implicitly benefit from these information as well."}, {"heading": "4.2 Action Set", "text": "To control traffic signal phases, we define a set of possible actions A = {North/South Green (NSG), East/West Green (EWG)}. NSG allows vehicles to pass from North to South and vice versa, and also indicates the vehicles on East/West route should stop and not proceed through the intersection. EWG allows vehicles to pass from East to West and vice versa, and implies the vehicles on North/South route should stop and not proceed through the intersection. At each time step t, an agent regarding its strategy chooses an action at \u2208 A. Depending the selected action, the vehicles on each lane are allowed to cross the intersection."}, {"heading": "4.3 Reward Function", "text": "Typically an immediate reward rt \u2208 R is a scalar value which the agent receives after taking the chosen action in the environment at each time step. We set the reward as the difference between the total cumulative delays of two consecutive actions, i.e.\nrt = Dt\u22121 \u2212Dt, (2) where Dt and Dt\u22121 are the total cumulative delays in the current and previous time steps. The total cumulative delay at time t, is the summation of the cumulative delay of all the vehicles appeared from t = 0 to current time step t in the system. The positive reward values imply the taken actions led to decrease the total cumulative delay and the negative rewards imply an increase in the delay. With regard to the reward values, the agent may decide to change its policy in certain states of the system in the future."}, {"heading": "4.4 Agent\u2019s Policy", "text": "The agent chooses the actions based on a policy \u03c0. In the policy-based algorithm, the policy is defined as a mapping from the input state to a probability distribution over actions A. We use the deep neural network as the function approximator and refer its parameters \u03b8 as policy parameters. The policy distribution \u03c0(at|st; \u03b8) is learned by performing gradient descent on the policy parameters. In the value-function based algorithm, the deep neural network is utilized to estimate the action-value function. The actionvalue function maps the input state to action values, which each represents the future reward that can be achieved for the given state and action. The optimal policy can then be extracted by performing a greedy approach to select the best possible action."}, {"heading": "4.5 Objective Function and System Training", "text": "There are many measures such as maximizing throughput, minimizing and balancing queue length, minimizing the\nAlgorithm 1 Deep Value-Function based reinforcement learning agent of traffic signal control with experience replay\n1: Initialize parameters, \u03b8 with random values 2: Initialize replay memory M with capacity L 3: for each simulation do 4: initialize s with current view of the intersection 5: repeat # each step in the simulation 6: choose action a according to -greedy policy 7: take action a, observe reward r and next state s\u2032 8: store transition (s, a, r, s\u2032) in M 9: s\u2190 s\u2032\n10: b \u2190 sample random minbatch of transitions from the replay memory, M 11: for each transition (sj , aj , rj , s \u2032 j) in b do 12: if s\u2032j is terminal then 13: yi \u2190 rj 14: else 15: yj = rj + \u03b3maxa\u2032Q(s \u2032 j , a \u2032; \u03b8\u2212i\u22121) 16: end if 17: update parameters \u03b8 according to equation (7) 18: end for 19: until s is terminal 20: end for\ndelay, etc. in the traffic signal management literature to consider as the learning agent\u2019s objective function. In this research, the agent aims to maximize the reduction in the total cumulative delay, which empirically has been shown to maximize throughput and to reduce queue length (more details discussed in Section 5.3).\nThe objective of agent is to maximize the expected cumulative discounted reward. We aim to maximize the reward under the probability distribution \u03c0(at|st; \u03b8):\nJ(\u03b8) = E\u03c0\u03b8 [ T\u2211 t=0 \u03b3trt] = E\u03c0\u03b8 [R]. (3)\nWe divide the system training based two RL approaches: Value-function based and Policy-based. in value-function based approach, the value function, Q\u03c0(s, a) is defined as follows:\nQ\u03c0(s, a) = E\u03c0[rt + \u03b3maxa\u2032Q(s \u2032, a\u2032)|s, a] (4)\nWhere it is implicit that s, s\u2032 \u2208 S and a \u2208 A. the value function can be parameterized, Q(s, s; \u03b8) with parameter vector \u03b8. Typically, the gradient-descent methods are used to learn parameters, \u03b8 by trying to minimize the following loss function of mean-squared error in Q values:\nJ(\u03b8) = E\u03c0[(r + \u03b3maxa\u2032Q(s \u2032, a\u2032; \u03b8)\u2212Q(s, a; \u03b8))2] (5)\nWhere r + \u03b3maxa\u2032Q(s \u2032, a\u2032; \u03b8) is the target value. In the DQN algorithm, a target Q-network is used to address the instability problem of the policy. The network is trained with the target Q-network to obtain consistent Q-learning targets by keeping the weight parameters (\u03b8\u2212) used in the Qlearning target fixed and updating them periodically every N steps through the parameters of the main network, \u03b8. The target value of the DQN is represented as follows:\nyi = r + \u03b3maxa\u2032Q(s \u2032, a\u2032; \u03b8\u2212i\u22121) (6)\nWhere \u03b8\u2212 is parameters of the target network. The stochastic gradient descent method is used in order to optimize equation (5). The parameters of the deep Q-learning algorithm are updated as follows:\n\u03b8i \u2190 \u03b8i\u22121 + \u03b1(yi \u2212Q(s, a; \u03b8i))5\u03b8i Q(s, a; \u03b8i) (7)\nWhere yi is the target value for iteration i and \u03b1 is a scalar learning rate. Algorithm 4.4 presents the pseudo-code for the training algorithm.\nIn policy-based approach, The gradient of the objective function represented in equation (3) is given by:\n5\u03b8 J = T\u2211 t=0 E\u03c0\u03b8 [5\u03b8log(at|st; \u03b8)Rt]. (8)\nThis equation (8) is standard learning rule of the REINFORCE algorithm [41]. It updates the policy parameters \u03b8 in the direction 5\u03b8log(at|st; \u03b8) so that the probability of action at at state st is increased if it has led to high cumulative reward, however it is decreased if the action has result in a low reward. The gradient estimate in equation 2 results to have high variance. It is common to reduce the variance by subtracting a baseline function bt(st) from the return Rt, without changing expectation. Commonly an estimate of the state value function is used as the baseline, bt(st) = V\n\u03c0\u03b8v (st). Thus, the adjusted gradient is 5\u03b8log(at|st; \u03b8)(Rt \u2212 bt(st)). The value Rt \u2212 bt is known as the advantage function.\nWith regard to the advantage actor-critic method [25], computing a single update is done by selecting actions using the underlying policy for up to M steps or till a terminal state is met. In this way, the agent obtains up to M rewards from the environment at each update point and updates the policy parameters after every n \u2264M steps regarding n-step returns. The vector parameters \u03b8 is updated through the stochastic gradient descent method:\n\u03b8 \u2190 \u03b8 + \u03b1 \u2211 t 5\u03b8log(at|st; \u03b8)A(st, at; \u03b8, \u03b8v), (9)\nwhere A(st, at; \u03b8, \u03b8v) is an estimate of the advantage function corresponding \u2211n\u22121 i=0 \u03b3 irt+i + \u03b3 nV (st+n; \u03b8)\u2212 V (st; \u03b8v), where n might have different values with respect to the state, up to M . this process is an actor-critic algorithm, the policy\n\u03c0(at|st; \u03b8) refers to the actor and the estimate of the state value function V \u03c0\u03b8v (st) implies to the critic [9, 25]. Algorithm 4.5 shows the pseudo-code for the training algorithm.\nAlgorithm 2 Deep Policy-Gradient based reinforcement learning agent of traffic signal control\n1: Initialize parameters, \u03b8, \u03b8v with random values 2: Initialize step counter t\u2190 0 3: for each simulation do 4: initialize s with current view of the intersection 5: tstart = t 6: repeat 7: perform action a according to policy \u03c0(a|s; \u03b8) 8: observe reward r and next state s\u2032\n9: t\u2190 t+ 1 10: until s is terminal or t\u2212 tstart == M (Max step) 11: if s is terminal then 12: R = 0 13: else 14: R = V (s; \u03b8v) 15: end if 16: for i \u2208 {t\u2212 1, ..., tstart} do # n <= M times 17: R\u2190 ri + \u03b3R 18: \u03b8 \u2190 \u03b8 + \u03b15\u03b8 log(ai|si; \u03b8)(R\u2212 V (si; \u03b8v)) 19: \u03b8v \u2190 \u03b8v + \u2202(R\u2212V (si;\u03b8v)) 2 \u2202\u03b8v 20: end for 21: end for"}, {"heading": "5. EXPERIMENT AND RESULTS", "text": "In this section, we present the simulation environment, where our experiments have been done. We then describe the details of the deep neural network utilised, including hyper-parameters to represent the agent\u2019s policy."}, {"heading": "5.1 Experiment Setup", "text": "We have used the Simulation of Urban MObility (SUMO) [18] tool to simulate traffic in all experiments. SUMO is a well-known open source traffic simulator which provides useful Application Programming Interfaces (APIs) and a Graphical User Interface (GUI) view to model large road networks as well as some possibilities to handle them. In particular, we utilised SUMO-GUI v0.28.0. as it allows to have snapshots of each step of the simulation. The intersection geometry used in this study is shown in Figure 2. There are 4 incoming lanes to the intersection and four outgoing lanes from the intersection. To generate traffic demands from different directions (i.e. north-to-south and west-to-east and vice versa) to the road network, a uniform probability distribution with the probability 0.1 was used."}, {"heading": "5.2 System Architecture and Hyperparameters", "text": "We took the snapshots from the SUMO-GUI and did some basic pre-processing. The snapshots are converted from redgreen-blue (RGB) representation to gray-scale and resized them to 128\u00d7 128 frames. To enable our system to memorize a history of the past observations, we stacked the last four frames of the history and provided them to the system as input. So, the input to the network was a 128\u00d7 128\u00d7 4 image. We applied approximately the same architecture of the Deep Q-Network (DQN) algorithm introduced by Mnih\nTable 1: Comparison of performance of the proposed methods against the SNN method\nEvaluation Metric (\u00b5, \u03c3; n = 100) Policy-based Value-function based SNN Queue (Vehicles) (1.79, 0.073) (1.74, 0.10) (5.55, 0.73) Cumulative Delay (s) (11.25, 0.39) (11.01, 0.69) (41.40, 7.31) Reward (r) (6.14, 0.29) (7.14, 0.44) (1.73, 0.62)\n0 100 200 300 400 500 600 700 800 900 1000 \u22124\n\u22122\n0\n2\n4\n6\n8\n10\nTraining Epochs\nA ve\nra ge\nR ew\nar d\npe r\nS im\nul at\nio n\nDeep Policy Gradient Deep Value\u2212function Based Baseline\nFigure 3: A comparison of performance of the average reward received during the evaluation time for the proposed method and the baseline.\net al. [26, 27]. The network consists a stack of two convolutional layers with filters 16 8\u00d7 8 and 32 4\u00d7 4 and strides 4 and 2, respectively. The final hidden layer is fully-connected with 256 hidden node. All three hidden layers are followed by a rectifier nonlinearity. The main difference with the network architecture of the DQN method is the last layer, where the last layer of DQN is a fully-connected linear layer with a number of output neurons (i.e. Q-values Q(a, s)) corresponding to each action in a given Atari 2600 game, while in policy-based model the last layer represents two set of outputs, a softmax output resulting in a probability distribution over the actions A (i.e. the policy \u03c0(a, s)), and a single linear output node resulting in the estimate of the state value function V (s). For value-function model we used the architecture, the same as the DQN. The output layer is corresponding to action values. In all of our experiments, the discount factor was set to \u03b3 = 0.99 and all weights of the network were updated by the Adam optimizer [17] with a learning rate \u03b1 = 0.00001 and with mini batches of size M (up to 32), the maximum number of steps that the agent can take to follow its policy and afterwards needs to update it. The network was trained for about 1050 epoch, approximately 2 million time steps. Each epoch is corresponded 10 episodes and each episode was a complete SUMO-GUI simulation. The learned policies by the agent was evaluated every 10 episodes by running SUMO-GUI for 5 episodes and averaging the resulting rewards, total cumulative delay and queue length.\nTo evaluate our proposed method we also built a shallow neural network (SNN) with one hidden layer. The hidden layer has 64 hidden nodes followed by a rectifier nonlinearity. The output layer is a fully-connected linear layer with\n0 100 200 300 400 500 600 700 800 900 1000 0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nTraining Epochs A\nve ra\nge C\num ul\nat iv\ne D\nel ay\np er\nS im\nul at\nio n\n(s )\nDeep Policy Gradient Deep Value\u2212function Based Baseline\nFigure 4: Average Cumulative delay per vehicle using the suggested model and the baseline during the evaluation time.\na number of output neurons corresponding to each traffic signal phase in the intersection. Two vectors are used as input state of the network. The first representing the number of queued vehicles at the lanes of the intersection (i.e. North, South, East and West) and the second corresponding to the current traffic signal phase of the intersection. SNN is trained with the same hyper-parameters and optimization method (i.e. the gradient decent algorithm) as the proposed methods."}, {"heading": "5.3 Results and Discussion", "text": "To evaluate the performance of the proposed methods, we compared them against a baseline traffic controller, a controller that gives an equal fixed time to each phase of the intersection. We ran SUMO-GUI simulator for the proposed model using the configuration setting explained in Section 5.2 and compared the average reward, average total cumulative delay and average queue length achieved to the baseline. Figure 3 shows the received average reward while the agent follows a certain policy. As shown in Figure 3, the proposed method performs significantly better than the baseline and results more reward magnitudes by doing more epochs. This gradually increasing reward reflects the agent\u2019s ability to learn an optimal control policy in a stable manner. Unlike using deep reinforcement learning for estimating the Q-values in traffic light optimisation problem [38], the proposed agent doesn\u2019t suffer stability issues. In order to assess the learned policy by the agent, two of the most common performance metrics in the traffic signal control literature is implemented: the cumulative delay and queue length. Figures 4 and 5 illustrate the performance comparison of the leaning agent regarding average cumulative delay time and\naverage queue length metrics, respectively, to the baseline, while the agent is following the learning policy over time. The plots clearly show the agent is able to find a policy resulting minimizing queue length and total cumulative delay. Moreover, these graphs reveal that by using the reward function for reducing cumulative delay, the intersection queue length is reduced as well as the total cumulative delay of all vehicles.\nWe also compared the proposed methods with the SNN, which is a shallow neural network with one hidden layer. Table 1 reports a comparison of the proposed models and the SNN model in terms of the average and standard deviation (\u00b5, \u03c3) of average queue length, average cumulative delay time and the received average reward metrics. The results on Table 1 are calculated from the last 100 training epochs of each method. Comparing the metrics shown in Table 1, demonstrates that the proposed models significantly outperform the SNN method. Based on the data in Table 1 we can induce 67% and 72% reductions in the average cumulative delay and queue length for the policy gradient method and 68% and 73% reductions for value-function based method compared to the SNN. Furthermore, we can see that the proposed methods have received average rewards superior to the SNN. Considering these results, it is obvious that the policy gradient and value-function agents could learn the control policies better than the SNN approach."}, {"heading": "6. CONCLUSION", "text": "In this paper, we applied deep reinforcement learning algorithms with focusing on both policy and value-function based methods to traffic signal control problem in order to find optimal control policies of signalling, just by using raw visual input data of the traffic simulator snapshots. Our approaches have led to promising results and showed they could find more stable control policies compared to previous work of using deep reinforcement learning in traffic light optimization. In our work, we developed and tested the proposed methods in a small application, extending the work for more complex traffic simulations, for instance consider-\ning many intersections and multiple agents to control each intersection, using multi-agent learning techniques to handle coordination problem between agents would be a direction for future research."}, {"heading": "Acknowledgments", "text": "We would like to thank FotoNation company for letting us to work with their GPU cluster."}], "references": [{"title": "Holonic multi-agent system for traffic signals control", "author": ["M. Abdoos", "N. Mozayani", "A.L. Bazzan"], "venue": "Engineering Applications of Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Reinforcement learning for true adaptive traffic signal control", "author": ["B. Abdulhai", "R. Pringle", "G.J. Karakoulas"], "venue": "Journal of Transportation Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Reinforcement learning-based multi-agent system for network traffic signal control", "author": ["I. Arel", "C. Liu", "T. Urbanik", "A. Kohls"], "venue": "IET Intelligent Transport Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Urban traffic signal control using reinforcement learning agents", "author": ["P. Balaji", "X. German", "D. Srinivasan"], "venue": "IET Intelligent Transport Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Experiments with infinite-horizon, policy-gradient estimation", "author": ["J. Baxter", "P.L. Bartlett", "L. Weaver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Optimizing traffic lights in a cellular automaton model for city traffic", "author": ["E. Brockfeld", "R. Barlovic", "A. Schadschneider", "M. Schreckenberg"], "venue": "Physical Review E,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Q-learning based traffic optimization in management of signal timing plan", "author": ["Y.K. Chin", "N. Bolong", "A. Kiring", "S.S. Yang", "K.T.K. Teo"], "venue": "International Journal of Simulation, Systems, Science & Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Model-free reinforcement learning with continuous action in practice", "author": ["T. Degris", "P.M. Pilarski", "R.S. Sutton"], "venue": "In American Control Conference (ACC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "An autonomous network aware vm migration strategy in cloud data centres", "author": ["M. Duggan", "J. Duggan", "E. Howley", "E. Barrett"], "venue": "In Cloud and Autonomic Computing (ICCAC),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A reinforcement learning approach for dynamic selection of virtual machines in cloud data centres", "author": ["M. Duggan", "K. Flesk", "J. Duggan", "E. Howley", "E. Barrett"], "venue": "In Sixth International Conference on Innovating Computing Technology", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Multiagent reinforcement learning for integrated  network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto", "author": ["S. El-Tantawy", "B. Abdulhai", "H. Abdelgawad"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Using a deep reinforcement learning agent for traffic signal control", "author": ["W. Genders", "S. Razavi"], "venue": "arXiv preprint arXiv:1611.01142,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Enhancing nash q-learning and team q-learning mechanisms by using bottlenecks", "author": ["B. Ghazanfari", "N. Mozayani"], "venue": "Journal of Intelligent & Fuzzy Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Extracting bottlenecks for reinforcement learning agent by holonic concept clustering and attentional functions", "author": ["B. Ghazanfari", "N. Mozayani"], "venue": "Expert Systems with Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Recent development and applications of sumo-simulation of urban mobility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal On Advances in Systems and Measurements,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep auto-encoder neural networks in reinforcement learning", "author": ["S. Lange", "M. Riedmiller"], "venue": "In Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Traffic signal timing via deep reinforcement learning", "author": ["L. Li", "Y. Lv", "F.-Y. Wang"], "venue": "IEEE/CAA Journal of Automatica Sinica,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Parallel systems for traffic control: A rethinking", "author": ["L. Li", "D. Wen"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "A survey of traffic control with vehicular communications", "author": ["L. Li", "D. Wen", "D. Yao"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.-J. Lin"], "venue": "PhD thesis, Fujitsu Laboratories Ltd,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1993}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Automatic abstraction controller in reinforcement learning agent via automata", "author": ["S.S. Mousavi", "B. Ghazanfari", "N. Mozayani", "M.R. Jahed-Motlagh"], "venue": "Applied Soft Computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Deep reinforcement learning: An overview", "author": ["S.S. Mousavi", "M. Schukat", "E. Howley"], "venue": "Intelligent Systems Conference,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Reinforcement learning with function approximation for traffic signal control", "author": ["L. Prashanth", "S. Bhatnagar"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Traffic light scheduling using policy-gradient reinforcement learning", "author": ["S. Ritcher"], "venue": "In The International Conference on Automated Planning and Scheduling.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["J. Schulman", "N. Heess", "T. Weber", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "Traffic light control using sarsa with three state representations", "author": ["T.L. Thorpe", "C.W. Anderson"], "venue": "Technical report, Citeseer,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1996}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Coordinated deep reinforcement learners for traffic light control", "author": ["E. Van der Pol", "F.A. Oliehoek"], "venue": "In NIPS\u201916 Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Multi-agent reinforcement learning for traffic light control", "author": ["M. Wiering"], "venue": "In ICML,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2000}, {"title": "Recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "Logic Journal of IGPL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1992}, {"title": "Reinforcement learning algorithms with function approximation: Recent advances and applications", "author": ["X. Xu", "L. Zuo", "Z. Huang"], "venue": "Information Sciences,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 3, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 0, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 21, "context": "To address this issue, a lot of solutions have been proposed [23, 4, 1, 22].", "startOffset": 61, "endOffset": 75}, {"referenceID": 12, "context": "traffic demand, queue length of vehicles in each lane of the intersection and traffic flow fluctuation) [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 25, "context": "In this study, we are interested in the third approach and aim to propose two novel methods for traffic signal control by leveraging recent advances in machine learning and artificial intelligence fields [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 26, "context": "In this study, we are interested in the third approach and aim to propose two novel methods for traffic signal control by leveraging recent advances in machine learning and artificial intelligence fields [26, 27].", "startOffset": 204, "endOffset": 212}, {"referenceID": 33, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 125, "endOffset": 132}, {"referenceID": 29, "context": "Reinforcement learning [34] as a machine learning technique for traffic signal control problem has led to impressive results [4, 30] and has shown a promising potential solver.", "startOffset": 125, "endOffset": 132}, {"referenceID": 11, "context": "Aside from traffic control, reinforcement learning has been applied to a number of real world problems such as cloud computing [12, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 10, "context": "Aside from traffic control, reinforcement learning has been applied to a number of real world problems such as cloud computing [12, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 26, "context": "Recently, deep learning has gained huge attraction and has been successfully combined with reinforcement learning techniques to deal with complex optimization problems such as playing Atari 2600 games [27], Computer Go program [33], etc.", "startOffset": 201, "endOffset": 205}, {"referenceID": 32, "context": "Recently, deep learning has gained huge attraction and has been successfully combined with reinforcement learning techniques to deal with complex optimization problems such as playing Atari 2600 games [27], Computer Go program [33], etc.", "startOffset": 227, "endOffset": 231}, {"referenceID": 19, "context": "a convolutional neural network [20]) trained by reinforcement learning techniques to predict the next possible optimal action(s).", "startOffset": 31, "endOffset": 35}, {"referenceID": 37, "context": "learning in the traffic signal control problem [38, 14], in this research the state representation is different.", "startOffset": 47, "endOffset": 55}, {"referenceID": 13, "context": "learning in the traffic signal control problem [38, 14], in this research the state representation is different.", "startOffset": 47, "endOffset": 55}, {"referenceID": 38, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 1, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 6, "context": "In particular, significant research has been conducted employing reinforcement learning methods in the area of traffic light signal control [39, 2, 7].", "startOffset": 140, "endOffset": 150}, {"referenceID": 12, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 0, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 7, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 2, "context": "Developing advance traffic simulation tools have made researchers develop novel state representation and reward functions for reinforcement learning algorithms, which could consider more aspects of complexity and reality of real-world traffic problems [13, 1, 8, 3].", "startOffset": 252, "endOffset": 265}, {"referenceID": 30, "context": "However, Richter\u2019s study formulated the traffic problem as a partially observable MDP (POMDP) and applied policy gradient methods to guarantee local convergence under a partial observable environment [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "By utilizing advances in deep learning and its application to different domains [10, 11], deep learning has gained attention in the area of traffic management systems.", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "By utilizing advances in deep learning and its application to different domains [10, 11], deep learning has gained attention in the area of traffic management systems.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "Previous research has used deep stacked autoencoders (SAE) neural networks to estimate Q-values, where each Q-value is corresponding to each available signal phase [21].", "startOffset": 164, "endOffset": 168}, {"referenceID": 37, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 22, "endOffset": 30}, {"referenceID": 13, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 22, "endOffset": 30}, {"referenceID": 26, "context": "Two recent studies by [38, 14] provided deep reinforcement learning agents that used deep Q-netwok [27] to map from given states to Q-values.", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "A common reinforcement learning [34] setting is shown in Figure 1 where an RL agent interacts with an environment.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "A MDP is defined as a five-tuple < S,A, T,R, \u03b3 > , where S is the set of states in the state space of the environment, A is the set of actions in the action space that the agent can use in order to interact with the environment, T is the transition function, which is the probability of moving between the environment states, R is the reward function and \u03b3 \u2208 [0, 1] is known as the discount factor, which models the importance of the future and immediate rewards.", "startOffset": 359, "endOffset": 365}, {"referenceID": 0, "context": "The goal of the learning agent in reinforcement learning framework is to learn an optimal policy \u03c0 : S \u00d7 A \u2192 [0, 1] which defines the probability of selecting action at in state st, so that with following the underlying policy the expected cumulative discounted reward over time is maximized.", "startOffset": 109, "endOffset": 115}, {"referenceID": 41, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 14, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 27, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 157, "endOffset": 168}, {"referenceID": 15, "context": "Hence, it is common to use function approximators [42] or decomposition and aggregation techniques like Hierarchical Reinforcement Learning (HRL) approaches [5, 15, 28] and advance HRL [16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": "have also been commonly used as function approximators for large reinforcement learning tasks [19, 26].", "startOffset": 94, "endOffset": 102}, {"referenceID": 25, "context": "have also been commonly used as function approximators for large reinforcement learning tasks [19, 26].", "startOffset": 94, "endOffset": 102}, {"referenceID": 28, "context": "The interested readers are referred to [29] for a review of using deep neural networks", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "A deep Q-learning Network (DQN) [26] uses this benefit of deep learning in order to represent the agent\u2019s observation as an abstract representation in learning an optimal control policy.", "startOffset": 32, "endOffset": 36}, {"referenceID": 36, "context": "Applying non-linear function approximators such as neural networks with modelfree reinforcement learning algorithms in high-dimensional continuous state and action spaces, has some convergence problems [37].", "startOffset": 202, "endOffset": 206}, {"referenceID": 23, "context": "For the problem of correlated states, DQN uses the previously proposed experience replay approach [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 34, "context": "This approach has demonstrated better convergence properties in some RL problems [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 34, "context": "They can also deal with the complexity of continuous state and action spaces better than purely value-based methods [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Policy gradient methods estimate policy gradients using Monte Carlo estimates of the policy gradients [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 31, "context": "The baseline function can be calculated in different manners [32, 40].", "startOffset": 61, "endOffset": 69}, {"referenceID": 39, "context": "The baseline function can be calculated in different manners [32, 40].", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "SUMO-GUI [18]) which is a vector of row pixel values of current view of the intersection at each step of simulation (as shown in Figure 1).", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 272, "endOffset": 280}, {"referenceID": 35, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 272, "endOffset": 280}, {"referenceID": 13, "context": "The state representation in the traffic light control literature usually uses a vector representing the presence of a vehicle at the intersection, a Boolean-valued vector where a value 1 indicates the presence of a vehicle and a value 0 indicates the absence of a vehicle [38, 36], or a combination of the presence vector with another vector indicating the vehicle\u2019s speed at the given intersection [14].", "startOffset": 399, "endOffset": 403}, {"referenceID": 40, "context": "This equation (8) is standard learning rule of the REINFORCE algorithm [41].", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "With regard to the advantage actor-critic method [25], computing a single update is done by selecting actions using the underlying policy for up to M steps or till a terminal state is met.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "this process is an actor-critic algorithm, the policy \u03c0(at|st; \u03b8) refers to the actor and the estimate of the state value function V \u03c0\u03b8v (st) implies to the critic [9, 25].", "startOffset": 164, "endOffset": 171}, {"referenceID": 24, "context": "this process is an actor-critic algorithm, the policy \u03c0(at|st; \u03b8) refers to the actor and the estimate of the state value function V \u03c0\u03b8v (st) implies to the critic [9, 25].", "startOffset": 164, "endOffset": 171}, {"referenceID": 17, "context": "We have used the Simulation of Urban MObility (SUMO) [18] tool to simulate traffic in all experiments.", "startOffset": 53, "endOffset": 57}, {"referenceID": 25, "context": "[26, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[26, 27].", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "99 and all weights of the network were updated by the Adam optimizer [17] with a learning rate \u03b1 = 0.", "startOffset": 69, "endOffset": 73}, {"referenceID": 37, "context": "Unlike using deep reinforcement learning for estimating the Q-values in traffic light optimisation problem [38], the proposed agent doesn\u2019t suffer stability issues.", "startOffset": 107, "endOffset": 111}], "year": 2017, "abstractText": "Recent advances in combining deep neural network architectures with reinforcement learning techniques have shown promising potential results in solving complex control problems with high dimensional state and action spaces. Inspired by these successes, in this paper, we build two kinds of reinforcement learning algorithms: deep policy-gradient and value-function based agents which can predict the best possible traffic signal for a traffic intersection. At each time step, these adaptive traffic light control agents receive a snapshot of the current state of a graphical traffic simulator and produce control signals. The policy-gradient based agent maps its observation directly to the control signal, however the value-function based agent first estimates values for all legal control signals. The agent then selects the optimal control action with the highest value. Our methods show promising results in a traffic network simulated in the SUMO traffic simulator, without suffering from instability issues during the training process.", "creator": "LaTeX with hyperref package"}}}