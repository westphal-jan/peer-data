{"id": "1705.00253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2017", "title": "Multi-dueling Bandits with Dependent Arms", "abstract": "The dueling bandits problem is an online learning framework for learning from pairwise preference feedback, and is particularly well-suited for modeling settings that elicit subjective or implicit human feedback. In this paper, we study the problem of multi-dueling bandits with dependent arms, which extends the original dueling bandits setting by simultaneously dueling multiple arms as well as modeling dependencies between arms. These extensions capture key characteristics found in many real-world applications, and allow for the opportunity to develop significantly more efficient algorithms than were possible in the original setting. We propose the \\selfsparring algorithm, which reduces the multi-dueling bandits problem to a conventional bandit setting that can be solved using a stochastic bandit algorithm such as Thompson Sampling, and can naturally model dependencies using a Gaussian process prior. We present a no-regret analysis for multi-dueling setting, and demonstrate the effectiveness of our algorithm empirically on a wide range of simulation settings.\n\n\n\n\n\n\nWe suggest that this approach is compatible with the approach described above.", "histories": [["v1", "Sat, 29 Apr 2017 23:47:52 GMT  (3695kb)", "http://arxiv.org/abs/1705.00253v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yanan sui", "vincent zhuang", "joel w burdick", "yisong yue"], "accepted": false, "id": "1705.00253"}, "pdf": {"name": "1705.00253.pdf", "metadata": {"source": "CRF", "title": "Multi-dueling Bandits with Dependent Arms", "authors": ["Yanan Sui"], "emails": ["ysui@caltech.edu", "vzhuang@caltech.edu", "jwb@robotics.caltech.edu", "yyue@caltech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n00 25\n3v 1\n[ cs\n.L G\n] 2\n9 A\npr 2\nThe dueling bandits problem is an online learning framework for learning from pairwise preference feedback, and is particularly wellsuited for modeling settings that elicit subjective or implicit human feedback. In this paper, we study the problem of multi-dueling bandits with dependent arms, which extends the original dueling bandits setting by simultaneously dueling multiple arms as well as modeling dependencies between arms. These extensions capture key characteristics found in many realworld applications, and allow for the opportunity to develop significantly more efficient algorithms than were possible in the original setting. We propose the SELFSPARRING algorithm, which reduces the multi-dueling bandits problem to a conventional bandit setting that can be solved using a stochastic bandit algorithm such as Thompson Sampling, and can naturally model dependencies using a Gaussian process prior. We present a no-regret analysis for multi-dueling setting, and demonstrate the effectiveness of our algorithm empirically on a wide range of simulation settings."}, {"heading": "1 Introduction", "text": "In many online learning settings, particularly those that involve human feedback, reliable feedback is often limited to pairwise preferences (e.g., \u201cis A better than B?\u201d). Examples include implicit or subjective feedback for information retrieval and various recommender systems (Chapelle et al., 2012; Sui & Burdick, 2014). This setup motivates the dueling bandits problem (Yue et al., 2012), which formalizes the problem of online regret minimization via preference feedback.\nThe original dueling bandits setting ignores many real world considerations. For instance, in personalized clinical recommendation settings (Sui & Burdick, 2014), it is often more practical for subjects to provide preference feedback on several actions (or treatments) simultaneously rather than just two. Furthermore, the action space can be very large, possibly infinite, but often has a lowdimensional dependency structure.\nIn this paper, we address both of these challenges in a unified framework, which we call multi-dueling bandits with dependent arms. We extend the original dueling bandits problem by simultaneously dueling multiple arms as well as modeling dependencies between arms using a kernel. Explicitly formalizing these real-world characteristics provides an opportunity to develop principled algorithms that are much more efficient than algorithms designed for the original setting. For instance, most dueling bandits algorithms suffer regret that scales linearly with the number of arms, which is not practical when the number of arms is very large or infinite.\nFor this setting, we propose the SELFSPARRING algorithm, inspired by the Sparring algorithm from Ailon et al. (2014), which algorithmically reduces the multi-dueling bandits problem into a conventional muilti-armed bandit problem that can be solved using a stochastic bandit algorithm such as Thompson Sampling (Chapelle & Li, 2011; Russo & Van Roy, 2014). Our approach can naturally incorporate dependencies using a Gaussian process prior with an appropriate kernel.\nWhile there have been some prior work on multi-dueling (Brost et al., 2016) and learning from pairwise preferences over kernels (Gonzalez et al., 2016), to the best of our knowledge, our approach is the first to address to both in a unified framework. We are also the first to provide a regret analysis of the multi-dueling setting. We further demonstrate the effectiveness of our approach over conventional dueling bandits approaches in a wide range of simulation experiments."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Dueling Bandits", "text": "The original dueling bandits problem is a sequential optimization problem with relative feedback. Let B = {b1, . . . , bK} be the set of K bandits (or arms). At each iteration, the algorithm duels or compares a single pair of arms bi, bj from the set of K arms (bi and bj can be identical). The outcome of each duel between bi and bj is an independent sample of a Bernoulli random variable. We define the probability that arm bi beats bj as:\nP (bi \u227b bj) = \u03c6(bi, bj) + 1/2,\nwhere \u03c6(bi, bj) \u2208 [\u22121/2, 1/2] denotes the stochastic preference between bi and bj , thus bi \u227b bj \u21d4 \u03c6(bi, bj) > 0. We assume there is a total ordering, and WLOG that bi \u227b bj \u21d4 i < j.\nThe setting proceeds in a sequence of iterations or rounds. At each iteration t, the decision maker must choose a pair of bandits b\n(1) t and b (2) t to compare, and\nobserves the outcome of that comparison. The quality of the decision making is then quantified using a notion of cumulative regret of T iterations:\nRT = T \u2211\nt=1\n[\n\u03c6(b1, b (1) t ) + \u03c6(b1, b (2) t )\n]\n. (1)\nWhen the algorithm has converged to the best arm b1, then it can simply duel b1 against itself, thus incurring no additional regret. In the recommender systems setting, one can interpret (1) as the how much the user(s) would have preferred the best bandit over the the ones presented by the algorithm.\nTo date, there have been several algorithms proposed for the stochastic dueling bandits problem, including Interleaved Filter (Yue et al., 2012), Beat the Mean (Yue & Joachims, 2011), SAVAGE (Urvoy et al., 2013), RUCB (Zoghi et al., 2014, 2015b), Sparring (Ailon et al., 2014; Dud\u0131\u0301k et al., 2015), RMED (Komiyama et al., 2015), and DTS (Wu & Liu, 2016). Our proposed approach, SELFSPARRING, is inspired by Sparring, which along with RUCB-style algorithms are the best performing methods. In contrast to Sparring, which has no theoretical guarantees, we provide noregret guarantees for SELFSPARRING, and demonstrate significantly better performance in the multi-dueling setting.\nPrevious work on extending the original dueling bandits setting have been largely restricted to settings that duel a single pair of arms at a time. These include continuousarmed convex dueling bandits (Yue & Joachims, 2009),\ncontextual dueling bandits which also introduces the von Neumann winner solution concept (Dud\u0131\u0301k et al., 2015), sparse dueling bandits that focuses on the Borda winner solution concept (Jamieson et al., 2015), Copeland dueling bandits that focuses on the Copeland winner solution concept (Zoghi et al., 2015a), and adversarial dueling bandits (Gajane et al., 2015). In contrast, our work studies the complementary directions of how to formalize multiple duels simultaneously, as well as how to reduce the dimensionality of modeling the action space using a low-dimensional similarity kernel.\nRecently, there have been increasing interest in studying personalization settings that simultaneously elicit multiple pairwise comparisons. Example settings include information retrieval (Hofmann et al., 2011; Schuth et al., 2014, 2016) and clinical treatment (Sui & Burdick, 2014). There have also been some previous work on multi-dueling bandits settings (Brost et al., 2016; Sui & Burdick, 2014; Schuth et al., 2016), however the previous approaches are limited in their scope and lack rigorous theoretical guarantees. In contrast, our approach can handle a wide range of multi-dueling mechanisms, has near-optimal regret guarantees, and can be easily composed with kernels to model dependent arms."}, {"heading": "2.2 Multi-armed Bandits", "text": "Our proposed algorithm, SELFSPARRING, utilizes a multi-armed bandit (MAB) algorithm as a subroutine, and so we provide here a brief formal description of the conventional MAB problem for completeness. The stochastic MAB problem (Robbins, 1952) refers to an iterative decision making problem where the algorithm repeatedly chooses among K actions (or bandits or arms). In contrast to the dueling bandits setting, where the feedback is relative between two arms, here, we receive an absolute reward that depends on the arm selected. We assume WLOG that every reward is bounded between [0, 1].1 The goal then is to minimize the cumulative regret compared to the best arm:\nRMABT =\nT \u2211\nt=1\n[ \u00b51 \u2212 \u00b5(bt) ] , (2)\nwhere bt denotes the arm chosen at time t, \u00b5(b) denotes the expected reward of arm b, and \u00b51 = argmaxb \u00b5(b). Popular algorithms for the stochastic setting include UCB (upper confidence bound) algorithms (Auer et al., 2002a), and Thompson Sampling (Chapelle & Li, 2011; Russo & Van Roy, 2014).\nIn the adversarial setting, the rewards are chosen in an\n1So long as the rewards are bounded, one can shift and rescale them to fit within [0, 1].\nAlgorithm 1 Thompson Sampling for Bernoulli Bandits"}, {"heading": "1: For each arm i = 1, 2, \u00b7 \u00b7 \u00b7 ,K , set Si = 0, Fi = 0.", "text": "2: for t = 1, 2, . . . do 3: For each arm i = 1, 2, \u00b7 \u00b7 \u00b7 ,K , sample \u03b8i from Beta(Si + 1, Fi + 1) 4: Play arm i(t) := argmaxi \u03b8i(t), observe reward rt 5: Si \u2190 Si + rt, Fi \u2190 Fi + 1\u2212 rt 6: end for\nadversarial fashion, rather than sampled independently from some underlying distribution. In this case, regret (2) is rephrased as the difference in the sum of rewards. The predominant algorithm for the adversarial setting is EXP3 (Auer et al., 2002b)."}, {"heading": "2.3 Thompson Sampling", "text": "The specific MAB algorithm used by our SELFSPARRING approach is Thompson Sampling. Thompson Sampling is a stochastic algorithm that maintains a distribution over the arms, and chooses arms by sampling (Chapelle & Li, 2011). This distribution is updated using reward feedback. The entropy of the distribution thus corresponds to uncertainty regarding which is the best arm, and flatter distributions lead to more exploration.\nConsider the Bernoulli bandits setting where observed rewards are either 1 (win) or 0 (loss). Let Si and Fi denote the historical number of wins and losses of arm i, and letDt denote the set of all parameters at round t:\nDt = {S1, \u00b7 \u00b7 \u00b7 , SK ;F1, \u00b7 \u00b7 \u00b7 , FK}t.\nFor brevity, we often represent Dt by D, since only the current iteration matters at run-time. The sampling process of Beta-Bernoulli Thompson Sampling givenD is:\n\u2022 For each arm i, sample \u03b8i \u223c Beta(Si+1, Fi+1). \u2022 Choose the arm with maximal \u03b8i.\nIn other words, we model the average utility of each arm using a Beta prior, and rewards for arm i as Bernoulli distributed according to latent mean utility \u03b8i. As we observe more rewards, we can compute the posterior, which is also Beta distributed by conjugation between Beta and Bernoulli. The sampling process above can be shown to be sampling for the following distribution:\nP (i|D) = P (i = argmax b \u03b8b|D). (3)\nThus, any arm i is chosen with probability that it has maximal reward under the Beta posterior. Algorithm 1 describes the Beta-Bernoulli Thompson Sampling algorithm, which we use as a subroutine for our approach.\nThompson Sampling enjoys near-optimal regret guarantees in the stochasticMAB setting, as given by the lemma below (which is a direct consequence of main theorems in Agrawal & Goyal (2012); Kaufmann et al. (2012)).\nLemma 1. For the K-armed stochastic MAB problem, Thompson Sampling has expected regret: E[RMABT ] = O (\nK \u2206 lnT\n)\n, where\u2206 is the difference between expected rewards of the best two arms."}, {"heading": "2.4 Gaussian Processes & Kernels", "text": "Normally, when one observes measurements about one arm (in both dueling bandits and conventional multiarmed bandits), one cannot use that measurement to infer anything about other arms \u2013 i.e., the arms are independent. This limitation necessarily implies that regret scales linearly w.r.t. the number of arms K , since each arm must be explored at least once to collect at least one measurement about it. We will use Gaussian processes and kernels to model dependencies between arms.\nFor simplicity, we present Gaussian processes in the context of multi-armed bandits. We will describe how to apply them to multi-dueling bandits in Section 3 A Gaussian process (GP) is a probability measure over functions such that any linear restriction is multivariate Gaussian. A GP is fully determined by its mean and a positive definite covariance operator, also known as a kernel. A GP (\u00b5(b), k(b, b\u2032)) is a probability distribution across a class of \u201csmooth\u201d functions, which is parameterized by a kernel function k(b, b\u2032) that characterizes the smoothness of f . One can think of f has corresponding to the reward function in the standard MAB setting.\nWe assume WLOG that \u00b5(b) = 0, and that our observations are perturbed by i.i.d. Gaussian noise, i.e., for samples at points AT = [b1 . . . bT ], we have yt = f(bt) +nt where nt \u223c T (0, \u03c32) (e will relax this later). The posterior over f is then also Gaussian with mean \u00b5T (b), covariance kT (b, b) and variance \u03c3 2 T (b, b \u2032) that satisfy:\n\u00b5T (b) = kT (b) T (KT + \u03c3 2I)\u22121yT\nkT (b, b \u2032) = k(b, b\u2032)\u2212 kT (x) T (KT + \u03c3 2I)\u22121kT (b \u2032)\n\u03c32T (b) = kT (b, b),\nwhere kT (b) = [k(b1, b) . . . k(bT , b)] T and KT is the positive definite kernel matrix [k(x, x\u2032)]bx, b \u2032 \u2208 AT ].\nPosterior inference updates the mean reward estimates for all the arms that share dependencies (as specified by the kernel) with the arms selected formeasurement. Thus one can show that MAB algorithms using Gaussian processes have regret that scale linearly w.r.t. the dimensionality of the kernel rather than the number of arms (which can now be infinite) (Srinivas et al., 2010)."}, {"heading": "3 Multi-dueling Bandits", "text": "We now formalize the multi-dueling bandits problem. We inherit all notation from original dueling bandits setting (Section 2.1). The key difference is that the algorithm now selects a (multi-)set St of arms at each iteration t, and observes outcomes of duels between some pairs of arms in St. For example, in information retrieval this can be implemented via multi-leaving (Schuth et al., 2014) the ranked lists of the subset, St, of rankers and then inferring the relative quality of the lists (and the corresponding rankers) from user feedback.\nIn general, we assume the number of arms being dueled at each iteration is some fixed constantm = |St|. When m = 2, the problem reduces to the original dueling bandits setting. Extending the regret formulation from the original setting (1), we can write the regret as:\nRT =\nT \u2211\nt=1\n\u2211\nb\u2208St\n\u03c6(b1, b). (4)\nThe goal then is to select subsets of arms St so that the cumulative regret (4) is minimized. Intuitively, all arms have to be selected a small number of times in order to be explored, but the goal of the algorithm is to minimize the number of times when suboptimal arms are selected. When the algorithm has converged to the best arm b1, then it can simply choose St to only contain b1, thus incurring no additional regret.\nOur setting differs from Brost et al. (2016) in two ways. First, we play a fixed, rather than variable, number of arms at each iteration. Furthermore, we focus on total regret, rather than the instantaneous average regret in a single iteration; in many applications (e.g., Sui & Burdick (2014)), playing each arm incurs its own regret .\nFeedback Mechanisms. Simultaneously dueling multiple arms opens up multiple options for collecting feedback. For example, in some applications it may be viable to collect all pairwise feedback for all chosen arms St. In other applications, it is more realistic to only observe the \u201cwinner\u201d of St, in which we observe feedback that one b \u2208 St wins against all other arms in St, but nothing about pairwise preferences between the other arms.\nApproximate Linearity. One assumption that we leverage in developing our approach is approximate linearity, which fully generalizes the linear utility-based dueling bandits setting studied in Ailon et al. (2014). For any triplet of bandits bi \u227b bj \u227b bk and some constant \u03b3 > 0:\n\u03c6(bi, bk)\u2212 \u03c6(bj , bk) \u2265 \u03b3\u03c6(bi, bj). (5)\nTo understand Approximate Linearity, consider the special case when the preference function follows the form\n\u03c6(bi, bj) = \u03a6(ui \u2212 uj), where ui is a bounded utility measure of bi. Approximate linearity of \u03c6(\u00b7, \u00b7) is equivalent to having \u03a6(\u00b7) be not far from some linear function on its bounded support (see Figure 1), and is satisfied by any continuous monotonic increasing function. When \u03a6 is linear, then our setting reduces to the utility-based dueling bandits setting of Ailon et al. (2014).2"}, {"heading": "4 Algorithms & Results", "text": "We start with a high-level description of our general framework, called SELFSPARRING, which is inspired by the Sparring algorithm from Ailon et al. (2014). The high-level strategy is to reduce the multi-dueling bandits problem to a multi-armed bandit (MAB) problem that can be solved using a MAB algorithm, and ideally lift existing MAB guarantees to the multi-dueling setting.\nAlgorithm 2 describes the SELFSPARRING approach. SELFSPARRING uses a stochastic MAB algorithm such as Thompson sampling as a subroutine to independently sample the set of m arms, St to duel. The distribution of St is generally not degenerate (e.g., all the same arm) unless the algorithm has converged. In contrast, the Sparring algorithm uses m MAB algorithms to control the choice of the each arm, which essentially reduces the conventional dueling bandits problem to two multiarmed bandit problems \u201csparring\u201d against each other.\nSELFSPARRING takes as input S the total set of arms, m the number of arms to be dueled at each iteration, and \u03b7 the learning rate for posterior updates. S can be a finite set of K arms for independent setting, or a continuous action space of arms for kernelized setting. A prior\n2Compared to the assumptions of Yue et al. (2012), Approximate Linearity is a stricter requirement than strong stochastic transitivity, and is a complementary requirement to stochastic triangle inequality. In particular, stochastic triangle inequality requires that the curve in Figure 1 exhibits diminishing returns in the top-right quadrant (i.e., is sub-linear), whereas Approximate Linearity requires that the curve be not too far from linear.\nAlgorithm 2 SELFSPARRING\ninput arms 1, . . . ,K in space S, m the number of arms drawn at each iteration, \u03b7 the learning rate\n1: Set priorD0 over S 2: for t = 1, 2, . . . do 3: for j = 1, . . . ,m do 4: select arm ij(t) usingDt\u22121 5: end for 6: Playm arms {ij(t)}j and observem\u00d7m pairwise feedback matrix R = {rij \u2208 {0, 1, \u2205}}m\u00d7m 7: updateDt\u22121 using R to obtainDt 8: end for\ndistributionD0 is used to initialize the sampling process over S. In the t-th iteration, SELFSPARRING selects m arms by sampling over the distribution Dt\u22121 as shown in line 2 of Algorithm 2. The preference feedback can be any type of comparisons ranging from full comparison over them arms (a full matrix for R, aka \u2018all pairs\u201d) to single comparison of one pair (just two valid entries in R). The posterior distribution over arms Dt then gets updated by R and the priorDt\u22121.\nWe specialize SELFSPARRING in two ways. The first, INDSELFSPARRING (Algorithm 3), is the independentarmed version of SELFSPARRING. The second, KERNELSELFSPARRING (Algorithm 4), uses Gaussian processes to make predictions about preference function f based on noisy evaluations over comparisons. We emphasize here that SELFSPARRING is very modular approach, and is thus easy to implement and extend."}, {"heading": "4.1 Independent Arms Case", "text": "INDSELFSPARRING (Algorithm 3) instantiates SELFSPARRING using Beta-Bernoulli Thompson sampling. The posterior Beta distributionsDt over the arms are updated by the preference feedback within the iteration and the prior Beta distributionsDt\u22121.\nWe present a no-regret guarantee of INDSELFSPARRING in Theorem 2 below. We now provide a high-level outline of the main components leading to the result. Detail proofs are deferred to the supplementary material.\nOur first step is to prove that INDSELFSPARRING is asymptotically consistent, i.e., it is guaranteed (with high probability) to converge to the best bandit. In order to guarantee consistency, we first show that all arms are sampled infinitely often in the limit.\nLemma 2. Running INDSELFSPARRING with infinite time horizon will sample each arm infinitely often.\nIn other words, Thompson sampling style algorithms do\nAlgorithm 3 INDSELFSPARRING\ninput m the number of arms drawn at each iteration, \u03b7 the learning rate\n1: For each arm i = 1, 2, \u00b7 \u00b7 \u00b7 ,K , set Si = 0, Fi = 0. 2: for t = 1, 2, . . . do 3: for j = 1, . . . ,m do 4: For each arm i = 1, 2, \u00b7 \u00b7 \u00b7 ,K , sample \u03b8i from Beta(Si + 1, Fi + 1) 5: Select ij(t) := argmaxi \u03b8i(t) 6: end for 7: Play m arms {ij(t)}j , observe pairwise feedback matrix R = {rjk \u2208 {0, 1, \u2205}}m\u00d7m\n8: for j, k = 1, . . . ,m do 9: if rjk 6= \u2205 then 10: Sj \u2190 Sj + \u03b7 \u00b7 rjk , Fj \u2190 Fj + \u03b7(1\u2212 rjk) 11: end if 12: end for 13: end for\nnot eliminate any arms. Lemma 2 also guarantees concentration of any statistical estimates for each arm as t \u2192 \u221e. We next show that the sampling of INDSELFSPARRING will concentrate around the optimal arm.\nTheorem 1. Under Approximate Linearity, INDSELFSPARRING converges to the optimal arm b1 as running time t \u2192 \u221e: limt\u2192\u221e P(bt = b1) = 1.\nTheorem 1 implies that INDSELFSPARRING is asymptotically no-regret. As t \u2192 \u221e, the Beta distribution for each arm i is converging to P (bi \u227b b1), which implies converging to only choosing the optimal arm.\nMost existing dueling bandits algorithm chooses one arm as a \u201creference\u201d arm and the other arm as a competing arm for exploration/exploitation (in the m = 2 setting). If the distribution over reference arms never changes, then the competing arm is playing against a fixed \u201cenvironment\u201d, i.e., it is a standard MAB problem. For general m, we can analogously consider choosing only one arm against a fixed distribution over all the other arms. Using Thompson sampling, the following lemma holds.\nLemma 3. Under Approximate Linearity, selecting only one arm via Thompson sampling against a fixed distribution over the remaining arms leads to optimal regret w.r.t. choosing that arm.\nLemma 3 and Theorem 1 motivate the idea of analyzing the regret of each individual arm against near-fixed (i.e., converging) environments.\nTheorem 2. Under Approximate Linearity, INDSELFSPARRING converges to the optimal arm with asymptotically optimal no-regret rate of O(K ln(T )/\u2206).\nTheorem 2 shows an no-regret guarantee for INDSELF-\nAlgorithm 4 KERNELSELFSPARRING\ninput Input space S, GP prior (\u00b50, \u03c30), m the number of arms drawn at each iteration\n1: for t = 1, 2, . . . do 2: for j = 1, . . . ,m do 3: Sample fj from (\u00b5t\u22121, \u03c3t\u22121) 4: Select ij(t) := argmaxx fj(x) 5: end for 6: Play m arms {ij(t)}j , observe pairwise feedback matrix R = {rjk \u2208 {0, 1, \u2205}}m\u00d7m 7: for j, k = 1, . . . ,m do 8: if rjk 6= \u2205 then 9: apply Bayesian update using (ij(t), rjk) to\nobtain (\u00b5t, \u03c3t) 10: end if 11: end for 12: end for\nSPARRING that asymptotically matches the optimal rate ofO(K ln(T )/\u2206) up to constant factors. In other words, once t > C for some problem-dependent constant C, the regret of INDSELFSPARRING matches informationtheoretic bounds up to constant factors (see Yue et al. (2012) for lower bound analysis).3 The proof technique follows two major steps: (1) prove the convergence of INDSELFSPARRING as shown in Theorem 1; and (2) bound the expected total regret for sufficiently large T ."}, {"heading": "4.2 Dependent Arms Case", "text": "We use Gaussian processes (see Section 2.4) to model dependencies among arms. Applying Gaussian pro-\n3A finite-time guarantee requires more a refined analysis of C, and is an interesting direction for future work.\ncesses is not straightforward, since the underlying utility function is not directly observable or does not exist. We instead use Gaussian processes to model a specific the preference function. In Gaussian process notation, the preference function f(b) represents the preference of choosing b over the perfect \u201cenvironment\u201d of competing arms. Like in the independent arms case (Section 4.1), the perfect environment corresponds to having all the remaining arms be deterministically selected as the best arm b1, yielding f(b) = P (b \u227b b1). We model f(b) as a sample from a Gaussian processGP (\u00b5(b), k(b, b\u2032)). Note that this setup is analogous to the independent arms case, which uses a Beta prior to estimate the probability of each arm defeating the environment (and converges to competing against the best environment).\nAlgorithm 4 describes KERNELSELFSPARRING, which instantiates SELFSPARRING using a Gaussian process Thompson sampling algorithm. The input space S can be continuous. At each iteration t, m arms are sampled using the Gaussian process priorDt\u22121. The posteriorDt is then updated by the responses R and the prior.\nFigure 2 illustrates the optimization process in a onedimensional example. The underlying preference function against the best environment is shown in blue. Dashed lines are the mean function of GP. Shaded areas are \u00b12 standard deviations regions (high confidence regions). Figures 2(a)(b)(c) represent running KERNELSELFSPARRING algorithm at 5, 20, and 100 iterations. The GP model can be observed to be converging to the preference function against the best environment.\nWe conjecture that it is possible to prove no-regret guarantees that scale w.r.t. the dimensionality of the kernel. However, there does not yet exist suitable regret analyses for Gaussian Process Thompson Sampling in the kernel-\nized MAB setting to leverage."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Simulation Settings & Datasets", "text": "Synthetic Functions. We evaluated on a range of 16- arm synthetic settings derived from the utility-based dueling bandits setting of Ailon et al. (2014). For the multidueling setting, we used the following preference functions:\nlinear: \u03c6(x, y) \u2212 1/2 = (1 + x\u2212 y)/2 logit: \u03c6(x, y)\u2212 1/2 = (1 + exp (y \u2212 x))\u22121\nand the utility functions shown in Table 1 (generalized from those in Ailon et al. (2014)). Note that although these preference functions do not satisfy approximate linearity over their entire domains, they do for the utility samples (over the a finite subset of arms).\nMSLR Dataset. Following the evaluation setup of Brost et al. (2016), we also used the Microsoft Learning to Rank (MSLR) WEB30k dataset, which consists of over 3 million query-document pairs labeled with relevance scores (Liu et al., 2007). Each pair is scored along 136 features, which can be treated as rankers (arms). For any subset of arms, we can estimate a preference matrix using the expected probability over the entire dataset of one arm beating another using top-10 interleaving and a perfect-click model. We simulate user feedback by using team-draft multileaving (Schuth et al., 2014)."}, {"heading": "5.2 Vanilla Dueling Bandits Experiments", "text": "We first compare against the vanilla dueling bandits setting of dueling a single pair of arms at a time. These experiments are included as a sanity check to confirm that SELFSPARRING (with m = 2) is a competitive algorithm in the original dueling bandits setting, and are not the main focus of our empirical analysis.\nWe empirically evaluate against a range of conventional dueling bandit algorithms, including:\n\u2022 Interleaved Filter (IF) (Yue et al., 2012)\n\u2022 Beat the Mean (BTM) (Yue & Joachims, 2011)\n\u2022 RUCB (Zoghi et al., 2014)\n\u2022 MergeRUCB (Zoghi et al., 2015b)\n\u2022 Sparring + UCB1 (Ailon et al., 2014)\n\u2022 Sparring + EXP3 (Dud\u0131\u0301k et al., 2015)\n\u2022 RMED1 (Komiyama et al., 2015)\n\u2022 Double Thompson Sampling (Wu & Liu, 2016)\nFor Double Thompson Sampling and INDSELFSPARRING, we set the learning rates to be 2.5 and 3.5 as optimized over a separate dataset of uniformly sampled utility functions. We use \u03b1 = 0.51 for RUCB/MergeRUCB, \u03b3 = 1 for BTM, and f(K) = 0.3K1.01 for RMED1.\nResults. For each scenario, we run each algorithm 100 times for 20000 iterations. For brevity, we show in Figure 3 the average regret of one synthetic simulation along with shaded one standard-deviation areas. We observe that SELFSPARRING is competitive with the best performing methods in the original dueling bandits setting. More complete experiments that replicate Ailon et al. (2014) are provided in the supplementary material, and demonstrate the consistency of this result.\nDouble Thompson Sampling (DTS) is the best performing approach in Figure 3, which is a fairly consistent result in the extended results in the supplementary material. However, given their high variances they are essentially comparable w.r.t. all other algorithms. Furthermore, INDSELFSPARRING has the advantage of being easily extensible to the more realistic multi-dueling and kernelized settings, which is not true of DTS."}, {"heading": "5.3 Multi-Dueling Bandits Experiments", "text": "We next evaluate the multi-dueling setting with independent arms. We compare against the main existing approaches that are applicable to the multi-dueling setting, including the MDB algorithm (Brost et al., 2016), and the multi-dueling extension of Sparring, which we refer to as MultiSparring (Ailon et al., 2014). Following Brost et al. (2016), we use \u03b1 = 0.5 and \u03b2 = 1.5 for the MDB algorithm. For INDSELFSPARRING, we set learning rate to be the default 1. Note that the vast majority dueling bandits algorithms are not easily applicable to the multi-dueling setting. For instance, RUCB-style algorithms treat the two arms asymmetrically, which is not\neasily generalized to multi-dueling.\nResults on Synthetic Experiments. We test m = 4 on the linear 1good and arith datasets in Figure 4 and Figure 5, respectively. We observe that INDSELFSPARRING significantly outperforms competing approaches.\nResults on MSLR Dataset. Following the simulation setting of Brost et al. (2016) on the MSLR dataset (see Section 5.1), we compared against the MDB algorithm over the same collection of 50 randomly sampled 16- arm subsets. We ensured that each 16-arm subset had a Condorcet winner; in general it is likely for any random subset of arms in the MSLR dataset to have a Condorcet winner (Zoghi et al., 2015a). Figure 6 shows the results, where we again see that INDSELFSPARRING enjoys significantly better performance."}, {"heading": "5.4 Kernelized (Multi-)Dueling Experiments", "text": "We finally evaluate the kernelized setting for both the 2- dueling and the multi-dueling case. We evaluate KERNELSELFSPARRING against BOPPER (Gonzalez et al., 2016) and Sparring (Ailon et al., 2014) with GP-UCB\n(Srinivas et al., 2010). BOPPER is a Bayesian optimization method can be applied to kernelized 2-dueling setting (but not multi-dueling). Sparring with GP-UCB, which refer to as GP-Sparring, is essentially a variant of our KERNELSELFSPARRING approach but maintains a m GP-UCB bandit algorithms (one controlling each choice of arm to be dueled), rather than just a single one.\nKERNELSELFSPARRING and GP-Sparring use GPs that model the preference function, i.e. are one-sided, whereas BOPPER uses a GP to model the entire preference matrix. Following Srinivas et al. (2010), we use a squared exponential kernel with lengthscale parameter 0.2 for both GP-Sparring and KERNELSELFSPARRING, and use a squared exponential kernel with parameter 1 for BOPPER. We initialize all GPs with a zero-mean prior, and use sampling noise variance \u03c32 = 0.025. For GP-Sparring, we use the scaled-down version of \u03b2t as suggested by Srinivas et al. (2010).\nWe use the Forrester and Six-Hump Camel functions as utility functions on [0, 1] and [0, 1]2, respectively, as in\nGonzalez et al. (2016). Similarly, we use the same uniform discretizations of 30 and 64 points for the Forrester and Six-Hump Camel settings respectively, and use the logit link function to generate preferences.\nSince the BOPPER algorithm is computationally expensive, we only include it in the Forrester setting, and run each algorithm 20 times for 100 iterations. In the SixHump Camel setting, we run KERNELSELFSPARRING and GP-Sparring for 500 iterations 100 times each. Results are presented in Figures 8 and 9, where we observe much better performance from KERNELSELFSPARRING against both BOPPER and GP-Sparring.\nIn the kernelized multi-dueling setting, we compare against GP-Sparring. We run each algorithm for 100 iterations 50 times on the Forrester and Six-Hump Camel functions, and plot their regrets in Figures 10 and 11 respectively. We use m = 4 for both algorithms, and the same discretization as in the standard dueling case. We again observe significant performance gains of our KERNELSELFSPARRING approach."}, {"heading": "6 Conclusions", "text": "We studied multi-dueling bandits with dependent arms. This setting extends the original dueling bandits setting by dueling multiple arms per iteration rather than just\ntwo, and modeling low-dimensional dependencies between arms rather than treat each arm independently. Both extensions are motivated by practical real-world considerations such as in personalized clinical treatment (Sui & Burdick, 2014). We proposed SELFSPARRING, which is simple and easy to extend, e.g., by integrating with kernels to model dependencies across arms. Our experimental results demonstrated significant reduction in regret compared to state-of-the-art dueling bandit algorithms. Generally, relative benefits compared to dueling bandits increased with the number of arms being compared. For SELFSPARRING, the incurred regret did not increase substantially as the number of arms increased.\nOur approach can be extended in several important directions. Most notably, the theoretical analysis could be improved. For instance, it would be more desirable to provide explicit finite-time regret guarantees rather than asymptotic ones. Furthermore, an analysis of the kernelized multi-dueling setting is also lacking. From a more practical perspective, we assumed that the choice of arms does not impact the feedback mechanism (e.g., all pairs), which is not true in practice (e.g., humans can have a hard time distinguishing very different arms)."}, {"heading": "A Proofs", "text": "This section provides the proof sketch of Lemmas and Theorems mentioned in the main paper.\nLemma 1. For the K-armed stochastic MAB problem, Thompson Sampling has expected regret: E[RMABT ] = O (\nK \u2206 lnT\n)\n, where\u2206 is the difference between expected rewards of the best two arms.\nProof. This lemma is a direct result from Theorem 2 of Agrawal & Goyal (2012) and Theorem 1 of Kaufmann et al. (2012).\nLemma 2. Running INDSELFSPARRING with infinite time horizon will sample each arm infinitely often.\nProof. Proof by contradiction. Let B(x;\u03b1, \u03b2) = \u222b x\n0 t\u03b1\u22121(1 \u2212 t)\u03b2\u22121dt. Then the CDF\nof Beta distribution with parameters (\u03b1, \u03b2) is\nF (x;\u03b1, \u03b2) = B(x;\u03b1, \u03b2)\nB(1;\u03b1, \u03b2) .\nSuppose arm b can only be sampled in finite number of iterations. Then there exists finite upper bound Tb for \u03b1b + \u03b2b. For any given x \u2208 (0, 1), the probability of sampling values of arm b \u03b8b greater than x is\nP (\u03b8b > x) = 1\u2212 F (x;\u03b1b, \u03b2b)\n\u2265 1\u2212 F (x; 1, Tb \u2212 1) = (1\u2212 x) Tb\u22121 > 0\nThen by running INDSELFSPARRING, the probability of choosing arm b after it has been chosen Tb times:\nP (\u03b8b \u2265 maxi{\u03b8bi}) \u2265 \u220f\ni\nP (\u03b8b \u2265 \u03b8bi)\nis strictly non-zero. That violates any fixed upper bound Tb.\nTheorem 1. Under Approximate Linearity, INDSELFSPARRING converges to the optimal arm b1 as running time t \u2192 \u221e: limt\u2192\u221e P(bt = b1) = 1.\nProof. INDSELFSPARRING keeps one Beta distribution Beta(\u03b1i(t), \u03b2i(t)) for each arm bi at time step t. Let \u00b5\u0302i(t) = \u03b1i(t)\n\u03b1i(t)+\u03b2i(t) , \u03c3\u03022i (t) =\n\u03b1i(t)\u03b2i(t) (\u03b1i(t)+\u03b2i(t))2(\u03b1i(t)+\u03b2i(t)+1) be the empirical mean and variance for arm bi. Obviously, \u03c3\u03022i (t) \u2192 0 as (\u03b1i(t) + \u03b2i(t)) = (Si(t) + Fi(t)) \u2192 \u221e. By Lemma 2 we have (Si(t) + Fi(t)) \u2192 \u221e as t \u2192 \u221e. That shows every Beta distribution is concentrating to a Dirac function at \u00b5\u0302i(t) when t \u2192 \u221e.\nDefine \u00b5\u0302(t) = [\u00b5\u03021(t), \u00b7 \u00b7 \u00b7 , \u00b5\u0302K(t)]T \u2208 [0, 1]K to be the vector of means of all arms. Then \u00b5 = {\u00b5i = P (bi \u227b b1)}i=1,\u00b7\u00b7\u00b7 ,K is a stable point for INDSELFSPARRING in theK dimensional mean space.\nSuppose there exists another stable point \u03bd \u2208 [0, 1]K(\u03bd 6= \u00b5) for INDSELFSPARRING, consider the following two possibilities: (1) \u03bd1 = maxi{\u03bdi} and (2) \u03bd1 < maxi{\u03bdi} = \u03bdj .\nSince the Beta distributions for each arm bi is concentrating to Dirac functions at \u03bdi, P (\u03b8i > \u03b8j) \u2208 [I(\u03bdi > \u03bdj) \u2212 \u03b4, I(\u03bdi > \u03bdj) + \u03b4] for any fixed \u03b4 > 0 with high probability.\nIf (1) holds, then \u03bd1 will converge to 1 2 = \u00b51 and \u03bdi will converge to P (bi \u227b b1) = \u00b5i. Thus \u03bd = \u00b5. Contradict to \u03bd 6= \u00b5.\nIf (2) holds, then \u03bdj will converge to 1 2 = \u00b51 and \u03bd1 \u2208 [P (b1 \u227b bj) \u2212 \u03b4, P (b1 \u227b bj) + \u03b4] for any fixed \u03b4 > 0 with high probability. Since P (b1 \u227b bj) \u2265 1 2 +\u2206, \u03bd1 \u2208 [P (b1 \u227b bj)\u2212 \u03b4, P (b1 \u227b bj) + \u03b4] \u2265 1 2 +\u2206\u2212 \u03b4 . Since \u03b4 can be arbitrarily small, we have \u03bd1 \u2265 1 2 + \u2206 \u2212 \u03b4 > 1 2 + \u03b4 > \u03bdj . That contradict to \u03bd1 < \u03bdj .\nIn summary, \u00b5 = {\u00b5i = P (bi \u227b b1)}i=1,\u00b7\u00b7\u00b7 ,K is the only stable point in the mean space. As \u00b5\u0302(t) \u2192 \u00b5, P(bt = b1) \u2192 1.\nDefine Pt = [P1(t), P2(t), ..., PK(t)] as the probabilities of picking each arm at time t. Let P = {Pt}t=1,2,... be the sequence of probabilities w.r.t. time. Assume INDSELFSPARRING is non-convergent. It is equivalent to say that P is not converging to a fixed distribution. Then \u2203\u03b4 > 0 and arm i s.t. the sequence of probabilities {Pi(t)}t satisfies:\nlim sup t\u2192\u221e Pi(t)\u2212 lim inf t\u2192\u221e Pi(t) > \u03b4\nw.h.p. which is equivalent of having:\nlim sup t\u2192\u221e \u00b5\u0302i(t)\u2212 lim inf t\u2192\u221e \u00b5\u0302i(t) > \u01eb\nw.h.p. for some fixed \u01eb > 0. This violates the stability of INDSELFSPARRING in the K dimensional mean space as shown above. So as t \u2192 \u221e, \u00b5\u0302(t) \u2192 \u00b5, P(bt = b1) \u2192 1.\nLemma 3. Under Approximate Linearity, selecting only one arm via Thompson sampling against a fixed distribution over the remaining arms leads to optimal regret w.r.t. choosing that arm.\nProof. We first prove the results for m = 2. Results for anym > 2 can be proved in a similar way.\nConsider Player 1 drawing arms from a fixed distribution L. Player 2\u2019s drawing strategy is an MAB algorithmA.\nLet RA(T ) be the regret of algorithm A within horizon T . B(T ) = supE[RA(T )] is the supremum of the expected regret ofA.\nThe reward of Player 2 at iteration t is \u03c6(b2t, b1t). Reward of keep playing the optimal arm is \u03c6(b1, b1t). So the total regret after T rounds is\nRA(T ) =\nT \u2211\nt=1\n[\u03c6(b1, b1t)\u2212 \u03c6(b2t, b1t)]\nSince Approximate Linearity yields\n\u03c6(b1, b1t)\u2212 \u03c6(b2t, b1t) \u2265 \u03b3 \u00b7 \u03c6(b1, b2t)\nWe have\nE[RA(T )] = EEb1t\u223cL\n[\nT \u2211\nt=1\n[\u03c6(b1, b1t)\u2212 \u03c6(b2t, b1t)\n]\n\u2265 EEb1t\u223cL\n[\nT \u2211\nt=1\n\u03b3 \u00b7 \u03c6(b1, b2t)\n]\n= \u03b3 \u00b7 E\n[\nT \u2211\nt=1\n\u03c6(b1, b2t)\n]\n= \u03b3 \u00b7 E[R(T )]\nSo the total regret of Player 2 is bounded by\nE[R(T )] \u2264 1\n\u03b3 E[RA(T )] \u2264\n1 \u03b3 supE[RA(T )] = 1 \u03b3 B(T )\nCorollary 1. If approximate linearity holds, competing with a drifting but converging distribution of arms guarantees the one-side convergence for Thompson Sampling.\nProof. LetDt be the drifting but converging distribution and Dt \u2192 D as t \u2192 \u221e. Let bT be the drifting mean bandit of DT after T iterations. Since Dt is convergent, \u2203T > K such that\n\u03c6(sup t>T bT , inf t>T bT ) < \u03c6(b1, b2)\nwhere \u03c6(b1, b2) is the preference between the best two arms. The mean value of feedback by playing arm i is \u03c6(bi, bT ). If bT is fixed, by Lemma3, Thompson sampling converges to the arm: i\u2217 = argmaxi \u03c6(bi, bT ). For drifting bT , define b + = supt>T bT and b \u2212 = inft>T bT .\nThompson sampling convergence to the optimal arm implies that:\n\u03c6(b1, b +) > \u03c6(bi, b \u2212)\nfor all i 6= 1. Consider:\n\u03c6(b1, b +)\u2212 \u03c6(b2, b \u2212)\n= \u03c6(b1, b +)\u2212 \u03c6(b2, b \u2212) + \u03c6(b1, b \u2212)\u2212 \u03c6(b1, b \u2212)\n= \u03c6(b1, b \u2212)\u2212 \u03c6(b2, b \u2212) + \u03c6(b2, b +)\u2212 \u03c6(b1, b \u2212)\n\u2265 \u03b3 \u00b7 [\u03c6(b1, b2)\u2212 \u03c6(b +, b\u2212)] > 0\nby approximate linearity.\nSo we have \u03c6(b1, b +) > \u03c6(b2, b \u2212). Since \u03c6(b2, b \u2212) > \u03c6(bi, b \u2212) for i > 2. Then we have\n\u03c6(b1, b +) > \u03c6(bi, b \u2212)\nholds for all i 6= 1. So Thompson sampling converge to the optimal arm.\nTheorem 2. Under Approximate Linearity, INDSELFSPARRING converges to the optimal arm with asymptotically optimal no-regret rate of O(K ln(T )/\u2206). Where \u2206 is the difference between the rewards of the best two arms.\nProof. Theorem1 provides the convergenceguarantee of INDSELFSPARRING. Corollary 1 shows one-side convergence for playing against a converging distribution.\nSince INDSELFSPARRING converges to the optimal arm b1 as running time t \u2192 \u221e: limt\u2192\u221e P(bt = b1) = 1. For \u2200\u03b4 > 0, there exists C(\u03b4) > 0 such that for any t > C(\u03b4), the following condition holds w.h.p.: P (bt = b1) \u2265 1\u2212 \u03b4.\nFor the triple of bandits b1 \u227b bi \u227b bK , Approximate Linearity guarantees:\n\u03c6(bi, bK) < \u03c6(b1, bK) \u2264 \u03c9\nholds for some fixed \u03c9 > 0 and \u2200i \u2208 {2, \u00b7 \u00b7 \u00b7 ,K \u2212 1}. With small \u03b4, the competing environment of any Player p is bounded. If \u03b4 < \u2206\u2206+\u03c9 , (1\u2212\u03b4) \u00b7(\u2212\u2206)+\u03b4 \u00b7\u03c6(b2 , bK) < 0 = 1 \u00b7 \u03c6(b1, b1). The competing environment can be considered as unbiased and the theoretical guarantees for Thompson sampling for stochastic multi-armed bandit is valid (up to a constant factor).\nThen INDSELFSPARRING has an no-regret guarantee that asymptotically matches the optimal rate of O(K ln(T )/\u2206) up to constant factors, which proves Theorem 2.\nB Further Experiments"}], "references": [{"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Reducing dueling bandits to cardinal bandits", "author": ["Ailon", "Nir", "Karnin", "Zohar", "Joachims", "Thorsten"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Ailon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2014}, {"title": "Finitetime analysis of the multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Fischer", "Paul"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Multi-dueling bandits and their application to online ranker evaluation", "author": ["Brost", "Brian", "Seldin", "Yevgeny", "Cox", "Ingemar J", "Lioma", "Christina"], "venue": "In ACM Conference on Information and Knowledge Management,", "citeRegEx": "Brost et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brost et al\\.", "year": 2016}, {"title": "An empirical evaluation of thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Large-scale validation and analysis of interleaved search evaluation", "author": ["Chapelle", "Olivier", "Joachims", "Thorsten", "Radlinski", "Filip", "Yue", "Yisong"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Chapelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2012}, {"title": "Contextual dueling bandits", "author": ["Dud\u0131\u0301k", "Miroslav", "Hofmann", "Katja", "Schapire", "Robert E", "Slivkins", "Aleksandrs", "Zoghi", "Masrour"], "venue": "In Conference on Learning Theory (COLT),", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2015}, {"title": "A relative exponential weighing algorithm for adversarial utility-based dueling bandits", "author": ["Gajane", "Pratik", "Urvoy", "Tanguy", "Cl\u00e9rot", "Fabrice"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Gajane et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gajane et al\\.", "year": 2015}, {"title": "Bayesian optimisation with pairwise preferential returns", "author": ["Gonzalez", "Javier", "Dai", "Zhenwen", "Damianou", "Andreas", "Lawrence", "Neil D"], "venue": "In NIPS Workshop on Bayesian Optimization,", "citeRegEx": "Gonzalez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2016}, {"title": "A probabilistic method for inferring preferences from clicks", "author": ["Hofmann", "Katja", "Whiteson", "Shimon", "De Rijke", "Maarten"], "venue": "In ACM Conference on Information and Knowledge Management,", "citeRegEx": "Hofmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2011}, {"title": "Sparse dueling bandits", "author": ["Jamieson", "Kevin", "Katariya", "Sumeet", "Deshpande", "Atul", "Nowak", "Robert"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Jamieson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2015}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Kaufmann", "Emilie", "Korda", "Nathaniel", "Munos", "R\u00e9mi"], "venue": "In Algorithmic Learning Theory (ALT),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Regret lower bound and optimal algorithm in dueling bandit problem", "author": ["Komiyama", "Junpei", "Honda", "Junya", "Kashima", "Hisashi", "Nakagawa", "Hiroshi"], "venue": "In COLT, pp", "citeRegEx": "Komiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Komiyama et al\\.", "year": 2015}, {"title": "Letor: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Liu", "Tie-Yan", "Xu", "Jun", "Qin", "Tao", "Xiong", "Wenying", "Li", "Hang"], "venue": "In SIGIR 2007 workshop on learning to rank for information retrieval,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Some aspects of the sequential design of experiments", "author": ["Robbins", "Herbert"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Robbins and Herbert.,? \\Q1952\\E", "shortCiteRegEx": "Robbins and Herbert.", "year": 1952}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "Multileaved comparisons for fast online evaluation", "author": ["Schuth", "Anne", "Sietsma", "Floor", "Whiteson", "Shimon", "Lefortier", "Damien", "de Rijke", "Maarten"], "venue": "In ACM Conference on Conference on Information and Knowledge Management,", "citeRegEx": "Schuth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schuth et al\\.", "year": 2014}, {"title": "Multileave gradient descent for fast online learning to rank", "author": ["Schuth", "Anne", "Oosterhuis", "Harrie", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In ACM Conference on Web Search and Data Mining,", "citeRegEx": "Schuth et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schuth et al\\.", "year": 2016}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham", "Seeger", "Matthias"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Clinical online recommendation with subgroup rank feedback", "author": ["Sui", "Yanan", "Burdick", "Joel"], "venue": "In ACM Conference on Recommender Systems (RecSys),", "citeRegEx": "Sui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sui et al\\.", "year": 2014}, {"title": "Generic exploration and k-armed voting bandits", "author": ["Urvoy", "Tanguy", "Clerot", "Fabrice", "F\u00e9raud", "Raphael", "Naamane", "Sami"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Urvoy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2013}, {"title": "Double thompson sampling for dueling bandits", "author": ["Wu", "Huasen", "Liu", "Xin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Interactively optimizing information retrieval systems as a dueling bandits problem", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2009}, {"title": "Beat the mean bandit", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "The k-armed dueling bandits problem", "author": ["Yue", "Yisong", "Broder", "Josef", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yue et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2012}, {"title": "Relative upper confidence bound for the karmed dueling bandit problem", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "Munos", "Remi", "de Rijke", "Maarten"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zoghi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2014}, {"title": "Copeland dueling bandits", "author": ["Zoghi", "Masrour", "Karnin", "Zohar S", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}, {"title": "Mergerucb: A method for large-scale online ranker evaluation", "author": ["Zoghi", "Masrour", "Whiteson", "Shimon", "de Rijke", "Maarten"], "venue": "In ACM International Conference on Web Search and Data Mining (WSDM),", "citeRegEx": "Zoghi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zoghi et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Examples include implicit or subjective feedback for information retrieval and various recommender systems (Chapelle et al., 2012; Sui & Burdick, 2014).", "startOffset": 107, "endOffset": 151}, {"referenceID": 25, "context": "This setup motivates the dueling bandits problem (Yue et al., 2012), which formalizes the problem of online regret minimization via preference feedback.", "startOffset": 49, "endOffset": 67}, {"referenceID": 1, "context": "For this setting, we propose the SELFSPARRING algorithm, inspired by the Sparring algorithm from Ailon et al. (2014), which algorithmically reduces the multi-dueling bandits problem into a conventional muilti-armed bandit problem that can be solved using a stochastic bandit algorithm such as Thompson Sampling (Chapelle & Li, 2011; Russo & Van Roy, 2014).", "startOffset": 97, "endOffset": 117}, {"referenceID": 4, "context": "While there have been some prior work on multi-dueling (Brost et al., 2016) and learning from pairwise preferences over kernels (Gonzalez et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 9, "context": ", 2016) and learning from pairwise preferences over kernels (Gonzalez et al., 2016), to the best of our knowledge, our approach is the first to address to both in a unified framework.", "startOffset": 60, "endOffset": 83}, {"referenceID": 25, "context": "To date, there have been several algorithms proposed for the stochastic dueling bandits problem, including Interleaved Filter (Yue et al., 2012), Beat the Mean (Yue & Joachims, 2011), SAVAGE (Urvoy et al.", "startOffset": 126, "endOffset": 144}, {"referenceID": 21, "context": ", 2012), Beat the Mean (Yue & Joachims, 2011), SAVAGE (Urvoy et al., 2013), RUCB (Zoghi et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": ", 2014, 2015b), Sparring (Ailon et al., 2014; Dud\u0131\u0301k et al., 2015), RMED (Komiyama et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 7, "context": ", 2014, 2015b), Sparring (Ailon et al., 2014; Dud\u0131\u0301k et al., 2015), RMED (Komiyama et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 13, "context": ", 2015), RMED (Komiyama et al., 2015), and DTS (Wu & Liu, 2016).", "startOffset": 14, "endOffset": 37}, {"referenceID": 7, "context": "These include continuousarmed convex dueling bandits (Yue & Joachims, 2009), contextual dueling bandits which also introduces the von Neumann winner solution concept (Dud\u0131\u0301k et al., 2015), sparse dueling bandits that focuses on the Borda winner solution concept (Jamieson et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 11, "context": ", 2015), sparse dueling bandits that focuses on the Borda winner solution concept (Jamieson et al., 2015), Copeland dueling bandits that focuses on the Copeland winner solution concept (Zoghi et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 8, "context": ", 2015a), and adversarial dueling bandits (Gajane et al., 2015).", "startOffset": 42, "endOffset": 63}, {"referenceID": 10, "context": "Example settings include information retrieval (Hofmann et al., 2011; Schuth et al., 2014, 2016) and clinical treatment (Sui & Burdick, 2014).", "startOffset": 47, "endOffset": 96}, {"referenceID": 4, "context": "There have also been some previous work on multi-dueling bandits settings (Brost et al., 2016; Sui & Burdick, 2014; Schuth et al., 2016), however the previous approaches are limited in their scope and lack rigorous theoretical guarantees.", "startOffset": 74, "endOffset": 136}, {"referenceID": 18, "context": "There have also been some previous work on multi-dueling bandits settings (Brost et al., 2016; Sui & Burdick, 2014; Schuth et al., 2016), however the previous approaches are limited in their scope and lack rigorous theoretical guarantees.", "startOffset": 74, "endOffset": 136}, {"referenceID": 12, "context": "Thompson Sampling enjoys near-optimal regret guarantees in the stochasticMAB setting, as given by the lemma below (which is a direct consequence of main theorems in Agrawal & Goyal (2012); Kaufmann et al. (2012)).", "startOffset": 189, "endOffset": 212}, {"referenceID": 19, "context": "the dimensionality of the kernel rather than the number of arms (which can now be infinite) (Srinivas et al., 2010).", "startOffset": 92, "endOffset": 115}, {"referenceID": 17, "context": "For example, in information retrieval this can be implemented via multi-leaving (Schuth et al., 2014) the ranked lists of the subset, St, of rankers and then inferring the relative quality of the lists (and the corresponding rankers) from user feedback.", "startOffset": 80, "endOffset": 101}, {"referenceID": 4, "context": "Our setting differs from Brost et al. (2016) in two ways.", "startOffset": 25, "endOffset": 45}, {"referenceID": 4, "context": "Our setting differs from Brost et al. (2016) in two ways. First, we play a fixed, rather than variable, number of arms at each iteration. Furthermore, we focus on total regret, rather than the instantaneous average regret in a single iteration; in many applications (e.g., Sui & Burdick (2014)), playing each arm incurs its own regret .", "startOffset": 25, "endOffset": 294}, {"referenceID": 1, "context": "One assumption that we leverage in developing our approach is approximate linearity, which fully generalizes the linear utility-based dueling bandits setting studied in Ailon et al. (2014). For any triplet of bandits bi \u227b bj \u227b bk and some constant \u03b3 > 0: \u03c6(bi, bk)\u2212 \u03c6(bj , bk) \u2265 \u03b3\u03c6(bi, bj).", "startOffset": 169, "endOffset": 189}, {"referenceID": 1, "context": "When \u03a6 is linear, then our setting reduces to the utility-based dueling bandits setting of Ailon et al. (2014).", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "We start with a high-level description of our general framework, called SELFSPARRING, which is inspired by the Sparring algorithm from Ailon et al. (2014). The high-level strategy is to reduce the multi-dueling bandits problem to a multi-armed bandit (MAB) problem that can be solved using a MAB algorithm, and ideally lift existing MAB guarantees to the multi-dueling setting.", "startOffset": 135, "endOffset": 155}, {"referenceID": 23, "context": "Compared to the assumptions of Yue et al. (2012), Approximate Linearity is a stricter requirement than strong stochastic transitivity, and is a complementary requirement to stochastic triangle inequality.", "startOffset": 31, "endOffset": 49}, {"referenceID": 23, "context": "In other words, once t > C for some problem-dependent constant C, the regret of INDSELFSPARRING matches informationtheoretic bounds up to constant factors (see Yue et al. (2012) for lower bound analysis).", "startOffset": 160, "endOffset": 178}, {"referenceID": 1, "context": "We evaluated on a range of 16arm synthetic settings derived from the utility-based dueling bandits setting of Ailon et al. (2014). For the multidueling setting, we used the following preference functions: linear: \u03c6(x, y) \u2212 1/2 = (1 + x\u2212 y)/2 logit: \u03c6(x, y)\u2212 1/2 = (1 + exp (y \u2212 x))", "startOffset": 110, "endOffset": 130}, {"referenceID": 1, "context": "and the utility functions shown in Table 1 (generalized from those in Ailon et al. (2014)).", "startOffset": 70, "endOffset": 90}, {"referenceID": 14, "context": "(2016), we also used the Microsoft Learning to Rank (MSLR) WEB30k dataset, which consists of over 3 million query-document pairs labeled with relevance scores (Liu et al., 2007).", "startOffset": 159, "endOffset": 177}, {"referenceID": 17, "context": "We simulate user feedback by using team-draft multileaving (Schuth et al., 2014).", "startOffset": 59, "endOffset": 80}, {"referenceID": 4, "context": "Following the evaluation setup of Brost et al. (2016), we also used the Microsoft Learning to Rank (MSLR) WEB30k dataset, which consists of over 3 million query-document pairs labeled with relevance scores (Liu et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 25, "context": "\u2022 Interleaved Filter (IF) (Yue et al., 2012) \u2022 Beat the Mean (BTM) (Yue & Joachims, 2011) \u2022 RUCB (Zoghi et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 26, "context": ", 2012) \u2022 Beat the Mean (BTM) (Yue & Joachims, 2011) \u2022 RUCB (Zoghi et al., 2014) \u2022 MergeRUCB (Zoghi et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": ", 2015b) \u2022 Sparring + UCB1 (Ailon et al., 2014) \u2022 Sparring + EXP3 (Dud\u0131\u0301k et al.", "startOffset": 27, "endOffset": 47}, {"referenceID": 7, "context": ", 2014) \u2022 Sparring + EXP3 (Dud\u0131\u0301k et al., 2015) \u2022 RMED1 (Komiyama et al.", "startOffset": 26, "endOffset": 47}, {"referenceID": 13, "context": ", 2015) \u2022 RMED1 (Komiyama et al., 2015) \u2022 Double Thompson Sampling (Wu & Liu, 2016) Figure 3: Vanilla dueling bandits setting.", "startOffset": 16, "endOffset": 39}, {"referenceID": 1, "context": "More complete experiments that replicate Ailon et al. (2014) are provided in the supplementary material, and demonstrate the consistency of this result.", "startOffset": 41, "endOffset": 61}, {"referenceID": 4, "context": "We compare against the main existing approaches that are applicable to the multi-dueling setting, including the MDB algorithm (Brost et al., 2016), and the multi-dueling extension of Sparring, which we refer to as MultiSparring (Ailon et al.", "startOffset": 126, "endOffset": 146}, {"referenceID": 1, "context": ", 2016), and the multi-dueling extension of Sparring, which we refer to as MultiSparring (Ailon et al., 2014).", "startOffset": 89, "endOffset": 109}, {"referenceID": 1, "context": ", 2016), and the multi-dueling extension of Sparring, which we refer to as MultiSparring (Ailon et al., 2014). Following Brost et al. (2016), we use \u03b1 = 0.", "startOffset": 90, "endOffset": 141}, {"referenceID": 4, "context": "Following the simulation setting of Brost et al. (2016) on the MSLR dataset (see Section 5.", "startOffset": 36, "endOffset": 56}, {"referenceID": 9, "context": "We evaluate KERNELSELFSPARRING against BOPPER (Gonzalez et al., 2016) and Sparring (Ailon et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 1, "context": ", 2016) and Sparring (Ailon et al., 2014) with GP-UCB Figure 6: Multi-dueling regret for MSLR-30K experiments", "startOffset": 21, "endOffset": 41}, {"referenceID": 19, "context": "(Srinivas et al., 2010).", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Following Srinivas et al. (2010), we use a squared exponential kernel with lengthscale parameter 0.", "startOffset": 10, "endOffset": 33}, {"referenceID": 19, "context": "Following Srinivas et al. (2010), we use a squared exponential kernel with lengthscale parameter 0.2 for both GP-Sparring and KERNELSELFSPARRING, and use a squared exponential kernel with parameter 1 for BOPPER. We initialize all GPs with a zero-mean prior, and use sampling noise variance \u03c3 = 0.025. For GP-Sparring, we use the scaled-down version of \u03b2t as suggested by Srinivas et al. (2010).", "startOffset": 10, "endOffset": 394}], "year": 2017, "abstractText": "The dueling bandits problem is an online learning framework for learning from pairwise preference feedback, and is particularly wellsuited for modeling settings that elicit subjective or implicit human feedback. In this paper, we study the problem of multi-dueling bandits with dependent arms, which extends the original dueling bandits setting by simultaneously dueling multiple arms as well as modeling dependencies between arms. These extensions capture key characteristics found in many realworld applications, and allow for the opportunity to develop significantly more efficient algorithms than were possible in the original setting. We propose the SELFSPARRING algorithm, which reduces the multi-dueling bandits problem to a conventional bandit setting that can be solved using a stochastic bandit algorithm such as Thompson Sampling, and can naturally model dependencies using a Gaussian process prior. We present a no-regret analysis for multi-dueling setting, and demonstrate the effectiveness of our algorithm empirically on a wide range of simulation settings.", "creator": "LaTeX with hyperref package"}}}