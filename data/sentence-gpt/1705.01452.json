{"id": "1705.01452", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "abstract": "In typical neural machine translation~(NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same time-scale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. The main implication of this model is that the encoding of a word at different time scales, which we say makes it a natural model for decoder decoding.\n\n\n\n\nWe summarize these two ideas using an example of decoding decoder encoding, as described below. In addition to decoding the word in a single word, we describe the decoding of the word as a decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder decoder dec", "histories": [["v1", "Wed, 3 May 2017 14:39:56 GMT  (1131kb,D)", "http://arxiv.org/abs/1705.01452v1", "Accepted as a short paper by ACL 2017"]], "COMMENTS": "Accepted as a short paper by ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hao zhou", "zhaopeng tu", "shujian huang", "xiaohua liu", "hang li", "jiajun chen"], "accepted": true, "id": "1705.01452"}, "pdf": {"name": "1705.01452.pdf", "metadata": {"source": "CRF", "title": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation", "authors": ["Hao Zhou", "Zhaopeng Tu", "Shujian Huang", "Xiaohua Liu", "Hang Li", "Jiajun Chen"], "emails": ["zhouh@nlp.nju.edu.cn", "tuzhaopeng@gmail.com", "huangsh@nlp.nju.edu.cn", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com", "chenjj@nlp.nju.edu.cn"], "sections": [{"heading": null, "text": "In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model."}, {"heading": "1 Introduction", "text": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).\n\u2217Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah\u2019s Ark Lab.\nThe decoder state stores translation information at different granularities, determining which segment should be expressed (phrasal), and which word should be generated (lexical), respectively. However, due to the extensive existence of multiword phrases and expressions, the varying speed of the lexical component is much faster than the phrasal one. As in the generation of \u201cthe French Republic\u201d, the lexical component in the decoder will change thrice, each of which for a separate word. But the phrasal component may only change once. The inconsistent varying speed of the two components may cause translation errors.\nTypical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-jussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007).\nWe propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated with-\n1In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words.\nar X\niv :1\n70 5.\n01 45\n2v 1\n[ cs\n.C L\n] 3\nM ay\n2 01\n7\nout attention. The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary gate automatically. In this way, we incorporate soft phrases into NMT, which makes the model flexible at capturing both global reordering of phrases and local translation inside phrases. Our model has following benefits:\n1. The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible. 2. Our model recognizes phrase structures explicitly. Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words. 3. Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars. 4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart.\nExperiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously."}, {"heading": "2 Standard Neural Machine Translation Model", "text": "Generally, neural machine translation system directly models the conditional probability of the translation y word by word (Bahdanau et al., 2014). Formally, given an input sequence x = [x1, x2, . . . , xJ ], and the previously generated sequence y<t = [y1, y2, . . . , yt\u22121], the probability of next target word yt is\nP (yt|x) = softmax(f(eyt\u22121 , st, ct)) (1)\nwhere f(\u00b7) is a non-linear function, eyt\u22121 is the embedding of yt\u22121; st is the decode state at the time step t, which is computed by\nst = g(st\u22121, eyt\u22121 , ct) (2)\nHere g(\u00b7) is a transition function of decoder RNN. ct is the context vector computed by\nct = J\u2211\nj=1\nATT(st\u22121, hj) \u00b7 hj = J\u2211\nj=1\n\u03b1t,j \u00b7 hj (3)\nwhere ATT is an attention operation, which outputs alignment distribution \u03b1:\n\u03b1t,j = exp(et,j)\u2211Tx k=1 exp(et,k)\n(4)\net,j = v T a tanh(Wast\u22121 + Uahj) (5)\nand h is the annotation of x from a bi-directional RNNs. The training objective is to maximize the likelihood of the training data. Beam search is adopted for decoding."}, {"heading": "3 Chunk-Based Bi-Scale Neural Machine Translation Model", "text": "Instead of the word-based decoder, we propose to use a chunk-based bi-scale decoder, which generates translation hierarchically with chunk and word time-scales, as shown in Figure 1. Intuitively, we firstly generate a chunk state with the attention model, which extracts the source context for the current phrasal scope. Then we generate multiple lexical words based on the same chunk state, which does not require attention operations. The boundary of a chunk is determined by a boundary gate, which decides whether to update the chunk state or not at each step.\nFormally, the probability of next word yt is\nP (yt|x) = softmax(f(eyt\u22121 , st, pt)) (6) st = g(st\u22121, eyt\u22121 , pt) (7)\nhere pt is the chunk state at step t. Compared with Equations 1 and 2, the generation of target word is based on the chunk state instead of the context vector ct produced by the attention model.\nSince a chunk may correspond to multiple words, we employ a boundary gate bt to decide the boundary of each chunk:\np(bt) = softmax(st\u22121, eyt\u22121) (8)\nbt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not. Two different operations would be executed:\npt = { pt\u22121, bt = 0 (COPY) g(pt\u22121, ept\u22121 , pct), bt = 1 (UPDATE)\nIn the COPY operation, the chunk state is kept the same as the previous step. In the UPDATE operation, ept\u22121 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016):\nept\u22121 = m(st\u22121, eyt\u22121)\u2212m(st\u2032 , eyt\u2032 ) (9)\nhere t\u2032 is the boundary of last chunk and m(\u00b7) is a linear function. pct is the context vector for chunk pt, which is calculated by a chunk attention model:\npct = Ts\u2211 j=1 ATT(pt\u22121, hj) \u00b7 hj (10)\nThe chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt\u22121 rather than word state st\u22121, and 2) it is only executed at boundary of each chunk rather than at each decoding step.\nIn this way, our model only extracts source context once for a chunk, and the words in one chunk will share the same context for word generation. The chunk attention mechanism adds a constrain that target words in the same chunk shares the same source context.\nTraining To encourage the proposed model to learn reasonable chunk state, we add two additional objectives in training: Chunk Tag Prediction: For each chunk, we predict the probability of its tag P (lk|x) = softmax ( f(pt, ept , ct) ) , where lk is the syntactic tag of the k-th chunk such as NP (noun phrase) and VP (verb phrase), and t is time step of its boundary.\nChunk Boundary Prediction: At each decoding step, we predict the probability of chunk boundary P (bt|x) = softmax(st\u22121, eyt\u22121).\nAccordingly, given a set of training examples {[xn,yn]}Nn=1, the new training objective is\nJ(\u03b8, \u03b3) = argmax N\u2211\nn=1\n{ logP (yn|xn)\n+ logP (ln|xn) + logP (bn|xn) } (11)\nwhere ln and bn are chunk tag sequence and boundary sequence on yn, respectively."}, {"heading": "4 Experiments", "text": "We carry out experiments on a Chinese-English translation task. Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002).\nIn training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014)."}, {"heading": "4.1 Results on Chinese-English", "text": "We list the BLEU score of our proposed model in Table 1, comparing with Moses (Koehn et al., 2007) and dl4mt3 (Bahdanau et al., 2014), which are state-of-the-art models of SMT and NMT, respective. For Moses, we use the default configuration with a 4-gram language model trained on the target portion of the training data. For dl4mt, we also report the results (dl4mt-2) by using two\n23LDC2002E18, LDC2003E14, the Hansards portion of LDC2004T08, and LDC2005T06.\n3https://github.com/nyu-dl/ dl4mt-tutorial\ndecoder layers (Wu et al., 2016) for better comparison.\nAs shown in Table 1, our proposed model outperforms different baselines on all sets, which verifies that the chunk-based bi-scale decoder is effective for NMT. Our model gives a 1.6 BLEU score improvement upon the standard NMT baseline (dl4mt). We conduct experiment with dl4mt-2 to see whether the neural NMT system can model the bi-scale components with different varying speeds automatically. Surprisingly, we find that dl4mt-2 obtains lower BLEU scores than dl4mt. We speculate that the more complex model dl4mt2 may need more training data for obtaining reasonable results.\nEffectiveness of Chunk Attention As described in Section 3, we propose to use the chunk attention to replace the word level attention in our model, in which the source context extracted by the chunk attention will be used for the corresponding word generations in the chunk. We also report the result of our model using conventional word attention for comparison. As shown in Table 2, our model with the chunk attention gives higher BLEU score than the word attention.\nIntuitively, we think chunks are more specific in semantics, thus could extract more specific source context for translation. The chunk attention could be considered as a compromise approach between encoding the whole source sentence into decoder without attention (Sutskever et al., 2014) and utilizing word level attention at each step (Bahdanau et al., 2014). We also draw the figure of alignments by chunk attention (Figure 2), from which we can see that our chunk attention model can well explore the alignments from phrases to words.\nPredictions of the Chunk Boundary and Chunk Label We also compute predicted accuracies of chunk boundaries and chunk labels on the autochunked development and testing data (Table 3). We find that the chunk boundary could be predicted well, with an average accuracy of 89%, which shows that our model could capture the phrasal boundary information in the translation process. However, our model could not predict chunk labels as well as chunk boundaries. We speculate that more syntactic context features should be added to improve the performance of predicting chunk labels.\nSubjective Evaluation Following Tu et al. (2016, 2017a,b), we also compare our model with the dl4mt baseline by subjective evaluation. Two human evaluators are asked to evaluate the translations of 100 source sentences randomly sampled from the test sets without knowing which system\nthe translation is translated by. The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, which are set to 1 when under or over translation errors occurs, otherwise set to 0.\nWe list the averaged scores in Table 5. We find that our proposed model improves the dl4mt baseline on both the translation adequacy and fluency aspects. Specifically, the over translation error rate drops by 6%, which confirms the assumption in the introduction that splitting the fast and slow varying components in different time-scales could help alleviate the over translation errors."}, {"heading": "4.2 Results on German-English", "text": "We evaluate our model on the WMT15 translation task from German to English. We find that our proposed chunk-based NMT model also obtains considerable accuracy improvements on GermanEnglish. However, the BLEU score gains are not as significant as on Chinese-English. We speculate that the difference between Chinese and English is larger than German and English. The chunk-based NMT model may be more useful for bilingual data with bigger difference."}, {"heading": "5 Related Work", "text": "NMT with Various Granularities. A line of previous work propose to utilize other granularities besides words for NMT. By further exploiting the character level (Ling et al., 2015; Costajussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the infor-\nmation inside the word and alleviate the problem of unknown words. While most of them focus on decomposing words into characters or sub-words, our work aims at composing words into phrases.\nIncorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results. Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages. Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate."}, {"heading": "6 Conclusion", "text": "We propose a chunk-based bi-scale decoder for neural machine translation, in which way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model outperforms the standard attention-based neural machine translation baseline. Future work includes abandoning labeled chunk data, adopting reinforcement learning to explore the boundaries of phrase automatically (Mou et al., 2016). Our code is released on https: //github.com/zhouh/chunk-nmt.\nAcknowledge\nWe would like to thank the anonymous reviewers for their insightful comments. We also thank Lili Mou for helpful discussion and Hongjie Ji, Zhenting Yu, Xiaoxue Hou and Wei Zou for their help in data preparation and subjective evaluation. This work was partially founded by the Natural Science Foundation of China (61672277, 71503124) and the China National 973 project 2014CB340301."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 263\u2013270.", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["R. Marta Costa-juss\u00e0", "R. Jos\u00e9 A. Fonollosa."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics,", "citeRegEx": "Costa.juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Associ-", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "Factored neural machine translation", "author": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": null, "citeRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Chunk-based decoder for neural machine translation", "author": ["Shonosuke Ishiwatari", "Jingtao Yao", "Shujie Liu", "Mu Li", "Ming Zhou", "Naoki Yoshinaga", "Masaru Kitsuregawa", "Weijia Jia."], "venue": "Proceedings of the 55th annual meeting of the Association for Compu-", "citeRegEx": "Ishiwatari et al\\.,? 2017", "shortCiteRegEx": "Ishiwatari et al\\.", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "arXiv preprint arXiv:1610.03017 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586 .", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Treeto-string alignment template for statistical machine translation", "author": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "International Conference on Learning Representations (ICLR). San Juan, Puerto Rico.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "D. Christopher Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Soft syntactic constraints for hierarchical phrased-based translation", "author": ["Yuval Marton", "Philip Resnik."], "venue": "ACL. pages 1003\u20131011.", "citeRegEx": "Marton and Resnik.,? 2008", "shortCiteRegEx": "Marton and Resnik.", "year": 2008}, {"title": "Coupling distributed and symbolic execution for natural language queries", "author": ["Lili Mou", "Zhengdong Lu", "Hang Li", "Zhi Jin."], "venue": "arXiv preprint arXiv:1612.02741 .", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Libin Shen", "Jinxi Xu", "Ralph M Weischedel."], "venue": "ACL. pages 577\u2013585.", "citeRegEx": "Shen et al\\.,? 2008", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational Linguistics, pages 1526\u2013", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Associa-", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Context gates for neural machine translation", "author": ["Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li."], "venue": "Transactions of the Association for Computational Linguistics 5:87\u201399.", "citeRegEx": "Tu et al\\.,? 2017a", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of AAAI 2017. pages 3097\u20133103.", "citeRegEx": "Tu et al\\.,? 2017b", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Graph-based dependency parsing with bidirectional lstm", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of ACL. volume 1, pages 2306\u20132315.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 2, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 24, "context": "Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al.", "startOffset": 126, "endOffset": 200}, {"referenceID": 10, "context": ", 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005).", "startOffset": 235, "endOffset": 269}, {"referenceID": 1, "context": ", 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005).", "startOffset": 235, "endOffset": 269}, {"referenceID": 0, "context": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 138, "endOffset": 181}, {"referenceID": 15, "context": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 138, "endOffset": 181}, {"referenceID": 12, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 4, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 3, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 14, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 11, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 19, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 20, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 6, "context": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", "startOffset": 196, "endOffset": 388}, {"referenceID": 9, "context": "However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007).", "startOffset": 133, "endOffset": 153}, {"referenceID": 5, "context": "Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars.", "startOffset": 60, "endOffset": 110}, {"referenceID": 19, "context": "Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars.", "startOffset": 60, "endOffset": 110}, {"referenceID": 0, "context": "Generally, neural machine translation system directly models the conditional probability of the translation y word by word (Bahdanau et al., 2014).", "startOffset": 123, "endOffset": 146}, {"referenceID": 28, "context": "In the UPDATE operation, ept\u22121 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016):", "startOffset": 113, "endOffset": 135}, {"referenceID": 18, "context": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002).", "startOffset": 76, "endOffset": 99}, {"referenceID": 27, "context": "The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 0, "context": "All the other settings are the same as in Bahdanau et al. (2014).", "startOffset": 42, "endOffset": 65}, {"referenceID": 9, "context": "We list the BLEU score of our proposed model in Table 1, comparing with Moses (Koehn et al., 2007) and dl4mt3 (Bahdanau et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 0, "context": ", 2007) and dl4mt3 (Bahdanau et al., 2014), which are state-of-the-art models of SMT and NMT, respective.", "startOffset": 19, "endOffset": 42}, {"referenceID": 24, "context": "The chunk attention could be considered as a compromise approach between encoding the whole source sentence into decoder without attention (Sutskever et al., 2014) and utilizing word level attention at each step (Bahdanau et al.", "startOffset": 139, "endOffset": 163}, {"referenceID": 0, "context": ", 2014) and utilizing word level attention at each step (Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 79}, {"referenceID": 12, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 3, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 14, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 11, "context": "By further exploiting the character level (Ling et al., 2015; Costajuss\u00e0 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al.", "startOffset": 42, "endOffset": 151}, {"referenceID": 19, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 20, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 6, "context": ", 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the information inside the word and alleviate the problem of unknown words.", "startOffset": 31, "endOffset": 113}, {"referenceID": 13, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 16, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 21, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al.", "startOffset": 93, "endOffset": 155}, {"referenceID": 23, "context": ", 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016).", "startOffset": 152, "endOffset": 176}, {"referenceID": 11, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly.", "startOffset": 94, "endOffset": 344}, {"referenceID": 11, "context": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results.", "startOffset": 94, "endOffset": 488}, {"referenceID": 5, "context": "Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages.", "startOffset": 0, "endOffset": 210}, {"referenceID": 17, "context": "Future work includes abandoning labeled chunk data, adopting reinforcement learning to explore the boundaries of phrase automatically (Mou et al., 2016).", "startOffset": 134, "endOffset": 152}], "year": 2017, "abstractText": "In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "creator": "LaTeX with hyperref package"}}}