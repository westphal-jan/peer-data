{"id": "1705.01462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Ternary Neural Networks with Fine-Grained Quantization", "abstract": "We propose a novel fine-grained quantization method for ternarizing pre-trained full precision models, while also constraining activations to 8-bits. Using this method, we demonstrate minimal loss in classification accuracy on state-of-the-art topologies without additional training. This enables a full 8-bit inference pipeline, with best reported accuracy using ternary weights on ImageNet dataset with no more than 50,000 nodes. The resulting dataset, using a linearization algorithm, produces a high level of classification accuracy on the surface of the dataset. We describe how this is constructed by combining data from the topologies in the general classification system, as shown below:", "histories": [["v1", "Tue, 2 May 2017 10:15:21 GMT  (647kb,D)", "https://arxiv.org/abs/1705.01462v1", null], ["v2", "Thu, 11 May 2017 09:19:55 GMT  (652kb,D)", "http://arxiv.org/abs/1705.01462v2", null], ["v3", "Tue, 30 May 2017 17:10:24 GMT  (675kb,D)", "http://arxiv.org/abs/1705.01462v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["naveen mellempudi", "abhisek kundu", "dheevatsa mudigere", "dipankar das", "bharat kaul", "pradeep dubey"], "accepted": false, "id": "1705.01462"}, "pdf": {"name": "1705.01462.pdf", "metadata": {"source": "CRF", "title": "Ternary Neural Networks with Fine-Grained Quantization", "authors": ["Naveen Mellempudi", "Abhisek Kundu", "Dheevatsa Mudigere", "Dipankar Das", "Bharat Kaul", "Pradeep Dubey"], "emails": [], "sections": [{"heading": null, "text": "We propose a novel fine-grained quantization (FGQ) method to ternarize pretrained full precision models, while also constraining activations to 8 and 4-bits. Using this method, we demonstrate minimal loss in classification accuracy on state-of-the-art topologies without additional training. We provide an improved theoretical formulation that forms the basis for a higher quality solution using FGQ. Our method involves ternarizing the original weight tensor in groups of N weights. Using N = 4, we achieve Top-1 accuracy within 3.7% and 4.2% of the baseline full precision result for Resnet-101 and Resnet-50 respectively, while eliminating 75% of all multiplications. These results enable a full 8/4-bit inference pipeline, with best reported accuracy using ternary weights on ImageNet dataset, with a potential of 9\u00d7 improvement in performance. Also, for smaller networks like AlexNet, FGQ achieves state-of-the-art results. We further study the impact of group size on both performance and accuracy. With a group size of N = 64, we eliminate \u2248 99% of the multiplications; however, this introduces a noticeable drop in accuracy, which necessitates fine tuning the parameters at lower precision. We address this by fine-tuning Resnet-50 with 8-bit activations and ternary weights at N = 64, improving the Top-1 accuracy to within 4% of the full precision result with < 30% additional training overhead. Our final quantized model can run on a full 8-bit compute pipeline using 2-bit weights and has the potential of up to 15\u00d7 improvement in performance compared to baseline full-precision models."}, {"heading": "1 Introduction", "text": "Today\u2019s deep learning models achieve state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1]. Mathematically, this involves solving a non-convex optimization problem with order of millions or more parameters. Solving this optimization problem - also referred to as training the neural network - is a compute-intensive process that, for current state-of-the-art networks, requires days to weeks. Once trained, the network evaluates a function on specific input data - referred to as inference. While the compute intensity for inference is much lower than that of training, owing to the fact that inference is done on a large number of input data, the total computing resources spent on inference is likely to dwarf those spent on training. The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11]. Most of these cases requires partial or full training of network in low precision. Training at low-precision allows for the network to implicitly learn the low precision representation (along with the inherent noise); however, it introduces significant resource overheads which can be prohibitive for many resource-constrained applications, specifically those involving edge devices.\nar X\niv :1\n70 5.\n01 46\n2v 3\n[ cs\n.L G\n] 3\n0 M\nay 2\nReducing precision for both weights and activations has significant power-performance implication on system design. Low-precision not only allows increasing compute density, but also reduce pressure on the memory sub-system. Most of the current solutions are focused on compressing the model [14, 16], going as low as binary weights, which allows storing the model on the limited on-chip local memory. However, activations (input) need to be fetched from external memory or I/O-device (camera). Fetching data contributes to majority of the system power consumption. Hence reducing the size of activations is essential for more efficient utilization of the available computational resources. There have been a few solutions [8, 9] using lower precision representation for activations, however they necessitate specialized hardware for efficient implementation. Further, with widespread adoption of deep learning across various applications, such as autonomous driving, augmented reality etc, there is an increased demand for inference tasks to be done on edge devices efficiently. To address both the aforementioned system and application requirements, there is a general trend to move towards a full lower precision inference pipeline [11]. This is evident from the advent of 8-bit and sub 8-bit hardware such as Google\u2019s TPU [11] and other main stream GPU1, CPU offerings. Further, there is also software support for 8-bit inference through popular frameworks such as TensorFlow, Theano and compute libraries like NVidia\u2019s TensorRT1.\nIn this paper, we focus on enabling a sub 8-bit inference pipeline by using ternary weights and 8/4-bit activations, with minimal or no re-training, and yet achieving near state-of-art accuracy. The rationale behind our approach is to carefully convert the full-precision weights to low-precision, such that the element-wise distance between full-precision and low-precision weights are small. Consequently, the low-precision weights remain in the neighborhood of pre-trained full-precision weights in the search space of network parameters, and we expect them to generalize in a similar manner, despite no re-training.\nWe summarize our contributions below:\n1. Based on an improved theoretical formulation, propose a novel fine-grained quantization (FGQ) method to convert pre-trained models to a ternary representation with minimal loss in test accuracy, without re-training.\n2. With ternary weights, achieve classification accuracy (Top-1) of 73.85% with 8-bit activations (2w-8a) and 70.69% with 4-bit activations (2w-4a), on the ImageNet dataset[3] using a pre-trained Resnet-101 model (no re-training). To the best of our knowledge, these are the highest reported accuracies in this category on ImageNet dataset[17].\n3. Demonstrate the general applicability of FGQ, with state-of-art results (2w-8a, 2w-4a) on smaller models such as Resnet-50 and Alexnet[12]. And also show the efficacy of using FGQ for (re)training at low precision.\n4. Study the performance-accuracy trade-off using different group sizes. For a group of N filters, we reduce the number of multiplications to one in everyN additions, thus significantly reducing computation complexity with a potential for up to 16\u00d7 improvement baseline full precision models.\nThe rest of the paper is organized as follows, Section2 discusses related work on ternary weights, low precision inference and contrast them with FGQ. Section3 describes the FGQ formulation and the theoretical basis for this method. This is followed by Section4, which includes experimental results and related discussion. Finally, in Section5 we conclude with summarizing the implications of of FGQ, our results and the future research directions."}, {"heading": "2 Related Work", "text": "Deep learning inference using low-precision weights and activations is a well-researched topic. Many researchers have experimented with custom data representations to perform deep learning tasks and have shown significant benefits over the general purpose floating point representation. [20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware. With this they show up to 4\u00d7 improvement over an aggressively tuned floating point implementation. [5] have\n1https://devblogs.nvidia.com/parallelforall/new-pascal-gpus-accelerate-inference-in-the-data-center/\ndone a comprehensive study on the effect of low precision fixed point computation for deep learning and have successfully trained smaller networks using 16-bit fixed point on specialized hardware. This suggests that fixed point representations are better suited for low(er) precision deep learning. There have also been recent efforts exploring 8-bit floating point representation [4], however such schemes have the additional overhead of reduced precision since the exponent is replicated for each value. Whereas with fixed point representation, using a single shared exponent improves capacity for precision. Typically for deep neural networks, with reduced bit-widths it is desired to preserve the numerical precision, since the loss in range can be augmented by the dynamic scaling of the shared exponent.\nCommonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6]. This can be prohibitive in applications which rely on using previously trained models. Such use cases are typical in many edge device deployments. To address such cases, FGQ is developed with motivation to be able to achieve state-of-art accuracies without any training and hence enabling direct use of pre-trained models. This requirement results in the quantization scheme being quite complex but making it more widely applicable and making it also easily usable for the former case - with training from scratch.\nMany of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4]. Using low precision also for activations is essential to realize the full power-performance benefits with using 2-bit (ternary) weights. The hardware needs to operate at the throughput that is close to the precision of the weights (i.e. 16\u00d7 better throughput compared to using 32-bit weights). This cannot be achieved (or would be very hard to achieve) when the activations are at full precision because streaming 32-bit activations from main memory at that rate requires much higher(16\u00d7) bandwidth and compute engine needs to be much wider to deliver the desired throughput. All of which will increase the area and power budget which is not desirable when designing for low-power edge devices. Hence, reducing the size of activations is essential for reducing the compute requirements at the edge. Using 8-bits and below for activations dramatically reduces the design requirements on the edge and opens up the possibility of achieving 16\u00d7 throughput improvement. [14, 16] propose low precision networks with binary weights, while retaining the activations in full precision. [14] use a stochastic binarization scheme, achieving state-ofart (SOTA) accuracies on smaller data-sets (MNIST, CIFAR10, SVHN). [16] demonstrate near-SOTA accuracies on the large ImageNet data-set using AlexNet topology. Further, they also demonstrate a variant with binary weights and activations, with all computations are simplified bit-count operations but with significant loss in accuracy. Lower precision for activations have also been used, [8] use 1-bit for both weights and activations for smaller networks. For larger Imagenet-class networks [9], use 2-bit activations and binary weights showing reasonable accuracies. However, both these [8, 9] use specialized data representation requiring custom hardware for efficient implementation. Other solutions such as [24], employ a more tailored approach with different precision for each - weights (1-bit), activations (2-bits) and gradients (6-bits); implemented with special-purpose hardware.\n[13] introduces a theoretical formulation for ternary weight network using a threshold based approach (symmetric threshold \u00b1\u2206) with one scaling factor for each layer. They provide an approximation to the optimal ternary representation assuming weights follow a Gaussian distribution. However, one scaling factor per layer may not approximate the full-precision network well as the model capacity is very limited in this case. To increase model capacity [25] modify this solution to use two symmetric thresholds (\u00b1\u2206) and two scaling factors (separately for positive and negative weights). However, despite improving the accuracy this approach typically makes the inferencing inefficient by requiring multiple passes over the positive and negative values, hence increasing the bandwidth requirements. [23] have proposed a post-facto incremental quantization approach, which aims to find the optimal representation using an iterative method, constraining weights to either 0 or powers of 2, using a 5-bit representation. and re-training activations with full precision. All the aforementioned implementation require partial or full training of the network in low precision. Alternatively, [15] used log quantization method on pre-trained models and achieved good accuracy by tuning the bit length for each layer without re-training.\nAchieving near-SOTA accuracy on the Imagenet dataset with deeper networks [7], without any training in low precision (for both weights and activations) is still a challenge. Our work is an attempt to address this problem and improve over existing approaches."}, {"heading": "3 Ternary Conversion of Trained Network", "text": "Our goal is to convert the full-precision trained weights W to ternary values {\u2212\u03b1, 0,+\u03b1}, \u03b1 \u2265 0, without re-training. We use a threshold (\u2206 > 0) based approach similar to [13]: i-th element W\u0302i = sign(Wi), if |Wi| > \u2206, and 0 otherwise. Then, the element-wise error isE(\u03b1,\u2206) = \u2016W\u2212\u03b1W\u0302\u20162F and an optimal ternary representation \u03b1\u2217W\u0302\u2217 \u2248W is as follows:\n\u03b1\u2217,\u2206\u2217 = argmin \u03b1\u22650,\u2206>0 E(\u03b1,\u2206), s.t. \u03b1 \u2265 0,W\u0302i \u2208 {\u22121, 0,+1}, i = 1, 2, ..., n (1)\nwhere n is the size of W (W \u2208 Rn). We hypothesize that weights that learn different types of features may follow different distributions. Combining all the weights together represents a mixture of various distributions, and ternarizing them using a single threshold (\u2206) and magnitude (\u03b1) may not preserve the distributions of individual weights. Consequently, many weights are approximated poorly (if not totally pruned out) leading to loss of valuable information that they learn. We may not be able to compensate for this loss of information as we do not train the network in low precision.\nThis motivates us to use a fine-grained quantization technique involving multiple scaling factors in order to increase model capacity that can lead to better preservation of distributions learned by filters. Moreover, we hypothesize that positive and negative weight distributions are not always symmetric around the mean, further refinement of this solution maybe possible using two separate thresholds, \u2206p and \u2206n > 0, for positive and negative weights, respectively, along with a scaling factor \u03b1 to ternarize the weights."}, {"heading": "3.1 Our Formulation", "text": "Computing separate \u2206 and \u03b1 for each weight compensates for information loss and better preserves the underlying distributions. However, such solution, while showing significant improvement in accuracy, does not reduce the number of multiplications leading to a less efficient implementation. Therefore, we seek to find a trade-off between achieving higher accuracy and reducing the total number of multiplications. We propose a fine-grained quantization approach that creates groups of weights, and ternarizes each group independently. Let us consider the weights represented as a vector W \u2208 Rn. We partition the set I of n indices into k disjoint subsets, c1, c2, ..., ck, with cardinality |ci| = ni, such that, ci\u2229cj = \u2205, \u222aici = I , \u2211 i ni = n. We can decompose W into k orthogonal vectors W\n(i) \u2208 Rn, i = 1, ..., k, where j-th component W(i)j = Wj if j \u2208 ci, otherwise 0. Clearly, \u2211 i W\n(i) = W; then we ternarize each orthogonal component W(i) as \u03b1iW\u0302(i), where components of W\u0302(i) are in {\u22121, 0,+1}. Threshold-based pruning never turns 0 to non-zero, and the following orthogonality holds. W(i) \u22a5W(j),W\u0302(i) \u22a5 W\u0302(j),W(i) \u22a5 W\u0302(j), for i 6= j. It follows that, (W(i)\u2212\u03b1iW\u0302(i)) \u22a5 (W(j) \u2212 \u03b1jW\u0302(j)), for i 6= j, and we have \u2016W \u2212 \u2211 i \u03b1iW\u0302 (i)\u20162F = \u2211 i \u2016W\n(i) \u2212 \u03b1iW\u0302(i)\u20162F . Then, for a given group of k filters {W(i)}, i = 1, ..., k, and W\u0302(i)j \u2208 {\u22121, 0,+1},\u2200j,\n\u03b1\u22171, .., \u03b1 \u2217 k,W\u0302 (1)\u2217, ..,W\u0302(k)\u2217 = argmin \u03b1i,W\u0302(i) \u2016W \u2212 \u2211 i \u03b1iW\u0302 (i)\u20162F ,= \u2211 i argmin \u03b1i,W\u0302(i) \u2016W(i) \u2212 \u03b1iW\u0302(i)\u20162F (2)\nTherefore, we need to solve k independent sub-problems. This formulation allows a better ternary approximation to the original full-precision weights, ensuring that they remain within a neighborhood of the original solution in the complex search space of parameters, despite no re-training. Consequently, we expect the full-precision solution and the ternary counterpart to generalize in a similar manner. From model capacity point of view, we can have only three distinct values, {\u2212\u03b1, 0,+\u03b1}, for a ternary weight vector without such grouping. With k such groups, however, we can represent 2k + 1 distinct values, thus increasing model capacity linearly with number of groups.\nWe can solve each sub-problem using a threshold-based approach the following way. We are given a vector of elements W \u2208 Rn, and we can use separate thresholds for positive and negative weights, along with one scaling factor \u03b1 to ternarize W. Let I+\u2206 = {i : Wi > \u2206p}, I \u2212 \u2206 = {i : Wi < \u2212\u2206n}, for some \u2206p,\u2206n > 0. We want to solve\n\u03b1\u2217,\u2206\u2217p,\u2206 \u2217 n = argmin \u03b1\u22650,\u2206p>0,\u2206n>0 \u2016W \u2212 \u03b1W\u0302\u20162F , s.t. W\u0302i \u2208 {\u22121, 0,+1}, i = 1, ..., n. (3)\nWe have the following analytical solution.\n\u2206\u2217p,\u2206 \u2217 n = argmax\n\u2206p>0,\u2206n>0\n( \u2211 i\u2208I+\u2206 |Wi|+ \u2211 i\u2208I\u2212\u2206 |Wi|)2\n|I+\u2206 |+ |I \u2212 \u2206 |\n, \u03b1\u2217 =\n\u2211 i\u2208I+\u2206 |Wi|+ \u2211 i\u2208I\u2212\u2206\n|Wi| |I+\u2206 |+ |I \u2212 \u2206 | (4)\nNote that for \u2206p = \u2206n = \u2206, I\u2206 = I+\u2206 \u222a I \u2212 \u2206 , and (4) reproduces the formulation in ([13]).\n\u03b1\u2217 = ( \u2211 i\u2208I\u2206 |Wi|)/|I\u2206|, \u2206\u2217 = argmax \u2206>0 ( \u2211 i\u2208I\u2206 |Wi|)2/|I\u2206| (5)\nThe advantage of our formulation (2) is that the smaller independent sub-problems can be solved efficiently using brute-force methods to achieve better approximation. However, we also explore analytical solutions to establish the theoretical veracity of our approach. Assuming that the magnitude of the learned weights follow exponential distribution with parameter \u03bb, we analytically derive the optimal \u2206\u2217 from the following lemma.\nLemma 1. Using above notations, if |Wi| \u223c exp(\u03bb), then, \u2206\u2217 in (5) is \u2206\u2217 \u2248 1/\u03bb = \u2211 i |Wi|/n\nFrom this analysis, we see the need for a higher threshold value to prune larger number of smaller elements. This is intuitive from the shape of the model distributions, which are typically heavy-tailed distributions. In reality, however, it may not be appropriate to use a single distribution to model the weights of all the layers of a neural network. We can apply KolmogorovSmirnov (K-S) test as a goodness-of-fit measure to identify an appropriate reference distribution (here we choose between Gaussian and exponential), and find \u2206\u2217 accordingly. We approximate a heavy-tailed distribution by an exponential one by pruning out some of the smaller elements. This gives us an exponential approximation with smaller \u03bb. Further, we can use maximum likelihood functions to estimate the parameters of such distributions. For Gaussian N (0, \u03c3), estimated \u03c3\u0302 = \u221a\u2211n\ni=1 |Wi|2/n = rms(W), and for exponential case, estimated parame-\nter \u03bb\u0302 = \u2211n i=1 |Wi|/n. Based on such refined analysis, we observe significant improvement in the theoretical ternary error over Gaussian assumption of [13] (Figure 1). It is interesting to observe that for earlier convolution layers of ResNet-101 trained on ImageNet, the magnitude of the weights follow exponential distribution, and later layer weights are Gaussian."}, {"heading": "3.2 Weight Grouping", "text": "Our method (2) is agnostic to how the (full-precision) weights are grouped , but leverages that consequence of grouping - which allows for solving these as independent sub-problems more efficiently. The specifics of the grouping mechanism and memory layout used for accessing these groups of weights is an independent problem to explore. The primary objective of grouping is to minimize the dynamic range within each group and split the weights in such a way that the smaller groups have a uniform distribution. This helps in reducing the complexity of finding an optimal solution (\u03b1) for each independent sub-problem using either analytical or brute-force techniques.\nHowever, to realize the full performance potential of ternarization, it is essential to ensure that the grouping mechanism itself does not introduce a significant overhead. Similarity based clustering algorithms such as K-means, despite being better at finding optimal grouping of weights that may even lead to better accuracy, are not friendly for efficient implementations (in both software hardware), because of the random grouping of elements from non-contiguous memory locations. This leads to irregular memory accesses with longer latencies, to gather arbitrarily grouped weights that use a common \u03b1, for a partial accumulation of the output.\n= = , , , \u2026 , \u221d = \u221d = \u221d , \u2026 , = 1. . , = 1\u2026 \u00d7\n\u221d\nelements are accessed from contiguous memory locations. Since the elements along C accumulate to the same output feature, this layout is also amenable to efficient vectorization along K. Figure 2 shows an example of this grouping scheme applied to 3\u00d7 3 filters. Each group of N 3\u00d7 3 ternary filters, has 3\u00d7 3 scaling factors (\u03b1) corresponding to each element of the filter.\n4 Experimental Results\nFor experimental results, we focused on Resnet50 and Resnet-101[7] using ILSVRC-2012[3] dataset, to demonstrate the efficacy of our method on large, sophisticated models using 2-bit weights and 8-bit activations (2w-8a). We extended our study by applying FGQ on activations to help further reduce the precision of activations to 4-bits (2w-4a) and show results comparable with 8-bit activations for all the tested networks. Further, towards establishing the broader applicability of FGQ we demonstrate state-of-the-art accuracy also for Alexnet[12].\nOur setup consists of a modified version of Caffe[10] that emulates low-precision dynamic fixed point (DFP2) computations described in Fig. 3. We use 32-bit accumulator for all our low precision computations to minimize the chances of an overflow. We split the pre-trained weights into groups of N elements using the mechanism\ndescribed in section3.2, and use brute-force technique to compute the floating point values of the threshold (\u2206) and scaling-factor (\u03b1) for each group. The scaling factors are then quantized to a 8/4-bit fixed point and the weights are stored in the memory format described in3.2. The activations are quantized to 8/4-bits before performing convolution operation and the 32-bit outputs are down converted to 8-bit and appropriately rounded before they are passed to the next layer.\nOur experiments indicate that it is essential to use higher precision of the first layer (8w-8a), to minimize the accumulation of quantization loss. We also observe that using pre-trained parameters in batch normalization layers leads to a loss of accuracy due to shift in variance introduced by the quantization. We prevent this loss by recomputing batch normalization parameters during the inference phase to compensate for the shift in variance.\nWe explored the accuracy-performance trade-off using different group sizes of N , our experiments show that FGQ with a group size of N=4 (FGQ-N4) achieves highest accuracy with no re-training and a potential 9\u00d7 performance benefit. FGQ-N4 applied to a pre-trained Resnet-101 model with 2w-8a achieves Top-1 accuracy of 73.9%, which is within 4% of the full-precision results. With activations reduced to 4-bits (2w-4a), the Top-1 accuracy drops only marginally to 70.7%. FGQ-N4 performs equally well on Resnet-50, achieving with Top-1 accuracy of 70.8% with 2w-8a which is 4.2% off from full-precision result, and 68.4% with 2w-4a. To the best of our knowledge, these are\n2Please not that fixed point and dynamic fixed point are used interchangeably\nthe highest reported accuracies using 2w-8a and 2w-4a on Imagenet dataset[3] using state-of-the-art networks.\nTo understand the general applicability of our method to a wider range of networks, we apply FGQ to the smaller Alexnet[12] model. FGQ-N4 applied to a pre-trained Alexnet model, achieves 49% Top-1 accuracy with 2w-8a without any re-training, this is 8% away from the baseline full-precision result( 56.8%). With 2w-4a we do not see any further reduction in accuracy. There are no previously published results that can be directly compared to FGQ, which perform quantizaion on pre-trained models and work with end-to-end low-precision. Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision. Our Alexnet result using FGQ-N is comparable to previously published result[24] which is 6% away from the baseline using 1w-4a while also employing training in low-precision with full precision gradients. Table 1 has a comparison with previous reported results from[23] using 5-bit weights and [21] using ternary weights. While they report slightly better absolute numbers, our numbers are relatively better because both these results use full-precision activations and train the network in low precision to achieve those numbers. While without any low precision training and reduced precision for activation, results with FGQ is still competitive with other similar (aforementioned) results.3 With additional low precision training with FGQ we are able significantly improve accuracy and get closer to state-of-art full precision results, as outlined in the next section along with associated performance implications."}, {"heading": "4.1 Discussion", "text": "In order to realize the full performance potential of ternary networks, the inference platform needs to operate at the throughput that is close to the precision of the weights. This would increase the amount of memory bandwidth required to stream activations by 16\u00d7 and a compute engine that is much wider to deliver the desired compute throughput. Building such solution around full-precision activations would be prohibitive in terms of areas and power requirements, whereas it is more amenable to build such solution when the activations are 8 or 4-bits.\nFigure4a shows the performance Vs accuracy trade-off for Resnet-50[18] for a FGQ based 8-bit inference design. Our model projects the lower bound of the performance potential based on the percentage of FMA operations that can be converted into ternary accumulations at each group size N . In the ideal case, where N is equal to the total number of weights in the layer, the best case performance potential is 16\u00d7 compared to the baseline full-precision performance. For a group size of N=4, 75% of all FMA operations can be performed in ternary, Using slightly larger sub-groups of N=8 we can replace 87.5% of FMA operations with ternary while losing an additional 3.7% Top-1 accuracy. At group sizeN = 64,\u2248 99% of all FMA operations can be replaced by ternary accumulations, resulting in 15\u00d7 potential improvement in performance. But the performance comes at a cost of significant drop in accuracy. Using larger groups of weights results in a poor ternary approximation to the full-precision model. Consequently, the ternary solution moves away from the full-precision local optima and display different generalization behavior.\n3It should be noted that both these works use Resnet-50 with slight variations and hence have slightly different baseline accuracies. For [23] the baseline full precision a Top-1 accuracy is 73.22%, for [21] it is 76% and for [24] it is 55.9%\nWe have trained low-precision (2w-8a) ResNet-50[18] at group size N=64 on ImageNet[3] dataset to recover the accuracy lost because of ternarization. We initialized the network with a pre-trained full-precision model and fine-tuned the parameters of the low-precision network. We reduced the learning rate to an order of 1e-4 to avoid exploding gradients, retaining all the other hyper parameters from full-precision training and performed gradient updates in full precision. After training for 20-epochs, we recover most of the lost accuracy and achieved 71.1% Top-1 and 90.01% Top-5 bringing it to within 4% of the full-precision baseline accuracy. Figure4b shows the reduction of training error and improvements in validation accuracy."}, {"heading": "5 Conclusion", "text": "We propose a fine-grained ternarization method which exploits local correlations in dynamic range of the parameters to minimize the impact of quantization on overall accuracy. We demonstrate near SOTA accuracy on Imagenet data-set using pre-trained models with quantized networks without re-training. Using ternary weights on Resnet-101 and Resnet-50 with 8-bit activations our results are within\u2248 4% from the full precision (FP32) accuracy. Using 4-bit activations we see a further drop of\u2248 3% in accuracy. To the best of our knowledge, these are the highest reported accuracies using ternary weights and low-precision activations.\nOur weight grouping based approach allows us to obtain solutions that can be tailored for specific hardware, as well as, can be used on general purpose hardware, based on the accuracy and performance requirements. Smaller group sizes with N=4 achieve best accuracy, and use 75% of the computations ternary operations (simple 8-bit additions) and this is better suited for implementation on specialized hardware. Larger group sizes are more suitable for current general purpose hardware, with a larger portion of computations as low precision operations (\u2248 99% for N=64), although this comes with the cost of reduced accuracy. This gap may be bridged with additional low precision training as shown in Section4. Our final quantized model can be efficiently run on full 8-bit compute pipeline, thus offering a potential 16X performance-power benefit.\nWe continue to actively work on closing the current accuracy gap, exploring both low precision (re)training and extensions to the FGQ method itself. Also we are looking into a more theoretical exploration to better understand the formal relationship between the weight grouping and final accuracy, with an attempt to establish realistic bounds for given network-performance-accuracy requirements."}, {"heading": "5.1 Proof of Lemma 1", "text": "Let n denote the number of elements. Let f(x) = \u03bbe\u2212\u03bbx be the pdf of exponential distribution with parameter \u03bb > 0, and F (x) = 1\u2212 e\u2212\u03bbx be the cdf. Then,\n|I\u2206| \u2248 n \u222b x>\u2206 f(x)dx = n(1\u2212 F (\u2206)) = ne\u2212\u03bb\u2206\nFurthermore,\u2211 i\u2208I\u2206 |Wi| \u2248 n \u222b x>\u2206 xf(x)dx = n \u222b x>\u2206 (\u03bbe\u2212\u03bbx)xdx \u2248 n \u03bb (\u03bb\u2206 + 1) e\u2212\u03bb\u2206\nThen,\nG(\u2206) = ( \u2211 i\u2208I\u2206 |Wi|)2\n|I\u2206| =\nn\n\u03bb2 (1 + \u03bb\u2206)2e\u2212\u03bb\u2206\nG\u2032(\u2206) = n\n\u03bb2 (2\u03bb(1 + \u03bb\u2206)e\u2212\u03bb\u2206 \u2212 \u03bb(1 + \u03bb\u2206)2e\u2212\u03bb\u2206)\nG\u2032(\u2206) = 0\u21d2 \u2206 = 1\n\u03bb G\u2032\u2032(\u2206) \u2223\u2223\u2223 \u2206=1/\u03bb < 0 (maxima)\nTherefore,\n\u2206\u2217 = 1\n\u03bb =\n1\nn \u2211 i |Wi|"}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Yoshua Bengio", "Ian Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "8-bit approximations for parallelism in deep learning", "author": ["Tim Dettmers"], "venue": "arXiv preprint arXiv:1511.04561,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Binarized neural networks", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Google supercharges machine learning tasks with tpu custom chip", "author": ["N Jouppi"], "venue": "Google Blog, May,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["Daisuke Miyashita", "Edward H Lee", "Boris Murmann"], "venue": "arXiv preprint arXiv:1603.01025,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "In ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Imagenet pre-trained models with batch normalization", "author": ["Marcel Simon", "Erik Rodner", "Joachim Denzler"], "venue": "arXiv preprint arXiv:1612.01452v2,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Finn: A framework for fast, scalable binarized neural network inference", "author": ["Yaman Umuroglu", "Nicholas J Fraser", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "arXiv preprint arXiv:1612.07119,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["Ganesh Venkatesh", "Eriko Nurvitadhi", "Debbie Marr"], "venue": "arXiv preprint arXiv:1610.00324,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Dynamically scaled fixed point arithmetic", "author": ["Darrell Williamson"], "venue": "In Communications, Computers and Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Incremental network quantization: Towards lossless cnns with low-precision weights", "author": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "venue": "poster at International Conference on Learning Representations,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Today\u2019s deep learning models achieve state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 7, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 23, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 14, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 12, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 24, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 20, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 18, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 10, "context": "The large and somewhat unique compute requirements for both deep learning training and inference operations motivate the use of customized low precision arithmetic [9, 2, 8, 24, 15, 13] and specialized hardware to run these computations as efficiently as possible [5, 25, 21, 19, 11].", "startOffset": 264, "endOffset": 283}, {"referenceID": 13, "context": "Most of the current solutions are focused on compressing the model [14, 16], going as low as binary weights, which allows storing the model on the limited on-chip local memory.", "startOffset": 67, "endOffset": 75}, {"referenceID": 15, "context": "Most of the current solutions are focused on compressing the model [14, 16], going as low as binary weights, which allows storing the model on the limited on-chip local memory.", "startOffset": 67, "endOffset": 75}, {"referenceID": 7, "context": "There have been a few solutions [8, 9] using lower precision representation for activations, however they necessitate specialized hardware for efficient implementation.", "startOffset": 32, "endOffset": 38}, {"referenceID": 8, "context": "There have been a few solutions [8, 9] using lower precision representation for activations, however they necessitate specialized hardware for efficient implementation.", "startOffset": 32, "endOffset": 38}, {"referenceID": 10, "context": "To address both the aforementioned system and application requirements, there is a general trend to move towards a full lower precision inference pipeline [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "This is evident from the advent of 8-bit and sub 8-bit hardware such as Google\u2019s TPU [11] and other main stream GPU1, CPU offerings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "69% with 4-bit activations (2w-4a), on the ImageNet dataset[3] using a pre-trained Resnet-101 model (no re-training).", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "To the best of our knowledge, these are the highest reported accuracies in this category on ImageNet dataset[17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "Demonstrate the general applicability of FGQ, with state-of-art results (2w-8a, 2w-4a) on smaller models such as Resnet-50 and Alexnet[12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "[20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[20] have show that 8-bit dynamically scaled fixed point representation [22] can be used to speed up convolution neural networks using general purpose CPU hardware by carefully choosing right data layout, compute batching and using implementation optimized for target hardware.", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "[5] have", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "There have also been recent efforts exploring 8-bit floating point representation [4], however such schemes have the additional overhead of reduced precision since the exponent is replicated for each value.", "startOffset": 82, "endOffset": 85}, {"referenceID": 13, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 15, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 8, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 1, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 7, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 24, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 5, "context": "Commonly, low precision networks are designed to be trained from scratch, leveraging the inherent ability of network to learn the approximations introduced by low precision computations [14, 16, 9, 2, 8, 25, 6].", "startOffset": 186, "endOffset": 210}, {"referenceID": 22, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 20, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 5, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 14, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 15, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 13, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 3, "context": "Many of the recent reduced precision work, look at the low precision only for the weights while retaining the activations in full precision [23, 21, 6, 15, 16, 14, 4].", "startOffset": 140, "endOffset": 166}, {"referenceID": 13, "context": "[14, 16] propose low precision networks with binary weights, while retaining the activations in full precision.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[14, 16] propose low precision networks with binary weights, while retaining the activations in full precision.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[14] use a stochastic binarization scheme, achieving state-ofart (SOTA) accuracies on smaller data-sets (MNIST, CIFAR10, SVHN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] demonstrate near-SOTA accuracies on the large ImageNet data-set using AlexNet topology.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Lower precision for activations have also been used, [8] use 1-bit for both weights and activations for smaller networks.", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "For larger Imagenet-class networks [9], use 2-bit activations and binary weights showing reasonable accuracies.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "However, both these [8, 9] use specialized data representation requiring custom hardware for efficient implementation.", "startOffset": 20, "endOffset": 26}, {"referenceID": 8, "context": "However, both these [8, 9] use specialized data representation requiring custom hardware for efficient implementation.", "startOffset": 20, "endOffset": 26}, {"referenceID": 23, "context": "Other solutions such as [24], employ a more tailored approach with different precision for each - weights (1-bit), activations (2-bits) and gradients (6-bits); implemented with special-purpose hardware.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "[13] introduces a theoretical formulation for ternary weight network using a threshold based approach (symmetric threshold \u00b1\u2206) with one scaling factor for each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "To increase model capacity [25] modify this solution to use two symmetric thresholds (\u00b1\u2206) and two scaling factors (separately for positive and negative weights).", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "[23] have proposed a post-facto incremental quantization approach, which aims to find the optimal representation using an iterative method, constraining weights to either 0 or powers of 2, using a 5-bit representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Alternatively, [15] used log quantization method on pre-trained models and achieved good accuracy by tuning the bit length for each layer without re-training.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "Achieving near-SOTA accuracy on the Imagenet dataset with deeper networks [7], without any training in low precision (for both weights and activations) is still a challenge.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "We use a threshold (\u2206 > 0) based approach similar to [13]: i-th element \u0174i = sign(Wi), if |Wi| > \u2206, and 0 otherwise.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "Note that for \u2206p = \u2206n = \u2206, I\u2206 = I \u2206 \u222a I \u2212 \u2206 , and (4) reproduces the formulation in ([13]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "Based on such refined analysis, we observe significant improvement in the theoretical ternary error over Gaussian assumption of [13] (Figure 1).", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "Figure 3: Schematic describing our low precision experimental setup in Caffe, to emulate finegrained quantization (FGQ) with ternary weights and 8-bit activations For experimental results, we focused on Resnet50 and Resnet-101[7] using ILSVRC-2012[3] dataset, to demonstrate the efficacy of our method on large, sophisticated models using 2-bit weights and 8-bit activations (2w-8a).", "startOffset": 226, "endOffset": 229}, {"referenceID": 2, "context": "Figure 3: Schematic describing our low precision experimental setup in Caffe, to emulate finegrained quantization (FGQ) with ternary weights and 8-bit activations For experimental results, we focused on Resnet50 and Resnet-101[7] using ILSVRC-2012[3] dataset, to demonstrate the efficacy of our method on large, sophisticated models using 2-bit weights and 8-bit activations (2w-8a).", "startOffset": 247, "endOffset": 250}, {"referenceID": 11, "context": "Further, towards establishing the broader applicability of FGQ we demonstrate state-of-the-art accuracy also for Alexnet[12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Our setup consists of a modified version of Caffe[10] that emulates low-precision dynamic fixed point (DFP2) computations described in Fig.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Networks Our Baseline FGQ-N4 2w-8a FGQ-N4 2w-4a INQ 5w-32a [23] dLAC 2w-32a [21] DoReFa 1w-4a-32g [24]", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "the highest reported accuracies using 2w-8a and 2w-4a on Imagenet dataset[3] using state-of-the-art networks.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "To understand the general applicability of our method to a wider range of networks, we apply FGQ to the smaller Alexnet[12] model.", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 22, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 23, "context": "Hence, we compare with [21, 23, 24], which are the closest in terms of the networks used and/or the target precision.", "startOffset": 23, "endOffset": 35}, {"referenceID": 23, "context": "Our Alexnet result using FGQ-N is comparable to previously published result[24] which is 6% away from the baseline using 1w-4a while also employing training in low-precision with full precision gradients.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Table 1 has a comparison with previous reported results from[23] using 5-bit weights and [21] using ternary weights.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "Table 1 has a comparison with previous reported results from[23] using 5-bit weights and [21] using ternary weights.", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "Figure4a shows the performance Vs accuracy trade-off for Resnet-50[18] for a FGQ based 8-bit inference design.", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "For [23] the baseline full precision a Top-1 accuracy is 73.", "startOffset": 4, "endOffset": 8}, {"referenceID": 20, "context": "22%, for [21] it is 76% and for [24] it is 55.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "22%, for [21] it is 76% and for [24] it is 55.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "We have trained low-precision (2w-8a) ResNet-50[18] at group size N=64 on ImageNet[3] dataset to recover the accuracy lost because of ternarization.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "We have trained low-precision (2w-8a) ResNet-50[18] at group size N=64 on ImageNet[3] dataset to recover the accuracy lost because of ternarization.", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "We propose a novel fine-grained quantization (FGQ) method to ternarize pretrained full precision models, while also constraining activations to 8 and 4-bits. Using this method, we demonstrate minimal loss in classification accuracy on state-of-the-art topologies without additional training. We provide an improved theoretical formulation that forms the basis for a higher quality solution using FGQ. Our method involves ternarizing the original weight tensor in groups of N weights. Using N = 4, we achieve Top-1 accuracy within 3.7% and 4.2% of the baseline full precision result for Resnet-101 and Resnet-50 respectively, while eliminating 75% of all multiplications. These results enable a full 8/4-bit inference pipeline, with best reported accuracy using ternary weights on ImageNet dataset, with a potential of 9\u00d7 improvement in performance. Also, for smaller networks like AlexNet, FGQ achieves state-of-the-art results. We further study the impact of group size on both performance and accuracy. With a group size of N = 64, we eliminate \u2248 99% of the multiplications; however, this introduces a noticeable drop in accuracy, which necessitates fine tuning the parameters at lower precision. We address this by fine-tuning Resnet-50 with 8-bit activations and ternary weights at N = 64, improving the Top-1 accuracy to within 4% of the full precision result with < 30% additional training overhead. Our final quantized model can run on a full 8-bit compute pipeline using 2-bit weights and has the potential of up to 15\u00d7 improvement in performance compared to baseline full-precision models.", "creator": "LaTeX with hyperref package"}}}