{"id": "1705.02395", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow", "abstract": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier or algorithm (e.g., a model for classifier analysis), but it also offers some general insights into machine learning.\n\n\n\n\nIt is easy to build machine learning models and models using code that is written with regular coding and code that is used to generate real-world examples and models. However, it has a few limitations to the actual machine learning model, such as its lack of understanding of the general information about the underlying model and the number of samples generated by the model, the lack of data needed to generate and extract data, and a lack of code that can be written by hand. Also, the model does not rely solely on the training and validation methods. It also includes a number of special features, such as the ability to write code without ever having to make a change of the model. The fact that a lot of examples contain real-world examples of the problems in machine learning are limited to the actual training and validation methods that can be written to generate real-world examples, even if it is still a relatively short and simple task.\nIn contrast, the training and validation methods of some particular machine learning models can offer the best results. However, the train and validation methods of some particular machine learning models can offer the best results. For example, the training and validation methods of many other machine learning models can offer the best results. For example, a model with a number of models in the training and validation methods of some particular machine learning models can offer the best results. The training and validation methods of some particular machine learning models can offer the best results.\nThe training and validation methods of some particular machine learning models can offer the best results. However, the train and validation methods of some particular machine learning models can offer the best results. For example, the training and validation methods of some particular machine learning models can offer the best results. For example, a model with a number of models in the training and validation methods of some particular machine learning models can offer the best results. For example, a model with a number of models in the training and validation methods of some particular machine learning models can offer the best results. For example, a model with a number of models in the training and validation methods of some particular machine learning models can", "histories": [["v1", "Wed, 26 Apr 2017 20:47:36 GMT  (443kb,D)", "http://arxiv.org/abs/1705.02395v1", "Preprint of paper accepted for the Proc. of the 21st International Conference on Evaluation and Assessment in Software Engineering, 2017"]], "COMMENTS": "Preprint of paper accepted for the Proc. of the 21st International Conference on Evaluation and Assessment in Software Engineering, 2017", "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.LG cs.SE", "authors": ["markus borg", "iben lennerstad", "rasmus ros", "elizabeth bjarnason"], "accepted": false, "id": "1705.02395"}, "pdf": {"name": "1705.02395.pdf", "metadata": {"source": "CRF", "title": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow", "authors": [], "emails": ["markus.borg@ri.se", "iben.lennerstad@gmail.com", "firstname.lastname@cs.lth.se"], "sections": [{"heading": null, "text": "Keywords-text mining, classification, active learning, selftraining, human annotation.\nI. INTRODUCTION\nLarge datasets are key to successful machine learning and text mining. For example, applying natural language related machine learning to text at web scale [1] has enabled many of the advances in the last decade. It is well known that an algorithm that works well on small datasets might be beaten by simpler alternatives as more data are used for training [2]. However, while the web contains huge amounts of text, supervised learning requires annotated data \u2013 data that are hard to obtain.\nA common solution to acquire enough annotated data is crowdsourcing using services such as Amazon Mechanical Turk. The possibility to employ a massive, distributed, anonymous crowd of individuals to perform general humanintelligence micro-tasks for micro-payments has radically changed the way many researchers work [3]. However, when annotation requires more than general human intelligence, i.e., for non-trivial micro-tasks, such crowdsourcing solutions might not work. Annotation of developers\u2019 posts on Stack Overflow is an example of non-trivial classification for which successful crowdsourcing cannot be expected.\nActive Learning (AL) is a semi-automated approach to establish a training set. The idea is to reduce the overall human effort by focusing on annotating examples that maximize the gained learning, i.e., the examples for which the classifier is the most uncertain. AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6]. AL has also been used in several other fields of research, e.g., for creating large training sets for speech recognition and information extraction [7]. Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.\nIn this study, our target training set is Stack Overflow discussions on performance of software components. Our work is part of the ORION project, in which we aim at developing a decision-support system for software component selection [11]. One aspect under study is how to collect and store experiences from previous decisions [12]. The ORION project proposes collecting experiences from both internal and external sources, i.e., both from the company and from other organizations. In this paper, we address using machine learning to extract external experiences from the software engineering community by text mining Stack Overflow, the leading technical Q&A platform for software developers [13].\nWe report our experiences from using AL and an SVM classifier in a systematic way consisting of 16 iterations. Our findings show that not only the classifier is uncertain regarding the borderline cases \u2013 also the human annotators display limited agreement. Consequently, we stress that annotation criteria must continuously evolve during AL. Moreover, we suggest that AL with multiple annotators should be designed with partly overlapping iterations to enable detection of different interpretations. Finally, we demonstrate that self-training has the potential to improve classification accuracy.\nThe rest of the paper is organized as follows: Section II introduces background and related work, Section III presents the design of our study, and Section IV discusses our findings. Finally, we summarize our implications for future mining operations in Section V.\nar X\niv :1\n70 5.\n02 39\n5v 1\n[ cs\n.C L\n] 2\n6 A\npr 2"}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "Stack Overflow is the dominant technical Q&A platform for software developers, with 101 million monthly unique visitors (March 2017). The information available on Stack Overflow has been studied extensively in the software engineering community, mostly through text mining, but also through qualitative analysis. Fig. 1 shows an example of a Stack Overflow question with an answer, in which we highlight text chunks related to performance.\nTreude et al. investigated the type of questions asked and the quality of the answers and found that the information is particularly useful for code reviews and conceptual questions, and for novice developers [14]. Soliman et al. found that Stack Overflow contains information relevant to and useful for decisions within software architectural design, and have idenitified a list of words that may be used to automatically classify such information [15]. Topic modelling has been used to identify what topics that are discussed and relationships between these. In this way, Barua et al. identify a number of current trends within software development, e.g., that mobile app development is increasing faster than web development [13]. It is suggested that knowledge mined from Stack Overflow can be used to provide context-relevant hints in IDEs [16], [17] and for filtering out off-topic posts, e.g., in chat channels [18].\nAL is a semi-supervised machine learning approach in which a learning algorithm interactively queries the human to obtain labels for specific examples, typically the most difficult ones. The method for selecting examples to query should be optimized to maximize the gained learning. Uncertainty sampling is a simple technique that selects examples where the classifier is least certain on which label to apply [7]. This has the effect of separating the examples into two distinct groups and thus remove borderline cases, see the horizontal histograms in Fig. 2. AL enables a shift of focus from momentary data analysis to a process with a feedback loop [19], [20].\nWhen mining from crowdsourced data there are usually too many unlabelled examples to annotate them all manually.\nSemi-supervised learning are methods that use also remaining unlabelled examples to improve the classifier. Self-training (or bootstrap learning [21]) is one such method that extends the training set with the unlabelled examples classified with the highest degree of certainty. This complements AL with uncertainty sampling well, since it maximizes the available confident labels [7]. To the best of our knowledge, we present the first application of both AL and self-training for Stack Overflow mining."}, {"heading": "III. METHOD", "text": "We designed a study to evaluate AL when mining Stack Overflow. Fig. 3 shows an overview of the research design that consisted of a preparation step and two iterative training steps. In the preparation step, we downloaded the dataset used for the MSR Mining Challenge in 2015 containing 43,336,603 posts [22]. We extracted all that were tagged with \u2018performance\u2019 and at least one of the following tags: \u2018apache\u2019, \u2018nginx\u2019 or \u2018rails\u2019 \u2013 an attempt to get an initial dataset related to components we know well, resulting in 2,304 posts in total.\nPreparation To assist the manual annotation task, we developed a prototype tool integrating an SVM classifier from scikit-learn [23], i.e., the classifier finds the optimal hyperplane separating two categories of examples [24]. In our application, we trained an SVM classifier with n-grams as features (n=1-5) to separate Stack Overflow posts related to performance discussions of software components and other posts. We refer to the two categories as positive and negative examples, respectively.\nDuring the tool development, the first and second authors alternated annotating posts and evolving initial annotation criteria \u2013 note that this inital step was done without AL. In total, we annotated 970 posts (25.4% positive) and the criteria evolved into \u201ca positive post discusses the performance of a software component, rather than programming languages, the development environment, or measurements tools\u201d. While manually annotating the initially posts, we identified 67 additional component names that also had explicit Stack Overflow tags. We used this to extend our dataset, i.e., we complemented \u2018apache\u2019, \u2018nginx\u2019 or \u2018rails\u2019 with 67 new tags to obtain a larger\ndataset of Stack Overflow posts. In total we collected 15,287 Stack Overflow posts potentially related to performance of software components1.\nActive learning After the preparation, the first and second authors alternated manual annotation of the next 100 posts2 closest to the SVM hyperplane \u2013 we refer to each such annotation batch [25] as an AL iteration. For each iteration, we measured the classification accuracy complemented by precision, recall, and F1-score using 5-fold cross-validation. Furthermore, we calculated the distance from each post, both labelled and unlabelled, to the SVM hyperplane. We visualize the distribution of posts at different distances from the SVM hyperplane using histograms and beanplots.\nSelf-training We investigated self-training based on the second author\u2019s annotation activity (cf. \u2018Self-train. in Fig. 3) by adding unlabelled examples as if they were manually annotated. We explored extending the training set with different percentages of unlabelled data, corresponding to different distances to the SVM hyperplane. Our ambition was to identify a successful application of self-training, useful as a proof-ofconcept, rather than finding the optimal parameter settings for this particular case.\nHuman annotation To measure the uncertainty in classifying Stack Overflow posts close to the SVM hyperplane, we evaluated the inter-rater reliability of human annotators. The first and second author discussed experiences after each completed iteration, and the annotation criteria evolved. After 8 iterations, halfway into the study, we considered the criteria mature enough for evaluation. The criteria were then:\n\u201cA positive post (both questions and answers) addresses the performance of a specific software component (incl. frameworks, platforms, and libraries) that could be used to evolve a software-intensive system. Examples: database management systems (MySQL, Oracle, ..), content management systems (Drupal, Joomla, ..), web servers.\nA post is negative if it discusses performance of/from: \u2022 programming languages (e.g., Java, PHP) \u2022 operational environments (e.g., Windows, Linux)\n1Replication package: URL 2A reasonable annotation task that requires roughly 90 min.\n\u2022 development tools (e.g., compilers, IDEs, build systems.) \u2022 alternative detailed implementations (e.g., formulation of\nSQL queries, parsing of XML/JSON structures) \u2022 tweaking of components\nor if the post discusses components used to measure performance (e.g., JMeter, SQLTest). The exclusion criteria apply, unless such a discussion clearly originates in poor performance of a specific component\u201d.\nWe designed a hands-on annotation exercise during a research workshop with 12 senior software engineering researchers (cf. \u2018Group annotation\u2019 in Fig. 3). First, we introduced the exercise, showed some examples, and provided the above criteria. Second, everyone independently annotated 11 posts, printed on paper, during a 20 minute session. In total, 66 posts were distributed using pairwise assignment: two annotators per post, and each possible human pair represented once. Finally, we calculated Krippendorff\u2019s \u03b1 to assess inter-rater reliability, as recommended for difficult nominal tasks [26].\nAfter the group annotation, we discussed the outcome to better understand our differences. We continued the annotation activity, following the same process and expecting a growing shared understanding, until iteration 16. Once finished, we had 2,567 annotated posts (32.6% positive). To check our hypothesis of improving agreement, we randomly selected 50 posts among the already annotated (cf. \u2018Pair annotation\u2019 in Fig. 3). Again we calculated Krippendorff\u2019s \u03b1, both 1) between the first and second authors (referred to as A and B), and 2) between the new labels and the previous labels. For each post annotated differently, we quantified the certainty of the set label (1-5) and we provided a rationale."}, {"heading": "IV. RESULTS AND LESSONS LEARNED", "text": "Human annotation We begin this section by reporting on the inter-rater reliability. The results from our group annotation exercise after 8 iterations confirmed the challenge of annotating posts close to the SVM hyperplane. Despite annotation criteria that evolved during 8 AL-iterations, the 12 annotators obtained a Krippendorff\u2019s \u03b1 of 0.126 (37/66 shared labels, 56%) \u2013 a poor agreement. The first and second authors analyzed the discrepancies, along with posts for which there were agreement, without identifying any concrete patterns. The presence of borderline cases is obvious, but we hypothesized that the alignment between the first and second authors was stronger than within the whole group, and that it would continue improving during the remaining iterations.\nAfter 16 AL-iterations, we calculated the inter-rater reliability between A and B for a random sample of 50 previously annotated posts. The exercise yielded a Krippendorff\u2019s \u03b1 of 0.028 (29/50 shared labels, 58%), considerably lower than from the group exercise. We also calculated the inter-rater reliability against our previous annotations of the 50 posts, obtaining a Krippendorff\u2019s \u03b1 of 0.768 (18/20 shared labels, 90%), and 0.577 (24/30 shared labels, 80%) for A and B, respectively. Our results show that while our individual annotation remained stable over time, our shared view still differed after 16 iterations.\nIn most cases at least one of us was very uncertain, expressing a certainty level of 1 or 2, which means the post was more or less randomly labelled. More alarming, however, was that in several cases both annotators felt certain but used different labels. An analysis of the latter cases revealed that A was more inclusive regarding posts that related to implementation details and component tweaking, whereas B was more inclusive concerning quality attributes not necessarily related to performance. Furthermore, B did not include posts that could be interpreted as anecdotal experiences. We conclude that AL for text classification is difficult, even after annotating 2,674 posts with several intermediate discussions, our interrater reliability was low.\nActive learning Since our annotation criteria did not properly align our annotation activity, we hesitated to pool our training data. Instead, we trained three separate SVM classifiers using: 1) A data, 2) B data, and 3) A+B data \u2013 we refer to these as SVM A, SVM B, and SVM A+B, respectively. Note that we also split the training data from iteration 0 into either A or B, resulting in differently large initial training sets.\nFig. 4 shows the mean value from five runs of 5-fold crossvalidation for each iteration. The solid lines with markers show accuracy and F1-score for SVM A, the dashed lines with markers represent SVM B, and the solid lines without markers illustrate SVM A+B. Regarding accuracy, all three classifiers show similar behavior: The accuracy decreases as additional iterations are added, but the differences are minor. The curves do not resemble typical learning curves, instead they appear to stabilize between 0.7 and 0.8. We explain this by the posts annotated for iteration 0, i.e., clearly positive and negative examples were selected to span the document space, followed by nothing but borderline cases selected using AL. Looking at F1-score, SVM B and SVM A+B remain fairly stable around 0.5. On the other hand, SVM A improves considerably as more iterations are added. This is likely due to the distribution of examples in the small A iteration 0 training set, containing only 373 examples and a recall of only 0.18 \u2013 even adding borderline cases was useful in this case.\nFig. 5 depicts distances between annotated posts and the\nSVM hyperplanes (SVM A and SVM B) after the preparation step and after the final iterations. The vertical histograms show frequency distributions of posts with distances from the hyperplane on the y-axis, where the sign denotes positive and negative classifications, respectively. Moreover, the figure displays the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). We notice that as more posts are annotated, the distribution around the hyperplane increases, which is particularly evident for the true negatives. This shows that, from the perspective of the SVM classifiers, 16 AL iterations did not reduce the number of borderline posts.\nFig. 6 presents an analogous view for the unlabelled posts, also separating SVM A and SVM B. However, in this figure we show beanplots, i.e., the frequencies are mirrored on the y-axis. We also report the number of unlabelled posts on both sides of the hyperplanes (cf. |p|). SVM A suggests that there are 716 positive posts remaining in the set of set 13,745 posts, whereas SVM B gives 259 remaining positive posts \u2013 these figures reflect A\u2019s more inclusive interpretation of the annotation criteria. The goal of AL is to focus annotation efforts on borderline cases to create two clearly separated clusters of examples (cf. Fig. 2). This phenomenon is not obvious in the Fig. 6, although we observe that SVM B indeed has fewer negative examples close to the hyperplane, i.e., the beanplot close to 0 is thinner after iteration 16. The pattern for SVM A is less clear, and we aim at investigating this in future work by conducting additional iterations.\nSelf-training The rightmost part of Fig. 6 also illustrates how we evaluated self-training using data annotated by B. As depicted by the dashed horizontal line, we explored adding different fractions of the most confidently classified examples (cf. the white bars) to the training set, annotated with the label predicted by the classifier. As a proof-of-concept, we report our results from adding the following unlabelled examples: 1) 5% positive examples, 2) 50% negative examples, and 3) 5% positive examples, and 50% negative examples. These additions represent adding unlabelled examples farther away from the hyperplane than 1.76 on the positive side, and 0.88 on the negative side.\nTable I shows our results, compared to the baseline provided by iteration 16 without any self-training. Our results show that active learning combined with self-training can be used to improve an SVM classifier for Stack Overflow posts. Both adding positive and negative examples from the unlabelled examples can improve classification accuracy. We obtained\nthe best results when adding both types of data, resulting in improvements from the baseline corresponding to +4.3% accuracy, +10.3% precision, +6.7% recall, and +7.9% F1score.\nLimitations Finally, we briefly discuss two aspects of threats to validity. First, we stress that we have populated Table I by cherry-picking results from successful self-training runs. Most of our trial runs with self-training generated similar or worse results. Using an approach to semi-exhaustively evaluate different self-training settings, in total running about 50 experimental runs, Table I shows the best results we obtained. However, our work is not a case of publication bias as we aim only to exhibit the existence of a phenomenon [27] \u2013 a beneficial application of self-training when text mining software repositories. Most self-training settings might deteriorate the accuracy, and a more systematic approach to parameter tuning [28] would probably identify even better settings.\nSecond, the external validity [29] of our work is limited. AL might be better suited for other software engineering text annotation tasks with less human interpretation. It is probable that another set of annotators, guided by other annotation guidelines, would result in a different inter-rater reliability. As highlighted by Settles [25], while evolving annotation criteria is often a practical reality when applying AL, changes is a\nviolation of the basic stability assumption. We also cannot claim that self-training is beneficial to all types of text mining tasks in software engineering. What we can say, however, is that for our particular task of classifying Stack Overflow posts related to performance of software components, self-training yielded improvements \u2013 and that is enough to recommend further research."}, {"heading": "V. CONCLUSION AND IMPLICATIONS FOR FUTURE TEXT MINING", "text": "We explored using AL and an SVM classifier for Stack Overflow posts with two alternating annotators. The primary lesson learned is that AL and text mining appears to be a difficult combination, at least for short texts such as Stack Overflow posts. In contrast to image classification tasks3, human Stack Overflow annotators must interpret incomplete information presented with limited context \u2013 differences in annotations are inevitable. However, we argue that awareness of this intrinsic challenge of AL can be used to complement a traditional annotation process, i.e., AL can be used to identify the borderline cases that are worthwhile to discuss.\nBased on our experiences, we present two recommendations when using AL for text mining software repositories. First,\n3Please refer to Karen Zack\u2019s viral tweets, e.g., \u201cchihuahua or muffin?\u201d: http://ow.ly/zpF1308F7kK\nthe annotation criteria must continuously evolve, in parallel to the annotators\u2019 interpretation of them, in line with coding guidelines for qualitative research [29]. It is not enough to simply count the number of differing labels, instead qualitative analysis is needed to identify any potential systematic differences \u2013 before it is too late. Second, we suggest that AL settings with multiple annotators should be designed with partly overlapping iterations to enable early detection of discrepancies. The size of the labelled training set would increase at a slower rate with overlapping iterations, thus this must be balanced against the value of better annotator alignment. In future attempts with AL, we plan to initially design iterations with 25% overlap, and then gradually decrease it to 5% as consensus increases.\nBased on the second author\u2019s AL process, we evaluated complementing the training set using self-training. Our results are promising, we show that adding both positive and negative examples to the training set can increase the classification accuracy. In a semi-structured approach, we achieved improvements of 4.3% accuracy and 7.9% F1-score. We stress that our findings do not suggest that self-training generally is a good idea, rather our results constitute a proof-of-concept that self-training can be successfully combined with AL. Furthermore, we expect that further improvements from selftraining would be possible, and plan to conduct systematic parameter optimization as the next step [28]."}, {"heading": "ACKNOWLEDGMENT", "text": "The work is partially supported by a research grant for the ORION project (reference number 20140218) from The Knowledge Foundation in Sweden, the Wallenberg Autonomous Systems and Software Program (WASP), and the Industrial Excellence Center EASE - Embedded Applications Software Engineering4."}], "references": [{"title": "The Unreasonable Effectiveness of Data,", "author": ["F. Pereira", "P. Norvig", "A. Halevy"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation,", "author": ["M. Banko", "E. Brill"], "venue": "in Proc. of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Comparing Person- and Processcentric Strategies for Obtaining Quality Data on Amazon Mechanical Turk,", "author": ["T. Mitra", "C. Hutto", "E. Gilbert"], "venue": "in Proc. of the 33rd Annual ACM Conference on Human Factors in Computing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "CARIAL: Cost-Aware Software Reliability Improvement with Active Learning,", "author": ["B. Sun", "G. Shu", "A. Podgurski", "S. Ray"], "venue": "in Proc. of the 5th International Conference on Software Testing, Verification and Validation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An Adaptive Approach with Active Learning in Software Fault Prediction,", "author": ["H. Lu", "B. Cukic"], "venue": "Proc. of the 8th International Conference on Predictive Models in Software Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Defect Prediction between Software Versions with Active Learning and Dimensionality Reduction,", "author": ["H. Lu", "E. Kocaguneli", "B. Cukic"], "venue": "in Proc. of the 25th International Symposium on Software Reliability Engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Active Learning Literature Survey,", "author": ["B. Settles"], "venue": "University of Wisconsin- Madison, Tech. Rep. Computer Sciences Technical Report", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Combining Self Learning and Active Learning for Chinese Named Entity Recognition,", "author": ["Y. Lin", "C. Sun", "W. Xiaolong", "W. Xuan"], "venue": "Journal of Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "An Active Learning Framework for Hyperspectral Image Classification Using Hierarchical Segmentation,", "author": ["Z. Zhang", "E. Pasolli", "M. Crawford", "J.C. Tilton"], "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Annotating Handwritten Characters with Minimal Human Involvement in a Semi-supervised Learning Strategy,", "author": ["J. Richarz", "S. Vajda", "G. Fink"], "venue": "in Proc. of the International Conference on Frontiers in Handwriting Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "What are Developers Talking About? An Analysis of Topics and Trends in Stack Overflow,", "author": ["A. Barua", "S. Thomas", "A. Hassan"], "venue": "Empirical Software Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "How do Programmers Ask and Answer Questions on the Web?", "author": ["C. Treude", "O. Barzilay", "M. Storey"], "venue": "NIER track,\u201d in Proc. of the 33rd International Conference on Software Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Architectural Knowledge for Technology Decisions in Developer Communities: An Exploratory Study with StackOverflow,", "author": ["M. Soliman", "M. Galster", "A. Salama", "M. Riebisch"], "venue": "in Proc. of the 13th Working IEEE/IFIP Conference on Software Architecture,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Why, When, and What: Analyzing Stack Overflow Questions by Topic, Type, and Code,", "author": ["M. Allamanis", "C. Sutton"], "venue": "in Proc. of the 10th Working Conference on Mining Software Repositories,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Mining StackOverflow to Turn the IDE into a Self-confident Programming Prompter,", "author": ["L. Ponzanelli", "G. Bavota", "M. Di Penta", "R. Oliveto", "M. Lanza"], "venue": "in Proc. of the 11th Working Conference on Mining Software Repositories,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Mining StackOverflow to Filter out Offtopic IRC Discussion,", "author": ["S. Chowdhury", "A. Hindle"], "venue": "Proc. of the 12th Working Conference on Mining Software Repositories,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Software Intelligence: The Future of Mining Software Engineering Data,", "author": ["A. Hassan", "T. Xie"], "venue": "Proc. of the FSE/SDP Workshop on Future of Software Engineering Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Kocaganeli, \u201cThe Inductive Software Engineering Manifesto: Principles for Industrial Data Mining,", "author": ["T. Menzies", "C. Bird", "T. Zimmermann", "W. Schulte"], "venue": "in Proc. of the International Workshop on Machine Learning Technologies in Software Engineering,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods,", "author": ["D. Yarowsky"], "venue": "Proc. of the 33rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Mining Challenge 2015: Comparing and Combining Different Information Sources on the Stack Overflow Data Set,", "author": ["A. Ying"], "venue": "in Proc. of the 12th Working Conference on Mining Software Repositories,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "From Theories to Queries: Active Learning in Practice,", "author": ["B. Settles"], "venue": "Proc. of the JMRL Workshop on Active Learning and Experimental Design,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Mistakes and How to Avoid Mistakes in Using Intercoder Reliability Indices,", "author": ["G. Feng"], "venue": "Methodology: European Journal of Research Methods for the Behavioral and Social Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "The Role of Deliberate Artificial Design Elements in Software Engineering Experiments,", "author": ["J. Hannay", "M. Jorgensen"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "TuneR: A Framework for Tuning Software Engineering Tools with Hands-On Instructions in R,", "author": ["M. Borg"], "venue": "Journal of Software: Evolution and Process,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "For example, applying natural language related machine learning to text at web scale [1] has enabled many of the advances in the last decade.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "beaten by simpler alternatives as more data are used for training [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "intelligence micro-tasks for micro-payments has radically changed the way many researchers work [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "AL has been used for software fault prediction, successfully reducing the need for human intervention [4], [5], [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": ", for creating large training sets for speech recognition and information extraction [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Several studies show that AL can successfully be combined with self-training, which is a method to extend the training set by automatic labeling of a trained classifier [8], [9], [10], but the techniques have not previously been used for text mining Stack Overflow.", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "In this paper, we address using machine learning to extract external experiences from the software engineering community by text mining Stack Overflow, the leading technical Q&A platform for software developers [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "investigated the type of questions asked and the quality of the answers and found that the information is particularly useful for code reviews and conceptual questions, and for novice developers [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 12, "context": "found that Stack Overflow contains information relevant to and useful for decisions within software architectural design, and have idenitified a list of words that may be used to automatically classify such information [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 10, "context": ", that mobile app development is increasing faster than web development [13].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "[16], [17] and for filtering out off-topic posts, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16], [17] and for filtering out off-topic posts, e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": ", in chat channels [18].", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "Uncertainty sampling is a simple technique that selects examples where the classifier is least certain on which label to apply [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 16, "context": "AL enables a shift of focus from momentary data analysis to a process with a feedback loop [19], [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "AL enables a shift of focus from momentary data analysis to a process with a feedback loop [19], [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Self-training (or bootstrap learning [21]) is one such method that extends the training set with the unlabelled examples classified with the highest degree of certainty.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "This complements AL with uncertainty sampling well, since it maximizes the available confident labels [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "In the preparation step, we downloaded the dataset used for the MSR Mining Challenge in 2015 containing 43,336,603 posts [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": ", the classifier finds the optimal hyperplane separating two categories of examples [24].", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "Active learning After the preparation, the first and second authors alternated manual annotation of the next 100 posts2 closest to the SVM hyperplane \u2013 we refer to each such annotation batch [25] as an AL iteration.", "startOffset": 191, "endOffset": 195}, {"referenceID": 22, "context": "Finally, we calculated Krippendorff\u2019s \u03b1 to assess inter-rater reliability, as recommended for difficult nominal tasks [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "However, our work is not a case of publication bias as we aim only to exhibit the existence of a phenomenon [27] \u2013 a beneficial application of self-training when text mining software repositories.", "startOffset": 108, "endOffset": 112}, {"referenceID": 24, "context": "tuning [28] would probably identify even better settings.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "As highlighted by Settles [25], while evolving annotation criteria", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "Furthermore, we expect that further improvements from selftraining would be possible, and plan to conduct systematic parameter optimization as the next step [28].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator\u2019s work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training. Keywords-text mining, classification, active learning, selftraining, human annotation.", "creator": "LaTeX with hyperref package"}}}