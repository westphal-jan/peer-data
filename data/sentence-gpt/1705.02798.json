{"id": "1705.02798", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "Reinforced Mnemonic Reader for Machine Comprehension", "abstract": "Recently, several end-to-end neural models have been proposed for machine comprehension tasks. Typically, these models use attention mechanisms to capture the complicated interaction between the context and the query and then point the boundary of answer. To better point the correct answer, we introduce the Mnemonic Reader for machine comprehension tasks, which enhance the attention reader in two aspects: first, the user's experience, and second, the complexity of the query. In order to create this user-generated machine-readable database, we use an easy-to-use method to generate data for the user, the user's machine-readable query, and the user's machine-readable query. The Mnemonic Reader enables a simple, easy-to-use and easy-to-use interface for machine comprehension tasks and a variety of advanced technologies. The Mnemonic Reader can be accessed through the User interface for machine-readable queries. Here are a few examples of how the Mnemonic Reader can be used to generate, analyze, and create data for the user and the user.\n\n\n\n\n\n\n\nThe user's machine-readable query will automatically generate data for the user. It will automatically generate data for the user. The user will then turn the machine-readable query into a database of machine-readable queries. The user can also create, edit, and store information about the user, allowing it to process and manipulate the input data in a manner consistent with what is present on the machine, and in any case, how the data is processed. As the user is already prepared for the query, the user will start to process and store information about the user before it moves into the database.\n\nThe user will then decide which parameters and properties of the user will be included. A user will also select a randomizer for the user, and choose a randomizer for the query. The user will then decide which data will be included by adding a randomizer to the query. To make this choice, the user will select a randomizer as the starting point for the user and the user.\nThe user will then select a randomizer for the query. The user will then choose a randomizer for the query. The user will then select a randomizer for the query. The user will then select a randomizer for the query. The user will then choose a randomizer for the query.\nThe user will then choose a randomizer for the query. The user will then choose a randomizer for the query. The user will then choose a randomizer", "histories": [["v1", "Mon, 8 May 2017 09:43:05 GMT  (967kb)", "http://arxiv.org/abs/1705.02798v1", "9 pages, 4 figures"], ["v2", "Mon, 31 Jul 2017 08:38:05 GMT  (657kb,D)", "http://arxiv.org/abs/1705.02798v2", "9 pages, 5 figures"], ["v3", "Tue, 5 Sep 2017 14:17:27 GMT  (666kb,D)", "http://arxiv.org/abs/1705.02798v3", "8 pages, 2 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minghao hu", "yuxing peng", "xipeng qiu"], "accepted": false, "id": "1705.02798"}, "pdf": {"name": "1705.02798.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Minghao Hu", "Yuxing Peng", "Xipeng Qiu"], "emails": ["huminghao09@nudt.edu.cn", "pengyuxing@nudt.edu.cn", "xpqiu@fudan.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n02 79\n8v 1\n[ cs\n.C L\n] 8\nM ay\n2 01\n7\nRecently, several end-to-end neural models have been proposed for machine comprehension tasks. Typically, these models use attention mechanisms to capture the complicated interaction between the context and the query and then point the boundary of answer. To better point the correct answer, we introduce the Mnemonic Reader for machine comprehension tasks, which enhance the attention reader in two aspects. Firstly, we use a self-alignment attention to model the long-distance dependency among context words, and obtain query-aware and selfaware contextual representation for each word in the context. Second, we use a memory-based query-dependent pointer to predict the answer, which integrates both explicit and implicit query information, such as query category. Our experimental evaluations show that our model obtains the state-of-the-art result on the large-scale machine comprehension benchmarks SQuAD."}, {"heading": "1 Introduction", "text": "Benefiting from the rapid development of deep learning techniques (Goodfellow et al., 2016) and the large-scale benchmarks (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016), the endto-end neural based methods have achieved promising results on machine comprehension tasks. Among the existing methods, various attention mechanisms (Bahdanau et al., 2014) play a vital role to establish the interaction between the\n\u2217Most of this work was done while MHwas in Fudan University.\nquery and the context, which allows a differentiable neural model to focus on attentive parts of context document in term of the query.\nThe attention mechanism is often used in two modules: the interaction layer and the pointer layer.\n\u2022 In the interaction layer, the existing methods, such as Gated-Attention Reader\n(Dhingra et al., 2016), Multi-perspective Matching (Wang et al., 2016) and Dynamic Coattention Networks (Xiong et al., 2017), integrate the attentive information from query to construct the query-aware context representation. Such representation of each context word is expected to model its contextual information with respect to the entire context paragraph and the query. However, the existing models only adopt long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Chung et al., 2014) to model the contextual information of contexts, therefore they actually just model the local contextual information due to the long term dependency problem.\n\u2022 In the pointer layer, the pointer network (Vinyals et al., 2015) is used to predict the\nboundary of the answer. A neural pointer use attention mechanism again to assign a probability of being selected as start or end of an answer span to each word in context. However, most of the existing methods, such as Answer Pointer (Wang and Jiang, 2017) and Ruminating Reader (Gong and Bowman, 2017), focus on the query-aware context representation and use query-independent pointer vector to select the answer boundary. Intuitively, the pointer vector should be query\nsensitive since the answer prediction depends on the query information, such as query category. Besides, one-hop pointing procedure may fail to fully understand the query and predict the wrong span. Therefore the pointing mechanism should also be multi-hop for refining the answer span iteratively.\nIn this paper, we propose the Mnemonic Reader for machine comprehension tasks, which enhances the attention reader in two aspects. Firstly, we improve the contextual representation of each word in the context with a self-alignment attention. The self-alignment attention can model the longdistance contextual information, which cannot be captured by the LSTM or GRU. Thus, the representation of each word in the context should contain the contextual information of the whole context passage and the query. Second, we enhance the answer pointer by introducing a querysensitive pointing mechanism, which contains the implicit and explicit query information such as question type. Besides, we extend the one-hop answer pointer to a multi-hop architecture for continuing refining the answer span. The contributions of this paper can be summarized as follows.\n1. The proposed mnemonic reader is a strategy\nto remember key terms within a text with extra information of context and query, which is helpful for deeper comprehension of text.\n2. Specifically, we use two kinds of mnemonic\ninformation to improve attention reader: selfaware context representation for synthesizing contextual information and multi-hop querysensitive answer pointer for predicting the answer span iteratively.\n3. Our model obtains the state-of-the-art result\non the large machine comprehension benchmarks SQuAD. Our single model obtains an F1 of 79.21% while our ensemble model obtains an F1 of 81.69%."}, {"heading": "2 Related Work", "text": "Reading comprehension. The significant advance on reading comprehension has largely benefited from the availability of large-scale datasets. Richardson et al. (2013) released the MCTest data which was too small to train end-to-end neural models. Later large cloze-style datasets such as\nCNN/DailyMail (Hermann et al., 2015) and Childrens Book Test (Hill et al., 2016) were released, make it possible to solve RC tasks with deep neural architectures. The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a more recently released dataset with over 100k queries-context pairs. The biggest difference of SQuAD from previous cloze-style datasets is that the answer of SQuAD is a segment of text but not a single entity, which make it more difficult to solve.\nAttention mechanism. From the perspective of attention mechanism, previous works for RC can be coarsely divided into two groups: first-order attention and second-order attention. First-order attention means that the model computes the attention in only one direction. The most popular approach is using a single fix-sized query vector to attend to contexts, and the computed attention is then either used for fusing the context into a single vector (Hermann et al., 2015; Chen et al., 2016) or acting as a \u201dpointer\u201d (Vinyals et al., 2015) to indicate the position of the answer (Kadlec et al., 2016). Sordonif et al. (2016) produces the query vector and document vector by computing the first-order attention twice, followed by a multi-hop iteration.\nSecond-order attention means that the model computes the attention bidirectionally, which usually forms an alignment matrix containing similarity scores corresponding to all pairs of context words and query words. Cui et al. (2016) computes a query-level average attention based on the alignment matrix, which is then used to further compute a weighted sum of context-level attention. BiDAF (Seo et al., 2017) computes both the context-to-query attention and the query-tocontext attention by using second-order attention. In contrast to these models, Mnemonic Reader uses second-order attention not only between the context and the query, but also inside the context, which can model the long-distance contextual information.\nReasoning mechanism. Based on how models perform the inference in RC, previous works can be divided into another two groups: onehop reasoning and multi-hop reasoning. Onehop reasoning models use either first-order attention or second-order attention to emphasis relevant parts between the context and the query. The architecture of these models is quite shallow, usually containing only one interaction layer\n(Weissenborn et al., 2017; Zhang et al., 2017).\nMulti-hop reasoning models, on the other hand, try to simulate the phenomenon that human increase their understanding by reread contexts and queries. This is typically done by maintaining a memory state which incorporates the current information of reasoning with the previous information in the memory, called Memory Networks (Sukhbaatar et al., 2015; Kumar et al., 2015). ReasoNet (Shen et al., 2016) combines the memory network with reinforcement learning to dynamically determine when to stop reading. Dhingra et al. (2016) extends the attentionsum reader to multi-hop reasoning with a gating mechanism. Dynamic Coattention Networks (Xiong et al., 2017) utilizes a multi-hop pointing decoder to indicate the answer span iteratively. In contrast to these works, Mnemonic Reader contains a multi-hop query-sensitive pointing layer which is able to continue refining the answer span."}, {"heading": "3 Model", "text": "Mnemonic Reader consists of four basic layers: input layer, interactive alignment layer, self alignment layer and mnemonic pointing layer, which are depicted in Figure 1(a). Below we discuss these layers in more details. Input Layer: The input layer encodes each context and the corresponding query into distributed vector space. Interactive Alignment Layer: Given a pair of encoded vector representations, the interactive alignment layer retrieves information among them to produce a query-aware context representation. Self Alignment Layer: The self alignment layer aligns the query-aware context against itself to futher generate a self-aware context representation. Mnemonic Pointing Layer: The mnemonic pointing layer predicts the start and end positions of the answer in a multi-hop manner, by using a query-sensitive answer pointer."}, {"heading": "3.1 Input Layer", "text": "Embedding. In reading comprehension task, the context and query are both word sequences. The input layer is responsible for mapping each word x to its corresponding word embedding xw, which is typically done by using pre-trained word vectors. At a more low-level granularity, the input layer also embeds each word x by encoding their char-\nacter sequences with a convolutional neural network followed by max-pooling over time (Kim, 2014), resulting in a character-level embedding xc. Each word embedding x is then represented as the concatenation of character-level embedding and word-level embedding, denoted as x = [xw, xc] \u2208 R d, where d is the total dimensionality of wordlevel embedding and character-level embedding.\nSimilar to BiDAF (Seo et al., 2017), word embeddings of both query and context are further transformed by the Highway Network (Srivastava et al., 2015), resulting in two word embedding matrices: XQ = [x q 1 , x q 2 , ..., x q n] \u2208 Rd\u00d7n for the query and XC = [x c 1, x c 2, ..., x c m] \u2208 R d\u00d7m for the context, where n and m denote the length of query and context respectively.\nBesides the word embedding, we use a simple yet effective binary feature of exact matching as additional inputs. This binary feature indicates whether a word in context can be exactly matched to one query word, and vice versa for a word in query. We exclude those words which do not have specific meanings, such as \u201dthe\u201d, \u201da\u201d, \u201ddo\u201d, etc. Eq. 1 formally defines this feature:\nem q i = 1(\u2203j : x q i = x c j) emcj = 1(\u2203i : x c j = x q i ) (1)\nEncoding. In order to model the word sequence under its contextual information, we use a bidirectional long short-term memory network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to encode both the context and the query as follows:\nqi = BiLSTM([x q i ; em q i ]),\u2200i \u2208 [1, ..., n]\ncj = BiLSTM([x c j ; em c j ]),\u2200j \u2208 [1, ...,m] (2)\nwhere qi and cj \u2208 R 2h are the concatenated hidden states of two BiLSTMs for the i-th query word and the j-th context word, and h is the dimensionality of hidden state. Finally we obtain two encoding matrices: Q = [q1, q2, ..., qn] \u2208 R 2h\u00d7n for the query and C = [c1, c2, ..., cm] \u2208 R 2h\u00d7m for the context."}, {"heading": "3.2 Interactive Alignment Layer", "text": "The interactive alignment layer attends to the query and context simultaneously so as to capture the interaction between them, and finally generates a query-aware context representation D. Such representation is constructed by fusing query information into the context. More specifically, we\nfirst compute an alignment matrix A \u2208 Rn\u00d7m between the query and the context by Aij = qi\nT \u00b7 cj . The element Aij of matrix A indicates the similarity between the i-th query word and the j-th context word.\nFor each context word, we intend to find the most relevant query word by computing a softaligned query vector, and fuse it back to the context word. Let aj \u2208 R n denote the normalized attention distribution of query for the j-th context word, and Q\u0303:j \u2208 R 2h denote the corresponding soft-aligned query vector. The computation is defined in Eq. 3:\naj = softmax(A:j)\nQ\u0303:j = Q \u00b7 aj ,\u2200j \u2208 [1, ...,m] (3)\nHence Q\u0303 \u2208 R2h\u00d7m contains the attended query vectors for the entire context.\nTo get the query-aware context representation, we combine the encoded representation of context C and the attended query vectors Q\u0303 by using an effective heuristics, and feed them into a BiLSTM for aggregation:\nD = BiLSTM([C; Q\u0303;C \u25e6 Q\u0303;C \u2212 Q\u0303]) (4)\nwhere D \u2208 R2h\u00d7m contains the contextual information of context with respect to attended query\ninformation, \u201d\u25e6\u201d stands for element-wise multiplication and \u201d\u2212\u201d means the element-wise subtraction."}, {"heading": "3.3 Self Alignment Layer", "text": "The self alignment layer aligns the query-aware context representation D \u2208 R2h\u00d7m against itself to futher synthesize contextual information among context words. The insight is that the limited capability of recurrent neural networks make it difficult to model long-term dependencies. Hence, it is hard to answer questions that require synthesizing information from different part of contexts (Weissenborn et al., 2017). Fusing information between context words allows crucial contextual information to flow close to the correct answer.\nSimilar to the interactive alignment layer, we first compute a self alignment matrix B \u2208 Rm\u00d7m for the context:\nBij = 1(i 6= j)di T \u00b7 dj (5)\nwhere Bij indicates the similarity between the ith context word and the j-th context word, and the diagonal of alignment matrix B is set to be zero in case of the word being aligned with itself.\nNext, for each context word, we compute a selfaligned context vector D\u0303:j \u2208 R 2h in a similar way of Eq. 3. We first calculate the normalized context\nattention distribution bj \u2208 R m for the j-th context word through bj = softmax(B:j). Then the corresponding self-aligned context vector can be computed as D\u0303:j = D \u00b7 bj . Hence, D\u0303 \u2208 R 2h\u00d7m contains the attended context vectors for the entire query-aware context.\nFinally, we use the same heuristics to combine query-aware context representation D and selfaligned context vectors D\u0303, and aggregate them through another BiLSTM:\nT = BiLSTM([D; D\u0303;D \u25e6 D\u0303;D \u2212 D\u0303]) (6)\nwhere T \u2208 R2h\u00d7m is the self-aware context representation, and T:j is expected to contain contextual information about the j-th word with respect to word-specific context and query information."}, {"heading": "3.4 Mnemonic Pointing Layer", "text": "Unlike the cloze-style QA task in which the answer is a single token, complex QA tasks require a span of text as the final answer. The answer span (i, j) is usually decided by first predicting the start position i of answers and then the end position j, based on the implicit query representation:\np(i, j|C,Q) = ps(i|C,Q) \u00b7 pe(j, |i, C,Q) (7)\nMoreover, the explicit query category (i.e., who, when, where, and so on) is obviously a high-level abstraction of the expression of queries, providing additional clues for searching the answer. For example, a \u201dwhen\u201d query paies more attention on temporal information while a \u201dwhere\u201d query seeks for spatial information.\nTherefore, in order to utilize both the implicit and explicit query information, we propose a query-sensitive pointing mechanism to select an answer span. Firstly, the module attends over the self-aware context representation while taking into consideration a query-memory vector and a querycategory vector to produce the estimate of the start position and an evidence vector. Next, the evidence vector is used, alongside the previous querymemory vector, to update the new query-memory vector, which is further used to produce the estimate of the end position. A detailed visualization of the layer is shown in Figure 1(b). Query-sensitive representation. The querysensitive representation consists of two parts: the query-memory vector and the query-category vector. The query-memory vector ms \u2208 R2h for the\nstart position of the answer is initialized as an implicit query summary q\u0303 \u2208 R2h: ms = q\u0303. In our experiment we found that simply using the last hidden states of query representations Q as q\u0303 results in good performance.\nTo explicitly model the query category, we count the key word frequency of all queries and obtain top-9 most frequent query categories, which is similar in (Zhang et al., 2017): what, how, who, when, which, where, why, be, other, in which be stands for queries beginning with different forms of key word such as is, am and are. Each query category k is represented by a trainable embedding xvk \u2208 R dv , where dv is the dimensionality of query-category embedding. The query-category embedding is then fed into a twolayer feedforward neural network which produces a query-category vector v \u2208 R2h. Prediction of the answer span. The probability distribution for the start position ps(i) is computed by using a pointer network (Vinyals et al., 2015):\nsi = FN(\u03b1(T:i,m s, v)\nps(i) = softmax(wssi) (8)\nwhere the abbreviation FN means a feedforward neural network that provides a non-linear mapping of its input. ws \u2208 R h is a trainable weight vector, and \u03b1 is a effective heuristic function that fuses its input vectors, which is defined as: \u03b1(t,m, v) = [t;m; v; t \u25e6m; t \u25e6 v] \u2208 R10h\nThe normalized probability ps \u2208 Rm is also used as a first-order attention to aggregate information of candidate start positions, producing an evidence vector us \u2208 R2h for the start position: us = T \u00b7 ps. The evidence vector and the querymemory vector are fed into a memory update function G, to produce a new query-memory vector me \u2208 R2h for the end position of the answer: me = G(ms, us).\nFinally the probability distribution for the end position pe(j) is computed similarly as Eq. 8, by using me instead of ms:\nej = FN(\u03b1(T:j ,m e, v)\npe(j) = softmax(weej) (9)\nMemory update mechanism. The memory update function G accepts two inputs (the old querymemory vector min and the evidence vector u), and produces a new query-memory vector mout. The new query memory is not necessarily identical to the original query memory, but is allowed\nto retrieve correlative information from the evidence. In order to generate the output, the function G consists of two parts: composition and gate. The composition part produces a hidden state z \u2208 R2h which is a linear interpolation between the previous query memory and the evidence. The gate part generates an update gate f \u2208 R2h to control the composition degree to which the query memory is exposed. The new query-memory vector is calculated as follows:\nz = tanh(W z([min;u]) + b z)\nf = \u03c3(W z([min;u]) + b z)\nmout = f \u25e6min + (1\u2212 f) \u25e6 u (10)\nMulti-hop extension. In complex query-context pairs, there may exist several candidate answer spans. The naive one-hop reasoning may fail to fully understand the query and predict the wrong answer. Therefore, the above one-hop answer pointing mechanism can by nature be extended to a multi-hop arhitecture, which allows the model to recover from the wrong answer span. In other word, multi-hop reasoning can help refining the predicted span to the correct position.\nLet l denote the index of current hop and L denote the total number of hops. In the l-th hop, we obtain two estimated probability distributions: psl for the start position of the answer and pel for the\nend position. Next, we also utilize pel as a firstorder attention to aggregate information of candidate end positions, and generate an evidence vector uel \u2208 R 2h for the end position: uel = T \u00b7p e l . The query-memory vector for the start position in the next hop is then computed as: msl+1 = G(m e l , u e l ), and the following procedure for computing psl+1 and pel+1 is exactly the same as before. Such multihop extension allows the model to incorporate the information of both candidate start positions and end positions into the query memory, and therefore is helpful for rectifying the estimated probabilities. Finally the two probability distributions psL and p e L are outputted for computing the answer span."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Experimental Configuration", "text": "We use the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to evaluate our model. The SQuAD corpus totally contains more than 100k queries, which are manually annotated by crowdsourcing workers on 536 Wikipedia articles. The dataset includes 90k query-context tuples for training, 10k tuples for validation, and a large hidden test set. The answer to each query is a segment of text from the corresponding context. Two metrics, Exact Match (EM) and F1 score, are\nused to evaluate models.\nWe use the Adam optimizer (Kingma and Ba, 2014) for training, with a minibatch size of 64 and an initial learning rate of 0.0006. We will half the learning rate when meet a bad epoch. A dropout rate (Srivastava et al., 2014) of 0.2 is used for word embeddings, all BiLSTM layers, and all linear transofrmations to prevent overfitting. Word embeddings are initialized with 100 dimensional Glove vectors (Pennington et al., 2014) and remain fixed during training, while the size of char embedding is set to be 10 and 100 1D filters are used for CNN. Out of vocabulary words are randomly sampled from Gaussian distributions. The dimension of hidden states h is 150 and the size of query-category embedding v is 100. The number of hops in mnemonic pointing layer is set to be 2. We set the max length of document to be 600 so as to efficiently train the model."}, {"heading": "4.2 Overall Results", "text": "Table 1 shows the performance comparison of our models and other competing models on the official leaderboard of SQuAD. Our single Mnemonic Reader achieves an EM score of 69.86% and a F1 score of 79.21%. Since SQuAD is a very competitve machine comprehension benchmark, we also build an ensemble model which consists of 14 single models with the same architecture but initialized with different parameters. The answer span with the highest probability is choosen among all models for each query. Our ensemble model improves the EM score to 73.67%, and the F1 score to 81.69%, achieving competitive results on the official leaderboard."}, {"heading": "4.3 Ablation Results", "text": "In order to evaluate the individual contribution of each model component, we run an ablation\nstudy on the the SQuAD development set, which is shown in Table 2. The interactive alignment between context and query is most critical to performance as ablating it results a drop of nearly 5.5% on both metrics. The reason may be that similar semantics between query and context act as strong evidences to the correct answer span. The self alignment accounts for about 2% of performance degradation, which clearly shows the effectiveness of aligning context words against themselves. We conjecture that self alignment helps information better flow through each word, alleviating long-term dependency problem in long sequence such as the context. To evaluate the performance of mnemonic pointing layer, we replace the multi-hop query-sensitive answer pointer with the standard pointer network (Vinyals et al., 2015), where the self-aware context representation T alongside the query summary q\u0303 are fed into a feedforward neural network, outputting unnormalized probability distributions followed by the softmax function. The result shows that mnemonic answer pointing outperforms the pointer network by nearly 1.5% on both metrics.\nFurther ablation study shows that the simple exact match feature has positive effect on the overall performace, which has been demonstrated by Weissenborn et al.(2017). Finally, character embeddings have a notable influence on the perfor-\nmance which was already observed by Seo et al.(2017). We ran a simple statistics and found that 92.4% of answers in SQuAD are out-of-vocab (OOV) words. This phenomenon may explain the importance of character embeddings since character embeddings can better represent out-of-vocab (OOV) or rare words."}, {"heading": "4.4 Result Analysis", "text": "To better understand the performance of Mnemonic Reader, we first conduct some quantitative analysis on the SQuAD development set. A natural point of interests is to analyze how much performance Mnemonic Reader improves with multi-hop reasoning, therefore we change the number of hops in mnemonic pointing layer and compare their different performances, which is shown in Table 3. As we can see, the performances on both metrics are increased gradually\nas the number of hops enlarges. The model with 2 hops achieves the best performance. The larger number of hops could potentially result in overfitting on the training set, therefore harming the performance.\nNext we evaluate the performance of F1 score divided by frequent query types, which is shown in Figure 2. Here we use the state-of-the-art BiDAF single model as the baseline. Mnemonic Reader outperforms the baseline in every query category comfortably. The overall F1 score of our model is 79.1%, while the F1 of the reproduced BiDAF is 76.6%. Moreover, we can see that the F1 of \u201dwhen\u201d and \u201dwho\u201d queries are highest and those of the \u201dwhy\u201d query are the lowest. We conjecture that \u201dwhen\u201d and \u201dwho\u201d queries only require the model to focus on specific information in the context (i.e., temporal information for \u201dwhen\u201d and hominine information for \u201dwho\u201d), while the \u201dwhy\u201d query requires the model be capable of high-level reasoning ability.\nFigure 4 presents how the performance of two models degrades as the length of the answer increases. Although the performance of both models drops as the answer length increases, the performance of Mnemonic Reader is always better than BiDAF in both EM and F1 score.\nBesides the quantitative analysis, we also provide a qualitative inspection of mnemonic pointing layer to show its ability of iteratively refining the answer span, as is shown in Figure 3. In the first hop the probabilities of candidate answer spans such as \u201dDenver Broncos\u201d and \u201dCarolina Panthers\u201d are both high. But in the subsequent\nhop, the model adjusts the answer span, lowering the probability of \u201dCarolina\u201d and \u201dPanthers\u201d as start position and end position respectively. This demonstrates that the model is capable of gradually locating the correct span by incorporating information of candidate answers into the query memory."}, {"heading": "5 Conclusion", "text": "In this paper, we propose the Mnemonic Reader, an enhanced attention reader with mnemonic information such as self-aware representation and multi-hop query-sensitive pointer. Experiments on the large-scale SQuAD dataset showed that these mnemonic information leads to a significant performance improvement. On the test dataset, our single model obtains an F1 of 79.21% while our ensemble model achieves an F1 of 81.69%. For future work, we will introduce more useful mnemonic information to further augment the attention reader, such as common knowledge."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Reading wikipedia to answer opendomain questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1704.00051 .", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of NIPS.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.04423 .", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Ruminating reader: Reasoning with gated multi-hop attention", "author": ["Yichen Gong", "Samuel R. Bowman."], "venue": "arXiv preprint arXiv:1704.07415 .", "citeRegEx": "Gong and Bowman.,? 2017", "shortCiteRegEx": "Gong and Bowman.", "year": 2017}, {"title": "Deep learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."], "venue": "MIT Press.", "citeRegEx": "Goodfellow et al\\.,? 2016", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "Proceedings of NIPS", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading childrens books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of ICLR.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of ACL.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Lei Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing ankit", "author": ["Ankit Kumar", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Shimi Salant", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das", "Jonathan Berant."], "venue": "arXiv preprint arXiv:1611.01436 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Structural embedding of syntactic trees for machine comprehension", "author": ["Rui Liu", "Junjie Hu", "Wei Wei", "Zi Yang", "Eric Nyberg."], "venue": "arXiv preprint arXiv:1703.00572 .", "citeRegEx": "Liu et al\\.,? 2017", "shortCiteRegEx": "Liu et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domainmachine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hananneh Hajishirzi."], "venue": "Proceedings of ICLR.", "citeRegEx": "Seo et al\\.,? 2017", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "arXiv preprint arXiv:1609.05284 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordonif", "Phillip Bachmanf", "Yoshua Bengiog."], "venue": "arXiv preprint arXiv:1606.02245 .", "citeRegEx": "Sordonif et al\\.,? 2016", "shortCiteRegEx": "Sordonif et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research pages 1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Highway networks", "author": ["RupeshKumar Srivastava", "Klaus Greff", "Jurgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387 .", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Proceedings of NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Proceedings of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of ICLR.", "citeRegEx": "Wang and Jiang.,? 2017", "shortCiteRegEx": "Wang and Jiang.", "year": 2017}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Fastqa: A simple and efficient neural architecture for question answering", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."], "venue": "arXiv preprint arXiv:1703.04816 .", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "Proceedings of ICLR.", "citeRegEx": "Xiong et al\\.,? 2017", "shortCiteRegEx": "Xiong et al\\.", "year": 2017}, {"title": "Words or characters? fine-grained gating for reading comprehension", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "WilliamW. Cohen", "Ruslan Salakhutdinov."], "venue": "Proceedings of ICLR.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1610.09996 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Exploring question understanding and adaptation in neuralnetwork-based question answering", "author": ["Junbei Zhang", "Xiaodan Zhu", "Qian Chen", "Lirong Dai", "Si Wei", "Hui Jiang."], "venue": "arXiv preprint arXiv:1703.04617 .", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Benefiting from the rapid development of deep learning techniques (Goodfellow et al., 2016) and the large-scale benchmarks (Hermann et al.", "startOffset": 66, "endOffset": 91}, {"referenceID": 8, "context": ", 2016) and the large-scale benchmarks (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016), the endto-end neural based methods have achieved promising results on machine comprehension tasks.", "startOffset": 39, "endOffset": 104}, {"referenceID": 9, "context": ", 2016) and the large-scale benchmarks (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016), the endto-end neural based methods have achieved promising results on machine comprehension tasks.", "startOffset": 39, "endOffset": 104}, {"referenceID": 18, "context": ", 2016) and the large-scale benchmarks (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016), the endto-end neural based methods have achieved promising results on machine comprehension tasks.", "startOffset": 39, "endOffset": 104}, {"referenceID": 0, "context": "Among the existing methods, various attention mechanisms (Bahdanau et al., 2014) play a vital role to establish the interaction between the", "startOffset": 57, "endOffset": 80}, {"referenceID": 5, "context": "\u2022 In the interaction layer, the existing methods, such as Gated-Attention Reader (Dhingra et al., 2016), Multi-perspective Matching (Wang et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 28, "context": ", 2016), Multi-perspective Matching (Wang et al., 2016) and Dynamic Coattention Networks (Xiong et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 30, "context": ", 2016) and Dynamic Coattention Networks (Xiong et al., 2017), integrate the attentive information from query to construct the query-aware context representation.", "startOffset": 41, "endOffset": 61}, {"referenceID": 10, "context": "However, the existing models only adopt long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Chung et al.", "startOffset": 78, "endOffset": 112}, {"referenceID": 3, "context": "However, the existing models only adopt long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Chung et al., 2014) to model the contextual information of contexts, therefore they actually just model the local contextual information due to the long term dependency problem.", "startOffset": 144, "endOffset": 164}, {"referenceID": 26, "context": "\u2022 In the pointer layer, the pointer network (Vinyals et al., 2015) is used to predict the boundary of the answer.", "startOffset": 44, "endOffset": 66}, {"referenceID": 27, "context": "However, most of the existing methods, such as Answer Pointer (Wang and Jiang, 2017) and Ruminating Reader (Gong and Bowman, 2017), focus on the query-aware context representation and use query-independent pointer vector to select the answer boundary.", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": "However, most of the existing methods, such as Answer Pointer (Wang and Jiang, 2017) and Ruminating Reader (Gong and Bowman, 2017), focus on the query-aware context representation and use query-independent pointer vector to select the answer boundary.", "startOffset": 107, "endOffset": 130}, {"referenceID": 8, "context": "Later large cloze-style datasets such as CNN/DailyMail (Hermann et al., 2015) and Childrens Book Test (Hill et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 9, "context": ", 2015) and Childrens Book Test (Hill et al., 2016) were released, make it possible to solve RC tasks with deep neural architectures.", "startOffset": 32, "endOffset": 51}, {"referenceID": 18, "context": "The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a more recently released dataset with over 100k queries-context pairs.", "startOffset": 48, "endOffset": 72}, {"referenceID": 16, "context": "Richardson et al. (2013) released the MCTest data which was too small to train end-to-end neural models.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "The most popular approach is using a single fix-sized query vector to attend to contexts, and the computed attention is then either used for fusing the context into a single vector (Hermann et al., 2015; Chen et al., 2016) or acting as a \u201dpointer\u201d (Vinyals et al.", "startOffset": 181, "endOffset": 222}, {"referenceID": 1, "context": "The most popular approach is using a single fix-sized query vector to attend to contexts, and the computed attention is then either used for fusing the context into a single vector (Hermann et al., 2015; Chen et al., 2016) or acting as a \u201dpointer\u201d (Vinyals et al.", "startOffset": 181, "endOffset": 222}, {"referenceID": 26, "context": ", 2016) or acting as a \u201dpointer\u201d (Vinyals et al., 2015) to indicate the position of the answer (Kadlec et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 11, "context": ", 2015) to indicate the position of the answer (Kadlec et al., 2016).", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": ", 2015; Chen et al., 2016) or acting as a \u201dpointer\u201d (Vinyals et al., 2015) to indicate the position of the answer (Kadlec et al., 2016). Sordonif et al. (2016) produces the query vector and document vector by computing the first-order attention twice, followed by a multi-hop iteration.", "startOffset": 8, "endOffset": 160}, {"referenceID": 20, "context": "BiDAF (Seo et al., 2017) computes both the context-to-query attention and the query-tocontext attention by using second-order attention.", "startOffset": 6, "endOffset": 24}, {"referenceID": 4, "context": "Cui et al. (2016) computes a query-level average attention based on the alignment matrix, which is then used to further compute a weighted sum of context-level attention.", "startOffset": 0, "endOffset": 18}, {"referenceID": 29, "context": "(Weissenborn et al., 2017; Zhang et al., 2017).", "startOffset": 0, "endOffset": 46}, {"referenceID": 33, "context": "(Weissenborn et al., 2017; Zhang et al., 2017).", "startOffset": 0, "endOffset": 46}, {"referenceID": 25, "context": "This is typically done by maintaining a memory state which incorporates the current information of reasoning with the previous information in the memory, called Memory Networks (Sukhbaatar et al., 2015; Kumar et al., 2015).", "startOffset": 177, "endOffset": 222}, {"referenceID": 14, "context": "This is typically done by maintaining a memory state which incorporates the current information of reasoning with the previous information in the memory, called Memory Networks (Sukhbaatar et al., 2015; Kumar et al., 2015).", "startOffset": 177, "endOffset": 222}, {"referenceID": 21, "context": "ReasoNet (Shen et al., 2016) combines the memory network with reinforcement learning to dynamically determine when to stop reading.", "startOffset": 9, "endOffset": 28}, {"referenceID": 30, "context": "Dynamic Coattention Networks (Xiong et al., 2017) utilizes a multi-hop pointing decoder to indicate the answer span iteratively.", "startOffset": 29, "endOffset": 49}, {"referenceID": 5, "context": "Dhingra et al. (2016) extends the attentionsum reader to multi-hop reasoning with a gating mechanism.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "At a more low-level granularity, the input layer also embeds each word x by encoding their character sequences with a convolutional neural network followed by max-pooling over time (Kim, 2014), resulting in a character-level embedding xc.", "startOffset": 181, "endOffset": 192}, {"referenceID": 20, "context": "Similar to BiDAF (Seo et al., 2017), word embeddings of both query and context are further transformed by the Highway Network (Srivastava et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 24, "context": ", 2017), word embeddings of both query and context are further transformed by the Highway Network (Srivastava et al., 2015), resulting in two word embedding matrices: XQ = [x q 1 , x q 2 , .", "startOffset": 98, "endOffset": 123}, {"referenceID": 10, "context": "In order to model the word sequence under its contextual information, we use a bidirectional long short-term memory network (BiLSTM) (Hochreiter and Schmidhuber, 1997) to encode both the context and the query as follows:", "startOffset": 133, "endOffset": 167}, {"referenceID": 29, "context": "Hence, it is hard to answer questions that require synthesizing information from different part of contexts (Weissenborn et al., 2017).", "startOffset": 108, "endOffset": 134}, {"referenceID": 33, "context": "To explicitly model the query category, we count the key word frequency of all queries and obtain top-9 most frequent query categories, which is similar in (Zhang et al., 2017): what, how, who, when, which, where, why, be, other, in which be stands for queries beginning with different forms of key word such as is, am and are.", "startOffset": 156, "endOffset": 176}, {"referenceID": 26, "context": "The probability distribution for the start position p(i) is computed by using a pointer network (Vinyals et al., 2015):", "startOffset": 96, "endOffset": 118}, {"referenceID": 18, "context": "Logistic Regression Baseline (Rajpurkar et al., 2016) 40.", "startOffset": 29, "endOffset": 53}, {"referenceID": 32, "context": "Dynamic Chunk Reader (Yu et al., 2016) 62.", "startOffset": 21, "endOffset": 38}, {"referenceID": 31, "context": "95 Fine-Grained Gating (Yang et al., 2017) 62.", "startOffset": 23, "endOffset": 42}, {"referenceID": 27, "context": "33 Match-LSTM with Ans-Ptr (Boundary) (Wang and Jiang, 2017) 67.", "startOffset": 38, "endOffset": 60}, {"referenceID": 15, "context": "02 RaSoR (Lee et al., 2016) 69.", "startOffset": 9, "endOffset": 27}, {"referenceID": 29, "context": "69 FastQAExt (Weissenborn et al., 2017) 70.", "startOffset": 13, "endOffset": 39}, {"referenceID": 2, "context": "Document Reader (Chen et al., 2017) 70.", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": "35 Ruminating Reader (Gong and Bowman, 2017) 70.", "startOffset": 21, "endOffset": 44}, {"referenceID": 33, "context": "45 jNet (Zhang et al., 2017) 70.", "startOffset": 8, "endOffset": 28}, {"referenceID": 30, "context": "82 Dynamic Coattention Networks (Xiong et al., 2017) 71.", "startOffset": 32, "endOffset": 52}, {"referenceID": 28, "context": "38 Multi-Perspective Matching (Wang et al., 2016) 73.", "startOffset": 30, "endOffset": 49}, {"referenceID": 20, "context": "26 BiDAF (Seo et al., 2017) 73.", "startOffset": 9, "endOffset": 27}, {"referenceID": 16, "context": "53 SEDT+BiDAF (Liu et al., 2017) 73.", "startOffset": 14, "endOffset": 32}, {"referenceID": 21, "context": "69 ReasoNet (Shen et al., 2016) 75.", "startOffset": 12, "endOffset": 31}, {"referenceID": 18, "context": "We use the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) to evaluate our model.", "startOffset": 55, "endOffset": 79}, {"referenceID": 13, "context": "We use the Adam optimizer (Kingma and Ba, 2014) for training, with a minibatch size of 64 and an initial learning rate of 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 23, "context": "A dropout rate (Srivastava et al., 2014) of 0.", "startOffset": 15, "endOffset": 40}, {"referenceID": 17, "context": "Word embeddings are initialized with 100 dimensional Glove vectors (Pennington et al., 2014) and remain fixed during training, while the size of char embedding is set to be 10 and 100 1D filters are used for CNN.", "startOffset": 67, "endOffset": 92}, {"referenceID": 26, "context": "To evaluate the performance of mnemonic pointing layer, we replace the multi-hop query-sensitive answer pointer with the standard pointer network (Vinyals et al., 2015), where the self-aware context representation T alongside the query summary q\u0303 are fed into a feedforward neural network, outputting unnormalized probability distributions followed by the softmax function.", "startOffset": 146, "endOffset": 168}, {"referenceID": 29, "context": "Further ablation study shows that the simple exact match feature has positive effect on the overall performace, which has been demonstrated by Weissenborn et al.(2017). Finally, character embeddings have a notable influence on the perfor-", "startOffset": 143, "endOffset": 168}, {"referenceID": 20, "context": "mance which was already observed by Seo et al.(2017). We ran a simple statistics and found that 92.", "startOffset": 36, "endOffset": 53}], "year": 2017, "abstractText": "Recently, several end-to-end neural models have been proposed for machine comprehension tasks. Typically, these models use attention mechanisms to capture the complicated interaction between the context and the query and then point the boundary of answer. To better point the correct answer, we introduce the Mnemonic Reader for machine comprehension tasks, which enhance the attention reader in two aspects. Firstly, we use a self-alignment attention to model the long-distance dependency among context words, and obtain query-aware and selfaware contextual representation for each word in the context. Second, we use a memory-based query-dependent pointer to predict the answer, which integrates both explicit and implicit query information, such as query category. Our experimental evaluations show that our model obtains the state-of-the-art result on the large-scale machine comprehension benchmarks SQuAD.", "creator": "LaTeX with hyperref package"}}}