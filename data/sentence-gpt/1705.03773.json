{"id": "1705.03773", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "abstract": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "histories": [["v1", "Wed, 10 May 2017 13:55:53 GMT  (374kb,D)", "http://arxiv.org/abs/1705.03773v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["jiyuan zhang", "yang feng", "dong wang", "yang wang", "andrew abel", "shiyue zhang", "andi zhang"], "accepted": true, "id": "1705.03773"}, "pdf": {"name": "1705.03773.pdf", "metadata": {"source": "CRF", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "authors": ["Jiyuan Zhang", "Yang Feng", "Dong Wang", "Yang Wang", "Andrew Abel", "Shiyue Zhang", "Andi Zhang"], "emails": ["ml@pku.edu.cn,", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Classical Chinese poetry is a special cultural heritage with over 2,000 years of history and is still fascinating us today. Among the various genres, perhaps the most popular one is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulated rhythmical form (the last characters in the second and fourth lines must follow the same rhythm), and a required tonal pattern (tones of characters in some positions should satisfy a predefined regulation) (Wang, 2002). This genre flourished mostly in the Tang Dynasty, and so are often called \u2018Tang\n1Corresponding author: Dong Wang; RM 1-303, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China.\npoems\u2019. An example of a quatrain written by Wei Wang, a famous poet in the Tang Dynasty, is shown in Table 1.\nDue to the stringent restrictions in both rhythm and tone, it is not trivial to create a fully rule-compliant quatrain. More importantly, besides such strict regulations, a good quatrain should also read fluently, hold a consistent theme, and express a unique affection. Therefore, poem generation is widely recognized as a very intelligent activity and can be performed only by knowledgeable people with a lot of training.\nIn this paper we are interested in machine poetry generation. Several approaches have been studied by researchers. For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al., 2016a,c). Compared to\nar X\niv :1\n70 5.\n03 77\n3v 1\n[ cs\n.A I]\n1 0\nM ay\n2 01\n7\nprevious approaches (e.g., rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a).\nIn spite of these promising results, neural models suffer from a particular problem in poem generation, a lack of innovation. Due to the statistical nature of neural models, they pay much more attention to high-frequency patterns, whereas they ignore low-frequency ones. In other words, the more regular and common the patterns, the better the neural model is good at learning them and tends to use them more frequently at run-time. This property certainly helps to generate fluent sentences, but it is not always useful: the major value of poetry is not fluency, but the aesthetic innovation that can stimulate some unique feelings. This is particularly true for Chinese quatrains that are highly compact and expressive: it is nearly impossible to find two similar works in the thousands of years of history in this genre, demonstrating the importance of uniqueness or innovation. Ironically, the most important thing, innovation, is largely treated as trivial, if not noise, by present neural models.\nActually this problem is shared by all generation models based on statistics (although it is more serious for neural models) and has aroused a long-standing criticism for machine poem generation: it can generate, and sometimes generate well, but the generation tends to be unsurprising and not particularly interesting. More seriously, this problem exists not only in poem generation, but also in all generation tasks that require innovation.\nThis paper tries to solve this extremely challenging problem. We argue that the essential problem is that statistical models are good at learning general rules (usage of regular words and their combinations) but are less capable of remembering special instances that are difficult to cover with general rules. In other words, there is only rule-based reasoning, no instance-based memory. We therefore present a memory-augmented neural model which involves a neural memory so that special instances can be saved and referred to at run-time. This is like a human poet who creates poems by not only referring to common rules and patterns, but also recalls poems that he has read before. It is hard to say whether this combination of rules and instances produces true innovation\n(which often requires real-life motivation rather than simple word reordering), but it indeed offers interesting flexibility to generate new outputs that look creative and are still rule-compliant. Moreover, this flexibility can be used in other ways, e.g., generating poems with different styles.\nIn this paper, we use the memory-augmented neural model to generate flexible and creative Chinese poems. We investigate three scenarios where adding a memory may contribute: the first scenario involves a well trained neural model where we aim to promote innovation by adding a memory, the second scenario involves an over-fitted neural model where we hope the memory can regularize the innovation, and in the third scenario, the memory is used to encourage generation of poems of different styles."}, {"heading": "2 Related Work", "text": "A multitude of methods have been proposed for automatic poem generation. The first approach is based on rules and/or templates. For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al., 2009), template search (Oliveira, 2012), genetic search (Zhou et al., 2010), text summarization (Yan et al., 2013). Another approach involves various SMT methods, e.g., (Jiang and Zhou, 2008; He et al., 2012). A disadvantage shared by the above methods is that they are based on the surface forms of words or characters, having no deep understanding of the meaning of a poem.\nMore recently, neural models have been the subject of much attention. A clear advantage of the neural-based methods is that they can \u2018discover\u2019 the meaning of words or characters, and can therefore more deeply understand the meaning of a poem. Here we only review studies on Chinese poetry generation that are mostly related to our research. The first study we have found in this direction is the work by Zhang and Lapata (2014), which proposed an RNN-based approach that produces each new line character-by-character using a recurrent neural network (RNN), with all the lines generated already (in the form of a vector) as a contextual input. This model can generate quatrains of reasonable quality. Wang et al. (2016b) proposed a much simpler neural model that treats a poem as an entire character sequence, and poem generation is conducted character-by-character. This approach can be easily extended to various\ngenres such as Song Iambics. To avoid theme drift caused by this long-sequence generation, Wang et al. (2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation. Yan (2016) proposed a hierarchical RNN model that conducts iterative generation. Recently, Wang et al. (2016c) proposed a similar sequence generation model, but with the difference that attention is placed not only on the human input, but also on all the characters that have been generated so far. They also proposed a topic planning scheme to encourage a smooth and consistent theme.\nAll the neural models mentioned above try to generate fluent and meaningful poems, but none of them consider innovation. The memory-augmented neural model proposed in this study intends to address this issue. Our system was built following the model structure and training strategy proposed by Wang et al. (2016a) due to its simplicity and demonstrated quality, but the memory mechanism is general and can be applied to any of the models presented above.\nThe idea of memory argumentation was inspired by the recent advance in neural Turing machine (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). These new models equip neural networks with an external memory that can be accessed and manipulated via some trainable operations. In comparison, the memory in our work plays a simple role of knowledge storage, and the only operation is simple pre-defined READ. In this sense, our model can be regarded as a simplified neural Turing machine that omits training."}, {"heading": "3 Memory-augmented neural model", "text": "In this section, we first present the idea of memory augmentation, and then describe the model structure and training method."}, {"heading": "3.1 Memory augmentation", "text": "The idea of memory augmentation is illustrated in Fig. 1. It contains two components, the neural model component on the left, and the memory component on the right. In this work, the attention-based RNN generation model presented by (Wang et al., 2016a) is used as the neural model component, although any neural model is suit-\nable. The memory component involves a set of \u2018direct\u2019 mappings from input to output, and therefore can be used to memorize some special cases of the generation that can not be represented by the neural model. For poem generation, the memory stores the information regarding which character should be generated in a particular context. The output from the two components are then integrated, leading to a consolidated output.\nThere are several ways to understand the memory-augmented neural model. Firstly, it can be regarded as a way of combining reasoning (neural model) and knowledge (memory). Secondly, it can be regarded as a way of combining rule-based inference (neural model) and instance-based retrieval (memory). Thirdly, it can be regarded as a way of combining predictions from complementary systems, where the neural model is continuous and parameter-shared, while the memory is discrete and contains no parameter sharing. Finally, the memory can be regarded as an effective regularization that constrains and modifies the behavior of the neural model, resulting in generations with desired properties. Note that this memory-augmented neural model is inspired by and related to the memory network proposed by Weston et al.(2014) and Graves et al.(2016), but we more focus on an accompanying memory that plays the role of assistance and regularization."}, {"heading": "3.2 Model structure", "text": "Using the Chinese poetry generation model shown in Fig. 1 as an example, this section discusses the creation of a memory-augmented neural model. Firstly, the neural model part is an attentionbased sequence-to-sequence model (Bahdanau et al., 2014). The encoder is a bi-directional RNN (with GRU units) that converts the in-\nput topic words, denoted by the embeddings of the compositional characters (x1, x2, ..., xN ), into a sequence of hidden states (h1, h2, ..., hN ). The decoder then generates the whole quatrain character-by-character, denoted by the corresponding embeddings (y1, y2, ...). At each step t, the prediction for the state st is based on the last generation yt\u22121, the previous status st\u22121 of the decoder, as well as all the hidden states (h1, h2, ...) of the encoder. Each hidden state hi contributes to the generation according to a relevance factor \u03b1t that measures the similarity between st\u22121 and hi. This is written as:\nst = fd(yt\u22121, st\u22121, N\u2211 i=1 \u03b1t,ihi)\nwhere \u03b1t,i represents the contribution of hi to the present generation, and can be implemented as any function. The output of the model is a posterior probability over the whole set of characters, written by\nzt = \u03c3(stW )\nwhere W is the projection parameter. The memory consists of a set of elements {mi}Ki=1, whereK is the size of the memory. Each element mi involves two parts, the source part mi(s), that encodes the context, i.e. when this element should be selected, and the target partmi(g), that encodes what should be output if this element is selected. In our study, the neural model is firstly trained, and then the memory is created by running fd (the decoder of the neural model). Specifically, for the k-th poem selected to be in the memory, the character sequence is input to the decoder one by one, with the contribution from the encoder set to zero. Denoting the starting position of this poem in the memory is pk, the status of the decoder at the j-th step is used as the source part of the (pk+j)-th element of the memory, and the embedding of the corresponding character, xj , is set to be the target part. this is formally written as:\nmi(s) = fd(xj\u22121, sj\u22121, 0) (1)\nand mi(g) = xj\nwhere i = pk + j.\nAt run-time, the memory elements are selected according to their fit to the present decoder status\nst, and then the outputs of the selected elements are averaged as the output of the memory component. We choose cosine distance to measure the fitting degree, and have1:\nvt = K\u2211 i=1 cos(st,mi(s))mi(g). (2)\nThe output of the neural model and the memory can be combined in various ways. Here, a simple linear combination before the softmax is used, i.e.,\nzt = \u03c3(stW + \u03b2vtE) (3)\nwhere \u03b2 is a pre-defined weighting factor, and E contains word embeddings of all the characters. Although it is possible to train \u03b2 from the data, we found that the learned \u03b2 is not better than the manually-selected one. This is probably because \u03b2 is a factor to trade-off the contribution from the model and the memory, and how to make the trade-off should be a \u2018prior knowledge\u2019 rather than a tunable parameter. In fact, if it is trained, than it will be immediately adapted to match the training data, which will nullify our effort to encourage innovative generation."}, {"heading": "3.3 Model Training", "text": "In our implementation, only the neural model component is required to be trained. The training algorithm follows the scheme defined in (Wang et al., 2016a), where the cross entropy between the distributions over Chinese characters given by the decoder and the ground truth is used as the objective function. The optimization uses the SGD algorithm together with AdaDelta to adjust the learning rate (Zeiler, 2012)."}, {"heading": "4 Memory augmentation for Chinese poetry generation", "text": "This section describes how the memory mechanism can be used to trade-off between the requirements for rule-compliant generation and aesthetic innovation, and how it can also be used to do more interesting things, for example style transfer."}, {"heading": "4.1 Memory for innovative generation", "text": "In this section, we describe how the memory mechanism promotes innovation. Monitoring the\n1In fact, we run a parallel decoder to provide st in Eq.(2). This decoder does not accept input from the encoder and so is consistent with the memory construction process as Eq.(1).\ntraining process for the attention-based model, we found that the cost on the training set will keep decreasing until approaching zero, but on the validation set, the degradation stops after only one iteration. This can be explained by the fact that Chinese quatrains are highly unique, so the common patterns can be fully learned in one iteration, resulting in overfitting with additional iterations. Due to the overfitting, we observe that with the one-iteration model, reasonable poems can be generated, and with the over-fitted model, the generated poems are meaningless, in that they do not resemble feasible character sequences.\nThe energy model perspective helps to explain this difference. For the one-iteration model, the energy surface is smooth and the energy of the training data is not very low, as illustrated in plot (a) in Fig. 2, where the x-axis represents the input and y-axis represents the output, and the z-axis represents the energy. With this model, inputs with small variance will be attracted to the same low-energy area, leading to similar generations. These generations are trivial, but at least reasonable. If the model is overfitted, however, the energy at the locations of the training data becomes much lower than their surrounding areas, leading to a bumpy energy surface as shown in plot (b) in Fig. 2. With this model, inputs with a small variation may be attracted to very different low-energy areas, leading to significantly different generations. Since many of the low-energy areas are nothing to do with good generations but are simply caused by the complex energy function, the generations can be highly surprising for human readers, and the quality is not guaranteed. In some sense, these generations can be regarded as \u2018innovative\u2019 , but based on observations made in our experiments, most of them are meaningless.\nThe augmented memory introduces a new energy function, which is combined with the energy function of the neural model to change the energy surface of the generation system. This can be seen in Eq. (3), where stW and \u03b2vtE can be regarded as the energy function of the neural model component and the memory component, respectively, and the energy function of the memory-augmented system is the sum of the energy functions of these two components. For this reason, the effect of the memory mechanism can be regarded as a regularization of the neural model that will adjust its generation behavior.\nThis regularization effect is illustrated in Fig. 2, where the energy function of the memory shown in plot (c) is added to the energy function of the one-iteration model and the overfitted model, as shown in plot (e) and plot (f) respectively. It can be seen that with the memory involved, the energy surface becomes more bumpy with the one-iteration model, and more smooth with the overfitted model. In the former case, the effect of the memory is to encourage innovation, while still focusing on rule-compliance, and in the latter case, the effect is to encourage rule compliance, while keeping the capability for innovation.\nIt is important to notice that the energy function of the memory component is a linear combination of the energy functions of the compositional elements (see Eq.(2)), each of which is convex and is minimized at the location represented by the element. This means that the energy surface of the memory is rather \u2018healthy\u2019, in the sense that low-energy locations mostly correspond to good generations. For this reason, the regularization provided by the memory is safe and helpful."}, {"heading": "4.2 Memory for style transfer", "text": "The effect of the memory is easy to control. For example, the complexity of the behavior can be controlled by the memory size, the featured bias can be controlled by memory selection, and the strength of the impact can be controlled by the weighting parameter \u03b2. This means that the memory mechanism is very flexible and can be used to produce poems with desired properties.\nIn this work, we use these capabilities to generate poems with different styles. This has been illustrated in Fig. 2, where the energy function of the style memory shown in plot (d) is biased towards a particular style, and once it is added to energy function of the one-iteration model, the resulting energy function shown in plot (g) obtains lower values at locations corresponding to the locations of the memory, which encourages generation of poems with similar styles as those poems in the memory."}, {"heading": "5 Experiments", "text": "This section describes the experiments and results carried out in this paper. Here, The baseline system was a reproduction of the Attention-based system presented in (Wang et al., 2016a). the model in This system has been shown to be rather flexi-\nble and powerful: it can generate different genres of Chinese poems, and when generating quatrains it has been shown to be able to fool human experts in many cases (Wang et al., 2016a) and the authors had did a thorough comparison with competitive methods mentioned in the related work of this paper. We obtained the database and the source code (in theano), and reproduced their system using Tensorflow from Google2. We didn\u2019t make comparisons with some previous methods such as NNLM, SMT, RNNPG as they had been fully compared in (Wang et al., 2016a) and all of them were much worse than the attention-based system. Another reason was that the experts were not happy to evaluate poems with clearly bad quality. We also reproduced the model in (Wang et al., 2016c) with the help of the first author. However, since their implementation did not involve any restrictions on rhythm and tone, the experts were reluctant to recognize them as good poems. With a larger dataset (e.g., 1 Million poems), it is assumed that the rhythm and tone can be learned and their system would be good in both fluency and rule compliance. It should be also emphasized that the memory approach proposed in this paper is a general technique and is complementary to other efforts such as the planning approach (Wang et al., 2016c) and the recursive approach (Yan, 2016).\nBased on the baseline system, we built the memory-augmented model, and conducted two\n2https://www.tensorflow.org/\nexperiments to demonstrate its power. The first is an innovation experiment which employs memory to promote or regularize the generation of innovative poems, and the second is a style-transfer experiment which employs memory to generate flexible poems in different styles.\nWe invited 34 experts to participate in the experiments, and all of them have rich experience not only evaluating poems, but also in writing them. Most of the experts are from prestigious institutes, including Peking university and the Chinese Academy of Social Science (CASS). Following the suggestions of the experts, we use five metrics to evaluate the generation, as listed below:\n\u2022 Compliance: if regulations on tones and rhymes are satisfied;\n\u2022 Fluency: if the sentences read fluently and convey reasonable meaning;\n\u2022 Theme consistency: if the entire poem adheres to a single theme;\n\u2022 Aesthetic innovation: if the quatrain stimulates any aesthetic feeling with elaborate innovation;\n\u2022 Scenario consistency: if the scenario remains consistent."}, {"heading": "5.1 Datasets", "text": "The baseline system was built with two customized datasets. The first dataset is a Chinese\npoem corpus (CPC), which we used in this work to train the embeddings of Chinese characters. Our CPC dataset contains 284,899 traditional Chinese poems in various genres, including Tang quatrains, Song Iambics, Yuan Songs, and Ming and Qing poems. This large quantity of data ensures reliable learning for the semantic content of most Chinese characters.\nOur second dataset is a Chinese quatrain corpus (CQC) that we have collected from the internet, which consists of 13, 299 5-char quatrains and 65, 560 7-char quatrains. This corpus was used to train the attention-based RNN baseline. We filtered out the poems whose characters are all low-frequency (less than 100 counts in the database). After the filtering, the remaining corpus contains 9,195 5-char quatrains and 49,162 7-char quatrains. We used 9,000 5-char and 49,000 7-char quatrains to train the attention model, and the rest for validation.\nAnother two datasets were created for use in the memory-augmented system. Our first dataset, MEM-I, contains 500 quatrains randomly selected from our CQC corpus. This dataset was used to produce the memory in the innovation experiment; the second dataset, MEM-S, contains 300 quatrains with clear styles, including 100 pastoral, 100 battlefield and 100 romantic quatrains. It was used to generate memory with different styles in the style-transfer experiment. All the datasets will be released online3."}, {"heading": "5.2 Evaluation Process", "text": "We invited 34 experts to evaluate the quality of the poem generation. In the innovation experiment, the evaluation consisted of a comparison between different systems and configurations in terms of the five metrics. The innovation questions presented the expert with two poems, and asked them to judge which of the poems was better in terms of the five metrics; in the style-transfer experiment, the evaluation was performed by identifying the style of a generated poem. The evaluation was conducted online, with each questionnaire containing 11 questions focusing on innovation and 4 questions concerned with style-transfer. Each of the style-transfer questions presented the expert with a single poem and asked them to score it between 1 to 5, with a larger score being better, in terms of compliance, aesthetic innovation,\n3http://vivi.cslt.org\nscenario consistency, and fluency. They were also asked to specify the style of the poem.\nUsing the poems generated by our systems, we generated many different questions of both types, and then created a number of online questionnaires that randomly selected from these questions. This meant that as discussed above, each questionnaire had 11 randomly selected innovation questions, and 4 randomly selected style transfer questions. Each question was only used once, meaning that it was not duplicated on multiple questionnaires, and so each questionnaire was different.\nExperts could choose to answer multiple questionnaires if they wished, as each one was different. From the 34 experts, we collected 69 completed questionnaires, which equals to 759 innovation questions and 276 style-transfer questions."}, {"heading": "5.3 Innovation experiment", "text": "This experiment focuses on the contribution of memory for innovative poem generation. We experimented with two configurations: one is with a one-iteration model (C1) and the other is with an overfitted model (C\u221e). The memory was generated from the 500 quatrains in MEM-I, and the weighting factor was defined empirically as 16 for C1 and 49 for C\u221e.\nThe topics of the generation were 160 keywords randomly selected from Shixuhanyinge (Liu, 1735). Given a pair of poems generated by two different configurations using the same topic, the experts were asked to choose which one they preferred. The evaluation is therefore pair-wised, and each pair of configurations contains at least 180 evaluations. The results are shown in Table 2, where the preference ratio for each pair of configurations was tested in terms of the 5 metrics.\nFrom the first row of Table 2, we observe that the experts have a clear preference for the poems generated by the C1 model, the one that can produce fluent yet uninteresting poems. In particular, the \u2018aesthetic innovation\u2019 score for C\u221e is not better than C1, which was different from what we expected. Informal offline discussions with the poetry experts found that the experts identified some innovative expression in the C\u221e condition, but most of the them was regarded as being nonsense in the opinion of many of the experts. In comparison to sparking innovation, fluency and being meaningful is more important not only for non-expert readers, but also for professional po-\nets. In other words, only meaningful innovation is regarded as innovation, and irrational innovation is simply treated as junk.\nFrom the second and third rows of Table 2, it can be seen that involving memory significantly improves both C1 and C\u221e, particularly for C\u221e. For C1, the most substantial improvement is observed in terms of \u2018Aesthetic innovation\u2019, which is consistent with our argument that memory can help encourage innovation for this model. For C\u221e, \u2018Fluency\u2019 seems to be the most improved metric. This is also consistent with our argument that involving memory constrains over-innovation for over-fitted models.\nThe last row of Table 2 is an extra experiment that investigates if C\u221e is regularized well enough after introducing the memory. It seems that with the regularization, the overfitting problem is largely solved, and the generation is nearly as fluent and consistent as the C1 condition. Interestingly, the score for aesthetic innovation is also significantly improved. Since the regularization is not supposed to boost innovation, this seems confusing at first glance (in comparison to the result on the same metric in the first row), but this is probably because the increased fluency and consistency makes the innovation more appreciated, therefore doubly confirming our argument that true innovation should be reasonable and meaningful."}, {"heading": "5.4 Style-transfer experiment", "text": "In the second experiment, the memory mechanism is used to generate poems in different styles. We chose three styles: pastoral, battlefield, and romantic. A style-specific memory, which we call style memory, was constructed for each style by the corresponding quatrains in the MEM-S dataset. The system with one-iteration model C1 was used as the baseline. Two sets of topics were used in the experiment, one is general and the other is style-biased. The experiments then investigate if the memory mechanism can produce\na clear style if the topic is general, and can transfer to a different style if the topic is style-biased already. The experts were asked to specify the style from four options including the three defined above and a \u2018unclear style\u2019 option. In addition, the experts were asked to score the poems in terms of compliance, fluency, aesthetic innovation, and scenario consistency, which we can use to check if the style transfer impacts the quality of the poem generation. Note that we did not ask for the theme consistency to be scored in this experiment because the topic words were not presented to the experts, in order to prevent the topic affecting their judgment regarding the style. The score ranges from 1 to 5, with a larger score being better.\nTable 3 presents the results with the general topics. The numbers show the probabilities that the poems generated by a particular system were labeled as having various styles. Since the topics are unbiased in types, the generation of the baseline system is assumed to be with unclear styles. For other systems, the style of the generation is assumed to be the same as the style of their memories. The results in Table 3 clearly demonstrates these assumptions. The tendency that romantic poems are recognized as pastoral poems is a little surprising. Further analysis shows that experts tend to recognize romantic poems as pastoral poems only if there are any related symbols such as trees, mountain, river. These words are very general in Chinese quatrains. The indicator words of romantic poems such as skirt, rouge, and singing are not as popular and their indication power is not as strong, leading to less labeling of romantic poems, as shown in the results.\nWe also tested transferring from one style to another. This was achieved by generating poems with some style-biased topics, and then using a style memory to force the generation to change the style. Our experiments show that in 73% cases the style can be successfully transferred.\nFinally, the scores of the poems generated with and without the style memories are shown in Table 4, where the poems generated with both general and style-biased topics are accounted for. It can be seen that overall, the style transfer may degrade fluency a little. This is understandable, as enforcing a particular style has to break the optimal generation with the baseline, which is assumed to be good at generating fluent poems. Nevertheless the sacrifice is not significant."}, {"heading": "5.5 Examples", "text": "Table 5 to Table 7 shows example poems generated by the system C1, C1+Mem and C1+Style Mem where the style in this case is set to be romantic. The three poems were generated with the same, very general, topic (\u2018(oneself)\u2019). More examples are given in the supporting material."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed a memory mechanism to support innovative Chinese poem generation by neural models augmented with a memory. Experimental results demonstrated that mem-\nory can boost innovation from two opposite directions: either by encouraging creative generation for regularly-trained models, or by encouraging rule-compliance for overfitted models. Both strategies work well, although the former generated poetry that was preferred by experts in our experiments. Furthermore, we found that the memory can be used to modify the style of the generated poems in a flexible way. The experts we collaborated with feel that the present generation is comparable to today\u2019s experienced amateur poets. Future work involves investigating a better memory selection scheme. Other regularization methods (e.g., norm or drop out) are also interesting and may alleviate the over-fitting problem."}, {"heading": "Acknowledgments", "text": "This paper was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61371136, NO.61633013, NO.61472428."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401 .", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Generating Chinese classical poems with statistical machine translation models", "author": ["Jing He", "Ming Zhou", "Long Jiang."], "venue": "Twenty-Sixth AAAI Conference on Artificial Intelligence.", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating Chinese couplets using a statistical mt approach", "author": ["Long Jiang", "Ming Zhou."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics, volume 1, pages 377\u2013384.", "citeRegEx": "Jiang and Zhou.,? 2008", "shortCiteRegEx": "Jiang and Zhou.", "year": 2008}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad."], "venue": "Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Lin-", "citeRegEx": "Netzer et al\\.,? 2009", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation", "author": ["H Oliveira."], "venue": "Proceedings of the ECAI 2012 Workshop on Computational Creativity, Concept Invention, and General Intelligence.", "citeRegEx": "Oliveira.,? 2012", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "English Translation for Tang Poems (Ying Yi Tang Shi San Bai Shou)", "author": ["Yihe Tang."], "venue": "Tianjin People Publisher.", "citeRegEx": "Tang.,? 2005", "shortCiteRegEx": "Tang.", "year": 2005}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Naoko Tosa", "Hideto Obara", "Michihiko Minoh."], "venue": "Entertainment Computing-ICEC 2008 pages 209\u2013216. Springer.", "citeRegEx": "Tosa et al\\.,? 2009", "shortCiteRegEx": "Tosa et al\\.", "year": 2009}, {"title": "A Summary of Rhyming Constraints of Chinese Poems (Shi Ci Ge Lv Gai Yao), volume 1", "author": ["Li Wang."], "venue": "Beijin Press.", "citeRegEx": "Wang.,? 2002", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "Can machine generate traditional Chinese poetry? a feigenbaum test", "author": ["Qixin Wang", "Tianyi Luo", "Dong Wang."], "venue": "BICS 2016.", "citeRegEx": "Wang et al\\.,? 2016a", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese song iambics generation with neural attention-based model", "author": ["Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing."], "venue": "IJCAI 16.", "citeRegEx": "Wang et al\\.,? 2016b", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Chinese poetry generation with planning based neural network", "author": ["Zhe Wang", "Wei He", "Hua Wu", "Haiyang Wu", "Wei Li", "Haifeng Wang", "Enhong Chen."], "venue": "COLING 2016.", "citeRegEx": "Wang et al\\.,? 2016c", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1410.3916 .", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu."], "venue": "Entertainment Computing-ICEC 2009 pages 191\u2013196. Springer.", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "i, Poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema", "author": ["Rui Yan."], "venue": "IJCAI2016.", "citeRegEx": "Yan.,? 2016", "shortCiteRegEx": "Yan.", "year": 2016}, {"title": "i, Poet: automatic Chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li."], "venue": "Proceedings of the Twenty-Third", "citeRegEx": "Yan et al\\.,? 2013", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 670\u2013680.", "citeRegEx": "Zhang and Lapata.,? 2014", "shortCiteRegEx": "Zhang and Lapata.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of Chinese Songci", "author": ["Cheng-Le Zhou", "Wei You", "Xiaojun Ding."], "venue": "Journal of Software 21(3):427\u2013437.", "citeRegEx": "Zhou et al\\.,? 2010", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Among the various genres, perhaps the most popular one is the quatrain, a special style with a strict structure (four lines with five or seven characters per line), a regulated rhythmical form (the last characters in the second and fourth lines must follow the same rhythm), and a required tonal pattern (tones of characters in some positions should satisfy a predefined regulation) (Wang, 2002).", "startOffset": 383, "endOffset": 395}, {"referenceID": 6, "context": "The translation is from (Tang, 2005).", "startOffset": 24, "endOffset": 36}, {"referenceID": 18, "context": "For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": ", 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al.", "startOffset": 54, "endOffset": 93}, {"referenceID": 2, "context": ", 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al.", "startOffset": 54, "endOffset": 93}, {"referenceID": 9, "context": ", rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a).", "startOffset": 192, "endOffset": 212}, {"referenceID": 7, "context": "For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al.", "startOffset": 27, "endOffset": 63}, {"referenceID": 13, "context": "For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al.", "startOffset": 27, "endOffset": 63}, {"referenceID": 4, "context": ", 2009), word association norm (Netzer et al., 2009), template search (Oliveira, 2012), genetic search (Zhou et al.", "startOffset": 31, "endOffset": 52}, {"referenceID": 5, "context": ", 2009), template search (Oliveira, 2012), genetic search (Zhou et al.", "startOffset": 25, "endOffset": 41}, {"referenceID": 18, "context": ", 2009), template search (Oliveira, 2012), genetic search (Zhou et al., 2010), text summarization (Yan et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 15, "context": ", 2010), text summarization (Yan et al., 2013).", "startOffset": 28, "endOffset": 46}, {"referenceID": 3, "context": ", (Jiang and Zhou, 2008; He et al., 2012).", "startOffset": 2, "endOffset": 41}, {"referenceID": 2, "context": ", (Jiang and Zhou, 2008; He et al., 2012).", "startOffset": 2, "endOffset": 41}, {"referenceID": 13, "context": "The first study we have found in this direction is the work by Zhang and Lapata (2014), which proposed an RNN-based approach that produces each new line character-by-character using a recurrent neural network (RNN), with all the lines generated already (in the form of a vector) as a contextual input.", "startOffset": 63, "endOffset": 87}, {"referenceID": 8, "context": "Wang et al. (2016b) proposed a much simpler neural model that treats a poem as an entire character sequence, and poem generation is conducted character-by-character.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation.", "startOffset": 48, "endOffset": 71}, {"referenceID": 7, "context": "To avoid theme drift caused by this long-sequence generation, Wang et al. (2016b) utilized the neural attention mechanism (Bahdanau et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation.", "startOffset": 49, "endOffset": 190}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation. Yan (2016) proposed a hierarchical RNN model that conducts iterative generation.", "startOffset": 49, "endOffset": 234}, {"referenceID": 0, "context": "(2016b) utilized the neural attention mechanism (Bahdanau et al., 2014) by which human intention is encoded by an RNN to guide the generation. The same model was used by Wang et al. (2016a) for Chinese quatrain generation. Yan (2016) proposed a hierarchical RNN model that conducts iterative generation. Recently, Wang et al. (2016c) proposed a similar sequence generation model, but with the difference that attention is placed not only on the human input, but also on all the characters that have been generated so far.", "startOffset": 49, "endOffset": 334}, {"referenceID": 8, "context": "Our system was built following the model structure and training strategy proposed by Wang et al. (2016a) due to its simplicity and demonstrated quality, but the memory mechanism is general and can be applied to any of the models presented above.", "startOffset": 85, "endOffset": 105}, {"referenceID": 12, "context": ", 2014, 2016) and memory network (Weston et al., 2014).", "startOffset": 33, "endOffset": 54}, {"referenceID": 9, "context": "In this work, the attention-based RNN generation model presented by (Wang et al., 2016a) is used as the neural model component, although any neural model is suitFigure 1: The memory-augmented neural model used for Chinese poetry generation.", "startOffset": 68, "endOffset": 88}, {"referenceID": 11, "context": "Note that this memory-augmented neural model is inspired by and related to the memory network proposed by Weston et al.(2014) and Graves et al.", "startOffset": 106, "endOffset": 126}, {"referenceID": 1, "context": "(2014) and Graves et al.(2016), but we more focus on an accompanying memory that plays the role of assistance and regularization.", "startOffset": 11, "endOffset": 31}, {"referenceID": 0, "context": "Firstly, the neural model part is an attentionbased sequence-to-sequence model (Bahdanau et al., 2014).", "startOffset": 79, "endOffset": 102}, {"referenceID": 9, "context": "The training algorithm follows the scheme defined in (Wang et al., 2016a), where the cross entropy between the distributions over Chinese characters given by the decoder and the ground truth is used as the objective function.", "startOffset": 53, "endOffset": 73}, {"referenceID": 16, "context": "The optimization uses the SGD algorithm together with AdaDelta to adjust the learning rate (Zeiler, 2012).", "startOffset": 91, "endOffset": 105}, {"referenceID": 9, "context": "Here, The baseline system was a reproduction of the Attention-based system presented in (Wang et al., 2016a).", "startOffset": 88, "endOffset": 108}, {"referenceID": 9, "context": "ble and powerful: it can generate different genres of Chinese poems, and when generating quatrains it has been shown to be able to fool human experts in many cases (Wang et al., 2016a) and the authors had did a thorough comparison with competitive methods mentioned in the related work of this paper.", "startOffset": 164, "endOffset": 184}, {"referenceID": 9, "context": "We didn\u2019t make comparisons with some previous methods such as NNLM, SMT, RNNPG as they had been fully compared in (Wang et al., 2016a) and all of them were much worse than the attention-based system.", "startOffset": 114, "endOffset": 134}, {"referenceID": 11, "context": "We also reproduced the model in (Wang et al., 2016c) with the help of the first author.", "startOffset": 32, "endOffset": 52}, {"referenceID": 11, "context": "It should be also emphasized that the memory approach proposed in this paper is a general technique and is complementary to other efforts such as the planning approach (Wang et al., 2016c) and the recursive approach (Yan, 2016).", "startOffset": 168, "endOffset": 188}, {"referenceID": 14, "context": ", 2016c) and the recursive approach (Yan, 2016).", "startOffset": 36, "endOffset": 47}], "year": 2017, "abstractText": "It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.", "creator": "LaTeX with hyperref package"}}}