{"id": "1705.03881", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Net2Vec: Deep Learning for the Network", "abstract": "We present Net2Vec, a flexible high-performance platform that allows the execution of deep learning algorithms in the communication network. Net2Vec is able to capture data from the network at more than 60Gbps, transform it into meaningful tuples and apply predictions over the tuples in real time. This platform can be used for different purposes ranging from traffic classification to network performance analysis. We aim to capture the complexity of the data using the Net2Vec pipeline. We have demonstrated that our performance is comparable to the performance of other technologies for a number of reasons.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 10 May 2017 13:28:19 GMT  (1773kb,D)", "http://arxiv.org/abs/1705.03881v1", null]], "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["roberto gonzalez", "filipe manco", "alberto garcia-duran", "jose mendes", "felipe huici", "saverio niccolini", "mathias niepert"], "accepted": false, "id": "1705.03881"}, "pdf": {"name": "1705.03881.pdf", "metadata": {"source": "CRF", "title": "Net2Vec: Deep Learning for the Network", "authors": ["Roberto Gonzalez", "Filipe Manco", "Alberto Garcia-Duran", "Jose Mendes", "Felipe Huici", "Saverio Niccolini", "Mathias Niepert"], "emails": ["first.last@neclab.eu"], "sections": [{"heading": null, "text": "Finally, we showcase the use of Net2Vec by implementing and testing a solution able to profile network users at line rate using traces coming from a real network. We show that the use of deep learning for this case outperforms the baseline method both in terms of accuracy and performance."}, {"heading": "1. INTRODUCTION", "text": "Deep learning approaches have been successfully applied in a wide range of areas such as computer vision, natural language processing, and resource management, to name a few. The success of deep learning is largely explained by its ability to learn vector representations of the objects under consideration (images, words, system states). These representations are highly suitable for the downstream classification and regression problems.\nAs a result, a number of deep learning frameworks have risen in the past few years (e.g., Torch and TensorFlow) that enable data scientists to prototype, train, and evaluate neural network models. Further, these frameworks are optimized for batch processing, where large data sets are moved to and processed on GPUs.\nHowever, to the best of our knowledge, there is no platform that targets, and is optimized for, the development and deployment of deep learning methods in data communication networks, with their high throughput and low latency requirements. While there are some platforms for stream processing such as Flink [5], these have overheads due to fault tolerance considerations and/or the enforcement of in-sequence processing of the data streams. For most of the deep learning algorithms, however, in-sequence processing is not a strict requirement. Instead, the sequences only have to be processed in an order that approximates the original one. For in-\nstance, most representation learning methods [13] are agnostic to violations of the original order if these violations are local (e.g., permutations of two neighboring words). There are no implementations of deep learning methods for existing stream processing platforms.\nA platform optimized for network analytics would be highly beneficial as many of the typical problems could be addressed with deep learning methods. Some of these problems are CDNs/content popularity prediction, anomaly detection (e.g., DoS attacks, port scans, etc.), online advertisement, advanced traffic classification, and fault prediction, to name a few.\nTo plug this gap, we introduce Net2Vec, a proof-ofconcept implementation of a high performance platform that brings deep learning to the network. Net2Vec can work both on live traffic, using a high performance traffic capture module (at least 60Gb/s in our tests), or off-line, using modules that retrieve information from data or log files. In the case of live traffic, Net2Vec provides modules that transform incoming network packets into sequences of tuples; deep learning models are then applied to the tuple sequences to learn representations for downstream analytics tasks. Net2Vec also supports the acceleration of ML algorithms through the use of GPGPUs.\nTo showcase Net2Vec\u2019s performance and applicability, we use it to implement a user profiling use case. We show that it outperforms a baseline method both in terms of efficiency and accuracy of results."}, {"heading": "2. DESIGN AND IMPLEMENTATION", "text": "The main objective of Net2Vec is to provide data scientists with a framework to develop and deploy machine learning methods in networks. Network traffic consists of high-speed packet sequences (or data derived from them such as log files) carrying high-level information such as sequences of text, images, speech and so on. State of the art (deep) learning algorithms have shown remarkable performance for analytics tasks on such sequences of objects, and so Net2Vec aims to bring these state of the art ML methods to bear on communication networks.\nar X\niv :1\n70 5.\n03 88\n1v 1\n[ cs\n.N I]\n1 0\nM ay\n2 01\n7\nAt a high level, Net2Vec is in charge of capturing packet data, filtering it, constructing tuples from it, and feeding those tuples to the machine learning algorithms in charge of the analysis. More specifically, Net2Vec\u2019s architecture consists of a set of five components, each containing pluggable modules (see Figure 1): Data Capture: This component is in charge of capturing network data from a number of sources including network interfaces, traffic trace files or log files. Our proof-of-concept evaluation implements a Netmap [18] based module that captures live traffic from network interfaces. Filtering: A set of modules responsible for filtering out uninteresting traffic. This component is optional: depending on the data capture module, it may be that all traffic is relevant. In our evaluation we implement a module that filters for HTTP traffic. Tuple Generation: This component transforms the incoming packet data stream into a set of tuples suitable for the representation learning component. For example, in the user profiling use case described further in this paper, the tuples are of the form <src ip, hostname>. Splitter: This component takes, as input from users, a subset of attributes and uses it to split the incoming tuple stream into separate sequences of tuples. Analogous to the relational database terminology, we refer to these subsets of attributes as keys. The individual sequences become the input to the representation learning methods. Representation Learning and Prediction: This component supports any algorithm that complies with the basic methods of the scikit-learn API. In addition to the standard ML algorithms, most deep learning frameworks have wrappers that expose these methods. The input to the ML models are fixed-size (possibly padded)\nsequences of tuples, one per key value. Users of the platform can implement data analysis use cases by choosing modules for the different components described above, along with a specification of a key to be used by the splitter. Extending the platform\u2019s functionality can be done by developing additional modules. In the rest of this section we give a more detailed explanation of the various components, as well as a performance evaluation of the data capture, filtering and tuple generation components."}, {"heading": "2.1 Capture, Filtering and Tuple Generation Components", "text": "Net2Vec targets network data, and as such, the platform needs to be able to cope, in real-time, with the sort of traffic rates typical of operator networks. Specifically, the aim is to show that Net2Vec is able to capture traffic from multiple interfaces, perform filtering on it in order to extract the traffic relevant to specific use cases, and transform the result into the tuples needed by the representation learning algorithms.\nTo demonstrate this, we implement a Net2Vec highperformance packet capture module based on the Netmap framework [18]. An additional module then applies a filter (in our case dst port 80), and a third one parses the resulting packets to generate a tuple (a two-element tuple consisting of <src ip, hostname>.\nTo test the performance of these modules, we use an x86 server with an Intel Xeon E5-1650 v2 3.5 GHz CPU (6 cores), 16GB of RAM and 3 Intel X520-T2 10Gb/s dual port cards (a total of 60 Gb/s). For packet generation we use a separate server also with 3 Intel X520-T2 cards, and connect each of the 6 ports to those on the server running Net2Vec with direct cables. For packet generation we use the pkt-gen netmap application. In addition to the filtering/tuple described above, we mea-\n60 124 252 508 1020 1468\nPacket Size (bytes)\n0\n15\n30\n45\n60 75 T h ro u g h p u t (G b p s)\nCapture and discard Capture and HTTP parsing\n0\n20\n40\n60\n80\n100\nT h\nro u\ng h\np u\nt (M\np p\ns)\nCapture and discard Capture and HTTP parsing\nFigure 2: Performance of the Net2Vec high performance packet capture module. The bars correspond to the left y-axis (throughput in Gb/s), the curves to the right one (Mp/s)\nsure baseline capture performance by asking the module to receive packets, calculate a rate, and discard them.\nThe results are shown in Figure 2. The baseline experiment (\u201cCapture and discard\u201d) shows that the module is able to achieve line rate (60 Gb/s) for all packet sizes (except for a slight drop for minimum-sized ones). The full experiment (\u201cCapture and HTTP parsing\u201d) yields similar results: line rate for all packet sizes (note: in this case there are no results for small packets sizes due to headers and the HTTP payload). In short, Net2Vec is able to handle large traffic rates while generating the tuples needed by the machine learning algorithms."}, {"heading": "2.2 Splitter Component", "text": "The splitting component takes the user provided key and splits the incoming global stream of tuples into keyspecific streams. Each stream is associated with a queue of size n. Once a queue is filled, the content of the queue (which is a sequence of tuples) is sent to the ML component to update its parameters and/or to perform a prediction. The oldest tuple in the queue is removed. The splitting of the traffic with respect to some key is crucial in several use cases such as user profiling and can also serve load balancing purposes.\nIn the user profiling use case we will describe later, the key consists of the source IP address which we use to uniquely identify individual users. The queue stores sequences of hostnames that are used to train a deep unsupervised embedding model."}, {"heading": "2.3 Representation Learning and Prediction Component", "text": "The component accepts a machine learning model implemented as a Python class with the standard scikitlearn API methods such as predict, fit, etc. Net2Vec applies the model to the incoming stream of tuples so as to learn a representation for the problem specific objects (images, hostnames, etc.) and to perform prediction for the given task. We focus specifically on neural network\nbased machine learning methods and the corresponding frameworks such as Torch and TensorFlow. These parameters of the neural models are trained using highly efficient gradient descent type algorithms [4] which can be parallelized [16] and implemented for GPUs using platforms such as CUDA [15, 11]. Since the ML models have to be trained and queried at network speed, we use the cudamat library [21] and similar libraries for GPU processing.\nMachine learning models implicitly perform some form of representation learning. The input representations of objects such as words, hostnames, images, and so on, are mapped to a vector representation (an embedding) that is amenable to downstream classification or regression problems. Representation learning is especially useful in environments with an abundance of data but with few labeled examples. Unsupervised representation learning based on neural networks has been used in many different contexts [12, 15, 13, 14]. Net2Vec continuously feeds the incoming tuple sequences to the ML models by calling the method update (not included in the scikit-learn API) which performs a parameter update. These updates refine the learned representations and are able to capture concept drift.\nOnce the representations (embeddings) are learned, the system can perform the use case specific prediction task. In many use cases, labeled data only exists for a small set of tuples or sequences of tuples. The problem is then to predict the labels for those (sequences of) tuples that do not have labels. Given the representations of tuples in an Euclidean space (the embedding space), there are generally two ways to incorporate labeled data. One method simply performs a form of k-nearest neighbor search and assigns the categories of the most similar tuples to the unlabeled tuple. The alternative strategy trains a more sophisticated ML model using the learned embeddings as input representations and existing labels as supervision signal. The predict (and, if probabilities are required, also predict_probab) methods are called by Net2Vec to perform the prediction."}, {"heading": "3. USE CASE: USER PROFILING", "text": "To evaluate the implemented Net2Vec platform, we assess its performance on a user profiling use case, a typical network analytics task that enables network operators to participate in the lucrative online advertising ecosystem by serving targeted ads to users. In principle, network operators have an advantage over other user profiling players since they have access to their users\u2019 navigation history; this is even true if most of the navigation history is based on HTTPS requests [8]. The size of the data carried over the network and the privacy implications of storing user-specific data, however,\nmake the exploitation of network data challenging to operators.\nTo address this, we use Net2Vec to efficiently profile users based on real network data while maintaining only one ML model and no long-term user-specific data."}, {"heading": "3.1 Problem Description", "text": "For this use case we assume a network operator capturing customer HTTP(S) traffic. The HTTP(S) requests are captured by Net2Vec and mapped into tuples of the form (src ip, hostname). Each tuple represents a hostname visited by a user in the network.\nIn addition, the operator maintains a set of productrelated categories C and a subset of the hostnames is associated with some categories from C. Note that the number of hostnames for which categories are known is small compared to all known hostnames since many hostnames do not correspond to webpages but to CDNs, trackers, and mobile application APIs. The objective is now to assign, in real-time, a given user (here: IP address) to a set of product-related categories based on her current URL request sequence.\nThe standard approach would be to follow a strategy similar to the one used by online trackers, where all hostnames visited are stored and used to assign categories to users. This approach, however, has several drawbacks. First, it does not scale well since the amount of data per user grows continually. Second, storing the browsing history of users raises important difficult issues and is even illegal in several countries.\nTo overcome these problems, we use Net2Vec to train a neural network model that learns the behavior of\nusers. It is then deployed to predict, given a request sequence of a user, the product-related categories the user is likely to be interested in. The neural network learns a representation of the hostnames from a large number of request sequences. For each input sequence, the model is trained to reconstruct one-hot encoding of the hostname from the rest of the sequence. This ML model is similar to the word2vec model for learning word embeddings [13]. Figure 3 depicts the Net2Vec system for user profiling."}, {"heading": "3.2 Experiments & Results", "text": "For the experiments we used the first level IAB categories1. We bootstrapped the system by assigning IAB categories to hostnames using a text classification algorithm.\nWe generated artificial personas with specific interests according to the work by Carrascosa et al. [6] and evaluated the categorization provided by Net2Vec for these personas. Figure 4 shows a single profile returned by Net2Vec for a persona interested in video games. The profile indicates that the user is mostly interested in Technology & Computing, Hobbies & Interests and Arts & Entertainment, topics directly related with video games. In addition, the system also predicts interests in Education, Sports and Health & Fitness. A manual inspection of the request traces reveals that most users in the network who visited webpages related to video games also visited webpages related to education and sports. The instance of the Net2Vec system, therefore, is able to assign categories to users based on the general user behavior observed in one specific\n1The Internet Advertising Bureau (IAB) categories are considered to be the standard for the online advertising industry.\nnetwork. Similar results have been obtained for other personas.\nWe evaluated the performance of Net2Vec (running only on a CPU and accelerated by a GPU) by comparing it to a standard baseline method using real traces from a mobile network operator. We used an x86 server with an Intel Xeon E5-2637 v4 3.5Ghz CPU, 128GB of RAM and a GeForce GTX Titan X (900 Series) GPU with a processing power of 6Tflops. We assigned product-related categories to 200k hostnames using the text classification algorithm, and trained Net2Vec on the request sequences of one day of traffic, predicting the categories for users active during the following day. The trace for the second day is composed of about 125M HTTP(S) requests initiated by more than 40k users. Figure 5 depicts the number of tuples Net2Vec was able to process per second. At the beginning, the standard baseline that simply collects categories of visited hostnames is slightly more efficient than Net2Vec executed on a GPU. However, the baseline method\u2019s performance decreases over time since it has to maintain all of the encountered categories per user. The performance of Net2Vec remains constant over time and is able to analyze all the traffic of the 10Gb pipe the data came from. This highlights an intrinsic advantage of neural network based methods: prediction time remains constant.\nA crucial advantage of the Net2Vec deep learning model for user profiling is that it is able to generate a profile for a user even if none of the hostnames visited by the user have assigned categories. In Figure 6 we present the % of users profiled after a given amount of time using both Net2Vec and the baseline approach for different number of categorized hostnames. When we have categories for at least 50K hostnames, the Net2Vec system is able to generate a profile for all the users since the first HTTP request. Moreover, even when faced with only 10K categorized hostnames, the system needs less than 1 hour of traffic to generate the profile of 70% of the users. The baseline approach performs much worse, having to process more than 1 hour of traffic before being able to generate a profile for most\nof the users. Moreover, even with all the traffic of the day, the baseline approach cannot generate a profile for more than 20% of the users (even when 200K hostnames are categorized).\nWe also evaluate the robustness of the approaches to changes in the number of hostnames for which categories are known before the user profiling starts. This robustness is important since, increasingly, hostnames are not amenable to a content-based assignment of categories (e.g., due to more mobile app traffic and less web browser traffic). We computed the distance between the profiles generated with 100k, 50k, 10k, and 1k previously categorized hostnames with the profiles generated using 200k previously categorized hostnames. We do this comparison for every user after 10, 100, and 1000 observed HTTP(S) requests. One can observe that a small number of requests is sufficient to generate high quality profiles. The number of previously categorized hostnames has a significant impact on the difference. The more hostnames are associated with productrelated categories before the profiling commences, the better the quality of the generated profiles."}, {"heading": "4. RELATED WORK", "text": "This work focuses on the development of a platform for the deployment of deep learning methods in the communication network. Several platform exist that can perform data analysis in a general streaming setting; among these are four Apache projects [5, 20, 1, 2] and proprietary solutions such as [3]. However, they are not optimized for deep learning-based analysis nor for dealing with network data. The only solution optimized for the network setting (to the best of our knowledge) is Blockmon [10, 19]; however, it does not support the use of deep learning algorithms.\nWe can also find systems like Clipper [7] that allow predictions in a streaming setting. However, they are not optimized for working with high network data rates, nor do they provide any help for the collection and preprocessing of the data.\nFinally, several specific network problems have been solved using GPUs in recent years [17, 9, 22] Our solution is general and can be applied to a wide range of network problems ranging from traffic classification to user profiling while leveraging acceleration on heterogeneous hardware such as GPUs."}, {"heading": "5. CONCLUSION AND FUTURE WORK", "text": "We introduced Net2Vec, a framework for data scientists to develop and deploy machine learning methods in data communication networks. The modular design of Net2Vec provides enough flexibility to develop completely different use cases in an efficient manner without sacrificing performance. Our proof-of-concept evaluation shows that Net2Vec is able to efficiently profile network users at line rate while presenting better efficiency than baseline methods.\nAs future work, we are looking into efficient methods for transferring data into GPUs, as well as packet-based operations can be accelerated by these devices. A separate, but complementary, goal is to enhance Net2Vec to make it seamless for data scientists to obtain network data. Finally, we will add a new component that will allow the platform to actually perform actions in order to apply real-time re-configurations to the network."}, {"heading": "6. REFERENCES", "text": "[1] Apache. Apache Kafka. https://kafka.apache.org/. [2] Apache. Apache Spark Streaming.\nhttp://spark.apache.org/streaming/. [3] Biem, A., Bouillet, E., Feng, H., Ranganathan,\nA., Riabov, A., Verscheure, O., Koutsopoulos, H., and Moran, C. Ibm infosphere streams for scalable, real-time, intelligent transportation services. In Proceedings of the 2010 ACM SIGMOD (2010), ACM.\n[4] Bottou, L. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010. 2010. [5] Carbone, P., Ewen, S., Haridi, S., Katsifodimos, A., Markl, V., and Tzoumas, K. Apache flink:\nStream and batch processing in a single engine. Data Engineering (2015).\n[6] Carrascosa, J. M., Mikians, J., Cuevas, R., Erramilli, V., and Laoutaris, N. I always feel like somebody\u2019s watching me: measuring online behavioural advertising. In Proceedings of the 11th ACM CoNEXT (2015), ACM. [7] Crankshaw, D., Wang, X., Zhou, G., Franklin, M. J., Gonzalez, J. E., and Stoica, I. Clipper: A low-latency online prediction serving system. arXiv preprint arXiv:1612.03079 (2016). [8] Gonzalez, R., Soriente, C., and Laoutaris, N. User profiling in the time of https. In Proceedings of the 2016 ACM on IMC (2016), ACM. [9] Han, S., Jang, K., Park, K., and Moon, S. Packetshader: a gpu-accelerated software router. In ACM SIGCOMM CCR (2010), vol. 40, ACM.\n[10] Huici, F., Di Pietro, A., Trammell, B., Gomez Hidalgo, J. M., Martinez Ruiz, D., and d\u2019Heureuse, N. Blockmon: A high-performance composable network traffic measurement system. ACM SIGCOMM CCR 42, 4 (2012). [11] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell, T. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM MM (2014), ACM. [12] Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th ICML (2009). [13] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (2013). [14] Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 (2015). [15] Raina, R., Madhavan, A., and Ng, A. Y. Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th ICML (2009), ACM. [16] Recht, B., Re, C., Wright, S., and Niu, F. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems (2011). [17] Renart, E. G., Zhang, E. Z., and Nath, B. Towards a gpu sdn controller. In NetSys (2015), IEEE. [18] Rizzo, L. netmap: A novel framework for fast packet i/o. In USENIX ATC 12) (Boston, MA, 2012), USENIX Association, pp. 101\u2013112. [19] Simoncelli, D., Dusi, M., Gringoli, F., and Niccolini, S. Stream-monitoring with blockmon: convergence of network measurements and data analytics platforms. ACM SIGCOMM CCR 43, 2 (2013). [20] Storm, A. Storm, distributed and fault-tolerant realtime computation. [21] Volodymyr Mnih. CUDAMat: a CUDA-based matrix class for Python. [22] Wang, Y., Zu, Y., Zhang, T., Peng, K., Dong, Q., Liu, B., Meng, W., Dai, H., Tian, X., Xu, Z., et al. Wire speed name lookup: A gpu-based approach. In NSDI (2013)."}], "references": [{"title": "Ibm infosphere streams for scalable, real-time, intelligent transportation services", "author": ["A. Biem", "E. Bouillet", "H. Feng", "A. Ranganathan", "A. Riabov", "O. Verscheure", "H. Koutsopoulos", "C. Moran"], "venue": "In Proceedings of the 2010 ACM SIGMOD (2010),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Apache flink:  Stream and batch processing in a single engine", "author": ["P. Carbone", "S. Ewen", "S. Haridi", "A. Katsifodimos", "V. Markl", "K. Tzoumas"], "venue": "Data Engineering", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "I always feel like somebody\u2019s watching me: measuring online behavioural advertising", "author": ["J.M. Carrascosa", "J. Mikians", "R. Cuevas", "V. Erramilli", "N. Laoutaris"], "venue": "In Proceedings of the 11th ACM CoNEXT (2015),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clipper: A low-latency online prediction serving system", "author": ["D. Crankshaw", "X. Wang", "G. Zhou", "M.J. Franklin", "J.E. Gonzalez", "I. Stoica"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "User profiling in the time of https", "author": ["R. Gonzalez", "C. Soriente", "N. Laoutaris"], "venue": "In Proceedings of the 2016 ACM on IMC (2016),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Packetshader: a gpu-accelerated software router", "author": ["S. Han", "K. Jang", "K. Park", "S. Moon"], "venue": "In ACM SIGCOMM CCR (2010),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Blockmon: A high-performance composable network traffic measurement system", "author": ["F. Huici", "A. Di Pietro", "B. Trammell", "J.M. Gomez Hidalgo", "D. Martinez Ruiz", "N. d\u2019Heureuse"], "venue": "ACM SIGCOMM CCR 42,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the 22nd ACM MM (2014),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th ICML", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["R. Raina", "A. Madhavan", "A.Y. Ng"], "venue": "In Proceedings of the 26th ICML (2009),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Towards a gpu sdn controller", "author": ["E.G. Renart", "E.Z. Zhang", "B. Nath"], "venue": "NetSys", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "netmap: A novel framework for fast packet i/o", "author": ["L. Rizzo"], "venue": "In USENIX ATC 12) (Boston, MA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Stream-monitoring with blockmon: convergence of network measurements and data analytics platforms", "author": ["D. Simoncelli", "M. Dusi", "F. Gringoli", "S. Niccolini"], "venue": "ACM SIGCOMM CCR 43,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Wire speed name lookup: A gpu-based approach", "author": ["Y. Wang", "Y. Zu", "T. Zhang", "K. Peng", "Q. Dong", "B. Liu", "W. Meng", "H. Dai", "X. Tian", "Z Xu"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "While there are some platforms for stream processing such as Flink [5], these have overheads due to fault tolerance considerations and/or the enforcement of in-sequence processing of the data streams.", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "For instance, most representation learning methods [13] are agnostic to violations of the original order if these violations are local (e.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "Our proof-of-concept evaluation implements a Netmap [18] based module that captures live traffic from network interfaces.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "To demonstrate this, we implement a Net2Vec highperformance packet capture module based on the Netmap framework [18].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "These parameters of the neural models are trained using highly efficient gradient descent type algorithms [4] which can be parallelized [16] and implemented for GPUs using platforms such as CUDA [15, 11].", "startOffset": 106, "endOffset": 109}, {"referenceID": 13, "context": "These parameters of the neural models are trained using highly efficient gradient descent type algorithms [4] which can be parallelized [16] and implemented for GPUs using platforms such as CUDA [15, 11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "These parameters of the neural models are trained using highly efficient gradient descent type algorithms [4] which can be parallelized [16] and implemented for GPUs using platforms such as CUDA [15, 11].", "startOffset": 195, "endOffset": 203}, {"referenceID": 8, "context": "These parameters of the neural models are trained using highly efficient gradient descent type algorithms [4] which can be parallelized [16] and implemented for GPUs using platforms such as CUDA [15, 11].", "startOffset": 195, "endOffset": 203}, {"referenceID": 9, "context": "Unsupervised representation learning based on neural networks has been used in many different contexts [12, 15, 13, 14].", "startOffset": 103, "endOffset": 119}, {"referenceID": 12, "context": "Unsupervised representation learning based on neural networks has been used in many different contexts [12, 15, 13, 14].", "startOffset": 103, "endOffset": 119}, {"referenceID": 10, "context": "Unsupervised representation learning based on neural networks has been used in many different contexts [12, 15, 13, 14].", "startOffset": 103, "endOffset": 119}, {"referenceID": 11, "context": "Unsupervised representation learning based on neural networks has been used in many different contexts [12, 15, 13, 14].", "startOffset": 103, "endOffset": 119}, {"referenceID": 5, "context": "In principle, network operators have an advantage over other user profiling players since they have access to their users\u2019 navigation history; this is even true if most of the navigation history is based on HTTPS requests [8].", "startOffset": 222, "endOffset": 225}, {"referenceID": 10, "context": "This ML model is similar to the word2vec model for learning word embeddings [13].", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "[6] and evaluated the categorization provided by Net2Vec for these personas.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Several platform exist that can perform data analysis in a general streaming setting; among these are four Apache projects [5, 20, 1, 2] and proprietary solutions such as [3].", "startOffset": 123, "endOffset": 136}, {"referenceID": 0, "context": "Several platform exist that can perform data analysis in a general streaming setting; among these are four Apache projects [5, 20, 1, 2] and proprietary solutions such as [3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 7, "context": "The only solution optimized for the network setting (to the best of our knowledge) is Blockmon [10, 19]; however, it does not support the use of deep learning algorithms.", "startOffset": 95, "endOffset": 103}, {"referenceID": 16, "context": "The only solution optimized for the network setting (to the best of our knowledge) is Blockmon [10, 19]; however, it does not support the use of deep learning algorithms.", "startOffset": 95, "endOffset": 103}, {"referenceID": 4, "context": "We can also find systems like Clipper [7] that allow predictions in a streaming setting.", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "Finally, several specific network problems have been solved using GPUs in recent years [17, 9, 22] Our solution is general and can be applied to a wide range of network problems ranging from traffic classification to user profiling while leveraging acceleration on heterogeneous hardware such as GPUs.", "startOffset": 87, "endOffset": 98}, {"referenceID": 6, "context": "Finally, several specific network problems have been solved using GPUs in recent years [17, 9, 22] Our solution is general and can be applied to a wide range of network problems ranging from traffic classification to user profiling while leveraging acceleration on heterogeneous hardware such as GPUs.", "startOffset": 87, "endOffset": 98}, {"referenceID": 17, "context": "Finally, several specific network problems have been solved using GPUs in recent years [17, 9, 22] Our solution is general and can be applied to a wide range of network problems ranging from traffic classification to user profiling while leveraging acceleration on heterogeneous hardware such as GPUs.", "startOffset": 87, "endOffset": 98}], "year": 2017, "abstractText": "We present Net2Vec, a flexible high-performance platform that allows the execution of deep learning algorithms in the communication network. Net2Vec is able to capture data from the network at more than 60Gbps, transform it into meaningful tuples and apply predictions over the tuples in real time. This platform can be used for different purposes ranging from traffic classification to network performance analysis. Finally, we showcase the use of Net2Vec by implementing and testing a solution able to profile network users at line rate using traces coming from a real network. We show that the use of deep learning for this case outperforms the baseline method both in terms of accuracy and performance.", "creator": "LaTeX with hyperref package"}}}