{"id": "1705.04249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "K-sets+: a Linear-time Clustering Algorithm for Data Points with a Sparse Similarity Measure", "abstract": "In this paper, we first propose a new iterative algorithm, called the K-sets+ algorithm for clustering data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality. We show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We then extend the applicability of the K-sets+ algorithm from data points in a semi-metric space to data points that only have a symmetric similarity measure. We show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space.\n\n\n\nThis paper presents a new iterative algorithm, called the K-sets+ algorithm for clustering data points in a metric space. We demonstrate that this new iteration algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We also show that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a", "histories": [["v1", "Thu, 11 May 2017 15:39:48 GMT  (301kb,D)", "http://arxiv.org/abs/1705.04249v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["cheng-shang chang", "chia-tai chang", "duan-shin lee", "li-heng liou"], "accepted": false, "id": "1705.04249"}, "pdf": {"name": "1705.04249.pdf", "metadata": {"source": "CRF", "title": "K-sets: a Linear-time Clustering Algorithm for Data Points with a Sparse Similarity Measure", "authors": ["Cheng-Shang Chang", "Chia-Tai Chang", "Duan-Shin Lee"], "emails": ["cschang@ee.nthu.edu.tw;", "s104064540@m104.nthu.edu.tw;", "lds@cs.nthu.edu.tw;", "dacapo1142@gmail.com"], "sections": [{"heading": null, "text": "keywords: Clustering; community detection\nI. INTRODUCTION\nThe problem of clustering is of fundamental importance to data analysis and it has been studied extensively in the literature (see e.g., the books [1], [2] and the historical review papers [3], [4]). In such a problem, there is a set of data points and a similarity (or dissimilarity) measure that measures how similar two data points are. The objective of a clustering algorithm is to cluster the data points so that data points within the same cluster are similar to each other and data points in different clusters are dissimilar. Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17]. However, clustering theories that justify the use of these clustering algorithms are still unsatisfactory.\nRecently, a mathematical clustering theory was developed in [18] for clustering data points in a metric space. In that theory, clusters can be formally defined and stated in various equivalent forms. In addition to the definition of a cluster in\na metric space, the K-sets algorithm was proposed in [18] to cluster data points in a metric space. The key innovation of the K-sets algorithm in [18] is the triangular distance that measures the distance from a data point to a set (of data points) by using the triangular inequality. Like the K-means algorithm, the K-sets algorithm is an iterative algorithm that repeatedly assigns every data point to the closest set in terms of the triangular distance. It was shown in [18] that the Ksets algorithm converges in a finite number of iterations and outputs K disjoint sets such that any two sets of these K sets are two disjoint clusters when they are viewed in isolation.\nThe first contribution of this paper is to extend the clustering theory/algorithm in [18] to data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality. Without the triangular inequality, the triangular distance in the K-sets algorithm is no longer nonnegative and thus the K-sets algorithm may not converge at all. Even if it converges, there is no guarantee that the output of the K-sets algorithm are clusters. To tackle this technical challenge, we propose the K-sets+ algorithm for clustering in a semi-metric space. In the K-sets+ algorithm, we need to modify the original definition of the triangular distance so that the nonnegativity requirement of the triangular distance can be lifted. For this, we propose the adjusted triangular distance (in Definition 6) and show (in Theorem 7) that the K-sets+ algorithm that repeatedly assigns every data point to the closest set in terms of the adjusted triangular distance converges in a finite number of iterations. Moreover, the K-sets+ algorithm outputs K disjoint sets such that any two sets of these K sets are two disjoint clusters when they are viewed in isolation.\nThe second contribution of this paper is to further extend the applicability of the K-sets+ algorithm from data points in a semi-metric space to data points that only have a symmetric similarity measure. A similarity measure is generally referred to as a bivariate function that measures how similar two data points are. We show there is a natural mapping from a symmetric similarity measure to a distance measure in a semi-metric space and the the K-sets+ algorithm that uses this distance measure converges to the same partition as that using the original symmetric similarity measure. Such an extension leads to great reduction of computational complexity for the K-sets+ algorithm. For an n \u00d7 n similarity matrix\nar X\niv :1\n70 5.\n04 24\n9v 1\n[ cs\n.D S]\n1 1\nM ay\n2 01\nwith only m nonzero elements in the matrix, we show that the computational complexity of the K-sets+ algorithm is O((Kn + m)I), where I is the number of iterations. The memory complexity to achieve that computational complexity is O(Kn + m). If the n \u00d7 n similarity matrix is sparse, i.e., m = O(n), then both the computational complexity and the memory complexity are linear in n.\nTo evaluate the performance of the K-sets+ algorithm, we conduct two experiments: (i) community detection of signed networks generated by the stochastic block model, and (ii) clustering of a real network from the WonderNetwork website [19]. Our experiments show that the K-sets+ algorithm is very effective in recovering the ground-truth edge signs even when the signs of a certain percentage of edges are flipped. For the real network of servers, the K-sets+ algorithm yields various interesting observations from the clustering results obtained by using the geographic distance matrix and the latency matrix."}, {"heading": "II. CLUSTERING IN A SEMI-METRIC SPACE", "text": "In this paper, we consider the clustering problem for data points in a semi-metric space. Specifically, we consider a set of n data points, \u2126 = {x1, x2, . . . , xn} and a distance measure d(x, y) for any two points x and y in \u2126. The distance measure d(\u00b7, \u00b7) is assumed to be a semi-metric and it satisfies the following three properties:\n(D1) (Nonnegativity) d(x, y) \u2265 0. (D2) (Null condition) d(x, x) = 0. (D3) (Symmetry) d(x, y) = d(y, x). The semi-metric assumption is weaker than the metric assumption in [18], where the distance measure is assumed to satisfy the triangular inequality. In [18], the K-sets algorithm was proposed for clustering data points in a metric space. One of the main contributions of this paper is to propose the K-sets+ algorithm (as a generalization of the K-sets algorithm) for clustering data points in a semi-metric space. As both Euclidean spaces and metric spaces are spacial cases of semimetric spaces, such a generalization allows us to unify the well-known K-means algorithm and the K-sets algorithm in [18]."}, {"heading": "A. Semi-cohesion measure", "text": "Given a semi-metric d(\u00b7, \u00b7) for \u2126, we define the induced semi-cohesion measure as follows:\ng(x, y) = 1\nn \u2211 z2\u2208\u2126 d(z2, y) + 1 n \u2211 z1\u2208\u2126 d(x, z1)\n\u2212 1 n2 \u2211 z2\u2208\u2126 \u2211 z1\u2208\u2126 d(z2, z1)\u2212 d(x, y). (1)\nIt is easy to verify that the induced semi-cohesion measure satisfies the following three properties:\n(C1) (Symmetry) g(x, y) = g(y, x) for all x, y \u2208 \u2126. (C2) (Null condition) For all x \u2208 \u2126, \u2211 y\u2208\u2126 g(x, y) = 0. (C3) (Nonnegativity) For all x, y in \u2126,\ng(x, x) + g(y, y) \u2265 2g(x, y). (2)\nMoreover, we have\nd(x, y) = (g(x, x) + g(y, y))/2\u2212 g(x, y). (3)\nAnalogous to the argument in [18], one can easily show the following duality theorem.\nTheorem 1: Consider a set of data points \u2126. For a semimetric d(\u00b7, \u00b7) that satisfies (D1)\u2013(D3), let\nd\u2217(x, y) = 1\nn \u2211 z2\u2208\u2126 d(z2, y) + 1 n \u2211 z1\u2208\u2126 d(x, z1)\n\u2212 1 n2 \u2211 z2\u2208\u2126 \u2211 z1\u2208\u2126 d(z2, z1)\u2212 d(x, y) (4)\nbe the induced semi-cohesion measure of d(\u00b7, \u00b7). On the other hand, for a semi-cohesion measure g(\u00b7, \u00b7) that satisfies (C1)\u2013 (C3), let\ng\u2217(x, y) = (g(x, x) + g(y, y))/2\u2212 g(x, y). (5)\nThen g\u2217(x, y) is a semi-metric that satisfies (D1)\u2013(D3). Moreover, d\u2217\u2217(x, y) = d(x, y) and g\u2217\u2217(x, y) = g(x, y) for all x, y \u2208 \u2126.\nIn view of the duality result, there is a one-to-one mapping between a semi-metric and a semi-cohesion measure. Thus, we will simply say data points are in a semi-metric space if there is either a semi-cohesion measure or a semi-metric associated with these data points."}, {"heading": "B. Clusters in a semi-metric space", "text": "In this section, we define what a cluster is for a set of data points in a semi-metric space.\nDefinition 2: (Cluster) Consider a set of n data points, \u2126 = {x1, x2, . . . , xn}, with a semi-cohesion measure g(\u00b7, \u00b7). For two sets S1 and S2, define\ng(S1, S2) = \u2211 x\u2208S1 \u2211 y\u2208S2 g(x, y). (6)\nTwo sets S1 and S2 are said to be cohesive (resp. incohesive) if g(S1, S2) \u2265 0 (resp. g(S1, S2) \u2264 0). A nonempty set S of \u2126 is called a cluster (with respect to the semi-cohesion measure g(\u00b7, \u00b7)) if\ng(S, S) \u2265 0. (7)\nFollowing the same argument in [18], one can also show a theorem for various equivalent statements for what a cluster is in a semi-metric space.\nTheorem 3: Consider a set of n data points, \u2126 = {x1, x2, . . . , xn}, with a semi-cohesion measure g(\u00b7, \u00b7). Let d(x, y) = (g(x, x)+g(y, y))/2\u2212g(x, y) be the induced semimetric and\nd\u0304(S1, S2) = 1 |S1| \u00d7 |S2| \u2211 x\u2208S1 \u2211 y\u2208S2 d(x, y). (8)\nbe the average \u201cdistance\u201d between two randomly selected points with one point in S1 and the other point in S2. Consider a nonempty set S that is not equal to \u2126. Let Sc = \u2126\\S be the set of points that are not in S. The following statements are equivalent.\n(i) The set S is a cluster, i.e., g(S, S) \u2265 0. (ii) The set Sc is a cluster, i.e., g(Sc, Sc) \u2265 0. (iii) The two sets S and Sc are incohesive, i.e., g(S, Sc) \u2264 0. (iv) The set S is more cohesive to itself than to Sc, i.e., g(S, S) \u2265 g(S, Sc). (v) 2d\u0304(S,\u2126)\u2212 d\u0304(\u2126,\u2126)\u2212 d\u0304(S, S) \u2265 0. (vi) 2d\u0304(S, Sc)\u2212 d\u0304(S, S)\u2212 d\u0304(Sc, Sc) \u2265 0. The condition for a cluster in Theorem 3(vi) is of particular importance as it allows us to characterize a cluster by using the average distance measures on the set S and its complement Sc. Such a condition will be used for proving our main result in Theorem 7.\nC. The K-sets+ algorithm\nThough the extensions of the duality result and the equivalent statements for clusters to semi-metric spaces are basically the same as those in [18], one problem arises when extending the K-sets algorithm to a semi-metric space. The key problem is that the triangular distance (\u2206-distance) defined in the K-sets algorithm (see Definition 4 below) might not be nonnegative in a semi-metric space.\nDefinition 4: (\u2206-distance [18]) For a symmetric bivariate function g(\u00b7, \u00b7) on a set of data points \u2126 = {x1, x2, . . . , xn}, the \u2206-distance from a point x to a set S, denoted by \u2206(x, S), is defined as follows:\n\u2206(x, S) = g(x, x)\u2212 2 |S| g(x, S) + 1 |S|2 g(S, S), (9)\nwhere g(S1, S2) is defined (6). Note from (1) that the \u2206-distance from a point x to a set S in a semi-metric space can also be written as follows:\n\u2206(x, S) = 1 |S|2 \u2211 z1\u2208S \u2211 z2\u2208S ( d(x, z1) + d(x, z2)\u2212 d(z1, z2) ) .\n(10) Now consider the data set of three points \u2126 = {x, y, z} with the semi-metric d(\u00b7, \u00b7) in Table I. For S = {y, z}, one can easily compute from (10) that \u2206(x, S) = \u22121 < 0.\nSince the \u2206-distance might not be nonnegative in a semimetric space, the proofs for the convergence and the performance guarantee of the K-sets algorithm in [18] are no longer valid. Fortunately, the \u2206-distance in a semi-metric space has the following (weaker) nonnegative property that will enable us to prove the performance guarantee of the K-sets+ algorithm (defined in Algorithm 1 later) for clustering data points in a semi-metric space.\nALGORITHM 1: The K-sets+ Algorithm Input: A data set \u2126 = {x1, x2, . . . , xn}, a symmetric\nmatrix G = (g(\u00b7, \u00b7)) and the number of sets K. Output: A partition of sets {S1, S2, . . . , SK}. (0) Initially, choose arbitrarily K disjoint nonempty sets S1, . . . , SK as a partition of \u2126. (1) for i = 1, 2, . . . , n do Compute the adjusted \u2206-distance \u2206a(xi, Sk) for each set Sk by using (9) and (12). Find the set to which the point xi is closest in terms of the adjusted \u2206-distance. Assign that point xi to that set. end (2) Repeat from (1) until there is no further change.\nProposition 5: Consider a data set \u2126 = {x1, x2, . . . , xn} with a semi-metric d(\u00b7, \u00b7). For any subset S of \u2126,\u2211\nx\u2208S \u2206(x, S) = |S|d\u0304(S, S) \u2265 0. (11)\nThe proof of (11) in Proposition 5 follows directly from (10) and (8). To introduce the K-sets+ algorithm, we first define the adjusted \u2206-distance in Definition 6 below.\nDefinition 6: (Adjusted \u2206-distance) The adjusted \u2206- distance from a point x to a set S, denoted by \u2206a(x, S), is defined as follows:\n\u2206a(x, S) =  |S| |S|+1\u2206(x, S), if x 6\u2208 S, |S| |S|\u22121\u2206(x, S), if x \u2208 S and |S| > 1, \u2212\u221e, if x \u2208 S and |S| = 1.\n(12) Instead of using the \u2206-distance for the assignment of a data point in the K-sets algorithm, we use the adjusted \u2206-distance for the assignment in the K-sets+ algorithm. We outline the K-sets+ algorithm in Algorithm 1. Note that in Algorithm 1, the bivariate function g(\u00b7, \u00b7) is required to be symmetric, i.e., g(x, y) = g(y, x). If g(\u00b7, \u00b7) is not symmetric, one may consider using g\u0302(x, y) = (g(x, y) + g(y, x))/2.\nIn the following theorem, we show the convergence and the performance guarantee of the K-sets+ algorithm. The proof of Theorem 7 is given in Appendix A.\nTheorem 7: For a data set \u2126 = {x1, x2, . . . , xn} with a symmetric matrix G = (g(\u00b7, \u00b7)), consider the clustering problem that finds a partition {S1, S2, . . . , SK} of \u2126 with a fixed K that maximizes the objective function \u2211K k=1 1 |Sk|g(Sk, Sk).\n(i) The K-sets+ algorithm in Algorithm 1 converges monotonically to a local optimum of the optimization problem in a finite number of iterations. (ii) Suppose that g(\u00b7, \u00b7) is a semi-cohesion measure. Let S1, S2, . . . , SK be the K sets when the algorithm converges. Then for all i 6= j, the two sets Si and Sj are two clusters if these two sets are viewed in isolation (by removing the data points not in Si\u222aSj from \u2126).\nIn particular, if K = 2, it then follows from Theorem 7(ii) that the K-sets+ algorithm yields two clusters for data points in a semi-metric space."}, {"heading": "III. BEYOND SEMI-METRIC SPACES", "text": ""}, {"heading": "A. Clustering with a symmetric similarity measure", "text": "In this section, we further extend the applicability of the K-sets+ algorithm to the clustering problem with a symmetric similarity measure. A similarity measure is generally referred to as a bivariate function that measures how similar two data points are. The clustering problem with a similarity measure is to cluster data points so that similar data points are clustered together. For a symmetric similarity measure g(\u00b7, \u00b7), we have shown in Theorem 7(i) that the K-sets+ algorithm in Algorithm 1 converges monotonically to a local optimum of the optimization problem \u2211K k=1 1 |Sk|g(Sk, Sk) within a finite number of iterations. Thus, the K-sets+ algorithm can be applied for clustering with a symmetric similarity measure. But what is the physical meaning of the sets returned by the K-sets+ algorithm for such a symmetric similarity measure? In order to answer this question, we show there is a natural semicohesion measure from a symmetric similarity measure and the K-sets+ algorithm that uses this semi-cohesion measure converges to the same partition as that using the original symmetric similarity measure (if they both use the same initial partition). As a direct consequence of Theorem 7(ii), any two sets returned by the K-sets+ algorithm for such a symmetric similarity measure are clusters with respect to the semi-cohesion measure when they are viewed in isolation.\nIn Lemma 8 below, we first show how one can map a symmetric similarity measure to a semi-cohesion measure. The proof is given in Appendix B.\nLemma 8: For a symmetric similarity measure g(\u00b7, \u00b7), let\ng\u0303(x, y) = g(x, y)\u2212 1 n g(x,\u2126)\u2212 1 n g(y,\u2126)\n+ 1 n2 g(\u2126,\u2126) + \u03c3\u03b4(x, y)\u2212 \u03c3 n , (13)\nwhere \u03b4(x, y) is the usual \u03b4 function (that has value 1 if x = y and 0 otherwise), and \u03c3 is a constant that satisfies\n\u03c3 \u2265 max x 6=y [g(x, y)\u2212 (g(x, x) + g(y, y))/2]. (14)\nThen the bivariate function g\u0303(\u00b7, \u00b7) in (14) is a semi-cohesion measure for \u2126, i.e., it satisfies (C1), (C2) and (C3).\nIn the following lemma, we further establish the connections for the \u2206-distance and the adjusted \u2206-distance between the original symmetric similarity measure g(\u00b7, \u00b7) and the semicohesion measure g\u0303(\u00b7, \u00b7) in (13). The proof is given in Appendix C.\nLemma 9: Let \u2206(x, S) (resp. \u2206\u0303(x, S)) be the \u2206-distance from a point x to a set S with respect to g(\u00b7, \u00b7) (resp. g\u0303(\u00b7, \u00b7)). Also, let \u2206a(x, S) (resp. \u2206\u0303a(x, S)) be the adjusted \u2206-distance from a point x to a set S with respect to g(\u00b7, \u00b7) (resp. g\u0303(\u00b7, \u00b7)). Then\n\u2206\u0303(x, S) = { \u2206(x, S) + \u03c3(1\u2212 1|S| ), if x 6\u2208 S, \u2206(x, S) + \u03c3(1 + 1|S| ), if x \u2208 S.\n(15)\nMoreover, \u2206\u0303a(x, S) = \u2206a(x, S) + \u03c3. (16)\nIt is easy to see that for any partition S1, S2, . . . , , SK\nK\u2211 k=1 1 |Sk| g\u0303(Sk, Sk)\n= K\u2211 k=1 1 |Sk| g(Sk, Sk)\u2212 1 n g(\u2126,\u2126) + (K \u2212 1)\u03c3. (17)\nThus, optimizing \u2211K\nk=1 1 |Sk|g(Sk, Sk) with respect to the\nsymmetric similarity measure g(\u00b7, \u00b7) is equivalent to optimizing\u2211K k=1 1 |Sk| g\u0303(Sk, Sk) with respect to the semi-cohesion measure g\u0303(\u00b7, \u00b7). Since\n\u2206\u0303a(x, S) = \u2206a(x, S) + \u03c3, (18)\nwe conclude that for these two optimization problems the K-sets+ algorithm converges to the same partition if they both use the same initial partition.\nNote that the K-means algorithm needs the data points to be in a Euclidean space, the kernel K-means algorithm needs the data points to be mapped into some Euclidean space, and the K-sets algorithm needs the data points to be in a metric space. The result in Lemma 9 shows that the K-sets+ algorithm lifts all the constraints on the data points and it can be operated merely by a symmetric similarity measure."}, {"heading": "B. Computational complexity", "text": "In this section, we address the computational complexity and the memory complexity of the K-sets+ algorithm. For an n \u00d7 n symmetric similarity matrix with only m nonzero elements in the matrix, we show that the computational complexity of the K-sets+ algorithm is O((Kn+m)I), where I is the number of iterations. The memory complexity to achieve that computational complexity is O(Kn+m).\nNote that the main computation overhead of the K-sets+ algorithm is mainly for the computation of the adjusted \u2206- distance. In view of (9), we know that one needs to compute g(x, S) and 1|S|2 g(S, S) in order to compute \u2206(x, S). Let\ng\u0304(S1, S2) = 1\n|S1||S2| g(S1, S2). (19)\nOur approach to reduce the computational complexity is to store g\u0304(Sk, Sk), k = 1, 2, . . . ,K and g(xi, Sk) for i = 1, 2, . . . , n and k = 1, 2, . . . ,K. Once these are stored in memory, one can compute the adjusted \u2206-distance \u2206a(xi, Sk) in O(1) steps. Suppose that xi is originally in the set S1 and it is reassigned to S2. Then g\u0304(S2 \u222a {xi}, S2 \u222a {xi}) can be updated by computing\n|S2|2\n(|S2|+ 1)2 g\u0304(S2, S2) + 2|S2| (|S2|+ 1)2 g\u0304({xi}, S2)\n+ 1\n(|S2|+ 1)2 g(xi, xi). (20)\nAlso, g\u0304(S1\\{xi}, S1\\{xi}) can be updated by computing\n|S1|2 (|S1| \u2212 1)2 ( g\u0304(S1, S1)\u2212 2 1 |S1| g\u0304({xi}, S1) ) + 1\n(|S1| \u2212 1)2 g(xi, xi). (21)\nSuch updates can be done in O(1) steps. On the other hand, let\nNei(i) = {j : g(xi, xj) 6= 0} (22)\nbe the set of data points that are neighbors of xi. Note that if g(xi, xi) 6= 0, then xi is also in Nei(i). When xi is moved from S1 to S2, we only need to update g(y, S1) and g(y, S2) for the data point y that is a neighbor of xi. Specifically, For each node y \u2208 Nei(i), update\ng(y, S2)\u2190 g(y, S2) + g(y, xi), g(y, S1)\u2190 g(y, S1)\u2212 g(y, xi).\nSuch updates can be done in O(Nei(i)) steps. Let\nm = n\u2211 i=1 |Nei(i)| (23)\nbe the total number of nonzero entries in the n\u00d7n symmetric matrix G = (g(\u00b7, \u00b7)). Then the total number of updates for all the data points xi, i = 1, 2, . . . , n, can be done in O(m) steps. Since we need to compute the \u2206-distance for the K sets for each data point in the for loop in Algorithm 1, the computational complexity of the K-sets+ algorithm of this implementation is thus O((Kn + m)I), where I is the number of iterations in the for loop of Algorithm 1. Regarding the memory complexity, one can store the symmetric matrix G = (g(\u00b7, \u00b7)) in the adjacency list form and that requires O(m) amount of memory. The memory requirement for storing Sk, k = 1, 2, . . . ,K, g\u0304(Sk, Sk), k = 1, 2, . . . ,K and g(xi, Sk) for i = 1, 2, . . . , n and k = 1, 2, . . . ,K is O(Kn). Thus, the overall memory complexity is O(Kn+m)."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we evaluate the performance of the K-sets+ algorithm by conducting two experiments: (i) community detection of signed networks generated by the stochastic block model in Section IV-A, and (ii) clustering of a real network from the WonderNetwork website [19] in Section IV-B."}, {"heading": "A. Community detection of signed networks with two communities", "text": "In this section, we conduct experiments for the K-sets+ algorithm by using the signed networks from the stochastic block model. We follow the procedure in [20] to generate the test networks. Each test network consists of n nodes and two ground-truth blocks, each with n/2 nodes. There are three key parameters pin, pout, and p for generating a test network. The parameter pin is the probability that there is a positive edge between two nodes within the same block and pout is the probability that there is a negative edge between two nodes in two different blocks. All edges are generated independently\naccording to pin and pout. After all the signed edges are generated, we then flip the sign of an edge independently with the crossover probability p.\nIn our experiments, the total number of nodes in the stochastic block model is n = 2000 with 1000 nodes in each block. Let c = (n/2\u2212 1)pin +npout/2 be the average degree of a node, and it is set to be 6, 8, and 10, respectively. Also, let cin = npin and cout = npout. The value of cin \u2212 cout is set to be 5 and that is used with the average degree c to uniquely determine pin and pout. The crossover probability p is in the range from 0.01 to 0.2 with a common step of 0.01. We generate 20 graphs for each p and c. We remove isolated nodes, and thus the exact numbers of nodes in the experiments might be less than 2000. We show the experimental results with each point averaged over 20 random graphs. The error bars represent the 95% confident intervals.\nTo test the K-sets+ algorithm, we use the similarity matrix G with\nG = A+ 0.5A2, (24)\nwhere A is the adjacency matrix of the signed network after randomly flipping the sign of an edge. Such a similarity matrix was suggested in [20] for community detection in signed networks as it allows us to \u201csee\u201d more than one step relationship between two nodes.\nIn Figure 1, we show our experimental results for edge accuracy (the percentage of edges that are correctly detected) as a function of the crossover probability p. As shown in Figure 1, the K-sets+ algorithm performs very well. For c = 10, it can still recover almost 100% of the edges even when the crossover probability p is 0.1 and roughly 95% of the edges when the crossover probability p is 0.2. Also, increasing the average degree c in the stochastic block model also increases the edge accuracy for the K-sets+ algorithm. This might be due to the fact that the tested signed networks with a larger average degree are more dense."}, {"heading": "B. Clustering of a real network", "text": "In this section, we test the K-sets+ algorithm on the real network from the WonderNetwork website [19]. In this dataset, there are 216 servers in different locations and the latency (measured by the round trip time) between any two servers of these 216 servers are recorded in real time. The dataset in our experiment is a snapshot on Sept. 24, 2016. For\nthis dataset, the triangular inequality is not always satisfied. For example, we notice that latency(Adelaide, Athens)=250, latency(Athens, Albany)=138, latency(Adelaide, Albany)=400, and 250 + 138 \u2264 400. In addition to the latency data, the WonderNetwork website also provides the geographic location of each server. We then use the Haversine formula to compute the distance between any two servers. In the WonderNetwork dataset, the latency measure from location L1 to location L2 is slightly different from that from location L2 to location L1. To ensure that the latency measure is a semi-metric, we simply symmetrize the latency matrix by taking the average of the latency measures from both directions. In our experiments, the number of clusters K is set to 5. We run 20 times of the K-sets+ algorithm by using the distance matrix and the latency matrix, respectively. In each of the 20 trials, the initial partition is randomly selected. The output partition that has the best objective value from these 20 trials is selected. The results for the distance matrix and the latency matrix are shown in Figure 2(a) and (b), respectively. In Figure 2(a) and (b), the servers that are in the same cluster are marked with the same colored marker. In view of Figure 2(a), we can observe that almost all the servers are partitioned into densely packed clusters except for the servers in South America and Africa. On the other hand, as shown in Figure 2(b), the servers in South America and Africa are merged into other clusters. To shed more light on these interesting differences, we compare the findings in Figure 2(b) to the Submarine Cable Map [21] (which records the currently active submarine cables). We notice that there are many cables placed around South America, connecting to the Caribbean and then to the East Coast of the United States. These cables greatly reduce the latency from South America to North America and thus cause the servers in South America to be clustered with the servers in North America. Similarly, there are a few connected cables between Africa and Europe. Therefore, the servers in Africa and Europe are clustered together. Due to many directly connected cables from Dubai to the Singapore Strait, servers around India are not clustered with the other servers in Asia. In particular, there are two servers marked with green dots, located in Jakarta and Singapore, that are clustered with servers in India even though they are geographically closer to the East Asia. These outliers have low latency to communicate with those servers in India. Finally, there are three servers, two in Russia and one in Lahore, that have low latency to the servers in Europe and they are clustered with the servers in Europe."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed the K-sets+ algorithm for clustering data points in a semi-metric space and data points that only have a symmetric similarity measure. We showed that the K-sets+ algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm in [18]. Moreover, both the computational complexity and the memory complexity are linear in n when the n \u00d7 n similarity matrix is sparse, i.e., m = O(n). To show the effectiveness of the K-sets+ algorithm, we also\nconducted various experiments by using a synthetic dataset from the stochastic block model and a real network from the WonderNetwork website [19]."}, {"heading": "APPENDIX A", "text": "In this section, we prove Theorem 7.\n(i) It suffices to show that if x is in a set S1 with |S1| > 1 and \u2206a(x, S2) < \u2206a(x, S1), then move point x from S1 to S2 increases the value of the objective function. Let Sk (resp. S\u2032k), k = 1, 2, . . . ,K, be the partition before (resp. after) the change. Also let R (resp. R\u2032) be the value of the objective function before (resp. after) the change. Then\nR\u2032 \u2212R = g(S1\\{x}, S1\\{x})\n|S1| \u2212 1 + g(S2 \u222a {x}, S2 \u222a {x}) |S2|+ 1\n\u2212g(S1, S1) |S1| \u2212 g(S2, S2) |S2| .\nSince\ng(S2 \u222a {x}, S2 \u222a {x}) = g(S2, S2) + 2g(x, S2) + g(x, x),\nwe have from (9) and (12) that\ng(S2 \u222a {x}, S2 \u222a {x}) |S2|+ 1 \u2212 g(S2, S2) |S2| = 2|S2|g(x, S2) + |S2|g(x, x)\u2212 g(S2, S2)\n|S2| \u00b7 (|S2|+ 1)\n= g(x, x)\u2212 |S2| |S2|+ 1 \u2206(x, S2) = g(x, x)\u2212\u2206a(x, S2).\nOn the other hand, we note that\ng(S1\\{x}, S1\\{x}) = g(S1, S1)\u2212 2g(x, S1) + g(x, x). (25)\nUsing (25), (9) and (12) yields\ng(S1\\{x}, S1\\{x}) |S1| \u2212 1 \u2212 g(S1, S1) |S1| = \u22122|S1|g(x, S1) + |S1|g(x, x) + g(S1, S1)\n|S1| \u00b7 (|S1| \u2212 1)\n= \u2212g(x, x) + |S1| |S1| \u2212 1 \u2206(x, S1) = \u2212g(x, x) + \u2206a(x, S1).\nThus, R\u2032 \u2212R = \u2206a(x, S1)\u2212\u2206a(x, S2) > 0.\nAs the objective value is non-increasing after a change of the partition, there is no loop in the algorithm. Since the number of partitions is finite, the algorithm thus converges in a finite number of steps (iterations). (ii) Let d(\u00b7, \u00b7) be the induced semi-metric. In view of Theorem 3(vi), it suffices to show that for all i 6= j\n2d\u0304(Si, Sj)\u2212 d\u0304(Si, Si)\u2212 d\u0304(Sj , Sj) \u2265 0. (26)\nIf the set Si contains a single element x, then\nd\u0304(Si, Si) = d(x, x) = 0.\nThus, the inequality in (26) holds trivially if |Si| = |Sj | = 1. Now suppose that min(|Si|, |Sj |) \u2265 2. Without loss of generality, we assume that |Si| \u2265 2. When the K-sets+ algorithm converges, we know that for any x \u2208 Si,\n\u2206a(x, Si) \u2264 \u2206a(x, Sj).\nSumming over x \u2208 Si yields\u2211 x\u2208Si \u2206a(x, Si) \u2264 \u2211 x\u2208Si \u2206a(x, Sj). (27)\nNote from (12) that for any x \u2208 Si,\n\u2206a(x, Si) = |Si| |Si| \u2212 1 \u2206(x, Si),\nand \u2206a(x, Sj) =\n|Sj | |Sj |+ 1 \u2206(x, Sj).\nThus, it follows from (27) that\n|Si| |Si| \u2212 1 \u2211 x\u2208Si \u2206(x, Si) \u2264 |Sj | |Sj |+ 1 \u2211 x\u2208Si \u2206(x, Sj). (28)\nNote from (10) that\u2211 x\u2208Si \u2206(x, Si) = |Si|d\u0304(Si, Si), (29) and that\u2211 x\u2208Si \u2206(x, Sj) = |Si|(2d\u0304(Si, Sj)\u2212 d\u0304(Sj , Sj)). (30)\nSince d(\u00b7, \u00b7) is the induced semi-metric, we know from Proposition 5 that \u2211\nx\u2208Si\n\u2206(x, Si) \u2265 0.\nUsing this in (28) yields\u2211 x\u2208Si \u2206(x, Sj) \u2265 0.\nThus, we have from (28) that\u2211 x\u2208Si \u2206(x, Si) \u2264 |Si| |Si| \u2212 1 \u2211 x\u2208Si \u2206(x, Si)\n\u2264 |Sj | |Sj |+ 1 \u2211 x\u2208Si \u2206(x, Sj) \u2264 \u2211 x\u2208Si \u2206(x, Sj). (31)\nThat the inequality in (26) holds follows directly from (29), (30) and (31)."}, {"heading": "APPENDIX B", "text": "In this section, we prove Lemma 8. Since g(\u00b7, \u00b7) is symmetric, clearly g\u0303(\u00b7, \u00b7) is also symmetric. Thus, (C1) is satisfied trivially. To see that (C2) is satisfied, observe from (13) that\u2211\ny\u2208\u2126 g\u0303(x, y) = g(x,\u2126)\u2212 g(x,\u2126)\u2212 1 n g(\u2126,\u2126)\n+ 1\nn g(\u2126,\u2126) + \u03c3 \u2212 \u03c3 = 0. (32)\nTo see that (C3) holds, we note that\ng\u0303(x, x) = g(x, x)\u2212 2 n g(x,\u2126) + 1 n2 g(\u2126,\u2126) + (n\u2212 1) n \u03c3, (33)\nand that\ng\u0303(y, y) = g(y, y)\u2212 2 n g(y,\u2126) + 1 n2 g(\u2126,\u2126) + (n\u2212 1) n \u03c3.\nThus, for x 6= y, we have from (14) that\ng\u0303(x, x) + g\u0303(y, y)\u2212 2g\u0303(x, y) = g(x, x) + g(y, y)\u2212 2g(x, y) + 2\u03c3 \u2265 0."}, {"heading": "APPENDIX C", "text": "In this section, we prove Lemma 9. Note from Definition 4 and Definition 6 that\n\u2206\u0303(x, S) = g\u0303(x, x)\u2212 2 |S| g\u0303(x, S) + 1 |S|2 g\u0303(S, S), (34)\nand that\n\u2206\u0303a(x, S) =  |S| |S|+1\u2206\u0303(x, S), if x 6\u2208 S, |S| |S|\u22121\u2206\u0303(x, S), if x \u2208 S and |S| > 1, \u2212\u221e, if x \u2208 S and |S| = 1.\n(35) To show (15), we need to consider two cases: (i) x \u2208 S and (ii) x 6\u2208 S. For both cases, we have from (13) that\ng\u0303(x, x) = g(x, x)\u2212 2 n g(x,\u2126) + 1 n2 g(\u2126,\u2126) + (n\u2212 1) n \u03c3, (36)\nand\ng\u0303(S, S) = g(S, S)\u2212 |S| n g(S,\u2126)\u2212 |S| n g(S,\u2126)\n+ |S|2 n2 g(\u2126,\u2126) + \u03c3|S| \u2212 \u03c3 |S| 2 n . (37)\nNow we consider the first case that x \u2208 S. In this case, note that for x \u2208 S\ng\u0303(x, S) = g(x, S)\u2212 |S| n g(x,\u2126)\u2212 1 n g(S,\u2126)\n+ |S| n2 g(\u2126,\u2126) + \u03c3 \u2212 \u03c3 |S| n . (38)\nUsing (36), (38) and (37) in (34) yields\n\u2206\u0303(x, S) = g\u0303(x, x)\u2212 2 |S| g\u0303(x, S) + 1 |S|2 g\u0303(S, S)\n= g(x, x)\u2212 2 |S| g(x, S) + 1 |S|2 g(S, S)\n+\u03c3(1\u2212 1 |S| )\n= \u2206(x, S) + \u03c3(1\u2212 1 |S| ). (39)\nFrom (35), it then follows that\n\u2206\u0303a(x, S) = |S| |S| \u2212 1 \u2206\u0303(x, S) = |S| |S| \u2212 1 ( \u2206(x, S) + \u03c3(1\u2212 1 |S| ) )\n= \u2206a(x, S) + \u03c3. (40)\nNow we consider the second case that x 6\u2208 S. In this case, note that for x 6\u2208 S\ng\u0303(x, S) = g(x, S)\u2212 |S| n g(x,\u2126)\u2212 1 n g(S,\u2126)\n+ |S| n2 g(\u2126,\u2126)\u2212 \u03c3 |S| n . (41)\nUsing (36), (41) and (37) in (34) yields\n\u2206\u0303(x, S) = g\u0303(x, x)\u2212 2 |S| g\u0303(x, S) + 1 |S|2 g\u0303(S, S)\n= g(x, x)\u2212 2 |S| g(x, S) + 1 |S|2 g(S, S)\n+\u03c3(1 + 1\n|S| )\n= \u2206(x, S) + \u03c3(1 + 1\n|S| ). (42)\nFrom (35), it then follows that\n\u2206\u0303a(x, S) = |S| |S|+ 1 \u2206\u0303(x, S) = |S| |S|+ 1 ( \u2206(x, S) + \u03c3(1 + 1 |S| ) )\n= \u2206a(x, S) + \u03c3. (43)"}], "references": [{"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J. Leskovec", "J.D. Ullman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR), vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory, vol. 28, no. 2, pp. 129\u2013137, 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "k-means++ under approximation stability", "author": ["M. Agarwal", "R. Jaiswal", "A. Pal"], "venue": "Theory and Applications of Models of Computation. Springer, 2013, pp. 84\u201395.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding groups in data: an introduction to cluster analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A new partitioning around medoids algorithm", "author": ["M. Van der Laan", "K. Pollard", "J. Bryan"], "venue": "Journal of Statistical Computation and Simulation, vol. 73, no. 8, pp. 575\u2013584, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A simple and fast algorithm for K-medoids clustering", "author": ["H.-S. Park", "C.-H. Jun"], "venue": "Expert Systems with Applications, vol. 36, no. 2, pp. 3336\u2013 3341, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and Computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176\u2013190, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral redemption in clustering sparse networks", "author": ["F. Krzakala", "C. Moore", "E. Mossel", "J. Neeman", "A. Sly", "L. Zdeborov\u00e1", "P. Zhang"], "venue": "Proceedings of the National Academy of Sciences, vol. 110, no. 52, pp. 20 935\u201320 940, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in KDD, vol", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Cluster analysis: a further approach based on density estimation", "author": ["A. Cuevas", "M. Febrero", "R. Fraiman"], "venue": "Computational Statistics & Data Analysis, vol. 36, no. 4, pp. 441\u2013459, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "A density-based cluster validity approach using multi-representatives", "author": ["M. Halkidi", "M. Vazirgiannis"], "venue": "Pattern Recognition Letters, vol. 29, no. 6, pp. 773\u2013786, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Clustering under approximation stability", "author": ["M.-F. Balcan", "A. Blum", "A. Gupta"], "venue": "Journal of the ACM (JACM), vol. 60, no. 2, p. 8, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A mathematical theory for clustering in metric spaces", "author": ["C.-S. Chang", "W. Liao", "Y.-S. Chen", "L.-H. Liou"], "venue": "IEEE Transactions on Network Science and Engineering, vol. 3, no. 1, pp. 2\u201316, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Community detection in signed networks: an error-correcting code approach", "author": ["S.-M. Lu", "L.-H. Liou", "C.-S. Chang", "D.-S. Lee"], "venue": "submitted for publication, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": ", the books [1], [2] and the historical review papers [3], [4]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 179, "endOffset": 182}, {"referenceID": 2, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 206, "endOffset": 209}, {"referenceID": 3, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 250, "endOffset": 253}, {"referenceID": 6, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 255, "endOffset": 258}, {"referenceID": 7, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 260, "endOffset": 263}, {"referenceID": 8, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 311, "endOffset": 315}, {"referenceID": 9, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 317, "endOffset": 321}, {"referenceID": 10, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 323, "endOffset": 327}, {"referenceID": 11, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 329, "endOffset": 333}, {"referenceID": 12, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 371, "endOffset": 375}, {"referenceID": 13, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 377, "endOffset": 381}, {"referenceID": 14, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 383, "endOffset": 387}, {"referenceID": 15, "context": "Clustering is in general considered as an ill-posed problem and there are already many clustering algorithms proposed in the literature, including the hierarchical algorithm [1], [2], the K-means algorithm [4], [5], [6], the K-medoids algorithm [1], [7], [8], [9], the kernel and spectral clustering algorithms [10], [11], [12], [13], and the definition-based algorithms [14], [15], [16], [17].", "startOffset": 389, "endOffset": 393}, {"referenceID": 16, "context": "Recently, a mathematical clustering theory was developed in [18] for clustering data points in a metric space.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "In addition to the definition of a cluster in a metric space, the K-sets algorithm was proposed in [18] to cluster data points in a metric space.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "The key innovation of the K-sets algorithm in [18] is the triangular distance that measures the distance from a data point to a set (of data points) by using the triangular inequality.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "It was shown in [18] that the Ksets algorithm converges in a finite number of iterations and outputs K disjoint sets such that any two sets of these K sets are two disjoint clusters when they are viewed in isolation.", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "The first contribution of this paper is to extend the clustering theory/algorithm in [18] to data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality.", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "The semi-metric assumption is weaker than the metric assumption in [18], where the distance measure is assumed to satisfy the triangular inequality.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "In [18], the K-sets algorithm was proposed for clustering data points in a metric space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "As both Euclidean spaces and metric spaces are spacial cases of semimetric spaces, such a generalization allows us to unify the well-known K-means algorithm and the K-sets algorithm in [18].", "startOffset": 185, "endOffset": 189}, {"referenceID": 16, "context": "Analogous to the argument in [18], one can easily show the following duality theorem.", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "Following the same argument in [18], one can also show a theorem for various equivalent statements for what a cluster is in a semi-metric space.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Though the extensions of the duality result and the equivalent statements for clusters to semi-metric spaces are basically the same as those in [18], one problem arises when extending the K-sets algorithm to a semi-metric space.", "startOffset": 144, "endOffset": 148}, {"referenceID": 16, "context": "Definition 4: (\u2206-distance [18]) For a symmetric bivariate function g(\u00b7, \u00b7) on a set of data points \u03a9 = {x1, x2, .", "startOffset": 26, "endOffset": 30}, {"referenceID": 16, "context": "Since the \u2206-distance might not be nonnegative in a semimetric space, the proofs for the convergence and the performance guarantee of the K-sets algorithm in [18] are no longer valid.", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "We follow the procedure in [20] to generate the test networks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "Such a similarity matrix was suggested in [20] for community detection in signed networks as it allows us to \u201csee\u201d more than one step relationship between two nodes.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "We showed that the K-sets algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm in [18].", "startOffset": 152, "endOffset": 156}], "year": 2017, "abstractText": "In this paper, we first propose a new iterative algorithm, called the K-sets algorithm for clustering data points in a semi-metric space, where the distance measure does not necessarily satisfy the triangular inequality. We show that the K-sets algorithm converges in a finite number of iterations and it retains the same performance guarantee as the K-sets algorithm for clustering data points in a metric space. We then extend the applicability of the K-sets algorithm from data points in a semi-metric space to data points that only have a symmetric similarity measure. Such an extension leads to great reduction of computational complexity. In particular, for an n\u00d7 n similarity matrix with m nonzero elements in the matrix, the computational complexity of the K-sets algorithm is O((Kn+m)I), where I is the number of iterations. The memory complexity to achieve that computational complexity is O(Kn + m). As such, both the computational complexity and the memory complexity are linear in n when the n \u00d7 n similarity matrix is sparse, i.e., m = O(n). We also conduct various experiments to show the effectiveness of the K-sets algorithm by using a synthetic dataset from the stochastic block model and a real network from the WonderNetwork website. keywords: Clustering; community detection", "creator": "LaTeX with hyperref package"}}}