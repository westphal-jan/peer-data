{"id": "1705.04434", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2017", "title": "Arc-swift: A Novel Transition System for Dependency Parsing", "abstract": "Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition. This allows the parser to leverage lexical information more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small beam size. Our parsers reduce error by 3.7--7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies. This paper outlines the benefits of arc-swift.\n\n\n\nThe paper examines the effects of arc-swift on grammar and behavior in the English Universal Dependencies paradigm. It is important to note that the authors chose this paper based on a rather conventional method. As the name suggests, arc-swift improves the ability of the parser to read text from the lexical content of the lexical content of the sentence context rather than to read text in another way.\nThis paper shows the benefits of arc-swift in reducing the error propagation of the lexical content of the sentence context rather than to read text in another way. As the name suggests, arc-swift improves the ability of the parser to read text from the lexical content of the sentence context rather than to read text in another way. In this paper, we consider the use of arc-swift to reduce the error propagation of the lexical content of the sentence context instead of to read text in another way.\nThe authors propose two approaches to arc-swift: one that reduces the error propagation of the lexical content of the sentence context and a second that improves its efficiency. First, arc-swift reduces the error propagation of the lexical content of the sentence context. This means that when we use the arc-swift-filter function to read text from the lexical content of the sentence context and read text in another way, the text is more quickly read, even as the sentence context is larger and faster. This approach also increases the efficiency of the Arc-swift-filter function to read text from the lexical content of the sentence context.\nThe authors propose a novel approach to arc-swift: a recursive function that allows the parser to read text from the lexical content of the sentence context and read text in another way. This approach also increases the efficiency", "histories": [["v1", "Fri, 12 May 2017 03:44:34 GMT  (156kb,D)", "http://arxiv.org/abs/1705.04434v1", "Accepted at ACL 2017"]], "COMMENTS": "Accepted at ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peng qi 0003", "christopher d manning"], "accepted": true, "id": "1705.04434"}, "pdf": {"name": "1705.04434.pdf", "metadata": {"source": "CRF", "title": "Arc-swift: A Novel Transition System for Dependency Parsing", "authors": ["Peng Qi", "Christopher D. Manning"], "emails": ["manning}@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016).\nAttractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003). Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007).\nHowever, the transition systems employed in state-of-the-art dependency parsers usually define very local transitions. At each step, only one or two words are affected, with very local attachments made. As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate\u2192chopsticks in Figure 1, which requires two transitions). This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).\nIn this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence). Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).\nWe make two main contributions in this paper. Firstly, we introduce a novel transition system for dependency parsing, which alleviates the difficulty of distant attachments in previous systems by allowing direct attachments anywhere in the stack. Secondly, we compare parsers by the number of mistakes they make in common linguistic con-\nar X\niv :1\n70 5.\n04 43\n4v 1\n[ cs\n.C L\n] 1\n2 M\nay 2\n01 7\nstructions. We show that arc-swift parsers reduce errors in attaching prepositional phrases and conjunctions compared to parsers using existing transition systems."}, {"heading": "2 Transition-based Dependency Parsing", "text": "Transition-based dependency parsing is performed by predicting transitions between states (see Figure 1 for an example). Parser states are usually written as (\u03c3|i, j|\u03b2,A), where \u03c3|i denotes the stack with token i on the top, j|\u03b2 denotes the buffer with token j at its leftmost, and A the set of dependency arcs. Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse. A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1\nArc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing. It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack. This system builds the parse bottom-up, i.e., a constituent is only attached to its head after it has received all of its dependents. A potential drawback is that during parsing, it is difficult to predict if a constituent has consumed all of its right dependents. Arc-eager (Nivre, 2003) remedies this drawback by defining arc-inducing transitions that operate between the stack and the buffer. As a result, a constituent no longer needs to be complete\n1We only focus on projective parses for the scope of this paper.\nbefore it can be attached to its head to the left, as a right arc doesn\u2019t prevent the attached dependent from taking further dependents of its own.2 Kuhlmann et al. (2011) propose a hybrid system derived from a tabular parsing scheme, which they have shown both arc-standard and arc-eager can be derived from. Arc-hybrid combines LArc from arc-eager and RArc from arc-standard to build dependencies bottom-up.\n3 Non-local Transitions with arc-swift\nThe traditional transition systems discussed in Section 2 only allow very local transitions affecting one or two words, which makes long-distance dependencies difficult to predict. To illustrate the limitation of local transitions, consider parsing the following sentences:\nI ate fish with ketchup. I ate fish with chopsticks.\nThe two sentences have almost identical structures, with the notable difference that the prepositional phrase is complementing the direct object in the first case, and the main verb in the second.\nFor arc-standard and arc-hybrid, the parser would have to decide between Shift and RArc when the parser state is as shown in Figure 3a, where ? stands for either \u201cketchup\u201d or \u201cchopsticks\u201d.3 Similarly, an arc-eager parser would deal with the state shown in Figure 3b. Making the correct transition requires information about context words \u201cate\u201d and \u201cfish\u201d, as well as \u201c?\u201d.\n2A side-effect of arc-eager is that there is sometimes spurious ambiguity between Shift and Reduce transitions. For the example in Figure 1, the first Reduce can be inserted before the third Shift without changing the correctness of the resulting parse, i.e., both are feasible at that time.\n3For this example, we assume that the sentence is being parsed into Universal Dependencies.\nParsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).\nIn contrast, inspired by graph-based parsers, we propose arc-swift, which defines non-local transitions as shown in Figure 2. This allows direct comparison of different attachment points, and provides a direct solution to parsing the two example sentences. When the arc-swift parser encounters a state identical to Figure 3b, it could directly compare transitions RArc[1] and RArc[2] instead of evaluating between local transitions. This results in a direct attachment much like that in a graph-based parser, informed by lexical information about affinity of the pairs of words.\nArc-swift also bears much resemblance to arceager. In fact, an LArc[k] transition can be viewed as k\u2212 1 Reduce operations followed by one LArc in arc-eager, and similarly for RArc[k]. Reduce is no longer needed in arc-swift as it becomes part of LArc[k] and RArc[k], removing the ambiguity in derived transitions in arc-eager. arc-swift is also equivalent to arc-eager in terms of soundness and completeness.4 A caveat is that the worst-case time complexity of arc-swift is O(n2) instead of O(n), which existing transition-based parsers enjoy. However, in practice the runtime is nearly\n4This is easy to show because in arc-eager, all Reduce transitions can be viewed as preparing for a later LArc or RArc transition. We also note that similar to arc-eager transitions, arc-swift transitions must also satisfy certain pre-conditions. Specifically, an RArc[k] transition requires that the top k \u2212 1 elements in the stack are already attached; LArc[k] additionally requires that the k-th element is unattached, resulting in no more than one feasible LArc candidate for any parser state.\nlinear, thanks to the usually small number of reducible tokens in the stack."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data and Model", "text": "We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD). PTB-SD is converted to Stanford Dependencies (De Marneffe and Manning, 2008) with CoreNLP 3.3.0 (Manning et al., 2014) following previous work. We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations.\nOur model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification. For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input. For arc-swift, features of the head and dependent tokens for each arc-inducing transition are concatenated to compute scores for classification, and features of the leftmost buffer token is used for Shift. For other details we defer to Appendix A. The full specification of the model can also be found in our released code online at https://github. com/qipeng/arc-swift."}, {"heading": "4.2 Results", "text": "We use static oracles for all transition systems, and for arc-eager we implement oracles that always Shift/Reduce when ambiguity is present (arceager-S/R). We evaluate our parsers with greedy parsing (i.e., beam size 1). The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).\nFor each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD. The resulting pvalues are analyzed with a 10-group BonferroniHolm test, with results shown in Table 2. We note\n5In the interest of space, we abbreviate all transition systems (TS) as follows in tables: asw for arc-swift, asd for arcstandard, aeS/R for arc-eager-S/R, and ah for arc-hybrid.\nthat with almost the same implementation, arcswift parsers significantly outperform those using traditional transition systems. We also analyzed the performance of parsers on attachments of different distances. As shown in Figure 4, arc-swift is equally accurate as existing systems for short dependencies, but is more robust for longer ones.\nWhile arc-swift introduces direct long-distance transitions, it also shortens the overall sequence necessary to induce the same parse. A parser could potentially benefit from both factors: direct attachments could make an easier classification task, and shorter sequences limit the effect of error propagation. However, since the two effects are correlated in a transition system, precise attribution of the gain is out of the scope of this paper.\nComputational efficiency. We study the computational efficiency of the arc-swift parser by\n6https://github.com/tensorflow/models/ blob/master/syntaxnet/g3doc/universal.md\ncomparing it to an arc-eager parser. On the PTBSD development set, the average transition sequence length per sentence of arc-swift is 77.5% of that of arc-eager. At each step of parsing, arc-swift needs to evaluate only about 1.24 times the number of transition candidates as arc-eager, which results in very similar runtime. In contrast, beam search with beam size 2 for arc-eager requires evaluating 4 times the number of transition candidates compared to greedy parsing, which results in a UAS 0.14% worse and LAS 0.22% worse for arc-eager compared to greedily decoded arcswift."}, {"heading": "4.3 Linguistic Analysis", "text": "We automatically extracted all labelled attachment errors by error type (incorrect attachment or relation), and categorized a few top parser errors by hand into linguistic constructions. Results on PTB-SD are shown in Table 3.7 We note that the arc-swift parser improves accuracy on prepositional phrase (PP) and conjunction attachments, while it remains comparable to other parsers on other common errors. Analysis on EN-UD shows a similar trend. As shown in the table, there are still many parser errors unaccounted for in our analysis. We leave this to future work.\n7We notice that for some examples the parsers predicted a ccomp (complement clause) attachment to verbs \u201csays\u201d and \u201csaid\u201d, where the CoreNLP output simply labelled the relation as dep (unspecified). For other examples the relation between the prepositions in \u201cout of\u201d is labelled as prep (preposition) instead of pcomp (prepositional complement). We suspect this is due to the converter\u2019s inability to handle certain corner cases, but further study is warranted."}, {"heading": "5 Related Work", "text": "Previous work has also explored augmenting transition systems to facilitate longer-range attachments. Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift. A notable difference is that their transitions retain tokens between the head and dependent. Ferna\u0301ndez-Gonza\u0301lez and Go\u0301mezRodr\u0131\u0301guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed. However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively. More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right \u201cspines\u201d of the top two nodes in the stack. While this work shares very similar motivations as arc-swift, it requires additional data structures to keep track of the left and right spines of nodes. This transition system also introduces spurious ambiguity where multiple transition sequences could lead to the same correct parse, which necessitates easy-first training to achieve a more noticeable improvement over arcstandard. In contrast, arc-swift can be easily implemented given the parser state alone, and does not give rise to spurious ambiguity.\nFor a comprehensive study of transition systems for dependency parsing, we refer the reader to (Bohnet et al., 2016), which proposed a generalized framework that could derive all of the traditional transition systems we described by configuring the size of the active token set and the maximum arc length, among other control parameters. However, this framework does not cover\narc-swift in its original form, as the authors limit each of their transitions to reduce at most one token from the active token set (the buffer). On the other hand, the framework presented in (Go\u0301mezRodr\u0131\u0301guez and Nivre, 2013) does not explicitly make this constraint, and therefore generalizes to arc-swift. However, we note that arc-swift still falls out of the scope of existing discussions in that work, by introducing multiple Reduces in a single transition."}, {"heading": "6 Conclusion", "text": "In this paper, we introduced arc-swift, a novel transition system for dependency parsing. We also performed linguistic analyses on parser outputs and showed arc-swift parsers reduce errors in conjunction and adverbial attachments compared to parsers using traditional transition systems."}, {"heading": "Acknowledgments", "text": "We thank Timothy Dozat, Arun Chaganty, Danqi Chen, and the anonymous reviewers for helpful discussions. Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract No. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government."}, {"heading": "A Model and Training Details", "text": "Our model setup is similar to that of (Kiperwasser and Goldberg, 2016) (See Figure 5). We employ two blocks of bidirectional long short-term memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997) that share very similar structures, one for part-of-speech (POS) tagging, the other for parsing. Both BiLSTMs have 400 hidden units in each direction, and the output of both are concatenated and fed into a dense layer of rectified linear units (ReLU) before 32-dimensional representations are derived as classification features. As the input to the tagger BiLSTM, we represent words with 100-dimensional word embeddings, initialized with GloVe vectors (Pennington et al., 2014).8 The output distribution of the tagger classifier is used to compute a weighted sum of 32- dimensional POS embeddings, which is then concatenated with the output of the tagger BiLSTM (800-dimensional per token) as the input to the parser BiLSTM. For the parser BiLSTM, we use two separate sets of dense layers to derive a \u201chead\u201d and a \u201cdependent\u201d representation for each token. These representations are later merged according to the parser state to make transition predictions.\nFor traditional transition systems, we follow (Kiperwasser and Goldberg, 2016) by featurizing the top 3 tokens on the stack and the leftmost token in the buffer. To derive features for each token, we take its head representation vhead and dependent representation vdep, and perform the following biaffine combination\nvfeat,i = [f(vhead, vdep)]i = ReLU ( v>headWivdep + b > i vhead\n+ c>i vdep + di ) (1)\nwhere Wi \u2208 R32\u00d732, bi, ci \u2208 R32, and di is a scalar for i = 1, . . . , 32. The resulting 32- dimensional features are concatenated as the input\n8We also kept the vectors of the top 400k words trained on Wikipedia and English Gigaword for a broader coverage of unseen words.\nto a fixed-dimensional softmax classifier for transition decisions.\nFor arc-swift, we featurize for each arcinducing transition with the same composition function in Equation (1) with vhead of the head token and vdep of the dependent token of the arc to be induced. For Shift, we simply combine vhead and vdep of the leftmost token in the buffer with the biaffine combination, and obtain its score by computing the inner-product of the feature and a vector. At each step, the scores of all feasible transitions are normalized to a probability distribution by a softmax function.\nIn all of our experiments, the parsers are trained to maximize the log likelihood of the desired transition sequence, along with the tagger being trained to maximize the log likelihood of the correct POS tag for each token.\nTo train the parsers, we use the ADAM optimizer (Kingma and Ba, 2014), with \u03b22 = 0.9, an initial learning rate of 0.001, and minibatches of size 32 sentences. Parsers are trained for 10 passes through the dataset on PTB-SD. We also find that annealing the learning rate by a factor of 0.5 for every pass after the 5th helped improve performance. For EN-UD, we train for 30 passes, and anneal the learning rate for every 3 passes after the 15th due to the smaller size of the dataset. For all of the biaffine combination layers and dense layers, we dropout their units with a small probability of 5%. Also during training time, we randomly replace 10% of the input words by an artificial \u3008UNK\u3009 token, which is then used to replace\nall unseen words in the development and test sets. Finally, we repeat each experiment with 3 independent random initializations, and use the average result for reporting and statistical significance tests.\nThe code for the full specification of our models and aforementioned training details are available at https://github.com/qipeng/ arc-swift."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Leveraging linguistic structure for open domain information extraction", "author": ["Gabor Angeli", "Melvin Johnson Premkumar", "Christopher D Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Angeli et al\\.,? 2015", "shortCiteRegEx": "Angeli et al\\.", "year": 2015}, {"title": "Experiments with a multilanguage non-projective dependency parser", "author": ["Giuseppe Attardi."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, pages 166\u2013170.", "citeRegEx": "Attardi.,? 2006", "shortCiteRegEx": "Attardi.", "year": 2006}, {"title": "Generalized transition-based dependency parsing via control parameters", "author": ["Bernd Bohnet", "Ryan McDonald", "Emily Pitler", "Ji Ma."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. volume 1, pages", "citeRegEx": "Bohnet et al\\.,? 2016", "shortCiteRegEx": "Bohnet et al\\.", "year": 2016}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts."], "venue": "Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750. http://www.aclweb.org/anthology/D14-1082.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "The Stanford typed dependencies representation", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "COLING 2008: Proceedings of the Workshop on Cross-framework and Cross-domain Parser Evaluation. pages 1\u20138.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Improving transition-based dependency parsing with buffer transitions", "author": ["Daniel Fern\u00e1ndez-Gonz\u00e1lez", "Carlos G\u00f3mezRodr\u0131\u0301guez"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and G\u00f3mezRodr\u0131\u0301guez.,? \\Q2012\\E", "shortCiteRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and G\u00f3mezRodr\u0131\u0301guez.", "year": 2012}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "COLING. pages 959\u2013976. http://www.aclweb.org/anthology/C12-1059.", "citeRegEx": "Goldberg and Nivre.,? 2012", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2012}, {"title": "Divisible transition systems and multiplanar dependency parsing", "author": ["Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Joakim Nivre"], "venue": "Computational Linguistics", "citeRegEx": "G\u00f3mez.Rodr\u0131\u0301guez and Nivre.,? \\Q2013\\E", "shortCiteRegEx": "G\u00f3mez.Rodr\u0131\u0301guez and Nivre.", "year": 2013}, {"title": "Dependency language models for sentence completion", "author": ["Joseph Gubbins", "Andreas Vlachos."], "venue": "EMNLP. volume 13, pages 1405\u20131410. http://www.aclweb.org/anthology/D13-1143.", "citeRegEx": "Gubbins and Vlachos.,? 2013", "shortCiteRegEx": "Gubbins and Vlachos.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics (TACL) https://aclweb.org/anthology/Q16-1023.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Dynamic programming algorithms for transition-based dependency parsers", "author": ["Marco Kuhlmann", "Carlos G\u00f3mez-Rodr\u0131\u0301guez", "Giorgio Satta"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Kuhlmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuhlmann et al\\.", "year": 2011}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Characterizing the errors of data-driven dependency parsing models", "author": ["Ryan T McDonald", "Joakim Nivre."], "venue": "EMNLP-CoNLL. pages 122\u2013131. http://www.aclweb.org/anthology/D07-1013.", "citeRegEx": "McDonald and Nivre.,? 2007", "shortCiteRegEx": "McDonald and Nivre.", "year": 2007}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the 8th International Workshop on Parsing Technologies. pages 149\u2013160. http://stp.lingfil.uu.se/ nivre/docs/iwpt03.pdf.", "citeRegEx": "Nivre.,? 2003", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. pages 50\u201357. https://www.aclweb.org/anthology/W04-0308.", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A transition-based dependency parser using a dynamic parsing strategy", "author": ["Francesco Sartorio", "Giorgio Satta", "Joakim Nivre."], "venue": "Association for Computational Linguistics. pages 135\u2013144. http://www.aclweb.org/anthology/P13-1014.", "citeRegEx": "Sartorio et al\\.,? 2013", "shortCiteRegEx": "Sartorio et al\\.", "year": 2013}, {"title": "Open language learning for information extraction", "author": ["Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni"], "venue": null, "citeRegEx": "Schmitz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schmitz et al\\.", "year": 2012}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015).", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al.", "startOffset": 154, "endOffset": 197}, {"referenceID": 1, "context": "Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al.", "startOffset": 154, "endOffset": 197}, {"referenceID": 10, "context": ", 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al.", "startOffset": 27, "endOffset": 54}, {"referenceID": 4, "context": ", 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016).", "startOffset": 84, "endOffset": 105}, {"referenceID": 18, "context": "A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003).", "startOffset": 131, "endOffset": 144}, {"referenceID": 17, "context": "Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007).", "startOffset": 166, "endOffset": 192}, {"referenceID": 5, "context": "This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 0, "context": "This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).", "startOffset": 109, "endOffset": 153}, {"referenceID": 16, "context": "Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).", "startOffset": 145, "endOffset": 168}, {"referenceID": 19, "context": "Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing.", "startOffset": 13, "endOffset": 26}, {"referenceID": 18, "context": "Arc-eager (Nivre, 2003) remedies this drawback by defining arc-inducing transitions that operate between the stack and the buffer.", "startOffset": 10, "endOffset": 23}, {"referenceID": 14, "context": "2 Kuhlmann et al. (2011) propose a hybrid system derived from a tabular parsing scheme, which they have shown both arc-standard and arc-eager can be derived from.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).", "startOffset": 172, "endOffset": 216}, {"referenceID": 0, "context": "Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).", "startOffset": 172, "endOffset": 216}, {"referenceID": 20, "context": "3 (Nivre et al., 2016) (EN-UD).", "startOffset": 2, "endOffset": 22}, {"referenceID": 15, "context": "0 (Manning et al., 2014) following previous work.", "startOffset": 2, "endOffset": 24}, {"referenceID": 13, "context": "Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification.", "startOffset": 37, "endOffset": 69}, {"referenceID": 8, "context": "5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).", "startOffset": 54, "endOffset": 80}, {"referenceID": 2, "context": "Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift. A notable difference is that their transitions retain tokens between the head and dependent. Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mezRodr\u0131\u0301guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed.", "startOffset": 0, "endOffset": 289}, {"referenceID": 2, "context": "Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift. A notable difference is that their transitions retain tokens between the head and dependent. Fern\u00e1ndez-Gonz\u00e1lez and G\u00f3mezRodr\u0131\u0301guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed. However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively. More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right \u201cspines\u201d of the top two nodes in the stack.", "startOffset": 0, "endOffset": 664}, {"referenceID": 3, "context": "For a comprehensive study of transition systems for dependency parsing, we refer the reader to (Bohnet et al., 2016), which proposed a generalized framework that could derive all of the traditional transition systems we described by configuring the size of the active token set and the maximum arc length, among other control parameters.", "startOffset": 95, "endOffset": 116}], "year": 2017, "abstractText": "Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments. Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition. This allows the parser to leverage lexical information more directly in transition decisions. Hence, arc-swift can achieve significantly better performance with a very small beam size. Our parsers reduce error by 3.7\u20137.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.", "creator": "LaTeX with hyperref package"}}}