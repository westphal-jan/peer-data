{"id": "1705.04630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2017", "title": "Forecasting using incomplete models", "abstract": "We consider the task of forecasting an infinite sequence of future observation based on some number of past observations, where the probability measure generating the observations is \"suspected\" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation. We also propose that an estimate of the probabilities of predicting infinite events (i.e., for example, in the universe) is less than 1, but the probability of convergence in the Kantorovich-Rubinstein metric is higher than the probability of convergence in the Kantorovich-Rubinstein metric. In this method, convergence between the Kantorovich-Rubinstein metric is about a third greater than the probability of convergence in the Kantorovich-Rubinstein metric. If we add in an assumption that we do not converge the probability of convergence in the Kantorovich-Rubinstein metric, the probability of convergence will be much lower.\n\n\nIt was argued in Part IV of the Discussion Group that the number of prior observations must be arbitrarily divided by the following points.\nThis section discusses the probability distribution in terms of probabilities.\nAs mentioned earlier, the probability distribution of predictions is a very powerful and widely used tool in the field of estimation of probability distributions. In the present discussion, we discussed the probability distribution in terms of probabilities. This approach, introduced in Part IV of the Discussion Group, is based on the notion of posterior probabilities, that if the probability distribution of predictions for some (e.g., for example, in the universe) is less than 1, then it will be greater than 1, if a posterior probability is greater than 1, if a posterior probability is greater than 1, then it will be greater than 1, if a posterior probability is greater than 1, and therefore the likelihood of convergence in the Kantorovich-Rubinstein metric is higher than 1,", "histories": [["v1", "Fri, 12 May 2017 15:38:57 GMT  (19kb)", "https://arxiv.org/abs/1705.04630v1", null], ["v2", "Fri, 28 Jul 2017 12:33:39 GMT  (23kb)", "http://arxiv.org/abs/1705.04630v2", "28 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vadim kosoy"], "accepted": false, "id": "1705.04630"}, "pdf": {"name": "1705.04630.pdf", "metadata": {"source": "CRF", "title": "Forecasting using incomplete models", "authors": ["Vadim Kosoy"], "emails": ["vadim.kosoy@intelligence.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n04 63\n0v 2\n[ cs\n.L G\n] 2\n8 Ju\nl 2 01\n1 Introduction\nForecasting future observations based on past observations is one of the fundamental problems in machine learning, and more broadly is one of the fundamental components of rational reasoning in general. This problem received much attention, using different methods (see e.g. [1] or Chapter 21 in [2]). Most of those methods assume fixing a class H of models or \u201chypotheses\u201d, each of which defines a probability measure on sequences of observations (and also conditional probability measures), and produce a forecast FH which satisfies at least one of two kinds of guarantees:\n\u2022 In the realizable setting, the guarantee is that if the observations are sampled from some \u00b5 \u2208 H, then FH converges to an \u201cideal\u201d forecast in some sense.\n\u2022 In the unrealizable setting, the guarantee is that for any sequence of observations, FH is asymptotically as good as the forecast produced by any \u00b5 \u2208 H.\nThe realizable setting is often unrealistic, in particular because it requires that the environment under observation is simpler than the observer itself. Indeed, even though we avoid analyzing computational complexity in this work, it should be noted that the computational (e.g. time) complexity of a forecaster is always greater than the complexities of all \u00b5 \u2208 H. On the other hand, the unrealizable setting usually only provides guarantees for short-term forecasts (since otherwise the training data is insufficient). The latter is in contrast to e.g. Bayesian inference where the time to learn the model depends on its prior probability, but once \u201clearned\u201d (i.e. once FH converged to a given total variation distance from the true probability measure), arbitrarily long-term forecasts become reliable.\nThe spirit of our approach is that the environment might be very complex, but at the same time it might posses some simple features, and it is these features that the forecast must capture. For example, if we consider a sequence of observations {on \u2208 {0, 1}}n\u2208N s.t. o2k+1 = o2k, then whatever is the behavior of the even observations o2k, the property o2k+1 = o2k should asymptotically be assigned high probability by the forecast (this idea was discussed in [3] as \u201copen problem 4j\u201d).\nFormally, we introduce the notion of an incomplete model, which is a convex set M in the space P(O\u03c9) of probability measures on the space of sequences O\u03c9. Such an incomplete model may be regarded as a hybrid of probabilistic and Knightian uncertainty. We then consider a countable1 set H of incomplete models. For any M \u2208 H and \u00b5 \u2208 M , our forecasts will converge to M in an appropriate sense with \u00b5-probability 1. This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see [4]), and is our main result. Our setting can be considered to be in between realizable and unrealizable: it is \u201cpartially realizable\u201d since we require the environment to conform to some M \u2208 H, it is \u201cpartially unrealizable\u201d since \u00b5 \u2208 M can be chosen adversarially (we can even allow non-oblivious choice, i.e. dependence on the forecast itself).\nThe forecasting method we demonstrate is based on the principles introduced in [5]. The forecast may be regarded as the pricing of a certain combinatorial prediction market, with a countable set of gamblers making bets. The market pricing is then defined by the requirement that the aggregate of all gamblers doesn\u2019t make a net profit. The existence of such a pricing follows from the Kakutani-Glicksberg-Fan fixed point theorem and the Kuratowski-Rill-Nardzewski measurable selection theorem (the latter in order to show that the dependence on observation history can be made measurable). The fact that the aggregate of all gamblers cannot make a net profit implies that each individual gambler can only make a bounded profit.\nThe above method is fairly general, but for our purposes, we associate with each incomplete model M \u2208 H a set of gamblers that make bets which are guaranteed to be profitable assuming the true environment \u00b5 satisfies the incomplete model (i.e. \u00b5 \u2208 M). The existence of these gamblers follows from the Hahn-Banach separation theorem (where the incomplete model defines the convex set in question) and selection theorems used to ensure that the choice of separating functional depends in a sufficiently \u201cregular\u201d way on the market pricing. In order to show that the forecaster we described satisfies the desired convergence property, we use tools from martingale theory.\nThe structure of the paper is as follows. Section 2 defines the setting and states the main theorem. Section 3 lays out the formalism of \u201ccombinatorial prediction markets,\u201d and the central concept of a dominant forecaster. Section 4 introduces the concept of a prudent gambling strategy, which is a technique for proving convergence theorems about dominant forecasters. Section 5 describes the gamblers associated with incomplete models and completes the proof of the main theorem.\n2 Learning Incomplete Models\nN will denote the set of natural numbers {0, 1, 2 . . .}. Let {On}n\u2208N be a sequence of compact Polish spaces. On represents the space of possible observations at time n. Denote On := \u220f\nm<nOn and O \u03c9 :=\n\u220f\nn\u2208NOn. \u03c0 O n : O \u03c9 \u2192 On is the\nprojection mapping and x:n := \u03c0 O n (x). Given y \u2208 O\nn, yO\u03c9 := (\n\u03c0On\n)\u22121 (y) is a closed subspace of\nO\u03c9. On will be regarded as a topological space using the product topology and as a measurable\n1This work only deals with the nonparametric setting in which H is discrete.\nspace with the \u03c3-algebra of universally measurable sets2. For any measurable space X, P (X) will denote the space of probability measures on X. When X is a Polish space with the Borel \u03c3-algebra, P (X) will be regarded as a topological space using the weak topology and as a measurable space using the \u03c3-algebra of Borel sets. Given \u00b5 \u2208 P (X), we denote supp\u00b5 \u2286 X the support of \u00b5.\nDefinition 1. A forecaster F is a family of measurable mappings\n{Fn : O n \u2192 P(O\u03c9)}n\u2208N\ns.t. suppFn (y) \u2286 yO \u03c9.\nGiven a forecaster F and y \u2208 On, Fn (y) represents the forecast corresponding to observation history y. The condition suppFn (y) \u2286 yO\n\u03c9 reflects the obvious requirement of consistency with past observations.\nIn order to formulate a claim about forecast convergence, we will need a metric on P(O\u03c9). Consider \u03c1 : O\u03c9\u00d7O\u03c9 \u2192 R a metrization of O\u03c9. Let Lip (O\u03c9, \u03c1) be the Banach space of \u03c1-Lipschitz functions on O\u03c9, equipped with the norm\n\u2016f\u2016\u03c1 := max x |f (x)|+ sup x 6=y\n|f (x)\u2212 f (y)|\n\u03c1 (x, y) (1)\nP(O\u03c9) can be regarded as a compact subset of the dual space Lip (O\u03c9, \u03c1)\u2032, yielding the following metrization of P(O\u03c9):\nd\u03c1KR (\u00b5, \u03bd) := sup \u2016f\u2016\u03c1\u22641\n( E\u00b5 [f ]\u2212 E\u03bd [f ] )\n(2)\nWe call d\u03c1KR the Kantorovich-Rubinstein metric 3. Fix any x \u2208 O\u03c9. It is easy to see that\nlim n\u2192\u221e max x\u2032\u2208x:nO\u03c9\n\u03c1 ( x\u2032, x ) = 0 (3)\nDenote \u03b4x \u2208 P(O \u03c9) the unique probability measure s.t. \u03b4x\n( {x} )\n= 1. It follows that for any sequence {\u00b5n \u2208 P(O \u03c9)}n\u2208N s.t. supp\u00b5n \u2286 x:nO \u03c9\nlim n\u2192\u221e\nd\u03c1KR (\u00b5n, \u03b4x) = 0 (4)\nTherefore, formulating a non-vacuous convergence theorem requires \u201crenormalizing\u201d dKR for each n \u2208 N. To this end, we consider a sequence of metrizations of O\u03c9: {\u03c1n : O\n\u03c9 \u00d7O\u03c9 \u2192 R}n\u2208N. In the special case when On = O for all n \u2208 N and some fixed space O, there is a natural class of metrizations with the property that \u03c1n(yx, yx\n\u2032) = \u03c1(x, x\u2032) for some fixed metric \u03c1 on O\u03c9 and any y \u2208 On, x, x\u2032 \u2208 O\u03c94. We denote dnKR := d \u03c1n KR.\n2The fact we use universally measurable sets rather than Borel sets will play an important role in the proof of Lemma 4.\n3This is slightly different from the conventional definition but strongly equivalent (i.e. each metric is bounded by a constant multiple of the other). Other names used in the literature for the strongly equivalent metric are \u201c1st Wasserstein metric\u201d and \u201cearth mover\u2019s distance.\u201d\n4In this special case, the need for renormalization is a side effect of our choice of notation where the forecaster produces a probability measure over the entire sequence rather than over future observations only. Also, in this case the choice of \u03c1 determines everything, since we only use Kantorovich-Rubinstein distance between measures with support in yO\u03c9 for the same y \u2208 On.\nAnother ingredient we will need is a notion of regular conditional probability for incomplete models. Given measurable spaces X and Y , we will use the notation K : X k \u2212\u2192 Y to denote a Markov kernel with source X and target Y . Given x \u2208 X, we will use the notation K (x) \u2208 P (Y ). Given \u00b5 \u2208 P (X), \u00b5 \u22c9 K \u2208 P (X \u00d7 Y ) denotes the semidirect product of \u00b5 and K, that is, the unique measure satisfying\n(\u00b5\u22c9K)(A\u00d7B) =\n\u222b\nA\nK(x)(B)\u00b5(dx) (5)\nK\u2217\u00b5 \u2208 P (Y ) denotes the pushforward of \u00b5 by K (i.e. the pushforward of \u00b5 \u22c9 K by the projection to Y ). When X,Y are Polish and \u03c0 : X \u2192 Y is Borel measurable, \u00b5 | \u03c0 : Y k \u2212\u2192 X is defined to be s.t. \u03c0\u2217\u00b5 \u22c9 ( \u00b5 | \u03c0 ) is supported on the graph of \u03c0 and ( \u00b5 | \u03c0 )\n\u2217 \u03c0\u2217\u00b5 = \u00b5 (i.e. \u00b5 | \u03c0 is\na regular conditional probability). By the disintegration theorem, \u00b5 | \u03c0 exists and is defined up to coincidence \u03c0\u2217\u00b5-almost everywhere.\nDefinition 2. Let X,Y be compact Polish spaces, \u03c0 : X \u2192 Y continuous and M \u2286 P(X). We say that N : Y \u2192 2P(X) is a regular upper bound for M | \u03c0 when\ni. The set graphN := {(y, \u00b5) \u2208 Y \u00d7P(X) | \u00b5 \u2208 N(y)} is closed.\nii. For every y \u2208 On, N(y) is convex.\niii. For every \u00b5 \u2208 M and \u03c0\u2217\u00b5-almost every y \u2208 Y , (\u00b5 | \u03c0)(y) \u2208 N(y).\nNote that, although technically we haven\u2019t assumedM is convex, we might as well have assumed it, since it is not hard to see that, if N is a regular upper bound for M | \u03c0 and M \u2032 is the convex hull of M , then N is a regular upper bound for M \u2032 | \u03c0. The same remark applies to Theorem 1 below.\nWe give a few examples for Definition 2. The proofs that these examples are valid are in Appendix B.\nExample 1. Suppose thatM is convex and Y is a finite set (in particular, this example is applicable when the On are finite sets, X = O \u03c9, Y = On and \u03c0 = \u03c0On ). Then, there is a unique N : Y \u2192 2 P(X) which is a minimal (w.r.t. set inclusion) regular upper bound for M | \u03c0 and we have\nN(y) = { ( \u00b5 | \u03c0\u22121(y) ) | \u00b5 \u2208 M, \u00b5 ( \u03c0\u22121(y) ) > 0} (6)\nHere, \u00b5 | \u03c0\u22121(y) is the conditional probability measure.\nExample 2. Consider some I \u2286 N and suppose that for each n \u2208 I, we are given Kn : O n k\u2212\u2192 On. Define MK by\nMK := {\u00b5 \u2208 P(O\u03c9) | \u2200n \u2208 I : \u03c0On+1\u2217\u00b5 = \u03c0 O n\u2217\u00b5\u22c9Kn} (7)\nFor each n \u2208 N, define MKn : O n \u2192 2P(O \u03c9) by\nMKn (y) = {\u00b5 \u2208 P(O \u03c9) | supp\u00b5 \u2286 yO\u03c9, \u2200m \u2208 I : m \u2265 n =\u21d2 \u03c0Om+1\u2217\u00b5 = \u03c0 O m\u2217\u00b5\u22c9Km} (8)\nThen, MKn is a regular upper bound for M K | \u03c0On .\nExample 3. Suppose that Y is a compact subset of Rd for some d \u2208 N (in particular, this example is applicable when each On is a compact subset of R\ndn , X = O\u03c9, Y = On and \u03c0 = \u03c0On ; in that case d = \u2211\nm<n dn). We regard Y as a metric space using the Euclidean metric. For any y \u2208 Y and r > 0, Br (y) will denote the open ball of radius r with center at y.\nFix any M \u2286 P(X). Then, there is a unique N : Y \u2192 2P(X) which is minimal among mappings which both satisfy conditions i and ii of Definition 2 and are s.t. for any y \u2208 Y , \u03bd \u2208 P(X) and\n\u00b5 \u2208 M , if y \u2208 supp\u03c0\u2217\u00b5 and \u03bd = limr\u21920\n(\n\u00b5 | \u03c0\u22121 ( Br (y) )\n)\n, then \u03bd \u2208 N(y) (the limit is defined\nusing the the weak topology). Moreover, N is a regular upper bound for M | \u03c0.\nWe are now ready to formulate the main result. Given a metric space X with metric \u03c1 : X\u00d7X \u2192 R, x \u2208 X and A \u2286 X, we will use the notation\n\u03c1 (x,A) := inf y\u2208A \u03c1 (x, y) (9)\nTheorem 1. Fix any H \u2286 2P(O \u03c9) countable. For every M \u2208 H and n \u2208 N, let Mn be a regular upper bound for M | \u03c0On . Then, there exists a forecaster F H s.t. for any M \u2208 H, \u00b5 \u2208 M and \u00b5-almost any x \u2208 O\u03c9\nlim n\u21920\ndnKR\n( FHn (x:n) ,Mn (x:n) ) = 0 (10)\nNote that FH as above exists for any choice of {\u03c1n}n\u2208N but it depends on the choice. Informally, we can think of this choice as determining the duration of the future time period over which we need our forecast to be reliable. It also might depend on the choice of regular upper bounds (of course, if there are minimal regular upper bounds, these yield a forecaster than works for any other regular upper bounds).\nThe example from the Introduction section can now be realized as follows. TakeOn = O = {0, 1} and let MK \u2286 P(O\u03c9) be as in Example 2, where I = 2N+1 and Pro\u223cK2n+1(y) [o = y2n] = 1. Define \u03c1n by\n\u03c1n(x, x \u2032) :=\n{\nmax{2n\u2212m | xm 6= x \u2032 m} if x 6= x \u2032 0 if x = x\u2032 (11)\nIf MK \u2208 H, and x \u2208 O\u03c9 is s.t. x2n+1 = x2n, then for any k \u2208 N, the forecaster F H of Theorem 1\nsatisfies\nlim n\u2192\u221e Pr x\u2032\u223cF2n(x:2n)\n[\nx\u20322(n+k)+1 = x \u2032 2(n+k)\n]\n= 1 (12)\nlim n\u2192\u221e Pr x\u2032\u223cF2n+1(x:2n+1)\n[\nx\u20322(n+k)+1 = x \u2032 2(n+k)\n]\n= 1 (13)\nIf we only cared about predicting the next observation (as opposed to producing a probability measure over the entire sequence), this example (and any other instance of Example 2, at least in the case On = O finite) would be a special case of the \u201csleeping experts\u201d setting in online learning (see [6]). However, our formalism is much more general, even for predicting the next observation. For instance, we can consider O = {0, 1, 2, 3} and have an incomplete model specifying that the next observation is an odd number (thus, the \u201cexpert\u201d specializes by predicting specific properties of the observation rather than only making predictions at specific times). As another example, we can consider O = {0, 1, 2} and an incomplete model specifying that the probability distribution\n(p0, p1, p2) of each observation satisfies p0, p1, p2 \u2265 0.1. This model would capture any environment that can be regarded as a process observed through a noisy sensor that has a probability of 0.3 to output a uniformly random observation instead of the real state of the process.\nThe rest of the paper is devoted to proving Theorem 1.\n3 Dominant Forecasters\nIn this section we explain our generalization of the methods introduced in [5] under the name \u201clogical inductors.\u201d The main differences between our formalism and that of [5] are\n\u2022 We are interested in sequence forecasting rather than formal logic.\n\u2022 We consider probability measures on certain Polish spaces, rather than probability assignment functions on finite sets.\n\u2022 In particular, no special treatment of expected values is required.\n\u2022 The observations are stochastic rather than deterministic.\nOur terminology also differs from [5]: their \u201cmarket\u201d is our \u201cforecaster\u201d, their \u201ctrader\u201d is our \u201cgambler\u201d. In any case, our exposition assumes no prior knowledge about logical inductors.\nThe idea is to consider a collection of gamblers making bets against the forecaster. If a gambler with finite budget cannot make an infinite profit, the gambler is said to be \u201cdominated\u201d by the forecaster. We then prove that for any countable collection of gamblers, there is a forecaster that dominates all of them.\nGiven a compact Polish space X, C (X) will denote the Banach space of continuous functions with uniform norm. We will also use the shorthand notation B (X) := C ( P (X)\u00d7X )\n. Equivalently, B (X) can be regarded to be the space of continuous functions from P (X) to C (X), and we will curry implicitly in our notation.\nWe regard B(O\u03c9) as the space of bets that can be made against a forecaster. Given \u03b2 \u2208 B(O\u03c9), a forecast \u00b5 \u2208 P(O\u03c9) and observation sequence x \u2208 O\u03c9, the value of the bet \u03b2 is\nV \u03b2 (\u00b5, x) := \u03b2 (\u00b5, x)\u2212 Ex\u2032\u223c\u00b5\n[\n\u03b2 ( \u00b5, x\u2032 )\n]\n(14)\nNote that this defines a bounded linear operator V : B (X) \u2192 B (X). C (X) and B (X) will also be regarded as a measurable spaces, using the \u03c3-algebra of Borel\nsets. We remind that the \u03c3-algebra on On is the algebra of universally measurable sets.\nDefinition 3. A gambler is a family of measurable mappings\n{Gn : O n \u00d7 P(O\u03c9)n \u2192 B (O\u03c9)}n\u2208N\nA gambler is considered to observe a forecaster and bet against it. Given y \u2208 On and \u00b5 \u2208 P(O\u03c9)n, Gn (y,\u00b5) = \u03b2 means that, if y are the first n observations and \u00b5 are the first n forecasts (made after observing n \u2212 1 out of the n observations), the gambler will make bet \u03b2 (which is in itself a function of the forecast the forecaster makes after the n-th observation).\nWe now introduce some notation regarding the interaction of gamblers and forecasters. Given a gambler G and a forecaster F , we define the measurable mappings GFn : O\nn \u2192 B(O\u03c9) by\nGFn (y) := Gn ( y, F0, F1 (y:1) . . . Fn\u22121 (y:n\u22121) )\n(15)\nHere, y:m denotes the projection of y to O m. We define the measurable mappings VG F n : O n \u2192 C(O\u03c9) and \u03a3VGFn : O n\u22121 \u2192 C(O\u03c9) by\nVG F n (y) :=\n( VGFn (y) ) ( Fn (y) )\n(16)\n\u03a3VGFn (y) := \u2211\nm<n\nVG F m (y:m) (17)\nIn the definition of \u03a3VGF0 , O \u22121 is considered to be equal to O0 (i.e. the one point space). We define the measurable mappings \u03a3VminG F n ,\u03a3Vmax G F n : O n\u22121 \u2192 R by\n\u03a3VminG F n (y) := min yO\u03c9 \u03a3VGFn (y) (18)\n\u03a3Vmax G F n (y) := max yO\u03c9 \u03a3VGFn (y) (19)\nWe are now ready to state the formally state the definition of \u201cdominance\u201d alluded to in the beginning of the section.\nDefinition 4. Consider a forecaster F and a gambler G. F is said to dominate G when for any x \u2208 O\u03c9, if condition 20 holds then condition 21 holds:\ninf n\u2208N\n\u03a3VminG F n (x:n\u22121) > \u2212\u221e (20)\nsup n\u2208N\n\u03a3VmaxG F n (x:n\u22121) < +\u221e (21)\nThe following is the main theorem of this section, which is our analogue of Theorem 3.6.1 from [5].\nTheorem 2. Let {Gk}k\u2208N be a family of gamblers. Then, there exists a forecaster F G s.t. for any k \u2208 N, F dominates Gk.\nThe rest of the section is devoted to proving Theorem 2. The proof will consist of two steps. First we show that for any single gambler, there is a dominant forecaster. Then, given a countable set G of gamblers, we construct a single gambler s.t. dominating this gambler implies dominating all gamblers in the set.\nThe following lemma shows that for any bet, there is a forecast which makes the bet unwinnable.\nLemma 1. Consider X a compact Polish space and \u03b2 \u2208 B (X). Then, there exists \u00b5 \u2208 P (X) s.t.\nsupp\u00b5 \u2286 argmax\u03b2 (\u00b5) (22)\nProof. Define K : P (X) \u2192 2P(X) as follows:\nK (\u00b5) := {\u03bd \u2208 P (X) | supp \u03bd \u2286 argmax\u03b2 (\u00b5)}\nFor any \u00b5, K (\u00b5) is obviously convex and non-empty. It is also easy to see that the graph of K in P (X) \u00d7 P (X) is closed. Applying the Kakutani-Glicksberg-Fan theorem, we conclude that K has a fixed point, i.e. \u00b5 \u2208 P (X) s.t. \u00b5 \u2208 K (\u00b5).\nNext, we show that the probability measure of Lemma 1 can be made to depend measurably on past observations and the bet.\nLemma 2. Fix Y1,2 compact Polish spaces and denote X := Y1 \u00d7 Y2. Then, there exists a Borel measurable mapping \u03b1 : Y1 \u00d7 B (X) \u2192 P (X) s.t. for any y \u2208 Y1 and \u03b2 \u2208 B (X)\nsupp\u03b1 (y, \u03b2) \u2286 argmax y\u00d7Y2\n\u03b2 ( \u03b1 (y, \u03b2) )\n(23)\nProof. Define Z1, Z2, Z \u2286 Y1 \u00d7 B (X)\u00d7 P (X) by\nZ1 := {(y, \u03b2, \u00b5) \u2208 Y1 \u00d7 B (X)\u00d7 P (X) | supp\u00b5 \u2286 y \u00d7 Y2}\nZ2 := {(y, \u03b2, \u00b5) \u2208 Y1 \u00d7 B (X)\u00d7 P (X) | E\u00b5 [ \u03b2 (\u00b5) ]\n= max y2\u2208Y2 \u03b2 (\u00b5, y, y2)}\nZ := Z1 \u2229 Z2 = {(y, \u03b2, \u00b5) \u2208 Y1 \u00d7 B (X)\u00d7 P (X) | supp\u00b5 \u2286 argmax y\u00d7Y2 \u03b2 (\u00b5)}\nWe can view Z as the graph of a multivalued mapping from Y1 \u00d7 B (X) to P (X) (i.e a mapping from Y1 \u00d7 B (X) to 2\nP(X)). We will now show that this multivalued mapping has a selection, i.e. a single-valued Borel measurable mapping whose graph is a subset of Z. Obviously, the selection is the desired \u03b1.\nFix \u03c1 a metrization of Y1. Z1 is the vanishing locus of the continuous function E(y1,y2)\u223c\u00b5 [ \u03c1 (y1, y) ]\n. Z2 is the vanishing locus of the continuous function E\u00b5 [ \u03b2 (\u00b5) ]\n\u2212 maxy2\u2208Y2 \u03b2 (\u00b5, y, y2). Therefore Z1,2 are closed and so is Z. In particular, the fiber Zy\u03b2 of Z over any (y, \u03b2) \u2208 Y1 \u00d7 B (X) is also closed.\nFor any y \u2208 Y1, \u03b2 \u2208 B (X), define iy : Y2 \u2192 X by iy (y2) := (y, y2) and \u03b2y \u2208 B (Y2) by \u03b2y ( \u03bd, y\u2032 )\n:= \u03b2 ( iy\u2217\u03bd, y, y \u2032 ) . Applying Lemma 1 to \u03b2y we get \u03bd \u2208 P (Y2) s.t.\nsupp \u03bd \u2286 argmax\u03b2y (\u03bd)\nIt follows that ( y, \u03b2, iy\u2217\u03bd )\n\u2208 Z and hence Zy\u03b2 is non-empty. Consider any U \u2286 P (X) open. Then, AU := ( Y1 \u00d7 B (X)\u00d7 U )\n\u2229 Z is locally closed, and in particular, it is an F\u03c3 set. Therefore, the image of AU under the projection to Y1 \u00d7 B (X) is also F\u03c3 and in particular Borel.\nApplying the Kuratowski-Rill-Nardzewski measurable selection theorem, we get the desired result.\nWe define the measurable mappings G\u0304Fn : O n \u2192 C(O\u03c9) by\nG\u0304Fn (y) := G F n\n( y;Fn (y) )\n(24)\nCorollary 1. Let G be a gambler. Then, there exists a forecaster F s.t. for all n \u2208 N and y \u2208 On\nsuppFn (y) \u2286 argmax yO\u03c9\nG\u0304Fn (25)\nProof. For every n \u2208 N, let \u03b1n : On \u00d7 B(O \u03c9) \u2192 P(O\u03c9) be as in Lemma 2. Observing that the definition of GFn depends only on Fm for m < n, we define F recursively as\nFn (y) := \u03b1n\n( y,GFn (y) )\nIn particular, the forecaster F of Corollary 1 dominates G since, as easy to see, VG F n \u2264 0 and hence \u03a3VGFn \u2264 0. We will now introduce a way to transform a gambler in a way that enforces a \u201cfinite spending budget.\u201d Consider a gambler G and fix b > 0 (the size of the \u201cbudget\u201d). Define the measurable functions \u03a3VGn : O n\u22121 \u00d7 P(O\u03c9)n \u2192 C(O\u03c9) by\n\u03a3VGn (y,\u00b5) := \u2211\nm<n\n( VGm (y:m,\u00b5:m) ) (\u00b5m) (26)\nHere, \u00b5:m denotes the projection of \u00b5 \u2208 P(O \u03c9)n to P(O\u03c9)m that takes components l < m and \u00b5m denotes the m-th component of \u00b5. Define the measurable functions \u03a3VminGn,\u03a3Vmax Gn : On\u22121 \u00d7 P(O\u03c9)n \u2192 R by\n\u03a3VminGn (y,\u00b5) := min yO\u03c9 \u03a3VGn (y,\u00b5) (27)\n\u03a3Vmax Gn (y,\u00b5) := max yO\u03c9 \u03a3VGn (y,\u00b5) (28)\nDefine the measurable sets\nAbGn := {(y,\u00b5) \u2208 O n\u22121 \u00d7 P(O\u03c9)n | \u03a3VminGn (y,\u00b5) > \u2212b} (29)\nNote that Ab Gn\u00d7On is a measurable subset of O n\u00d7P(O\u03c9)n and in particular can be regarded\nas a measurable space in itself. Define the measurable functions Nb Gn : Ab Gn \u00d7On \u2192 C ( P(O\u03c9) ) by\nNb Gn (y,\u00b5; \u03bd) := max\n(\n1,max yO\u03c9\n\u2212 ( VGn (y,\u00b5) ) (\u03bd)\n\u03a3VGn (y:n\u22121,\u00b5) + b\n)\u22121\n(30)\nFinally, define the gambler \u039bbG as follows\n\u039bbGn (y,\u00b5) :=\n{\n0 if \u2203m < n : (y:m,\u00b5:m+1) 6\u2208 Ab Gm+1 NbGn (y,\u00b5) \u00b7Gn (y,\u00b5) otherwise (31)\nThe next proposition shows that as long as G stays \u201cwithin budget b,\u201d the operator \u039bb has no effect.\nProposition 1. Consider any gambler G, b > 0, n \u2208 N, y \u2208 On\u22121 and \u00b5 \u2208 P(O\u03c9)n. Assume that for all m < n, (y:m,\u00b5:m+1) \u2208 Ab Gm+1. Then, for all m < n\n\u039bbGm (y:m,\u00b5:m;\u00b5m) = Gm (y:m,\u00b5:m;\u00b5m) (32)\nProof. Consider any m < n. We have\n\u03a3VminGm+1 (y:m,\u00b5:m+1) + b > 0\n\u2200x \u2208 y:mO \u03c9 : \u03a3VGm+1 (y:m,\u00b5:m+1;x) + b > 0\n\u2200x \u2208 y:mO \u03c9 : \u03a3VGm (y:m\u22121,\u00b5:m;x) +\n( VGm (y:m,\u00b5:m) ) (\u00b5m, x) + b > 0\n\u2200x \u2208 y:mO \u03c9 : \u03a3VGm (y:m\u22121,\u00b5:m;x) + b > \u2212\n( VGm (y:m,\u00b5:m) ) (\u00b5m, x)\nUsing the assumption again, the left hand side is positive. It follows that\n\u2200x \u2208 y:mO \u03c9 : 1 >\n\u2212 ( VGm (y:m,\u00b5:m) ) (\u00b5m, x)\n\u03a3VGm (y:m\u22121,\u00b5:m;x) + b\nNbGm (y:m,\u00b5:m;\u00b5m) = 1\nSince we are in the second case in the definition of \u039bbGm (y:m,\u00b5:m), we get the desired result.\nNow, we show that \u039bbG indeed never \u201cgoes over budget.\u201d\nProposition 2. Consider any gambler G and b > 0. Then, for any n \u2208 N, y \u2208 On\u22121 and \u00b5 \u2208 P(O\u03c9)n\n\u03a3Vmin\u039bbGn (y,\u00b5) \u2265 \u2212b (33)\nProof. Let m0 \u2208 N be the largest number s.t. m0 \u2264 n and\n\u2200m \u2264 m0 : \u03a3VminGm (y:m\u22121,\u00b5:m) > \u2212b\nFor any m \u2264 m0, we have, by Proposition 1\n\u03a3Vmin\u039bbGm (y:m\u22121,\u00b5:m) = \u03a3VminGm (y:m\u22121,\u00b5:m) > \u2212b\n(Note that the sum in the definition of \u03a3Vmin\u039bbGm only involves \u039bbGl for l < m \u2264 m0) For m = m0 + 1 and any x \u2208 y:m0O \u03c9, we have\n\u03a3V\u039bbGm0+1 ( y:m0 ,\u00b5:m0+1;x ) = \u03a3V\u039bbGm0 ( y:m0\u22121,\u00b5:m0 ;x )\n+ (\nV\u039bbGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x )\nThe first term only involves \u039bbGl for l < m0, allowing us to apply Proposition 1. The second term is in the second case of the definition of \u039bb.\n\u03a3V\u039bbGm0+1 ( y:m0 ,\u00b5:m0+1;x ) =\u03a3VGm0 ( y:m0\u22121,\u00b5:m0 ;x ) +\nNbGm0 ( y:m0 ,\u00b5:m0 ;\u00b5m0 )\n\u00b7 (\nVGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x )\nIf x is s.t. (\nVGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x ) \u2265 0, then\n\u03a3V\u039bbGm0+1 ( y:m0 ,\u00b5:m0+1;x ) \u2265 \u03a3VGm0 ( y:m0\u22121,\u00b5:m0 ;x ) \u2265 \u03a3VminGm0 ( y:m0\u22121,\u00b5:m0 ) > \u2212b\nIf x is s.t. (\nVGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x ) < 0, then, by the definition of Nb\n\u03a3V\u039bbGm0+1 ( y:m0 ,\u00b5:m0+1;x ) \u2265\u03a3VGm0 ( y:m0\u22121,\u00b5:m0 ;x ) +\n\u03a3VGm0 ( y:m0\u22121,\u00b5:m0 ;x ) + b\n\u2212 (\nVGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x )\n\u00b7 (\nVGm0 ( y:m0 ,\u00b5:m0 )\n)\n( \u00b5m0 , x )\n\u03a3V\u039bbGm0+1 ( y:m0 ,\u00b5:m0+1;x ) \u2265 \u2212b\nFinally, consider m > m0 + 1.\n\u03a3V\u039bbGm (y:m\u22121,\u00b5:m;x) = \u03a3V\u039bbGm\u22121 (y:m\u22122,\u00b5:m\u22121;x) + ( V\u039bbGm\u22121 (y:m\u22121,\u00b5:m\u22121) ) (\u00b5m\u22121, x)\nThe second term is in the first case of the definition of \u039bb and therefore vanishes.\n\u03a3V\u039bbGm (y:m\u22121,\u00b5:m;x) = \u03a3V\u039bbGm\u22121 (y:m\u22122,\u00b5:m\u22121;x)\nBy induction on m, we conclude:\n\u03a3Vmin\u039bbGm (y:m\u22121,\u00b5:m) \u2265 \u03a3Vmin\u039bbGm\u22121 (y:m\u22122,\u00b5:m\u22121) \u2265 \u2212b\nNext, we show that for any gambler there is \u201climited budget\u201d gambler s.t. any forecaster that dominates it also dominates the original gambler. For any x \u2208 R, we denote x+ := max(x, 0).\nProposition 3. Let G be a gambler and \u03b6 : N \u2192 (0, 1] be s.t.\nb\u03b6 :=\n\u221e \u2211\nb=0\n\u03b6 (b) b < \u221e (34)\nDefine the gambler \u039b\u03b6G by\n\u039b\u03b6G :=\n\u221e \u2211\nb=1\n\u03b6 (b)\u039bbG (35)\nThen\n\u03a3Vmin\u039b\u03b6Gn \u2265 \u2212b\u03b6 (36)\nMoreover, if F is a forecaster that dominates \u039b\u03b6G then it also dominates G.\nProof. Equation 36 follows from Proposition 2. Now, consider any F that dominates \u039b\u03b6G. For any x \u2208 O\u03c9, 36 and Definition 4 imply\nsup n\u2208N\n\u03a3Vmax \u039b\u03b6G F n (x:n\u22121) < +\u221e\nConsider x \u2208 O\u03c9 s.t. condition 20 holds. Denote\nb0 :=\n(\n\u2212 inf n\u2208N\n\u03a3VminG F n (x:n\u22121)\n)\n+\nBy Proposition 1, for any b > b0 we have \u039bbG F = GF . We get\nsup n\u2208N max x\u2032\u2208x:n\u22121O\u03c9\n\n\n\u2211\nb\u2264b0\n\u03b6 (b)\u03a3V\u039bbG F n\n( x:n\u22121, x \u2032 ) + \u2211\nb>b0\n\u03b6 (b)\u03a3VGFn ( x:n\u22121, x \u2032 )\n\n < +\u221e\nApplying Proposition 2 to the first term\nsup n\u2208N max x\u2032\u2208x:n\u22121O\u03c9\n\n\u2212 \u2211\nb\u2264b0\n\u03b6 (b) b+ \u2211\nb>b0\n\u03b6 (b) \u03a3VGFn ( x:n\u22121, x \u2032 )\n\n < +\u221e\nsup n\u2208N\n\u03a3VmaxG F n (x:n\u22121) < +\u221e\nThe last ingredient we need to prove Theorem 2 is the step from one gambler to a countable set of gamblers.\nProposition 4. Let {Gk}k\u2208N be a family of gamblers, \u03be : N \u2192 (0, 1] and b\u03be > 0 s.t.\n\u221e \u2211\nk=0\n\u03be (k)min (\n\u03a3VminG k n, 0\n)\n\u2265 \u2212b\u03be (37)\nDefine the gambler G\u03be by\nG\u03ben := n \u2211\nk=0\n\u03be (k)Gkn (38)\nConsider a forecaster F that dominates G\u03be. Then, F dominates Gk for all k \u2208 N.\nProof. Fix k \u2208 N and x \u2208 O\u03c9. Equation 37 implies\ninf n\u2208N\n\u03a3VminG \u03beF n\n( x:n\u22121, x \u2032 ) > \u2212\u221e\nTherefore we have\nsup n\u2208N\n\u03a3VmaxG \u03beF n (x:n\u22121) < +\u221e\nsup n\u2265k max x\u2032\u2208x:n\u22121O\u03c9\n\n    \u03be (k) \u03a3VGkFn ( x:n\u22121, x \u2032 ) + \u2211\nj 6=k j\u2264n\n\u03be (j) \u03a3VGjFn ( x:n\u22121, x \u2032 )\n\n    < +\u221e\nsup n\u2265k max x\u2032\u2208x:n\u22121O\u03c9\n(\n\u03be (k) \u03a3VGkFn ( x:n\u22121, x \u2032 ) \u2212 b\u03be\n)\n< +\u221e\nsup n\u2265k\n\u03a3VmaxG kF n (x:n\u22121) < +\u221e\nSince there are only finitely many values of n less than k, it follows that\nsup n\u2208N\n\u03a3Vmax G kF n (x:n\u22121) < +\u221e\nProof of Theorem 2. Define \u03b6 (b) := max (b, 1)\u22123. By Proposition 3, we have\n\u03a3Vmin\u039b\u03b6G k n \u2265 \u2212\n\u03c02\n6\nDefine \u03be (k) := (k + 1)\u22122. We have\n\u221e \u2211\nk=0\n\u03be (k)min (\n\u03a3Vmin\u039b\u03b6G k n, 0\n) \u2265 \u2212 \u03c04\n36\nBy Corollary 1, there is a forecaster FG that dominates \u039b\u03b6G \u03be. By Proposition 4, FG dominates\n\u039b\u03b6G k for all k \u2208 N. By Proposition 3, FG dominates Gk for all k \u2208 N.\n4 Prudent Gambling Strategies\nIn this section we construct a tool for proving asymptotic convergence results about dominant forecasters. We start by introducing a notion of \u201cprudent gambling,\u201d which requires that bets are only made when the expected payoff is a certain fraction of the risk. First, we need some notation.\nGiven X,Y Polish spaces, \u00b5 \u2208 P(X), \u03c0 : X \u2192 Y Borel measurable and g : X \u2192 R Borel measurable, we will use the notation\nE\u00b5\n[ g | \u03c0\u22121 (y) ]\n:= E(\u00b5|\u03c0)(y) [g] (39)\nWe remind that \u03c0On is the projection mapping from O \u03c9 to On and yO\u03c9 :=\n(\n\u03c0On\n)\u22121 (y).\nDefinition 5. A gambling strategy is a uniformly bounded family of measurable mappings\n{Sn : O n \u2192 B(O\u03c9)}n\u2208N\nDefinition 6. Given a gambling strategy S and \u00b5\u2217 \u2208 P(O\u03c9), S is said to be \u00b5\u2217-prudent when there is \u03b1 > 0 s.t. for any n \u2208 N, \u03c0On\u2217\u00b5 \u2217-almost any y \u2208 On and any \u00b5 \u2208 P(O\u03c9)\nE\u00b5\u2217 [ Sn (y;\u00b5) | yO \u03c9 ] \u2212 E\u00b5 [ Sn (y;\u00b5) ] \u2265 \u03b1\n(\nmax yO\u03c9 Sn (y;\u00b5)\u2212min yO\u03c9 Sn (y;\u00b5)\n)\n(40)\nThe \u201ccorrect\u201d way for a gambler to use a prudent gambling strategy is not employing it all the time: in order to avoid running out of budget, a gambler shouldn\u2019t place too many bets simultaneously. To address this, we define a gambler that \u201cplays\u201d (employs the given strategy) only when all previous bets are close to being settled.\nDefinition 7. Consider any gambling strategy S. We define the gambler \u0393S recursively as follows\n\u0393Sn (y,\u00b5) :=\n{\nSn (y) if \u03a3Vmax \u0393Sn (y:n\u22121,\u00b5) \u2212 \u03a3Vmin \u0393Sn (y:n\u22121,\u00b5) \u2264 1 0 otherwise (41)\nTheorem 3. Consider \u00b5\u2217 \u2208 P(O\u03c9), S a \u00b5\u2217-prudent gambling strategy and a forecaster F . Assume F dominates \u0393S. Then, for \u00b5\u2217-almost any x \u2208 O\u03c9\n\u221e \u2211\nn=0\n(\nE\u00b5\u2217 [ Sn ( x:n;Fn (x:n) ) | x:nO \u03c9 ] \u2212 EFn(x:n) [ Sn ( x:n;Fn (x:n) ) ]\n)\n< +\u221e (42)\nIn particular, the terms of the series on the left hand-side converge to 0 (they are non-negative since S is \u00b5\u2217-prudent).\nThe rest of the section is dedicated to proving Theorem 3. We start by showing, in a more abstract setting (not specific to forecasting), that for any sequences of bets that is \u201cprudent\u201d in the sense that the expected payoff is bounded below by a fixed fraction of the risk (= bet magnitude), the probability to lose an infinite amount vanishes and, moreover, it holds with probability 1 that if the total bet magnitude is unbounded then there is infinite gain.\nLemma 3. Consider \u2126 a probability space, {Fn \u2286 2 \u2126}n\u2208N a filtration of \u2126, \u03b1,M > 0, {Xn : \u2126 \u2192 R}n\u2208N and {Yn : \u2126 \u2192 [0,M ]}n\u2208N stochastic processes adapted to F (i.e. Xn and Yn are Fn-measurable). Assume the following conditions 5:\ni. E [ |X0| ] < +\u221e\nii. \u2200n \u2208 N,m \u2265 n : |Xn \u2212Xm| \u2264 \u2211m\u22121 l=n Yl +M\niii. E [ Xn+1 | Fn ] \u2265 Xn + \u03b1Yn\nThen, for almost all \u03c9 \u2208 \u2126\na. infnXn (\u03c9) > \u2212\u221e\nb. If supn Xn (\u03c9) < +\u221e then \u2211 n Yn (\u03c9) < +\u221e\nProof. Without loss of generality, we can assume M = 1 (otherwise we can renormalize X and Y by a factor of M\u22121). Define {X0n : \u2126 \u2192 R}n\u2208N by\nX0n := Xn \u2212 \u03b1 n\u22121 \u2211\nm=0\nYm\nIt is easy to see that X0 is a submartingale. We define {Nn : \u2126 \u2192 N \u2294 {\u221e}}n\u2208N recursively as follows\nN0 (\u03c9) := 0\nNk+1 (\u03c9) :=\n\n\n\ninf{n \u2208 N | \u2211n\u22121\nm=Nk(\u03c9) Ym (\u03c9) \u2265 1} if Nk (\u03c9) < \u221e\n\u221e if Nk (\u03c9) = \u221e\nFor each k \u2208 N, Nk is a stopping time w.r.t. F . As easy to see, for each k \u2208 N, {X 0 min(n,Nk) }n\u2208N is a uniformly integrable submartingale. Using the fact that Nk \u2264 Nk+1 to apply Theorem A.1 (see Appendix A), we get\nE [\nX0Nk+1 | FNk\n]\n\u2265 X0Nk\nClearly, {X0Nk}k\u2208N is adapted to {FNk}k\u2208N. Doob\u2019s second martingale convergence theo-\nrem implies that E [ |X0Nk | ] < \u221e (X0Nk is the limit of the uniformly integrable submartingale {X0min(n,Nk)}n\u2208N). We conclude that {X 0 Nk }k\u2208N is a submartingale.\n5Equalities and inequalities between random variables are implicitly understood as holding almost surely.\nIt is easy to see that |X0Nk+1 \u2212X 0 Nk | \u2264 3. Applying the Azuma-Hoeffding inequality, we conclude that for any positive integer k:\nPr [ X0Nk \u2212X0 < \u2212k 3 4 ] \u2264 exp\n(\n\u2212 k\n3 2\n2 \u00b7 32k\n)\n= exp\n(\n\u2212 k\n1 2\n18\n)\nIt follows that\n\u221e \u2211\nk=1\nPr [ X0Nk \u2212X0 < \u2212k 3 4 ] < \u221e\nPr [\n\u2203k \u2208 N\u2200l > k : X0Nl \u2212X0 \u2265 \u2212l 3 4\n]\n= 1\nPr\n\n\u2203k \u2208 N\u2200l > k : XNl \u2212X0 \u2265 \u03b1\nNl\u22121 \u2211\nn=0\nYn \u2212 l 3 4\n\n = 1\nIt is easy to see that if \u03c9 \u2208 \u2126 is s.t. \u2211\u221e n=0 Yn (\u03c9) = \u221e then \u2211Nl(\u03c9)\u22121 n=0 Yn (\u03c9) \u2265 l. We get\nPr\n\n\n\u221e \u2211\nn=0\nYn = \u221e =\u21d2 \u2203k \u2208 N\u2200l > k : XNl \u2212X0 \u2265 \u03b1l \u2212 l 3 4\n\n = 1\nTo complete the proof, observe that if \u03c9 \u2208 \u2126 is s.t. infnXn (\u03c9) = \u2212\u221e then \u2211\u221e\nn=0 Yn (\u03c9) = \u221e (since |Xn (\u03c9)\u2212X0 (\u03c9)| \u2264 \u2211n\u22121 m=0 Yn (\u03c9) + 1).\nCorollary 2. Consider \u2126 a probability space, {Fn \u2286 2 \u2126}n\u2208N a filtration of \u2126, \u03b1,M > 0, {X \u2032 n : \u2126 \u2192 R}n\u2208N and {Yn : \u2126 \u2192 [0,M ]}n\u2208N stochastic processes adapted to F and {Xn : \u2126 \u2192 R}n\u2208N an arbitrary stochastic process. Assume the following conditions:\ni. |Xn \u2212X \u2032 n| \u2264 M\nii. E [ |X0| ] < +\u221e\niii. |Xn+1 \u2212Xn| \u2264 Yn\niv. E [ Xn+1 \u2212Xn | Fn ] \u2265 \u03b1Yn\nThen, for almost all \u03c9 \u2208 \u2126\na. infnXn (\u03c9) > \u2212\u221e\nb. If supn Xn (\u03c9) < +\u221e then \u2211 n Yn (\u03c9) < +\u221e\nProof. Define X \u2032\u2032n := E [ Xn | Fn ] . We have\nE [ |X \u2032\u20320 | ]\n= E [ |E [ X0 | F0 ] | ] \u2264 E [ E [ |X0| | F0 ] ] = E [ |X0| ] < \u221e\n|X \u2032\u2032n \u2212X \u2032 n| = |E\n[ Xn | Fn ] \u2212X \u2032n| = |E [ Xn \u2212X \u2032 n | Fn ] | \u2264 M\n|X \u2032\u2032n \u2212Xn| \u2264 |X \u2032\u2032 n \u2212X \u2032 n|+ |X \u2032 n \u2212Xn| \u2264 2M\n\u2200m \u2265 n : |X \u2032\u2032m \u2212X \u2032\u2032 n| \u2264 |Xm \u2212Xn|+ 4M \u2264\nn\u22121 \u2211\nl=n\nYl + 4M\nE [ X \u2032\u2032n+1 \u2212X \u2032\u2032 n | Fn ]\n= E [ E [ Xn+1 | Fn+1 ] \u2212 E [ Xn | Fn ] | Fn ] = E [ Xn+1 \u2212Xn | Fn ] \u2265 \u03b1Yn\nApplying Lemma 3 to X \u2032\u2032 and using |X \u2032\u2032n \u2212Xn| \u2264 2M , we get the desired result.\nWe will also need the following property of the operator \u0393, reflecting that at any given moment, the uncertainty of the gambler \u0393S about the total worth of its bets is bounded.\nProposition 5. Given any gambling strategy S, we have\n\u03a3V\u0393Sn \u2212 \u03a3Vmin \u0393Sn \u2264 2max m<n sup y\u2208Om \u2016Sm (y)\u2016+ 1 (43)\nIn particular, the left hand side is uniformly bounded.\nProof. We prove by induction. For n = 0 the claim is obvious. Consider any n \u2208 N, y \u2208 On and \u00b5 \u2208 P(O\u03c9)n+1. First, assume that\n\u03a3Vmax \u0393Sn (y:n\u22121,\u00b5:n)\u2212 \u03a3Vmin \u0393Sn (y:n\u22121,\u00b5:n) > 1\nThen, by definition of \u0393S, \u0393Sn (y,\u00b5:n) \u2261 0 and therefore\n\u03a3V\u0393Sn+1 (y,\u00b5) = \u03a3V\u0393Sn (y:n\u22121,\u00b5:n)\n\u03a3Vmin \u0393Sn+1 (y,\u00b5) \u2265 \u03a3Vmin \u0393Sn (y:n\u22121,\u00b5:n)\n\u03a3V\u0393Sn+1 (y,\u00b5) \u2212\u03a3Vmin \u0393Sn+1 (y,\u00b5) \u2264 \u03a3V\u0393Sn (y:n\u22121,\u00b5:n)\u2212 \u03a3Vmin \u0393Sn (y:n\u22121,\u00b5:n)\nBy the induction hypothesis, we get\n\u03a3V\u0393Sn+1 (y,\u00b5)\u2212 \u03a3Vmin \u0393Sn+1 (y,\u00b5) \u2264 2max m<n sup y\u2208Om \u2016Sm (y)\u2016+ 1 \u2264 max m\u2264n sup y\u2208Om \u2016Sm (y)\u2016+ 1\nNow, assume that\n\u03a3Vmax \u0393Sn (y:n\u22121,\u00b5:n)\u2212 \u03a3Vmin \u0393Sn (y:n\u22121,\u00b5:n) \u2264 1\nThen, \u0393Sn (y,\u00b5:n) = Sn (y) and therefore\n\u03a3V\u0393Sn+1 (y,\u00b5) = \u03a3V\u0393Sn (y:n\u22121,\u00b5:n) + ( VSn (y) ) (\u00b5n)\nWe get\n\u03a3V\u0393Sn+1 (y,\u00b5)\u2212\u03a3Vmin \u0393Sn+1 (y,\u00b5) \u2264 \u03a3Vmax \u0393Sn (y:n\u22121,\u00b5:n)\u2212\u03a3Vmin \u0393Sn (y:n\u22121,\u00b5:n)+2\u2016Sn (y)\u2016\nBy the assumption, the first two terms total to \u2264 1, yielding the desired result.\nAs a final ingredient for the proof of Theorem 3, we will need another property of \u0393, namely that when the total magnitude of all bets made is finite, the gambler eventually starts \u201cplaying\u201d in every round.\nProposition 6. Let S be a gambling strategy, F a forecaster and x \u2208 O\u03c9. Assume that\n\u221e \u2211\nn=0\n(\nmax x:nO\u03c9\n\u0393S F n (x:n)\u2212 min\nx:nO\u03c9 \u0393S\nF n (x:n)\n)\n< +\u221e (44)\nThen, for any n \u226b 0, \u0393SFn (x:n) = Sn (x:n).\nProof. Choose n0 \u2208 N s.t.\n\u221e \u2211\nn=n0\n(\nmax x:nO\u03c9\n\u0393S F n (x:n)\u2212 min\nx:nO\u03c9 \u0393S\nF n (x:n)\n)\n< 1\n2\nSince \u0393S F n (x:n) ,V\u0393S F n (x:n) \u2208 C(O \u03c9) differ by a constant function, we have\n\u221e \u2211\nn=n0\n(\nmax x:nO\u03c9\nV\u0393S F n (x:n)\u2212 min\nx:nO\u03c9 V\u0393S\nF n (x:n)\n)\n< 1\n2\nIn particular, for any n \u2265 n0\nn\u22121 \u2211\nm=n0\n(\nmax x:nO\u03c9\nV\u0393S F m (x:m)\u2212 min\nx:nO\u03c9 V\u0393S\nF m (x:m)\n)\n< 1\n2\nOn the other hand, it is easy to see that there is n1 \u2265 n0 s.t. for any n \u2265 n1\nmax x:nO\u03c9\n\u03a3V\u0393SFn0 ( x:n0\u22121 )\n\u2212 min x:nO\u03c9\n\u03a3V\u0393SFn0 ( x:n0\u22121 )\n< 1\n2\nTaking the sum of the last two inequalities, we conclude that for any n \u2265 n1\n\u03a3Vmax \u0393S F n (x:n\u22121)\u2212 \u03a3Vmin \u0393S F n (x:n\u22121) < 1\nUsing the definition of \u0393S, we get the desired result.\nProof of Theorem 3. We regard O\u03c9 as a probability space using the \u03c3-algebra F of universally measurable sets and the probability measure \u00b5\u2217. For any n \u2208 N, we define Fn \u2286 F and Xn,X \u2032 n, Yn : O\u03c9 \u2192 R by\nFn := { ( \u03c0On )\u22121 (A) | A \u2286 On universally measurable}\nXn (x) := \u03a3V\u0393S F n (x:n\u22121, x)\nX \u2032n (x) := \u03a3Vmin \u0393S F n (x:n\u22121)\nYn (x) := max x:nO\u03c9\n\u0393S F n (x:n)\u2212 min\nx:nO\u03c9 \u0393S\nF n (x:n)\nClearly, F is a filtration of O\u03c9, X,X \u2032, Y are stochastic processes and X \u2032, Y are adapted to F . S is uniformly bounded, therefore \u0393S is uniformly bounded and so is Y . Obviously, Y is also non-negative.\nBy Proposition 5, |Xn \u2212X \u2032 n| are uniformly bounded. X0 vanishes and in particular E\n[ |X0| ]\n< \u221e. We have\n|Xn+1 (x)\u2212Xn (x)| = |V\u0393S F n (x:n, x)| \u2264 Yn (x)\nMoreover:\nE [ Xn+1 \u2212Xn | Fn ] (x) = Ex\u2032\u223c\u00b5\u2217 [ V\u0393S F n ( x:n, x \u2032 ) | x\u2032 \u2208 x:nO \u03c9 ]\n(As before, the notation on the right hand side signifies the use of a regular conditional probability)\nE [ Xn+1 \u2212Xn | Fn ] (x) = Ex\u2032\u223c\u00b5\u2217 [ \u0393S F n ( x:n, x \u2032 ) | x\u2032 \u2208 x:nO \u03c9 ] \u2212 Ex\u2032\u223cFn(x:n) [ \u0393S F n ( x:n, x \u2032 ) ]\nLet \u03b1 > 0 be as in Definition 6. By definition of \u0393S, \u0393S F n (x:n) is equal to either Sn ( x:n;Fn (x:n) )\nor 0. In either case, we get that for \u00b5\u2217-almost any x \u2208 O\u03c9\nE [ Xn+1 \u2212Xn | Fn ] (x) \u2265 \u03b1\n(\nmax x:nO\u03c9\n\u0393S F n (x:n)\u2212 min\nx:nO\u03c9 \u0393S\nF n (x:n)\n)\n= \u03b1Yn (x)\nBy Corollary 2a, for \u00b5\u2217-almost any x \u2208 O\u03c9\ninf n\u2208N\n\u03a3Vmin \u0393S F n (x:n\u22121) > \u2212\u221e\nSince F dominates \u0393S, it follows that\nsup n\u2208N\n\u03a3Vmax \u0393S F n (x:n\u22121) < +\u221e\nBy Corollary 2b, \u2211 n Yn (x) < \u221e. By Proposition 6, it follows that for any n \u226b 0, \u0393S F n (x:n) =\nSn (x:n). We get, for n \u226b 0\nE\u00b5\u2217 [ Sn ( x:n;Fn (x:n) ) | x:nO \u03c9 ] \u2212 EFn(x:n) [ Sn ( x:n;Fn (x:n) ) ] = E\u00b5\u2217 [ \u0393S F n (x:n) | x:nO \u03c9 ] \u2212 EFn(x:n) [ \u0393S F n (x:n) ] \u2264 Yn (x)\n\u221e \u2211\nn=0\n(\nE\u00b5\u2217 [ Sn ( x:n;Fn (x:n) ) | x:nO \u03c9 ] \u2212 EFn(x:n) [ Sn ( x:n;Fn (x:n) ) ]\n)\n< +\u221e\n5 Proof of Main Theorem\nIn order to prove Theorem 1, we will need, given an incomplete model M \u2286 P(O\u03c9) and \u01eb > 0, a gambling strategy SM\u01eb s.t. SM\u01eb is \u00b5-prudent for any \u00b5 \u2208 M and s.t. Theorem 3 implies that any F that dominates SM\u01eb satisfies equation 10 \u201cwithin \u01eb.\u201d This motivates the following definition and lemma. We remind that, given x \u2208 R, x+ := max(x, 0).\nDefinition 8. Consider X a compact Polish metric space, M \u2286 P (X) convex and r0 > 0. \u03b2 \u2208 B (X) is said to be (M, r0)-savvy when for any \u00b5 \u2208 P (X), denoting r\u00b5 := dKR (\u00b5,M):\ni. \u2016\u03b2 (\u00b5)\u2016 \u2264 ( r\u00b5 \u2212 r0 )\n+\nii. \u2200\u03bd \u2208 M : E\u03bd [ \u03b2 (\u00b5) ] \u2212 E\u00b5 [ \u03b2 (\u00b5) ] \u2265 12 ( r\u00b5 \u2212 r0 ) r\u00b5\nLemma 4. Consider X a compact Polish metric space, Y a compact Polish space and M : Y \u2192 2P(X). Denote\ngraphM := {(y, \u00b5) \u2208 Y \u00d7 P (X) | \u00b5 \u2208 M (y)}\nAssume that graphM is closed and that M (y) is convex for any y \u2208 Y . Regard Y as a measurable space using the \u03c3-algebra of universally measurable sets and regard B (X) as a measurable space using the \u03c3-algebra of Borel sets. Then, for any \u01eb > 0, there exists S\u01eb : Y \u2192 B (X) measurable s.t. for all y \u2208 Y , S\u01eb (y) is ( M (y) , \u01eb ) -savvy.\nIn order to prove Lemma 4, we will need a few other technical lemmas.\nLemma 5. Consider X a compact Polish space, \u03c1 a metrization of X, \u03d5 \u2208 Lip (X, \u03c1)\u2032\u2032 and \u01eb > 0. Then, there exists f \u2208 Lip (X, \u03c1) s.t.\ni. \u2016f\u2016\u03c1 \u2264 \u2016\u03d5\u2016\nii. \u2200\u00b5 \u2208 P (X) : |E\u00b5 [f ]\u2212 \u03d5 (\u00b5)| < \u01eb\nProof. Without loss of generality, assume that \u2016\u03d5\u2016 = 1 (if \u2016\u03d5\u2016 = 0 the theorem is trivial, otherwise we can rescale everything by a scalar). Using the compactness of P (X), choose a finite set A \u2286 P (X) s.t.\nmax \u00b5\u2208P(X) min \u03bd\u2208A\ndKR (\u00b5, \u03bd) < \u01eb\n4\nUsing the Goldstine theorem, choose f \u2208 Lip (X) s.t. \u2016f\u2016\u03c1 \u2264 1 and for any \u03bd \u2208 A\n|E\u03bd [f ]\u2212 \u03d5 (\u03bd)| < \u01eb\n2\nConsider any \u00b5 \u2208 P (X). Choose \u03bd \u2208 A s.t. dKR (\u00b5, \u03bd) < \u01eb 4 . We get\n|E\u00b5 [f ]\u2212 \u03d5 (\u00b5)| \u2264 |E\u00b5 [f ]\u2212 E\u03bd [f ]|+ |E\u03bd [f ]\u2212 \u03d5 (\u03bd)|+ |\u03d5 (\u03bd)\u2212 \u03d5 (\u00b5)| < \u01eb\n4 +\n\u01eb 2 + \u01eb 4 = \u01eb\nLemma 6. Consider X a compact Polish space, \u03c1 a metrization of X, M \u2286 P (X) convex nonempty, r0 > 0 and \u00b50 \u2208 P (X) s.t. r := dKR (\u00b50,M) > r0. Then, there exists f \u2208 Lip (X, \u03c1) s.t.\ni. \u2016f\u2016\u03c1 < r \u2212 r0\nii. inf\u03bd\u2208M E\u03bd [f ]\u2212 E\u00b5 [f ] > 1 2 (r \u2212 r0) r\nProof. Define M r \u2286 Lip (X, \u03c1)\u2032 by\nM r := {\u00b5 \u2208 Lip (X, \u03c1)\u2032 | inf \u03bd\u2208M \u2016\u00b5\u2212 \u03bd\u2016 < r}\nBy the Hahn-Banach separation theorem, there is \u03d5 \u2208 Lip (X, \u03c1)\u2032\u2032 s.t. for all \u03bd \u2208 M r, \u03d5 (\u03bd) > \u03d5 (\u00b50). Multiplying by a scalar, we can make sure that \u2016\u03d5\u2016 = 3 4 (r \u2212 r0). For any \u03b4 > 0, we can choose \u03b6 \u2208 Lip (X, \u03c1)\u2032 s.t. \u2016\u03b6\u2016 < 1 and \u03d5 (\u03b6) > 34 (r \u2212 r0) \u2212 \u03b4. For any \u03bd \u2208 M , \u03bd \u2212 r\u03b6 \u2208 M r and therefore\n\u03d5 (\u03bd \u2212 r\u03b6) > \u03d5 (\u00b50)\n\u03d5 (\u03bd) > \u03d5 (\u00b50) + r\u03d5 (\u03b6) > \u03d5 (\u00b50) + r\n(\n3 4 (r \u2212 r0)\u2212 \u03b4\n)\nTaking \u03b4 to 0 we get \u03d5 (\u03bd) \u2265 \u03d5 (\u00b50) + 3 4r (r \u2212 r0). Applying Lemma 5 for \u01eb = 1 16r (r \u2212 r0), we\nget f \u2208 Lip (X, \u03c1) s.t.\n\u2016f\u2016\u03c1 \u2264 3\n4 (r \u2212 r0) < r \u2212 r0\nE\u03bd [f ] \u2265 \u03d5 (\u03bd)\u2212 1\n16 r (r \u2212 r0) \u2265 \u03d5 (\u00b50) +\n11 16 r (r \u2212 r0) \u2265 E\u00b50 [f ] + 5 8 r (r \u2212 r0) > E\u00b50 [f ] + 1 2 r (r \u2212 r0)\nLemma 7. Consider X a compact Polish space, \u03c1 a metrization of X, M \u2286 P (X) convex nonempty and r0 > 0. Define U \u2286 P (X) by\nU := {\u00b5 \u2208 P (X) | dKR (\u00b5,M) > r0}\nThen, there exists \u03b2\u2032 : U \u2192 Lip (X, \u03c1) continuous s.t. for all \u00b5 \u2208 U , denoting r\u00b5 := dKR (\u00b5,M):\ni. \u2016\u03b2\u2032 (\u00b5)\u2016\u03c1 < r\u00b5 \u2212 r0\nii. inf\u03bd\u2208M E\u03bd [ \u03b2\u2032 (\u00b5) ] \u2212 E\u00b5 [ \u03b2\u2032 (\u00b5) ] > 12 ( r\u00b5 \u2212 r0 ) r\u00b5\nProof. Define B : U \u2192 2Lip(X,\u03c1) by\nB (\u00b5) := {f \u2208 Lip (X, \u03c1) | \u2016f\u2016\u03c1 < r\u00b5 \u2212 r0, inf \u03bd\u2208M\nE\u03bd [f ]\u2212 E\u00b5 [f ] > 1\n2\n( r\u00b5 \u2212 r0 ) r\u00b5}\nBy Lemma 6, for any \u00b5 \u2208 U , B (\u00b5) 6= \u2205. Clearly, B (\u00b5) is convex. Fix f \u2208 Lip (X, \u03c1) and consider B\u22121 (f) := {\u00b5 \u2208 U | f \u2208 B (\u00b5)}. Consider any \u00b50 \u2208 B \u22121 (f), and take \u01eb > 0 s.t.\ni. \u2016f\u2016\u03c1 < r\u00b50 \u2212 \u01eb\u2212 r0\nii. inf\u03bd\u2208M E\u03bd [f ]\u2212 E\u00b50 [f ]\u2212 \u01eb > 1 2\n( r\u00b50 + \u01eb\u2212 r0 ) ( r\u00b50 + \u01eb )\nDefine V \u2286 U by\nV := {\u00b5 \u2208 U | dKR (\u00b5, \u00b50) < \u01eb, E\u00b5 [f ] < E\u00b50 [f ] + \u01eb}\nObviously V is open and V \u2286 B\u22121 (f), hence B\u22121 (f) is open. Applying Theorem A.2 (see Appendix A), we get the desired result.\nCorollary 3. Consider X a compact Polish metric space, M \u2286 P (X) convex and r0 > 0. Then, there exists \u03b2 \u2208 B (X) which is (M, r0)-savvy.\nProof. If M is empty, the claim is trivial, so assume M is non-empty. Let U \u2286 P (X) be defined by\nU := {\u00b5 \u2208 P (X) | dKR (\u00b5,M) > r0}\nUse Lemma 7 to obtain \u03b2\u2032 : U \u2192 Lip (X). Define \u03b2 : P (X) \u2192 Lip (X) by\n\u03b2 (\u00b5) :=\n{\n\u03b2\u2032 (\u00b5) if \u00b5 \u2208 U\n0 if \u00b5 6\u2208 U\nObviously \u03b2 is continuous in U . Consider any \u00b5 6\u2208 U and {\u00b5k \u2208 P (X)}k\u2208N s.t. limk\u2192\u221e \u00b5k = \u00b5. We have\nlim k\u2192\u221e dKR (\u00b5k,M) = dKR (\u00b5,M) \u2264 r0\nDenote the metric on X by \u03c1.\nlim sup k\u2192\u221e \u2016\u03b2 (\u00b5k)\u2016\u03c1 \u2264 lim sup k\u2192\u221e\n( dKR (\u00b5k,M)\u2212 r0 )\n+ = 0\nTherefore, \u03b2 is continuous everywhere.\nProof of Lemma 4. Define Z1,2,3 \u2286 Y \u00d7 B (X)\u00d7 P (X) 4 by\nZ1 := {(y, \u03b2, \u00b5, \u03bd, \u03be, \u03b6) \u2208 Y \u00d7 B (X)\u00d7 P (X) 4 | \u03b6 \u2208 M (y)}\nZ2 := {(y, \u03b2, \u00b5, \u03bd, \u03be, \u03b6) \u2208 Y \u00d7B (X)\u00d7 P (X) 4 | \u2016\u03b2 (\u00b5)\u2016 \u2264\n( dKR (\u00b5, \u03be)\u2212 \u01eb )\n+ }\nZ3 := {(y, \u03b2, \u00b5, \u03bd, \u03be, \u03b6) \u2208 Y\u00d7B (X)\u00d7P (X) 4 | E\u03bd\n[ \u03b2 (\u00b5) ] \u2212E\u00b5 [ \u03b2 (\u00b5) ] \u2265 1\n2\n( dKR (\u00b5, \u03b6)\u2212 \u01eb )\n+ dKR (\u00b5, \u03b6)}\nDefine Z := Z1 \u2229 Z2 \u2229 Z3. It is easy to see that Z1,2,3 are closed and therefore Z also. Define Z \u2032 \u2286 Y \u00d7 B (X)\u00d7 P (X)3 by\nZ \u2032 := {(y, \u03b2, \u00b5, \u03bd, \u03be) \u2208 Y \u00d7 B (X)\u00d7 P (X)3 | \u2203\u03b6 \u2208 P (X) : (y, \u03b2, \u00b5, \u03bd, \u03be, \u03b6) \u2208 Z}\nZ \u2032 is closed since it is the projection of Z and P (X) is compact. The M (y) are compact, therefore dKR ( \u00b5,M (y) ) = dKR (\u00b5, \u03b6) for some \u03b6 \u2208 M (y), and hence (y, \u03b2, \u00b5, \u03bd, \u03be) \u2208 Z \u2032 if and only if the following conditions hold:\ni. \u2016\u03b2 (\u00b5)\u2016 \u2264 ( dKR (\u00b5, \u03be)\u2212 \u01eb )\n+\nii. E\u03bd [ \u03b2 (\u00b5) ] \u2212 E\u00b5 [ \u03b2 (\u00b5) ] \u2265 12\n(\ndKR ( \u00b5,M (y) ) \u2212 \u01eb )\n+ dKR\n( \u00b5,M (y) )\nDefine W \u2286 Y \u00d7B (X)\u00d7 P (X)3 by\nW := {(y, \u03b2, \u00b5, \u03bd, \u03be) \u2208 Y \u00d7 B (X)\u00d7 P (X)3 | \u03bd, \u03be \u2208 M (y) , (y, \u03b2, \u00b5, \u03bd, \u03be) 6\u2208 Z \u2032}\nW is locally closed and in particular it is an F\u03c3 set. Define W \u2032 \u2286 Y \u00d7 B (X) by\nW \u2032 := {(y, \u03b2) \u2208 Y \u00d7 B (X) | \u2203\u00b5, \u03bd, \u03be \u2208 P (X) : (y, \u03b2, \u00b5, \u03bd, \u03be) \u2208 W}\nW \u2032 is the projection of W and P (X)3 is compact, therefore W \u2032 is F\u03c3. Let A \u2286 Y \u00d7 B (X) be the complement of W \u2032. A is G\u03b4 and in particular Borel. As easy to see, (y, \u03b2) \u2208 A if and only if \u03b2 is ( M (y) , \u01eb )\n-savvy. For any y \u2208 Y , Ay := {\u03b2 \u2208 B (X) | (y, \u03b2) \u2208 A} is closed since\nAy = \u22c2\n\u00b5\u2208P(X)\n\u22c2\n\u03bd,\u03be\u2208M(y)\n{\u03b2 \u2208 B (X) | (y, \u03b2, \u00b5, \u03bd, \u03be) \u2208 Z \u2032}\nMoreover, Ay is non-empty by Corollary 3. Consider any U \u2286 B (X) open. Then, A \u2229 (Y \u00d7 U) is Borel and therefore its image under the projection to Y is analytic and in particular universally measurable. Applying the Kuratowski\u2013 Ryll-Nardzewski measurable selection theorem, we obtain a measurable selection of the multivalued mapping with graph A. This selection is the desired S\u01eb.\nWe now obtain a relation between the notions of savviness and prudence. We remind that given X,Y Polish, \u03c0 : X \u2192 Y Borel measurable and \u00b5 \u2208 P (X), \u00b5 | \u03c0 : Y k \u2212\u2192 X stands for any corresponding regular conditional probability.\nProposition 7. Consider M \u2286 P(O\u03c9), {Mn : O n \u2192 2P(O \u03c9)}n\u2208N and \u01eb > 0. Assume Mn is a regular upper bound for M | \u03c0On . Let S be a gambling strategy s.t. for each n \u2208 N and y \u2208 O\nn, Sn (y) is ( Mn(y), \u01eb ) -savvy (relatively to \u03c1n). Then, S is \u00b5 \u2217-prudent for any \u00b5\u2217 \u2208 M .\nProof. Fix any \u00b5\u2217 \u2208 M . By Definition 2, for \u03c0On\u2217\u00b5-almost any y \u2208 O n,\n(\n\u00b5\u2217 | \u03c0On\n)\n(y) \u2208 Mn(y).\nSince Sn (y) is ( Mn(y), \u01eb ) -savvy, for any \u00b5 \u2208 P (X), denoting r\u00b5y := d n KR ( \u00b5, Mn(y) ) :\nE\u00b5\u2217 [ Sn (y;\u00b5) | yO \u03c9 ] \u2212 E\u00b5 [ Sn (y;\u00b5) ] \u2265 1\n2\n( r\u00b5y \u2212 \u01eb )\n+ r\u00b5y \u2265\n1 2 \u2016Sn (y;\u00b5)\u2016r\u00b5y\nWhen r\u00b5y \u2264 \u01eb, Sn (y;\u00b5) = 0, hence for any \u00b5 \u2208 P (X), \u2016Sn (y;\u00b5)\u2016r\u00b5y \u2265 \u2016Sn (y;\u00b5)\u2016\u01eb. We get\nE\u00b5\u2217 [ Sn (y;\u00b5) | yO \u03c9 ] \u2212 E\u00b5 [ Sn (y;\u00b5) ] \u2265 \u01eb\n2 \u2016Sn (y;\u00b5)\u2016\nCorollary 4. Consider M \u2286 P(O\u03c9), {Mn : O n \u2192 2P(O \u03c9)}n\u2208N and \u01eb > 0. Assume Mn is a regular upper bound for M | \u03c0On . Let S be a gambling strategy s.t. for each n \u2208 N and y \u2208 O n, Sn (y) is (\nMn(y), \u01eb )\n-savvy. Let F be a forecaster that dominates \u0393S. Then, for any \u00b5 \u2208 M and \u00b5-almost any x \u2208 O\u03c9\nlim sup n\u2192\u221e\ndnKR ( Fn (x:n) , Mn (x:n) ) \u2264 \u01eb (45)\nProof. By Proposition 7, S is \u00b5-prudent. By Theorem 3, for \u00b5-almost any x \u2208 X\nlim n\u2192\u221e\n(\nE\u00b5\n[\nSn ( x:n;Fn (x:n) ) | x:nO \u03c9 ] \u2212 EFn(x:n) [ Sn ( x:n;Fn (x:n) ) ]\n)\n= 0\nOn the other hand, (\n\u00b5 | \u03c0On\n)\n(x:n) \u2208 Mn (x:n) and hence, since Sn (x:n) is ( Mn (x:n) , \u01eb ) -savvy,\ndenoting rn := d n KR\n( Fn (x:n) ,Mn (x:n) ) :\nE\u00b5\n[\nSn ( x:n;Fn (x:n) ) | x:nO \u03c9 ] \u2212 EFn(x:n) [ Sn ( x:n;Fn (x:n) ) ] \u2265 1\n2 (rn \u2212 \u01eb) rn\nWe get\nlim sup n\u2192\u221e (rn \u2212 \u01eb) rn \u2264 0\nlim sup n\u2192\u221e rn \u2264 \u01eb\nWe are finally ready to complete the proof of the main theorem.\nProof of Theorem 1. Consider any M \u2208 H and a positive integer k. By Lemma 4, for any n \u2208 N, there exists SMkn : O n \u2192 B(O\u03c9) measurable s.t. for all y \u2208 On, SMkn (y) is ( Mn(y), 1 k ) -savvy. We fix such a SMkn for each M and k. It is easy to see that the d n KR-diameter of P(O\n\u03c9) is at most 2, therefore \u2016SMkn (y)\u2016 \u2264 2 and {S Mk n }n\u2208N is a gambling strategy. By Theorem 2, there is a forecaster FH that dominates \u0393SMk for all M and k. By Corollary 4, for any M \u2208 H, \u00b5 \u2208 M , positive integer k and \u00b5-almost any x \u2208 X\nlim sup n\u2192\u221e\ndnKR\n( FHn (x:n) ,Mn (x:n) ) \u2264 1\nk\nIt follows that\nlim n\u2192\u221e\ndnKR\n( FHn (x:n) ,Mn (x:n) ) = 0\n6 Discussion\nWe see several natural directions for extending this work. Theorem 1 is formulated using the Kantorovich-Rubinstein metric, but instead we could have considered to total variation metric dTV, as in Bayesian merging of opinions [4]. This would remove the need to renormalize the metrization of O\u03c9 and yield a stronger result. However, we currently don\u2019t know whether this stronger hypothesis is true.\nThe present work doesn\u2019t analyze any considerations of computational complexity or even computability. It would be very valuable, especially for practical applications, to understand the complexity of forecasters satisfying equation 10 given various complexity-theoretic assumptions on H. In particular, that would entail designing concrete forecasting algorithms of this type, whereas the existence proof in the present work is non-constructive. Also, it would be valuable to analyze\nthe speed of convergence in 10 and the possibly also the trade-offs between forecaster complexity and speed of convergence.\nFinally, we think that the current work may have some bearing on the so-called \u201cgrain of truth\u201d problem (problem 5j in [3]). The problem is, given a system of interacting rational agents, proving their convergence to some reasonable game-theoretic solution concept, e.g. a Nash equilibrium. For a pair of Bayesian agents, this normally requires each agent to lie in the model class H of the other agent. However, this requirement is difficult to satisfy in settings that are fairly general, since usually the computational complexity of a Bayesian agent is higher than the complexities of all models in its prior. On the other hand, using our approach, each agent may learn incomplete models satisfied by the other agent that are computationally simpler than a full simulation. This might be sufficient to produce meaningful game-theoretic guarantees.\nA Appendix: Some Useful Theorems\nThe following variant of the Optional Stopping Theorem appears in [7] as Theorem 5.4.7.\nTheorem A.1. Let \u2126 be a probability space, {Fn \u2286 2 \u2126}n\u2208N a filtration of \u2126, {Xn : \u2126 \u2192 R}n\u2208N a stochastic process adapted to F and M,N : \u2126 \u2192 N \u2294 {\u221e} stopping times (w.r.t. F) s.t. M \u2264 N . Assume that {Xmin(n,N)}n\u2208N is a uniformly integrable submartingale. Using Doob\u2019s martingale convergence theorem, we can define XN : \u2126 \u2192 R by 6\nXN (x) := lim n\u2192\u221e Xmin(n,N) (x) =\n{\nXN(x) (x) if N (x) < \u221e limn\u2192\u221eXn (x) if N (x) = \u221e (46)\nWe define XM : \u2126 \u2192 R is an analogous way 7. We also define the \u03c3-algebra FM \u2286 2 \u2126 by\nFM := {A \u2286 X measurable | \u2200n \u2208 N : A \u2229M \u22121 (n) \u2208 Fn} (47)\nThen:\nE [ XN | FM ] \u2265 XM (48)\nThe following variant of the Michael selection theorem appears in [8] as Theorem 3.1.\nTheorem A.2 (Yannelis and Prabhakar). Consider X a paracompact Hausdorff topological space and Y a topological vector space. Suppose B : X \u2192 2Y is s.t.\ni. For each x \u2208 X, B (x) 6= \u2205.\nii. For each x \u2208 X, B (x) is convex.\niii. For each y \u2208 Y , {x \u2208 X | y \u2208 B (x)} is open.\nThen, there exists \u03b2 : X \u2192 Y continuous s.t. for all x \u2208 X, \u03b2 (x) \u2208 B (x).\n6The limit in equation 46 is converges almost surely, which is sufficient for our purpose. 7This time we can\u2019t use Doob\u2019s martingale convergence theorem, but wheneverM (x) = \u221e we also haveN (x) = \u221e,\ntherefore the limit still almost surely converges.\nB Appendix: Examples of Regular Upper Bounds\nProof of Example 1. In this case, condition i means that N must be pointwise closed and condition iii means that for every \u00b5 \u2208 M and y \u2208 Y s.t. \u00b5 ( \u03c0\u22121(y) )\n> 0, \u00b5 | \u03c0\u22121(y) \u2208 N(y). Since all three conditions are closed w.r.t. arbitrary set intersections, there is a unique minimum (it is the intersection of all multivalued mappings satisfying the conditions). It remains to show that N(y) defined by equation 6 is convex. Fix y \u2208 Y and denote A the set appearing on the right hand side of equation 6 under the closure. Consider any \u00b51, \u00b52 \u2208 M s.t. \u00b51 ( \u03c0\u22121(y) ) , \u00b52 ( \u03c0\u22121(y) )\n> 0 and p \u2208 (0, 1). We have\np\u00b51 + (1\u2212 p)\u00b52 \u2208 M\n( p\u00b51 + (1\u2212 p)\u00b52 ) | \u03c0\u22121(y) \u2208 A\n\u00b51 ( \u03c0\u22121(y) ) p ( \u00b51 | \u03c0 \u22121(y) ) + \u00b52 ( \u03c0\u22121(y) ) (1\u2212 p) ( \u00b52 | \u03c0 \u22121(y) )\n\u00b51 ( \u03c0\u22121(y) ) p+ \u00b52 ( \u03c0\u22121(y) ) (1\u2212 p) \u2208 A\nIt is easy to see that for any q \u2208 (0, 1) there is p \u2208 (0, 1) s.t.\nq = \u00b51\n( \u03c0\u22121(y) ) p\n\u00b51 ( \u03c0\u22121(y) ) p+ \u00b52 ( \u03c0\u22121(y) ) (1\u2212 p)\nIt follows that for any q \u2208 (0, 1)\nq ( \u00b51 | \u03c0 \u22121(y) ) + (1\u2212 q) ( \u00b52 | \u03c0 \u22121(y) ) \u2208 A\nWe got that A is convex and therefore N(y) = A is also convex.\nProposition B.1. Consider W1,2,3,4 Polish spaces, X := \u220f4 i=1 Wi, Y := \u220f3\ni=1Wi and Z = W1 \u00d7 W2. Let \u03c0 Y : X \u2192 Y , \u03c0Z : X \u2192 Z and \u03c0W : X \u2192 W1 be the projection mappings, \u00b5 \u2208 P(X) and K : Z k \u2212\u2192 W3. Assume that \u03c0 Y \u2217 \u00b5 = \u03c0 Z \u2217 \u00b5\u22c9K. Then, for \u03c0 W \u2217 \u00b5-almost any w \u2208 W1\n\u03c0Y\u2217\n( \u00b5 | \u03c0W )\n(w) = \u03c0Z\u2217\n( \u00b5 | \u03c0W ) (w)\u22c9K (49)\nProof. We know that \u00b5 = ( \u00b5 | \u03c0W )\n\u2217 \u03c0W\u2217 \u00b5. It follows that\n\u03c0Y\u2217 \u00b5 = \u03c0 Y \u2217\n( \u00b5 | \u03c0W )\n\u2217 \u03c0W\u2217 \u00b5 = \u03c0 Z \u2217\n( \u00b5 | \u03c0W )\n\u2217 \u03c0W\u2217 \u00b5\u22c9K\nDefine K \u2032 : Z k \u2212\u2192 Y by K \u2032(z) := \u03b4z \u00d7 K(z) and denote composition of Markov kernels by \u25e6.\nSince pushforward commutes with composition, we have\n\u03c0Y\u2217 \u00b5 = ( \u03c0Y \u25e6 \u00b5 | \u03c0W )\n\u2217 \u03c0W\u2217 \u00b5 =\n( K \u2032 \u25e6 \u03c0Z \u25e6 \u00b5 | \u03c0W )\n\u2217 \u03c0W\u2217 \u00b5\nLet \u03c0Y W : Y \u2192 W be the projection mapping and denote \u03bd = \u03c0Y\u2217 \u00b5. We get\n\u03bd = ( \u03c0Y \u25e6 \u00b5 | \u03c0W )\n\u2217 \u03c0Y W\u2217 \u03bd =\n( K \u2032 \u25e6 \u03c0Z \u25e6 \u00b5 | \u03c0W )\n\u2217 \u03c0Y W\u2217 \u03bd\nWe also know that\nsupp\u03c0YW\u2217 \u03bd \u22c9 \u00b5 | \u03c0 W = supp\u03c0W\u2217 \u00b5\u22c9 \u00b5 | \u03c0 W \u2286 graph \u03c0W\nAs easy to see, the above implies\nsupp\u03c0Y W\u2217 \u03bd \u22c9 ( \u03c0Y \u25e6 \u00b5 | \u03c0W ) \u2286 graph\u03c0Y W\nsupp\u03c0YW\u2217 \u03bd \u22c9 ( K \u2032 \u25e6 \u03c0Z \u25e6 \u00b5 | \u03c0W ) \u2286 graph \u03c0YW\nBy the uniqueness of regular conditional probability, we conclude that for \u03c0W\u2217 \u00b5-almost any w \u2208 W1\n( \u03bd | \u03c0Y W ) (w) = ( \u03c0Y \u25e6 \u00b5 | \u03c0W ) (w) = ( K \u2032 \u25e6 \u03c0Z \u25e6 \u00b5 | \u03c0W ) (w)\nThis immediately implies the desired result.\nProof of Example 2. Fix \u03c1 a metrization of O\u03c9. The condition supp\u00b5 \u2286 yO\u03c9 is closed because it is equivalent to the vanishing of the continuous function Ex\u223c\u00b5 [ \u03c1 (x, yO\u03c9) ]\n. The other condition in the definition of MKn is closed because pushforward and semidirect product are continuous in the weak topology. This gives us condition i. Condition ii holds since the convex combination of measures supported on yO\u03c9 is supported on yO\u03c9 and since pushforward and semidirect product commute with convex combinations.\nConsider any \u00b5 \u2208 MK . We know that for any m \u2265 n, \u03c0Om+1\u2217\u00b5 = \u03c0 O m\u2217\u00b5 \u22c9 Km. Applying\nProposition B.1, this implies that for \u03c0On\u2217\u00b5-almost any y \u2208 O n\n\u03c0Om+1\u2217\n(\n\u00b5 | \u03c0On\n)\n(y) = \u03c0Om\u2217\n(\n\u00b5 | \u03c0On\n)\n(y)\u22c9Km\nWe conclude that (\n\u00b5 | \u03c0On\n)\n(y) \u2208 MKn (y), proving condition iii.\nProposition B.2. Consider X a compact Polish space, d \u2208 N, Y a compact subset of Rd, \u03c0 : X \u2192 Y continuous and \u00b5 \u2208 P (X). Then, for \u03c0\u2217\u00b5-almost any y \u2208 Y\nlim r\u21920\n\u00b5 | \u03c0\u22121 ( Br (y) ) = ( \u00b5 | \u03c0 ) (y) (50)\nProof. Let {fk \u2208 C (X)}k\u2208N be dense in C (X). For any y \u2208 Y and r > 0 we denote \u03c7yr : Y \u2192 {0, 1} the characteristic function of Br (y) and \u03c7 \u03c0 yr : X \u2192 {0, 1} the characteristic function of \u03c0 \u22121 ( Br (y) ) . For any k \u2208 N and y0 \u2208 supp\u03c0\u2217\u00b5 we have \u00b5 ( \u03c0\u22121 ( Br (y0) ) ) = \u03c0\u2217\u00b5 ( Br (y0) ) > 0 and\nE\u00b5\n[\nfk | \u03c0 \u22121\n( Br (y0) )\n] = E\u00b5\n[\n\u03c7\u03c0y0rfk\n]\n\u00b5 ( \u03c0\u22121 ( Br (y0) )\n) =\nEy\u223c\u03c0\u2217\u00b5\n[\nE\u00b5\n[\n\u03c7\u03c0y0rfk | \u03c0 \u22121 (y)\n]\n]\n\u03c0\u2217\u00b5 ( Br (y0) )\nE\u00b5\n[\nfk | \u03c0 \u22121\n( Br (y0) )\n] = Ey\u223c\u03c0\u2217\u00b5\n[\n\u03c7y0r E\u00b5 [ fk | \u03c0 \u22121 (y) ]\n]\n\u03c0\u2217\u00b5 ( Br (y0) )\nIn particular, the above holds for \u03c0\u2217\u00b5-almost any y0 \u2208 Y . Applying the Lebesgue differentiation theorem, we conclude that there is A \u2286 Y s.t. \u03c0\u2217\u00b5 (A) = 1 and for any y \u2208 A\n\u2200k \u2208 N : lim r\u21920 E\u00b5\n[\nfk | \u03c0 \u22121\n( Br (y) )\n]\n= E\u00b5\n[\nfk | \u03c0 \u22121 (y)\n]\nNow consider any f \u2208 C (X). For any \u01eb > 0, there is k \u2208 N s.t. \u2016f \u2212 fk\u2016 < \u01eb and therefore, for any y \u2208 A\nlim sup r\u21920 E\u00b5\n[\nf | \u03c0\u22121 ( Br (y) )\n]\n\u2264 lim sup r\u21920 E\u00b5\n[\nfk | \u03c0 \u22121\n( Br (y) )\n]\n+ \u01eb = E\u00b5\n[\nfk | \u03c0 \u22121 (y)\n]\n+ \u01eb\nlim sup r\u21920 E\u00b5\n[\nf | \u03c0\u22121 ( Br (y) )\n]\n\u2264 E\u00b5\n[ f | \u03c0\u22121 (y) ] + 2\u01eb\nSimilarly\nlim inf r\u21920 E\u00b5\n[\nf | \u03c0\u22121 ( Br (y) )\n]\n\u2265 E\u00b5\n[ f | \u03c0\u22121 (y) ] \u2212 2\u01eb\nTaking \u01eb to 0, we conclude that\nlim r\u21920 E\u00b5\n[\nf | \u03c0\u22121 ( Br (y) )\n]\n= E\u00b5\n[ f | \u03c0\u22121 (y) ]\nlim r\u21920\n\u00b5 | \u03c0\u22121 ( Br (y) ) = ( \u00b5 | \u03c0 ) (y)\nProof of Example 3. There is a unique minimum since all three defining properties of N are closed w.r.t. arbitrary set intersections. To see N is a regular upper bound, note that conditions i and ii are part of the definition whereas condition iii follows immediately from Proposition B.2.\nAcknowledgments\nThis work was supported by the Machine Intelligence Research Institute in Berkeley, California. We wish to thank Michael Greinecker for pointing out Theorem A.2 to us.\nReferences\n[1] Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.\n[2] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, New York, NY, USA, 2014.\n[3] Marcus Hutter. Open problems in universal induction & intelligence. Algorithms, 2(3):879\u2013906, 2009.\n[4] David Blackwell and Lester Dubins. Merging of opinions with increasing information. Ann. Math. Statist., 33(3):882\u2013886, 09 1962.\n[5] Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor. Logical induction. CoRR, abs/1609.03543, 2016.\n[6] Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. Using and combining predictors that specialize. In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing, STOC \u201997, pages 334\u2013343, New York, NY, USA, 1997. ACM.\n[7] Rick Durrett. Probability: Theory and Examples. Cambridge University Press, New York, NY, USA, 4th edition, 2010.\n[8] Nicholas C. Yannelis and N.D. Prabhakar. Existence of maximal elements and equilibria in linear topological spaces. Journal of Mathematical Economics, 12(3):233 \u2013 245, 1983."}], "references": [{"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Open problems in universal induction ", "author": ["Marcus Hutter"], "venue": "intelligence. Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Merging of opinions with increasing information", "author": ["David Blackwell", "Lester Dubins"], "venue": "Ann. Math. Statist., 33(3):882\u2013886,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1962}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth"], "venue": "In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Probability: Theory and Examples", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Existence of maximal elements and equilibria in linear topological spaces", "author": ["Nicholas C. Yannelis", "N.D. Prabhakar"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}], "referenceMentions": [{"referenceID": 0, "context": "[1] or Chapter 21 in [2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1] or Chapter 21 in [2]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "o2k+1 = o2k, then whatever is the behavior of the even observations o2k, the property o2k+1 = o2k should asymptotically be assigned high probability by the forecast (this idea was discussed in [3] as \u201copen problem 4j\u201d).", "startOffset": 193, "endOffset": 196}, {"referenceID": 3, "context": "This convergence theorem can be regarded as an analogue for incomplete models of Bayesian merging of opinions (see [4]), and is our main result.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "= 1 (13) If we only cared about predicting the next observation (as opposed to producing a probability measure over the entire sequence), this example (and any other instance of Example 2, at least in the case On = O finite) would be a special case of the \u201csleeping experts\u201d setting in online learning (see [6]).", "startOffset": 307, "endOffset": 310}, {"referenceID": 3, "context": "Theorem 1 is formulated using the Kantorovich-Rubinstein metric, but instead we could have considered to total variation metric dTV, as in Bayesian merging of opinions [4].", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "Finally, we think that the current work may have some bearing on the so-called \u201cgrain of truth\u201d problem (problem 5j in [3]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "A Appendix: Some Useful Theorems The following variant of the Optional Stopping Theorem appears in [7] as Theorem 5.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "We also define the \u03c3-algebra FM \u2286 2 \u03a9 by FM := {A \u2286 X measurable | \u2200n \u2208 N : A \u2229M \u22121 (n) \u2208 Fn} (47) Then: E [ XN | FM ] \u2265 XM (48) The following variant of the Michael selection theorem appears in [8] as Theorem 3.", "startOffset": 195, "endOffset": 198}], "year": 2017, "abstractText": "We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is \u201csuspected\u201d to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) KantorovichRubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.", "creator": "LaTeX with hyperref package"}}}