{"id": "1705.04839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2017", "title": "Annotating and Modeling Empathy in Spoken Conversations", "abstract": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline. This suggests that automatic classification for empathy influenced judgments, emotion recognition, or behavior. Empathy in speech is not an independent trait. To better describe the differences between different speech and language, we suggest the addition of different types of processing and labeling and labeling, to identify differences in language, language, and speech.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 13 May 2017 14:49:08 GMT  (884kb,D)", "http://arxiv.org/abs/1705.04839v1", "Journal"]], "COMMENTS": "Journal", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["firoj alam", "morena danieli", "giuseppe riccardi"], "accepted": false, "id": "1705.04839"}, "pdf": {"name": "1705.04839.pdf", "metadata": {"source": "CRF", "title": "Annotating and Modeling Empathy in Spoken Conversations", "authors": ["Firoj Alam", "Morena Danieli", "Giuseppe Riccardi"], "emails": ["giuseppe.riccardi}@unitn.it"], "sections": [{"heading": null, "text": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.\nKeywords: Empathy, Emotion, Spoken Conversation, Behavior Analysis\n\u2217Corresponding author Email address: {firoj.alam, morena.danieli, giuseppe.riccardi}@unitn.it\n(Firoj Alam, Morena Danieli and Giuseppe Riccardi)\nPreprint submitted to Journal of CSL Templates May 16, 2017\nar X\niv :1\n70 5.\n04 83\n9v 1\n[ cs\n.C L\n] 1\n3 M\nay 2"}, {"heading": "1. Introduction", "text": "We are interested in the problem of learning computational models of emotional states manifested in human conversation. While early works on affective computing mostly focused on the recognition of basic emotions ([1]), in this work we investigate empathy. In this paper, we refer to the psychological definition of empathy by Hoffman, who defines it as \"an emotional state triggered by another\u2019s emotional state or situation, in which one feels what the other feels or would normally be expected to feel in his situation\" ([2]). In particular, we focus on identifying the manifestations of empathy emerging in spoken real-life conversations.\nThe word empathy was coined by Titchener in 1909 [3] as a translation of the German term Einf\u00fchlung. Since then this concept has been widely used to refer to a range of pro-social emotional behaviors, from sympathy to compassion, and including accurate understanding of the other person\u2019s feelings. Recently, the hypothesis that one\u2019s empathy is triggered by understanding and sharing others\u2019 emotional states has found neuroscientific underpinnings in the discovery of the mirror-neurons system. This is hypothesized to embody the automatic and unconscious routines of emotional and empathic behaviors in interpersonal relations. These include action understanding, attribution of intentions (mind-reading), and recognition of emotions and sensations [4]. In everyday life, empathy supports important aspects of inter-personal communication to an extent where some psychic diseases that affect the relationship with other persons, such as autism and Asperger syndrome, are explained in terms of impairment of empathic ability [5].\nIn order to develop a computational model of empathy we need to design an experiment where the unfolding of emotions occurring in an empathic process can be observed. To this end we adopt the modal model of emotions by Gross [6] as a promising framework for defining an operational concept of empathy. We use this framework for developing the annotation guidelines for the annotation of empathy in a real-life spoken conversation corpus. Our research work is orga-\nnized into three phases: data observation and analysis, corpus annotation, and automatic classification. The guidelines for the annotation of the spoken corpus recommend a continuous selection of the emotion perception on the speech channel, and the assignment of discrete labels.\nThe main contributions of this paper are 1) the design and evaluation of an annotation model of empathy manifestations, 2) the training of an automatic classification system for empathy based on acoustic, lexical and psycholinguistic features, and 3) the evaluation of the classification system on a corpus of humanhuman spoken conversations. To the best of our knowledge, this is the first published research work on perceiving, annotating and automatically recognizing empathy manifestations from human-human spoken conversations occurring in in-vivo situations.\nThe paper is organized as follows. In Section 2, we provide a review of relevant research on empathy. We then briefly discuss the modal model of emotions and its relevance to our work in Section 3. In Section 4 we describe the proposed annotation scheme for the annotators. In Section 5 we discuss the details of the corpus analysis. We provide the computational architecture of the segmentation and classification system including feature extraction and fusion along with results and discussions in Section 6. We then provide the results and discussion of our work in Section 6.8."}, {"heading": "2. Background Research", "text": "Psychology and Neuroscience Research Over the past decades there have been significant efforts in investigating empathy in the fields of psychology and neuroscience [7] [8]. The complexity of the neural and psychological phenomena to be accounted for is huge and, in part, that complexity explains the existence of several psychological definitions of empathy. For example, the work in [9] accounts for different empathic phenomena occurring in the literature on empathy, and [10] examines eight distinct phenomena commonly labeled as empathy including emotional contagion, sympathy, projection, and\naffective inferential processes. Decety and Lamm [11] observe that some of the different definitions of empathy may share the underlying concept of \"[...] an emotional experience that is more congruent with another\u2019s situation than with one\u2019s own\". Those authors also state that empathic emotional experiences imply self-other differentiation, as opposed to emotional contagion. Actually if we abstract from the differences in the theoretical perspectives, we may find some common features. Most of the definitions describe empathy as a type of emotional experience and/or emotional state. Moreover, the different definitions can be divided into two main classes. One encompasses the cognitive aspects of empathic behavior, such as one\u2019s ability to accurately understand the feelings of another person. The other class entails sharing or the subject\u2019s internal mimic of such feelings such as sympathy, emotional contagion, and compassion. The work in [12] takes a different perspective on empathy manifestations by focusing on the role of motivations for explaining a particular feature of the empathic experience. i.e. its oscillation between automaticity and dependency from the situational context.\nComputational Models Computational models of emotional states are needed to design machines that can understand and interact affectively with humans. Different signal components have been considered for analyzing the manifestation of emotional experience in speech. Both verbal and non-verbal levels of spoken communication [13] have been considered since both are suggested to embody the expressive potential of language. Major focus has been devoted to the paralinguistic features of emotional speech, on the basis of the experimental evidence that emotional information is mostly conveyed by those levels (see [14] for a state-of-the-art review). In the field of spoken language processing there are several collections of emotional annotated corpora. The authors in [15] report a significant disparity among those corpora in terms of complexity of the annotated emotions, explicitness of the emotion definitions, and identification of the annotation units. Most emotional corpora have been designed to perform specific tasks such as emotion recognition or emotional speech synthesis [16, 17]. The associated annotation schemes often depend on\nthe specific tasks as well. The HUMAINE project [18] and the emotion multilingual collection in [19] base their annotation schemes on sets of discrete emotional lexical items or basic emotions.\nProviding explicative models for annotating the emotional process itself in naturally occurring conversations is still an open challenge. Efforts in this direction are currently being made in the affective computing research, where awareness about the need for continuous annotation is increasing. The models that foster this approach, such as those discussed in [20] and [21], require annotators to continuously assign values to emotional dimensions and sets of emotional descriptors. Metallinou and Narayanan [22] emphasize that continuous annotation has several benefits, as well as some open challenges. One interesting finding is that continuous annotation may show regions which are characterized by perceived transitions of the emotional state. In [22], authors report a high number of factors that may affect inter-annotator agreement, such as person-specific annotation delays and confidence in understanding emotional attributes. There are very few studies in terms of empathy classification and most of them are carried out within controlled scenarios. Kumano et al. [23] studied four-party meeting conversations to estimate and classify empathy, antipathy and unconcerned emotional interactions utilizing facial expression, gaze and speech-silence features. In [24] and [25], Xiao et al. analyzed therapists\u2019 conversation to automatically classify empathic and non-empathic utterances using linguistic and acoustic information."}, {"heading": "3. The Modal Model of Emotions", "text": "Many psychologists have studied emotional episodes from the point of view of appraisal dimensions. Gross [6] has provided evidence that concepts such as emergence \u2014 derivation from the expectations of relationships \u2014 and unfolding \u2014 sequences that persist over time \u2014 may help in explaining emotional events. It has been shown that temporal unfolding of emotions can be conceptualized and experimentally tested [26]. The modal model of emotions developed\nby Gross [6, 27] emphasizes the attentional and appraisal acts underlying the emotion-arousing process. In Figure 1, we provide the original schema of Gross model. The individuals\u2019 core Attention-Appraisal processes (included in the box) are affected by the Situation that is defined objectively in terms of physical or virtual spaces and objects. The Situation compels the Attention of the individual; it triggers an Appraisal process and gives rise to coordinated and malleable Responses. It is important to note that this model is dynamic and the situation may be modified (directed arc from the Response to the Situation) by the actual value of the Response generated by the AttentionAppraisal process. The modal model of emotions provides a useful framework for describing the contextual dynamics of emotions within an affective scene([28]), since it decomposes the emotional process in terms of situation selection, attentional deployment, and situational modification. Gross\u2019 informal model can provide insight in the process where the manifestation of empathic responses may modify the initial emotional context.\nThe definitions of empathy available in the literature are not sufficient to guide observers through their perception process. We overcome this limitation by using the modal model to provide the observers with grounded descriptions of the unfolding of the emotions while they are requested to observe and interpret the affective scene. The annotator will need to identify the emergence, appraisal, and unfolding of emotions felt by the call center agent in the ongoing (sub)dialogs. In doing so, the annotator should also be able to perceive if, and to what extent, an empathic response may modify a situation where other emotions such as frustration or anger are expressed by the customer. In section 4.3 we\ndescribe how the modal model of emotions applies to our annotation model of empathy."}, {"heading": "4. The Empathy Annotation Model", "text": "The goals of the annotation model for empathy is to provide empirical evidence of the reference definition and provide annotated signal examples to train a computational model. In the following sections we describe the challenges in annotating real-life stimuli (section 4.1), report on the qualitative data analysis of the dialogue corpus (section 4.2), the operational definition of empathy (section 4.3) in the context of call center conversations, the annotation process (section 4.4) and the evaluation of the annotators\u2019 decisions (section 4.5)."}, {"heading": "4.1. Annotation of Real-Life Spoken Conversations", "text": "The annotation unit is the stimulus presented to the observer (annotator) to perform a selection task over a decision space such as the set of emotional state tags. In general the annotator may be presented with images or speech segments (stimuli), and a set of emotional labels. The stimulus is defined in terms of the medium the emotion is being transmitted through and its content and context. The medium may be speech [29], image [30] or multimodal [31]. The content refers to the information encoded in the stimulus signal such as facial expression of anger or a speech utterance. The context of the stimulus is represented by the spatial or temporal signals neighboring the stimulus. Knowledge of the context may be crucial in interpreting the cause of emotion manifestations. For a speech stimulus in a conversation the context is represented by the preceding dialog turns [29]. The above description of the annotation unit does not assume a univocal relationship between the occurrence of an emotion and its corresponding expression. Last but not least there might be a non-deterministic relation between the emotion-expression and motive-communication ([32]).\nMost research in affective computing has been limited to stimuli that are designed in advance and are artificially generated. Respective examples of such\nstimuli are sentences to be read and actors enacting affective scenes [33]. Another limitation of previous annotation tasks is that stimuli are context-free and speech utterances or images are annotated in isolation.\nThe limitations of determining the temporal boundaries of the annotation units have motivated researchers to investigate the process of continuous annotation [22, 20]. Yet, state-of-the-art complete continuous affective scene annotation techniques are highly demanding for observers and are not very effective in terms of inter-annotator agreement. In our work we address the complex task of defining and searching the annotation unit in real-life spoken conversations. We describe below how the annotation model we propose exploits the conversation context for the perception and annotation of the empathic events."}, {"heading": "4.2. Qualitative Data Analysis", "text": "We have performed qualitative analysis of the spoken conversation corpus to gain insights into the manifestation of emotions and empathic phenomena in affective scenes. The analysis was carried out over a corpus of human-human dyadic Italian call center conversations that will be described in Section 5. We analyzed one hundred conversations (more than 11 hours), and selected dialog turns where the speech signal showed the emergence of the set of basic emotions (e.g. frustration, anger) on the customer channel and empathy on the agent channel.\nIn Table 1, we present a dialog excerpt with annotations to illustrate the paralinguistic, lexical and discourse cues. The dialog excerpt is reported in the first column of the table, where C is the customer, and A is the agent. The situation is the following: C is calling because a payment is actually overdue: he is ashamed for not being able to pay immediately and his speech has plenty of hesitations. This causes an empathic response by A: that emerges from the intonation profile of A\u2019s reply and from her lexical choices. In the second question of A\u2019s turn, she uses the hortatory first person plural instead of the singular one. Also, the rhetorical structure of A\u2019s turn, i.e., the use of questions instead of assertions, conveys her empathic attitude. The annotator perceived\nthe intonation variation and marked the speech segment corresponding to the intonation unit starting with the word proviamo (let us try) as onset of the emotional process.\nThe outcome of the qualitative analysis has supported the view that emotionally relevant conversational segments are often characterized by significant transitions in the paralinguistic patterns or the occurrence of lexical cues. As expected, such variations may co-occur not only with emotionally-connoted words but also with functional parts of speech (POS) such as Adverbs and Interjections. Phrases and Verbs, as shown in Table 1, could also lexically support the expression of emotional states."}, {"heading": "4.3. The Operational Definition of Empathy", "text": "Following the modal model of emotions and the annotation model discussed above, we first describe the context of the situation, the attention, the appraisal\nand the response components of the empathic process.\nThe context of the situation: In call center conversations, customers may call to ask for information or for help to resolve technical or accounting issues. Agents are supposed to be cooperative and empathic and they are trained for the task. However, variabilities in the agents\u2019 or customers\u2019 personalities, behaviors and random events lead to statistical variations in the emotional unfolding. The operational definition of empathy that may be applied to this context requires the annotators to be informed of the social context and task. They are trained to focus over sub-dialogues where the agent anticipates or proposes solutions and clarifications (attention), based on the understanding of the customer\u2019s problem (appraisal). As a consequence, the acts of the agent may prevent or releave customer\u2019s unpleasant feelings (response). Therefore, we operationally define empathy as a situation where an agent anticipates or views solutions and clarifications, based on the understanding of a customer\u2019s problem or issue, that can relieve or prevent the customer\u2019s unpleasant feelings.\nThe selection of the stimuli includes a continuous search of the speech segments preceding and following the perception of the outset of the empathy manifestation. The task of the annotator is to identify the context (left of the outset) and the target (right of the outset) empathy segment. The context is defined to be neutral with respect to the target empathic segment. This does not mean that emotions rather than empathy cannot be manifested by the speaker in the speech segment that the annotator identifies as context, but that the possibly occurring emotions are not recognized as empathy by the annotator. The reference to a neutral, i.e. non-empathic, segment supports the annotators in their perception process while identifying the outset of the empathic segment. This annotation protocol has been applied to annotate sequences of basic emotions occurring in affective scenes ([34]).\nInstructions were given to the annotators to achieve the most confident decision in identifying and tagging the neutral and empathy segment pairs, based on the perceived paralinguistic and/or linguistic cues. The annotation guidelines provided examples of situations of kind described in section 4.2."}, {"heading": "4.4. The Annotation Procedure", "text": "The typical approach of annotation selects a set of conversation and annotate each segments in the conversation. In this work, we focused on first instance of neutral-empathy segment pairs within each conversation. The rationale behind this was to maximize the number of annotated conversations with many speakers given limited resources. Annotators manually refined the boundaries of the segments generated by an off-the-shelf Speech/Non-Speech segmenter. The annotators tagged the Empathy segments on the agent channel and the basic emotions (Frustation and Anger) on the customer channel. The reason of choosing these basic emotions was due to the limited resources for annotation. Another reason was that we observed these are the most frequently occuring cases in such conversations. Annotators were instructed to select the candidate segment pairs with a decision refinement process. Once the relevant speech region was identified, the annotators could listen to the speech as many times as they needed to judge if the selected segment(s) could be tagged with any of the target labels. The average per-conversation annotation time was 18 minutes for an average duration of a conversation of 6 minutes. For the annotation task, the annotators used the EXMARaLDA Partitur Editor [35]. The corpus has been annotated with four labels: Empathy, Anger, Frustration, Neutral to describe the complete affective scene of the context (more in Section 5)."}, {"heading": "4.5. Evaluation of the Annotation Model", "text": "To assess the reliability of the annotation model, we designed the following evaluation task. Two annotators, with psychology background, worked independently over a set of 64 spoken conversations randomly selected from the call center corpus. The annotators were of similar age, same ethnicity and opposite gender. We intended to assess if the annotators could perceive a change in the emotional state at the same onset position (Neutral leading into Empathy), as well as their agreement with the assignment of the empathy label.\nThe inter-annotator agreement for the annotated segments between the two annotators is 53.1%, where the agreement on the same onset position is 31.2%.\nTo measure the reliability of the annotations we calculated inter-annotator agreement by using the kappa statistics [36]. Kappa statistics is frequently used to assess the degree of agreement among any number of annotators by excluding the probability that they agree by chance. The kappa coefficient ranges between 0 (agreement is due to chance) and 1 (perfect agreement). Values above 0.6 suggest acceptable agreement. Our annotation task was challenging because it combined categorical annotation with continuous perception of the slowly varying emotion expression from speech-only stimuli. Thus we evaluated the agreement between annotators based on a partial match: two annotators agreed on the selection of the onset time stamps within a tolerance window of 5 sec. We found reliable results with kappa value for empathy 0.74.\nMost categorical emotion annotation research in speech deals with lower human agreement (greater than 0.50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22]. In our case the positive evaluation results may be motivated by the operationalized definition of empathy, by the observability of the complete paralinguistic and linguistic contexts, and finally by the binary annotation task."}, {"heading": "5. Corpus Analysis", "text": "The corpus includes 1894 randomly selected customer-agent conversations, which were collected over the course of six-months, amounting to 210 hours of speech data. These conversations were recorded on two separate channels of 16 bits per sample and 8kHz sampling rate. The average length of the conversations was 406 seconds.\nWe analyzed the distribution of emotion state label sequences describing the affective scene in the conversations. We observed that empathy was perceived by annotators in 27.72% of the conversations out of whole set as can be seen in Table 2, in which column represents agent\u2019s channel and row represents customer\u2019s\nchannel. By analyzing only empathic conversations (27.72%), we observed that when agent manifested empathy no frustration or anger was manifested by the customer in 70% of the cases. This affective scene may be explained, in part, by specific training of agents in anticipating customers\u2019 issues and taking appropriate actions. For the remaining 30% of the conversations customers had manifested anger or frustration or both.\nMoreover, for 12.99% (see in Table 2) of the conversations out of whole set, customers had manifested anger or frustration or both while no empathy had been perceived on the agent side. From our analysis we can then characterize the stereotypical situations that occur in the affective scene with the two following scenarios:\n\u2022 Customer shows anger and/or frustration and the agent does not react to\nthe speech and language cues of the customer. This is the case in which agent was expected to be empathic, but failed to recognize or react to customer\u2019s signals.\n\u2022 Agent is empathic in response to or in anticipation of the customer\u2019s anger,\nfrustration or both."}, {"heading": "5.1. Acoustic Feature Analysis", "text": "We investigated and compared the pattern sequences of low-level acoustic features before and after the onset point, from the neutral to the empathy segment. An example of the speech segment annotation is shown in Figure\n2, where we plot the spectral centroid 1 feature values across the neutral and empathy connoted segment. Each segment is 15 seconds long and the onset is marked by a vertical bold line, which separates the left (context) and right segment annotated with empathy. From the signal trend of this feature we see that there is a distinctive profile change, which is corroborated by its high statistical significance (p \u2212 value = 4.61E \u2212 51, d = 1.4 and t = \u221216.6 using two-tailed two-sample t-test).\nThe low-level features were extracted from both left and right segment with 100 overlapping frames per second, pre-emphasis with k=0.97, and hammingwindowing, using openSMILE [39]. For voice quality features we used gaussian windowing function. Then, we computed averages for each segment of the corresponding conversation. In order to evaluate the relevance of each feature we applied a statistical significance test, the two-tailed two-sample t-test at p-value = 0.01. We analyzed 45 low-level acoustic features from five categories: pitch\n1Spectral centroid measure the brightness (i.e., high frequency signals) of a sound. It is defined as the frequency-weighted sum of the power spectrum normalized by its unweighted sum [38]. Using spectral centroid we can capture how the centroid of two signals differ.\n(4), loudness (1), zero-crossings (1), spectral (13), and auditory-spectrum bands (26). In Table 3, we list the acoustic features that passed the significance test for male (40 features) and female (34 features) speakers. We also report effect sizes, d, which are computed using Cohen\u2019s d, as in Equation 1. Effect size refers to the how big or small between the means of two groups. Here two groups refers to the neutral and empathy segments. The number of samples for this analysis is 302 conversations, n = 302, sample size.\nCohen\u2032s d = m1\u2212m2\n\u03c3p ; \u03c3p =\n\u221a \u03c321 + \u03c3 2 2\n2 (1)\nwhere m1 and m2 are the means of two samples, and \u03c31 and \u03c32 are the\nstandard deviations of the two samples.\nFrom our analysis we observe that the pitch patterns are higher in nonempathic segments. The spectral features such as centroid and flux are more stable and smooth when the agent is empathic compared to abrupt changes in non-empatic segments. Spectral patterns captures the perceptual impression of sharpness of sounds. The vocal pattern of loudness is higher in non-empathic situations while it is low when the agent is empathic. We also observed that the auditory-spectrum bands of non-empathic segments were comparatively higher. Our analysis on the relevance of pitch and loudness for empathy signal realization is consistent with the findings of [40] and [25]. There are no significant differences in the relevance of features in Table 3 for male and female speakers."}, {"heading": "5.2. Lexical Feature Analysis", "text": "Several categories of personnel who interact with customers or patients, including call center agents and physicians, are trained to improve their communication skills and develop empathy in their interactions through careful choice of words [42, 43]. For example, they are recommended to use phrases such as \"I understand\" when they are listening to the customer who is explaining their problem, or to use \"I would\". For example, after hearing a customer\u2019s story,\nagent may respond by saying \"I would be upset as well if I were in a similar situation\", before proceeding to propose possible solutions or provide advice.\nIn our call center corpus, we analyzed the lexical realization occuring in empathic and neutral segments by comparing the different word frequencies and POS distributions of unigrams, bigrams and trigrams respectively. The\ngoal was to extract the most relevant word n-grams from empathic as well as neutral segments.\nWe tested the statistical significance over the observed differences with a two-tailed two-sample t-test and a p-value of 0.01 with the same number of conversations (n = 302) we used for the analysis of acoustic features.\nThe comparison between neutral and empathy word trigrams showed that agents\u2019 phrases such as vediamo un po\u2019 (let\u2019s see), vediamo un attimo (let\u2019s see a bit), vediamo subito allora (let\u2019s see now, then) are statistically significant lexical cues for the agents while interacting and manifesting their empathy. It is worth noticing that the Italian verb vedere (see) is more frequently used in the first person plural; the same holds true for other frequent verbs such as facciamo (let\u2019s do) and controlliamo (let\u2019s check). Those lexical choices are usual when the speaker cares about the problem of the other person. Similarly, significantly different rankings in the empathic distribution affect unigrams, bigrams and trigrams such as non si preoccupi (do not worry), allora vediamo (so, let\u2019s see) that are often used in Italian to grab the floor of the conversation and reassure the other person. For the above lexical features p-value was < 0.01 and the range of effect sizes, d, was [0.7 to 3.8], and t-statistic was [\u22125.5 to \u2212 3.0] as shown in Table 4.\nRegarding the POS distributions, the Adverbs that occur frequently in the empathy distribution, such as assolutamente (absolutely) and perfettamente (perfectly), may have a kind of evocative potential for showing understanding of the other person\u2019s point of view, in particular when they are uttered with a tone of voice appropriate to the context."}, {"heading": "6. Automatic Classification of Empathy", "text": "In this Section, we describe the training of an automatic classification system for the recognition of empathy from spoken conversations. We report experimental details of the feature extraction, fusion and classification task and discuss the results."}, {"heading": "6.1. Classification Task and Data-set", "text": "In the automatic classification experiment, our goal was to investigate the segment level operator\u2019s empathic manifestations. We have selected a subset of the corpus, that includes a total of 526 conversations annotated with automatic speech transcriptions as well as as Neutral, Empathy segment labels. This subset of the corpus allows us to perform a complete computational model training and evaluation in noisy and clean input signal conditions. We partitioned the data-set into train, development and test with 70%, 15% and 15% partitions and no-speaker overlap amongst them. In order to train and evaluate the system we extracted neutral-empathic segment-pair from these conversations, which has been obtained from manual annotation.\nDuration-based descriptive statistics of these segment pairs are provided in Table 5 along with averages and standard deviations on the natural distribution of the data. The segment length of neutral is comparatively longer than the empathic segment as we see in the Table, since it spans from the start of the conversation until the onset of the first empathic event. The total net duration of these segment-pairs is \u223c35 hours. It is quite usual that in real-world conversations, the distribution of neutral segments is significantly higher than the manifested emotions as can also be seen in [44]."}, {"heading": "6.2. Classification System", "text": "In Figure 3, we present a computational architecture of the automatic classification system, which takes the agent\u2019s speech channel as input, then pass it to the automatic speech vs non-speech segmenter. After that it generates a binary decision for each speech segment of the agent\u2019s behavior in terms of neutral vs empathy. In order to evaluate the relative impact of lexical features we considered the case of noisy transcriptions (left branch in Figure 3) provided by an automatic speech recognizer (ASR). We extracted, combined, and selected acoustic features directly from the speech signal and generated the classifier\u2019s training set. We implemented both feature fusion and decision fusion algorithms (bottom part of Figure 3) to investigate the performance of different classifier configurations. This architectural design can be used in real time application, which combines all automatic processes."}, {"heading": "6.3. Speech vs Non-Speech Segmenter", "text": "An HMM-based speech vs non-speech segmenter has been trained using a set of 150 conversations, containing approximately 100 hours of spoken content and used Kaldi [45] for the training and decoding processes. Training data has been prepared using force-aligned transcriptions. Mel Frequency Cepstral Coefficient (MFCC) and their derivatives has been used as features. Number of gaussian and beam width have been optimized using a development set of 50 conversations. Final model has been tested using a test of 50 conversations. The F-measure of the system was 66.0% on the test set."}, {"heading": "6.4. Undersampling and Oversampling", "text": "The statistics in Table 5 manifest the data imbalance problem for the two classes, Empathy and Neutral. Once the manual segments are processed through the automatic segmenter, the ratio of Empathy/Neutral labelled segments is 6% vs 94%. A common approach to cope with that is via oversampling or undersampling in the data or feature space. We have under-sampled the instance of the majority class (i.e., neutral) at the data level and oversampled the minority class (i.e., empathy) at the feature level. In the literature it is reported that the combination of oversampling and undersampling often leads to better performance [46]. For undersampling, we defined a set of bins with different segment lengths, and then randomly selected K segments from each bin. We used K = 1 for this study. The number of bin and size of K has been optimized empirically on the development set and by investigating the descriptive statistics such as percentiles, mean and standard deviation. The undersampling stage generated a 18% vs 82% ratio of Empathy vs Neutral segments up from the\ninitial 6% vs 94%. For oversampling we used Synthetic Minority Oversampling Technique (SMOTE) [46] and its open-source implementation in weka [47]. In SMOTE, the upsampled examples (i.e., empathy) are generated based on the K nearest neighbors of the minority class. Nearest neighbors have been chosen randomly based on the percentage of target oversampling. More details of this approach can be found in [46]. The oversampling was tuned on the development set and we achieved a further improvement on the imbalance problem. Before oversampling the class distribution was 18% vs 82%, and following oversampling that it became 30% vs 70%. Using this approach classifier learns the variations of segments in different lengths and also reducing the effect of imbalance class distribution."}, {"heading": "6.5. Feature Extraction", "text": "In this section we report the algorithms used in the extraction of the acoustic, linguistic and psycholinguistic features in the empathy recognition task. We discuss differences and similarities of the features used in our experiments to those used in other emotion and personality recognition tasks [48, 49]."}, {"heading": "6.5.1. Acoustic Features", "text": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49]. We followed a similar approach and we extracted a very large number of low-level features and their statistical functionals using the openSMILE tool [39]. The low-level acoustic features include the feature set of the computational paralinguistic challenge\u2019s (COMPARE-2013) feature set [51], Geneva minimalistic acoustic feature set [52] and few more formant features. For the sake of the replicability we made the configuration files of the feature set publicly available2. We extracted low-level acoustic features at approximately 100 frames per second. Regarding voice-quality features the frame size was 60 milliseconds\n2https://github.com/firojalam/openSMILE-configuration\nwith a gaussian window function and \u03c3 = 0.4. Regarding other low-level features the frame size was 25 milliseconds with a hamming window function. The details of the low-level features and their statistical functional are provided in Table 6. After feature extraction, the size of the resulted feature set is 6861.\nTable 6: Extracted low-level acoustic features\nLow-level acoustic features\nVoice Quality Probability of voicing, jitter-local, jitter-DDP, shimmer-local, log harmonics-to-noise ratio (HNR) Cepstral MFCC 1-14 Spectral Auditory spectrum (RASTA-style) bands 0-25 (0-8kHz), Spectral energy 250-650Hz, 1-4kHz, Spectral roll-off points (0.25, 0.50, 0.75, 0.90), Spectral slope 0-500Hz and 500-1500Hz, Spectral flux, centroid, entropy, variance, skewness, kurtosis, Spectral slope, Difference of spectral flux between two consecutive frames Psychoacoustic spectral sharpness, harmonicity Alpha Ratio - ratio of the total energy from 50-1000Hz, and 1-5 Hz Relative energy of formant 1, 2 and 3 First and second harmonic difference First and third harmonic difference Hammarberg index Prosody F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]"}, {"heading": "6.5.2. Lexical Features", "text": "We extracted lexical features from automatic transcriptions. The automatic transcriptions were generated using a large vocabulary ASR system [53]. We designed HMM-based ASR system using a subset of 1894 conversations containing approximately 100 hours of spoken content and a lexicon of size \u223c15K\nTable 7: Statistical functionals\nLow-level acoustic features\nVoice Quality\nStatistical functionals\nPercentile 1%, 99% and percentile range 1%-99% Quartile (1-3) and inter-quartile (1-2, 2-3, 3-1) ranges Relative position of max, min, mean and range Arithmatic mean, root quadratic mean Mean of non-zero values (nnz) Contour centroid, flatness Std. deviation, skewness, kurtosis Uplevel time 25, 50, 75, 90, Rise time, fall time, left curvature time, duration Mean, max, min and Std. deviation of segment length Linear prediction coefficients (lpc0-5), lpc-gain Linear regression coefficients (1-2) and error Quadratic regression coefficients (1-3) and error\nwords. From the conversations we extracted Mel Frequency Cepstral Coefficient (MFCC) features and then spliced by taking 3 frames from each side of the current frame. It was followed by Linear Discriminant Analysis (LDA) and Maximum Likelihood Linear Transform (MLLT) feature-space transformations to reduce the feature space. Then, we trained acoustic model using speaker adaptive training (SAT). In order to achieve the best accuracy we also used Maximum Mutual Information (MMI). The Word Error Rate (WER) of the ASR system was 31.78% on the test set and 20.87% on the training set, using a trigram language model of perplexity 87.69. For the training and decoding process, we used Kaldi [45].\nThe training set of the ASR system overlapped with the task classification test corpus that was used in the classification experiments (Section 6.7). However, the training error rate of 20.87% was still realistic and useful for the classification task.\nWe mapped the speech transcriptions into lexical features in the step preceding the training of the classifier. The transcriptions of each segment were converted to bag-of-words vectors weighted with logarithmic term frequencies (tf) multiplied with inverse document frequencies (idf), presented in equation\n2.\ntf \u00d7 idf = log(1 + fij)\u00d7 log ( number of conversations\nnumber of conversations that include word i ) (2)\nwhere fij is the frequency for word i in conversation j. In order to take advantage of the contextual benefits of n-grams, we extracted trigram features. Because this resulted in an unreasonably large dictionary and we filtered out lower frequency features by preserving 10K most frequent n-grams."}, {"heading": "6.5.3. Psycholinguistic Features", "text": "Similar to the lexical features we extracted the so-called psycholinguistic features from automatic transcriptions. Over the past few decades, Pennebaker et al. have designed psycholinguistic word categories using high frequency words and developed the Linguistic Inquiry Word Count (LIWC) [54]. These word categories are mostly used to study gender, age, personality, and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references therein). The types of LIWC features include the following:\n1. General features such as word count, average number of words per sen-\ntence, percentage of words found in the dictionary and percentage of words longer than six letters and numerals.\n2. Linguistic features include pronouns and articles.\n3. Psychological features include affect, cognition and biological phenomena.\n4. Features about personal concern include work and home.\n5. Paralinguistic features include accents, fillers and disfluencies.\n6. Punctuation categories include periods and commas.\nWe used the dictionary that is available within LIWC for Italian [55]. The Italian dictionary contains 85 word categories. LIWC extracted 5 general descriptors and 12 punctuation categories constituting a total of 102 features. The LIWC feature processing differs according to types of features. Some features are counts and others are relative frequencies (see [56])."}, {"heading": "6.6. Feature Fusion and Selection", "text": "We applied feature selection to each individual feature set (Figure 3). We also merged acoustic and lexical features into a single vector to represent each instance in a high-dimensional feature space. Let A = {a1, a2, ..., am} and L = {l1, l2, ..., ln} denote the acoustic and lexical feature vectors respectively. The feature-combined vector was Z = {a1, a2, ..., am, l1, l2, ..., ln} with Z \u2208 Rm+n. Given the high-dimension of the feature vector and the curse of dimensionality, we applied feature selection on the Z space to achieve an optimal feature dimension of size k, k < (m+ n). Another objective of the feature selection process was to find the best compromise between the dimension of the input and the performance of a target classifier.\nFor feature selection we used the Relief [57] feature selection technique. In [58], we comparatively evaluated the Relief method against other algorithms and it outperformed them in classification performance and computational cost. We ranked the feature set according to Relief scores and generated learning curves by incrementally adding batches of ranked features. We then selected the optimal set of features by stopping when performance saturated or started decreasing [58].\nThe goal of Relief is to estimate weights to find relevant attributes with the ability to differentiate between instances of different classes under the assumption that nearest instances of the same class have the same feature values and different class have different values. Relief estimates weight of a feature, F , using Equation 3.\nW [F ] = P (x|nearest miss)\u2212 P (x|nearest hit) (3)\nwhere x is a value of F , nearest miss and nearest hit are the nearest instances\nof the same and a different class, respectively.\nSince some feature selection algorithms do not support numeric feature values such as information gain and suffer from data sparseness such as Relief, we discretized feature values into 10 equal-frequency bins [47] as a pre-processing\nstep of feature selection. The equal-frequency binning approach divides data into k = 10 groups, where each group contains approximately equal number of values. Moreover, the feature selection and discretization approach were performed on development set (see Section 6.7) in order to avoid biases in classification experiments. The equal-frequency binning approach and the size of the bin, k = 10, were empirically found optimal in other paralinguistic classification task [58]. Furthermore, we did not apply the selection algorithm to the psycholinguistic features due to the limited size of the feature set.\nAfter selecting the features, we analyzed the top-ranked features per category. The top ranked acoustic features included spectral, mfcc, probability of voicing, and pitch. The spectral feature type included spectral variance, flux, auditory-spectrum with bands 1K \u2212 4K Hz, roll-off points. The highly relevant LIWC feature category included verb, article, negations, and, social and cognitive processes. The lexical features included allora vediamo se (so let\u2019s see if), assolutamente (absolutely), sicuramente (certainly) tranquillamente (nicely) and non si preoccupi (do not worry). The statistical analysis of acoustic and lexical features for the neutral-empathy segment annotation ( see Section 5.2 and 5.1), corroborate these latter findings of the top-ranked features designed for the classification step."}, {"heading": "6.7. Classification and Evaluation", "text": "We designed our classification models using an open-source implementation of SMO [59] by the Weka machine learning toolkit [47], with its linear kernel for lexical and acoustic features, and its gaussian kernel for the psycholinguistic features. SMO is an optimization technique for solving quadratic optimization problems, which arise during the training of Support Vector Machines (SVM). We chose SMO for its high generalization performance. In addition, we chose the linear kernel in order to alleviate the problem of higher dimensions of acoustic, lexical and combination of acoustic+ lexical features. We used gaussian kernel with psycholinguistic features as it performed better with small-sized feature set. We optimized the penalty parameter C of the error term by tuning it in\nthe range C \u2208 [10\u22125, ..., 10] and the gaussian kernel parameter G in the same range as well, using the development set. To obtain the results on the test set we combined the training and development set and trained the models using the optimized parameters. We have not attempted to use deep neural network, due to the smaller sized data-set.\nRegarding the classifiers trained on different feature sets, we combined de-\ncisions from each classifier by applying majority voting, as in Equation 4.\nH(s) = cj\u0302 ; where j\u0302 = argmaxj T\u2211 i=1 hji (s) (4)\nwhere H(s) is the combined classification decision for an input instance s; hji (s) is the output of the classifier hi for the class label cj ; i = 1...T is the number of classifiers; j = 1...C is the number of classes.\nIn the experiments we used SMO across different classification systems and\nevaluated different sets of features such as lexical vs acoustic.\nThe evaluation procedure requires the alignment of the manual segment and labels with the output of the automatic segmentation and classification system described in the previous section. In Figure 4, we show a sample alignment of the reference (manual) and automatically generated segments and their labels. The reference segmentation spans from t = 0 to t = te and labels the Neutral (N) segment spanning from t = 0 to t = ti and the Empathy (E) segment from t = ti to t = te. Automatic segments inherit the reference label that falls inside its boundaries (e.g., the segment spanning the interval [0, t1] or [t3, t4]). For the evaluation purpose, automatic segments that span across the onset, t = ti, and end, t = te, (e.g., the segment spanning the interval [t2, t3]) are split in two segments with two distinct reference labels. For instance the segment spanning [t2, t3] will be evaluated with the segment [t2, ti] (reference label (N)) and the segment [ti, t3] (reference label (E)). The alignment process will generate the correct label statistics for all segments as shown on the last row of Figure 4.\nWe have measured the performance of the system using the Un-weighted Average (UA), which has been widely used in the evaluation of paralinguistic tasks [60]. We have extended such measure to account for the segmentation\nerrors as evaluated in similar cases by NIST in diarization tasks [61, 62]. UA is the average recall of positive and negative classes and is computed as UA = 1 2 ( tp tp+fn + tn tn+fp ) , where tp, tn, fp, fn are the number of true positives, true negatives, false positives and false negatives, respectively. We computed tp, tn, fp and fn from a weighted confusion matrix, as shown in Equation 5, where the weight for each instance is the corresponding segment length:\nC(f) = { ci,j(f) =\n\u2211 s\u2208ST [((y = i) \u2227 (f(s) = j))\u00d7 length(s)]\n} (5)\nIn Equation 5, C(f) is the 2\u00d72 confusion matrix of the classifier f , s are the automatic segments in the test set ST , including the segments with boundary ti and te (see Figure 4), length(s) is the duration of s, y is the reference label of s, f(s) is the automatic label for s. The indices i and j represent the reference and automatic class label of the confusion matrix."}, {"heading": "6.8. Results and Discussion", "text": "In Table 8, we report the performances of the automatic classification system trained on different feature types: lexical (automatic transcriptions), acoustic and psycholinguistic. We report test set results for feature combination-based system as well as and classifier combination. In the latter system we applied majority voting. In order to compute the baseline we have randomly selected the\nclass labels based on the prior class distribution. For the statistical significance, we have computed McNemar\u2019s significant test over the test set [63].\nFor single feature-type systems, acoustic-based models provided the best performance compared to lexical and psycholinguistic alone. The results of acoustic-based system are significantly better than random baseline with p < 2.2E \u2212 16. The acoustic-based system provides a useful and low-computation classification model, when no automatic transcriptions are available. LIWC\u2019s system performance improve over the lexical-only system with very few lexical features (89). In addition, all system\u2019s UAs are higher and statistically significant compared to the baseline results with p < 2.2E \u2212 16, chi-square value 228.6 to 1866.9 and d 0.4 to 1.1. For McNemar test, the test statistic is usually computed by a paired version of Chi-square and effect size, d, is computed using Cramer\u2019s \u03c6. The value of \u03c6, 0.1, 0.3 and 0.5 represents small, medium and large effect respectively.\nIn terms of feature and system combination, we obtained the best results with majority voting. The statistical significance test showed that the results of the majority voting are statistically significant compared to any other system\u2019s results with p <= 0.0004, chi-square value 12.2 to 1446.5 and d 0.1 to 1.0.\nCompared to the baseline, the best model for automatic classification provides a relative improvement over the baseline of 35.7%. Linear combination of lexical with acoustic features has not improved performance, despite its success in other paralinguistic tasks [49], linear combination in the feature space has not improved performance even when combined with feature selection.\nIn order to assess the impact of the automated transcriptions, in a different study, we compared the performance between automatic and manual transcriptions for a automatic classification of emotional state. The results show that performance differences are very low, only 1.2% drop with automated transcriptions [64]. Therefore, we found that the use of automatic transcriptions are reasonable for the experiment given that manual transcriptions are not available in call cases."}, {"heading": "7. Conclusion", "text": "Empathy refers to an emotional state triggered by a shared emotional experience. In this paper, we have addressed the problem of observing and annotating empathy manifestations in real-life spoken conversations. The annotation process describes the scene through the Situation-Attention-Appraisal-Response model. We have operationalized the definition of empathy and designed an annotation process of the human-human dialogues in call centers. We have designed and evaluated a system that automatically segments and classifies empathic events from spoken conversations. We have investigated the effectiveness of acoustic, lexical, and psycholinguistic features and their combinations. The performance of our best classification system is very promising and significantly above the baseline. Such annotation model and classifier performances may lead to extensions in other situations and applications in human-machine interaction and automatic behaviour analysis. In future work, we foresee to extend this work with a comparative study of the concurrent signals from agent and operator channels and understand the effect of interlocutor\u2019s behaviors on each other."}, {"heading": "Acknowledgment", "text": "The research leading to these results has received funding from the European Union - Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 610916- SENSEI."}], "references": [{"title": "Computational paralinguistics: emotion", "author": ["B. Schuller", "A. Batliner"], "venue": "affect and personality in speech and language processing, John Wiley & Sons", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Empathy and prosocial behavior", "author": ["M.L. Hoffman"], "venue": "Handbook of Emotions 3 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Lectures on the Experimental Psychology of the Thoughtprocesses", "author": ["E.B. Titchener"], "venue": "Macmillan", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1909}, {"title": "Mirror neurons and the simulation theory of mindreading", "author": ["V. Gallese", "A. Goldman"], "venue": "Trends in Cognitive Sciences 2 (12) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "D", "author": ["S. Baron-Cohen", "M. Lombardo", "H. Tager-Flusberg"], "venue": "Cohen (Eds.), Understanding Other Minds: Perspectives from Developmental Social Neuroscience, 3rd Edition, Oxford University Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The emerging field of emotion regulation: An integrative review", "author": ["J.J. Gross"], "venue": "Review of General Psychology 2 (3) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "The neuroscience of empathy: progress", "author": ["J. Zaki", "K.N. Ochsner"], "venue": "pitfalls and promise, Nature neuroscience 15 (5) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "S", "author": ["R.A. Calvo"], "venue": "D\u2019Mello, Affect detection: An interdisciplinary review of models, methods, and their applications, IEEE Transactions on affective computing 1 (1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The many faces of empathy: Parsing empathic phenomena through a proximate", "author": ["S.D. Preston", "A.J. Hofelich"], "venue": "dynamic-systems view of representing the other in the self, Emotion Review 4 (1) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "The Social Neuroscience of Empathy", "author": ["C.D. Batson"], "venue": "MIT press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Human empathy through the lens of social neuroscience", "author": ["J. Decety", "C. Lamm"], "venue": "The Scientific World Journal 6 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Empathy: a motivated account", "author": ["J. Zaki"], "venue": "Psychological Bulletin", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "W", "author": ["P.R. Gesn"], "venue": "Ickes, The development of meaning contexts for empathic accuracy: Channel and sequence effects., Journal of Personality and Social Psychology 77 (4) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge", "author": ["B. Schuller", "A. Batliner", "S. Steidl", "D. Seppi"], "venue": "Speech Communication 53 (9) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Survey on speech emotion recognition: Features", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "classification schemes, and databases, Pattern Recognition 44 (3) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Modelli prosodici emotivi per la sintesi dell\u2019italiano", "author": ["F. Tesser", "P. Cosi", "C. Drioli", "G. Tisato", "P. ISTC-CNR"], "venue": "in: Proc. of Associazione Italiana di Scienze della Voce (AISV)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Prosodic analysis of a multistyle corpus in the perspective of emotional speech synthesis", "author": ["E. Zovato", "S. Sandri", "S. Quazza", "L. Badino"], "venue": "in: Proc. of Interspeech, Vol. 2", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The description of naturally occurring emotional speech", "author": ["E. Douglas-Cowie", "R. Cowie", "M. Schroeder"], "venue": "in: Proc. of 15th Int. Congr. of Phonetic Sciences", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic classification of emotions via global and local prosodic features on a multilingual emotional database", "author": ["A. Origlia", "V. Galat\u00e0", "B. Ludusan"], "venue": "in: Proc. of Speech Prosody", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "The SE- MAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent", "author": ["G. McKeown", "M. Valstar", "R. Cowie", "M. Pantic", "M. Schroder"], "venue": "IEEE Trans. on Affective Computing 3 (1) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "in: Proc. of 10th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities", "author": ["A. Metallinou", "S. Narayanan"], "venue": "in: Proc. of 10th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Analyzing empathetic interactions based on the probabilistic modeling of the co-occurrence patterns of facial expressions in group meetings", "author": ["S. Kumano", "K. Otsuka", "D. Mikami", "J. Yamato"], "venue": "in: Proc. of IEEE Int. Conf. on Automatic Face & Gesture Recognition and Workshops ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Analyzing the language of therapist empathy in motivational interview based psychotherapy", "author": ["B. Xiao", "P. Georgiou", "S. Narayanan"], "venue": "in: Proc. of Asia Pacific Signal & Inf. Process. Assoc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Bo", "author": ["B. Xiao"], "venue": "S. Daniel, I. Maarten Van, A. Zac E., G. David C., N. Panayiotis G., S. S., Modeling therapist empathy through prosody in drug addiction counseling, in: Proc. of Interspeech", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A systems approach to appraisal mechanisms in emotion", "author": ["D. Sander", "D. Grandjean", "K.R. Scherer"], "venue": "Neural Networks 18 (4) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion regulation: Conceptual foundations", "author": ["J.J. Gross", "R.A. Thompson"], "venue": "Handbook of Emotion Regulation 3 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Emotion unfolding and affective scenes: A case study in spoken conversations", "author": ["M. Danieli", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Emotion Representations and Modelling for Companion Systems (ERM4CT) 2015\u201e ICMI", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Using context to improve emotion detection in spoken dialog systems", "author": ["J. Liscombe", "G. Riccardi", "D. Hakkani-Tur"], "venue": "in: Proc. of Interspeech", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic analysis of facial expressions: The state of the art", "author": ["M. Pantic", "L.J.M. Rothkrantz"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (12) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Context-sensitive learning for enhanced audiovisual emotion classification", "author": ["A. Metallinou", "M. Wollmer", "A. Katsamanis", "F. Eyben", "B. Schuller", "S. Narayanan"], "venue": "IEEE Trans. on Affective Computing 3 (2) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Do facial movements express emotions or communicate motives", "author": ["B. Parkinson"], "venue": "Personality and Social Psychology Review 9 (4) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz", "J.G. Taylor"], "venue": "Signal Processing Magazine, IEEE 18 (1) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion unfolding and affective scenes: A case study in spoken conversations", "author": ["M. Danieli", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Emotion Representations and Modelling for Companion Systems (ERM4CT) 2015\u201e ICMI", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Transcribing and annotating spoken language with EXMAR- ALDA", "author": ["T. Schmidt"], "venue": "in: Proc. of LREC 2004 Workshop on XML-based Richly Annotated Corpora", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Assessing agreement on classification tasks: the kappa statistic", "author": ["J. Carletta"], "venue": "Computational linguistics 22 (2) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Emotv1: Annotation of real-life emotions for the specification of multimodal affective interfaces", "author": ["S. Abrilian", "L. Devillers", "S. Buisine", "J.-C. Martin"], "venue": "in: HCI International", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "An introduction to audio content analysis: Applications in signal processing and music informatics", "author": ["A. Lerch"], "venue": "John Wiley & Sons", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent developments in opensmile", "author": ["F. Eyben", "F. Weninger", "F. Gross", "B. Schuller"], "venue": "the munich open-source multimedia feature extractor, in: Proc. of the 21st ACM international conference on Multimedia (ACMM), ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Prosody and empathic communication in psychotherapy interaction", "author": ["E. Weiste", "A. Per\u00e4kyl\u00e4"], "venue": "Psychotherapy Research 24 (6) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Let me see if i have this right", "author": ["J.L. Coulehan", "F.W. Platt", "B. Egener", "R. Frankel", "C.-T. Lin", "B. Lown", "W.H. Salazar"], "venue": ". . : words that help build empathy, Annals of Internal Medicine 135 (3) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Keep Them Calling: Superior Service on the Telephone", "author": ["S. Barrett"], "venue": "American Media Inc", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Segmenting into adequate units for automatic recognition of emotion-related episodes: a speech-based approach", "author": ["A. Batliner", "S. Steidl", "D. Seppi", "B. Schuller"], "venue": "Advances in Human-Computer Interaction 2010 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "P", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian"], "venue": "Schwarz, et al., The kaldi speech recognition toolkit, in: Proc. of Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Smote: synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of artificial intelligence research ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2002}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Paralinguistics in speech and language state-of-the-art and the challenge", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "F. Burkhardt", "L. Devillers", "C. M\u00fcller", "S. Narayanan"], "venue": "Computer Speech & Language 27 (1) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Fusion of acoustic", "author": ["F. Alam", "G. Riccardi"], "venue": "linguistic and psycholinguistic features for speaker personality traits recognition, in: Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic emotion recognition: A benchmark comparison of performances", "author": ["B. Schuller", "B. Vlasenko", "F. Eyben", "G. Rigoll", "A. Wendemuth"], "venue": "in: Proc. of Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "E", "author": ["B. Schuller", "S. Steidl", "A. Batliner", "A. Vinciarelli", "K. Scherer", "F. Ringeval", "M. Chetouani", "F. Weninger", "F. Eyben"], "venue": "Marchi, et al., The interspeech 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism, in: Proc. of Interspeech", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised recognition and clustering of speech overlaps in spoken conversations", "author": ["S.A. Chowdhury", "G. Riccardi", "F. Alam"], "venue": "in: Proc. of Workshop on Speech, Language and Audio in Multimedia - SLAM2014", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic inquiry and word count", "author": ["J.W. Pennebaker", "M.E. Francis", "R.J. Booth"], "venue": "Liwc", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2001}, {"title": "A", "author": ["F. Alparone", "S. Caso", "A. Agosti"], "venue": "Rellini, The italian liwc2001 dictionary., Tech. rep., LIWC.net, Austin, TX ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "The psychological meaning of words: Liwc and computerized text analysis methods", "author": ["Y.R. Tausczik", "J.W. Pennebaker"], "venue": "Journal of Language and Social Psychology 29 (1) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating attributes: analysis and extensions of relief", "author": ["I. Kononenko"], "venue": "in: Proc. of Machine Learning: European Conference on Machine Learning (ECML), Springer", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1994}, {"title": "Comparative study of speaker personality traits recognition in conversational and broadcast news speech", "author": ["F. Alam", "G. Riccardi"], "venue": "in: Proc. of Interspeech, ISCA", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast Training of Support Vector Machines using Sequential Minimal Optimization", "author": ["J. Platt"], "venue": "MIT Press", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "A", "author": ["B. Schuller", "S. Steidl"], "venue": "Batliner, The interspeech 2009 emotion challenge., in: Proc. of Interspeech", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio segmentation-byclassification approach based on factor analysis in broadcast news domain", "author": ["D. Cast\u00e1n", "A. Ortega", "A. Miguel", "E. Lleida"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing 2014 (1) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Note on the sampling error of the difference between correlated proportions or percentages", "author": ["Q. McNemar"], "venue": "Psychometrika 12 (2) ", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1947}, {"title": "Can we detect speakers\u2019 empathy?: A real-life case study", "author": ["A. Firoj", "D. Morena", "R. Giuseppe"], "venue": "in: 7th IEEE International Conference on Cognitive InfoCommunications", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "While early works on affective computing mostly focused on the recognition of basic emotions ([1]), in this work we investigate empathy.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "In this paper, we refer to the psychological definition of empathy by Hoffman, who defines it as \"an emotional state triggered by another\u2019s emotional state or situation, in which one feels what the other feels or would normally be expected to feel in his situation\" ([2]).", "startOffset": 267, "endOffset": 270}, {"referenceID": 2, "context": "The word empathy was coined by Titchener in 1909 [3] as a translation of the German term Einf\u00fchlung.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "These include action understanding, attribution of intentions (mind-reading), and recognition of emotions and sensations [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "In everyday life, empathy supports important aspects of inter-personal communication to an extent where some psychic diseases that affect the relationship with other persons, such as autism and Asperger syndrome, are explained in terms of impairment of empathic ability [5].", "startOffset": 270, "endOffset": 273}, {"referenceID": 5, "context": "To this end we adopt the modal model of emotions by Gross [6] as a promising framework for defining an operational concept of empathy.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "Psychology and Neuroscience Research Over the past decades there have been significant efforts in investigating empathy in the fields of psychology and neuroscience [7] [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "Psychology and Neuroscience Research Over the past decades there have been significant efforts in investigating empathy in the fields of psychology and neuroscience [7] [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "For example, the work in [9] accounts for different empathic phenomena occurring in the literature on empathy, and [10] examines eight distinct phenomena commonly labeled as empathy including emotional contagion, sympathy, projection, and", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "For example, the work in [9] accounts for different empathic phenomena occurring in the literature on empathy, and [10] examines eight distinct phenomena commonly labeled as empathy including emotional contagion, sympathy, projection, and", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "Decety and Lamm [11] observe that some of the different definitions of empathy may share the underlying concept of \"[.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "The work in [12] takes a different perspective on empathy manifestations by focusing on the role of motivations for explaining a particular feature of the empathic experience.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "Both verbal and non-verbal levels of spoken communication [13] have been considered since both are suggested to embody the expressive potential of language.", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Major focus has been devoted to the paralinguistic features of emotional speech, on the basis of the experimental evidence that emotional information is mostly conveyed by those levels (see [14] for a state-of-the-art review).", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "The authors in [15] report a significant disparity among those corpora in terms of complexity of the annotated emotions, explicitness of the emotion definitions, and identification of the annotation units.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Most emotional corpora have been designed to perform specific tasks such as emotion recognition or emotional speech synthesis [16, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "Most emotional corpora have been designed to perform specific tasks such as emotion recognition or emotional speech synthesis [16, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 17, "context": "The HUMAINE project [18] and the emotion multilingual collection in [19] base their annotation schemes on sets of discrete emotional lexical items or basic emotions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "The HUMAINE project [18] and the emotion multilingual collection in [19] base their annotation schemes on sets of discrete emotional lexical items or basic emotions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "The models that foster this approach, such as those discussed in [20] and [21], require annotators to continuously assign values to emotional dimensions and sets of emotional descriptors.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The models that foster this approach, such as those discussed in [20] and [21], require annotators to continuously assign values to emotional dimensions and sets of emotional descriptors.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "Metallinou and Narayanan [22] emphasize that continuous annotation has several benefits, as well as some open challenges.", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "In [22], authors report a high number of factors that may affect inter-annotator agreement, such as person-specific annotation delays and confidence in understanding emotional attributes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "[23] studied four-party meeting conversations to estimate and classify empathy, antipathy and unconcerned emotional interactions utilizing facial expression, gaze and speech-silence features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "In [24] and [25], Xiao et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [24] and [25], Xiao et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "Gross [6] has provided evidence that concepts such as emergence \u2014 derivation from the expectations of relationships \u2014 and unfolding \u2014 sequences that persist over time \u2014 may help in explaining emotional events.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "It has been shown that temporal unfolding of emotions can be conceptualized and experimentally tested [26].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Figure 1: The modal model of emotion [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "by Gross [6, 27] emphasizes the attentional and appraisal acts underlying the emotion-arousing process.", "startOffset": 9, "endOffset": 16}, {"referenceID": 26, "context": "by Gross [6, 27] emphasizes the attentional and appraisal acts underlying the emotion-arousing process.", "startOffset": 9, "endOffset": 16}, {"referenceID": 27, "context": "The modal model of emotions provides a useful framework for describing the contextual dynamics of emotions within an affective scene([28]), since it decomposes the emotional process in terms of situation selection, attentional deployment, and situational modification.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 25, "endOffset": 29}, {"referenceID": 29, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "The medium may be speech [29], image [30] or multimodal [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "For a speech stimulus in a conversation the context is represented by the preceding dialog turns [29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "Last but not least there might be a non-deterministic relation between the emotion-expression and motive-communication ([32]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 32, "context": "stimuli are sentences to be read and actors enacting affective scenes [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "The limitations of determining the temporal boundaries of the annotation units have motivated researchers to investigate the process of continuous annotation [22, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "The limitations of determining the temporal boundaries of the annotation units have motivated researchers to investigate the process of continuous annotation [22, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 33, "context": "This annotation protocol has been applied to annotate sequences of basic emotions occurring in affective scenes ([34]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 34, "context": "For the annotation task, the annotators used the EXMARaLDA Partitur Editor [35].", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "To measure the reliability of the annotations we calculated inter-annotator agreement by using the kappa statistics [36].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 81, "endOffset": 89}, {"referenceID": 28, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 81, "endOffset": 89}, {"referenceID": 21, "context": "50) maybe due to a variety of factors, including short audio clips or utterance ([37, 29]), multi-label annotation tasks, and annotator agreement when the annotation task is based on continuous and discrete label annotations [22].", "startOffset": 225, "endOffset": 229}, {"referenceID": 38, "context": "97, and hammingwindowing, using openSMILE [39].", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "It is defined as the frequency-weighted sum of the power spectrum normalized by its unweighted sum [38].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "Our analysis on the relevance of pitch and loudness for empathy signal realization is consistent with the findings of [40] and [25].", "startOffset": 118, "endOffset": 122}, {"referenceID": 24, "context": "Our analysis on the relevance of pitch and loudness for empathy signal realization is consistent with the findings of [40] and [25].", "startOffset": 127, "endOffset": 131}, {"referenceID": 40, "context": "Several categories of personnel who interact with customers or patients, including call center agents and physicians, are trained to improve their communication skills and develop empathy in their interactions through careful choice of words [42, 43].", "startOffset": 242, "endOffset": 250}, {"referenceID": 41, "context": "Several categories of personnel who interact with customers or patients, including call center agents and physicians, are trained to improve their communication skills and develop empathy in their interactions through careful choice of words [42, 43].", "startOffset": 242, "endOffset": 250}, {"referenceID": 42, "context": "It is quite usual that in real-world conversations, the distribution of neutral segments is significantly higher than the manifested emotions as can also be seen in [44].", "startOffset": 165, "endOffset": 169}, {"referenceID": 43, "context": "An HMM-based speech vs non-speech segmenter has been trained using a set of 150 conversations, containing approximately 100 hours of spoken content and used Kaldi [45] for the training and decoding processes.", "startOffset": 163, "endOffset": 167}, {"referenceID": 44, "context": "In the literature it is reported that the combination of oversampling and undersampling often leads to better performance [46].", "startOffset": 122, "endOffset": 126}, {"referenceID": 44, "context": "For oversampling we used Synthetic Minority Oversampling Technique (SMOTE) [46] and its open-source implementation in weka [47].", "startOffset": 75, "endOffset": 79}, {"referenceID": 45, "context": "For oversampling we used Synthetic Minority Oversampling Technique (SMOTE) [46] and its open-source implementation in weka [47].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "More details of this approach can be found in [46].", "startOffset": 46, "endOffset": 50}, {"referenceID": 46, "context": "We discuss differences and similarities of the features used in our experiments to those used in other emotion and personality recognition tasks [48, 49].", "startOffset": 145, "endOffset": 153}, {"referenceID": 47, "context": "We discuss differences and similarities of the features used in our experiments to those used in other emotion and personality recognition tasks [48, 49].", "startOffset": 145, "endOffset": 153}, {"referenceID": 48, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 46, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 47, "context": "Recent studies in emotion and personality recognition showed that the paralinguistic properties of speech are well represented by low-level features [50, 48, 49].", "startOffset": 149, "endOffset": 161}, {"referenceID": 38, "context": "We followed a similar approach and we extracted a very large number of low-level features and their statistical functionals using the openSMILE tool [39].", "startOffset": 149, "endOffset": 153}, {"referenceID": 49, "context": "The low-level acoustic features include the feature set of the computational paralinguistic challenge\u2019s (COMPARE-2013) feature set [51], Geneva minimalistic acoustic feature set [52] and few more formant features.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 1, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 2, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 3, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 178, "endOffset": 183}, {"referenceID": 0, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 1, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 2, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 3, "context": "F0 final, F0 envelope, F0final with non-zero frames, Root-mean-square signal frame energy, Sum of RASTA-style auditory spectra, Loudness, Zero crossing rate, Formant frequencies [1-4], bandwidths [1-4]", "startOffset": 196, "endOffset": 201}, {"referenceID": 50, "context": "The automatic transcriptions were generated using a large vocabulary ASR system [53].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "For the training and decoding process, we used Kaldi [45].", "startOffset": 53, "endOffset": 57}, {"referenceID": 51, "context": "have designed psycholinguistic word categories using high frequency words and developed the Linguistic Inquiry Word Count (LIWC) [54].", "startOffset": 129, "endOffset": 133}, {"referenceID": 47, "context": "These word categories are mostly used to study gender, age, personality, and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references therein).", "startOffset": 157, "endOffset": 165}, {"referenceID": 33, "context": "These word categories are mostly used to study gender, age, personality, and health to estimate the correlation between these attributes and word usage (see [49, 34] and the references therein).", "startOffset": 157, "endOffset": 165}, {"referenceID": 52, "context": "We used the dictionary that is available within LIWC for Italian [55].", "startOffset": 65, "endOffset": 69}, {"referenceID": 53, "context": "Some features are counts and others are relative frequencies (see [56]).", "startOffset": 66, "endOffset": 70}, {"referenceID": 54, "context": "For feature selection we used the Relief [57] feature selection technique.", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "In [58], we comparatively evaluated the Relief method against other algorithms and it outperformed them in classification performance and computational cost.", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "We then selected the optimal set of features by stopping when performance saturated or started decreasing [58].", "startOffset": 106, "endOffset": 110}, {"referenceID": 45, "context": "Since some feature selection algorithms do not support numeric feature values such as information gain and suffer from data sparseness such as Relief, we discretized feature values into 10 equal-frequency bins [47] as a pre-processing", "startOffset": 210, "endOffset": 214}, {"referenceID": 55, "context": "The equal-frequency binning approach and the size of the bin, k = 10, were empirically found optimal in other paralinguistic classification task [58].", "startOffset": 145, "endOffset": 149}, {"referenceID": 56, "context": "We designed our classification models using an open-source implementation of SMO [59] by the Weka machine learning toolkit [47], with its linear kernel for lexical and acoustic features, and its gaussian kernel for the psycholinguistic features.", "startOffset": 81, "endOffset": 85}, {"referenceID": 45, "context": "We designed our classification models using an open-source implementation of SMO [59] by the Weka machine learning toolkit [47], with its linear kernel for lexical and acoustic features, and its gaussian kernel for the psycholinguistic features.", "startOffset": 123, "endOffset": 127}, {"referenceID": 57, "context": "We have measured the performance of the system using the Un-weighted Average (UA), which has been widely used in the evaluation of paralinguistic tasks [60].", "startOffset": 152, "endOffset": 156}, {"referenceID": 58, "context": "errors as evaluated in similar cases by NIST in diarization tasks [61, 62].", "startOffset": 66, "endOffset": 74}, {"referenceID": 59, "context": "For the statistical significance, we have computed McNemar\u2019s significant test over the test set [63].", "startOffset": 96, "endOffset": 100}, {"referenceID": 47, "context": "Linear combination of lexical with acoustic features has not improved performance, despite its success in other paralinguistic tasks [49], linear combination in the feature space has not improved performance even when combined with feature selection.", "startOffset": 133, "endOffset": 137}, {"referenceID": 60, "context": "2% drop with automated transcriptions [64].", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. The lack of an operational definition of empathy makes it difficult to measure it. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline.", "creator": "LaTeX with hyperref package"}}}