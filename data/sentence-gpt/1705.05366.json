{"id": "1705.05366", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Maximum Selection and Ranking under Noisy Comparisons", "abstract": "We consider $(\\epsilon,\\delta)$-PAC maximum-selection and ranking for general probabilistic models whose comparisons probabilities satisfy strong stochastic transitivity and stochastic triangle inequality. Modifying the popular knockout tournament, we propose a maximum-selection algorithm that uses $\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta}\\right)$ comparisons, a number tight up to a constant factor. We then derive a general framework that improves the performance of many ranking algorithms, and combine it with merge sort and binary search to obtain a ranking algorithm that uses $\\mathcal{O}\\left(\\frac{n\\log n (\\log \\log n)^3}{\\epsilon^2}\\right)$ comparisons for any $\\delta\\ge\\frac1n$, a number optimal up to a $(\\log \\log n)^3$ factor.\n\n\nTo obtain the algorithm, we first introduce $\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta}\\right)$ comparisons. We then find that for every \\(\\delta\\ge\\frac1n,\\delta\\ge\\frac1n,\\delta\\ge\\frac1n)$, a number of weights and weights can be used for each weight in the order of $\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta\\ge\\frac1n)\\right). The weights and weights are used to obtain the ranking algorithm, and they will be given a linear log.\nTo obtain the algorithm, we first introduce $\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta\\ge\\frac1n)\\right). The weights and weights are used to obtain the ranking algorithm, and they will be given a linear log.\nTo obtain the algorithm, we first introduce $\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta\\ge\\frac1n)\\right). The weights and weights are used to obtain the ranking algorithm, and they will be given a linear log. The weights and weights are used to obtain the ranking algorithm, and they will be given a linear log. To obtain the", "histories": [["v1", "Mon, 15 May 2017 17:59:17 GMT  (49kb)", "http://arxiv.org/abs/1705.05366v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["moein falahatgar", "alon orlitsky", "venkatadheeraj pichapati", "ananda theertha suresh"], "accepted": true, "id": "1705.05366"}, "pdf": {"name": "1705.05366.pdf", "metadata": {"source": "CRF", "title": "Maximum Selection and Ranking under Noisy Comparisons", "authors": ["Moein Falahatgar", "Alon Orlitsky", "Venkatadheeraj Pichapati", "Ananda Theertha Suresh"], "emails": ["moein@ucsd.edu", "alon@ucsd.edu", "dheerajpv7@ucsd.edu", "theertha@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 36\n6v 1\n[ cs\n.L G\n] 1\n(\nn \u01eb2 log 1 \u03b4\n)\ncomparisons, a number tight up to a constant factor. We then derive a general framework that improves the performance of many ranking algorithms, and combine it with merge sort and binary search to obtain a ranking algorithm that uses O ( n logn(log logn)3\n\u01eb2\n)\ncomparisons for any \u03b4 \u2265 1 n , a number optimal up to a (log logn)3 factor."}, {"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Background", "text": "Maximum selection and sorting using pairwise comparisons are computer-science staples taught in most introductory classes and used in many applications. In fact, sorting, also known as ranking, has been claimed to utilize 25% of computer cycles worldwide Mukherjee [2011].\nIn many applications, the pairwise comparisons produce only random outcomes. For example, sports tournaments rank teams based on pairwise matches, but match outcomes are probabilistic in nature. Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers. Another important application is online advertising. Prominent web pages devote precious little space to advertisements, limiting companies like Google, Microsoft, or Yahoo! to present a typical user with just a couple of ads, of which the user selects at most one. Based on these small random comparisons, the company would like to rank the ads according to their appeal Radlinski & Joachims [2007], Radlinski et al. [2008].\nThis and related applications have brought about a resurgence of interest in maximum selection and ranking using noisy comparisons. Several noise models were considered, including the popular Plackett-Luce model Plackett [1975], Luce [2005]. Yet even for such specific models, the complexity of maximum selection was known only up to a log n factor and the complexity of ranking was known\nonly up to a log n factor. We consider a broader class of models and propose algorithms that are optimal up to a constant factor for maximum selection and up to (log log n)3 for ranking."}, {"heading": "1.2 Notation", "text": "Noiseless comparison assumes an unknown underlying ranking r(1), . . . ,r(n) of the elements such that if two elements are compared, the higher-ranked one is selected. Similarly for noisy comparisons, we assume an unknown ranking of the n elements, but now if two elements i and j are compared, i is chosen with some unknown probability p(i, j) and j is chosen with probability p(j, i) = 1 \u2212 p(i, j), where the higher-ranked element has probability \u2265 12 . Repeated comparisons are independent of each other.\nLet p\u0303(i, j) = p(i, j) \u2212 12 reflect the additional probability by which i is preferable to j. Note that p\u0303(j, i) = \u2212p\u0303(i, j) and p\u0303(i, j) \u2265 0 if r(i) > r(j). |p\u0303(i, j)| can also be seen as a measure of dissimilarity between i and j. In our model we assume that two very natural properties hold whenever r(i) > r(j) > r(k).\n(1)Strong stochastic transitivity:\np\u0303(i, k) \u2265 max(p\u0303(i, j), p\u0303(j, k));\n(2) Stochastic triangle inequality:\np\u0303(i, k) \u2264 p\u0303(i, j) + p\u0303(j, k).\nThese properties are satisfied by several popular preference models e.g., Plackett-Luce(PL) model. Two types of algorithms have been proposed for finding the maximum and ranking under noisy comparisons: non-adaptive or offline Rajkumar & Agarwal [2014], Negahban et al. [2012, 2016], Jang et al. [2016] where we cannot choose the comparison pairs, and adaptive or online where the comparison pairs are selected sequentially based on previous results. In this paper we focus on the latter.\nWe specify the desired output via the (\u01eb, \u03b4)-PAC paradigm Yue & Joachims [2011], Busa-Fekete et al. [2014b] that requires the output to likely closely approximate the intended value. Specifically, given \u01eb, \u03b4 > 0, with probability \u2265 1 \u2212 \u03b4, maximum selection must output an element i such that for j with r(j) = n,\np(i, j) \u2265 1\n2 \u2212 \u01eb.\nWe call such an output \u01eb-maximum. Similarly, with probability \u2265 1 \u2212 \u03b4, the ranking algorithm must output a ranking r\u2032(1), . . . ,r\u2032(n) such that whenever r\u2032(i) > r\u2032(j),\np(i, j) \u2265 1\n2 \u2212 \u01eb.\nWe call such a ranking \u01eb-ranking."}, {"heading": "1.3 Paper outline", "text": "In Section 2 we mention somerelated works. In Section 3 we highlight our main contributions. In Section 4 we propose the maximum selection algorithm. In Section 5 we propose the ranking algorithm. In Section 6 we provide experiments. In Section 7 we discuss the results and mention some future directions."}, {"heading": "2 Related work", "text": "Heckel et al. [2016], Urvoy et al. [2013], Busa-Fekete et al. [2014b,b] assume no underlying ranking or constraints on probabilities and find ranking based on Copeland, Borda count and Random Walk procedures. Urvoy et al. [2013], Busa-Fekete et al. [2014b] showed that if the probabilities p(i, j) are not constrained, both maximum selection and ranking problems require \u0398(n2) comparisons. Several models have therefore been considered to further constrain the probabilities.\nUnder the assumptions of strong stochastic transitivity and triangle inequality , Yue & Joachims [2011] derived a PACmaximum selection algorithm that usesO (\nn \u01eb2 log n \u01eb\u03b4\n)\ncomparisons. Szo\u0308re\u0301nyi et al. [2015] derived a PAC ranking algorithm for PL-model distributions that requires O( n\u01eb2 log n log n \u03b4\u01eb ) comparisons. In addition to PAC paradigm, Yue & Joachims [2011] also considered this problem under the bandit setting and bounded the regret of the resulting dueling bandits problem. Following this work, several other works e.g. Syrgkanis et al. [2016] looked at similar formulation.\nAnother non-PAC approach by Busa-Fekete et al. [2014a], Feige et al. [1994] solves the maximum selection and ranking problems. They assume a lower bound on |p\u0303(i, j)| and the number of comparisons depends on this lower bound. If |p\u0303(i, j)| is 0 for any pair, then these algorithms will never terminate.\nSeveral other noise models have also been considered in practice that have either adverserial noise or the stochastic noise that does not obey triangle inequality. For example, Acharya et al. [2014a, 2016, 2014b] considered adversarial sorting with applications to density estimation and Ajtai et al. [2015] considered the same with deterministic algorithms. Mallows stochastic model Busa-Fekete et al. [2014a] does not satisfy the stochastic triangle inequality and hence our theoretical guarantees do not hold under this model. However our simulations suggest that our algorithm can have a reasonable performance over Mallows model."}, {"heading": "3 New results", "text": "Recall that we study (\u01eb, \u03b4)-PAC model for the problems of online maximum selection and ranking using pairwise comparisons under strong stochastic transitivity and stochastic triangle inequality assumptions. The goal is to find algorithms that use small number of comparisons. Our main contributions are:\n\u2022 A maximum selection algorithm that uses O ( n \u01eb2 log 1\u03b4 )\ncomparisons and therefore our algorithm is optimal up to constants.\n\u2022 A ranking algorithm that uses at most O ( n(logn)3\n\u01eb2 log n\u03b4\n)\ncomparisons and outputs \u01eb-ranking\nfor any \u03b4.\n\u2022 A framework that given any ranking algorithm with O ( n(logn)x\n\u01eb2 log n\u03b4\n)\nsample complexity,\nprovides a ranking algorithm with O ( n logn(log logn)x\n\u01eb2\n)\nsample complexity for \u03b4 \u2265 1n .\n\u2022 Using the framework above, we present an algorithm that uses at most O ( n logn(log logn)3\n\u01eb2\n)\ncomparisons and outputs \u01eb-ranking for \u03b4 = 1n . We also show a lower bound of \u2126 ( n logn \u01eb2 )\non the number of comparisons used by any PAC ranking algorithm, therefore proving our algorithm is optimal up to log log n factors."}, {"heading": "4 Maximum selection", "text": ""}, {"heading": "4.1 Algorithm outline", "text": "We propose a simple maximum-selection algorithm based on Knockout tournaments. Knockout tournaments are often used to find a maximum element under non-noisy comparisons. Knockout tournament of n elements runs in \u2308log n\u2309 rounds where in each round it randomly pairs the remaining elements and proceeds the winners to next round.\nOur algorithm, given in Knockout uses O ( n \u01eb2 log 1\u03b4 )\ncomparisons and O(n) memory to find an \u01eb-maximum. Yue & Joachims [2011] uses O (\nn \u01eb2 log n\u01eb\u03b4\n)\ncomparisons and O(n2) memory to find an \u01eb-maximum. Hence we get log n-factor improvement in the number of comparisons and also we use linear memory compared to quadratic memory. Using the lower bound in Feige et al. [1994], it can be inferred that the best PAC maximum selection algorithm requires \u2126 (\nn \u01eb2 log 1\u03b4\n)\ncomparisons, hence up to constant factor, Knockout is optimal. Also our algorithm can be parallelized to run in O (\nlogn \u01eb2 log 1\u03b4\n)\ntime.\nDue to the noisy nature of the comparisons, we repeat each comparison several times to gain confidence about the winner. Note that in knockout tournaments, the number of pairs in a round decreases exponentially with each round. Therefore we afford to repeat the comparisons more times in the latter rounds and get higher confidence. Let bi be the highest-ranked element (according to unobserved underlying ranking) at the beginning of round i. We repeat the comparisons in round i enough times to ensure that p\u0303(bi, bi+1) \u2264\nc\u01eb 2i/3 with probability \u2265 1 \u2212 \u03b4 2i where c = 21/3 \u2212 1. By\nthe stochastic triangle inequality, p\u0303(b1, b\u2308log n\u2309+1) \u2264 \u2211\u2308logn\u2309+1 i=1 c\u01eb 2i/3\n\u2264 \u01eb with probability \u2265 1\u2212 \u03b4. There is a relaxed notion of strong stochastic transitivity. For \u03b3 \u2265 1, \u03b3-stochastic transitivity Yue & Joachims [2011]: if r(i) > r(j) > r(k), then max(p\u0303(i, j), p\u0303(j, k)) \u2264 \u03b3 \u00b7 p\u0303(i, k). Our results apply to this general notion of \u03b3-stochastic transitivity and the analysis of Knockout is presented under this model. Yue & Joachims [2011] uses O(n\u03b3 6\n\u01eb2 log n\u03b4 ) comparisons to find\nan \u01eb-maximum whereas Knockout uses only O(n\u03b3 2 \u01eb2 log 1\u03b4 ) comparisons. Hence we get a huge improvement in the exponent of \u03b3 as well as removing the extra log n factor. To simplify the analysis, we assume that n is a power of 2, otherwise we can add 2\u2308log n\u2309 \u2212 n dummy elements that lose to every original element with probability 1. Note that all \u01eb-maximums will still be from the original set."}, {"heading": "4.2 Algorithm", "text": "We start with a subroutine Compare that compares two elements. It compares two elements i, j and maintains empirical probability p\u0302i, a proxy for p(i, j). It also maintains a confidence value c\u0302 s.t., w.h.p., p\u0302i \u2208 (p(i, j) \u2212 c\u0302, p(i, j) + c\u0302). Compare stops if it is confident about the winner or if it reaches its comparison budget m. If it reaches m comparisons, it outputs the element with more wins breaking ties randomly.\nWe show that the subroutine Compare always outputs the correct winner if the elements are well seperated.\nAlgorithm 1 Comprare\nInput: element i, element j, bias \u01eb, confidence \u03b4. Initialize: p\u0302i = 1 2 , c\u0302 = 1 2 , m = 1 2\u01eb2 log 2 \u03b4 , r = 0, wi = 0.\n1. while (|p\u0302i \u2212 1 2 | \u2264 c\u0302\u2212 \u01eb and r \u2264 m)\n(a) Compare i and j. if i wins wi = wi + 1.\n(b) r = r + 1, p\u0302i = wi r , c\u0302 =\n\u221a\n1 2r log 4r2 \u03b4 .\nif p\u0302i \u2264 1 2 Output: j. else Output: i.\nLemma 1. If p\u0303(i, j) \u2265 \u01eb, then\nPr(Compare(i, j, \u01eb, \u03b4) 6= i) \u2264 \u03b4.\nNote that instead of using fixed number of comparisons, Compare stops the comparisons adaptively if it is confident about the winner. If |p\u0303(i, j)| \u226b \u01eb, Compare stops much before comparison budget 1\n2\u01eb2 log 2\u03b4 and hence works better in practice.\nNow we present the subroutine Knockout-Round that we use in main algorithm Knockout."}, {"heading": "4.2.1 Knockout-Round", "text": "Knockout-Round takes a set S and outputs a set of size |S|/2. It randomly pairs elements, compares each pair using Compare, and returns the set of winners. We will later show that maximum element in the output set will be comparable to maximum element in the input set.\nAlgorithm 2 Knockout-Round Input: Set S, bias \u01eb, confidence \u03b4. Initialize: Set O = \u2205.\n1. Pair elements in S randomly.\n2. for every pair (a, b):\n(a) Add Compare(a, b, \u01eb, \u03b4) to O."}, {"heading": "Output: O", "text": "Note that comparisons between each pair can be handled by a different processor and hence this algorithm can be easily parallelized.\nNote that a set S can have several maximum elements. Comparison probabilities corresponding to all maximum elements will be essentially same because of triangle inequality. We define max(S) to be the maximum element with the least index, namely,\nmax(S) def = S\n( min{i : p\u0303(S(i), S(j)) \u2265 0 \u2200j} ) .\nLemma 2. Knockout-Round(S, \u01eb, \u03b4) uses |S|4\u01eb2 log 2 \u03b4 comparisons and with probability \u2265 1\u2212 \u03b4,\np\u0303\n(\nmax(S),max ( Knockout-Round(S, \u01eb, \u03b4) )\n)\n\u2264 \u03b3\u01eb"}, {"heading": "4.2.2 Knockout", "text": "Now we present the main algorithm Knockout. Knockout takes an input set S and runs log n rounds of Knockout-Round halving the size of S at the end of each round. Recall that Knockout-Round makes sure that maximum element in the output set is comparable to maximum element in the input set. Using this, Knockout makes sure that the output element is comparable to maximum element in the input set.\nSince the size of S gets halved after each round, Knockout compares each pair more times in the latter rounds. Hence the bias between maximum element in input set and maximum element in output set is small in latter rounds.\nAlgorithm 3 Knockout Input: Set S, bias \u01eb, confidence \u03b4, stochasticity \u03b3. Initialize: i = 1, S = set of all elements, c = 21/3 \u2212 1. while |S| > 1\n1. S = Knockout-Round (\nS, c\u01eb \u03b32i/3 , \u03b4 2i\n)\n.\n2. i = i+ 1.\nOutput: the unique element in S.\nNote that Knockout uses only memory of set S and hence O(n) memory suffices. Now we bound the number of comparisons used by Knockout and prove the correctness.\nTheorem 3. Knockout(S, \u01eb, \u03b4) uses O ( \u03b32|S| \u01eb2 log 1 \u03b4 ) comparisons and with probability at least 1\u2212\u03b4, outputs an \u01eb-maximum."}, {"heading": "5 Ranking", "text": "We propose a ranking algorithm that with probability at least 1\u2212 1n uses O ( n logn(log logn)3 \u01eb2 ) comparisons and outputs an \u01eb-ranking.\nNotice that we use only O\u0303 (\nn logn \u01eb2\n)\ncomparisons for \u03b4 = 1n where as Szo\u0308re\u0301nyi et al. [2015] uses\nO ( n(log n)2/\u01eb2 )\ncomparisons even for constant error probability \u03b4. Furthermore Szo\u0308re\u0301nyi et al. [2015] provided these guarantees only under Plackett-Luce model which is more restrictive compared to ours. Also, their algorithm uses O(n2) memory compared to O(n) memory requirement of ours.\nOur main algorithm Binary-Search-Ranking assumes the existence of a ranking algorithm Rank-x that with probability at least 1\u2212 \u03b4 uses O (\nn \u01eb2 (log n)x log n\u03b4\n)\ncomparisons and outputs an \u01eb-ranking for any \u03b4 > 0, \u01eb > 0 and some x > 1. We also present a Rank-x algorithm with x = 3.\nObserve that we need Rank-x algorithm to work for any model that satisfies strong stochastic transitivity and stochastic triangle inequality. Szo\u0308re\u0301nyi et al. [2015] showed that their algorithm\nworks for Plackett-Luce model but not for more general model. So we present a Rank-x algorithm that works for general model.\nThe main algorithm Binary-Search-Ranking randomly selects n(logn)x elements (anchors) and rank them using Rank-x . The algorithm has then effectively created n(log n)x bins, each between two successively ranked anchors. Then for each element, the algorithm identifies the bin it belongs to using a noisy binary search algorithm. The algorithm then ranks the elements within each bin using Rank-x .\nWe first present Merge-Rank, a Rank-3 algorithm."}, {"heading": "5.1 Merge Ranking", "text": "We present a simple ranking algorithm Merge-Rank that uses O ( n(logn)3\n\u01eb2 log n\u03b4\n)\ncomparisons,\nO(n) memory and with probability \u2265 1\u2212\u03b4 outputs an \u01eb-ranking. Thus Merge-Rank is a Rank-x algorithm for x = 3.\nSimilar to Merge Sort, Merge-Rank divides the elements into two sets of equal size, ranks them separately and combines the sorted arrays. Due to the noisy nature of comparisons, MergeRank compares two elements i, j sufficient times, so that the comparison output is correct with high probability when |p\u0303(i, j)| \u2265 \u01eblogn . Put differently, Merge-Rank is same as the typical Merge Sort, except it uses Compare as the comparison function.\nLet\u2019s define the error of an ordered set S as the maximum distance between two wrongly ordered items in S, namely,\nerr(S) def = max\n1\u2264i\u2264j\u2264|S|\n( p\u0303(S(j), S(i)) ) .\nWe show that when we merge two ordered sets, the error of the resulting ordered set will be at most \u01eblogn more than the maximum of errors of individual ordered sets.\nObserve that Merge-Rank is a recursive algorithm and the error of a singleton set is 0. Two singleton sets each containing a unique element from the input set merge to form a set with two elements with an error at most 2\u01eblogn , then two sets with two elements merge to form a set with four elements with an error of at most 3\u01eblogn and henceforth. Therefore the error of the output ordered set is bounded by \u01eb.\nLemma 4 shows that Merge-Rank can output an \u01eb-ranking of S with probability \u2265 1\u2212 \u03b4. It also bounds the number of comparisons used by the algorithm.\nLemma 4. Merge-Rank (\nS, \u01eblog |S| , \u03b4 |S|2\n) takes O ( |S|(log |S|)3\n\u01eb2 log |S|\u03b4\n)\ncomparisons and with proba-\nbility \u2265 1\u2212 \u03b4, outputs an \u01eb-ranking. Hence, Merge-Rank is a Rank-3 algorithm.\nNow we present our main ranking algorithm."}, {"heading": "5.2 Binary-Search-Ranking", "text": "We first sketch the algorithm outline below. We then provide a proof outline."}, {"heading": "5.2.1 Algorithm outline", "text": "Our algorithm is stated in Binary-Search-Ranking. It can be summarized in three major parts.\nCreating anchors: (Steps 1 to 3) Binary-Search-Ranking first selects a set S\u2032 of n(logn)x random elements (anchors) and ranks them using Rank-x . At the end of this part, there are\nn (logn)x ranked anchors. Equivalently, the algorithm creates n (logn)x \u2212 1 bins, each bin between two successively ranked anchors. Coarse ranking: (Step 4) After forming the bins, the algorithm uses a random walk on a binary search tree, to find which bin each element belongs to. Interval-Binary-Search is similar to the noisy binary search algorithm in Feige et al. [1994]. It builds a binary search tree with the bins as the leaves and it does a random walk over this tree. Due to lack of space the algorithm Interval-Binary-Search is presented in Appendix B but more intuition is given later in this section.\nRanking within each bin: (Step 5) For each bin, we show that the number of elements far from both anchors is bounded. The algorithm checks elements inside a bin whether they are close to any of the bin\u2019s anchors. For the elements that are close to anchors, we rank them close to the anchor. And for the elements that are away from both anchors we rank them using Rank-x and output the resulting ranking.\nAlgorithm 4 Binary-Search-Ranking\nInput: Set S, bias \u01eb. Initialize: \u01eb\u2032 = \u01eb/16, \u01eb\u2032\u2032 = \u01eb/15, and So = \u2205. Sj = \u2205, Cj = \u2205 and Bj = \u2205, for 1 \u2264 j \u2264 \u230a n (log n)x \u230b +1.\n1. Form a set S\u2032 with \u230a\nn (log n)x\n\u230b\nrandom elements from S. Remove these elements from S.\n2. Rank S\u2032 using Rank-x ( S\u2032, \u01eb\u2032, 1n6 ) .\n3. Add dummy element a at the beginning of S\u2032 such that p(a, e) = 0 \u2200e \u2208 S \u22c3 S\u2032. Add dummy element b at the end of S\u2032 such that p(b, e) = 1 \u2200e \u2208 S \u22c3 S\u2032.\n4. for e \u2208 S:\n(a) k = Interval-Binary-Search(S\u2032, e, \u01eb\u2032\u2032).\n(b) Insert e in Sk.\n5. for j = 1 to \u230a\nn (logn)x\n\u230b\n+ 1:\n(a) for e \u2208 Sj :\ni. if Compare2(e, S\u2032(j), 10\u01eb\u2032\u2032\u22122 log n) \u2208 [ 1 2 \u2212 6\u01eb \u2032\u2032, 12 + 6\u01eb \u2032\u2032 ] , insert e in Cj .\nii. else if Compare2(e, S\u2032(j + 1), 10\u01eb\u2032\u2032\u22122 log n) \u2208 [ 1 2 \u2212 6\u01eb \u2032\u2032, 12 + 6\u01eb \u2032\u2032 ]\n, then insert e in Cj+1.\niii. else insert e in Bj.\n(b) Rank Bj using Rank-x ( Bj, \u01eb \u2032\u2032, 1\nn4\n)\n.\n(c) Append S\u2032(j), Cj, Bj in order at the end of S o.\nOutput: So\nAlgorithm 5 Compare2\nInput: element a, element b, number of comparisons k.\n1. Compare a and b for k times and return the fraction of times a wins over b."}, {"heading": "5.2.2 Analysis of Binary-Search-Ranking", "text": "Creating anchors In Step 1 of the algorithm we select n/(log n)x random elements. Since these are chosen uniformly random, they lie nearly uniformly in the set S. This intuition is formalized in the next lemma.\nLemma 5. Consider a set S of n elements. If we select n(logn)x elements uniformly randomly from S and build an ordered set S\u2032 s.t. p\u0303(S\u2032(i), S\u2032(j)) \u2265 0 \u2200i > j , then with probability \u2265 1 \u2212 1 n4 , for any \u01eb > 0 and all k,\n|{e \u2208 S : p\u0303(e, S\u2032(k)) > \u01eb, p\u0303(S\u2032(k + 1), e) > \u01eb}| \u2264 5(log n)x+1.\nIn Step 2, we use Rank-x to rank S\u2032. Lemma 6 shows the guarantee of ranking S\u2032.\nLemma 6. After Step 2 of the Binary-Search-Ranking with probability \u2265 1\u2212 1n6 , S \u2032 is \u01eb\u2032-ranked.\nAt the end of Step 2, we have n(logn)x \u2212 1 bins, each between two successively ranked anchors. Each bin has a left anchor and a right anchor . We say that an element belongs to a bin if it wins over the bin\u2019s left anchor with probability \u2265 12 and wins over the bin\u2019s right anchor with probability \u2264 12 . Notice that some elements might win over S\n\u2032(1) with probability < 12 and thus not belong to any bin. So in Step 3, we add a dummy element a at the beginning of S\u2032 where a loses to every element in S \u22c3\nS\u2032 with probability 1. For similar reasons we add a dummy element b to the end of S\u2032 where every element in S \u22c3\nS\u2032 loses to b with probability 1. Coarse Ranking Note that S\u2032(i) and S\u2032(i + 1) are respectively the left and right anchors of the bin Si. Since S\u2032 is \u01eb\u2032-ranked and the comparisons are noisy, it is hard to find a bin Si for an element e such that p(e, S\u2032(i)) \u2265 12 and p(S \u2032(i + 1), e) \u2265 12 . We call a bin Si a \u01eb\n\u2032\u2032\u2212nearly correct bin for an element e if p(e, S\u2032(i)) \u2265 12\u01eb \u2032\u2032 and p(S\u2032(i+ 1), e) \u2265 12 \u2212 \u01eb \u2032\u2032 for some \u01eb\u2032\u2032 > \u01eb\u2032.\nIn Step 4, for each element we find a \u01eb\u2032\u2032-nearly correct bin using Interval-Binary-Search . Next we describe an outline of Interval-Binary-Search.\nInterval-Binary-Search first builds a binary search tree of intervals (see Appendix B) as follows: the root node is the entire interval between the first and the last elements in S\u2032. Each non-leaf node interval I has two children corresponding to the left and right halves of I. The leaves of the tree are the bins between two successively ranked anchors.\nTo find a \u01eb\u2032\u2032-nearly correct bin for an element e, the algorithm starts at the root of the binary search tree and at every non-leaf node corresponding to interval I, it checks if e belongs to I or not by comparing e with I\u2019s left and right anchors. If e loses to left anchor or wins against the right anchor, the algorithm backtracks to current node\u2019s parent.\nIf e wins against I\u2019s left anchor and loses to its right one, the algorithm checks if e belongs to the left child or the right one by comparing e with the middle element of I and moves accordingly.\nWhen at a leaf node, the algorithm checks if e belongs to the bin by maintaining a counter. If e wins against the bin\u2019s left anchor and loses to the bin\u2019s right anchor, it increases the counter\nby one and otherwise it decreases the counter by one. If the counter is less than 0 the algorithm backtracks to the bin\u2019s parent. By repeating each comparison several times, the algorithm makes a correct decision with probability \u2265 1920 .\nNote that there could be several \u01eb\u2032\u2032-nearly correct bins for e and even though at each step the algorithm moves in the direction of one of them, it could end up moving in a loop and never reaching one of them. We thus run the algorithm for 30 log n steps and terminate.\nIf the algorithm is at a leaf node by 30 log n steps and the counter is more than 10 log n we show that the leaf node bin is a \u01eb\u2032\u2032-nearly correct bin for e and the algorithm outputs the leaf node. If not, the algorithm puts in a set Q all the anchors visited so far and orders Q according to S\u2032.\nWe select 30 log n steps to ensure that if there is only one nearly correct bin, then the algorithm outputs that bin w.p. \u2265 1\u2212 1\nn6 . Also we do not want too many steps so as to bound the size of Q.\nBy doing a simple binary search in Q using Binary-Search (see Appendix B) we find an anchor f \u2208 Q such that |p\u0303(e, f)| \u2264 4\u01eb\u2032\u2032. Since Interval-Binary-Search ran for at most 30 log n steps, Q can have at most 60 log n elements and hence Binary-Search can search effectively by repeating each comparison O(log n) times to maintain high confidence. Next paragraph explains how Binary-Search finds such an element f .\nBinary-Search first compares e with the middle element m of Q for O(log n) times. If the fraction of wins for e is between 12\u22123\u01eb \u2032\u2032 and 12+3\u01eb \u2032\u2032, then w.h.p. |p\u0303(e,m)| \u2264 4\u01eb\u2032\u2032 and hence BinarySearch outputs m. If the fraction of wins for e is less than 12 \u2212 3\u01eb \u2032\u2032, then w.h.p. p\u0303(e,m) \u2264 \u22122\u01eb\u2032\u2032 and hence it eliminates all elements to the right of m in Q. If the fraction of wins for e is more than 12 + 3\u01eb\n\u2032\u2032, then w.h.p. p\u0303(e,m) \u2265 2\u01eb\u2032\u2032 and hence it eliminates all elements to the left of m in Q. It continues this process until it finds an element f such that the fraction of wins for e is between 1 2 \u2212 3\u01eb \u2032\u2032 and 12 + 3\u01eb \u2032\u2032.\nIn next Lemma, we show that Interval-Binary-Search achieves to find a 5\u01eb\u2032\u2032-nearly correct bin for every element.\nLemma 7. For any element e \u2208 S, Step 4 of Binary-Search-Ranking places e in bin Sl such that p\u0303(e, S\u2032(l)) > \u22125\u01eb\u2032\u2032 and p\u0303(S\u2032(l + 1), e) > \u22125\u01eb\u2032\u2032 with probability \u2265 1\u2212 1n5 .\nRanking within each bin Once we have identified the bins, we rank the elements inside each bin. By Lemma 5, inside each bin all elements are close to the bin\u2019s anchors except at most 10(log n)x+1 of them.\nThe algorithm finds the elements close to anchors in Step 5a by comparing each element in the bin with the bin\u2019s anchors. If an element in bin Sj is close to bin\u2019s anchors S\n\u2032(j) or S\u2032(j + 1) , the algorithm moves it to the set Cj or Cj+1 accordingly and if it is far away from both, the algorithm moves it to the set Bj. The following two lemmas state that this separating process happens accurately with high probability. The proofs of these results follow from the Chernoff bound and hence omitted.\nLemma 8. At the end of Step 5a, for all j, \u2200e \u2208 Cj , |p\u0303(e, S \u2032(j))| < 7\u01eb\u2032\u2032 with probability \u2265 1\u2212 1\nn3 .\nLemma 9. At the end of Step 5a, for all j, \u2200e \u2208 Bj, min(p\u0303(e, S \u2032(j)), p\u0303(S\u2032(j + 1), e)) > 5\u01eb\u2032\u2032 with probability \u2265 1\u2212 1 n3 .\nCombining Lemmas 5, 6 and 9 next lemma shows that the size of Bj is bounded for all j.\nLemma 10. At the end of Step 5a, |Bj | \u2264 10(log n) x+1 for all j, with probability \u2265 1\u2212 3\nn3 .\nSince all the elements in Cj are already close to an anchor, they do not need to be ranked. By Lemma 5 with probability \u2265 1\u2212 3\nn3 the number of elements in Bj is at most 10(log n) x+1. Therefore we use Rank-x to rank these elements and output the final ranking.\nLemma 11 shows that all Bj\u2019s are \u01eb \u2032\u2032-ranked at the end of Step 5b. Proof follows from properties\nof Rank-x and union bound.\nLemma 11. At the end of Step 5b, all Bjs are \u01eb \u2032\u2032-ranked with probability \u2265 1\u2212 1n3 .\nCombining the above set of results yields our main result.\nTheorem 12. Given access to Rank-x , Binary-Search-Ranking uses O ( n logn(log logn)x\n\u01eb2\n)\ncom-\nparisons and produces an \u01eb-ranking with probability \u2265 1\u2212 1n .\nUsing Merge-Rank as a Rank-x algorithm with x = 3 leads to the following corollary.\nCorollary 13. Binary-Search-Ranking uses O ( n logn(log logn)3\n\u01eb2\n)\ncomparisons and produces an\n\u01eb-ranking with probability \u2265 1\u2212 1n .\nUsing PALPAC-AMPRR Szo\u0308re\u0301nyi et al. [2015] as a Rank-x algorithm with x = 1 leads to the following corollary over PL model.\nCorollary 14. Over PL model, Binary-Search-Ranking uses O (\nn logn log logn \u01eb2\n)\ncomparisons\nand produces an \u01eb-ranking with probability \u2265 1\u2212 1n .\nIt is well known that to rank a set of n values under the noiseless setting, \u2126(n log n) comparisons are necessary. We show that under the noisy model, \u2126 (\nn \u01eb2 log n \u03b4\n)\nsamples are necessary to output an \u01eb-ranking and hence our algorithm is near-optimal.\nTheorem 15. There exists a noisy model that satisfies strong stochastic transitivity and stochastic triangle inequality such that to output an \u01eb-ranking with probability \u2265 1\u2212\u03b4, \u2126 (\nn \u01eb2 log n\u03b4\n)\ncomparisons are necessary."}, {"heading": "6 Experiments", "text": "We compare the performance of our algorithms with that of others over simulated data. Similar to Yue & Joachims [2011], we consider the stochastic model where p(i, j) = 0.6 \u2200i < j. Note that this model satisfies both strong stochastic transitivity and triangle inequality. We find 0.05- maximum with error probability \u03b4 = 0.1. Observe that i = 1 is the only 0.05-maximum. We compare the sample complexity of Knockout with that of BTM-PAC Yue & Joachims [2011], MallowsMPI Busa-Fekete et al. [2014a], and AR Heckel et al. [2016]. BTM-PAC is an (\u01eb, \u03b4)PAC algorithm for the same model considered in this paper. MallowsMPI finds a Condorcet winner which exists under our general model. AR finds the maximum according to Borda scores. We also tried PLPAC Szo\u0308re\u0301nyi et al. [2015], developed originally for PL model but the algorithm could not meet guarantees of \u03b4 = 0.1 under this model and hence omitted. Note that in all the experiments the reported numbers are averaged over 100 runs.\nIn Figure 1, we compare the sample complexity of algorithms when there are 7, 10 and 15 elements. Our algorithm outperforms all the others. BTM-PAC performs much worse in comparison to others because of high constants in the algorithm. Further BTM-PAC allows comparing an element with itself since the main objective in Yue & Joachims [2011] is to reduce the regret. We include more comparisons with BTM-PAC in Appendix G. We exclude BTM-PAC for further experiments with higher number of elements.\nIn Figure 2, we compare the algorithms when there are 50, 100, 200 and 500 elements. Our algorithm outperforms others for higher number of elements too. Performance of AR gets worse as the number of elements increases since Borda scores of the elements get closer to each other and\nhence AR takes more comparisons to eliminate an element. Notice that number of comparisons is in logarithmic scale and hence the performance of MallowsMPI appears to be close to that of ours.\nAs noted in Szo\u0308re\u0301nyi et al. [2015], sample complexity of MallowsMPI gets worse as p\u0303(i, j) gets close to 0. To show the pronounced effect, we use the stochastic model p(1, j) = 0.6 \u2200j > 1, p(i, j) = 0.5 + p\u0303 \u2200j > i, i > 1 where p\u0303 < 0.1, and the number of elements is 15. Here too we find 0.05-maximum with \u03b4 = 0.1. Note that i = 1 is the only 0.05-maximum in this stochastic model.\nIn Figure 3, we compare the algorithms for different values of p\u0303: 0.01, 0.005 and 0.001. As discussed above, the performance of MallowsMPI gets much worse whereas our algorithm\u2019s performance stays unchanged. The reason is that MallowsMPI finds the Condorcet winner using successive elimination technique and as p\u0303 gets closer to 0, MallowsMPI takes more comparisons for each elimination. Our algorithm tries to find an alternative which defeats Condorcet winner with probability \u2265 0.5 \u2212 0.05 and hence for alternatives that are very close to each other, our algorithm declares either one of them as winner after comparing them for certain number of times.\nNext we evaluate Knockout on Mallows model which does not satisfy triangle inequality. Mallows is a parametric model which is specified by single parameter \u03c6. As in Busa-Fekete et al. [2014a], we consider n = 10 elements and various values for \u03c6: 0.03, 0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95 and 0.99. Here again we seek to find 0.05-maximum with \u03b4 = 0.05. As we can see in Figure 4, sample complexity of Knockout and MallowsMPI is essentially same under small values of \u03c6 butKnockout outperformsMallowsMPI as \u03c6 gets close to 1 since comparison probabilities grow closer to 1. Surprisingly, for all values of \u03c6 except for 0.99, Knockout returned Condorcet winner in all runs. For \u03c6 = 0.99, Knockout returned second best element in 10 runs out of 100. Note that p\u0303(1, 2) = 0.0025 and hence Knockout still outputed a 0.05-maximum. Even though we could not show theoretical guarantees of Knockout under Mallows model, our simulations suggest that it can perform well even under this model.\nMore experiments are provided in Appendix G."}, {"heading": "7 Conclusion", "text": "We studied maximum selection and ranking using noisy comparisons for the broad model where the comparison probabilities satisfy strong stochastic transitivity and the triangle inequality. For maximum selection, we presented a simple algorithm with linear, hence optimal, sample complexity. For ranking we presented a framework that improves the performance of many ranking algorithms and applied it to merge ranking to derive a near-optimal ranking algorithm.\nWe conducted several experiments and showed that our algorithms perform well not only in theory, but also in practice. Furthermore, they out-performed all existing algorithms.\nThe maximum-selection experiments suggest that our algorithm performs well even without the triangle-inequality assumption. It would be of interest to extend our theoretical guarantees to this case. For ranking, it would be interesting to close the (log log n)3 ratio between the upper- and lower- complexity bounds."}, {"heading": "A Merge Ranking", "text": "We first introduce a subroutine that is used by Merge-Rank. It merges two ordered sets in the presence of noisy comparisons."}, {"heading": "A.1 Merge", "text": "Merge takes two ordered sets S1 and S2 and outputs an ordered set Q by merging them. Merge starts by comparing the first elements in each set S1 and S2 and places the loser in the first position of Q. It compares the two elements sufficient times to make sure that output is near-accurate. Then it compares the winner and the element right to loser in the corresponding set. It continues this process until we run out of one of the sets and then adds the remaining elements to the end of Q and outputs Q.\nAlgorithm 6 Merge\nInput: Sets S1, S2, bias \u01eb, confidence \u03b4. Initialize: i = 1, j = 1 and O = \u2205.\n1. while i \u2264 |S1| and j \u2264 |S2|.\n(a) if S1(i) = Compare(S1(i), S2(j), \u01eb, \u03b4), then append S1(i) at the end of O and i = i+1.\n(b) else append S2(j) at the end of O and j = j + 1.\n2. if i \u2264 |S1|, then append S1(i : |S1|) at the end of O.\n3. if j \u2264 |S2|, then append S2(j : |S2|) at the end of O."}, {"heading": "Output: O.", "text": "We show that when we merge two ordered sets using Merge, the error of resulting ordered set is not high compared to the maximum of errors of individual ordered sets.\nLemma 16. With probability \u2265 1 \u2212 (|S1| + |S2|)\u03b4, error of Merge(S1, S2, \u01eb, \u03b4) is at most \u01eb more than the maximum of errors of S1 and S2. Namely, with probability \u2265 1\u2212 (|S1|+ |S2|)\u03b4,\nerr(Merge(S1, S2, \u01eb, \u03b4)) \u2264 max (err(S1), err(S2)) + \u01eb."}, {"heading": "A.2 Merge-Rank", "text": "Now we present the algorithm Merge-Rank. Merge-Rank partitions the input set S into two sets S1 and S2 each of size |S|/2. It then orders S1 and S2 separately using Merge-Rank and combines the ordered sets using Merge. Notice that Merge-Rank is a recursive algorithm. The singleton sets each containing an unique element in S are merged first. Two singleton sets are merged to form a set with two elements, then the sets with two elements are merged to form a set with four elements and henceforth. By Lemma 16, each merge with bound parameter \u01eb\u2032 adds at most \u01eb\u2032 to the error. Since error of singleton sets is 0 and each element takes part in log n merges, the error of the output set is at most \u01eb\u2032 log n. Hence with bound parameter \u01eb/ log n, the error of the output set is less than \u01eb.\nAlgorithm 7 Merge-Rank\nInput: Set S, bias \u01eb, confidence \u03b4.\n1. S1 = Merge-Rank(S(1 : \u230a|S|/2\u230b), \u01eb, \u03b4).\n2. S2 = Merge-Rank(S(\u230a|S|/2\u230b+ 1 : |S|), \u01eb, \u03b4).\nOutput: Merge(S1, S2, \u01eb, \u03b4)."}, {"heading": "B Algorithms for Ranking", "text": "Algorithm 8 Interval-Binary-Search Input: Ordered array S, search element e, bias \u01eb\n1. T = Build-Binary-Search-Tree(|S|).\n2. Initialize set Q = \u2205, node \u03b1 = root(T ), and count c = 0.\n3. repeat for 30 log n times\n(a) if \u03b12 \u2212 \u03b11 > 1,\ni. Add \u03b11, \u03b12 and \u2308 \u03b11+\u03b12 2 \u2309 to Q.\nii. if Compare(S(\u03b11), e, 10 \u01eb2 ) > 1/2 or Compare(e, S(\u03b12), 10 \u01eb2 ) > 1/2 then go back to\nthe parent, \u03b1 = parent(\u03b1).\niii. else\n\u2022 if Compare(S( \u2308 \u03b11+\u03b12 2 \u2309 ), e, 10 \u01eb2 ) > 1/2 go to the left child,\u03b1 = left(\u03b1). \u2022 else go to the right child, \u03b1 = right(\u03b1).\n(b) else\ni. if Compare(e, S(\u03b11), 10 \u01eb2 ) > 1/2 and Compare(S(\u03b12), a, 10 \u01eb2 ) > 1/2,\nc = c+ 1.\nii. else\nA. if c = 0, \u03b1 = parent(\u03b1).\nB. else c = c\u2212 1.\n4. (a) if c > 10 log n, Output: \u03b11.\n(b) else Output: Binary-Search(S,Q, e, 2\u01eb).\nAlgorithm 9 Build-Binary -Search-Tree Input: size n. // Recall that each node m in the tree is an interval between left end m1 and right end m2.\n1. Initialize set T \u2032 = \u2205.\n2. Initialize the tree T with the root node (1, n).\nm = (1, n) where m1 = 1 and m2 = n,\nroot(T ) = m\n3. Add m to T \u2032.\n4. while T \u2032 is not empty\n(a) Consider a node i in T \u2032.\n(b) if i2 \u2212 i1 > 1, create a left child and right child to i and set their parents as i.\n\u03b1 =\n(\ni1,\n\u2308\ni1 + i2 2\n\u2309)\n, \u03b2 =\n(\u2308\ni1 + i2 2\n\u2309\n, i2\n)\n,\nleft(i) = \u03b1, right(i) = \u03b2,\nparent(\u03b1) = i, parent(\u03b2) = i.\nand add nodes \u03b1 and \u03b2 to T \u2032.\n(c) Remove node i from T \u2032."}, {"heading": "Output: T .", "text": "Algorithm 10 Binary-Search Input: Ordered array S, ordered array Q, search item e, bias \u01eb. Initialize: l = 1, h = |Q|.\n1. while h\u2212 l > 0\n(a) t = Comapre ( e, S(Q( \u2308\nl+h 2\n\u2309\n), 10 logn\u01eb2\n)\n.\n(b) if t \u2208 [ 1 2 \u2212 3\u01eb, 1 2 + 3\u01eb ] , then Output: Q( \u2308 l+h 2 \u2309 ). (c) else if t < 12 \u2212 3\u01eb, then move to the right.\nl =\n\u2308\nl + h\n2\n\u2309\n.\n(d) else move to the left.\nh =\n\u2308\nl + h\n2\n\u2309\n.\nOutput: Q(h)."}, {"heading": "C Some tools for proving lemmas", "text": "We first prove an auxilliary result that we use in the future analysis.\nLemma 17. Let W = Compare(i, j, \u01eb, \u03b4) and L be the other element. Then with probability \u2265 1\u2212\u03b4,\np(W,L) \u2265 1\n2 \u2212 \u01eb.\nProof. Note that if |p\u0303(i, j)| < \u01eb, then p(i, j) > 12 \u2212 \u01eb and p(j, i) > 1 2 \u2212 \u01eb. Hence, p(W,L) \u2265 1 2 \u2212 \u01eb.\nIf |p\u0303(i, j)| \u2265 \u01eb, without loss of generality, assume that i is a better element i.e., p\u0303(i, j) \u2265 \u01eb. By Lemma 1, with probability atleast 1\u2212 \u03b4, W = i. Hence\nPr\n(\np(W,L) \u2265 1\n2 \u2212 \u01eb\n)\n= Pr(W = i) \u2265 1\u2212 \u03b4.\nWe now prove a Lemma that follows from strong stochastic transitivity and stochastic triangle inequality that we will use in future analysis.\nLemma 18. If p\u0303(i, j) \u2264 \u01eb1, p\u0303(j, k) \u2264 \u01eb2, then p\u0303(i, k) \u2264 \u01eb1 + \u01eb2.\nProof. We will divide the proof into four cases based on whether p\u0303(i, j) > 0 and p\u0303(j, k) > 0. If p\u0303(i, j) \u2264 0 and p\u0303(j, k) \u2264 0, then by strong stochastic transitivity, p\u0303(i, k) \u2264 0 \u2264 \u01eb1 + \u01eb2. If 0 < p\u0303(i, j) \u2264 \u01eb1 and 0 < p\u0303(j, k) \u2264 \u01eb2, then by stochastic traingle inequality, p\u0303(i, k) \u2264 \u01eb1 + \u01eb2. If p\u0303(i, j) < 0 and 0 < p\u0303(j, k) \u2264 \u01eb2, then by strong stochastic transitivity, p\u0303(i, k) \u2264 \u01eb2 \u2264 \u01eb1 + \u01eb2. If 0 < p\u0303(i, j) \u2264 \u01eb1 and p\u0303(j, k) < 0, then by strong stochastic transitivity, p\u0303(i, k) \u2264 \u01eb1 \u2264 \u01eb1 + \u01eb2."}, {"heading": "D Proofs of Section 4", "text": ""}, {"heading": "Proof of Lemma 1", "text": "Proof. Let p\u0302ri and c\u0302 r denote p\u0302i and c\u0302 respectively after r number of comparisons. Output of Compare(i, j, \u01eb, \u03b4) will not be i only if p\u0302ri < 1 2 + \u01eb \u2212 c\u0302 r for any r < m = 1 2\u01eb2 log 2\u03b4 or if p\u0302i < 1 2 for r = m. We will show that the probability of each of these events happening is bounded by \u03b42 . Hence by union bound, Lemma follows.\nAfter r comparisons, by Chernoff bound,\nPr(p\u0302ri < 1\n2 + \u01eb\u2212 c\u0302r) \u2264 e\u22122r(c\u0302\nr)2 = e\u2212 log 4r2 \u03b4 = \u03b4\n4r2 .\nUsing union bound,\nPr(\u2203r s.t. p\u0302ri \u2264 1\n2 + \u01eb\u2212 c\u0302r) \u2264\n\u03b4\n2\nAfter m = 1 2\u01eb2 log 2\u03b4 rounds, by Chernoff bound,\nPr(p\u0302mi < 1\n2 ) \u2264 e\u22122m\u01eb\n2 = \u03b4\n2 ."}, {"heading": "Proof of Lemma 2", "text": "Proof. Each of the |S|2 pairs is compared at most 1 2\u01eb2 log 2\u03b4 times, hence the total comparisons is \u2264 |S|4\u01eb2 log 2 \u03b4 . Let k\n\u2217 = max(Knockout-Round(S, \u01eb, \u03b4)) and s\u2217 = max(S). Let a be the element paired with s\u2217. There are two cases: p\u0303(s\u2217, a) \u2265 \u01eb and p\u0303(s\u2217, a) < \u01eb.\nIf p\u0303(s\u2217, a) \u2265 \u01eb, by Lemma 1 with probability \u2265 1\u2212 \u03b4, s\u2217 will win and hence by definitions of s\u2217 and k\u2217, p\u0303(s\u2217, k\u2217) = 0 \u2264 \u03b3\u01eb. Alternatively, if p\u0303(s\u2217, a) < \u01eb, let winner(i, j) denote the winner between i and j when compared for 1\n2\u01eb2 log 1\u03b4 times. Then,\nr(a) (a) \u2264 r(winner(s\u2217, a)) (b) \u2264 r(k\u2217) (c) \u2264 r(s\u2217)\nwhere (a) follows from r(a) \u2264 r(s\u2217), (b) and (c) follow from the definitions of s\u2217 and k\u2217 respectively. From strong stochastic tranisitivity on a, k\u2217 and s\u2217, p\u0303(s\u2217, k\u2217) \u2264 \u03b3p\u0303(s\u2217, a) \u2264 \u03b3\u01eb."}, {"heading": "Proof of Theorem 3", "text": "Proof. We first show that with probability \u2265 1 \u2212 \u03b4, the output of Knockout is an \u01eb-maximum. Let \u01ebi = c\u01eb/2 i/3 and \u03b4i = \u03b4/2 i. Note that \u01ebi and \u03b4i are bias and confidence values used in round i. Let bi be a maximum element in the set S before round i. Then by Lemma 2, with probability \u2265 1\u2212 \u03b4i,\np\u0303(bi, bi+1) \u2264 c\u01eb\n2i/3 . (1)\nBy union bound, the probability that Equation 1 does not hold for some round 1 \u2264 i \u2264 log |S| is\n\u2264\nlog |S| \u2211\ni=1\n\u03b4i =\nlog |S| \u2211\ni=1\n\u03b4 2i \u2264 \u03b4.\nWith probability \u2265 1\u2212 \u03b4, Equation 1 holds for all i and by stochastic triangle inequality,\np\u0303(b1, blog |S|+1) \u2264\nlog |S| \u2211\ni=1\np\u0303(bi, bi+1) \u2264 \u221e \u2211\ni=1\nc\u01eb\n2i/3 = \u01eb.\nWe now bound the number of comparisons. Let ni = |S| 2i\u22121\nbe the number of elements in the set at the beginning of round i. The number of comparisons at round i is\n\u2264 ni 2 \u00b7 \u03b3222i/3 2c2\u01eb2 \u00b7 log 2i+1 \u03b4 .\nHence the number of comparisons in all rounds is\nlog |S| \u2211\ni=1\n|S| 2i \u00b7 \u03b3222i/3 2c2\u01eb2 \u00b7 log 2i+1 \u03b4 \u2264 |S|\u03b32 2c2\u01eb2\n\u221e \u2211\ni=1\n1\n2i/3\n(\ni+ log 2\n\u03b4\n)\n= |S|\u03b32\n2c2\u01eb2\n(\n21/3\nc2 +\n1 c log 2 \u03b4\n)\n= O\n(\n|S|\u03b32\n\u01eb2 log\n1\n\u03b4\n)\n."}, {"heading": "E Proofs of Section 5.1", "text": ""}, {"heading": "Proof of Lemma 16", "text": "Proof. Let Q = Merge(S1, S2, \u01eb, \u03b4). We will show that for every k, w.p. \u2265 1\u2212 \u03b4, p\u0303(Q(k), Q(l)) \u2264 max(err(S1), err(S2))+\u01eb \u2200l > k. Note that if this property is true for every element then err(Q) \u2264 max(err(S1), err(S2)) + \u01eb. Since there are |S1|+ |S2| elements in the final merged set, the Lemma follows by union bound.\nIf S1(i) and S2(j) are compared in Merge algorithm, without loss of generality, assume that S1(i) loses i.e., S1(i) appears before S2(j) in T . The elements that appear to the right of S1(i) in Q belong to set Q\u2265S1(i) = {S1(k) : k > i} \u22c3\n{S2(k) : k \u2265 j}. We will show that w.p. \u2265 1 \u2212 \u03b4, \u2200e \u2208 Q\u2265S1(i), p\u0303(S1(i), e) \u2264 max (err(S1), err(S2)) + \u01eb.\nBy definition of error of an ordered set,\np\u0303(S1(i), S1(k)) \u2264 err(S1) \u2200k > i (2)\np\u0303(S2(j), S2(k)) \u2264 err(S2) \u2200k \u2265 j. (3)\nBy Lemma 17, w.p. \u2265 1\u2212 \u03b4,\np\u0303(S1(i), S2(j)) \u2264 \u01eb. (4)\nHence by Equations 3, 4 and Lemma 18, w.p. \u2265 1\u2212 \u03b4, p\u0303(S1(i), S2(k)) \u2264 \u01eb+ err(S2) \u2200k \u2265 j."}, {"heading": "Proof of Lemma 4", "text": "Proof. We first bound the total comparisons. Let C(Q, \u01eb\u2032, \u03b4\u2032) be the number of comparisons that the Merge-Rank uses on a set Q. Since Merge-Rank is a recursive algorithm,\nC(Q, \u01eb\u2032, \u03b4\u2032) \u2264C(Q[1 : \u230a|Q|/2\u230b], \u01eb\u2032, \u03b4\u2032)\n+ C(Q[\u230a|Q|/2\u230b : |Q|], \u01eb\u2032, \u03b4\u2032) + |Q|\n2\u01eb\u20322 log\n2 \u03b4\u2032 .\nFrom this one can obtain that C(S, \u01eb\u2032, \u03b4\u2032) = O (\n|S| log |S| \u01eb\u20322 log 1\u03b4\u2032\n)\n. Hence,\nC\n(\n|S|, \u01eb\nlog |S| ,\n\u03b4\n|S|2\n)\n= O\n( |S| log3 |S|\n\u01eb2 log\n|S|2\n\u03b4\n)\n.\nNow we bound the error. By Lemma 16, with probability \u2265 1\u2212 |Q|\u03b4,\nerr(Merge-Rank(Q, \u01eb\u2032, \u03b4\u2032)) \u2264 max{err ( Merge-Rank ( Q[1 : \u230a|Q|/2\u230b], \u01eb\u2032, \u03b4\u2032 )) , err ( Merge-Rank ( T [\u230a|Q|/2\u230b+ 1 : |Q|], \u01eb\u2032, \u03b4\u2032 )) }+ \u01eb\u2032. (5)\nWe can bound the total times Merge is called in a single instance of Merge-Rank(S, \u01eb\u2032, \u03b4\u2032). Merge combines the singleton sets and forms the sets with two elements, it combines the sets with two elements and forms the sets with four elements and henceforth. Hence the total times Merge is called is\n\u2211log |S| i=1 |S| 2i\n\u2264 |S|. Therefore, the probability that Equation 5 holds every time when two ordered sets are merged in Merge-Rank(S, \u01eb\u2032, \u03b4\u2032) is \u2264 |S| \u00b7 |S|\u03b4\u2032 = |S|2\u03b4\u2032.\nIf Equation 5 holds every time Merge is called, then error of Merge-Rank(S, \u01eb\u2032, \u03b4\u2032) is at most \u2211log |S|\ni=1 \u01eb \u2032 \u2264 \u01eb\u2032 log |S|. This is because err(S) is 0 if S has only one element. And a singleton set\nparticipates in log n merges before becoming the final output set. Therefore, w.p. \u2265 1\u2212 |S|2\u03b4\u2032,\nerr(Merge-Rank(S, \u01eb\u2032, \u03b4\u2032)) \u2264 log |S|\u01eb\u2032.\nHence with probability \u2265 1\u2212 \u03b4,\nerr\n(\nMerge-Rank\n(\nS, \u01eb\nlog |S| ,\n\u03b4\n|S|2\n))\n\u2264 \u01eb."}, {"heading": "F Proofs for Section 5.2", "text": ""}, {"heading": "Proof of Lemma 5", "text": "Proof. Let set S be ordered s.t. p\u0303(S(i), S(j)) \u2265 0 \u2200i > j. Let S\u2032\u2032k = S(k : k + 5(log n) x+1 \u2212 1) The probability that none of the elements in S\u2032\u2032k is selected for a given k is\n\u2264\n(\n1\u2212 5(log n)x+1\nn\n)n/(logn)x\n< 1\nn5 .\nTherefore by union bound, the probability that none of the elements in S\u2032\u2032k is selected for any k is\n\u2264 n \u00b7 1\nn5 =\n1\nn4 ."}, {"heading": "Proof of Lemma 7", "text": "We prove Lemma 7 by dividing it into further smaller lemmas. We divide all elements into S into two sets based on distance from anchors. First set contains all elements that are far away from all anchors and the second set contains all elements which are close to atleast one of the anchors. Interval-Binary-Search acts differently on both sets.\nWe first show that for elements in the first set, Interval-Binary-Search places them in between the right anchors by using just the random walk subroutine.\nFor elements in the second set, Interval-Binary-Search might fail to find the right anchors just by using the random walk subroutine. But we show that Interval-Binary-Search visits a close anchor during random walk and Binary-Search finds a close anchor from the set of visited anchors using simple binary search.\nWe first prove Lemma 7 for the elements of first set.\nLemma 19. For \u01eb\u2032\u2032 > \u01eb\u2032, consider an \u01eb\u2032-ranked S\u2032. If an element e is such that |p\u0303(e, S\u2032(j))| > \u01eb\u2032\u2032 \u2200j, then with probability \u2265 1 \u2212 1n6 step 4a of Interval-Binary-Search(S \u2032, e, \u01eb\u2032\u2032) outputs the index y such that p\u0303(e, S\u2032(y)) > \u01eb\u2032\u2032 and p\u0303(S\u2032(y + 1), e) > \u01eb\u2032\u2032.\nProof. We first show that there is a unique y s.t. p\u0303(e, S\u2032(y)) > \u01eb\u2032\u2032 and p\u0303(S\u2032(y + 1), e) > \u01eb\u2032\u2032. Let i be the largest index such that p\u0303(e, S\u2032(i)) > \u01eb\u2032\u2032. By Lemma 18, p\u0303(e, S\u2032(j)) > \u01eb\u2032\u2032 \u2212 \u01eb\u2032 > 0 \u2200j < i. Hence by the assumption on e, p\u0303(e, S\u2032(j)) > \u01eb\u2032\u2032 \u2200j < i. Let k be the smallest index\nsuch that p\u0303(S\u2032(k), e) > \u01eb\u2032\u2032. By a similar argument as previously, we can show that p\u0303(S\u2032(j), e) > \u01eb\u2032\u2032 \u2200j > k.\nHence by the above arguments and the fact that |p\u0303(e, S\u2032(j))| > \u01eb\u2032\u2032 \u2200j, there exists only one y such that p\u0303(e, S\u2032(y)) > \u01eb\u2032\u2032 and p\u0303(S\u2032(y + 1), e) > \u01eb\u2032\u2032.\nThus in the tree T , there is only one leaf node w such that p\u0303(e, S\u2032(w1)) > \u01eb \u2032\u2032 and p\u0303(S\u2032(w2), e) >\n\u01eb\u2032\u2032. Consider some node m which is not an ancestor of w. Then either p\u0303(S\u2032(m1), e) > \u01eb\n\u2032\u2032 or p\u0303(S\u2032(m2), e) < \u2212\u01eb \u2032\u2032. Since we compare e with S\u2032(ml) and S \u2032(mh) 10 \u01eb\u2032\u20322 times, we move to the parent of m with probability atleast 1920 . Consider some node m which is an ancestor of w. Then p\u0303(S\u2032(ml), e) < \u2212\u01eb \u2032\u2032 , p\u0303(S\u2032(mh), e) > \u01eb \u2032\u2032, and |p\u0303(S\u2032( \u2308\nml+mh 2\n\u2309\n), e)| > \u01eb\u2032\u2032. Therefore we move in direction of q with probability atleast 1920 . Therefore if we are not at q, then we move towards q with probability atleast 1920 and if we are at q then the count c increases with probability atleast 1920 . Since we start at most log n away from q if we move towards e for 21 log n then the algorithm will output y. The probability that we will have less than 21 log n right comparisons is \u2264 e\u221230 lognD( 21 30 || 19 20 ) \u2264 e\u221230 lognD( 21 30 || 19 20\n) \u2264 1 n6 .\nTo prove Lemma 7 for the elements of the second set, we first show that the random walk subroutine of algorithm Interval-Binary-Search placing an element in wrong bin is highly unlikely.\nLemma 20. For \u01eb\u2032\u2032 > \u01eb\u2032, consider an \u01eb\u2032-ranked set S\u2032. Now consider an element e and y such that either p\u0303(S\u2032(y), e) > \u01eb\u2032\u2032 or p\u0303(S\u2032(y + 1), e) < \u2212\u01eb\u2032\u2032, then step 4a of Interval-BinarySearch(S\u2032, e, \u01eb\u2032\u2032) will not output y with probability \u2265 1\u2212 1\nn7 .\nProof. Recall that step 4a of Interval-Binary-Search outputs y if we are at the leaf node (y, y + 1) and the count c is atleast 10 log n.\nSince either p\u0303(S\u2032(y), e) > \u01eb\u2032\u2032 or p\u0303(S\u2032(y + 1), e) < \u2212\u01eb\u2032\u2032, when we are at leaf node (y, y + 1), the count decreases with probability atleast 1920 . Hence the probability that Interval-BinarySearch is at (y,y+1) and the count is greater than 10 log n is at most \u221130 logn\ni=10 logn e \u2212i\u00b7D( i\u221210 log n 2i || 19 20 ) <\n10 log ne\u221210 lognD( 1 3 || 19 20 ) \u2264 1 n7 .\nWe now show that for an element of the second set, the random walk subroutine either places it in correct bin or visits a close anchor.\nLemma 21. For \u01eb\u2032\u2032 > \u01eb\u2032, consider an \u01eb\u2032-ranked set S\u2032. Now consider an element e that is close to an element in S\u2032 i.e., \u2203g : |p\u0303(S\u2032(g), e)| < \u01eb\u2032\u2032. Step 4a of Interval-Binary-Search(S\u2032, e, \u01eb\u2032\u2032) will either output the right index y such that p\u0303(S\u2032(y), e) < \u01eb\u2032\u2032 and p\u0303(S\u2032(y + 1), e) > \u2212\u01eb\u2032\u2032 or IntervalBinary-Search visits S\u2032(h) such that |p\u0303(S\u2032(h), e)| < 2\u01eb\u2032\u2032 with probability\u2265 1\u2212 1\nn6 .\nProof. By Lemma 20, step 4a of Interval-Binary-Search does not output a wrong interval with probability 1\u2212 1\nn7 . Hence we just need to show that w.h.p., e visits a close anchor.\nLet i be the largest index such that p\u0303(e, S\u2032(i)) > 2\u01eb\u2032\u2032. Then \u2200k < i, by Lemma 18, p\u0303(e, S(k)) > 2\u01eb\u2032\u2032 \u2212 \u01eb\u2032 > \u01eb\u2032\u2032 .\nLet j be the smallest index such that p\u0303(S\u2032(j), e) > 2\u01eb\u2032\u2032. Then \u2200k > j, by Lemma 18, p\u0303(S\u2032(k), e) > \u01eb\u2032\u2032 .\nTherefore for u < v such that min(|p\u0303(S\u2032(u), e)|, |p\u0303(S\u2032(v), e)|) \u2265 2\u01eb\u2032\u2032 only one of three sets {x : x < u},{x : u < x < v} and {x : x > v} contains an index z such that |p\u0303(S\u2032(z), e)| < \u01eb\u2032\u2032.\nLet a node \u03b1 be s.t. for some c \u2208 {\u03b11, \u03b12, \u2308 \u03b11+\u03b12 2 \u2309}, |p\u0303(S \u2032(c), e)| \u2264 2\u01eb\u2032\u2032. If Interval-Binary-\nSearch reaches such a node \u03b1 then we are done. So assume that Interval-Binary-Search is at a node \u03b2 s.t. \u2200c \u2208 {\u03b21, \u03b22, \u2308 \u03b21+\u03b22 2 \u2309}, |p\u0303(S \u2032(c), e)| > 2\u01eb\u2032\u2032. Note that only one of three sets {x : x < \u03b21 or x > \u03b22}, {x : \u03b21 < x < \u2308 \u03b21+\u03b22\n2 \u2309} and\n{x : \u2308\u03b21+\u03b222 \u2309 < x < \u03b22} contains an index z such that |p\u0303(S \u2032(z), e)| < \u01eb\u2032\u2032 and Interval-BinarySearch moves towards that set with probability 1920 . Hence the probability that we never visit an anchor that is less than 2\u01eb\u2032\u2032 away is at most e\u221230 lognD( 1 2 || 19\n20 ) \u2264 1n7 .\nWe now complete the proof by showing that for an element e from the second set, if Q contains an index y of an anchor that is close to e, Binary-Search will output one such index.\nLemma 22. For \u01eb\u2032\u2032 > \u01eb\u2032, consider ordered sets S\u2032, Q s.t. p(S\u2032(Q(i)), S\u2032(Q(j))) > 12 \u2212 \u01eb \u2032 \u2200i > j. For an element e s.t., \u2203g : |p\u0303(S\u2032(Q(g)), a)| < 2\u01eb\u2032\u2032, Binary-Search(S\u2032, Q, a, \u01eb\u2032\u2032) will return y such that |p\u0303(S\u2032(Q(y)), a)| < 4\u01eb\u2032\u2032 with probability \u2265 1\u2212 1n6 .\nProof. At any stage of Binary-Search, there are three possibilities that can happen . Consider the case when we are comparing e with S\u2032(Q(i)).\n1. |p\u0303(S\u2032(Q(i)), e) < 2\u01eb\u2032\u2032|. Probability that the fraction of wins for e is not between 12 \u2212 3\u01eb \u2032\u2032 and\n1 2 + 3\u01eb\n\u2032\u2032 is less than e\u2212 10 log n \u01eb\u2032\u20322 \u01eb\u2032\u20322 \u2264 1\nn10 . Hence Binary-Search outputs Q(i).\n2. p\u0303(S\u2032(Q(i)), e) > 2\u01eb\u2032\u2032. Probability that the fraction of wins for e is more than 12 is less than\ne\u2212 10 logn \u01eb\u2032\u20322 \u01eb\u2032\u20322 \u2264 1\nn10 . So Binary-Search will not move right. Also notice that p\u0303(S\u2032(Q(j)), e) >\n2\u01eb\u2032\u2032 \u2212 \u01eb\u2032 > \u01eb\u2032\u2032 \u2200j > i. 3. p\u0303(S\u2032(Q(i)), e) > 4\u01eb\u2032\u2032. Probability that the fraction of wins for e is more than 12 \u2212 3\u01eb \u2032\u2032 is less than e\u2212 10 log n \u01eb\u2032\u20322 \u01eb\u2032\u20322 \u2264 1\nn10 . Hence Binary-Search will move left. Also notice that p\u0303(S\u2032(Q(j)), e) >\n4\u01eb\u2032\u2032 \u2212 \u01eb\u2032 > \u01eb\u2032\u2032 \u2200j > i. We can show similar results for p\u0303(S\u2032(Q(i)), e) < \u22122\u01eb\u2032\u2032 and p\u0303(S\u2032(Q(i)), e) < \u22124\u01eb\u2032\u2032. Hence if |p\u0303(S\u2032(Q(i)), e)| < 2\u01eb\u2032\u2032 then Binary-Search outputs Q(i), and if 2\u01eb\u2032\u2032 < |p\u0303(S\u2032(Q(i)), e)| < 4\u01eb\u2032\u2032 then either Binary-Search outputs Q(i) or moves in the correct direction and if |p\u0303(S\u2032(Q(i)), e)| > 4\u01eb\u2032\u2032, then Binary-Search moves in the correct direction.\nLemma 23. Interval-Binary-Search(S, e, \u01eb) terminates in O( log n log logn \u01eb2\n) comparisons for any set S of size O(n).\nProof. Step 3 of Interval-Binary-Search runs for 30 log n iterations. In each iteration, IntervalBinary-Search compares e with at most 3 anchors and repeats each comparison for 10/\u01eb2. So total comparisons in step 3 is O(log n/\u01eb2). The size of Q is upper bounded by 90 log n and BinarySearch does a simple binary search over Q by repeating each comparison 10 log n/\u01eb2. Hence total comparisons used by Binary-Search is O(log n log log n/\u01eb2)\nCombining Lemmas 6, 20, 21, 22, 23 yields the result."}, {"heading": "Proof of Lemma 10", "text": "Proof. Combining Lemmas 6, 9 and using union bound, at the end of step 5a ,w.p. \u2265 1\u2212 2 n3 , S\u2032 is \u01eb\u2032ranked and \u2200j, e \u2208 Bj, min(p\u0303(e, S \u2032(j)), p\u0303(S\u2032(j+1), e)) > 5\u01eb\u2032\u2032. Hence by Lemma 18, \u2200j, k < j, e \u2208 Bj , p\u0303(e, S\u2032(k)) > 5\u01eb\u2032\u2032 \u2212 \u01eb\u2032 > 4\u01eb\u2032\u2032. Similarly, \u2200j, k > j, e \u2208 Bj, p\u0303(S \u2032(k), e) > 5\u01eb\u2032\u2032 \u2212 \u01eb\u2032 > 4\u01eb\u2032\u2032.\nIf |Bj | > 0, then p\u0303(e, S \u2032(k)) > 4\u01eb\u2032\u2032 for e \u2208 Bj , k \u2264 j, p\u0303(S \u2032(l), e) > 4\u01eb\u2032\u2032 for e \u2208 Bj, l \u2265 j. Hence by strong stochastic transitivity, p\u0303(S\u2032(l), S\u2032(k)) > 4\u01eb\u2032\u2032 for l \u2265 j \u2265 k. Therefore there exists k, l s.t. p\u0303(S\u2032(l), f) > 0 \u2200f \u2208 {S\u2032(y) : y \u2264 j}, p\u0303(S\u2032(k), S\u2032(l)) > 0 and p\u0303(f, S\u2032(k)) > 0 \u2200f \u2208 {S\u2032(y) : y \u2265 j}. Now by Lemma 5, w.p. \u2265 1\u2212 1\nn3 , size of such set Bj is less than 10(log n) x+1. Lemma follows by union bound."}, {"heading": "Proof of Theorem 12", "text": "We first bound the running time of Binary-Search-Ranking algorithm.\nTheorem 24. Binary-Search-Ranking terminates after O(n(log logn) x\n\u01eb2 log n) comparisons with probability \u2265 1\u2212 1n2 .\nProof. Step 2Rank-x (S\u2032, \u01eb\u2032, 1 n6 ) terminates afterO( n \u01eb2 log n) comparisons with probability\u2265 1\u2212 1 n6 .\nBy Lemma 7, for each element e, the step 4a Interval-Binary-Search(S\u2032, e, \u01eb\u2032\u2032) terminates after O( logn log logn\n\u01eb2 log log n) comparisons. Hence step 4 takes at most O(n logn log logn \u01eb2 ) comparisons.\nComparing each element with the anchors in steps 5a takes at most O( logn \u01eb2 ) comparisons. With probability \u2265 1 \u2212 1 n4 step 5b Sort-x(Bi, \u01eb \u2032\u2032, 1 n4 ) terminates after O(|Bi| (log |Bi|)x \u01eb2 log n)\ncomparisons. By Lemma 10, |Bi| \u2264 10(log n) x+1 for all i w.p. \u2265 1\u2212 3 n3 . Hence, w.p. \u2265 1\u2212 3 n3 , total comparisons to rank all Bis is at most \u2211 iO(|Bi| (log |Bi|)x \u01eb2 log n) \u2264 \u2211 iO( |Bi| logn(log(10 logn)x+1)x \u01eb2 ) = O(n logn(log logn) x\n\u01eb2 ).\nTherefore, by summing comparisons over all steps, with probability \u2265 1\u2212 1n2 total comparisons\nis at most O ( n logn(log logn)x\n\u01eb2\n)\n.\nNow we show that Binary-Search-Ranking outputs an \u01eb-ranking with high probability.\nTheorem 25. Binary-Search-Ranking produces an \u01eb-ranking with probability at least 1\u2212 1 n2 .\nProof. By combining Lemmas 6, 8, 9, 11 and using union bound, w.p. \u2265 1 \u2212 1 n2 , at the end of step 5b,\n\u2022 S\u2032 is \u01eb\u2032-\n\u2022 Each Ci has elements such that |p\u0303(Ci(j), S(i))| < 7\u01eb \u2032\u2032 for all j.\n\u2022 Each Bi has elements such that p\u0303(S \u2032(i), Bi(j)) < \u22125\u01eb \u2032\u2032 and p\u0303(S\u2032(i+ 1), Bi(j)) > 5\u01eb \u2032\u2032.\n\u2022 All Bis are \u01eb \u2032\u2032-ranked.\nFor j \u2265 i, e \u2208 Bj\u22121 \u22c3 S\u2032(j) \u22c3 Cj, f \u2208 S \u2032(k) \u22c3 Ck \u22c3 Bk, p\u0303(e, f) \u2264 p\u0303(e, S \u2032(i)) + p\u0303(S\u2032(i), S\u2032(j)) + p\u0303(S\u2032(j), f) \u2264 7\u01eb\u2032\u2032 + \u01eb\u2032 + 7\u01eb\u2032\u2032 < 15\u01eb\u2032\u2032 = \u01eb. Combining the above result with the fact that all Bis are \u01eb\u2032\u2032-ranked proves the Lemma.\nCombining Theorems 24, 25 yields the result."}, {"heading": "Proof Sketch for Theorem 15", "text": "Proof sketch. Consider a stochastic model where there is an inherent ranking r and for any two consecutive elements p(i, i+1) = 12 \u22122\u01eb. Suppose there is a genie that knows the true ranking r up to the sets {r(2i\u2212 1), r(2i)} for all i i.e., for each i, genie knows {r(2i \u2212 1), r(2i)} but it does not know the ranking between these two elements. Since consecutive elements have \u01eb(i, i+1) = 2\u01eb > \u01eb, to find an \u01eb-ranking, the genie has to correctly identify the ranking within all the n/2 pairs. Using Fano\u2019s inequality from information theory, it can be shown that the genie needs at least \u2126 (\nn \u01eb2 log n\u03b4\n)\ncomparisons to identify the ranking of the consecutive elements with probability 1\u2212 \u03b4."}, {"heading": "G Additional Experiments", "text": "As we mentioned in Section 6, BTM-PAC allows comparison of an element with itself. It is not beneficial when the goal is to find \u01eb-maximum. So we modify their algorithm by not allowing such comparisons. We refer to this restricted version as R-BTM-PAC.\nAs seen in figure, performance of BTM-PAC does not increase by much by restricting the comparisons.\nWe further reduce the constants inR-BTM-PAC. We change Equations (7) and (8) in Yue & Joachims\n[2011] to c\u03b4(t) = \u221a 1 t log n3N \u03b4 and N = \u2308 1 \u01eb2 log n3N \u03b4 \u2309, respectively.\nWe believe the same guarantees hold even with the updated constants. We refer to this improved restricted version as IR-BTM-PAC. Here too we consider the stochastic model where p(i, j) = 0.6\u2200 i < j and we find 0.05-maximum with error probability \u03b4 = 0.1.\nIn Figure 5 we compare the performance of Knockout and all variations of BTM-PAC. As the figure suggests, the performance of IR-BTM-PAC improves a lot but Knockout still outperforms it significantly.\nIn Figure 6, we consider the stochastic model where p(i, j) = 0.6 \u2200i < j and find \u01eb-maximum for different values of \u01eb. Similar to previous experiments, we use \u03b4 = 0.1. As we can see the number of comparisons increases almost linearly with n. Further the number of comparisons does not increase significantly even when \u01eb decreases. Also the number of comparisons seem to be converging as \u01eb goes to 0. Knockout outperforms MallowsMPI even for the very small \u01eb values. We attribute this to the subroutine Compare that finds the winner faster when the distance between elements are much larger than \u01eb.\nFor the stochastic model p(i, j) = 0.6 \u2200i < j, we run our Merge-Rank algorithm to find 0.05 ranking with \u03b4 = 0.1. Figure 7 shows that sample complexity does not increase a lot with decreasing \u01eb."}], "references": [{"title": "Sorting with adversarial comparators and application to density estimation", "author": ["Acharya", "Jayadev", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"], "venue": "In ISIT, pp. 1682\u20131686", "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "Near-optimalsample estimators for spherical gaussian mixtures", "author": ["Acharya", "Jayadev", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"], "venue": null, "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "Maximum selection and sorting with adversarial comparators and an application to density estimation", "author": ["Acharya", "Jayadev", "Falahatgar", "Moein", "Jafarpour", "Ashkan", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"], "venue": "arXiv preprint arXiv:1606.02786,", "citeRegEx": "Acharya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2016}, {"title": "Sorting and selection with imprecise comparisons", "author": ["Ajtai", "Mikl\u00f3s", "Feldman", "Vitaly", "Hassidim", "Avinatan", "Nelson", "Jelani"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "Ajtai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ajtai et al\\.", "year": 2015}, {"title": "Noisy sorting without resampling", "author": ["Braverman", "Mark", "Mossel", "Elchanan"], "venue": "In Proceedings of the nineteenth annual ACM-SIAM SODA,", "citeRegEx": "Braverman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2008}, {"title": "Sorting from noisy information", "author": ["Braverman", "Mark", "Mossel", "Elchanan"], "venue": "arXiv preprint arXiv:0910.1191,", "citeRegEx": "Braverman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2009}, {"title": "Top-k selection based on adaptive sampling of noisy preferences", "author": ["Busa-Fekete", "R\u00f3bert", "Szorenyi", "Balazs", "Cheng", "Weiwei", "Weng", "Paul", "H\u00fcllermeier", "Eyke"], "venue": "In Proc. of The ICML,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2013}, {"title": "Preference-based rank elicitation using statistical models: The case of mallows", "author": ["Busa-Fekete", "R\u00f3bert", "H\u00fcllermeier", "Eyke", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs"], "venue": "In Proc. of the ICML, pp. 1071\u20131079,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Pac rank elicitation through adaptive sampling of stochastic pairwise preferences", "author": ["Busa-Fekete", "R\u00f3bert", "Sz\u00f6r\u00e9nyi", "Bal\u00e1zs", "H\u00fcllermeier", "Eyke"], "venue": "In AAAI,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Computing with noisy information", "author": ["Feige", "Uriel", "Raghavan", "Prabhakar", "Peleg", "David", "Upfal", "Eli"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feige et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Feige et al\\.", "year": 1994}, {"title": "Active ranking from pairwise comparisons and when parametric assumptions don\u2019t help", "author": ["Heckel", "Reinhard", "Shah", "Nihar B", "Ramchandran", "Kannan", "Wainwright", "Martin J"], "venue": "arXiv preprint arXiv:1606.08842,", "citeRegEx": "Heckel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heckel et al\\.", "year": 2016}, {"title": "Trueskill: a bayesian skill rating system", "author": ["Herbrich", "Ralf", "Minka", "Tom", "Graepel", "Thore"], "venue": "In Proceedings of the 19th International Conference on Neural Information Processing Systems,", "citeRegEx": "Herbrich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2006}, {"title": "Top-k ranking from pairwise comparisons: When spectral ranking is optimal", "author": ["Jang", "Minje", "Kim", "Sunghyun", "Suh", "Changho", "Oh", "Sewoong"], "venue": "arXiv preprint arXiv:1603.04153,", "citeRegEx": "Jang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "Individual choice behavior: A theoretical analysis", "author": ["Luce", "R Duncan"], "venue": "Courier Corporation,", "citeRegEx": "Luce and Duncan.,? \\Q2005\\E", "shortCiteRegEx": "Luce and Duncan.", "year": 2005}, {"title": "Data structures using C : 1000 problems and solutions", "author": ["Mukherjee", "Sudipta"], "venue": null, "citeRegEx": "Mukherjee and Sudipta.,? \\Q2011\\E", "shortCiteRegEx": "Mukherjee and Sudipta.", "year": 2011}, {"title": "Rank centrality: Ranking from pairwise", "author": ["Negahban", "Sahand", "Oh", "Sewoong", "Shah", "Devavrat"], "venue": "isons. In NIPS,", "citeRegEx": "Negahban et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Negahban et al\\.", "year": 2012}, {"title": "Active exploration for learning rankings from clickthrough", "author": ["Radlinski", "Filip", "Joachims", "Thorsten"], "venue": "comparisons. Operations Research,", "citeRegEx": "Radlinski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2016}, {"title": "How does clickthrough data", "author": ["Radlinski", "Filip", "Kurup", "Madhu", "Joachims", "Thorsten"], "venue": "Proceedings of the 13th ACM SIGKDD,", "citeRegEx": "Radlinski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2007}, {"title": "A statistical convergence perspective of algorithms for rank", "author": ["Rajkumar", "Arun", "Agarwal", "Shivani"], "venue": null, "citeRegEx": "Rajkumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rajkumar et al\\.", "year": 2008}, {"title": "Efficient algorithms for adver", "author": ["Syrgkanis", "Vasilis", "Krishnamurthy", "Akshay", "Schapire", "Robert E"], "venue": "Proc. of the ICML,", "citeRegEx": "Syrgkanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2014}, {"title": "Online rank elicitation", "author": ["Sz\u00f6r\u00e9nyi", "Bal\u00e1zs", "Busa-Fekete", "R\u00f3bert", "Paul", "Adil", "H\u00fcllermeier", "Eyke"], "venue": "sarial contextual learning. arXiv preprint arXiv:1602.02454,", "citeRegEx": "Sz\u00f6r\u00e9nyi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sz\u00f6r\u00e9nyi et al\\.", "year": 2016}, {"title": "plackett-luce: A dueling bandits approach", "author": ["Urvoy", "Tanguy", "Clerot", "Fabrice", "F\u00e9raud", "Raphael", "Naamane", "Sami"], "venue": "In NIPS,", "citeRegEx": "Urvoy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Urvoy et al\\.", "year": 2015}, {"title": "k-armed voting bandits", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In Proc. of the ICML, pp. 91\u201399,", "citeRegEx": "Yue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers.", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers. Another important application is online advertising. Prominent web pages devote precious little space to advertisements, limiting companies like Google, Microsoft, or Yahoo! to present a typical user with just a couple of ads, of which the user selects at most one. Based on these small random comparisons, the company would like to rank the ads according to their appeal Radlinski & Joachims [2007], Radlinski et al.", "startOffset": 33, "endOffset": 498}, {"referenceID": 11, "context": "Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers. Another important application is online advertising. Prominent web pages devote precious little space to advertisements, limiting companies like Google, Microsoft, or Yahoo! to present a typical user with just a couple of ads, of which the user selects at most one. Based on these small random comparisons, the company would like to rank the ads according to their appeal Radlinski & Joachims [2007], Radlinski et al. [2008]. This and related applications have brought about a resurgence of interest in maximum selection and ranking using noisy comparisons.", "startOffset": 33, "endOffset": 523}, {"referenceID": 11, "context": "Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers. Another important application is online advertising. Prominent web pages devote precious little space to advertisements, limiting companies like Google, Microsoft, or Yahoo! to present a typical user with just a couple of ads, of which the user selects at most one. Based on these small random comparisons, the company would like to rank the ads according to their appeal Radlinski & Joachims [2007], Radlinski et al. [2008]. This and related applications have brought about a resurgence of interest in maximum selection and ranking using noisy comparisons. Several noise models were considered, including the popular Plackett-Luce model Plackett [1975], Luce [2005].", "startOffset": 33, "endOffset": 752}, {"referenceID": 11, "context": "Patented by Microsoft, TrueSkill Herbrich et al. [2006] is such a ranking system for Xbox gamers. Another important application is online advertising. Prominent web pages devote precious little space to advertisements, limiting companies like Google, Microsoft, or Yahoo! to present a typical user with just a couple of ads, of which the user selects at most one. Based on these small random comparisons, the company would like to rank the ads according to their appeal Radlinski & Joachims [2007], Radlinski et al. [2008]. This and related applications have brought about a resurgence of interest in maximum selection and ranking using noisy comparisons. Several noise models were considered, including the popular Plackett-Luce model Plackett [1975], Luce [2005]. Yet even for such specific models, the complexity of maximum selection was known only up to a log n factor and the complexity of ranking was known", "startOffset": 33, "endOffset": 765}, {"referenceID": 9, "context": "[2012, 2016], Jang et al. [2016] where we cannot choose the comparison pairs, and adaptive or online where the comparison pairs are selected sequentially based on previous results.", "startOffset": 14, "endOffset": 33}, {"referenceID": 9, "context": "[2012, 2016], Jang et al. [2016] where we cannot choose the comparison pairs, and adaptive or online where the comparison pairs are selected sequentially based on previous results. In this paper we focus on the latter. We specify the desired output via the (\u01eb, \u03b4)-PAC paradigm Yue & Joachims [2011], Busa-Fekete et al.", "startOffset": 14, "endOffset": 299}, {"referenceID": 6, "context": "We specify the desired output via the (\u01eb, \u03b4)-PAC paradigm Yue & Joachims [2011], Busa-Fekete et al. [2014b] that requires the output to likely closely approximate the intended value.", "startOffset": 81, "endOffset": 108}, {"referenceID": 6, "context": "[2013], Busa-Fekete et al. [2014b,b] assume no underlying ranking or constraints on probabilities and find ranking based on Copeland, Borda count and Random Walk procedures. Urvoy et al. [2013], Busa-Fekete et al.", "startOffset": 8, "endOffset": 194}, {"referenceID": 6, "context": "[2013], Busa-Fekete et al. [2014b,b] assume no underlying ranking or constraints on probabilities and find ranking based on Copeland, Borda count and Random Walk procedures. Urvoy et al. [2013], Busa-Fekete et al. [2014b] showed that if the probabilities p(i, j) are not constrained, both maximum selection and ranking problems require \u0398(n2) comparisons.", "startOffset": 8, "endOffset": 222}, {"referenceID": 6, "context": "[2013], Busa-Fekete et al. [2014b,b] assume no underlying ranking or constraints on probabilities and find ranking based on Copeland, Borda count and Random Walk procedures. Urvoy et al. [2013], Busa-Fekete et al. [2014b] showed that if the probabilities p(i, j) are not constrained, both maximum selection and ranking problems require \u0398(n2) comparisons. Several models have therefore been considered to further constrain the probabilities. Under the assumptions of strong stochastic transitivity and triangle inequality , Yue & Joachims [2011] derived a PACmaximum selection algorithm that usesO ( n \u01eb2 log n \u01eb\u03b4 )", "startOffset": 8, "endOffset": 545}, {"referenceID": 11, "context": "Sz\u00f6r\u00e9nyi et al. [2015] derived a PAC ranking algorithm for PL-model distributions that requires O( n \u01eb2 log n log n \u03b4\u01eb ) comparisons.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Sz\u00f6r\u00e9nyi et al. [2015] derived a PAC ranking algorithm for PL-model distributions that requires O( n \u01eb2 log n log n \u03b4\u01eb ) comparisons. In addition to PAC paradigm, Yue & Joachims [2011] also considered this problem under the bandit setting and bounded the regret of the resulting dueling bandits problem.", "startOffset": 0, "endOffset": 185}, {"referenceID": 11, "context": "Syrgkanis et al. [2016] looked at similar formulation.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Another non-PAC approach by Busa-Fekete et al. [2014a], Feige et al.", "startOffset": 28, "endOffset": 55}, {"referenceID": 2, "context": "Another non-PAC approach by Busa-Fekete et al. [2014a], Feige et al. [1994] solves the maximum selection and ranking problems.", "startOffset": 28, "endOffset": 76}, {"referenceID": 0, "context": "For example, Acharya et al. [2014a, 2016, 2014b] considered adversarial sorting with applications to density estimation and Ajtai et al. [2015] considered the same with deterministic algorithms.", "startOffset": 13, "endOffset": 144}, {"referenceID": 0, "context": "For example, Acharya et al. [2014a, 2016, 2014b] considered adversarial sorting with applications to density estimation and Ajtai et al. [2015] considered the same with deterministic algorithms. Mallows stochastic model Busa-Fekete et al. [2014a] does not satisfy the stochastic triangle inequality and hence our theoretical guarantees do not hold under this model.", "startOffset": 13, "endOffset": 247}, {"referenceID": 9, "context": "Using the lower bound in Feige et al. [1994], it can be inferred that the best PAC maximum selection algorithm requires \u03a9 ( n \u01eb2 log 1\u03b4 )", "startOffset": 25, "endOffset": 45}, {"referenceID": 20, "context": "comparisons for \u03b4 = 1 n where as Sz\u00f6r\u00e9nyi et al. [2015] uses O ( n(log n)2/\u01eb2 )", "startOffset": 33, "endOffset": 56}, {"referenceID": 20, "context": "Furthermore Sz\u00f6r\u00e9nyi et al. [2015] provided these guarantees only under Plackett-Luce model which is more restrictive compared to ours.", "startOffset": 12, "endOffset": 35}, {"referenceID": 20, "context": "Sz\u00f6r\u00e9nyi et al. [2015] showed that their algorithm", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Interval-Binary-Search is similar to the noisy binary search algorithm in Feige et al. [1994]. It builds a binary search tree with the bins as the leaves and it does a random walk over this tree.", "startOffset": 74, "endOffset": 94}, {"referenceID": 20, "context": "Using PALPAC-AMPRR Sz\u00f6r\u00e9nyi et al. [2015] as a Rank-x algorithm with x = 1 leads to the following corollary over PL model.", "startOffset": 19, "endOffset": 42}, {"referenceID": 6, "context": "We compare the sample complexity of Knockout with that of BTM-PAC Yue & Joachims [2011], MallowsMPI Busa-Fekete et al. [2014a], and AR Heckel et al.", "startOffset": 100, "endOffset": 127}, {"referenceID": 6, "context": "We compare the sample complexity of Knockout with that of BTM-PAC Yue & Joachims [2011], MallowsMPI Busa-Fekete et al. [2014a], and AR Heckel et al. [2016]. BTM-PAC is an (\u01eb, \u03b4)PAC algorithm for the same model considered in this paper.", "startOffset": 100, "endOffset": 156}, {"referenceID": 6, "context": "We compare the sample complexity of Knockout with that of BTM-PAC Yue & Joachims [2011], MallowsMPI Busa-Fekete et al. [2014a], and AR Heckel et al. [2016]. BTM-PAC is an (\u01eb, \u03b4)PAC algorithm for the same model considered in this paper. MallowsMPI finds a Condorcet winner which exists under our general model. AR finds the maximum according to Borda scores. We also tried PLPAC Sz\u00f6r\u00e9nyi et al. [2015], developed originally for PL model but the algorithm could not meet guarantees of \u03b4 = 0.", "startOffset": 100, "endOffset": 401}, {"referenceID": 20, "context": "As noted in Sz\u00f6r\u00e9nyi et al. [2015], sample complexity of MallowsMPI gets worse as p\u0303(i, j) gets close to 0.", "startOffset": 12, "endOffset": 35}, {"referenceID": 6, "context": "As in Busa-Fekete et al. [2014a], we consider n = 10 elements and various values for \u03c6: 0.", "startOffset": 6, "endOffset": 33}], "year": 2017, "abstractText": "We consider (\u01eb, \u03b4)-PAC maximum-selection and ranking for general probabilistic models whose comparisons probabilities satisfy strong stochastic transitivity and stochastic triangle inequality. Modifying the popular knockout tournament, we propose a maximum-selection algorithm that uses O (", "creator": "LaTeX with hyperref package"}}}