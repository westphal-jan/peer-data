{"id": "1705.05933", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2017", "title": "Sub-sampled Cubic Regularization for Non-convex Optimization", "abstract": "We consider the minimization of non-convex functions that typically arise in machine learning. Specifically, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods, since it is easy to specify and verify that the bounds of the underlying functions (e.g., a first, second, and third-order) are well defined. This paper addresses these shortcomings, which we describe in this paper.\n\n\n\n\nThe best estimate is that in addition to the standard non-convex function, the probability of one of the two values of the function is proportional to the value of the first function, so that one of the two values of the function is true. In general, the probability of one of the two values of the function is less than 1.2 in the case of the two values of the function is less than 1.5 in the case of the two values of the function is less than 0.7 in the case of the two values of the function is less than 0.5 in the case of the two values of the function is less than 0.3 in the case of the two values of the function is less than 0.3 in the case of the two values of the function is less than 0.7 in the case of the two values of the function is less than 0.3 in the case of the two values of the function is less than 0.1 in the case of the two values of the function is less than 0.6 in the case of the two values of the function is less than 0.7 in the case of the two values of the function is less than 0.8 in the case of the two values of the function is less than 0.1 in the case of the two values of the function is less than 0.2 in the case of the two values of the function is less than 0.7 in the case of the two values of the function is less than 0.1 in the case of the two values of the function is less than 0.0 in the case of the two values of the function is less than 0.1 in the case of the two values of the function is less than 0.0 in the case of the two values of the function is less than 0.1 in the case of the two values of the function is less than 0.", "histories": [["v1", "Tue, 16 May 2017 21:44:44 GMT  (1031kb,D)", "https://arxiv.org/abs/1705.05933v1", "ICML 2017"], ["v2", "Fri, 23 Jun 2017 11:49:57 GMT  (3667kb,D)", "http://arxiv.org/abs/1705.05933v2", "Proceedings of the 34th International Conference on Machine Learning"], ["v3", "Sat, 1 Jul 2017 11:17:59 GMT  (3667kb,D)", "http://arxiv.org/abs/1705.05933v3", "Proceedings of the 34th International Conference on Machine Learning"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["jonas moritz kohler", "aur\u00e9lien lucchi"], "accepted": true, "id": "1705.05933"}, "pdf": {"name": "1705.05933.pdf", "metadata": {"source": "META", "title": "Sub-sampled Cubic Regularization for Non-convex Optimization", "authors": ["Jonas Moritz Kohler", "Aurelien Lucchi"], "emails": ["<jonas.kohler@student.kit.edu>,", "lien.lucchi@inf.ethz.ch>."], "sections": [{"heading": "1. Introduction", "text": "In this paper we address the problem of minimizing an objective function of the form\nx\u2217 = arg min x\u2208Rd\n[ f(x) := 1\nn n\u2211 i=1 fi(x)\n] , (1)\nwhere f(x) \u2208 C2(Rd,R) is a not necessarily convex, (regularized) loss function over n datapoints. Stochastic Gradient Descent (SGD) is a popular method to optimize this type of objective especially in the context of large-scale\n1Department of Computer Science, ETH Zurich, Switzerland. Correspondence to: Jonas Moritz Kohler <jonas.kohler@student.kit.edu>, Aurelien Lucchi <aurelien.lucchi@inf.ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nlearning when n is very large. Its convergence properties are well understood for convex functions, which arise in many machine learning applications (Nesterov, 2004). However, non-convex functions are also ubiquitous and have recently drawn a lot of interest due to the growing success of deep neural networks. Yet, non-convex functions are extremely hard to optimize due to the presence of saddle points and local minima which are not global optima (Dauphin et al., 2014; Choromanska et al., 2015). In fact, the work of (Hillar & Lim, 2013) showed that even a degree four polynomial can be NP-hard to optimize. Instead of aiming for a global minimizer, we will thus seek for a local optimum of the objective. In this regard, a lot of attention has focused on a specific type of functions known as strict saddle functions or ridable functions (Ge et al., 2015; Sun et al., 2015). These functions are characterized by the fact that the Hessian of every saddle point has a negative eigenvalue. Geometrically this means that there is a direction of negative curvature where decreasing function values can be found. Examples of strict saddle functions include dictionary learning, orthogonal tensor decomposition and generalized phase retrieval (Ge et al., 2015; Sun et al., 2015).\nIn this work, we focus our attention on trust region methods to optimize Eq. 1. These methods construct and optimize a local model of the objective function within a region whose radius depends on how well the model approximates the real objective. One of the keys for efficiency of these methods is to pick a model that is comparably easy to optimize, such as a quadratic function (Conn et al., 2000). Following the trust region paradigm, cubic regularization methods (Nesterov & Polyak, 2006; Cartis et al., 2011a) suggest finding the step sk that minimizes a cubic model of the form\nmk(sk) := f(xk) + s \u1d40 k\u2207f(xk) +\n1 2 s\u1d40kHksk + \u03c3k 3 \u2016sk\u20163 ,\n(2) where Hk := \u22072f(xk) and \u03c3k > 0 1.\n(Nesterov & Polyak, 2006) were able to show that, if the\n1In the work of (Nesterov & Polyak, 2006), \u03c3k is assumed to be the Lipschitz constant of the Hessian in which case the model defined in Eq. 2 is a global overestimator of the objective, i.e. f(x) \u2264 m(x) \u2200x \u2208 Rd. We will elaborate on the role of \u03c3k in (Cartis et al., 2011a) later on.\nar X\niv :1\n70 5.\n05 93\n3v 3\n[ cs\n.L G\n] 1\nJ ul\n2 01\n7\nstep is computed by globally minimizing the cubic model and if the Hessian Hk is globally Lipschitz continuous, Cubic regularization methods possess the best known worst case complexity to solve Eq. 1: an overall worst-case iteration count of order \u22123/2 for generating \u2016\u2207f(xk)\u2016 \u2264 , and of order \u22123 for achieving approximate nonnegative curvature. However, minimizing Eq. 2 in an exact manner impedes the performance of this method for machine learning applications as it requires access to the full Hessian matrix. More recently, (Cartis et al., 2011a) presented a method (hereafter referred to as ARC) which relaxed this requirement by assuming that one can construct an approximate Hessian Bk that is sufficiently close to Hk in the following way:\n\u2016(Bk \u2212Hk)sk\u2016 \u2264 C \u2016sk\u20162 , \u2200k \u2265 0, C > 0 (3)\nFurthermore, they showed that it is sufficient to find an approximate minimizer by applying a Lanczos method to build up evolving Krylov spaces, which can be constructed in a Hessian-free manner, i.e. by accessing the Hessians only indirectly via matrix-vector products. However there are still two obstacles for the application of ARC in the field of machine learning: (1) The cost of the Lanczos process increases linearly in n and is thus not suitable for large datasets and (2) there is no theoretical guarantee that quasiNewton approaches satisfy Eq. 3 and (Cartis et al., 2011a) do not provide any alternative approximation technique.\nIn this work, we make explicit use of the finite-sum structure of Eq. 1 by applying a sub-sampling technique in order to provide guarantees for machine learning applications. Towards this goal, we make the following contributions:\n\u2022 We provide a theoretical Hessian sampling scheme that is guaranteed to satisfy Eq. 3 with high probability.\n\u2022 We extend the analysis to inexact gradients and prove that the convergence guarantees of (Nesterov & Polyak, 2006; Cartis et al., 2011a) can be retained.\n\u2022 Since the dominant iteration cost lie in the construction of the Lanczos process and increase linearly in n, we lower the computational cost significantly by reducing the number of samples used in each iteration.\n\u2022 Finally, we provide experimental results demonstrating significant speed-ups compared to standard first and second-order optimization methods for various convex and non-convex objectives."}, {"heading": "2. Related work", "text": "Sampling techniques for first-order methods. In largescale learning, when n dmost of the computational cost of traditional deterministic optimization methods is spent\nin computing the exact gradient information. A common technique to address this issue is to use sub-sampling in order to compute an unbiased estimate of the gradient. The simplest instance is Stochastic Gradient Descent (SGD) whose convergence does not depend on the number of datapoints n. However, the variance in the stochastic gradient estimates slows its convergence down. The work of (Friedlander & Schmidt, 2012) explored a sub-sampling technique for gradient descent in the case of convex functions, showing that it is possible to maintain the same convergence rate as full-gradient descent by carefully increasing the sample size over time. Another way to recover a linear rate of convergence for strongly-convex functions is to use variance-reduced methods (Johnson & Zhang, 2013; Defazio et al., 2014; Roux et al., 2012; Hofmann et al., 2015; Daneshmand et al., 2016). Recently, the convergence of SGD and its variance-reduced counterparts has also been extended to non-convex functions (Ghadimi & Lan, 2013; Reddi et al., 2016a) but the techniques used in these papers require using a randomized sampling scheme which is different from what is typically used in practice. Furthermore, the guarantees these methods provide are only in terms of convergence to critical points. However, the work of (Ge et al., 2015; Sun et al., 2015) recently showed that SGD can achieve stronger guarantees in the case of strict saddle functions. Yet, the convergence rate has a polynomial dependency to the dimension d and the smallest eigenvalue of the Hessian which can make this method fairly impractical.\nSecond-order methods. For second-order methods, the problem of avoiding saddle points is even worse as they might be attracted by saddle points or even points of local maximizers (Dauphin et al., 2014). Another predominant issue is the computation (and perhaps storage) of the Hessian matrix, which requires O(nd2) operations as well as computing the inverse of the Hessian, which requires O(d3) computations. Quasi-Newton methods such as the well-known (L-)BFGS algorithm partially address this issue by requiring O(nd + d2) per-iteration cost (Nesterov, 2004) instead of O(nd2 + d3). An increasingly popular alternative is to use sub-sampling techniques to approximate the Hessian matrix, such as done for example in (Byrd et al., 2011) and (Erdogdu & Montanari, 2015). The latter method, named NewSamp, approximates the Hessian with a low-rank approximation which reduces the complexity per iteration to O(nd + |S|d2) with |S| being the sample size 2. Although this is a significant reduction in terms of complexity, NewSamp yields a composite convergence rate: quadratic at first but only linear near the minimizer. Unlike NewSamp, our sampling scheme yields a locally quadratic rate of convergence (as well as faster global con-\n2Note that this method still requires O(nd) computation for the gradient as it only subsamples the Hessian.\nvergence). Our analysis also does not require using exact gradients and can thus further reduce the complexity per iteration.\nCubic regularization and trust region methods. Trust region methods are among the most effective algorithmic frameworks to avoid pitfalls such as local saddle points in non-convex optimization. Classical versions iteratively construct a local quadratic model and minimize it within a certain radius wherein the model is trusted to be sufficiently similar to the actual objective function. This is equivalent to minimizing the model function with a suitable quadratic penalty term on the stepsize. Thus, a natural extension is the cubic regularization method introduced by (Nesterov & Polyak, 2006) that uses a cubic over-estimator of the objective function as a regularization technique for the computation of a step to minimize the objective function. The drawback of their method is that it requires computing the exact minimizer of Eq. 2, thus requiring the exact gradient and Hessian matrix. However finding a global minimizer of the cubic model mk(s) may not be essential in practice and doing so might be prohibitively expensive from a computational point of view. (Cartis et al., 2011a) introduced a method named ARC which relaxed this requirement by letting sk be an approximation to the minimizer. The model defined by the adaptive cubic regularization method introduced two further changes. First, instead of computing the exact Hessian Hk it allows for a symmetric approximation Bk. Second, it introduces a dynamic positive parameter \u03c3k instead of using the global Lipschitz constant L.\nThere have been efforts to further reduce the computational complexity of this problem. For example, (Agarwal et al., 2016) refined the approach of (Nesterov & Polyak, 2006) to return an approximate local minimum in time which is linear in the input representation. Similar improvements have been made by (Carmon & Duchi, 2016) and (Hazan & Koren, 2016). These methods provide alternatives to minimize the cubic model and can thus be seen as complementary to our approach. Finally, the work of (Blanchet et al., 2016) proposed a stochastic variant of a trust region method but their analysis does not specify any accuracy level required for the estimation of the stochastic Hessian. (Cartis & Scheinberg, 2017) also analyzed a probabilistic cubic regularization variant that allows approximate secondorder models but they did not provide an explicit derivation of sampling conditions."}, {"heading": "3. Formulation", "text": "We are interested in optimizing Eq. 1 in a large-scale setting when the number of datapoints n is very large such that the cost of solving Eq. 2 exactly becomes prohibitive. In this regard we identify a sampling scheme that allows us to\nretain the convergence results of deterministic trust region and cubic regularization methods, including quadratic local convergence rates and global second-order convergence guarantees as well as worst-case complexity bounds. A detailed theoretical analysis is given in Section 4. Here we shall first state the algorithm itself and elaborate further on the type of local nonlinear models we employ as well as how these can be solved efficiently."}, {"heading": "3.1. Objective function", "text": "Instead of using deterministic gradient and Hessian information as in Eq. 2, we use unbiased estimates of the gradient and Hessian constructed from two independent sets of points denoted by Sg and SB . We then construct a local cubic model that is (approximately) minimized in each iteration:\nmk(sk) := f(xk) + s \u1d40 kgk +\n1 2 s\u1d40kBksk + \u03c3k 3 \u2016sk\u20163 (4)\nwhere gk := 1|Sg| \u2211 i\u2208Sg \u2207fi(xk)\nand Bk := 1|SB | \u2211 i\u2208SB \u2207 2fi(xk).\nThe model derivative with respect to sk is defined as:\n\u2207mk(sk) = gk + Bksk + \u03bbsk ,where \u03bb = \u03c3k \u2016sk\u2016 . (5)"}, {"heading": "3.2. Algorithm", "text": "Our Sub-sampled Cubic Regularization approach (SCR) is presented in Algorithm 1. At iteration step k, we subsample two sets of datapoints from which we compute a stochastic estimate of the gradient and the Hessian. We then solve the problem in Eq. 4 approximately using the method described in Section 3.4 and update the regularization parameter \u03c3k depending on how well the model approximates the real objective. In particular, very successful steps indicate that the model is (at least locally) an adequate approximation of the objective such that the penalty parameter is decreased in order to allow for longer steps. For unsuccessful iterations we proceed exactly the opposite way. Readers familiar with trust region methods might see that one can interpret the penalty parameter \u03c3k as inversely proportional to the trust region radius \u03b4k."}, {"heading": "3.3. Exact model minimization", "text": "Solving Eq. 4 requires minimizing an unconstrained nonconvex problem that may have isolated local minima. As shown in (Cartis et al., 2011a) the global model minimizer s\u2217k is characterized by following systems of equations,\n(Bk + \u03bb \u2217 kI)s \u2217 k = \u2212gk, \u03bb\u2217k = \u03c3k \u2016s\u2217k\u2016 , (Bk + \u03bb\u2217kI) 0.\n(9)\nIn order to find a solution we can express s\u2217k := sk(\u03bb \u2217 k) = \u2212(Bk+\u03bb\u2217kI)\u22121gk, apply this in the second equation of (9)\nAlgorithm 1 Sub-sampled Cubic Regularization (SCR) 1: Input:\nStarting point x0 \u2208 Rd (e.g x0 = 0) \u03b3 > 1, 1 > \u03b72 > \u03b71 > 0, and \u03c30 > 0\n2: for k = 0, 1, . . . , until convergence do 3: Sample gradient gk and Hessian Hk according to Eq. 17 & Eq. 19 respectively 4: Obtain sk by solving mk(sk) (Eq. 4) such that A1 holds 5: Compute f(xk + sk) and\n\u03c1k = f(xk)\u2212 f(xk + sk) f(xk)\u2212mk(sk)\n(6)\n6: Set\nxk+1 = { xk + sk if \u03c1k \u2265 \u03b71 xk otherwise\n(7)\n7: Set\n\u03c3k+1 =  max{min{\u03c3k, \u2016gk\u2016}, m} if \u03c1k > \u03b72 (very successful iteration) \u03c3k if \u03b72 \u2265 \u03c1k \u2265 \u03b71 (successful iteration) \u03b3\u03c3k otherwise (unsuccessful iteration),\n(8)\nwhere m \u2248 10\u221216 is the relative machine precision. 8: end for\nand obtain a univariate, nonlinear equation in \u03bbk\u2225\u2225\u2212(Bk + \u03bb\u2217kI)\u22121gk\u2225\u2225\u2212 \u03bb\u2217k\u03c3k = 0. (10) Furthermore, we need \u03bb\u2217k \u2265 max{\u2212\u03bb1(Bk), 0}, where \u03bb1(Bk) is the leftmost eigenvalue of Bk, in order to guarantee the semi-positive definiteness of (Bk + \u03bb\u2217kI).\nThus, computing the global solution of mk boils down to finding the root of Eq. 10 in the above specified range of \u03bbk. The problem can be solved by Newton\u2019s method, which involves factorizing Bk +\u03bbkI for various \u03bbk and is thus prohibitively expensive for large problem dimensions d. See Section 6.2 in (Cartis et al., 2011a) for more details. In the following Section we instead explore an approach to approximately minimize the model while retaining the convergence guarantees of the exact minimization."}, {"heading": "3.4. Approximate model minimization", "text": "(Cartis et al., 2011a) showed that it is possible to retain the remarkable properties of the cubic regularization algorithm with an inexact model minimizer. A necessary condition is that sk satisfies the two requirements stated in A1.\nAssumption 1 (Approximate model minimizer).\ns\u1d40kgk + s \u1d40 kBksk + \u03c3k \u2016sk\u2016 3 = 0 (11)\ns\u1d40kBksk + \u03c3k \u2016sk\u2016 3 \u2265 0 (12)\nNote that the first equation is equal to \u2207smk(sk)\u1d40sk = 0 and the second to s\u1d40k\u22072smk(sk)sk \u2265 0.\nAs shown in (Cartis et al., 2011a) Lemma 3.2, the global minimizer of mk(sk) in a Krylov subspace Kk := span{gk,Hkgk,H2kgk, ...} satisfies this assumption independent of the subspace dimension. This comes in handy, as minimizing mk in the Krylov subspace only involves factorizing a tri-diagonal matrix, which can be done at the cost of O(d). However, a Lanczos-type method must be used in order to build up an orthogonal basis of this subspace which typically involves one matrix-vector product (O((2d \u2212 1)n)) per additional subspace dimension (see Chapter 5 in (Conn et al., 2000) for more details).\nThus, in order to keep the per iteration cost of SCR low and in accordance to ARC, we apply the following termination criterion to the Lanczos process in the hope to find a suitable trial step before Kk is of dimensionality d.\nAssumption 2 (Termination Criteria). For each outer iteration k, assume that the Lanczos process stops as soon as some Lanczos iteration i satisfies the criterion\nTC: \u2016\u2207mk(si,k)\u2016 \u2264 \u03b8k \u2016gk\u2016 , (13)\nwhere \u03b8k = \u03ba\u03b8 min(1, \u2016si,k\u2016), \u03ba\u03b8 \u2208 (0, 1).\nHowever, we argue that especially for high dimensional problems, the cost of the Lanczos process may significantly slow down cubically regularized methods and since this cost increases linearly in n, carefully sub-sampled versions are an attractive alternative."}, {"heading": "4. Theoretical analysis", "text": "In this section, we provide the convergence analysis of SCR. For the sake of brevity, we assume Lipschitz continuous Hessians right away but note that a superlinear local convergence result as well as the global first-order convergence theorem can both be obtained without the former assumption.\nFirst, we lay out some critical assumptions regarding the gradient and Hessian approximations. Second, we show that one can theoretically satisfy these assumptions with high probability by sub-sampling first- and second-order information. Third, we give a condensed convergence analysis of SCR which is widely based on (Cartis et al., 2011a), but adapted for the case of stochastic gradients. There, we show that the local and global convergence properties of ARC can be retained by sub-sampled versions at the price of slightly worse constants."}, {"heading": "4.1. Assumptions", "text": "Assumption 3 (Continuity). The functions fi \u2208 C2(Rd), gi and Hi are Lipschitz continuous for all i, with Lipschitz constants \u03baf , \u03bag and \u03baH respectively.\nBy use of the triangle inequality, it follows that these assumptions hold for all g and H, independent of the sample size. Furthermore, note that the Hessian and gradient norms are uniformly bounded as a consequence of A3.\nIn each iteration, the Hessian approximation Bk shall satisfy condition AM.4 from (Cartis et al., 2011a), which we restate here for the sake of completeness. Assumption 4 (Sufficient Agreement of H and B).\n\u2016(Bk \u2212H(xk))sk\u2016 \u2264 C \u2016sk\u20162 , \u2200k \u2265 0, C > 0. (14)\nWe explicitly stress the fact that this condition is stronger than the well-known Dennis More\u0301 Condition. While quasiNewton approximations satisfy the latter, there is no theoretical guarantee that they also satisfy the former (Cartis et al., 2011a). Furthermore, any sub-sampled gradient shall satisfy the following condition. Assumption 5 (Sufficient Agreement of\u2207f and g).\n\u2016\u2207f(xk)\u2212 g(xk)\u2016 \u2264M \u2016sk\u20162 , \u2200k \u2265 0, M > 0. (15)"}, {"heading": "4.2. Sampling Conditions", "text": "Based on probabilistic deviation bounds for random vectors and matrices3, we now present sampling conditions\n3These bounds have lately become popular under the name of concentration inequalities. Unlike classic limit theorems, such as the Central Limit Theorem, concentration inequalities are specifically attractive for application in machine learning because of their non-asymptotic nature.\nthat guarantee sufficient steepness and curvature information in each iteration k. In particular, the Bernstein inequality gives exponentially decaying bounds on the probability of a random variable to differ by more than from its mean for any fixed number of samples. We use this inequality to upper bound the `2-norm distance \u2016\u2207f \u2212 g\u2016, as well as the spectral-norm distance \u2016B \u2212H\u2016 by quantities involving the sample size |S|. By applying the resulting bounds in the sufficient agreement assumptions (A4 & A5) and rearranging for |S|, we are able to translate the latter into concrete sampling conditions."}, {"heading": "4.2.1. GRADIENT SAMPLING", "text": "As detailed in the Appendix, the following Lemma arises from the Vector Bernstein Inequality.\nLemma 6 (Gradient deviation bound). Let the sub-sampled gradient gk be defined as in Eq. 4. For \u2264 2\u03baf we have with probability (1\u2212 \u03b4) that\n\u2016g(xk)\u2212\u2207f(xk)\u2016 \u2264 4 \u221a 2\u03baf\n\u221a log((2d)/\u03b4) + 1/4\n|Sg,k| .\n(16)\nIt constitutes a non-asymptotic bound on the deviation of the gradient norms that holds with high probability. Note how the accuracy of the gradients increases in the sample size. This bound yields the following condition.\nTheorem 7 (Gradient Sampling). If\n|Sg,k| \u2265 32\u03ba2f (log ((2d)/\u03b4) + 1/4)\nM2 \u2016sk\u20164 , M \u2265 0,\u2200k \u2265 0\n(17) then gk satisfies the sufficient agreement condition A5 with probability (1\u2212 \u03b4)."}, {"heading": "4.2.2. HESSIAN SAMPLING", "text": "In analogy to the gradient case, we use the matrix version of Bernstein\u2019s Inequality to derive the following Lemma.\nLemma 8 (Hessian deviation bound). Let the sub-sampled Hessian B be defined as in Eq. 4. For \u2264 4\u03bag we have with probability (1\u2212 \u03b4) that\n\u2016B(xk)\u2212H(xk)\u2016 \u2264 4\u03bag\n\u221a log(2d/\u03b4)\n|SB,k| , (18)\nThis, in turn, can be used to derive a Hessian sampling condition that is guaranteed to satisfy the sufficient agreement condition (A4) with high probability.\nTheorem 9 (Hessian Sampling). If\n|SB,k| \u2265 16\u03ba2g log(2d/\u03b4)\n(C \u2016sk\u2016)2 , C \u2265 0, and \u2200k \u2265 0 (19)\nthen Bk satisfies the strong agreement condition A4 with probability (1\u2212 \u03b4).\nAs expected, the required sample size grows in the problem dimensionality d and in the Lipschitz constants \u03baf and \u03bag . Finally, as outlined in the Appendix (Lemma 24), the samples size is eventually equal to the full sample size n as SCR converges and thus we have\ng\u2192 \u2207f as well as B\u2192 H as k \u2192\u221e. (20)"}, {"heading": "4.3. Convergence Analysis", "text": "The entire analysis of cubically regularized methods is prohibitively lengthy and we shall thus establish only the crucial properties that ensure global, as well as fast local convergence and improve the worst-case complexity of these methods over standard trust region approaches. Next to the cubic regularization term itself, these properties arise mainly from the penalty parameter updates and step acceptance criteria of the ARC framework, which give rise to a good relation between regularization and stepsize. Further details can be found in (Cartis et al., 2011a)."}, {"heading": "4.3.1. PRELIMINARY RESULTS", "text": "First, we note that the penalty parameter sequence {\u03c3k} is guaranteed to stay within some bounded positive range, which is essentially due to the fact that SCR is guaranteed to find a successful step as soon as the penalty parameter exceeds some critical value \u03c3sup.\nLemma 10 (Boundedness of \u03c3k). Let A3, A4 and A5 hold. Then\n\u03c3k \u2208 [\u03c3inf , \u03c3sup], \u2200k \u2265 0, (21)\nwhere \u03c3inf is defined in Step 7 of Algorithm 1 and\n\u03c3sup := {\u03c30, 3\n2 \u03b32(2M + C + \u03bag)}. (22)\nFurthermore, for any successful iteration the objective decrease can be directly linked to the model decrease via the step acceptance criterion in Eq. 8. The latter, in turn, can be shown to be lower bounded by the stepsize which combined gives the following result.\nLemma 11 (Sufficient function decrease). Suppose that sk satisfies A1. Then, for all successful iterations k \u2265 0\nf(xk)\u2212 f(xk+1) \u2265 \u03b71(f(xk)\u2212m(sk))\n\u2265 1 6 \u03b71\u03c3inf \u2016sk\u20163\n(23)\nFinally, the termination criterion (13) also guarantees step sizes that do not become too small compared to the respective gradient norm which leads to the following Lemma. Lemma 12 (Sufficiently long steps). Let A3, A4 and A5 hold. Furthermore, assume the termination criterion TC (A2) and suppose that xk \u2192 x\u2217, as k \u2192 \u221e. Then, for all sufficiently large successful iterations, sk satisfies\n\u2016sk\u2016 \u2265 \u03bas \u221a \u2016\u2207f(xk+1)\u2016 (24)\nwhere \u03bas is the positive constant\n\u03bas =\n\u221a 1\u2212 \u03ba\u03b8\n1 2\u03bag + (1 + \u03ba\u03b8\u03bag)M + C + \u03c3sup + \u03ba\u03b8\u03bag\n. (25)"}, {"heading": "4.3.2. LOCAL CONVERGENCE", "text": "We here provide a proof of local convergence for any sampling scheme that satisfies the conditions presented in Theorem 7 and Theorem 9 as well as the additional condition that the sample size does not decrease in unsuccessful iterations. We show that such sampling schemes eventually yield exact gradient and Hessian information. Based upon this observation, we obtain the following local convergence result (as derived in the Appendix).\nTheorem 13 (Quadratic local convergence). Let A3 hold and assume that gk and Bk are sampled such that 17 and 19 hold and |Sg,k| and |SB,k| are not decreased in unsuccessful iterations. Furthermore, let sk satisfy A1 and\nxk \u2192 x\u2217, as k \u2192\u221e, (26)\nwhere H(x\u2217) is positive definite. Moreover, assume the stopping criterion TC (A2). Then,\n\u2016xk+1 \u2212 x\u2217\u2016 \u2016xk \u2212 x\u2217\u20162 \u2264 c, c > 0 as k \u2192\u221e (w.h.p.). (27)\nThat is, xk converges in q-quadratically to x\u2217 as k \u2192 \u221e with high probability."}, {"heading": "4.3.3. GLOBAL CONVERGENCE TO FIRST-ORDER CRITICAL POINT", "text": "Lemma 10 and 11 allow us to lower bound the function decrease of a successful step in terms of the full gradient\u2207fk\n(as we will shorty detail in Eq. 31). Combined with Lemma 10, this allows us to give deterministic global convergence guarantees using only stochastic first order information.\nTheorem 14 (Convergence to 1st-order Critical Points). Let A1, A3, A4 and A5 hold. Furthermore, let {f(xk)} be bounded below by some finf > \u2212\u221e. Then\nlim k\u2192\u221e\n\u2016\u2207f(xk)\u2016 = 0 (28)"}, {"heading": "4.3.4. GLOBAL CONVERGENCE TO SECOND-ORDER CRITICAL POINT", "text": "Unsurprisingly, the second-order convergence guarantee relies mainly on the use of second-order information so that the stochastic gradients do neither alter the result nor the proof as it can be found in Section 5 of (Cartis et al., 2011a). We shall restate it here for the sake of completeness.\nTheorem 15 (Second-order global convergence). Let A3, A4 and A5 hold. Furthermore, let {f(xk)} be bounded below by finf and sk be a global minimizer of mk over a subspaceLk that is spanned by the columns of the d\u00d7 l orthogonal matrix Qk. As B\u2192 H asymptotically (Eq. 20), any subsequence of negative leftmost eigenvalues {\u03bbmin(Q\u1d40kH(xk)Qk)} converges to zero for sufficiently large, successful iterations. Hence\nlim k\u2208S inf k\u2192\u221e\n\u03bbmin(Q \u1d40 kH(xk)Qk) \u2265 0. (29)\nFinally, if Qk becomes a full orthogonal basis of Rd as k \u2192 \u221e, then any limit point of the sequence of successful iterates {xk} is second-order critical (provided such a limit point exists)."}, {"heading": "4.3.5. WORST-CASE ITERATION COMPLEXITY", "text": "For the worst-case analysis we shall establish the two disjoint index sets Uj and Sj , which represent the un- and successful SCR iterations that have occurred up to some iteration j > 0, respectively. As stated in Lemma 10 the penalty parameter \u03c3k is bounded above and hence SCR may only take a limited number of consecutive unsuccessful steps. As a consequence, the total number of unsuccessful iterations is at most a problem dependent constant times the number of successful iterations.\nLemma 16 (Number of unsuccessful iterations). For any\nfixed j \u2265 0, let Lemma 10 hold. Then we have that |Uj | \u2264 \u2308 (|Sj |+ 1) log(\u03c3sup)\u2212 log(\u03c3inf)\nlog(\u03b71)\n\u2309 . (30)\nRegarding the number of successful iterations we have already established the two key ingredients: (i) a sufficient function decrease in each successful iteration (Lemma 11) and (ii) a step size that does not become too small compared to the respective gradient norm (Lemma 12), which is essential to driving the latter below at a fast rate. Combined they give rise to the guaranteed function decrease for successful iterations\nf(xk)\u2212 f(xk+1) \u2265 1\n6 \u03b71\u03c3inf\u03ba\n3 s \u2016\u2207f(xk+1)\u2016 3/2 , (31)\nwhich already contains the power of 3/2 that appears in the complexity bound. Finally, by summing over all successful iterations one obtains the following, so far best know, worst-case iteration bound to reach first-order criticality.\nTheorem 17 (First-order worst-case complexity). Let A1, A3, A4 and A5 hold. Furthermore, be {f(xk)} bounded below by finf and TC applied (A2). Then, for > 0 the total number of iterations SCR takes to generate the first iterate j with \u2016\u2207f(xj+1)\u2016 \u2264 , and assuming \u2264 1, is\nj \u2264 \u2308 (1 + \u03bai)(2 + \u03baj) \u22123/2 \u2309 , (32)\nwhere\n\u03bai = 6 f(x0)\u2212 finf \u03b71\u03c3inf\u03ba3s and \u03baj = log(\u03c3sup)\u2212 log(\u03c3inf)\nlog(\u03b71) (33)\nNote that the constants \u03bai and \u03baj involved in this upper bound both increase in the gradient inaccuracy M and the Hessian inaccuracy C (via \u03bas and \u03c3sup), such that more inaccuracy in the sub-sampled quantities may well lead to an increased overall number of iterations.\nFinally, we want to point out that similar results can be established regarding a second-order worst-case complexity bound similar to Corollary 5.5 in (Cartis et al., 2011b), which we do not prove here for the sake of brevity."}, {"heading": "5. Experimental results", "text": "In this section we present experimental results on realworld datasets where n d 1. They largely confirm the analysis derived in the previous section. Please refer to the Appendix for more detailed results and experiments on higher dimensional problems."}, {"heading": "5.1. Practical implementation of SCR", "text": "We implement SCR as stated in Algorithm 1 and note the following details. Following (Erdogdu & Montanari, 2015), we require the sampling conditions derived in Section 4 to hold with probability O(1 \u2212 1/d), which yields the following practically applicable sampling schemes\n|Sk,H | \u2265 36\u03ba2g log(d)\n(C \u2016sk\u2016)2 , C > 0, \u2200k > 0\n|Sk,g| \u2265 32\u03ba2f (log(d) + 1/4)\nM2\u2016sk\u20164 , M > 0, \u2200k > 0.\n(34)\nThe positive constants C and M can be used to scale the sample size to a reasonable portion of the entire dataset and can furthermore be used to offset \u03bag and \u03baf , which are generally expensive to obtain.\nHowever, when choosing |S| for the current iteration k, the stepsize sk is yet to be determined. Based on the Lipschitz continuity of the involved functions, we argue that the previous stepsize is a fair estimator of the current one and this is confirmed by experimental results. Finally, we would like to point out that the sampling schemes derived in Eq. 34 gives our method a clear edge over sampling schemes that do not take any iteration information into account, e.g. linearly or geometrically increased samples."}, {"heading": "5.2. Baselines and datasets", "text": "We compare SCR to various optimization methods presented in Section 2. This includes SGD (with constant step-size), SAGA, Newton\u2019s method, BFGS, L-BFGS and ARC. More details concerning the choice of the hyper-\nparameters are provided in the appendix. We ran experiments on the datasets a9a, covtype and higgs (see details in the appendix). We experimented with a binary logistic regression model with two different regularizers: a standard `2 penalty \u03bb\u2016x\u20162, and a non-convex regularizer \u03bb \u2211d i=1 x 2 (i)/ ( 1 + x2(i) ) (see (Reddi et al., 2016b))."}, {"heading": "5.3. Results", "text": "The results in Figure 1 confirm our intuition that SCR can reduce ARCs computation time without losing its global convergence property. Newton\u2019s method is the closest in terms of performance. However, it suffer heavily from an increase in d as can be seen by additional results provided in the appendix. Furthermore, it cannot optimize the non-convex version of covtype due to a singular Hessian. Notably, BFGS terminates early on the non-convex higgs dataset due to a local saddle point. Finally, the high condition number of covtype has a significant effect on the performance of SGD, SAGA and L-BFGS."}, {"heading": "6. Conclusion", "text": "In this paper we proposed a sub-sampling technique to estimate the gradient and Hessian in order to construct a cubic model analogue to trust region methods. We show that this method exhibits the same convergence properties as its deterministic counterpart, which are the best known worstcase convergence properties on non-convex functions. Our proposed method is especially interesting in the large scale regime when n d. Numerical experiments on both real and synthetic datasets demonstrate the performance of the\nproposed algorithm which we compared to its deterministic variant as well as more classical optimization methods. As future work we would like to explore the adequacy of our method to train neural networks which are known to be hard to optimize due to the presence of saddle points."}, {"heading": "A. Appendix", "text": ""}, {"heading": "A.1. Concentration Inequalities and Sampling Schemes", "text": "For the sake of simplicity we shall drop the iteration subscript k in the following results of this section."}, {"heading": "A.1.1. GRADIENT SAMPLING", "text": "First, we extend the Vector Bernstein inequality as it can be found in (Gross, 2011) to the average of independent, zeromean vector-valued random variables.\nLemma 18 (Vector Bernstein Inequality). Let x1, . . . ,xn be independent vector-valued random variables with common dimension d and assume that each one is centered, uniformly bounded and also the variance is bounded above:\nE [xi] = 0 and \u2016xi\u20162 \u2264 \u00b5 as well as E [ \u2016xi\u20162 ] \u2264 \u03c32\nLet\nz = 1\nn n\u2211 i=1 xi.\nThen we have for 0 < < \u03c32/\u00b5\nP ( \u2016z\u2016 \u2265 ) \u2264 exp ( \u2212n \u00b7 2\n8\u03c32 +\n1\n4\n) (35)\nProof: Theorem 6 in (Gross, 2011) gives the following Vector Bernstein inequality for independent, zero-mean vectorvalued random variables\nP (\u2225\u2225\u2225\u2225\u2225 n\u2211 n=1 xi \u2225\u2225\u2225\u2225\u2225 \u2265 t+\u221aV ) \u2264 exp ( \u2212 t 2 4V ) , (36)\nwhere V = \u2211n i=1 E [ \u2016xi\u20162 ] is the sum of the variances of the centered vectors xi. First, we shall define = t+ \u221a V , which allows us to rewrite the above equation as\nP (\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 \u2265 ) \u2264 exp \u22121 4 ( \u2212 \u221a V\u221a V )2 = exp(\u22121 4 ( \u221a V \u2212 1 )2) . (37)\nBased on the observation that\n\u2212 1 4 ( \u221a V \u2212 1 )2 \u2264 \u22121 4 ( 2 2V ) + 1 4\n\u21d4 \u2212 2\nV + 2 \u221a V \u2212 1 \u2264 \u2212\n2\n2V + 1\n\u21d4 0 \u2264 2\n2V \u2212 2 \u221a V + 2\n\u21d4 0 \u2264 (\n\u221a 2V \u2212 \u221a 2\n)2 (38)\nalways holds, we can formulate a slightly weaker Vector Bernstein version as follows\nP (\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 \u2265 ) \u2264 exp ( \u2212 2 8V + 1 4 ) . (39)\nSince the individual variance is assumed to be bounded above, we can write\nV = n\u2211 i=1 E [ \u2016xi\u20162 ] \u2264 n\u03c32. (40)\nThis term also constitutes an upper bound on the variance of y = \u2211n i=1 xi, because the xi are independent and thus\nuncorrelated . However, z = 1n \u2211n i=1 xi and we must account for the averaging term. Since the xi are centered we have E [z] = 0, and thus\nV ar(z) = E [ \u2016z\u2212 E [z]\u20162 ] = E [ \u2016z\u20162 ] = E \u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 2  = 1 n2 E ( n\u2211 i=1 xi )\u1d40 n\u2211 j=1 xj  = 1\nn2 E \u2211 i,j ( x\u1d40jxi ) = 1 n2 \u2211 i,j E [( x\u1d40jxi )] = 1 n2  n\u2211 i=1 E [(x\u1d40i xi)] + n\u2211 i=1 n\u2211 j 6=i E [(x\u1d40i xj)]  = 1\nn2 n\u2211 i=1 E [ \u2016xi\u20162 ] \u2264 1 n \u03c32,\n(41)\nwhere we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E [ x\u1d40jxi ] = 0, j 6= i because of the independence assumption. Hence, we can bound the term V \u2264 1n\u03c3 2 for the random vector sum z.\nNow, since n > 1 and > 0, as well as P (z > a) is falling in a and exp(\u2212x) falling in x, we can use this upper bound on the variance of z in (39), which gives the desired inequality\nP ( \u2016z\u2016 \u2265 ) \u2264 exp ( \u2212n \u00b7 2\n8\u03c32 +\n1\n4\n) (42)\nThis result was applied in order to find the probabilistic bound on the deviation of the sub-sampled gradient from the full gradient as stated in Lemma 6, for which we will give the proof next."}, {"heading": "Proof of Lemma 6:", "text": "To apply vector Bernstein\u2019s inequality (35) we need to center the gradients. Thus we define\nxi = gi(x)\u2212\u2207f(x), i = 1, . . . , |Sg| (43)\nand note that from the Lipschitz continuity of f (A3), we have\n\u2016xi\u2016 = \u2016gi(x)\u2212\u2207f(x)\u2016 \u2264 \u2016gi(x)\u2016+ \u2016\u2207f(x)\u2016 \u2264 2\u03baf and \u2016xi\u20162 \u2264 4\u03ba2f , i = 1, . . . , |Sg|. (44)\nWith \u03c32 := 4\u03ba2f and\nz = 1 |Sg| \u2211 i\u2208Sg xi = 1 |Sg| \u2211 i\u2208Sg gi(x)\u2212 1 |Sg| \u2211 i\u2208Sg \u2207f(x) = g(x)\u2212\u2207f(x) (45)\nin equation (35), we can require the probability of a deviation larger or equal to to be lower than some \u03b4 \u2208 (0, 1]\nP ( \u2016g(x)\u2212\u2207f(x)\u2016 > ) \u22642d exp ( \u2212|Sg| \u00b7 2\n32\u03ba2f +\n1\n4\n) ! \u2264 \u03b4\n\u21d4|Sg| \u00b7 2 32\u03ba2f \u2212 1 4 ! \u2265 log((2d)/\u03b4)\n\u21d4 \u2265 4 \u221a\n2\u03baf\n\u221a log ((2d)/\u03b4) + 1/4\n|Sg| .\n(46)\nConversely, the probability of a deviation of\n< 4 \u221a\n2\u03baf\n\u221a log ((2d)/\u03b4) + 1/4\n|Sg| (47)\nis higher or equal to 1\u2212 \u03b4.\nOf course, any sampling scheme that guarantees the right hand side of (16) to be smaller or equal to M times the squared step size, directly satisfies the sufficient gradient agreement condition (A5). Consequently, plugging the former into the latter and rearranging for the sample size gives Theorem 7 as we shall prove now."}, {"heading": "Proof of Theorem 7:", "text": "By use of Lemma 6 we can write \u2016g(x)\u2212\u2207f(x)\u2016 \u2264M \u2016s\u20162\n\u21d4 4 \u221a\n2\u03baf\n\u221a log(1/\u03b4 + 1/4)\n|Sg| \u2264M \u2016s\u20162\n|Sg| \u2265 32\u03ba2f log (1/\u03b4 + 1/4)\nM2 \u2016s\u20164\n(48)"}, {"heading": "A.1.2. HESSIAN SAMPLING", "text": "Lemma 19 (Matrix Bernstein Inequality). Let A1, ..,An be independent random Hermitian matrices with common dimension d\u00d7 d and assume that each one is centered, uniformly bounded and also the variance is bounded above:\nE [Ai] = 0 and \u2016Ai\u20162 \u2264 \u00b5 as well as \u2225\u2225E [A2i ]\u2225\u22252 \u2264 \u03c32\nIntroduce the sum\nZ = 1\nn n\u2211 i=1 Ai\nThen we have\nP ( \u2016Z\u2016 \u2265 ) \u2264 2d \u00b7 exp ( \u2212n \u00b7min{ 2\n4\u03c32 , 2\u00b5 } )\n(49)\nProof: Theorem 12 in (Gross, 2011) gives the following Operator-Bernstein inequality\nP (\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 Ai \u2225\u2225\u2225\u2225\u2225 \u2265 ) \u2264 2d \u00b7 exp ( min{ 2 4V , 2\u00b5 } ) , (50)\nwhere V = n\u03c32. As well shall see, this is an upper bound on the variance of Y = \u2211n i=1 Ai since the Ai are independent and have an expectation of zero (E [Y ] = 0).\nV ar(Y) = \u2225\u2225\u2225E [Y2]\u2212 E [Y]2\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225E [ ( \u2211 i Ai) 2 ]\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225E \u2211 i,j AiAj \u2225\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i,j E [AiAj ] \u2225\u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i E [AiAi] + \u2211 i \u2211 j 6=i E [AiAj ] \u2225\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225\u2211 i E [ A2i ]\u2225\u2225\u2225\u2225\u2225 \u2264\u2211 i \u2225\u2225E [A2i ]\u2225\u2225 \u2264 n\u03c32, (51)\nwhere we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E [AjAi] = 0, j 6= i because of the independence assumption.\nHowever, Z = 1n \u2211n i=1 Ai and thus\nV ar(Z) = \u2225\u2225E [Z2]\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225E [ ( 1 n n\u2211 i=1 Ai) 2 ]\u2225\u2225\u2225\u2225\u2225 = 1n2 \u2225\u2225\u2225\u2225\u2225E [ ( n\u2211 i=1 Ai) 2 ]\u2225\u2225\u2225\u2225\u2225 \u2264 1n\u03c32. (52) Hence, we can bound V \u2264 1n\u03c3\n2 for the average random matrix sum Z. Furthermore, since n > 1 and , \u00b5 > 0 as well as exp(\u2212\u03b1) decreasing in \u03b1 \u2208 R we have that\nexp ( \u2212\n2\u00b5\n) \u2264 exp ( \u2212 n2\u00b5 ) . (53)\nTogether with the Operator-Bernstein inequality, (52) and (53) give the desired inequality (49).\nThis result exhibits that sums of independent random matrices provide normal concentration near its mean in a range determined by the variance of the sum. We apply it in order to derive the bound on the deviation of the sub-sampled Hessian from the full Hessian as stated in Lemma 8, which we shall prove next.\nProof of Lemma 8: Bernstein\u2019s Inequality holds as f \u2208 C2 and thus the Hessian is symmetric by Schwarz\u2019s Theorem. Since the expectation of the random matrix needs to be zero, we center the individual Hessians,\nXi = Hi(x)\u2212H(x), i = 1, ..., |SH |\nand note that now from the Lipschitz continuity of g (A3):\n\u2016Xi\u20162 \u2264 2\u03bag, i = 1...|SH | and \u2225\u2225X2i\u2225\u22252 \u2264 4\u03ba2g, i = 1...|SH |.\nHence, for \u2264 4\u03bag , we are in the small deviation regime of Bernstein\u2019s bound with a sub-gaussian tail. Then, we may plug\n1\n|SH | |SH |\u2211 i=1 Xi = B(x)\u2212H(x)\ninto (49), to get\nP ( \u2016B(x)\u2212H(x)\u2016 \u2265 ) \u2264 2d \u00b7 exp ( \u2212\n2|SH | 16\u03ba2g\n) . (54)\nFinally, we shall require the probability of a deviation of or higher to be lower than some \u03b4 \u2208 (0, 1]\n2d \u00b7 exp ( \u2212\n2|SH | 16\u03ba2g\n) ! = \u03b4\n\u21d4 \u2212 2|SH | 16\u03ba2g = log(\u03b4/2d)\n\u21d4 = 4\u03bag\n\u221a log(2d/\u03b4)\n|SH | ,\n(55)\nwhich is equivalent to \u2016B(x)\u2212H(x)\u2016 staying within this particular choice of with probability (1 \u2212 \u03b4), generally perceived as high probability.\nProof of Theorem 9: Since \u2016Av\u2016 \u2264 \u2016A\u2016op\u2016v\u2016 for every v \u2208 V we have for the choice of the spectral matrix norm and euclidean vector norm that any B that fulfils \u2016(B(x)\u2212H(x))\u2016 \u2264 C \u2016s\u2016 also satisfies condition A4. Furthermore\n\u2016(B\u2212H(x))\u2016 \u2264 C \u2016s\u2016\n\u21d4 4\u03bag\n\u221a log(2d/\u03b4)\n|SH | \u2264 C \u2016s\u2016\n\u21d4 |SH | \u2265 16\u03ba2g log(2d/\u03b4)\n(C \u2016s\u2016)2 , C > 0.\n(56)\nNote that there may be a less restrictive sampling conditions that satisfy A4 since condition (56) is based on the worst case bound \u2016Av\u2016 \u2264 \u2016A\u2016op\u2016v\u2016 which indeed only holds with equality if v happens to be (exactly in the direction of) the largest eigenvector of A.\nFinally, we shall state a Lemma which illustrates that the stepsize goes to zero as the algorithm converges. The proof can be found in Section 5 of (Cartis et al., 2011a).\nLemma 20. Let {f(xk)} be bounded below by some finf > \u2212\u221e. Also, let sk satisfy A1 and \u03c3k be bounded below by some \u03c3inf > 0. Then we have for all successful iterations that\n\u2016sk\u2016 \u2192 0, as k \u2192\u221e (57)\nA.1.3. ILLUSTRATION\nIn the top row of Figure 2 we illustrate the Hessian sample sizes that result when applying SCR with a practical version of Theorem 9 to the datasets used in our experiments 4. In the bottom row of Figure 2, we benchmark our algorithm to the deterministic as well as two naive stochastic versions of ARC with linearly and exponentially increasing sample sizes.\nNote that both the linear and the exponential sampling schemes do not quite reach the same performance as SCR even though they were carefully fine tuned to achieve the best possible performance. Furthermore, the sampling size was manually set to reach the full sample size at the very last iteration. This highlights another advantage of the automatic sampling scheme that does not require knowledge of the total number of iterations."}, {"heading": "A.2. Convergence Analysis", "text": ""}, {"heading": "A.2.1. PRELIMINARY RESULTS", "text": ""}, {"heading": "Proof of Lemma 10:", "text": "The lower bound \u03c3inf follows directly from Step 7 in the algorithm design (see Algorithm 1). Within the upper bound, the constant \u03c30 accounts for the start value of the penalty parameter. Now, we show that as soon as some \u03c3k > 3( 2M+C+\u03bag 2 ), the iteration is very successful and \u03c3k+1 < \u03c3k. Finally, \u03b32 allows for \u03c3k being \u2019close to\u2019 the successful threshold, but increased \u2019one last time\u2019.\nAny iteration with f(xk + sk) \u2264 m(sk) yields a \u03c1k \u2265 1 \u2265 \u03b72 and is thus very successful. From a 2nd-order Taylor 4see Section A.3 for details\napproximation of f(xk + sk) around xk we have:\nf(xk + sk)\u2212mk(sk) = (\u2207f(xk)\u2212 g(xk))\u1d40sk + 1\n2 s\u1d40k(H(xk + tsk)\u2212Bk)sk \u2212\n\u03c3 3 \u2016sk\u20163\n\u2264 e\u1d40ksk + 1\n2 \u2016sk\u20162 \u2016H(xk + tsk)\u2212H(x)\u2016+\n1 2 \u2016H(xk)\u2212Bk\u2016 \u2016sk\u2016 \u2212 \u03c3k 3 \u2016sk\u20163\n\u2264 \u2016ek\u2016 \u2016sk\u2016+ ( C + \u03bag\n2 \u2212 \u03c3k 3\n) \u2016sk\u20163\n\u2264M \u2016sk\u20163 + ( C + \u03bag\n2 \u2212 \u03c3k 3\n) \u2016sk\u20163\n=\n( 2M + C + \u03bag\n2 \u2212 \u03c3k 3\n) \u2016sk\u20163\n(58)\nRequiring the right hand side to be non-positive and solving for \u03c3k gives the desired result.\nProof of Lemma 11 : By definition of the stochastic model mk(sk) we have\nf(xk)\u2212mk(sk) =\u2212 s\u1d40kg(xk)\u2212 1\n2 s\u1d40kBksk \u2212\n1 3 \u03c3k \u2016sk\u20163\n= 1\n2 s\u1d40kBksk +\n2 3 \u03c3k \u2016sk\u20163\n\u22651 6 \u03c3k \u2016sk\u20163 ,\n(59)\nwhere we applied equation (11) first and equation (12) secondly.\nBefore proving the lower bound on the stepsize \u2016sk\u2016 we first transfer the rather technical result from Lemma 4.6 in (Cartis et al., 2011a) to our framework of stochastic gradients. For this purpose, let ek be the gradient approximation error, i.e. ek := gk \u2212\u2207f(xk).\nLemma 21. Let f \u2208 C2, Lipschitz continuous gradients (A3) and TC (A2) hold. Then, for each (very-)successful k, we have\n(1\u2212 \u03ba\u03b8) \u2016\u2207f(xk+1)\u2016 \u2264 \u03c3k \u2016sk\u20162 +(\u2225\u2225\u2225\u2225\u222b 1 0 (H(xk + tsk)\u2212H(xk))dt \u2225\u2225\u2225\u2225+ \u2016(H(xk)\u2212Bk)sk\u2016\u2016sk\u2016 + \u03ba\u03b8\u03bag \u2016sk\u2016+ (1 + \u03ba\u03b8\u03bag) \u2016ek\u2016\u2016sk\u2016 ) \ufe38 \ufe37\ufe37 \ufe38\n=dk\n\u00b7 \u2016sk\u2016 (60)\nwith \u03ba\u03b8 \u2208 (0, 1) as in TC (13).\nProof: We shall start by writing\n\u2016\u2207f(xk + sk)\u2016 \u2264 \u2016\u2207f(xk + sk)\u2212\u2207mk(sk)\u2016+ \u2016\u2207mk(sk)\u2016 \u2264 \u2016\u2207f(xk + sk)\u2212\u2207mk(sk)\u2016\ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ \u03b8k \u2016gk(xk)\u2016\ufe38 \ufe37\ufe37 \ufe38 (b) , (61)\nwhere the last inequality results from TC (Eq. (13)). Now, we can find the following bounds on the individual terms:\n(a) By (5) we have\n\u2016\u2207f(xk + sk)\u2212\u2207mk\u2016 = \u2016\u2207f(xk + sk)\u2212 gk(xk)\u2212Bksk \u2212 \u03c3ksk \u2016sk\u2016\u2016 . (62)\nWe can rewrite the right-hand side by a Taylor expansion of\u2207fk+1(xk + sk) around xk to get\n(62) = \u2225\u2225\u2225\u2225\u2207f(xk) + \u222b 1 0 H(xk + tsk)skdt\u2212 gk(xk)\u2212Bksk \u2212 \u03c3ksk \u2016sk\u2016 \u2225\u2225\u2225\u2225 . (63)\nContrary to the case of deterministic gradients, the first and third summand no longer cancel out. Applying the triangle inequality repeatedly, we thus get an error term in the final bound on (a):\n\u2016\u2207f(xk + sk)\u2212\u2207mk\u2016 \u2264 \u2225\u2225\u2225\u2225\u222b 1\n0\nH((xk + tsk)\u2212Bk)skdt \u2225\u2225\u2225\u2225+ \u03c3k \u2016sk\u20162 + \u2016\u2207f(xk)\u2212 gk(xk)\u2016\n\u2264 \u2225\u2225\u2225\u2225\u222b 1\n0\nH((xk + tsk)dt\u2212H(xk) \u2225\u2225\u2225\u2225 \u00b7 \u2016sk\u2016+ \u2016(H(xk)\u2212Bk)sk\u2016\n+ \u03c3k \u2016sk\u20162 + \u2016ek\u2016 .\n(64)\n(b) To bound the second summand, we can write\n\u2016g(xk)\u2016 \u2264 \u2016\u2207f(xk)\u2016+ \u2016ek\u2016 \u2264 \u2016\u2207f(xk + sk)\u2016+ \u2016\u2207f(xk)\u2212\u2207f(xk + sk)\u2016+ \u2016ek\u2016 \u2264 \u2016\u2207f(xk + sk)\u2016+ \u03bag \u2016sk\u2016+ \u2016ek\u2016\n(65)\nFinally, using the definition of \u03b8k as in (13) (which also gives \u03b8k \u2264 \u03ba\u03b8 and \u03b8k \u2264 \u03ba\u03b8hk) and combining (a) and (b) we get the above result.\nProof of Lemma 12: The conditions of Lemma 21 are satisfied. By multiplying dk \u2016sk\u2016 out in equation (60), we get\n(1\u2212 \u03ba\u03b8) \u2016\u2207f(xk+1)\u2016 \u2264\u2225\u2225\u2225\u2225\u222b 1 0 (H(xk + tsk)\u2212H(xk))dt \u2225\u2225\u2225\u2225 \u2016sk\u2016+ \u2016(H(xk)\u2212Bk)sk\u2016+ \u03ba\u03b8\u03bag \u2016sk\u20162 + (1 + \u03ba\u03b8\u03bag) \u2016ek\u2016+ \u03c3k \u2016sk\u20162 . (66)\nNow, applying the strong agreement conditions (A4) and (A5), as well as the Lipschitz continuity of H, we can rewrite this as\n(1\u2212 \u03ba\u03b8) \u2016\u2207f(xk+1)\u2016 \u2264 ( 1\n2 \u03bag + C + (1 + \u03ba\u03b8\u03bag)M + \u03c3max + \u03ba\u03b8\u03bag) \u2016sk\u20162 , (67)\nfor all sufficiently large, successful k. Solving for the stepsize \u2016sk\u2016 give the desired result."}, {"heading": "A.2.2. LOCAL CONVERGENCE", "text": "Before we can study the convergence rate of SCR in a locally convex neighbourhood of a local minimizer w\u2217 we first need to establish three crucial properties:\n1. a lower bound on \u2016sk\u2016 that depends on \u2016gk\u2016.\n2. an upper bound on \u2016sk\u2016 that depends on \u2016gk+1\u2016.\n3. an eventually full sample size\n4. conditions under which all steps are eventually very successful.\nWith this at hand we will be able to relate \u2016gk+1\u2016 to \u2016gk\u2016, show that this ratio eventually goes to zero at a quadratic rate and conclude from a Taylor expansion around gk that the iterates themselves converge as well.\nAssumption 22 (Sampling Scheme). Let gk and Bk be sampled such that 17 and 19 hold in each iteration k. Furthermore, for unsuccessful iterations, assume that the sample size is not decreasing.\nWe have already established a lower stepsize bound in Lemma 12 so let us turn our attention directly to 2.: Lemma 23 (Upper bound on stepsize). Suppose that sk satisfies (11) and that the Rayleigh coefficient\nRk(sk) := s\u1d40kBksk\n\u2016sk\u20162 (68)\nis positive, then\n\u2016sk\u2016 \u2264 1\nRk(sk) \u2016gk\u2016 =\n1\nRk(sk) \u2016\u2207f(wk) + ek\u2016 \u2264\n1\nRk(sk) ( \u2016\u2207f(wk)\u2016+ \u2016ek\u2016) (69)\nProof: Given the above assumptions we can rewrite (11) as follows\nRk(sk) \u2016sk\u20162 = \u2212s\u1d40kgk \u2212 \u03c3k \u2016sk\u2016 3 \u2264 \u2016sk\u2016 \u2016gk\u2016 , (70)\nwhere we used Cauchy-Schwarz inequality as well as the fact that \u03c3k > 0, \u2200k. Solving (70) for \u2016sk\u2016 gives (69).\nLemma 24 (Eventually full sample size). Let {f(xk)} be bounded below by some finf > \u2212\u221e. Also, let A1, A3 hold and let gk and Bk be sampled according to A22. Then we have w.h.p. that\n|Sg,k| \u2192 n and |SB,k| \u2192 n as k \u2192\u221e (71)\nThe sampling schemes from Theorem 7 and Theorem 9 imply that the sufficient agreement assumptions A5 and A4 hold with high probability. Thus, we can deduce from Lemma 10 that after a certain number of consecutive unsuccessful iterates the penalty parameter is so high (\u03c3k \u2265 \u03c3sup) that we are guaranteed to find a successful step. Consequently, the number of successful iterations must be infinite (|S| = \u221e) when we consider the asymptotic convergence properties of SCR. We are left with two possible scenarios:\n(i) If the number of unsuccessful iterations is finite (|U| \u2264 \u221e & |S| = \u221e) we have that \u2203 k\u0302 after which all iterates are successful, i.e. k \u2208 S,\u2200 k > k\u0302. From Lemma 20 we know that for all successful iterations \u2016sk\u2016 \u2192 0 as k \u2192 \u221e. Consequently, due to the sampling scheme as specified in Theorem 7 and Theorem 9, \u2203 k\u0304 \u2265 k\u0302 with |Sg,k| = |SB,k| = n, \u2200 k \u2265 k\u0304.\n(ii) If the number of unsuccessful iterations is infinite (|U| = \u221e & |S| = \u221e) we know for the same reasons that for the subsequence of successful iterates {k = 0, 1, . . .\u221e|k \u2208 S} again \u2016sk\u2016 \u2192 0, as k \u2208 S \u2192 \u221e and hence \u2203 k\u0303 with |Sg,k| = |SB,k| = n, \u2200 k \u2265 k\u0303 \u2208 S . Given that we do specifically not decrease the sample size in unsuccessful iterations we have that |Sg,k| = |SB,k| = n, \u2200 k \u2265 k\u0303.\nAs a result the sample sizes eventually equal n with high probability in all conceivable scenarios which proves the assertion5.\nNow that we have (asymptotic) stepsize bounds and gradient (Hessian) agreement we are going to establish that, when converging, all SCR iterations are indeed very successful asymptotically. Lemma 25 (Eventually successful iterations). Let f \u2208 C2,\u2207f uniformly continuous and Bk bounded above. Let Bk and gk be sampled according to A22, as well as sk satisfy (11). Furthermore, let\nwk \u2192 w\u2217, as k \u2192\u221e, (72)\nwith\u2207f(w\u2217) = 0 and H(w\u2217) positive definite. Then there exists a constant Rmin > 0 such that for all k sufficiently large\nRk(sk) \u2265 Rmin. (73)\nFurthermore, all iterations are eventually very successful w.h.p.\n5We shall see that, as a result of Lemma 25, the case of an infinite number of unsuccessful steps can actually not happen\nProof: Since f is continuous, the limit (72) implies that {f(wk)} is bounded below. Since H(w\u2217) is positive definite per assumption, so is H(wk) for all k sufficiently large. Therefore, there exists a constant Rmin such that\ns\u1d40kH(wk)sk\n\u2016sk\u20162 > 2Rmin > 0, for all k sufficiently large. (74)\nAs a result of Lemma 24 we have that \u2016ek\u2016 \u2192 0 as k \u2192 \u221e. Hence, Lemma 23 yields \u2016sk\u2016 \u2264 1/Rmin \u2016\u2207fk\u2016 which implies that the step size converges to zero as we approximate w\u2217. Consequently, we are able to show that eventually all iterations are indeed very successful. Towards this end we need to ensure that the following quantity rk becomes negative for sufficiently large k:\nrk := f(wk + sk)\u2212m(sk)\ufe38 \ufe37\ufe37 \ufe38 (i) +(1\u2212 \u03b72) (m(sk)\u2212 f(wk))\ufe38 \ufe37\ufe37 \ufe38 (ii) , (75)\nwhere \u03b72 \u2208 (0, 1) is the \u201dvery successful\u201d threshold.\n(i) By a (second-order) Taylor approximation around f(wk) and applying the Cauchy-Schwarz inequality, we have:\nf(wk + sk)\u2212m(sk) =(\u2207f(w)\u2212 gk)\u1d40sk + 1\n2 s\u1d40k((H(wk + \u03c4sk)\u2212Bk)sk \u2212 \u03c3k 3 \u2016s\u20163\n\u2264\u2016ek\u2016 \u2016sk\u2016+ 1\n2 \u2016((H(wk + \u03c4sk)\u2212Bk)sk\u2016 \u2016sk\u2016 ,\n(76)\nwhere the term \u2016ek\u2016 \u2016sk\u2016 is extra compared to the case of deterministic gradients.\n(ii) Regarding the second part we note that if sk satisfies (11), we have by the definition of Rk and equation (73) that\nf(wk)\u2212mk(sk) = 1\n2 s\u1d40kBsk +\n2 3 \u03c3k \u2016sk\u20163\n\u22651 2 Rmin \u2016sk\u20162 ,\n(77)\nwhich negated gives the desired bound on (ii). All together, the upper bound on rk is written as\nrk \u2264 1\n2 \u2016sk\u20162 ( 2 \u2016ek\u2016 \u2016sk\u2016 + \u2016((H(wk + \u03c4sk)\u2212Bk)sk\u2016 \u2016sk\u2016 \u2212 (1\u2212 \u03b72)Rmin ) . (78)\nLet us add and subtract H(wk) to the second summand and apply the triangle inequality\nrk \u2264 1\n2 \u2016sk\u20162 ( 2 \u2016ek\u2016 \u2016sk\u2016 + \u2016(H(wk + \u03c4sk)\u2212Hk)sk\u2016+ \u2016(Hk \u2212Bk)sk\u2016 \u2016sk\u2016 \u2212 (1\u2212 \u03b72)Rmin ) . (79)\nNow applying \u2016Av\u2016 \u2264 \u2016A\u2016 \u2016v\u2016 we get\nrk \u2264 1\n2 \u2016sk\u20162 ( 2 \u2016ek\u2016 \u2016sk\u2016 + \u2016H(wk + \u03c4sk)\u2212Hk\u2016+ \u2016(Hk \u2212Bk)\u2016 \u2212 (1\u2212 \u03b72)Rmin ) . (80)\nWe have already established in Lemma 24 that \u2016ek\u2016 \u2192 0 and \u2016(Hk \u2212Bk)\u2016 \u2192 0. Together with Lemma 23 and the assumption \u2016\u2207fk\u2016 \u2192 0 this implies \u2016sk\u2016 \u2192 0. Furthermore, since \u03c4 \u2208 [0, 1] we have that \u2016wk + \u03c4sk\u2016 \u2264 \u2016wk + sk\u2016 \u2264 \u2016sk\u2016. Hence, H(wk + \u03c4sk) and H(wk) eventually agree. Finally, \u03b72 < 1 and Rmin > 0 such that rk is negative for all k sufficiently large, which implies that every such iteration is very successful."}, {"heading": "Proof of Theorem 13:", "text": "From Lemma 10 we have \u03c3k \u2264 \u03c3sup. Furthermore, all assumptions needed for the step size bounds of Lemma 12 and 23 hold. Finally, Lemma 25 gives that all iterations are eventually successful. Thus, we can combine the upper (69) and lower (24) bound on the stepsize for all k sufficiently large to obtain\n1\nRmin ( \u2016\u2207f(wk)\u2016+ \u2016ek\u2016) \u2265 \u2016sk\u2016 \u2265 \u03bas\n\u221a \u2016\u2207f(wk+1)\u2016 (81)\nwhich we can solve for the gradient norm ratio\n\u2016\u2207f(wk+1)\u2016 \u2016\u2207f(wk)\u20162\n\u2264 ( 1\nRmin\u03bas\n( 1 +\n\u2016ek\u2016 \u2016\u2207f(wk)\u2016\n))2 . (82)\nConsequently, as long as the right hand side of (82) stays below infinity, i.e. \u2016ek\u2016 / \u2016\u2207f(wk)\u2016 6\u2192 \u221e, we have quadratic convergence of the gradient norms. From Lemma 24 we have that \u2016ek\u2016 \u2192 0 as k \u2192 \u221e w.h.p. and furthermore \u03bas is bounded above by a constant and Rmin is a positive constant itself which gives quadratic convergence of the gradient norm ratio with high probability. Finally, the convergence rate of the iterates follows from a Taylor expansion around gk."}, {"heading": "A.2.3. FIRST ORDER GLOBAL CONVERGENCE", "text": "Note that the preliminary results Lemma 11 and 12 allow us to lower bound the function decrease of a successful step in terms of the full gradient \u2207fk+1. Combined with Lemma 10, this enables us to give a deterministic global convergence guarantee while using only stochastic first order information6."}, {"heading": "Proof of Theorem 14:", "text": "We will consider two cases regarding the number of successful steps for this proof.\nCase (i): SCR takes only finitely many successful steps. Hence, we have some index k0 which yields the very last successful iteration and all further iterates stay at the same point xk0+1. That is xk0+1 = xk0+i, \u2200 i \u2265 1. Let us assume that \u2016\u2207f(xk0+1)\u2016 = > 0, then \u2016\u2207f(xk)\u2016 = , \u2200 k \u2265 k0 + 1. (83) Since, furthermore, all iterations k \u2265 k0 + 1 are unsuccessful \u03c3k increases by \u03b3, such that\n\u03c3k \u2192\u221e as k \u2192\u221e. (84)\nHowever, this is in contradiction with Lemma 10, which states that \u03c3k is bounded above. Hence, the above assumption cannot hold and we have \u2016\u2207f(xk0+1)\u2016 = \u2016\u2207f(x\u2217)\u2016 = 0.\nCase (ii): sARC takes infinitely many successful steps. While unsuccessful steps keep f(xk) constant, (very) successful steps strictly decrease f(xk) and thus the sequence {f(xk)} is monotonically decreasing. Furthermore, it is bounded below per assumption and thus the objective values converge\nf(xk)\u2192 finf , as k \u2192\u221e. (85)\nAll requirements of Lemma 11 and Lemma 12 hold and we thus can use the sufficient function decrease equation (31) to write\nf(xk)\u2212 finf \u2265 f(xk)\u2212 f(xk+1) \u2265 1\n6 \u03b71\u03c3inf\u03ba\n3 s \u2016\u2207f(xk+1)\u2016 3/2 . (86)\nSince (f(xk)\u2212 finf)\u2192 0 as k \u2192\u221e, \u03c3inf > 0, \u03b71 > 0 and \u03ba3s \u2265 0 (as \u03c3sup <\u221e), we must have \u2016\u2207f(xk)\u2016 \u2192 0, giving the result."}, {"heading": "A.2.4. SECOND ORDER GLOBAL CONVERGENCE AND WORST CASE ITERATION COMPLEXITY", "text": "For the proofs of Theorem 15 and Theorem 17 we refer the reader to Theorem 5.4 in (Cartis et al., 2011a) and Corollary 5.3 in (Cartis et al., 2011b). Note that, as already laid out above in the proofs of Lemma 10 and Lemma 11, the constants involved in the convergence Theorems change due to the stochastic gradients used in our framework.\nA.3. Details concerning experimental section\nWe here provide additional results and briefly describe the baseline algorithms used in the experiments as well as the choice of hyper-parameters. All experiments were run on a CPU with a 2.4 GHz nominal clock rate.\n6Note that this result can also be proven without Lipschitz continuity of H and less strong agreement conditions as done in Corollary 2.6 in (Cartis et al., 2011a).\nDatasets The real-world datasets we use represent very common instances of Machine Learning problems and are part of the libsvm library (Chang & Lin, 2011), except for cifar which is from Krizhevsky & Hinton (2009). A summary of their main characteristic can be found in table 1. The multiclass datasets are both instances of so-called image classification problems. The mnist images are greyscale and of size 28\u00d7 28. The original cifar images are 32\u00d7 32\u00d7 3 but we converted them to greyscale so that the problem dimensionality is comparable to mnist. Both datasets have 10 different classes, which multiplies the problem dimensionality of the multinomial regression by 10."}, {"heading": "Benchmark methods", "text": "\u2022 Stochastic Gradient Descent (SGD): To bring in some variation, we select a mini-batch of the size dn/10e on the real world classification- and dn/100e on the multiclass problems. On the artificial datasets we only sample 1 datapoint per iteration and update the parameters with respect to this point. We use a problem-dependent, constant step-size as this yields faster initial convergence (Hofmann et al., 2015),(Roux et al., 2012).\n\u2022 SAGA: is a variance-reduced variant of SGD that only samples 1 datapoint per iteration and uses a constant step-size.\n\u2022 Broyden-Fletcher-Goldfarb-Shanno (BFGS) is the most popular and stable Quasi-Newton method.\n\u2022 Limited-memory BFGS is a variant of BFGS which uses only the recent K iterates and gradients to construct an approximate Hessian. We used K = 20 in our experiments. Both methods employs a line-search technique that satisfies the strong Wolfe condition to select the step size.\n\u2022 NEWTON is the classic version of Newton\u2019s method which we apply with a backtracking line search.\nFor L-BFGS and BFGS we used the implementation available in the optimization library of scipy. All other methods are our own implementation. The code for our implementation of SCR is publicly available on the authors\u2019 webpage.\nInitialization. All of our experiments were started from the initial weight vector w0 := (0, . . . , 0).\nChoice of parameters for ARC and SCR. The regularization parameter updating is analog to the rule used in the reported experiments of (Cartis et al., 2011a), where \u03b3 = 2. Its goal is to reduce the penalty rapidly as soon as convergence sets in, while keeping some regularization in the non asymptotic regime. A more sophisticated approach can be found in (Gould et al., 2012). In our experiments we start with \u03c30 = 1, \u03b71 = 0.2, and \u03b72 = 0.8 as well as an initial sample size of 5%.\nInfluence of dimensionality To test the influence of the dimensionality on the progress of the above applied methods we created artificial datasets of three different sizes, labeled as gaussian s, gaussian m and gaussian l.\nThe feature vectors X = (x1,x2, ...,xd),xi \u2208 Rn were drawn from a multivariate Gaussian distribution\nX \u223c N (\u00b5, \u03a3) (87)\nwith a mean of zero \u00b5 = (0, . . . , 0) and a covariance matrix that has reasonably uniformly distributed off-diagonal elements in the interval (\u22121, 1).\nAs expected, the classic Newton methods suffers heavily from an increase in the dimension. The regularized Newton methods on the other hand scale comparably very well since they only need indirect access to the Hessian via matrixvector products. Evidently, these methods outperform the quasi-newton approaches even in high dimensions. Among these, the limited memory version of BFGS is significantly faster than its original variant.\nMulticlass regression In this section we leave the trust region method out because our implementation is not optimized towards solving multi-class problems. We do not run Newton\u2019s method or BFGS either as the above results suggests that they are unlikely to be competitive. Furthermore, Figure 5 does not show logarithmic but linear suboptimality because optimizing these problems to high precision takes very long and yields few additional benefits. For example, the 25th SCR iteration drove the gradient norm from 3.8 \u00b7 10\u22125 to 5.6 \u00b7 10\u22128 after building up a Krylov space of dimensionality 7800. It took 9.47 hours and did not change any of the first 13 digits of the loss. As can be seen, SCR provides early progress at a comparable rate to other methods but gives the opportunity to solve the problem to high precision if needed."}], "references": [{"title": "Finding local minima for nonconvex optimization in linear time", "author": ["Agarwal", "Naman", "Allen-Zhu", "Zeyuan", "Bullins", "Brian", "Hazan", "Elad", "Ma", "Tengyu"], "venue": "arXiv preprint arXiv:1611.01146,", "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Convergence rate analysis of a stochastic trust region method for nonconvex optimization", "author": ["Blanchet", "Jose", "Cartis", "Coralia", "Menickelly", "Matt", "Scheinberg", "Katya"], "venue": "arXiv preprint arXiv:1609.07428,", "citeRegEx": "Blanchet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchet et al\\.", "year": 2016}, {"title": "On the use of stochastic hessian information in optimization methods for machine learning", "author": ["Byrd", "Richard H", "Chin", "Gillian M", "Neveitt", "Will", "Nocedal", "Jorge"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Byrd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 2011}, {"title": "Gradient descent efficiently finds the cubic-regularized non-convex newton step", "author": ["Carmon", "Yair", "Duchi", "John C"], "venue": null, "citeRegEx": "Carmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carmon et al\\.", "year": 2016}, {"title": "Global convergence rate analysis of unconstrained optimization methods based on probabilistic models", "author": ["Cartis", "Coralia", "Scheinberg", "Katya"], "venue": "Mathematical Programming,", "citeRegEx": "Cartis et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cartis et al\\.", "year": 2017}, {"title": "Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results", "author": ["Cartis", "Coralia", "Gould", "Nicholas IM", "Toint", "Philippe L"], "venue": "Mathematical Programming,", "citeRegEx": "Cartis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cartis et al\\.", "year": 2011}, {"title": "Adaptive cubic regularisation methods for unconstrained optimization. part ii: worst-case function-and derivativeevaluation complexity", "author": ["Cartis", "Coralia", "Gould", "Nicholas IM", "Toint", "Philippe L"], "venue": "Mathematical programming,", "citeRegEx": "Cartis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cartis et al\\.", "year": 2011}, {"title": "Libsvm: a library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "The loss surfaces of multilayer networks", "author": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "venue": "In AISTATS,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "Starting small - learning with adaptive sample sizes", "author": ["Daneshmand", "Hadi", "Lucchi", "Aur\u00e9lien", "Hofmann", "Thomas"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Daneshmand et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daneshmand et al\\.", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defazio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Defazio et al\\.", "year": 2014}, {"title": "Convergence rates of sub-sampled newton methods", "author": ["Erdogdu", "Murat A", "Montanari", "Andrea"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Erdogdu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erdogdu et al\\.", "year": 2015}, {"title": "Hybrid deterministic-stochastic methods for data fitting", "author": ["Friedlander", "Michael P", "Schmidt", "Mark"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Friedlander et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Friedlander et al\\.", "year": 2012}, {"title": "Escaping from saddle points-online stochastic gradient for tensor decomposition", "author": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In COLT, pp", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Ghadimi", "Saeed", "Lan", "Guanghui"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi et al\\.", "year": 2013}, {"title": "Updating the regularization parameter in the adaptive cubic regularization algorithm", "author": ["Gould", "Nicholas IM", "M Porcelli", "Toint", "Philippe L"], "venue": "Computational optimization and applications,", "citeRegEx": "Gould et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gould et al\\.", "year": 2012}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["Gross", "David"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Gross and David.,? \\Q2011\\E", "shortCiteRegEx": "Gross and David.", "year": 2011}, {"title": "A linear-time algorithm for trust region problems", "author": ["Hazan", "Elad", "Koren", "Tomer"], "venue": "Mathematical Programming,", "citeRegEx": "Hazan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2016}, {"title": "Most tensor problems are np-hard", "author": ["Hillar", "Christopher J", "Lim", "Lek-Heng"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Hillar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hillar et al\\.", "year": 2013}, {"title": "Variance reduced stochastic gradient descent with neighbors", "author": ["Hofmann", "Thomas", "Lucchi", "Aurelien", "Lacoste-Julien", "Simon", "McWilliams", "Brian"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hofmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Johnson", "Rie", "Zhang", "Tong"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Introductory lectures on convex optimization", "author": ["Nesterov", "Yurii"], "venue": "applied optimization,", "citeRegEx": "Nesterov and Yurii.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 2004}, {"title": "Cubic regularization of newton method and its global performance", "author": ["Nesterov", "Yurii", "Polyak", "Boris T"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 2006}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Reddi", "Sashank J", "Hefny", "Ahmed", "Sra", "Suvrit", "Poczos", "Barnabas", "Smola", "Alex"], "venue": "arXiv preprint arXiv:1603.06160,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "Fast incremental method for nonconvex optimization", "author": ["Reddi", "Sashank J", "Sra", "Suvrit", "P\u00f3czos", "Barnab\u00e1s", "Smola", "Alex"], "venue": "arXiv preprint arXiv:1603.06159,", "citeRegEx": "Reddi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Roux", "Nicolas L", "Schmidt", "Mark", "Bach", "Francis R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "When are nonconvex problems not scary", "author": ["Sun", "Ju", "Qu", "Qing", "Wright", "John"], "venue": "arXiv preprint arXiv:1510.06096,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Yet, non-convex functions are extremely hard to optimize due to the presence of saddle points and local minima which are not global optima (Dauphin et al., 2014; Choromanska et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 13, "context": "In this regard, a lot of attention has focused on a specific type of functions known as strict saddle functions or ridable functions (Ge et al., 2015; Sun et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 27, "context": "In this regard, a lot of attention has focused on a specific type of functions known as strict saddle functions or ridable functions (Ge et al., 2015; Sun et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 13, "context": "Examples of strict saddle functions include dictionary learning, orthogonal tensor decomposition and generalized phase retrieval (Ge et al., 2015; Sun et al., 2015).", "startOffset": 129, "endOffset": 164}, {"referenceID": 27, "context": "Examples of strict saddle functions include dictionary learning, orthogonal tensor decomposition and generalized phase retrieval (Ge et al., 2015; Sun et al., 2015).", "startOffset": 129, "endOffset": 164}, {"referenceID": 10, "context": "Another way to recover a linear rate of convergence for strongly-convex functions is to use variance-reduced methods (Johnson & Zhang, 2013; Defazio et al., 2014; Roux et al., 2012; Hofmann et al., 2015; Daneshmand et al., 2016).", "startOffset": 117, "endOffset": 228}, {"referenceID": 26, "context": "Another way to recover a linear rate of convergence for strongly-convex functions is to use variance-reduced methods (Johnson & Zhang, 2013; Defazio et al., 2014; Roux et al., 2012; Hofmann et al., 2015; Daneshmand et al., 2016).", "startOffset": 117, "endOffset": 228}, {"referenceID": 19, "context": "Another way to recover a linear rate of convergence for strongly-convex functions is to use variance-reduced methods (Johnson & Zhang, 2013; Defazio et al., 2014; Roux et al., 2012; Hofmann et al., 2015; Daneshmand et al., 2016).", "startOffset": 117, "endOffset": 228}, {"referenceID": 9, "context": "Another way to recover a linear rate of convergence for strongly-convex functions is to use variance-reduced methods (Johnson & Zhang, 2013; Defazio et al., 2014; Roux et al., 2012; Hofmann et al., 2015; Daneshmand et al., 2016).", "startOffset": 117, "endOffset": 228}, {"referenceID": 13, "context": "However, the work of (Ge et al., 2015; Sun et al., 2015) recently showed that SGD can achieve stronger guarantees in the case of strict saddle functions.", "startOffset": 21, "endOffset": 56}, {"referenceID": 27, "context": "However, the work of (Ge et al., 2015; Sun et al., 2015) recently showed that SGD can achieve stronger guarantees in the case of strict saddle functions.", "startOffset": 21, "endOffset": 56}, {"referenceID": 2, "context": "An increasingly popular alternative is to use sub-sampling techniques to approximate the Hessian matrix, such as done for example in (Byrd et al., 2011) and (Erdogdu & Montanari, 2015).", "startOffset": 133, "endOffset": 152}, {"referenceID": 0, "context": "For example, (Agarwal et al., 2016) refined the approach of (Nesterov & Polyak, 2006) to return an approximate local minimum in time which is linear in the input representation.", "startOffset": 13, "endOffset": 35}, {"referenceID": 1, "context": "Finally, the work of (Blanchet et al., 2016) proposed a stochastic variant of a trust region method but their analysis does not specify any accuracy level required for the estimation of the stochastic Hessian.", "startOffset": 21, "endOffset": 44}, {"referenceID": 19, "context": "We use a problem-dependent, constant step-size as this yields faster initial convergence (Hofmann et al., 2015),(Roux et al.", "startOffset": 89, "endOffset": 111}, {"referenceID": 26, "context": ", 2015),(Roux et al., 2012).", "startOffset": 8, "endOffset": 27}, {"referenceID": 15, "context": "A more sophisticated approach can be found in (Gould et al., 2012).", "startOffset": 46, "endOffset": 66}], "year": 2017, "abstractText": "We consider the minimization of non-convex functions that typically arise in machine learning. Specifically, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than firstand second-order as well as classical trust region methods. However, it suffers from a high computational complexity that makes it impractical for large-scale learning. Here, we propose a novel method that uses sub-sampling to lower this computational cost. By the use of concentration inequalities we provide a sampling scheme that gives sufficiently accurate gradient and Hessian approximations to retain the strong global and local convergence guarantees of cubically regularized methods. To the best of our knowledge this is the first work that gives global convergence guarantees for a sub-sampled variant of cubic regularization on non-convex functions. Furthermore, we provide experimental results supporting our theory.", "creator": "LaTeX with hyperref package"}}}