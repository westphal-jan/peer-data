{"id": "1705.05992", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Frame Stacking and Retaining for Recurrent Neural Network Acoustic Model", "abstract": "Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding and decoding neural networks based on context-dependent state acoustic model, but not on context-dependent state acoustic model. The frame retaining method uses a low-resolution, uniform matrix of data encoded on top of each other that is encoded on top of each other (i.e., in one layer), a linear matrix of data encoded on top of each other to create a set of frames in which data can be encoded to either a separate layer or to a multiple layer of data. The current protocol uses only one layer and its data must be stored in a layer of data, and so a single layer of data (and thus the whole layer) is a very different set of data.\n\n\n\nThis article has been adapted from The Neural Network Model in order to explain why neural networks are a better way to classify information in neural network training.", "histories": [["v1", "Wed, 17 May 2017 02:34:27 GMT  (182kb)", "http://arxiv.org/abs/1705.05992v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei"], "accepted": false, "id": "1705.05992"}, "pdf": {"name": "1705.05992.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wj80290}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 99\n2v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\nnetwork training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve almost linear training speedup and reduces relative 41% real time factor (RTF). At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin."}, {"heading": "1. INTRODUCTION", "text": "In the last few years, deep neural networks (DNNs) combined with hiddenMarkovmodels (HMMs) have been widely employed in acoustic modeling for large vocabulary speech recognition [1]. More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].\nDNNs always stack the fixed number of neighboring frames feature together as a new feature of current frame, as lack of temporal information. Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7]. Though RNNs are able to remember long-term information, frame stacking could also provide useful contextual information [8]. For small LSTM network, it employs the same stacking method with DNNs, and the number of frames is no reduction. With the growth of LSTM models size, the influence of contextual information provided by frame stacking fades gradually.\nHowever, the neural networks combined with connectionist temporal classification (CTC) criterion gives the frame stacking rebirth [9, 10]. Connectionist temporal classification (CTC) criterion provides a mechanism to learn an neural network while mapping a input frame sequence to a output label\nsequence [11]. The length of output sequence could be much more shorter than that of input sequence, because of the blank symbol of CTC. Thus, there is no need of a frame-level alignment for cross-entropy (CE) training. CTC-LSTM acoustic models using context dependent phones (CD-phones) perform as well as conventional models [12]. As it utilizes a larger modeling unit, the successive frames could be stacked together as a super frame. If we regard DNNs frame stacking with a sliding window method, its sliding step is one. The frame stacking of CTC-LSTM is more flexible that its sliding step could be longer. Even the sliding step could be equal to window length, and there is no overlap between two windows. As a result, the frame stacking reduce the frame rate, and leads to faster training and decoding.\nThe traditional RNN models, which are still competitive, could also utilize the frame stacking directly in the training phase. But it brings prominent deterioration of decoding result, if the decoding network is unchanged. It is an intuitional way to remodel HMM structure in order to match the modeling unit, and decoding network is needed to rebuild correspondingly [13]. In this paper, we explore conventional RNN models using frame stacking, and propose a novel frame retaining method which is applied in decoding phase and keeps the original decoding network. Frame stacking and retaining will be describe in Section 2. LSTM models are successfully trained on large scale dataset in Section 3, followed by conclusions in Section 4."}, {"heading": "2. FRAME STACKING AND RETAINING", "text": ""}, {"heading": "2.1. Non-overlapping Frame Stacking", "text": "In the conventional acoustic modeling systems, features is extracted with frame segmentation, and they are computed every fixed steps on fixed frame windows. Frame stacking is a kind of frame re-segmentation, which stacks temporal neighboring frames to a super frame. There is two kinds of frame stacking, overlapping one and non-overlapping one, as shown in Figure 1. They could brings linear reduction of input frames, and the degree of it depends on the shift step of overlapping one or the frame window of non-overlapping one. Since the original feature is extracted with sliding frame window, there is no\nneed to use sliding window in frame stacking again. Therefore, we prefer non-overlapping frame stacking for RNNs, which has temporal memory structure.\nFor speech recognition applications, DNNs input frames always contain context information through packing temporal sequential left and right frames together. It could cover the shortage of no dynamic temporal behavior.\nIn contrast, RNNs do not need to pack the context information to obtain the sequential ability. It stacks neighboring frames to a super frame, because of the stationarity of the speech signal. Super frames provides multiple frames information as the new input of the network with no information missing, so the quantity of input frames decreases linearly. The super frame\u2019s label comes from the label of middle frame of successive frames. The network only needs to enlarge this architecture properly, and the main enlargement is for the input layer. As a result, the time cost of network training decreases almost linearly."}, {"heading": "2.2. Frame Retaining in Decoding", "text": "Frame stacking could substantially reduce the training time, and it could also have the same effect on decoding phase. It has been demonstrated in CTC systems [9]. As CTC is phone-level modeling, the granularity after stacking is still suitable for CTC decoding. But conventional RNNs is statelevel modeling and weighted finite state transducer (WFST) is state-level correspondingly, so the granularity is too large to decode. In order to maintain the decoding granularity, frame retaining is proposed as presented in Figure 2.\nThe size of frame stacking window is denoted asN . After N successive frames are extracted in a signal stream, they are stacked to a super frame in the same way of training phase. Consequently, the super frame retains forN frames time with frame retaining method. The neighboring frame has similar properties, so super frame represents them, after aggregating their features. In traditional decoding method, features of each frame needs to pass through the network, and N frames mean N times forward passes. But a super frame passes through the network only once carrying the all information\nof N frames, and the result of the super frame\u2019s forward pass is multiplexed at the rest of frame time. Moreover, WFST does not need to rebuild for frame stacking. Thus, decoder spends less time in general, as N \u2212 1 times of forward pass is skipped and computation consumption of one forward pass increases only a little."}, {"heading": "2.3. Acoustic Model Trained with Cross-Entropy", "text": "Let x = x1, . . . , xT denote a input sequence of T acoustic feature vectors, where xt \u2208 R N , and w an output word sequence. The acoustic likelihood is decomposed as follows:\np(x|w) =\nT\u220f\nt=1\np(xt|lt)p(lt|lt\u22121)\nwhere l1, . . . , lT is the label sequence, which is obtained by existing models. In the hybrid decoding, the emission probability of HMM is represented as p(xt|lt) = p(lt|xt)p(xt)/p(lt). The label posterior is given by the output of a neural network acoustic model, and it could be computed using a context of N frames with frame stacking. The label prior p(lt) is counted by the label of existing model\u2019s alignment.\nThe acoustic model of neural network is first trained to maximize the cross-entropy (CE) loss with the input sequence x and the corresponding frame-level alignment l, as follow:\n\u0141CE = \u2212 \u2211\n(x,l)\nT\u2211\nt=1\nlog p(lt|xt)\nWhere p(l|x) is the label posterior after the softmax output layer of the neural network."}, {"heading": "2.4. Sequence Discriminative Training", "text": "CE provides a kind of frame-wise discriminative training criterion, but it not enough for speech recognition which is a sequence problem. Sequence discriminative training using state-level minimum bayes risk (sMBR) has shown to further improve performance of neural networks first trained with CE [14, 15]. The model first trained by CE loss is frame-level accurate, and it is further trained with sMBR to get sequencelevel accuracy. Frame stacking and retaining are also applied in sMBR training. It also gets almost linear speedup. Moreover, on the basis of frame-level accurate model, only a part of dataset is needed for sMBR training."}, {"heading": "3. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "3.1. Experiments Setup", "text": "The neural network is trained on 17000 hours dataset which is collected from Shenma voice search. It is one of the most\npopular mobile search engines in China. The dataset is created from anonymous online users\u2019 search queries in Mandarin, and all audio file\u2019s sampling rate is 16kHz, recorded by mobile phones. This dataset consists of many different conditions, such as diverse noise even low signal-to-noise, babble, dialects, accents, hesitation and so on.\nThe dataset is divided into training set, validation set and test set separately, and the quantity of them is shown in Table 1. The three sets are split according to speakers, in order to avoid utterances of same speaker appearing in three sets simultaneously. The test sets of Shenma voice search are called Shenma Test.\nLSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [16, 10]. Shenma voice search is a streaming service that intermediate recognition results displayed while users are still speaking. So as for online recognition in real time, we prefer unidirectional LSTMmodel rather than bidirectional one. Thus, the training system is unidirectional LSTM-based.\nA 26-dimensional filter bank and 2-dimensional pitch feature is extracted for each frame, and is concatenated with first and second order difference as the final input of the network. The extraction happens every 10 ms with 25 ms frame window. The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a fullconnection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is CE. After CE training, the model is trained with sMBR. The performance metric of the system in Mandarin is reported with character error rate (CER). The alignment of frame-level ground truth is obtained by GMM-HMM"}, {"heading": "FS FR CER", "text": "system. Mini-batched SGD is utilized with momentum trick and the network is trained for a total of 4 epochs. 5-gram language model is leveraged in decoder, and the vocabulary size is as large as 760000.\nIt has shown that blockwisemodel-update filtering (BMUF)\noutperforms traditional model averaging method, and it is utilized at the synchronization phase [17]. Its block learning rate and block momentum are set as 1 and 0.9. After synchronizing with BMUF, exponential moving average (EMA) method further updates the model in non-interference way [18]. The training system is deployed on the MPI-based HPC cluster where 8 GPUs. Each GPU processes non-overlap subset split from the entire large scale dataset in parallel.\nLocal models from distributed workers synchronize with each other in decentralized way. In the traditional model averaging and BMUF method, a parameter server waits for all workers to send their local models, aggregate them, and send the updated model to all workers. Computing resource of workers is wasted until aggregation of the parameter server done. Decentralized method makes full use of computing resource, and we employ the MPI-based mesh AllReduce method [18]. It is significant to promote training efficiency, when the size of neural network model is too large. The EMA model is also updated additionally, but not broadcasting it."}, {"heading": "3.2. Results", "text": "Frame stacking cuts down the number of input frames, so it leads to almost linear speedup of training. But when its model is applied in the decoder directly, it will cause the great CER degradation, as the modeling duration does not match. The decoding network is generated to fit for original modeling duration. N frames corresponds only one input feature vectors of decoder for frame stacking, while N frames corresponds N of them for original modeling. Therefore, frame retaining in decoder could match the number of input feature vectors and that of frames. We denote the number of non-overlapping stacked frames as FS, and the times of a super frame retaining as FR. As shown in Table 2, if a super frame is stacked by 3 frames for 4-layers LSTMmodels, FR = 1 increase relative 415% CER, and the other modeling duration mismatch also results in worse performance of decoder. FR being 1 means no frame retaining, and it demonstrates that only frame stacking could not improve the accuracy of non-CTC neural network.\nFrame stacking and retaining not only spends less time"}, {"heading": "FS FR CER RTF", "text": "in training, but also brings faster decoder. Real time factor is utilized to evaluate the decoding speed. CERs and RTFs of 4-layers LSTM models with different number of stacked frames and matching frame retaining are presents in Table 3. Neighboring frames have similar features, so there is no information omitted in stacking process. It does not reduce the performance of recognition, and even improves it, as shown in Table 3. For our system, it is optimal that FS and FR are both set as 3. It reduces relative 41% RTF, and accuracy improves sightly."}, {"heading": "4. CONCLUSION", "text": "In this work, we propose frame retaining in conventional neural networks with frame stacking. The parameters of frame stacking and retaining should be equal, in order that they have the same modeling duration. It leads to almost linear training speedup and faster decoding, while the performance of speech recognition does not decrease. Unidirectional LSTM models are trained to verify it on large scale speech recognition. RTF reduces relative 41% and the character accuracy improves sightly compared with no use of frame stacking and retaining."}, {"heading": "5. REFERENCES", "text": "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[2] Alex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.\n[3] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-\nhamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[4] Hasim Sak, AndrewW Senior, and Franc\u0327oise Beaufays,\n\u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.,\u201d in INTERSPEECH, 2014, pp. 338\u2013342.\n[5] Frantisek Grezl and Petr Fousek, \u201cOptimizing bottle-\nneck features for lvcsr,\u201d in Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4729\u20134732.\n[6] Samuel Thomas, Sriram Ganapathy, and Hynek Her-\nmansky, \u201cPhoneme recognition using spectral envelope and modulation frequency features,\u201d in Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 4453\u20134456.\n[7] Vincent Vanhoucke, Matthieu Devin, and Georg\nHeigold, \u201cMultiframe deep neural networks for acoustic modeling,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7582\u20137585.\n[8] Martin Wo\u0308llmer, Bjo\u0308rn W Schuller, and Gerhard\nRigoll, \u201cFeature frame stacking in rnn-based tandem asr systems-learned vs. predefined context.,\u201d in INTERSPEECH, 2011, pp. 1233\u20131236.\n[9] Has\u0327im Sak, Andrew Senior, Kanishka Rao, and\nFranc\u0327oise Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition,\u201d arXiv preprint arXiv:1507.06947, 2015.\n[10] Has\u0327im Sak, Fe\u0301lix de Chaumont Quitry, Tara Sainath,\nKanishka Rao, et al., \u201cAcoustic modelling with cd-ctcsmbr lstm rnns,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.\n[11] Alex Graves, Santiago Ferna\u0301ndez, Faustino Gomez, and\nJu\u0308rgen Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.\n[12] Andrew Senior, Has\u0327im Sak, and Izhak Shafran, \u201cCon-\ntext dependent phone models for lstm rnn acoustic modelling,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4585\u20134589.\n[13] Golan Pundak and Tara N Sainath, \u201cLower frame rate\nneural network acoustic models,\u201d Interspeech 2016, pp. 22\u201326, 2016.\n[14] Brian Kingsbury, \u201cLattice-based optimization of se-\nquence classification criteria for neural-network acoustic modeling,\u201d in Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.\n[15] Has\u0327im Sak, Andrew Senior, Kanishka Rao, Ozan Irsoy,\nAlex Graves, Franc\u0327oise Beaufays, and Johan Schalkwyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.\n[16] Michiel Hermans and Benjamin Schrauwen, \u201cTraining\nand analysing deep recurrent neural networks,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.\n[17] Kai Chen and Qiang Huo, \u201cScalable training of deep\nlearning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.\n[18] Tian Xu, Zhang Jun, Ma Zejun, He Yi, and Wei Juan,\n\u201cExponential moving average model in parallel speech recognition training,\u201d arXiv preprint arXiv:1703.01024, 2017."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "IN- TERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimizing bottleneck features for lvcsr", "author": ["Frantisek Grezl", "Petr Fousek"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4729\u20134732.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Phoneme recognition using spectral envelope and modulation frequency features", "author": ["Samuel Thomas", "Sriram Ganapathy", "Hynek Hermansky"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 4453\u20134456.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiframe deep neural networks for acoustic modeling", "author": ["Vincent Vanhoucke", "Matthieu Devin", "Georg Heigold"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 7582\u20137585.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature frame stacking in rnn-based tandem asr systems-learned vs. predefined context", "author": ["Martin W\u00f6llmer", "Bj\u00f6rn W Schuller", "Gerhard Rigoll"], "venue": "INTER- SPEECH, 2011, pp. 1233\u20131236.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Context dependent phone models for lstm rnn acoustic modelling", "author": ["Andrew Senior", "Ha\u015fim Sak", "Izhak Shafran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4585\u20134589.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Lower frame rate neural network acoustic models", "author": ["Golan Pundak", "Tara N Sainath"], "venue": "Interspeech 2016, pp. 22\u201326, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Exponential moving average model in parallel speech recognition training", "author": ["Tian Xu", "Zhang Jun", "Ma Zejun", "He Yi", "Wei Juan"], "venue": "arXiv preprint arXiv:1703.01024, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "In the last few years, deep neural networks (DNNs) combined with hiddenMarkovmodels (HMMs) have been widely employed in acoustic modeling for large vocabulary speech recognition [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 2, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 3, "context": "More recently, Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, have been shown to outperform DNNs [2, 3, 4].", "startOffset": 131, "endOffset": 140}, {"referenceID": 4, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 5, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 6, "context": "Frame stacking is a effective way that DNNs could learn past and future context knowledge [5, 6, 7].", "startOffset": 90, "endOffset": 99}, {"referenceID": 7, "context": "Though RNNs are able to remember long-term information, frame stacking could also provide useful contextual information [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "However, the neural networks combined with connectionist temporal classification (CTC) criterion gives the frame stacking rebirth [9, 10].", "startOffset": 130, "endOffset": 137}, {"referenceID": 9, "context": "However, the neural networks combined with connectionist temporal classification (CTC) criterion gives the frame stacking rebirth [9, 10].", "startOffset": 130, "endOffset": 137}, {"referenceID": 10, "context": "Connectionist temporal classification (CTC) criterion provides a mechanism to learn an neural network while mapping a input frame sequence to a output label sequence [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 11, "context": "CTC-LSTM acoustic models using context dependent phones (CD-phones) perform as well as conventional models [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "It is an intuitional way to remodel HMM structure in order to match the modeling unit, and decoding network is needed to rebuild correspondingly [13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "It has been demonstrated in CTC systems [9].", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "Sequence discriminative training using state-level minimum bayes risk (sMBR) has shown to further improve performance of neural networks first trained with CE [14, 15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 14, "context": "Sequence discriminative training using state-level minimum bayes risk (sMBR) has shown to further improve performance of neural networks first trained with CE [14, 15].", "startOffset": 159, "endOffset": 167}, {"referenceID": 15, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [16, 10].", "startOffset": 185, "endOffset": 193}, {"referenceID": 9, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [16, 10].", "startOffset": 185, "endOffset": 193}, {"referenceID": 16, "context": "It has shown that blockwisemodel-update filtering (BMUF) outperforms traditional model averaging method, and it is utilized at the synchronization phase [17].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "After synchronizing with BMUF, exponential moving average (EMA) method further updates the model in non-interference way [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "Decentralized method makes full use of computing resource, and we employ the MPI-based mesh AllReduce method [18].", "startOffset": 109, "endOffset": 113}], "year": 2017, "abstractText": "Frame stacking is broadly applied in end-to-end neural network training like connectionist temporal classification (CTC), and it leads to more accurate models and faster decoding. However, it is not well-suited to conventional neural network based on context-dependent state acoustic model, if the decoder is unchanged. In this paper, we propose a novel frame retaining method which is applied in decoding. The system which combined frame retaining with frame stacking could reduces the time consumption of both training and decoding. Long short-term memory (LSTM) recurrent neural networks (RNNs) using it achieve almost linear training speedup and reduces relative 41% real time factor (RTF). At the same time, recognition performance is no degradation or improves sightly on Shenma voice search dataset in Mandarin.", "creator": "LaTeX with hyperref package"}}}