{"id": "1705.06353", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors", "abstract": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora to interpret a word at each step (see below). In contrast, when each vector represents a word, the word is translated into more than one word.\n\n\n\n\nThe literature reviews and reviews of SVD models suggest that most studies (with a particular degree of overlap) employ VD models (and not just VD models) as a way to identify the political implications of VD. This is not to say that the results are completely arbitrary and that these findings are important enough to inform decisions about which SVD models are correct or not. While many studies and other data on SVD modeling are conducted in general (with a particular degree of overlap), a few studies have done it in particular. One of the most influential and influential is the research conducted by Richard Lindbergh, of the University of California, Berkeley, in a systematic review of a paper on the topic of SVD models (H\u00f6tse). Lindbergh's research was conducted in collaboration with the University of Pennsylvania and the National Center for Integrative Studies in Philadelphia. Lindbergh's results suggest that the current models have very large sample sizes (15, 15, 30). However, Lindbergh's findings also suggest that only a few of the large-scale, extensive studies that examined the topic of SVD (H\u00f6tse) have used VD models (and not just VD models) as a way to identify the political implications of VD. Lindbergh's findings suggest that the current models have very large sample sizes (15, 15, 30). However, Lindbergh's findings also suggest that only a few of the large-scale, extensive studies that examined the issue of SVD (H\u00f6tse) have used VD models (and not just VD models) as a way to identify the political implications of VD. Lindbergh's findings suggest that the current models have very large sample sizes (15, 15, 30). However, Lindbergh's findings also suggest that only a few of the large-scale, extensive studies that examined the issue of SVD (H\u00f6tse) have used VD models (and not just VD models) as a way to identify the political implications", "histories": [["v1", "Wed, 17 May 2017 21:29:08 GMT  (993kb,D)", "http://arxiv.org/abs/1705.06353v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christophe bruchansky"], "accepted": false, "id": "1705.06353"}, "pdf": {"name": "1705.06353.pdf", "metadata": {"source": "CRF", "title": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors", "authors": ["Christophe Bruchansky"], "emails": ["christophe@plural.world"], "sections": [{"heading": null, "text": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors\nChristophe Bruchansky christophe@plural.world\nIn this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.\nKeywords: machine learning, natural language processing, vector space model, political discourse, semantics, word embeddings"}, {"heading": "Context and Methodology", "text": "Vector space models (VSMs) represent words in a continuous vector space where semantically similar words are mapped to nearby points (TensorFlow word2vec, 2017). VSMs are produced by algorithms that can analyze large corpora of text and determine how likely two words are to appear in the same passage (word \"co-occurrence\"): the more often two words appear together, the closer these algorithms will place their vectors. The resulting vector space models are not only statistically significant, but also have a semantic value. This is due to the distributional hypothesis stating that words appearing in the same context share semantic meaning (D. Turney & Pantel, 2010).\nVector space models allow machines to classify documents by meaning, understand natural language, and, in our case, create \"semantic word clouds\". They provide \"a bird\u2019seye view of different text sources, including text summaries and their source material. This enables users to explore a text source like a geographical map\u201d (Heuer, 2015).\nPublicly available vector space models have made their appearance in recent years (\u201cpre-trained\u201d vector space models). Examples are word2vec models from Google (Mikolov, Chen, Corrado, & Dean, 2013), GloVe models from Stanford University (Pennington, Socher, & D. Manning, 2014) and FastText models from Facebook (Bojanowski, Grave, Joulin, & Mikolov, 2016). All have further accelerated the adoption of the technology.\nCultural and political sciences are a natural fit for VSMs, and many research studies have started using the technology to analyze political opinions. A large proportion focus on social media, allowing, for instance, the categoriza-\ntion of election-related tweets; others focus on the political discourse itself, such as argument-based analysis (Hirst & Wei Feng, 2015) and semantic word clouds (Chah, 2017). This paper belongs to the latter category.\nA political footprint is a vector-based representation of a political discourse in which each vector represents a word. Political footprints are computed using machine learning technologies, which allows for a systematic and more objective political analysis.\nar X\niv :1\n70 5.\n06 35\n3v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\n01 7\nbased on large corpora of text, they lead to political discourse analysis that relies less on the researcher\u2019s political knowledge or set of beliefs. They are, however, very much dependent on the corpus they were trained with (Wikipedia, Google News, etc.), and more generally on the cultural context any political discourse originates in.\nPolitical footprints focus exclusively on what a statement or speaker says. They are, in this respect, very different from other popular word clouds that depict news and social media trends: the emphasis is on what a speaker has in his or her control.\nOur purpose in this paper is to establish a proof of concept: to use existing technologies and implement a simple version of political footprints in order to see if the results are in any way meaningful. It is based on the following technologies:\n\u2022 IBM Watson (IBM Watson Natural Language Understanding, 2017): returns a list of entities and keywords included in a text, with a relevance, sentiment, and emotion score for each of them;\n\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);\n\u2022 TensorFlow and TensorBoard (TensorFlow, 2017): open-source software library for machine intelligence, its TensorBoard interface is particularly useful for VSM quick visualisation.\nThe sections below describe each of the steps taken to produce political footprints. The first is to process raw data, the second is to identify key terms, the third is to compute political footprints, and the last is to interpret them using heuristics. All scripts and data are available on Github (Bruchansky, 2017)."}, {"heading": "Raw data", "text": "Four case studies were chosen to prove our concept: the Kyoto Protocol, the Paris Agreement, and the 2008 and 2016 U.S. presidential elections. The Kyoto Protocol and Paris Agreement were taken from the United Nations website (United Nations Framework Convention on Climate Change, 2017), and the U.S. elections televised debates from the American Presidency Project (Peters & T. Woolley, 2017), a \"nonprofit and nonpartisan source of presidential documents\" hosted at the University of California.\nThey have been chosen, firstly, because most readers will be familiar enough with the topics to compare the results with their own intuition, and, secondly, because they are accessible to anyone wishing to consult the data.\nIn the case of political elections, televised debates were chosen over rallies and other public declarations because they are assumed to be representative of the U.S. political landscape: processes defining which candidates were invited\nto the televised debates, how long they could speak, and on what matters are assumed to be objective enough for our purpose. We can thus easily apply the same method to any election. However, we are aware that no debate can perfectly reflect a political landscape. In the case of the U.S. elections, this assumption leads us to exclude independent and third party candidates, which is arguably regrettable.\nThe goal of this study is to focus on a political discourse, not on its context or how it has been received. We have thus removed any audience reaction from the transcripts (i.e. \u201capplause\u201d, \u201claugh\u201d, etc.) and all questions from the moderators. We lose a certain amount of important information in doing so, which means it is sometimes difficult to understand what a candidate\u2019s answer was about, but this is the price we pay when we only take into account the candidates\u2019 own words."}, {"heading": "Key terms identification", "text": "Our choice to use IBM Watson is a convenient one: it allows us to run our analysis on any personal computer, and with fast results. It comes with a cost: there is not much control over or explanation of how the terms are selected and weighted. For instance, IBM Watson Natural Language Understanding generates several files per text, including one for its entities and one for its keywords. It\u2019s not clear how the two lists are created, and it is assumed that entities are more relevant than keywords since they are more structured objects (entities have types and subtypes). If a term exists in both files, our script only keep the entity version.\nAs the reader will later realize, IBM Watson\u2019s emotion detection is not always totally reliable. A better solution could be to use an emotion detection mechanism that can adapt to political language (Rheault, Beelen, Cochrane, & Hirst, 2016). In any case, it is important to keep in mind that an emotion attached to a word is not necessary targeting that word. An angry feeling detected when using the word \u201cpeople\u201d does not mean that the discourse is necessary expressing anger towards people, but that speaking about people generates anger. In the case of Hillary Clinton\u2019s political footprint, we have also noticed how much linguistic features such as sarcasm remain problematic for tools like IBM Watson.\nNo other commercial (i.e. Google CloudPlatform) or open source solutions (i.e. NLTK or Stanford CoreNLP) have been tested as part of this paper. Words selected by the default IBM Watson API were for the most part corresponding to our intuition. They were considered good enough for our proof of concept, in the sense that if using a generic tool such as IBM Watson can provide meaningful data, more sophisticated implementations could only improve our results."}, {"heading": "Political footprints", "text": "We have chosen GloVe6B for our pre-trained vector space model, which is based on Wikipedia 2014 and Gigaword 5. We took this decision due to GloVe\u2019s public availability, its\npopularity among researchers, and a number of convenient features, such as being able to quickly run our scripts using a 50-dimension space, before extending it to 300 dimensions. A VSM that defines each word using 50 variables is good enough for early tests, but it will not capture as many nuances as one based on 300 variables.\nWe have, however, encountered several issues. The first is that this model is word based and does not include compounds nouns such as \u201cWall Street\u201d or \u201cNew York Times\u201d (see figure 2). This was not so much of an issue in our case study since we were familiar with the data. We could easily guess the compound nouns: we knew that \u201cstreet\u201d was a relevant word in Bernie Sanders\u2019s political footprint because of Wall Street. However, this requires a degree of guesswork and interpretation from the researcher that could be avoided if the pre-trained vector space model supported compound nouns.\nA second issue is the date of the reference corpora (2014 for Wikipedia and 2010 for Gigaword 5) compared to the times of the U.S. elections (2008, 2016). We could only assume that the words\u2019 meaning and usage remained unchanged during that period. An improvement would be to have access to yearly updates of pre-trained vector space models. However, this would create new issues, such as how to compare texts written in different years if we do not base our analysis on a single reference model.\nAnother problem is that GloVe\u2019s pre-trained vector space models are currently only available in English. A multilanguage alternative is FastText from Facebook, which is compatible with our scripts. It was, however, only few months old at the time of our research, and we decided to stick with a more established GloVe model for our proof of concept.\nLet us now look at how we can put political footprints to practical use.\nHeuristic 1: main themes of a discourse and what they mean\nThe first heuristic could be described as a clustering technique in which we leverage the relevance score obtained from IBM Watson: we take the most relevant words from a discourse, and select the closest words in the vector space model for each one. We obtain a series of clusters each centered on a relevant term.\nLet us look, for instance, at key terms used in the Kyoto protocol (8483 words text document, see figure 3) and the Paris agreement (7383 words text document, see figure 4), and highlight the closest words to \"climate\".\nIn the case of the 2016 presidential election, IBM Watson tells us that the most relevant words from Donald Trump\nwere trade deals (expressed with sadness), ISIS (anger), and Clinton (disgusted). Hillary Clinton was very much focused on new jobs and the affordable care act.\nThanks to political footprints, we can determine that words most closely related to trade (deals) in Donald Trump\u2019s vocabulary were Nafta (used with fear), China, Korea, the world (used with sadness), countries (disgust), and other economic terms such as tax, business, and companies (see figure 6).\nHillary Clinton\u2019s care-related vocabulary included terms such as women (used with disgust, probably in the context of gender equality), children, insurance and health, both presented positively (see figure 5).\nWhat makes us confident about our results is the appearance of terms that were widely commented on during the 2016 U.S. election: \"women\" and \"health care\" for Hillary Clinton, \"China\" and \"Nafta\" for Donald Trump.\nWhat is important to understand, and what is at the core of political footprints, is that word similarities have been identified without any human intervention: they have been discovered by machines based on how frequently words appear together, either on Wikipedia or other large corpora of text. This is what allows us to produce a more objective analysis of political discourses. To be clear, there is a strong cultural bias, originating in how these words have been used on Wikipedia, news feeds, etc. But it is not coming from the researcher performing the analysis.\nThere is no such thing as objective political discourse analysis. However, solutions to reduce bias from sources such as Wikipedia could be to train VSMs on a set of data coming from different sources (Wikipedia, Google News, social media, political discourses themselves, etc.), or to agree on the criteria for selecting unbiased texts. We are highly skeptical about the latter: a paradigm at the core of machine learning is that a large set of data is often closer to reality than a small curated set.\nWord similarities are not inferred from the discourse we analyze but from the large corpus of text that was used to train words. Comparing distances (cosine similarity) between words does not provide any information about the discourse we study, but only about the culture and language it is based on; information about the discourse lies in the choice of these words instead of others, their relevance and associated emotion.\nThis heuristic was performed manually but could have been automated using unsupervised machine learning techniques such as k-means (Piech, 2013). It would have then been possible to compare their standard implementation with one that takes into account word relevance. Using the most\nrelevant words to define our word clusters makes sense intuitively since they are \u201cat the center\u201d of the discourse.\nHeuristic 2: compare how a theme is appropriated by each debate participant\nIn this heuristic, we choose a term or theme of our choice. Let us assume we are interested in American \"values\".\nWe select the 20 closest words to \"values\" (i.e. \"social\", \"civilization\", \"inequality\", \"liberty\", etc.) and see how many presidential candidates have used them.\nWe then choose a couple of these terms, ideally those that have been used by many participants and that have many different meanings. \"Social\" and \"interest\" were picked in our 2016 US election example, but it could also have been \u201cethical\u201d, \u201cprinciple\u201d or \u201cfreedom\u201d: any term that can be used differently depending on a participant\u2019s political views.\nWe finally compute the list of related terms for each candidate using our first heuristic.\nFigure 7 shows the main value-related terms used during the 2008 U.S. presidential election. As we can see, debated values were social, related to property, civilization, inequality, and liberty.\nCivilization, property, standards, social, interests, and liberty also appear during the 2016 U.S. presidential election (see figure 8). This is an indication of both political footprints\u2019 robustness and the consistency of the terms used from one election to the other.\nWe have, with the two word clouds above, the confirmation that social values played an important role in the two elections. Let us now see how three candidates have referred to this topic (social-related terms in the GloVe vector space model).\nWhen John McCain talked about social issues during the 2008 Republican primaries, he talked mostly about healthcare, security (\u201cradical\u201d was used with fear), jobs (\u201cworker\u201d generated some anger), and rights. He decided to not focus on a single topic and instead discussed various social issues (see figure 9).\nDuring the 2008 Democrat primaries, Barrack Obama decided to focus more on health care. Jobs were high on his agenda (\u201cincome\u201d, \u201ceconomic\u201d), and inequalities were an important topic as well (see figure 10).\nIn 2016, Bernie Sanders also put health care at the top of his social agenda. His stance on economic issues centered more on the poor (income, unemployment, poverty) than the two other candidates, see figure 11.\nLet us explore what American \u201cinterests\u201d meant during the two presidential elections, and look at related words from\nBarack Obama and Donald Trump. During the 2008 Democrat primary, Barack Obama\u2019s closest topics to (American) interests were foreign affairs (with a sense of fear), the economy (\u201cowners\u201d,\u201d economic\u201d), and social issues (\u201ccommunity\u201d, \u201cordinary\u201d, \u201csocial\u201d) (see figure 12).\nDonald Trump had something different in mind when talking about American interests (2016 Republican primaries). His political footprint encompasses matters centering on the economy and trade deals, see figure 13.\nUsing a unique reference model is both the biggest weakness and strength of our political footprints. On the one hand, it does not accurately represent the words\u2019 meaning during each campaign (election years were not the same as the years covered by our VSM), but on the other hand it provides an easy way to compare them: the semantic relation between words have been defined at the GloVe level, we can thus compare texts that have not necessarily used the same words to address the same issue.\nHeuristic 3: sort discourses by style and affinity\nThe purpose of this last section is to test whether the general properties of our political footprints could be used for political discourse analysis. For this, we visualized political footprints coloured by relevance, sentiment, and emotions. We calculated their center of gravity, or centroids (Abdi, 2017), and compared their distance with one another.\nNone of these approaches were, at least in our current implementation, conclusive. A few interesting properties were revealed in our US election example, but not enough to exclude them being mere coincidences. Bernie Sanders\u2019 political footprint was more focused than the others: Wall Street was detected as being by far the most relevant of his topics (along with Hillary Clinton). But this might also have been due to a strategy of repeating the same words instead of describing his views in different ways. We cannot conclude that Bernie Sanders was fundamentally more focused.\nHillary Clinton\u2019s emotions were less visible. But as ex-\nplained above, this might have been due to her sarcastic tone, and more subtle ways of expressing her emotions.\nFinally, except possibly for the fact that Bernie Sanders\u2019 centroid was the furthest from Donald Trump\u2019s, their positions did not correspond to any of our intuitions.\nUsing centroids is arguably a simplistic way to look at political footprints: they do not take into account relevance, sentiments and emotions. For instance, seeing Islam as a positive and secondary topic counts exactly the same in our model as seeing Islam as a negative and central topic."}, {"heading": "Conclusions", "text": "In this paper, we have presented a very simple implementation of political footprints: one that can be computed on any personal computer and still leverage some of the semantic value embedded in vector space models. Identifying the \u201creal\u201d meaning of a political discourse is in itself an impossible endeavor. However, political footprints fit surprisingly well with the discourse of each US presidential candidate and how they have been covered in the press. They have been obtained with nearly no human intervention, which makes them a potentially very useful tool for political discourse analysis.\nA better way to test their validity would be to compare resulting word clouds with those that the public, the commentators, and the authors of a discourse themselves would draw. We could ask an audience to recognize a discourse based on its political footprint, and measure the success rate. Or we could compare political footprints of the same politician in different contexts (debates, rallies, public declarations, twitter) and test how consistent they are.\nWe have suggested various improvements, such as using a domain-specific and open source solution instead of IBM Watson, using a pre-trained vector space model that does not break compound nouns, and combining existing clustering techniques with our heuristics.\nPolitical footprints are meant to be used by anyone interested in political discourse analysis, in highlighting some of the different meanings a word can have in politics, and the underlying semantic tensions underlying any political debate. It is hoped that such tools will help researchers and commentators alike reduce their dependence on personal opinions when analyzing political discourse. Political footprints emphasize what politicians have control over and responsibility for: their own words.\n\"When a man commits himself to anything, fully realising that he is not only choosing what he will be, but is thereby at the same time a legislator deciding for the whole of mankind \u2013 in such a moment a man cannot escape from the sense of complete and profound responsibility\" (Sartre, 1946)."}, {"heading": "Acknowledgement", "text": "We would like to thank Niel Chah (Chah, n.d.) for his support during the political footprint development process\nand Araz Taeihagh (Taeihagh, n.d.) for his feedback and time spent reviewing the paper.\nReferences\nAbdi, H. (2017). Centro\u00efd, center of gravity, center of mass, barycenter. https://www.utdallas.edu/~herve/ Abdi-Centroid2007-pretty.pdf. Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2016). Enriching word vectors with subword information. https:// github.com/facebookresearch/fastText. Bruchansky, C. (2017). Political footprints: Political discourse analysis using pre-trained word vectors. https://github .com/Plural-thinktank/pfootprint. Plural think tank. Chah, N. (n.d.). https://nielchah.com/. Chah, N. (2017). word2vec4everything. https://github.com/ nchah/word2vec4everything. D. Turney, P., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics (Vol. 37). doi: http:// www.jair.org/media/2934/live-2934-4846-jair.pdf Heuer, H. (2015). Text comparison using word vector representations and dimensionality reduction. https://arxiv.org/ abs/1607.00534. Hirst, G., & Wei Feng, V. (2015). Automatic exploration of argument and ideology in political texts. http://ftp.cs.toronto .edu/pub/gh/Hirst+Feng-ECA-2016.pdf. Ibm watson natural language understanding. (2017). https://www.ibm.com/watson/developercloud/ natural-language-understanding.html. Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. https:// code.google.com/archive/p/word2vec/. Pennington, J., Socher, R., & D. Manning, C. (2014). Glove: Global vectors for word representation. https:// nlp.stanford.edu/pubs/glove.pdf. Peters, G., & T. Woolley, J. (2017). The american presidency project. http://www.presidency.ucsb.edu/. Piech, C. (2013). K means. http://stanford.edu/~cpiech/ cs221/handouts/kmeans.html. Rheault, L., Beelen, K., Cochrane, C., & Hirst, G. (2016). Measuring emotion in parliamentary debates with automated textual analysis. doi: https://doi.org/10.1371/journal.pone.0168843 Sartre, J.-P. (1946). Existentialism is a humanism. https://www.marxists.org/reference/archive/ sartre/works/exist/sartre.htm. (translated by Mairet, Philip) Taeihagh, A. (n.d.). Assistant professor of public policy, singapore management university (smu), form. oxford and unsw. https://scholar.google.ca/citations?user= Bwt1iRUAAAAJ&hl=en. Tensorflow. (2017). https://www.tensorflow.org/. Tensorflow word2vec. (2017). https://www.tensorflow.org/ tutorials/word2vec. United nations framework convention on climate change. (2017). http://unfccc.int."}], "references": [{"title": "Centro\u00efd, center of gravity, center of mass, barycenter", "author": ["H. Abdi"], "venue": "https://www.utdallas.edu/~herve/ Abdi-Centroid2007-pretty.pdf.", "citeRegEx": "Abdi,? 2017", "shortCiteRegEx": "Abdi", "year": 2017}, {"title": "Enriching word vectors with subword information. https:// github.com/facebookresearch/fastText", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Political footprints: Political discourse analysis using pre-trained word vectors", "author": ["C. Bruchansky"], "venue": "https://github .com/Plural-thinktank/pfootprint. Plural think tank.", "citeRegEx": "Bruchansky,? 2017", "shortCiteRegEx": "Bruchansky", "year": 2017}, {"title": "word2vec4everything", "author": ["N. Chah"], "venue": "https://github.com/ nchah/word2vec4everything.", "citeRegEx": "Chah,? 2017", "shortCiteRegEx": "Chah", "year": 2017}, {"title": "From frequency to meaning: Vector space models of semantics (Vol", "author": ["P.D. Turney", "P. Pantel"], "venue": null, "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Text comparison using word vector representations and dimensionality reduction", "author": ["H. Heuer"], "venue": "https://arxiv.org/ abs/1607.00534.", "citeRegEx": "Heuer,? 2015", "shortCiteRegEx": "Heuer", "year": 2015}, {"title": "Automatic exploration of argument and ideology in political texts. http://ftp.cs.toronto .edu/pub/gh/Hirst+Feng-ECA-2016.pdf", "author": ["G. Hirst", "V. Wei Feng"], "venue": null, "citeRegEx": "Hirst and Feng,? \\Q2015\\E", "shortCiteRegEx": "Hirst and Feng", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. https:// code.google.com/archive/p/word2vec", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation. https:// nlp.stanford.edu/pubs/glove.pdf", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The american presidency project. http://www.presidency.ucsb.edu", "author": ["G. Peters", "J.T. Woolley"], "venue": null, "citeRegEx": "Peters and Woolley,? \\Q2017\\E", "shortCiteRegEx": "Peters and Woolley", "year": 2017}, {"title": "K means", "author": ["C. Piech"], "venue": "http://stanford.edu/~cpiech/ cs221/handouts/kmeans.html.", "citeRegEx": "Piech,? 2013", "shortCiteRegEx": "Piech", "year": 2013}, {"title": "Measuring emotion in parliamentary debates with automated textual analysis", "author": ["L. Rheault", "K. Beelen", "C. Cochrane", "G. Hirst"], "venue": null, "citeRegEx": "Rheault et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rheault et al\\.", "year": 2016}, {"title": "Existentialism is a humanism", "author": ["Sartre", "J.-P"], "venue": null, "citeRegEx": "Sartre and J..P.,? \\Q1946\\E", "shortCiteRegEx": "Sartre and J..P.", "year": 1946}], "referenceMentions": [{"referenceID": 5, "context": "This enables users to explore a text source like a geographical map\u201d (Heuer, 2015).", "startOffset": 69, "endOffset": 82}, {"referenceID": 3, "context": "A large proportion focus on social media, allowing, for instance, the categorization of election-related tweets; others focus on the political discourse itself, such as argument-based analysis (Hirst & Wei Feng, 2015) and semantic word clouds (Chah, 2017).", "startOffset": 243, "endOffset": 255}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 33, "endOffset": 58}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 34, "endOffset": 122}, {"referenceID": 8, "context": "\u2022 GloVe from Stanford University (Pennington et al., 2014): package featuring word vectors trained using Wikipedia (2014) and Gigaword 5 (2011);", "startOffset": 34, "endOffset": 144}, {"referenceID": 2, "context": "All scripts and data are available on Github (Bruchansky, 2017).", "startOffset": 45, "endOffset": 63}, {"referenceID": 10, "context": "This heuristic was performed manually but could have been automated using unsupervised machine learning techniques such as k-means (Piech, 2013).", "startOffset": 131, "endOffset": 144}, {"referenceID": 0, "context": "We calculated their center of gravity, or centroids (Abdi, 2017), and compared their distance with one another.", "startOffset": 52, "endOffset": 64}], "year": 2017, "abstractText": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.", "creator": "TeX"}}}