{"id": "1705.06640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "abstract": "Deep learning (DL) systems are increasingly deployed in security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs.\n\n\n\n\nA simple example to test this question is the use of the ability to define an exact DDL model with a few parameters (typically the DDL model and the DDL model) that can be found in some situations. In these experiments, DDL models will be more efficient than the existing DDL models, and that can be improved to provide even more insights to the user. In contrast, the models with the same parameters will be more effective than the existing DDL models.\nTo show how DL-learning can be improved, we will use the example of a DDL model with a few parameters. The DDL model with the same parameters will be less efficient and can be implemented in some scenarios. The models using the DDL model will be more efficient and can be used in many scenarios. To show how DDL-learning can be improved, we will use the example of a DDL model with a few parameters. The DDL model with the same parameters will be more efficient and can be implemented in some scenarios.\nDDL modeling in a linear-scale model can be used in several scenarios. The model with the same parameters will be less efficient and can be implemented in some scenarios. A linear-scale model can be used in several scenarios.\nIn addition, we will use the example of a DDL model with the same parameters and have the same parameters in an integrated data model for all possible corner-cases and the DDL model.\nA linear-scale model can be used in several scenarios. The model with the same parameters will be less efficient and can be implemented in some scenarios. A linear-scale model can be used in multiple scenarios.\nIn addition, we will use the example of a DDL model with the same parameters and have the same parameters in an integrated data model for all possible corner-cases and the DDL model.\nDDL modeling in a linear-scale model can be used in several scenarios. The model with the same parameters will be less efficient and can be implemented in some scenarios", "histories": [["v1", "Thu, 18 May 2017 15:09:39 GMT  (7849kb,D)", "http://arxiv.org/abs/1705.06640v1", null], ["v2", "Sun, 4 Jun 2017 23:40:43 GMT  (4771kb,D)", "http://arxiv.org/abs/1705.06640v2", null], ["v3", "Mon, 10 Jul 2017 17:05:40 GMT  (4771kb,D)", "http://arxiv.org/abs/1705.06640v3", null], ["v4", "Sun, 24 Sep 2017 15:55:11 GMT  (5526kb,D)", "http://arxiv.org/abs/1705.06640v4", "To be published in SOSP'17"]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.SE", "authors": ["kexin pei", "yinzhi cao", "junfeng yang", "suman jana"], "accepted": false, "id": "1705.06640"}, "pdf": {"name": "1705.06640.pdf", "metadata": {"source": "CRF", "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "authors": ["Kexin Pei", "Yinzhi Cao", "Junfeng Yang", "Suman Jana"], "emails": ["suman}@cs.columbia.edu", "yinzhi.cao@lehigh.edu"], "sections": [{"heading": null, "text": "We design, implement, and evaluate DeepXplore, the first white-box framework for systematically testing real-world DL systems. We address two main problems: (1) generating inputs that trigger different parts of a DL system\u2019s logic and (2) identifying incorrect behaviors of DL systems without manual effort. First, we introduce neuron coverage for systematically estimating the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved efficiently using gradient-based optimization techniques.\nDeepXplore efficiently finds thousands of unique incorrect corner-case behaviors (e.g., self-driving cars crashing into guard rails, malware masquerading as benign software) in state-of-the-art DL models trained on five popular datasets including driving data collected by Udacity from cars around Mountain View (CA) and ImageNet data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 33.2% higher neuron coverage on average than existing testing methods. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy or identify polluted training data."}, {"heading": "1. Introduction", "text": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing\ngames like Go [43]. This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].\nHowever, such usage of DL techniques presents new challenges as the predictability and correctness of such systems are of crucial importance. Unfortunately, DL systems, despite their impressive capabilities, often demonstrate incorrect/unexpected behaviors in corner cases for several reasons such as biased training data, overfitting, and underfitting. In safety- and security-critical settings, such incorrect behaviors can lead to disastrous consequences like a fatal collision of a self-driving car. For example, a Google self-driving car recently crashed into a bus because it expected the bus to yield under a set of rare conditions but the bus did not [19]. Such a corner case was not part of Google\u2019s test set and therefore such behavior never showed up during testing.\nTherefore, safety- and security-critical DL systems, just like traditional software, must be tested systematically for different corner cases to detect any potential flaws or undesired behaviors. This presents a new system problem as large-scale automated and systematic testing of real-world DL systems with thousands of neurons and millions of parameters for all corner cases is extremely challenging.\nThe standard approach for testing DL systems is to gather as much real-world test data as possible. However, the large input spaces of real-world DL systems (e.g., all possible road conditions for a self-driving car) mean that data gathered by such an approach can never cover more than a tiny fraction (if any at all) of all possible corner cases. Moreover, such approaches also require manual labeling of test data that is hard to scale for a large number of test inputs.\nRecent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems. The key idea is to apply minimal perturbations on an existing image to create images that get classified by DL models differently than the original picture but still look the same to the human eye. While such adversarial images can expose some erroneous behaviors of a DL model, the main restriction of such an approach is that it must limit its perturbations to tiny changes or require manual inspection. Moreover, just like other forms of existing DL testing, the adversarial images only cover a small part of the logic of a DL system as we will show in Section 7. In essence, the current machine learning testing practices for\nar X\niv :1\n70 5.\n06 64\n0v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n01 7\nfinding incorrect corner cases are analogous to finding buggy behavior in conventional programs by using test inputs with low code coverage and thus are less likely to find the bugs.\nAutomated systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. The main challenges are twofold: (i) how to generate inputs that will cover different parts of the logic of a DL system to uncover different types of erroneous behaviors and (ii) how to identify erroneous behaviors of a DL system without manual labeling/checking. We address both of these problems in this paper.\nFirst, we introduce the concept of neuron coverage for estimating the amount of logic of a DL system exercised by a set of test inputs based on how many neurons are activated (i.e., output value goes over a threshold) by the test inputs. DeepXplore is designed to generate inputs that maximize a DL system\u2019s neuron coverage. At a high level, neuron coverage of DL systems is similar to code coverage of traditional systems, a standard metric for measuring the amount of code exercised by an input in a traditional software. However, code coverage itself is not a good metric for estimating coverage of DL systems as most rules in DL systems, unlike traditional software, are not written manually by a programmer but rather is learned from training data. In fact, we find that for most of the DL systems that we tested, even a single randomly picked test input was able to achieve 100% code coverage while the neuron coverage was less than 10%.\nNext, we show how multiple DL systems with similar functionality (e.g., self-driving cars by Google, Tesla, and Uber) can be used as cross-referencing oracles to identify erroneous corner-cases without manual checks. For example, if one self-driving car decides to turn left while others turn right for the same input, one of them is likely to be incorrect. Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61]. In this paper, we demonstrate how differential testing can be applied to DL systems.\nFinally, we demonstrate how the problem of generating test inputs that maximize neuron coverage of a DL system while also exposing as many differential behaviors (i.e., differences between multiple similar DL systems) as possible can be formulated as a joint optimization problem and it can be efficiently solved for large-scale real-world DL classifiers. Unlike traditional programs, the functions approximated by most popular Deep Neural Networks (DNNs) used by DL systems are differentiable. Therefore, their gradients with respect to inputs can be calculated accurately given whitebox access to the corresponding model. In this paper, we show how these gradients can be used to solve the above-mentioned joint optimization problem efficiently.\nWe design, implement, and evaluate DeepXplore, to our knowledge, the first efficient whitebox testing framework\nfor large-scale DL systems. Besides maximizing neuron coverage and behavioral differences between DL systems, DeepXplore also supports adding custom constraints by the users for simulating different types of input conditions while generating test inputs (e.g., different types of lighting and occlusion for image/video). We demonstrate that DeepXplore efficiently finds thousands of unique incorrect corner-case behaviors (e.g., self-driving cars crashing into guard rails) in 15 state-of-the-art DL models trained using five real-world datasets including driving data collected by Udacity from cars around Mountain View (CA), image data from ImageNet and MNIST, and Android/PDF malware data from Drebin and Contagio. For all of the tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 34.4% and 33.2% higher neuron coverage on average than the same number of randomly picked inputs and adversarial inputs respectively. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy as well as identify potentially polluted training data. We achieve up to 3% improvement in classification accuracy by retraining a DL model on inputs generated by DeepXplore compared to retraining on the same number of randomly picked or adversarial inputs.\nOur main contributions are:\n\u2022 We introduce neuron coverage as the first whitebox testing metric for DL systems to estimate the amount of logic explored by a set of test inputs.\n\u2022 We demonstrate how the problem of finding a large number of behavioral differences between similar DL systems while maximizing neuron coverage can be formulated as a joint optimization problem. We present a gradient-based algorithm for solving this problem efficiently.\n\u2022 We implement all of these techniques as part of DeepXplore, the first whitebox DL-testing framework that exposed thousands of unique incorrect corner-case behaviors\n(e.g., self-driving cars crashing into guard rails as shown in Figure 1) in 15 state-of-the-art DL models with a total of 132, 057 neurons trained on five popular datasets containing 162 GB of data.\n\u2022 We demonstrate how the tests generated by DeepXplore can also be used to retrain the corresponding DL systems to improve classification accuracy or to identify polluted training data."}, {"heading": "2. DL Background", "text": "The main component of any DL system is an underlying deep neural network (DNN). A DNN consists of multiple layers, each containing multiple neurons as shown in Figure 2. A neuron is an individual computing unit inside a DNN that applies an activation function on its inputs and passes the result to other connected neurons (see Figure 2). The common activation functions include sigmoid, hyperbolic tangent, or ReLU (Rectified Linear Unit) [32]. A DNN usually has at least three (often more) layers: one input, one output, and one or more hidden layers. Each neuron in one layer has directed connections to the neurons in the next layer. The numbers of neurons in each layer and the connections between them vary significantly across DNNs. Overall, a DNN can be defined mathematically as a multi-input, multioutput parametric function F composed of many parametric sub-functions representing different neurons.\nEach connection between the neurons in a DNN is bound to a weight parameter characterizing the strength of the connection between the neurons. For supervised learning, the weights of the connections are learned during training by minimizing a cost function over all the training data. DNNs can be trained using different training algorithms, but gradient descent using backpropagation is by far the most popular training algorithm for DNNs [40].\nDNNs are known for their extraordinary capability of learning layer-wise representations of high-dimensional data. Each layer of the network transforms the information contained in its input to a higher-level representation of the data. For example, consider a pre-trained network shown in Figure 3b for classifying images into two categories: human faces and cars. First, the input layer of the DNN takes a flattened vector of the pixel values of an input image. Next, the first few hidden layers transform the raw pixel values into lowlevel texture features like edges or colors and feed them to the deeper layers [62]. Finally, the last few layers extract and assemble the meaningful high-level abstractions like noses, eyes, wheels, and headlights to decide if the image contains a human face or a car. Note that such layer-wise representation described above is automatically learned during the training of the DNN without any human instructions or interactions."}, {"heading": "3. Limitations of existing DNN testing", "text": "Expensive labeling effort. Existing DNN testing techniques require prohibitively expensive human effort to provide cor-\nrect labels/actions for tasks that a DNN is designed to perform (e.g., self-driving a car, image classification, and malware detection). As machine learning systems are deployed to handle increasingly complex and high-dimensional real-world inputs, human beings, even the domain experts, often have difficulty in performing a task correctly in an efficient manner for a large dataset. For example, consider a DNN designed to identify potentially malicious executable files. Even a security professional can hardly determine whether an executable is malicious or benign without executing it. However, executing and monitoring it inside a sandbox incur a significant overhead and therefore makes manual labeling significantly harder to scale to a large number of inputs. Low test coverage. None of the existing DNN testing schemes even try to cover different rules of the DNN. Therefore, the test inputs often fail to uncover all possible erroneous behaviors of a DNN.\nFor example, DNNs are often tested by simply dividing a whole dataset into two random parts\u2014one for training and the other for testing. The testing set in such cases may only exercise a small subset of all rules learned by a DNN. Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60]. However, the adversarial inputs also only cover a small part the rules learned by a DNN as they are not designed to maximize coverage. Moreover, they are also inherently limited to small imperceptible perturbations around a test input as larger perturbations will visually change the input and therefore will require manual inspection for checking whether the DNN is indeed misclassifying the image.\nTo better understand the problem of low test coverage of rules learned by a DNN, we resort to and compare with a similar problem in testing traditional software. Figure 3 shows a side-by-side comparison of how a traditional program and a DNN handle inputs and produce outputs. Specifically, the figure shows the similarity between the traditional software and the DNN: in software program, each statement performs a certain operation to transform the output of previous statement(s) to the input to the following statement(s), while in\nDNN, each neuron transforms the output of previous neuron(s) to the input of the following neuron(s). Of course, unlike traditional software, there are no explicit branches in DNNs, but a neuron\u2019s influence on the downstream neurons decreases with the neuron\u2019s output value. A lower output value indicates less influence and vice versa. When the output value of a neuron becomes zero, the neuron does not have any influence on the downstream neurons.\nThe problem of low coverage in testing traditional software is well understood. As demonstrated in Figure 3a, the buggy behavior will never be seen unless the test input is 0xdeadbeef. The odds of randomly picking such a value is very small. Similarly, low-coverage test inputs will also leave different behaviors of DNNs unexplored. For example, consider a simplified neural network, as shown in Figure 3b, that takes an image as input and classifies it into two different classes: cars and faces. The text in each neuron (represented as a node) denotes the object/property that the neuron detects, and the number in each neuron is the real value outputted by that neuron. The number indicates how confident the neuron is about its output. Note that randomly picked inputs are highly unlikely to set high output values for the unlikely combination of neurons. Therefore, it will not be possible to observe whether the behavior of the DNN for such inputs is correct or not even when a large number of tests are used. For example, if an image results in neurons labeled as \u201cNose\u201d and \u201cBlue\u201d having high value and the neural network misclassifies it as a car, such a behavior will never be seen during regular testing as the chances of an image containing a blue nose (e.g., a picture of a clown) is very small."}, {"heading": "4. Overview of DeepXplore", "text": "In this section, we provide a general overview of our techniques for solving the aforementioned limitations of existing DNNs testing techniques.\nFigure 4 provides a high-level overview of DeepXplore, our whitebox framework for systematically testing DNNs\nfor erroneous corner case behaviors. DeepXplore takes unlabeled test inputs as seeds and generates new test inputs that cover a large number of different neurons (i.e., activates them to a value above a customizable threshold) while causing the tested DNNs to behave differently. Specifically, DeepXplore solves a joint optimization problem with constraints while maximizing differential behaviors and neuron coverage. DeepXplore also supports enforcing custom domain-specific constraints as part of the joint optimization process to ensure that the generated test inputs are valid and can appear in practice. For example, the value of an image pixel has to be between 0 and 255. Such domain-specific constraints can be specified by the users of DeepXplore.\nWe designed an algorithm for efficiently solving the joint optimization problem mentioned above using gradient ascent. First, we compute the gradient of the outputs of the neurons in both output and hidden layers with the input value as a variable and the weight parameter as a constant. Such gradients are efficiently computable for most DNNs. Note that the gradient computation is efficient because our whitebox approach has access to the tested DNNs\u2019 weights and the intermediate neuron values. Next, we iteratively perform gradient ascent to modify the test input toward maximizing the objective function of the joint optimization problem described above. Essentially, we perform a gradientguided local search starting from the seed inputs and find new inputs that maximize the properties described above. Note that, at a high level, our gradient computation is similar to the backpropagation performed during the training of a DNN, but the key difference is that, unlike us, backpropagation treats the input value as a constant and the weight parameter as a variable. A working example. We use Figure 3b as an example to show how DeepXplore generate test inputs for DNNs. Consider that we have two DNNs to test\u2014both perform similar tasks, i.e., classifying images into cars or faces, as shown in Figure 5, but they are trained independently with different datasets and parameters. Let us also assume that we have a seed test input, the image of a red car, which both DNNs identify as a car as shown in Figure 5a.\nDeepXplore tries to maximize the chances of finding differential behavior by perturbing the input, i.e., the image of the red car, towards maximizing its probability of being classified as a car by one DNN but minimizing its probability of being classified as a car by another DNN. DeepXplore\nalso tries to cover as many neurons as possible by activating (i.e., causing a neuron\u2019s output to have a value greater than a threshold) inactive neurons in the hidden layer. We also add domain-specific constraints (e.g., ensure the pixel values are integers within 0 and 255 for image input) to make sure that the perturbed input still represent real-world images. The joint optimization algorithm will iteratively perform a gradient ascent to find a modified input that satisfies the goals described above. DeepXplore eventually generates a test input where the DNNs\u2019 outputs differ, e.g., one DNN thinks it is a car while the other thinks it is a face as shown in Figure 5b. DeepXplore will continue to iteratively activate more neurons and generate new test inputs that demonstrate such differential behaviors.\nFigure 6 illustrates the basic concept of our technique using gradient ascent. Starting from a seed input, DeepXplore performs the guided search by the gradient in the input space of two similar DNNs supposed to handle the same task such that it finally uncovers the test inputs that lie between the decision boundary of these two DNNs. Such test inputs will be classified differently by the two DNNs. Note that while the gradient provides the rough direction toward reaching the goal (e.g., finding difference-inducing inputs), it does not guarantee the fastest convergence. Thus as shown in Figure 6, the gradient ascent process often does not follow a straight path towards reaching the target."}, {"heading": "5. Methodology", "text": "In this section, we provide a detailed technical description of our algorithm. First, we define and explain the concepts of\nneuron coverage and gradient for DNNs. Next, we describe how the testing problem can be formulated as a joint optimization problem. Finally, we provide the algorithm for joint optimization using gradients."}, {"heading": "5.1 Definitions", "text": "Neuron coverage. We define neuron coverage of a set of test inputs as the ratio of the number of unique activated neurons for all test inputs in the set and the total number of neurons in the DNN. We consider a neuron to be activated if its output is higher than a threshold value (e.g., 0).\nMore formally, let us assume that all neurons of a DNN are represented by the set N = {n1, n2, ...}, all test inputs are represented by the set T = {x1,x2, ...}, and out(n,x) is a function that returns the output value of neuron n in the DNN for a given test input x. Let t represent the threshold for considering a neuron to be activated. Neuron coverage can be defined as follows.\nNCov(T,x) = |{n|\u2200x\u2208T,out(n,x>t)}||N |\nTo demonstrate how neuron coverage is calculated in practice, consider the DNN showed in Figure 3b. The neuron coverage (with threshold 0) for the input picture of the red car shown in Figure 3b will be 5/8 = 0.625. Gradient. The gradients or forward derivatives of the output of neurons of a DNN with respect to the input are well known in deep learning literature. They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62]. We provide a brief definition here for the sake of completeness and refer interested readers to [62] for more details.\nLet \u03b8 and x represent the parameters and the test input of a DNN respectively. The parametric function performed by a neuron can be represented as y = f(\u03b8,x) where f is a function takes \u03b8 and x as input and output y. Note that y can be the output of any neuron defined in the DNN (e.g., neuron from output layer or intermediate layers). The gradient of f(\u03b8,x) with respect to input x can be defined as:\nG = \u2207xf(\u03b8,x) = \u2202y/\u2202x (1)\nThe computation inside f is essentially a sequence of stacked functions that computes the input from previous layer and output to the next layer. Thus, G can be calculated by utilizing the chain rule in calculus, i.e., by computing the layer-wise derivatives starting from the layer of the neuron that outputs y until reaching the input layer that takes x as input. Note that the dimension of the gradientG is identical to that of the input x."}, {"heading": "5.2 DeepXplore algorithm", "text": "The main advantage of the test input generation process for a DNN over traditional software is that the test generation process, once defined as an optimization problem, can be solved efficiently using gradient ascent. In this section, we describe the details of the formulation and solution of the\noptimization problem. Note that efficient solution of the optimization problem is possible for DNNs as the gradients of the objective functions of DNNs, unlike traditional software, can be efficiently computed.\nAs discussed earlier in Section 4, the objective of the test generation process is to maximize the number of observed differential behaviors as well as neuron coverage while preserving domain-specific constraints provided by the users. Algorithm 1 shows the algorithm for generating test inputs by solving this joint optimization problem. Below, we define the objectives of our joint optimization problem formally and explain the details of the algorithm for solving it. Maximizing differential behaviors. The first objective of the optimization problem is to generate test inputs that can induce different behaviors in the tested DNNs. Different behaviors in DNN classifiers mean that different DNNs will classify the same input into two different classes. Suppose we have n DNNs Fk\u22081..n : x\u2192 y, where Fk is the function modeled by the k-th neural network. x represents the input and y represents the output class probability vector. Given an arbitrary x as seed that gets classified to the same class by all DNNs, our goal is to perturb x such that the perturbed input x\u2032 will be classified differently by at least one of the n DNNs.\nLet Fk(x)[c] be the class probability that Fk predicts x to be c. We randomly select one neural network Fj (Algorithm 1 line 6) and maximize the following objective function:\nobj1(x) = \u03a3i6=jFi(x)[c]\u2212 \u03bb1 \u00b7 Fj(x)[c] (2)\nwhere \u03bb2 is a parameter to balance the objective terms between the DNNs Fk 6=j that maintain the same class outputs as before and the DNN Fj that produce different class outputs. As Fk\u22081..n are all differentiable, Equation 2 can be easily maximized using gradient ascent by perturbing x based on iteratively computed gradient: \u2202obj1(x)\u2202x (Algorithm 1 line 8-14 and procedure COMPUTE_OBJ1). Maximizing neuron coverage. The second objective is to generate inputs that maximize neuron coverage. We achieve this by iteratively picking inactivated neurons and perturbing the input such that output of that neuron goes above the neuron coverage threshold. Let us assume that we want to maximize the output of a neuron n, i.e., we want to maximize obj2(x) = fn(x) such that fn(x) > t, where t is the neuron coverage threshold, and we write fn(x) as the function performed by neuron n that takes x (the input to DNN) as input and produce the output of neuron n (as defined in Equation 1). We can again leverage the gradient ascent mechanism as fn(x) is a differentiable function whose gradient is \u2202fn(x)\u2202x .\nNote that we can also potentially jointly maximize multiple neurons like Equation 2. However, we choose to optimize one neuron at a time in this algorithm for ease of understanding (Algorithm 1 line 8-14 and procedure COMPUTE_OBJ2).\nAlgorithm 1 Test input generation via joint optimization Input: seed_set\u2190 unlabeled inputs as the seeds\ndnns\u2190 multiple DNNs under test \u03bb1 \u2190 parameter to balance output differences of DNNs (Equation 2) \u03bb2\u2190 parameter to balance coverage and differential behavior s\u2190 step size in gradient ascent t\u2190 threshold for determining if a neuron is activated p\u2190 desired neuron coverage cov_tracker\u2190 tracks which neurons have been activated\n1: /* main procedure */ 2: gen_test := empty set 3: for cycle(x \u2208 seed_set) do // infinitely cycling through seed_set 4: /* all dnns should classify the seed input to the same class */ 5: c = dnns[0].predict(x) 6: d = randomly select one dnn from dnns 7: while True do 8: obj1 = COMPUTE_OBJ1(x, d, c, dnns, \u03bb1) 9: obj2 = COMPUTE_OBJ2(x, dnns, cov_tracker)\n10: obj = obj1 + \u03bb2 \u00b7 obj2 11: grad = \u2202obj / \u2202x 12: /*apply domain-specific constraints to gradient*/ 13: grad = DOMAIN_CONSTRNTS(grad) 14: x = x + s \u00b7 grad //gradient ascent 15: if d.predict(x) 6= (dnns-d).predict(x) then 16: /* dnns predict x differently */ 17: gen_test.add(x) 18: Update cov_tracker 19: break 20: if DESIRED_COVERAGE_ACHVD(cov_tracker) then 21: return gen_test 22: /* utility functions for computing obj1 and obj2 */ 23: procedure COMPUTE_OBJ1(x, d, c, dnns, \u03bb1) 24: rest = dnns - d 25: loss1 := 0 26: for dnn in rest do 27: loss1 += dnnc(x) //confidence score of x being in class c 28: loss2 := dc(x) //d\u2019s output confidence score of x being in class c 29: return (loss1 - \u03bb1\u00b7loss2) 30: procedure COMPUTE_OBJ2(x, dnns, cov_tracker) 31: loss := 0 32: for dnn \u2208 dnns do 33: select a neuron n inactivated so far using cov_tracker 34: loss += n(x) //the neuron n\u2019s output when x is the dnn\u2019s input 35: return loss\nJoint optimization. We jointly maximize obj1 and fn described above and maximize the following function:\nobjjoint = (\u03a3i 6=jFi(x)[c]\u2212 \u03bb1Fj(x)[c]) + \u03bb2 \u00b7 fn(x) (3)\nwhere \u03bb2 is a parameter for balancing between the two objectives of the joint optimization process and n is the inactivated neuron that we randomly pick at each iteration (Algorithm 1 line 3). As all terms of objjoint is differentiable, we jointly maximize them using gradient ascent by perturbing x (Algorithm 1 line 14). Domain-specific constraints. One important aspect of the optimization process is that the generated test inputs must satisfy several domain-specific constraints to be considered physically realistic inputs [42]. In particular, we want to ensure that the perturbations applied to xi during the i-th iteration of gradient ascent process are physically realizable. For example, for a generated test image x the pixel values must be within a certain range (e.g., 0 to 255).\nWhile some such constraints may be efficiently embedded into the joint optimization process using the Lagrange Multipliers such as those leveraged in support vector machine [53], we found that the majority of them cannot be easily handled by the optimization algorithm. Therefore, we designed a simple rule-based method to ensure that generated tests\nsatisfy the custom domain-specific constraint. As the seed input xseed = x0 always satisfy the constraints by definition, our technique must ensure that after i-th (i > 0) iteration of gradient ascent, xi still satisfies the constraints. Our algorithm ensures this property by modifying the gradient grad such that xi+1 = xi + s \u00b7 grad (line 13 in Algorithm 1) still satisfies the constraints (s is the step size in the gradient ascent).\nFor example, in the case of discrete features, we round the gradient to an integer. For DNNs handling visual input (e.g., images), we add different spatial restrictions such that only part of the input images will be perturbed. A detailed description of the domain-specific constraints that we implemented can be found in Section 7.2."}, {"heading": "6. Implementation", "text": "We implement DeepXplore using Tensorflow 1.0.1 [1] and Keras 2.0.3 [10] DL frameworks. Our implementation consists of around 7, 086 lines of code. We leverage Tensorflow\u2019s efficient implementation of gradient computations in our joint optimization process. Tensorflow also supports creating subDNNs by marking any arbitrary neuron\u2019s output as the subDNN\u2019s output while keeping the input same as the original DNN\u2019s input. We use this feature to intercept and record the output of neurons in the intermediate layers of a DNN and compute the corresponding gradients with respect to the DNN\u2019s input. All our experiments were run on a Linux laptop running Ubuntu 16.04 (one Intel i7-6700HQ 2.60GHz processor with 4 cores, 16GB of memory, and a NVIDIA GTX 1070 GPU)."}, {"heading": "7. Experimental Setup", "text": "In this section, we first describe the datasets and DNNs that we used to evaluateDeepXplore and then present an overview of DeepXplore\u2019s performance and different types of erroneous behaviors it found in the tested DNNs."}, {"heading": "7.1 Test datasets and DNNs", "text": "For our evaluation, we adopt four popular public datasets with different types of data\u2014MNIST, ImageNet, Driving, Contagio, and Drebin. We evaluate DeepXplore on 3 DNNs For each dataset (i.e., a total of 15 DNNs). All the DNNs that we used are popular real-world architectures with results comparable to the current state-of-the-art for each dataset. All tested DNNs for each dataset have slightly different architectures, i.e., the types or number of neurons and layers. We used pre-trained weights for all three of the DNNs for the ImageNet dataset as published by the respective authors. For all other DNNs, we train them on the corresponding datasets by ourselves and achieve an accuracy comparable to the ones reported by the original designers. We provide the details about the datasets and the corresponding DNNs in Table 1. MNIST [27] is a large handwritten digit dataset containing images with class labels from 0 to 9 where each image has a size of 28x28 pixels. The dataset includes 60,000 training\nsamples and 10,000 testing samples. We construct three different neural networks based on the LeNet family [26], namely the LeNet-1, LeNet-4, and LeNet-5. The detailed architecture description of these DNNs can be found in [26]. ImageNet is a large image dataset with over 10, 000, 000 hand-annotated images that were crowdsourced and labeled manually [13]. We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22]. All of the three DNNs demonstrated competitive performance in the ILSVRC [41] competition. Driving is the Udacity self-driving car challenge dataset [52] that contains the front scenes of a driving car captured by a camera mounted behind the windshield and the simultaneous steering wheel angle applied by the human driver at each front scene. The dataset has 45,567 samples \u2013 80% randomly picked ones are used as training set and the rest as test set. We used three DNNs [4, 11, 55] based on the DAVE-2 selfdriving car architecture from Nvidia [5] with slightly different configurations, which are called DAVE-orig, DAVE-norminit, and DAVE-dropout respectively. Specifically, DAVE-orig [4] fully replicates the original architecture from the Nvidia\u2019s paper [5]. DAVE-norminit [55] removes the first batch normalization layer [23] and normalizes the randomly initialized network weights. DAVE-dropout [11] simplifies DAVE-orig with less number of convolutional layers and fully connected layers and adds two dropout layer [49] between the final 3 fully-connected layers. We trained all three implementations with the Udacity dataset mentioned above. Contagio is a dataset containing different benign and malicious PDF documents [12]. PDFrate [35, 47], an online service for PDF malware detection, defines 135 static features to transform a PDF document into a feature vector. Since, to the best of our knowledge, there is no DNN-based, publicly available PDF malware detection system, we defined and trained three different DNNs and ensured that they achieve similar performance as reported by [56] using SVMs. We use 5, 000 benign and 12, 205 malicious PDF documents from Contagio database, which are preprocessed into 135 feature vectors as described in [56]. Each neural network, besides one input layer and one softmax output layer, has N fully-connected hidden layers and each layer has 200 neurons. We choose N to be 2, 3, and 4 for the three tested DNNs respectively. We evaluated the trained DNNs\u2019 performance using 5, 000 malicious PDFs collected by VirusTotal [54] and 5, 000 benign PDFs collected from a Google crawl. Drebin is a dataset [3, 48] that includes 129, 013 Android applications among which 123, 453 are benign and 5, 560 are malicious. There is total of 545, 333 binary features categorized into 8 sets including the features captured from manifest files (e.g., requested permissions and intents) and disassembled code (e.g., restricted API calls and network addresses). Grosse et al. [21] evaluated 36 DNNs with different numbers of layers and number of neurons per layer. We pick 3 DNNs from those for our tests. As the weights of these DNNs are\nnot public, we trained the DNNs on our own. We use the same training/testing split (66%/33%) as reported in [3]."}, {"heading": "7.2 Domain-specific constraints", "text": "In order for the generated test inputs to be meaningful in practice, we must ensure that the generated tests will be valid and physical realizable. For example, the generate test image inputs should be physically possible to be produced by a camera; similarly, a PDF file needs to follow the PDF specification and a PDF viewer should be able to open it without any errors. To ensure such properties, we encode different domain-specific constraints into the process of test input generation. We describe the different types of domain-specific constraints that we used in our evaluation of DeepXplore for two major categories of input n our dataset: image data and other file data. Note that although we only describe a few domain-specific constraints here, DeepXplore allows users to add a wide variety of constraints.\nImage constraints (MNIST, ImageNet, and Driving). For the image datasets, we used three different types of constraints for simulating different environment conditions: (1) lighting effects for simulating different intensities of lighting, (2) occlusion by a single small rectangle for simulating an attacker potentially blocking some parts of an image, and (3) occlusion by multiple tiny black rectangles for simulating effects of dust on the lens of the camera.\nFor the first constraint, we restrict the applied perturbations to only make the image darker or brighter without changing its content. We define a custom perturbation function which increases or decreases all pixel values by the same amount (e.g., 1 \u2217 stepsize in Algorithm 1 line 14) depending on whether mean(G) > 0 or not where G denotes the gradients calculated at each iteration of gradient ascent. The first and second rows of Figure 7 show examples of the difference-\ninducing inputs generated by these constraints for different test DNNs.\nThe second constraint simulates the effect of the camera lens may be accidentally/deliberately occluded by a single small rectangle R (m\u00d7n pixels). Specifically, we apply only Gi:i+m,j:j+n to the original image (I) where Ii:i+m,j:j+n is the location of R. The third and fourth rows of Figure 7 show an example of occlusion rectangle for three datasets respectively.\nFor the third constraint, during each iteration of the gradient ascent, we randomly select a tiny m\u00d7 n size patch from G with upper-left corner at (i, j), Gi:i+m,j:j+m such that if the averaged value mean(Gi:i+m,j:j+m) of this patch is greater than 0, we set Gi:i+m,j:j+m = 0, i.e., we only allow the pixel values to be decreased (see the fifth and sixth rows of Figure 7).\nOther constraints (Drebin and Contagio). For Drebin dataset, we only allow the modifications of the features related to the android manifest file to ensure that the application code is not affected. Moreover, we only allow adding features (turning 0 to 1) but do not allow deleting features (changing 1 to 0) in the manifest files to ensure that no application functionality gets blocked due to insufficient permissions. Thus, after computing the gradient, we only perturb the manifest features whose corresponding gradients are greater than 0.\nFor Contagio dataset, we follow the feature definitions described in [56], which clearly defines the restrictions on each feature, i.e., whether they can be incrementable and decrementable, only incrementable, discrete/continuous, or unmodifiable."}, {"heading": "8. Results", "text": "DeepXplore was able to find thousands of erroneous behaviors in all tested DNNs. Due to space limitations, we only present some representative inputs generated by DeepXplore\nthat resulted in incorrect behaviors by the DNNs. Figure 7 shows some of the difference-inducing inputs generated by DeepXplore (with different domain-specific constraints) for MNIST, ImageNet and Driving dataset along with the erroneous behaviors of the corresponding DNNs. Table 2 (Contagio) and Table 3 (Drebin) show two sample inputs generated by DeepXplore that caused erroneous behaviors in at least one of the tested DNNs. We highlight the differences between the seed input features and the features perturbed by DeepXplore. We only list the top three perturbed features for Contagio due to space limitations."}, {"heading": "8.1 Effectiveness of neuron coverage as a testing metric", "text": "It has recently been shown that each neuron in a DNN tends to be responsible for extracting a specific feature of the input instead of multiple neurons collaborating to extract a feature [38, 62]. Essentially, each neuron tends to be learning a different set of rules than others. This finding intuitively explains why neuron coverage is a good metric for DNN testing comprehensiveness.\nIn order to empirically demonstrate that neuron coverage is a good metric for measuring the comprehensiveness of DNN testing, we perform two tests. First, we show that neuron coverage is a significantly better metric than code coverage for measuring comprehensiveness of the DNN test inputs. More specifically, we show that a small number of test inputs can achieve 100% code coverage for all DNNs where neuron coverage is actually less than 34%. Next, we show that for inputs from different classes tend to activate different neurons than inputs from the same class. Both of these findings justify the choice of neuron coverage as a good estimate of the numbers and types of DNN rules exercised by an input.\nNeuron coverage vs. code coverage. We compare the code coverage and neuron coverage achieved by the same number of inputs by feeding 10 randomly picked original testing set samples as described in Section 7.1 into the corresponding DNNs. Table 4 shows the comparison of code coverage and neuron coverage. C1, C2 and C3 denotes the three classifiers (cf. Section 7.1) used for each dataset. We measure code coverage in terms of line coverage. For the neuron coverage measurements, the threshold is set to t = 0.75, i.e., a neuron is considered covered only if its output is greater than 0.75 for at least one of the inputs.\nThe results in Table 4 clearly demonstrate that code coverage is not a good metric for estimating testing comprehensiveness of a DNN as even 10 inputs result in 100% code coverage for all DNNs while the neuron coverage never goes above 34% in any of the DNNs. Moreover, neuron coverage tends to vary widely across different DNNs. We found that, in several cases, even using a complete test set, e.g., MNIST dataset (10,000 testing items), the neuron coverage only reached to 57.7%, 76.4%, and 83.6% for C1, C2, and C3 respectively, which still left many neurons uncovered.\nActivation of neurons for different classes of inputs. Given that neurons have been shown to detect independent features, one should expect that two inputs of the same class will activate similar neurons (i.e., there will be more overlap among the corresponding activated neurons) while inputs of different classes will activate different neurons. In order to confirm this hypothesis, we measure the number of common active neurons when feeding a pair of MNIST samples of same and different classes into LeNet-5. In particular, we randomly select 200 input pairs where 100 pairs have the same label (i.e., belong to the same class) and 100 pairs have differ-\nent labels. We measure the number of common (overlapped) active neurons for these input pairs. The numbers are shown in Table 5. The results confirm our hypothesis that inputs coming from same class share more activated neurons than inputs coming from different classes. This also demonstrates that neuron coverage can effectively estimate the numbers of different rules that get activated when DNNs process inputs."}, {"heading": "8.2 DeepXplore performance", "text": "We evaluate DeepXplore\u2019s performance using two different metrics: neuron coverage achieved and time taken for generating difference-inducing test cases with high neuron coverage.\nNeuron coverage achieved by DeepXplore. In order to compare the neuron coverage achieved by the test cases generated by DeepXplore, we compare the neuron coverage for the same number of tests (1% of the original testing set) (1) generated by DeepXplore, (2) generated by adversarial testing [18], and (3) randomly selected from the original test set. The results are shown in Table 4. We found that the neuron coverage threshold t (defined in Section 5), which decides when a neuron has been activated, greatly affects the value of the achieved coverage. Therefore, Figure 8 compares the changes in average neuron coverage achieved by these three methods as the threshold t changes from 0 to 0.75. For the cases where the activation functions of intermediate DNN layers produce values in [0,+\u221e] and those in the final layers produce a value in [0, 1], we use a strategy similar to the batch normalization [23] for making the neuron outputs across all layers comparable.\nTable 4 demonstrates that DeepXplore, on average, covers 34.4% and 33.2% more neurons than random testing and adversarial testing. The results also show that as the threshold t increases, all three approaches cover fewer neurons. This is intuitive as it becomes increasingly harder to make neurons output higher values.\nExecution time of DeepXplore. Table 6 shows the time taken by DeepXplore to achieve 100% neuron coverage for all tested DNNs. Our experimental results show that DeepXplore is very efficient in terms of increasing test coverage as well as finding difference-inducing inputs.\nEffect of different parameter choices. DeepXplore has three main parameters: step size for gradient ascent (s), a knob for controlling the balance between maximizing and minimizing different DNNs\u2019 outputs (\u03bb1), and another knob for controlling the priority between higher coverage and more differential behaviors while test input generation (\u03bb2).\nTable 7, 8, and 9 show the variation in DeepXplore runtime with changes in these parameters."}, {"heading": "8.3 Other usages of inputs generated by DeepXplore", "text": "Besides highlighting incorrect behaviors in DNNs, the test inputs generated by DeepXplore can also be used for other purposes like augmenting training data to improve a DNN\u2019s accuracy and detecting training data pollution attacks. We summarize our experimental results for such usage of DeepXploregenerated inputs below. Augmenting training data to improve DNNs. Test inputs generated by DeepXplore can be added back to the training set and used to improve the accuracy of DNNs. Such strategy has also been adopted by the adversarial testing works [18] although, unlike us, they require manual labeling of test inputs. By contrast, we adopt majority voting [16] to automatically generate labels for generated test inputs.\nWe compare the improvement in a DNN\u2019s accuracy by augmenting the training set with same numbers of inputs generated using DeepXplore, adversarial learning, and randomly drawn from a set of test inputs. We found that the DNN consistently achieved higher accuracy when trained on inputs generated by DeepXplore compared to augmentations by both adversarial and randomly-drawn inputs. For this experiment, we train LeNet-1, LeNet-4 and LeNet-5 as shown in Table 1 with 60, 000 samples. We augment the training data with 100 new samples as one epoch each time. These epochs are generated by three approaches: random selection (called \u201crandom\u201d), adversarial testing (called \u201cadversarial\u201d) and DeepXplore. The resulting accuracy improvements are shown in Figure 9. DeepXplore achieved 1-3% more average accuracy improvement over adversarial and random augmentation. Detecting training data pollution attack. We demonstrate DeepXplore\u2019s ability to detect training data pollution attacks with an experiment with two LeNet-5 DNNs: one trained\non 60, 000 hand-written digits from MINST dataset and the other trained on an artificially polluted version of the same training data where 30% of the images originally labeled 9 are mislabeled as 1. We used DeepXplore to generate test inputs which the unpolluted LeNet-5 DNN classifies as 9 but the polluted DNN classifies as 1. We then searched for samples in the training set that are closest to the test inputs generated by DeepXplore in terms of structural similarity [57] and identified them as polluted data. We were able to correctly identify 95.6% of the polluted samples using this method."}, {"heading": "9. Discussion", "text": "DeepXplore adopts the technique of differential testing from software analysis, and therefore DeepXplore has the limitations of differential testing. We summarize two main limitations below.\nFirst, differential testing requires at least two different DNNs with the same functionality for identifying erroneous corner cases without manual effort. If multiple similar DNNs are not available, differential-testing-based tools like DeepXplore cannot identify the erroneous behavior of the DNNs. Further, if two DNNs only differ slightly (i.e., by one or two neurons), DeepXplore will take longer to find differential behavior than if the DNNs were significantly different from each other. However, our evaluation shows that in most cases it is fairly easy to find multiple different DNNs for a given problem. This is understandable because developers prefer to define and train their own DNNs for improved flexibility. Moreover, differential testing on the variants of a DNN configured with different parameters even by the same developer can help the developer identify the which variant is best suited for their needs.\nSecond, differential testing can only detect an erroneous behavior if at least one DNN produces different results than other DNNs. If all the tested DNNs make the same mistake, DeepXplore cannot generate the corresponding test case. However, we found this to be not a significant issue in practice as most DNNs are independently constructed and trained, the odds of all of them making the same mistake is low."}, {"heading": "10. Related Work", "text": "Adversarial deep learning. Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities. Many of these works have demonstrated that a DNN can be fooled by applying minute perturbations to an input image, which was originally classified correctly by the DNN, even though the modified image looks visibly indistinguishable from the original image to the human eye. The location and amount of these perturbations are usually guided by the gradients of a DNN with respect to a seed input.\nAdversarial images demonstrate a particular type of erroneous behaviors of DNNs. However, they suffer from two major limitations: (1) they have low neuron coverage (similar\nto the randomly selected test inputs as shown in Figure 8) and therefore, unlike DeepXplore, can not expose different types of erroneous behaviors; and (2) the adversarial image generation process is inherently limited by the tiny, undetectable perturbations as any visible change will require manual inspection. DeepXplore bypasses this issue by using differential testing and therefore can perturb inputs to create many realistic visible differences (e.g., different lighting, occlusion, mobile app permissions, etc.) and automatically detect erroneous behaviors of DNNs under these circumstances. Other applications of DNN gradients. There are a number of prior works that use gradient ascent/descent for understanding the inner working of DNNs. For example, Simonyan et al. [45] and Mahendran et al. [30] used gradients to visualize activation of different intermediate layers of a convolutional DNN for tasks like object segmentation. Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39]. By contrast, in this paper, we apply gradient ascent for solving the joint optimization problem that maximizes both neuron coverage and the number of differential behaviors among tested DNNs. Differential testing of traditional software. Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].\nThe key advantage of applying differential testing to DNNs over traditional software is that the problem of triggering a large number of differential behaviors while simultaneously maximizing neuron coverage can be expressed as a well-defined joint optimization problem. Moreover, the gradient of a DNN with respect to the input can be utilized for efficiently solving the optimization problem using gradient ascent."}, {"heading": "11. Conclusion", "text": "We designed and implemented DeepXplore, a whitebox differential testing system that can systematically test DL systems and automatically identify potentially erroneous behaviors. In this paper, we introduced the concept of neuron coverage as a way of estimating how much logic of a DNN gets exercised by a set of inputs. DeepXplore performs gradient ascent to solve a joint optimization problem that maximizes both neuron coverage and the number of potentially erroneous behaviors. We evaluated DeepXplore upon 15 state-of-the-art DNNs trained on 5 real-world datasets. DeepXplore found thousands of erroneous behaviors in all of the tested DNNs."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. ABADI", "P. BARHAM", "J. CHEN", "Z. CHEN", "A. DAVIS", "J. DEAN", "M. DEVIN", "S. GHEMAWAT", "G. IRVING", "M ISARD"], "venue": "In Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Sfadiff: Automated evasion attacks and fingerprinting using black-box differential automata learning", "author": ["G. ARGYROS", "I. STAIS", "S. JANA", "A.D. KEROMYTIS", "A. KIAYIAS"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (2016),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Effective and explainable detection of android malware in your pocket", "author": ["D. ARP", "M. SPREITZENBARTH", "M. HUBNER", "H. GASCON", "K. RIECK", "SIEMENS", "C. Drebin"], "venue": "In 21th Annual Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "End to end learning for self-driving cars", "author": ["M. BOJARSKI", "D. DEL TESTA", "D. DWORAKOWSKI", "B. FIRNER", "B. FLEPP", "P. GOYAL", "L.D. JACKEL", "M. MON- FORT", "U. MULLER", "J ZHANG"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Using frankencerts for automated adversarial testing of certificate validation in SSL/TLS implementations", "author": ["C. BRUBAKER", "S. JANA", "B. RAY", "S. KHURSHID", "V. SHMATIKOV"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy (S&P)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Towards making systems forget with machine unlearning", "author": ["Y. CAO", "J. YANG"], "venue": "In Proceedings of the 2015 IEEE Symposium on Security and Privacy (Washington, DC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Coveragedirected differential testing of JVM implementations", "author": ["CHEN Y", "SU T", "SUN C", "SU Z", "ZHAO J"], "venue": "In Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI) (2016),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Guided differential testing of certificate validation in ssl/tls implementations", "author": ["CHEN Y", "SU"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (2015),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. DENG", "W. DONG", "R. SOCHER", "LI", "LI L.-J", "L. FEI-FEI"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["M. FREDRIKSON", "S. JHA", "T. RISTENPART"], "venue": "In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Privacy in pharmacogenetics: An endto-end case study of personalized warfarin dosing", "author": ["M. FREDRIKSON", "E. LANTZ", "S. JHA", "S. LIN", "D. PAGE", "T. RISTENPART"], "venue": "In USENIX Security", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. FREUND", "R.E. SCHAPIRE"], "venue": "In European conference on computational learning theory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "A neural algorithm of artistic style", "author": ["L.A. GATYS", "A.S. ECKER", "M. BETHGE"], "venue": "arXiv preprint arXiv:1508.06576", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. GOODFELLOW", "J. SHLENS", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Randomized differential testing as a prelude to formal verification", "author": ["A. GROCE", "G. HOLZMANN", "R. JOSHI"], "venue": "In Proceedings of the 29th international conference on Software Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. GROSSE", "N. PAPERNOT", "P. MANOHARAN", "M. BACKES", "P. MCDANIEL"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. IOFFE", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Policy compression for aircraft collision avoidance systems", "author": ["K.D. JULIAN", "J. LOPEZ", "J.S. BRUSH", "M.P. OWEN", "M.J. KOCHENDERFER"], "venue": "In Digital Avionics Systems Conference (DASC),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Privacy oracle: a system for finding application leaks with black box differential testing", "author": ["J. JUNG", "A. SHETH", "B. GREENSTEIN", "D. WETHERALL", "G. MAGANIS", "T. KOHNO"], "venue": "In Proceedings of the 15th ACM conference on Computer and communications security (2008),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Mnist handwritten digit database", "author": ["Y. LECUN", "C. CORTES", "C.J. BURGES"], "venue": "AT&T Labs [Online]. Available: http://yann.lecun.com/exdb/mnist", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Combining markov random fields and convolutional neural networks for image synthesis", "author": ["C. LI", "M. WAND"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Understanding deep image representations by inverting them", "author": ["A. MAHENDRAN", "A. VEDALDI"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Differential testing for software", "author": ["W.M. MCKEEMAN"], "venue": "Digital Technical Journal 10,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. NAIR", "G.E. HINTON"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. NGUYEN", "J. YOSINSKI", "J. CLUNE"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "IEEE European Symposium on Security and Privacy (EuroS P) (March", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Misleading worm signature generators using deliberate noise injection", "author": ["R. PERDISCI", "D. DAGON", "W. LEE", "P. FOGLA", "M. SHARIF"], "venue": "IEEE Symposium on Security and Privacy (S", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "NEZHA: Efficient Domain-independent Differential Testing", "author": ["T. PETSIOS", "A. TANG", "S.J. STOLFO", "A.D. KEROMYTIS", "S. JANA"], "venue": "In Proceedings of the 38th IEEE Symposium on Security & Privacy (San Jose,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["A. RADFORD", "R. JOZEFOWICZ", "I. SUTSKEVER"], "venue": "arXiv preprint arXiv:1704.01444", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2017}, {"title": "Artistic style transfer for videos", "author": ["M. RUDER", "A. DOSOVITSKIY", "T. BROX"], "venue": "In German Conference on Pattern Recognition", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. RUMELHART", "G.E. HINTON", "R.J. WILLIAMS"], "venue": "Cognitive modeling 5,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1988}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. SHARIF", "S. BHAGAVATULA", "L. BAUER", "M.K. REITER"], "venue": "In Proceedings of the 23rd ACM SIGSAC Conference on Computer and Communications Security (Oct", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. SILVER", "A. HUANG", "C.J. MADDISON", "A. GUEZ", "L. SIFRE", "G. VAN DEN DRIESSCHE", "J. SCHRITTWIESER", "I. ANTONOGLOU", "V. PANNEERSHELVAM", "M. LANCTOT", "S. DIELEMAN", "D. GREWE", "J. NHAM", "N. KALCHBREN- NER", "I. SUTSKEVER", "T. LILLICRAP", "M. LEACH", "K. KAVUKCUOGLU", "T. GRAEPEL", "D. HASSABIS"], "venue": "Nature", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. SIMONYAN", "A. VEDALDI", "A. ZISSERMAN"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. SIMONYAN", "A. ZISSERMAN"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "HVLearn: Automated Black-box Analysis of Hostname Verification in SSL/TLS Implementations", "author": ["S. SIVAKORN", "G. ARGYROS", "K. PEI", "A.D. KEROMYTIS", "S. JANA"], "venue": "In Proceedings of the 38th IEEE Symposium on Security & Privacy (San Jose,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Malicious pdf detection using metadata and structural features", "author": ["C. SMUTZ", "A. STAVROU"], "venue": "In Proceedings of the 28th Annual Computer Security Applications Conference (2012),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Mobile-sandbox: having a deeper look into android applications", "author": ["M. SPREITZENBARTH", "F. FREILING", "F. ECHTLER", "T. SCHRECK", "J. HOFFMANN"], "venue": "In Proceedings of the 28th Annual ACM Symposium on Applied Computing (2013),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. SRIVASTAVA", "G.E. HINTON", "A. KRIZHEVSKY", "I. SUTSKEVER", "R. SALAKHUTDINOV"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. SZEGEDY", "W. ZAREMBA", "I. SUTSKEVER", "J. BRUNA", "D. ERHAN", "I. GOODFELLOW", "R. FERGUS"], "venue": "In arXiv preprint arXiv:1312.6199", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. TRAM\u00c8R", "F. ZHANG", "A. JUELS", "M.K. REITER", "T. RISTENPART"], "venue": "In 25th USENIX Security Symposium (USENIX Security 16) (Austin,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Practical evasion of a learningbased classifier: a case study", "author": ["N. \u0160RNDIC", "P. LASKOV"], "venue": "In Proceedings of the 2014 IEEE 14  Symposium on Security and Privacy (Washington, DC,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. WANG", "A.C. BOVIK", "H.R. SHEIKH", "E.P. SIMON- CELLI"], "venue": "IEEE transactions on image processing 13,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2004}, {"title": "A methodology for formalizing model-inversion attacks", "author": ["X. WU", "M. FREDRIKSON", "S. JHA", "J.F. NAUGHTON"], "venue": "In Computer Security Foundations Symposium (CSF),", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. XIONG", "J. DROPPO", "X. HUANG", "F. SEIDE", "M. SELTZER", "A. STOLCKE", "D. YU", "G. ZWEIG"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Automatically evading classifiers", "author": ["XU W", "QI Y", "EVANS D"], "venue": "In Proceedings of the 2016 Network and Distributed Systems Symposium", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2016}, {"title": "Finding and understanding bugs in c compilers", "author": ["X. YANG", "Y. CHEN", "E. EIDE", "J. REGEHR"], "venue": "In ACM SIGPLAN Notices", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2011}, {"title": "Understanding neural networks through deep visualization", "author": ["J. YOSINSKI", "J. CLUNE", "T. FUCHS", "H. LIPSON"], "venue": "ICML Workshop on Deep Learning", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Droid-sec: deep learning in android malware detection", "author": ["YUAN Z", "LU Y", "WANG Z", "XUE"], "venue": "In ACM SIG- COMM Computer Communication Review (2014),", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 181, "endOffset": 189}, {"referenceID": 36, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 181, "endOffset": 189}, {"referenceID": 46, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 210, "endOffset": 214}, {"referenceID": 34, "context": "Over the past few years, deep learning (DL) has made tremendous progress and have achieved/surpassed humanlevel performance on a diverse set of tasks including image classification [22, 45], speech recognition [59], and playing games like Go [43].", "startOffset": 242, "endOffset": 246}, {"referenceID": 3, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 126, "endOffset": 129}, {"referenceID": 50, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "This has also led to widespread adoption and deployment of DL in security- and safety-critical systems like self-driving cars [5], malware detection [63], and aircraft collision avoidance systems [24].", "startOffset": 196, "endOffset": 200}, {"referenceID": 13, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 26, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 41, "context": "Recent work on adversarial deep learning [18, 33, 50] have demonstrated how to create synthetic images that can fool state-of-the-art DL systems.", "startOffset": 41, "endOffset": 53}, {"referenceID": 1, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 4, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 7, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 24, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 48, "context": "Such differential testing techniques have been applied successfully in the past for detecting logic bugs without manual specifications in a wide variety of traditional software [2, 6, 8, 9, 31, 61].", "startOffset": 177, "endOffset": 197}, {"referenceID": 25, "context": "The common activation functions include sigmoid, hyperbolic tangent, or ReLU (Rectified Linear Unit) [32].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "DNNs can be trained using different training algorithms, but gradient descent using backpropagation is by far the most popular training algorithm for DNNs [40].", "startOffset": 155, "endOffset": 159}, {"referenceID": 49, "context": "Next, the first few hidden layers transform the raw pixel values into lowlevel texture features like edges or colors and feed them to the deeper layers [62].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 15, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 27, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 33, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 43, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 47, "context": "Recent works on adversarial evasion attacks against DNNs have demonstrated the existence of some corner-cases where a DNN-based image classifier with state-of-the-art performance on the testing sets still incorrectly classify an image specifically crafted by adding slight humanly imperceptible perturbations to a test image [18, 21, 34, 42, 56, 60].", "startOffset": 325, "endOffset": 349}, {"referenceID": 13, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 15, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 27, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 41, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 71, "endOffset": 87}, {"referenceID": 23, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 35, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 49, "context": "They have been extensively used both for crafting adversarial examples [18, 21, 34, 50] and visualizing/understanding DNNs [30, 44, 62].", "startOffset": 123, "endOffset": 135}, {"referenceID": 49, "context": "We provide a brief definition here for the sake of completeness and refer interested readers to [62] for more details.", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "One important aspect of the optimization process is that the generated test inputs must satisfy several domain-specific constraints to be considered physically realistic inputs [42].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "1 [1] and Keras 2.", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": "We construct three different neural networks based on the LeNet family [26], namely the LeNet-1, LeNet-4, and LeNet-5.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "The detailed architecture description of these DNNs can be found in [26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "ImageNet is a large image dataset with over 10, 000, 000 hand-annotated images that were crowdsourced and labeled manually [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "We test three well-known pre-trained DNNs: VGG-16 [45], VGG-19 [45], and ResNet50 [22].", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "We used three DNNs [4, 11, 55] based on the DAVE-2 selfdriving car architecture from Nvidia [5] with slightly different configurations, which are called DAVE-orig, DAVE-norminit, and DAVE-dropout respectively.", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "Specifically, DAVE-orig [4] fully replicates the original architecture from the Nvidia\u2019s paper [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 17, "context": "DAVE-norminit [55] removes the first batch normalization layer [23] and normalizes the randomly initialized network weights.", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "DAVE-dropout [11] simplifies DAVE-orig with less number of convolutional layers and fully connected layers and adds two dropout layer [49] between the final 3 fully-connected layers.", "startOffset": 134, "endOffset": 138}, {"referenceID": 38, "context": "PDFrate [35, 47], an online service for PDF malware detection, defines 135 static features to transform a PDF document into a feature vector.", "startOffset": 8, "endOffset": 16}, {"referenceID": 43, "context": "Since, to the best of our knowledge, there is no DNN-based, publicly available PDF malware detection system, we defined and trained three different DNNs and ensured that they achieve similar performance as reported by [56] using SVMs.", "startOffset": 218, "endOffset": 222}, {"referenceID": 43, "context": "We use 5, 000 benign and 12, 205 malicious PDF documents from Contagio database, which are preprocessed into 135 feature vectors as described in [56].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "Drebin is a dataset [3, 48] that includes 129, 013 Android applications among which 123, 453 are benign and 5, 560 are malicious.", "startOffset": 20, "endOffset": 27}, {"referenceID": 39, "context": "Drebin is a dataset [3, 48] that includes 129, 013 Android applications among which 123, 453 are benign and 5, 560 are malicious.", "startOffset": 20, "endOffset": 27}, {"referenceID": 15, "context": "[21] evaluated 36 DNNs with different numbers of layers and number of neurons per layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 98.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[26, 28] 99.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[26, 28] 99.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[45] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[45] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[22] 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] N/A 99.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[21] 98.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[21] 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[21] 92.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[56]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "We use the same training/testing split (66%/33%) as reported in [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 43, "context": "For Contagio dataset, we follow the feature definitions described in [56], which clearly defines the restrictions on each feature, i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "It has recently been shown that each neuron in a DNN tends to be responsible for extracting a specific feature of the input instead of multiple neurons collaborating to extract a feature [38, 62].", "startOffset": 187, "endOffset": 195}, {"referenceID": 49, "context": "It has recently been shown that each neuron in a DNN tends to be responsible for extracting a specific feature of the input instead of multiple neurons collaborating to extract a feature [38, 62].", "startOffset": 187, "endOffset": 195}, {"referenceID": 13, "context": "In order to compare the neuron coverage achieved by the test cases generated by DeepXplore, we compare the neuron coverage for the same number of tests (1% of the original testing set) (1) generated by DeepXplore, (2) generated by adversarial testing [18], and (3) randomly selected from the original test set.", "startOffset": 251, "endOffset": 255}, {"referenceID": 0, "context": "For the cases where the activation functions of intermediate DNN layers produce values in [0,+\u221e] and those in the final layers produce a value in [0, 1], we use a strategy similar to the batch normalization [23] for making the neuron outputs across all layers comparable.", "startOffset": 146, "endOffset": 152}, {"referenceID": 17, "context": "For the cases where the activation functions of intermediate DNN layers produce values in [0,+\u221e] and those in the final layers produce a value in [0, 1], we use a strategy similar to the batch normalization [23] for making the neuron outputs across all layers comparable.", "startOffset": 207, "endOffset": 211}, {"referenceID": 13, "context": "Figure 8: The neuron coverage achieved by the same number of inputs (1% of the original test set) produced by DeepXplore, adversarial testing [18], and random selection from the original test set.", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "generated by random selection (\u201crandom\u201d), adversarial testing (\u201cadversarial\u201d) [18], and DeepXplore.", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Such strategy has also been adopted by the adversarial testing works [18] although, unlike us, they require manual labeling of test inputs.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "By contrast, we adopt majority voting [16] to automatically generate labels for generated test inputs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 44, "context": "We then searched for samples in the training set that are closest to the test inputs generated by DeepXplore in terms of structural similarity [57] and identified them as polluted data.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 26, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 41, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 121, "endOffset": 133}, {"referenceID": 5, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 9, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 10, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 28, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 33, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 42, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 45, "context": "Recently, the security of machine learning has drawn significant attention from the researchers in both machine learning [18, 33, 50] and security [7, 14, 15, 36, 42, 51, 58] communities.", "startOffset": 147, "endOffset": 174}, {"referenceID": 36, "context": "[45] and Mahendran et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[30] used gradients to visualize activation of different intermediate layers of a convolutional DNN for tasks like object segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 22, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 31, "context": "Several prior works have used gradient ascent on a DNN\u2019s intermediate layers\u2019 outputs to maximize those layers\u2019 activation for transferring the artistic styles of one image to the other [17, 29, 39].", "startOffset": 186, "endOffset": 198}, {"referenceID": 6, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 24, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 148, "endOffset": 156}, {"referenceID": 48, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 148, "endOffset": 156}, {"referenceID": 4, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 7, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 29, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 37, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 197, "endOffset": 211}, {"referenceID": 29, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 225, "endOffset": 229}, {"referenceID": 14, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 279, "endOffset": 283}, {"referenceID": 1, "context": "Since then differential testing has been widely used for successfully testing various types of traditional software including JVMs [8], C compliers [31, 61], SSL/TLS certification validation logic [6, 9, 37, 46], PDF viewers [37], space flight software [20], mobile applications [25], and Web application firewalls [2].", "startOffset": 315, "endOffset": 318}], "year": 2017, "abstractText": "Deep learning (DL) systems are increasingly deployed in safetyand security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system\u2019s behavior for corner-case inputs are of great importance. However, systematic testing of large-scale DL systems with thousands of neurons and millions of parameters for all possible corner-cases is a hard problem. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose different erroneous behaviors for rare inputs. We design, implement, and evaluate DeepXplore, the first white-box framework for systematically testing real-world DL systems. We address two main problems: (1) generating inputs that trigger different parts of a DL system\u2019s logic and (2) identifying incorrect behaviors of DL systems without manual effort. First, we introduce neuron coverage for systematically estimating the parts of DL system exercised by a set of test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles and thus avoid manual checking for erroneous behaviors. We demonstrate how finding inputs triggering differential behaviors while achieving high neuron coverage for DL algorithms can be represented as a joint optimization problem and solved efficiently using gradient-based optimization techniques. DeepXplore efficiently finds thousands of unique incorrect corner-case behaviors (e.g., self-driving cars crashing into guard rails, malware masquerading as benign software) in state-of-the-art DL models trained on five popular datasets including driving data collected by Udacity from cars around Mountain View (CA) and ImageNet data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running on a commodity laptop. The inputs generated by DeepXplore achieved 33.2% higher neuron coverage on average than existing testing methods. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve classification accuracy or identify polluted training data.", "creator": "LaTeX with hyperref package"}}}