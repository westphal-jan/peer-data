{"id": "1705.06908", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Unbiased estimates for linear regression via volume sampling", "abstract": "Given a full rank matrix $X$ with more columns than rows consider the task of estimating the pseudo inverse $X^+$ based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$. When we evaluate the inverse of the sample $X$ with more columns than rows consider the inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$. When we evaluate the inverse of the sample $X$ with more columns than rows consider the inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor times $X^+X^{+\\top}$. When we evaluate the inverse of the sample $X$ with more columns than rows consider the inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). We", "histories": [["v1", "Fri, 19 May 2017 09:43:41 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v1", null], ["v2", "Wed, 7 Jun 2017 22:42:47 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v2", null], ["v3", "Tue, 13 Jun 2017 22:36:50 GMT  (23kb)", "http://arxiv.org/abs/1705.06908v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal derezinski", "manfred k warmuth"], "accepted": true, "id": "1705.06908"}, "pdf": {"name": "1705.06908.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mderezin@ucsc.edu", "manfred@ucsc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n06 90\n8v 1\n[ cs\n.L G\n] 1\n9 M\nPseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample fromX. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels.\nWe believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.\n1 Introduction\nLetX be a wide full rankmatrix with d rows and n columnswhere n \u2265 d. Our goal is to estimate the pseudo inverse X+ of X based on the pseudo inverse of a subset of columns. More precisely, we sample a subset S \u2286 {1..n} of s column indices (where s \u2265 d). We letXS be the sub-matrix of the s columns indexed by S (See Figure 1). Consider a version of X in which all but the columns of S are zero. This matrix equalsXIS where IS is an n-dimensional diagonal matrix with (IS)ii = 1 if i \u2208 S and 0 otherwise. We assume that the set of s column indices of X is selected proportional to the squared volume spanned by the rows of submatrix XS , i.e. proportional to det(XSX \u22a4 S ) and prove a number of new surprising expectation formulas for this type of volume sampling, such as\nE[(XIS) +] = X+ and E[ (XSX \u22a4 S ) \u22121\n\ufe38 \ufe37\ufe37 \ufe38 (XIS)+\u22a4(XIS)+\n] = n\u2212 d+ 1 s\u2212 d+ 1 X +\u22a4X+.\nNote that (XIS) + has the n \u00d7 d shape of X+ where the s rows indexed by S contain (XS) + and the remaining n \u2212 s rows are zero. The expectation\nof this matrix is X+ even though (XS) + is clearly not a sub-matrix of X+. In addition to the expectation formulas, our new techniques lead to an efficient volume sampling procedure which beats the state-of-the-art by a factor of n2 in time complexity.\nVolume sampling is useful in numerous applications, from clustering to matrix approximation, but we focus on the task of solving linear least squares problems: For an n\u2212dimensional label vector y, let w\u2217 = argmin\nw ||X\u22a4w \u2212 y||2 = X+y. Assume the entire design matrix X is known\nto the learner but labels are expensive and you want to observe as few of them as possible. Let w\u2217S = X + S yS be the solution to the sub-problem based on labels yS . What is the smallest number of labels s necessary, for which there is a sampling procedure on sets S of size s st the expected loss of w\u2217S is at most a constant factor larger than the loss ofw\n\u2217 that uses all n labels (where the constant is independent of n)? More precisely, using the short hand L(w) = ||X\u22a4w \u2212 y||2 for the loss on all n labels, what is the smallest size s such that E[L(w\u2217S)] \u2264 constL(w\u2217). This question is a version of the \u201cminimal coresets\u201d open problem posed in [2].\nThe size has to be at least d and one can show that randomization is necessary in that any deterministic algorithm for choosing a set of d columns can suffer loss larger by a factor of n. Also any iid sampling of S (such as the commonly used leverage scores [7]) requires at least \u2126(d log d) examples to achieve a finite factor. In this paper however we show that with a size d volume sample, E[L(w\u2217S)] = (d+ 1)L(w\n\u2217) ifX is in general position. Note again that we have an equality and not just an upper bound. Also we can show that the mulitplicative factor d + 1 is optimal. We further improve this factor to 1+ \u01eb through repeated volume sampling. Moreover, our expectation formulas imply that when S is a volume sampled set of size s \u2265 d, thenw\u2217S is an unbiased estimator forw\u2217, ie E[w\u2217S ] = w \u2217."}, {"heading": "2 Related Work", "text": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11]. Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in the design of randomized algorithms. Most of those methods focus on sampling s \u2264 d elements. In this paper, we study volume sampling sets of size s \u2265 d, which has been proposed in [1] and motivated with applications in graph theory, linear regression, matrix approximation and more. The only known polynomial time algorithm for size s > d volume sampling was recently proposed in [14] with time complexityO(n4s). We offer a new algorithm with runtimeO((n\u2212 s+ d)nd), which is faster by a factor of at least n2.\nThe problem of selecting a subset of input vectors for solving a linear regression task has been extensively studied in statistics literature under the terms optimal design [8] and pool-based active learning [16]. Various criteria for subset selection have been proposed, like A-optimality and Doptimality. For example, A-optimality seeks to minimize tr((XSX \u22a4 S )\n\u22121), which is combinatorially hard to optimize exactly. We show that for size s volume sampling (for s \u2265 d), E[(XSX\u22a4S )\u22121] = n\u2212d+1 s\u2212d+1 X +\u22a4X+ which provides an approximate randomized solution for this task.\nA related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2]. Here, subsampling appears as one of the key techniques for obtaining multiplicative bounds on the loss of the approximate solution. In this line of work, volume sampling size s \u2264 d has been used by [6, 10] for matrix approximation. Another common sampling technique is based on statistical leverage scores [7], which have been effectively used for the task of linear regression. However, this approach is based on iid sampling, and requires sampling at least \u2126(d log d) elements to achieve multiplicative loss bounds. On the other hand, the input vectors obtained from volume sampling are selected jointly, which makes the chosen subset more informative, and we show that just d volume sampled elements are sufficient to achieve a multiplicative bound."}, {"heading": "3 Unbiased estimators", "text": "Let n be an integer dimension. For each subset S \u2286 {1..n} of size s we are given a matrix formula FS . Our goal is to sample set S of size s using some sampling process and then develop concise expressions for ES:|S|=s[FS ]. Examples of formula classes FS will be given below.\nWe represent the sampling by a directed acyclic graph (dag), with a single root node corresponding to the full set {1..n}, Starting from the root, we proceed along the edges of the graph, iteratively removing elements from the set S. Concretely, consider a dag with levels s = n, n\u2212 1, ..., d. Level s contains ( n s ) nodes for sets S \u2286 {1..n} of size s. Every node S at level s > d has s directed edges to the nodes S \u2212 {i} at the next lower level. These edges are labeled with a conditional probability vector P (i|S). The probability of a (directed) path is the product of the probabilities along its edges. The outflow of probability from each node on all but the bottom level is 1. We let the probability P (S) of node S be the probability of all paths from the top node {1..n} to S and set the probability P ({1..n}) of the top node to 1. We associate a formula FS with each set node S in the dag. The following key equality lets us compute expectations, where we use S\u00b1i as S with element i added/subtracted.\nLemma 1 If for all S \u2286 {1..n} of size greater than d we have\nFS = \u2211\ni\u2208S\nP (i|S)FS\u2212i ,\nthen for any s \u2208 {d..n}: ES:|S|=s[FS ] = \u2211 S:|S|=s P (S)FS = F{1..n}.\nProof Suffices to show that expectations at successive layers are equal:\n\u2211\nS:|S|=s\nP (S)FS = \u2211\nS:|S|=s\n\u2211\ni\u2208S\nP (i|S)FS\u2212i = \u2211\nT :|T |=s\u22121\n\u2211\nj /\u2208T\nP (T+j)P (j|T+j)\n\ufe38 \ufe37\ufe37 \ufe38 P (T )\nFT ."}, {"heading": "3.1 Volume sampling", "text": "Given a wide full-rank matrix X \u2208 Rd\u00d7n and a sample size s \u2208 {d..n}, volume sampling chooses subset S \u2286 {1..n} of size s with probability proportional to volume spanned by the rows of submatrix XS , ie proportional to det(XSX \u22a4 S ). The following corollary uses the above dag setup to compute the normalization constant for this distribution. When s = d, the corollary provides a novel minimalist proof for the Cauchy-Binet formula: \u2211 S:|S|=s det(XSX \u22a4 S ) = det(XX \u22a4).\nCorollary 2 Let X \u2208 Rd\u00d7n and S \u2208 {1..n} of size n \u2265 s \u2265 d st det(XSX\u22a4S ) > 0. Then for any i \u2208 S, define\nP (i|S) := det(XS\u2212iX \u22a4 S\u2212i )\n(s\u2212 d) det(XSX\u22a4S ) =\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi s\u2212 d , (reverse iterative volume sampling)\nwhere xi is the ith column of X andXS is the sub matrix of columns indexed by S. Then P (i|S) is a proper probability distribution and thus \u2211 S:|S|=s P (S) = 1 for all s \u2208 {d..n}. Furthermore\nP (S) = det(XSX \u22a4 S )(\nn\u2212d s\u2212d\n) det(XX\u22a4) . (volume sampling)\nProof For any S, st det(XSX \u22a4 S ) > 0, it is easy to see that P (i|S) forms a probability vector:\n\u2211\ni\u2208S\nP (i|S) = \u2211\ni\u2208S\n1\u2212 tr((XSX\u22a4S )\u22121xix\u22a4i ) s\u2212 d = s\u2212 tr((XSX\u22a4S )\u22121XSX\u22a4S ) s\u2212 d = s\u2212 d s\u2212 d = 1.\nIt remains to show the formula for the probability P (S) of all paths ending at FS . We first consider the top node, ie {1..n}. In this case both the path definition and the formula are 1. For all but the\ntop node, the probability P (S) equals the total inflow of probability into that node from the previous level, ie\nP (S) = \u2211\ni/\u2208S\nP (i|S+i) P (S+i) = \u2211\ni/\u2208S\ndet(XSX \u22a4 S )\n(s+ 1\u2212 d)\u272d\u272d\u272d\u272d\u272d \u272d\u272ddet(XS+iX \u22a4 S+i\n) \u272d\u272d\u272d\n\u272d\u272d\u272d \u272d\ndet(XS+iX \u22a4 S+i ) (\nn\u2212d s+1\u2212d\n) det(XX\u22a4)\n= (n\u2212 s) det(XSX\u22a4S )\n(s+ 1\u2212 d) (\nn\u2212d s+1\u2212d\n) det(XX\u22a4)\n= det(XSX \u22a4 S )(\nn\u2212d s\u2212d\n) det(XX\u22a4) .\nNote that all paths from S to a subset T (of size \u2265 d) have the same probability because the ratios of determinants cancel along paths. Note that 00 ratios are avoided because paths with such ratios always lead to sets of probability 0."}, {"heading": "3.2 Expectation formulas for volume sampling", "text": "All expectations in the remainder of the paper are wrt volume sampling. We use the short hand E[FS ] for expectation with volume sampling where the size of the sampled set is fixed to s. The expectation formulas for two choices of FS are proven in the next two theorems. By Lemma 1 it suffices to show FS = \u2211 i\u2208S P (i|S)FS\u2212i for volume sampling.\nWe introduce a bit more notation first. Recall that XS is the sub matrix of columns indexed by S \u2286 {1..n} (See Figure 1). Consider a version of X in which all but the columns of S are zero. This matrix equalsXIS where IS is an n-dimensional diagonal matrix with (IS)ii = 1 if i \u2208 S and 0 otherwise.\nTheorem 3 Let X \u2208 Rd\u00d7n be a wide full rank matrix (ie n \u2265 d). For s \u2208 {d..n}, let S \u2286 1..n be a size s volume sampled set overX. Then\nE[(XIS) +] = X+.\nWe believe that this fundamental formula lies at the core of why volume sampling is important in many applications. In this work, we focus on its application to linear regression. However, [1] discuss many problems where controlling the pseudo-inverse of a submatrix is essential. For those applications, it is important to establish variance bounds for the estimator offered by Theorem 3. In this case, volume sampling once again offers very concrete guarantees. We obtain them by showing the following formula, which can be viewed as a second moment for this estimator.\nTheorem 4 Let X \u2208 Rd\u00d7n be a full-rank matrix and s \u2208 {d..n}. If size s volume sampling overX has full support, then\nE[ (XSX \u22a4 S ) \u22121\n\ufe38 \ufe37\ufe37 \ufe38 (XIS)+\u22a4(XIS)+\n] = n\u2212 d+ 1 s\u2212 d+ 1 (XX\n\u22a4)\u22121\ufe38 \ufe37\ufe37 \ufe38 X+\u22a4X+ ."}, {"heading": "If volume sampling does not have full support then the matrix equality \u201c=\u201d is replaced by the positive-definite inequality \u201c \u201d.", "text": "The condition that size s volume sampling overX has full support is equivalent to det(XSX \u22a4 S ) > 0 for all S \u2286 1..n of size s. Note that if size s volume sampling has full support, then size t > s also has full support. So full support for the smallest size d (often phrased as X being in general position) implies that volume sampling wrt any size s \u2265 d has full support. Surprisingly by combining theorems 3 and 4, we can obtain a \u201ccovariance type formula\u201d for the pseudo-inverse matrix estimator:\nE[((XIS) + \u2212 E[(XIS)+])\u22a4 ((XIS)+ \u2212 E[(XIS)+])]\n= E[(XIS) +\u22a4(XIS) +]\u2212 E[(XIS)+]\u22a4 E[(XIS)+] = n\u2212 d+ 1 s\u2212 d+ 1 X +\u22a4X+ \u2212X+\u22a4X+ = n\u2212 s s\u2212 d+ 1 X +\u22a4X+. (1)\nTheorem 4 can also be used to obtain an expectation formula for the Frobenius norm \u2016(XIS)+\u2016F of the estimator:\nE\u2016(XIS)+\u20162F = E[tr((XIS)+\u22a4(XIS)+)] = n\u2212 d+ 1 s\u2212 d+ 1 \u2016X +\u20162F . (2)\nThis norm formula has been shown in [1], with numerous applications. Theorem 4 can be viewed as a much stronger pre trace version of the norm formula. Also our proof techniques are quite different and much simpler. Note that if size s volume sampling for X does not have full support then (1) becomes a semi-definite inequality between matrices and (2) an inequality between numbers. Proof of Theorem 3 We apply Lemma 1 with FS = (XIS)\n+. It suffices to show FS =\u2211 i\u2208S P (i|S)FS\u2212i for P (i|S) := 1\u2212x\u22a4i (XSX \u22a4 S ) \u22121 xi s\u2212d , ie:\n(XIS) + =\n\u2211\ni\u2208S\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi s\u2212 d (XIS\u2212i) +\n\ufe38 \ufe37\ufe37 \ufe38 (XS\u2212iX \u22a4 S\u2212i )\u22121XIS\u2212i\n.\nProven by applying Sherman Morrison to (XS\u2212iX \u22a4 S\u2212i )\u22121 = (XSX \u22a4 S \u2212 xix\u22a4i )\u22121 on the rhs:\n\u2211\ni\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi n\u2212 d\n( (XSX \u22a4 S ) \u22121 + (XSX \u22a4 S ) \u22121xix \u22a4 i (XSX \u22a4 S ) \u22121\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi\n) (XIS \u2212 eix\u22a4i ).\nWe now expand the last two factors into 4 terms. The first becomes (XSX \u22a4 S ) \u22121XIS = (XIS) + (which is the lhs) and the remaining three terms times n\u2212 d sum to 0:\n\u2212 \u2211\ni\u2208S\n(1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi)(XSX\u22a4S )\u22121eix\u22a4i +\u2718\u2718\u2718\u2718 \u2718 (XSX \u22a4 S ) \u22121 \u271a \u271a \u271a \u271a\u2211 i\u2208S xix \u22a4 i (XSX \u22a4 S ) \u22121XIS\n\u2212 \u2211\ni\u2208S\nx\u22a4i (XSX \u22a4 S ) \u22121xi x \u22a4 i (XSX \u22a4 S ) \u22121eixi = 0.\nProof of Theorem 4 Choose FS = s\u2212d+1 n\u2212d+1 (XSX \u22a4 S ) \u22121. By Lemma 1 it suffices to show FS =\u2211 i\u2208S P (i|S)FS\u2212i for volume sampling:\ns\u2212 d+ 1 \u272d\u272d\u272d \u272dn\u2212 d+ 1(XSX \u22a4 S ) \u22121 = \u2211\ni\u2208S\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi \u2718\u2718\u2718s\u2212 d \u2718\u2718\u2718s\u2212 d \u272d\u272d\u272d \u272dn\u2212 d+ 1(XS\u2212iX \u22a4 S\u2212i) \u22121\nTo show this we apply Sherman Morrison to (XS\u2212iX \u22a4 S\u2212i )\u22121 on the rhs:\n\u2211\ni\u2208S\n(1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi) ( (XSX \u22a4 S ) \u22121 + (XSX \u22a4 S ) \u22121xix \u22a4 i (XSX \u22a4 S ) \u22121\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi\n)\n=(s\u2212 d)(XSX\u22a4S )\u22121 +\u2718\u2718\u2718\u2718 \u2718 (XSX \u22a4 S ) \u22121 \u271a \u271a \u271a \u271a\u2211 i\u2208S xix \u22a4 i (XSX \u22a4 S ) \u22121 = (s\u2212 d+ 1) (XSX\u22a4S )\u22121.\nIf some denominators 1 \u2212 x\u22a4i (XSX\u22a4S )\u22121xi are zero, then only sum over i for which the denominators are positive. In this case the above matrix equality becomes a positive-definite inequality .\n4 Linear regression with few labels\nOur main motivation for studying volume sampling came from asking the following simple question. Suppose we want to solve a d-dimensional linear regression problem with a matrix X \u2208 Rd\u00d7n of input column vectors and a label vector y \u2208 Rn, ie find w \u2208 Rd that minimizes the least squares loss L(w) = \u2016X\u22a4w \u2212 y\u20162:\nw\u2217 def = argmin w\u2208Rd L(w) = X+\u22a4y,\nbut the access to label vector y is restricted. We are allowed to pick a subset S \u2286 {1..n} for which the labels yi (where i \u2208 S) are revealed to us, and then solve the subproblem (XS ,yS), obtainingw \u2217\nS . What is the smallest number of labels such that for anyX, we\ncan findw\u2217S for which L(w \u2217 S) is only a multiplicative factor away from L(w \u2217) (independent of the number of input vectors n)? This question was posed as an open problem by [2]. It is easy to show that we need at least d labels (whenX is full-rank), so as to guarantee the uniqueness of solutionw\u2217S . We show that d labels are in fact sufficient, by using volume sampling to select the desired subset S (proof in Section 4.1).\nTheorem 5 If the input matrix X \u2208 Rd\u00d7n is in general position, then for any label vector y \u2208 Rn, the expected square loss (on all n labeled vectors) of the optimal solution w\u2217S for the subproblem (XS ,yS), with the d-element set S obtained from volume sampling, is given by\nE[L(w\u2217S)] = (d+ 1) L(w \u2217).\nIfX is not in general position, then the expected loss is upper-bounded by (d+ 1) L(w\u2217).\nThe factor d+ 1 cannot be improved when selecting only d labels (we omit the proof):\nProposition 6 For any d, there exists a least squares problem (X,y) with d+ 1 vectors in Rd such that for every d-element index set S \u2286 {1, ..., d+ 1}, we have\nL(w\u2217S) = (d+ 1) L(w \u2217).\nThe multiplicative factor in Theorem 5 does not depend on n. It is easy to see that this cannot be achieved by any deterministic algorithm (without the access to labels). Namely, suppose that d = 1 and X is a vector of all ones, whereas the label vector y is a vector of all ones except for a single zero. No matter which column index we choose deterministically, if that index corresponds to the label 0, the solution to the subproblem will incur loss L(w\u2217S) = nL(w\n\u2217). The fact that volume sampling is a joint distribution also plays an essential role in proving Theorem 5. Consider a matrix X with exactly d unique linearly independent columns (and an arbitrary number of duplicates). Any iid column sampling distribution (like for example leverage score sampling) will require \u2126(d log d) samples to retrieve all d unique columns (ie coupon collector problem), which is necessary to get any multiplicative loss bound.\nThe exact expectation formula for the least squares loss under volume sampling suggests a deep connection between linear regression and this distribution. We can use Theorem 3 to further strengthen that connection. Note, that the least squares estimator obtained through volume sampling can be written asw\u2217S = (XIS)\n+\u22a4y. Applying formula for the expectation of pseudo-inverse, we conclude thatw\u2217S is an unbiased estimator ofw \u2217.\nProposition 7 LetX \u2208 Rd\u00d7n be a full-rank matrix and n \u2265 s \u2265 d. Let S \u2286 1..n be a size s volume sampled set overX. Then, for arbitrary label vector y \u2208 Rn, we have\nE[w\u2217S ] = E[(XIS) +\u22a4y] = X+\u22a4y = w\u2217.\nNote, that the unbiasedness holds for volume sampling of any size, however the loss expectation formula is limited to size s = d. Bounding the loss expectation for s > d remains an open problem. However, we consider a different strategy for extending volume sampling in linear regression. Combining Proposition 7 with Theorem 5 we can compute the variance of predictions generated by volume sampling, and obtain tighter multiplicative loss bounds by sampling multiple d-element subsets S1, ..., St independently.\nTheorem 8 Let (X,y) be as in Theorem 5. For k independent size d volume samples S1, . . . , Sk, we have\nE  L  1 k k\u2211\nj=1\nw\u2217Sj\n    = ( 1 + d\nk\n) L(w\u2217).\nProof Denote y\u0302 def = X\u22a4w\u2217 and y\u0302S def = X\u22a4w\u2217S as the prediction vectors generated by w \u2217 and w\u2217S respectively. First, we perform bias-variance decomposition of the loss ofw\u2217S (set S is size d volume sampled):\nE[L(w\u2217S)] = E[\u2016y\u0302S \u2212 y\u20162] = E[\u2016y\u0302S \u2212 y\u0302 + y\u0302 \u2212 y\u20162] = E[\u2016y\u0302S \u2212 y\u0302\u20162] + E[2(y\u0302S \u2212 y\u0302)\u22a4(y\u0302 \u2212 y)] + \u2016y\u0302 \u2212 y\u20162\n(\u2217) =\nn\u2211\ni=1\nE[(y\u0302S,i \u2212 E[y\u0302S,i])2] + L(w\u2217) = n\u2211\ni=1\nVar[y\u0302S,i] + L(w \u2217),\nwhere (\u2217) follows from Theorem 3. Now, we use Theorem 5 to obtain the total variance of predictions:\nn\u2211\ni=1\nVar[y\u0302S,i] = E[L(w \u2217 S)]\u2212 L(w\u2217) = d L(w\u2217).\nFinally, we compute the expected loss of the average weight vector after sampling k sets S1, ..., Sk independently:\nE  L  1 k k\u2211\nj=1\nw\u2217Sj\n    = n\u2211\ni=1\nVar   1 k k\u2211\nj=1\ny\u0302Sj,i\n + L(w\u2217)\n= 1\nk2\n  k\u2211\nj=1\ndL(w\u2217)  + L(w\u2217) = ( 1 + d\nk\n) L(w\u2217).\nIt is worth noting that the average weight vector used in Theorem 8 is not expected to perform better than taking the solution to the joint subproblem, w\u2217S1:k , where S1:k = S1 \u222a ... \u222a Sk. However, theoretical guarantees for that case are not yet available."}, {"heading": "4.1 Proof of Theorem 5", "text": "We use the following lemma regarding the leave-one-out loss for linear regression [3]:\nLemma 9 Let w\u2217\u2212i denote the least squares solution for problem (X\u2212i,y\u2212i). Then, we have\nL(w\u2217) = L(w\u2217\u2212i)\u2212 x\u22a4i (XX\u22a4)\u22121xi \u2113i(w\u2217\u2212i), where \u2113i(w) def = (x\u22a4i w \u2212 yi)2.\nIn particular, whenX has d+1 columns andX\u2212i is a full-rank d\u00d7dmatrix, thenL(w\u2217\u2212i) = \u2113i(w\u2217\u2212i) and Lemma 9 leads to the following:\ndet(X\u0303X\u0303\u22a4) (1) = det(XX\u22a4) L(w\u2217)\ufe37 \ufe38\ufe38 \ufe37 \u2016y\u0302 \u2212 y\u20162 where X\u0303 = ( X\ny\u22a4\n)\n(2) = det(XX\u22a4)(1 \u2212 x\u22a4i (XX\u22a4)\u22121xi)\u2113i(w\u2217\u2212i) (3) = det(X\u2212iX \u22a4 \u2212i)\u2113i(w \u2217 \u2212i), (3)\nwhere (1) is the \u201cbase \u00d7 height\u201d formula for volume, (2) follows from Lemma 9 and (3) follows from a standard determinant formula. Returning to the proof, our goal is to find the expected loss E[L(w\u2217S)], where S is a size d volume sampled set. First, we rewrite the expectation as follows:\nE[L(w\u2217S)] = \u2211\nS,|S|=d\nProb(S)L(w\u2217S) = \u2211\nS,|S|=d\nProb(S)\nn\u2211\nj=1\n\u2113j(w \u2217 S)\n= \u2211\nS,|S|=d\n\u2211\nj /\u2208S\nProb(S) \u2113j(w \u2217 S) =\n\u2211\nT,|T |=d+1\n\u2211\nj\u2208T\nProb(T\u2212j) \u2113j(w \u2217 T\u2212j ). (4)\nWe now use (3) on the matrixXT and test instance xj (assuming rank(XT\u2212j ) = d):\nProb(T\u2212j) \u2113j(w \u2217 T\u2212j ) =\ndet(XT\u2212jX \u22a4 T\u2212j )\ndet(XX\u22a4) \u2113j(w\n\u2217 T\u2212j ) =\ndet(X\u0303T X\u0303 \u22a4 T )\ndet(XX\u22a4) . (5)\nSince the summand does not depend on the index j \u2208 T , the inner summation in (4) becomes a multiplication by d+ 1. This lets us write the expected loss as:\nE[L(w\u2217S)] = d+ 1\ndet(XX\u22a4)\n\u2211\nT,|T |=d+1\ndet(X\u0303T X\u0303 \u22a4 T )\n(1) = (d+ 1)\ndet(X\u0303X\u0303\u22a4)\ndet(XX\u22a4)\n(2) = (d+ 1)L(w\u2217), (6)\nwhere (1) follows from the Cauchy-Binet formula and (2) is an application of the \u201cbase \u00d7 height\u201d formula. If X is not in general position, then for some summands in (5), rank(XT\u2212j ) < d and Prob(T\u2212j) = 0. Thus the left-hand side of (5) is 0, while the right-hand side is non-negative, so (6) becomes an inequality, completing the proof of Theorem 5."}, {"heading": "5 Efficient algorithm for volume sampling", "text": "In this section we propose an algorithm for efficiently performing exact volume sampling for any s \u2265 d. This addresses the question posed by [1], asking for a polynomial-time algorithm for the case when s > d. [5, 10] gave an algorithm for the case when s = d, which runs in time O(nd3). Recently, [14] offered an algorithm for arbitrary s, which has complexity O(n4s). We propose a new method, which uses our techniques to achieve the time complexity O((n \u2212 s+ d)nd), a direct improvement over [14] by a factor of at least n2. Our algorithm also offers an improvement for s = d in certain regimes. Namely, when n = o(d2), then our algorithm runs in time O(n2d) = o(nd3), faster than the method proposed by [5].\nOur algorithm implements reverse iterative sampling from Corollary 2. After removing q columns, we are left with an index set of size n\u2212q that is distributed according to volume sampling for column set size n\u2212 q.\nTheorem 10 Algorithm 1 runs in timeO((n\u2212s+d)nd), while usingO(d2+n) additional memory, and returns set S which is distributed according to size s volume sampling overX.\nProof For correctness we show the following invariants that hold at the beginning of the while loop:\npi = 1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi = (|S| \u2212 d)P (i|S) and Z = (XSX\u22a4S )\u22121. At the first iteration the invariants trivially hold. When updating the pj we use Z and the pi from the previous iteration, so we can rewrite the update as Algorithm 1 Volume sampling\nInput: X\u2208Rd\u00d7n, s\u2208{d..n} Z \u2190 (XX\u22a4)\u22121 \u2200i\u2208{1..n} pi \u2190 1\u2212 x\u22a4i Zxi S \u2190 {1, .., n} while |S| > s Sample i \u221d pi out of S S \u2190 S \u2212 {i} v \u2190 Zxi/ \u221a pi\n\u2200j\u2208S pj \u2190 pj \u2212 (x\u22a4j v)2 Z \u2190 Z+ vv\u22a4\nend return S\npj \u2190 pj \u2212 (x\u22a4j v)2\n= 1\u2212 x\u22a4j (XSX\u22a4S )\u22121xj \u2212 (x\u22a4j Zxi) 2\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi\n= 1\u2212 x\u22a4j (XSX\u22a4S )\u22121xj \u2212 x\u22a4j (XSX \u22a4 S ) \u22121xix \u22a4 i (XSX \u22a4 S ) \u22121xj\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi\n= 1\u2212 x\u22a4j ( (XSX \u22a4 S ) \u22121 + (XSX \u22a4 S ) \u22121xix \u22a4 i (XSX \u22a4 S ) \u22121\n1\u2212 x\u22a4i (XSX\u22a4S )\u22121xi\n) xj\n(\u2217) = 1\u2212 x\u22a4j (XS\u2212iXS\u2212i)\u22121xj = (|S| \u2212 1\u2212 d)P (j|S\u2212i),\nwhere (\u2217) follows from the Sherman-Morrison formula. The update of Z is also an application of Sherman-Morrison and this concludes the proof of correctness.\nRuntime: Computing the initial Z = (XX\u22a4)\u22121 takes O(nd2), as does computing the initial values of pj\u2019s. Inside the while loop, updating pj\u2019s takes O(|S|d) = O(nd) and updating Z takes O(d2). The overall runtime becomes O(nd2 + (n \u2212 s)nd) = O((n \u2212 s + d)nd). The space usage (in addition to the input data) is dominated by the pi values and matrix Z."}, {"heading": "6 Conclusions", "text": "We developed exact formulas for E[(XIS) +)] and E[(XIS) +)2] when the subset S of s column indices is sampled proportionally to the volume det(XSX \u22a4 S ). The formulas hold for any fixed size s \u2208 {d..n}. These new expectation formulas imply that the solution w\u2217S for a volume sampled subproblem of a linear regression problem is unbiased. We also gave a formula relating the loss of the subproblem to the optimal loss (ie E(L(w\u2217S)) = (d+1)L(w\n\u2217)). However, this result only holds for sample size s = d. It is an open problem to obtain such an exact expectation formula for s > d.\nA natural algorithm is to draw k samples Si of size d and return w \u2217 S1:k\n, where S1:k = \u22c3 i Si. We\nwere able to get exact expressions for the loss L( 1k \u2211\niwSi) of the average predictor but it is an open problem to get nontrivial bounds for the loss of the best predictorw\u2217S1:k .\nWe believe that our results demonstrate a fundamental connection between volume sampling and linear regression, which demands further exploration. Our loss expectation formula has already been applied by [17] to the task of linear regression without correspondence."}], "references": [{"title": "Faster subset selection for matrices and applications", "author": ["Haim Avron", "Christos Boutsidis"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Rich coresets for constrained linear regression", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1202.3505,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "GrantWang"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Fast approximation of matrix coherence and statistical leverage", "author": ["Petros Drineas", "Malik Magdon-Ismail", "Michael W. Mahoney", "David P. Woodruff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Klimko, editors. Theory of optimal experiments. Probability and mathematical statistics", "author": ["Valeri Vadimovich Fedorov", "W.J. Studden", "E.M"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1972}, {"title": "Bayesian low-rank determinantal point processes", "author": ["Mike Gartrell", "Ulrich Paquet", "Noam Koenigstein"], "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["Byungkon Kang"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "k-DPPs: Fixed-Size Determinantal Point Processes", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Determinantal Point Processes for Machine Learning", "author": ["Alex Kulesza", "Ben Taskar"], "venue": "Now Publishers Inc.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Column Subset Selection via Polynomial Time Dual Volume Sampling", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "ArXiv e-prints,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Pool-based active learning in approximate linear regression", "author": ["Masashi Sugiyama", "Shinichi Nakajima"], "venue": "Mach. Learn.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Linear regression without correspondence. personal communication, 2017", "author": ["Xiaorui Sun", "Daniel Hsu"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "This question is a version of the \u201cminimal coresets\u201d open problem posed in [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "Also any iid sampling of S (such as the commonly used leverage scores [7]) requires at least \u03a9(d log d) examples to achieve a finite factor.", "startOffset": 70, "endOffset": 73}, {"referenceID": 12, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 206, "endOffset": 209}, {"referenceID": 10, "context": "Volume sampling is an extension of a determinantal point process [13], which has been given a lot of attention in the literature with many applications to machine learning, including recommendation systems [9] and clustering [11].", "startOffset": 225, "endOffset": 229}, {"referenceID": 4, "context": "Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in the design of randomized algorithms.", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "Many exact and approximate methods for efficiently generating samples from this distribution have been proposed [5, 12], making it a useful tool in the design of randomized algorithms.", "startOffset": 112, "endOffset": 119}, {"referenceID": 0, "context": "In this paper, we study volume sampling sets of size s \u2265 d, which has been proposed in [1] and motivated with applications in graph theory, linear regression, matrix approximation and more.", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "The only known polynomial time algorithm for size s > d volume sampling was recently proposed in [14] with time complexityO(ns).", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "The problem of selecting a subset of input vectors for solving a linear regression task has been extensively studied in statistics literature under the terms optimal design [8] and pool-based active learning [16].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "The problem of selecting a subset of input vectors for solving a linear regression task has been extensively studied in statistics literature under the terms optimal design [8] and pool-based active learning [16].", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 3, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 1, "context": "A related task has been explored in the field of computational geometry, where efficient algorithms are sought for approximately solving linear regression and matrix approximation [15, 4, 2].", "startOffset": 180, "endOffset": 190}, {"referenceID": 5, "context": "In this line of work, volume sampling size s \u2264 d has been used by [6, 10] for matrix approximation.", "startOffset": 66, "endOffset": 73}, {"referenceID": 9, "context": "In this line of work, volume sampling size s \u2264 d has been used by [6, 10] for matrix approximation.", "startOffset": 66, "endOffset": 73}, {"referenceID": 6, "context": "Another common sampling technique is based on statistical leverage scores [7], which have been effectively used for the task of linear regression.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "However, [1] discuss many problems where controlling the pseudo-inverse of a submatrix is essential.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "This norm formula has been shown in [1], with numerous applications.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "can findw S for which L(w \u2217 S) is only a multiplicative factor away from L(w ) (independent of the number of input vectors n)? This question was posed as an open problem by [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "We use the following lemma regarding the leave-one-out loss for linear regression [3]:", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "This addresses the question posed by [1], asking for a polynomial-time algorithm for the case when s > d.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "[5, 10] gave an algorithm for the case when s = d, which runs in time O(nd).", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[5, 10] gave an algorithm for the case when s = d, which runs in time O(nd).", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "Recently, [14] offered an algorithm for arbitrary s, which has complexity O(ns).", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "We propose a new method, which uses our techniques to achieve the time complexity O((n \u2212 s+ d)nd), a direct improvement over [14] by a factor of at least n.", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "Namely, when n = o(d), then our algorithm runs in time O(nd) = o(nd), faster than the method proposed by [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "Our loss expectation formula has already been applied by [17] to the task of linear regression without correspondence.", "startOffset": 57, "endOffset": 61}], "year": 2017, "abstractText": "Given a full rank matrix X with more columns than rows consider the task of estimating the pseudo inverseX based on the pseudo inverse of a sampled subset of columns (of size at least the number of rows). We show that this is possible if the subset of columns is chosen proportional to the squared volume spanned by the rows of the chosen submatrix (ie, volume sampling). The resulting estimator is unbiased and surprisingly the covariance of the estimator also has a closed form: It equals a specific factor timesXX. Pseudo inverse plays an important part in solving the linear least squares problem, where we try to predict a label for each column of X. We assume labels are expensive and we are only given the labels for the small subset of columns we sample fromX. Using our methods we show that the weight vector of the solution for the sub problem is an unbiased estimator of the optimal solution for the whole problem based on all column labels. We believe that these new formulas establish a fundamental connection between linear least squares and volume sampling. We use our methods to obtain an algorithm for volume sampling that is faster than state-of-the-art and for obtaining bounds for the total loss of the estimated least-squares solution on all labeled columns.", "creator": "LaTeX with hyperref package"}}}