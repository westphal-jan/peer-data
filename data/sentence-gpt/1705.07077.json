{"id": "1705.07077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "What do We Learn by Semantic Scene Understanding for Remote Sensing imagery in CNN framework?", "abstract": "Recently, deep convolutional neural network (DCNN) achieved increasingly remarkable success and rapidly developed in the field of natural image recognition. Compared with the natural image, the scale of remote sensing image is larger and the scene and the object it represents are more macroscopic. This study inquires whether remote sensing scene and natural scene recognitions differ and raises the following questions: What are the key factors in remote sensing scene recognition? Is the DCNN recognition mechanism centered on object recognition still applicable to the scenarios of remote sensing scene understanding? We performed several experiments to explore the influence of the DCNN structure and the scale of remote sensing scene understanding from the perspective of scene complexity. Our experiment shows that understanding a complex scene depends on an in-depth network and multiple-scale perception. Using a visualization method, we qualitatively and quantitatively analyze the recognition mechanism in a complex remote sensing scene and demonstrate the importance of multi-objective joint semantic support. Finally, we show how the DCNN structure has improved and the use of its multiple-scale perceptual support. Our experiments are supported by several large European institutions including the University of California at Berkeley, the University of Pennsylvania, the Institute for International Research in Imaging Science and a European-wide Society of Image Recognition for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for Imaging Science and the European Institute for", "histories": [["v1", "Fri, 19 May 2017 16:23:53 GMT  (1795kb)", "http://arxiv.org/abs/1705.07077v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["haifeng li", "jian peng", "chao tao", "jie chen", "min deng"], "accepted": false, "id": "1705.07077"}, "pdf": {"name": "1705.07077.pdf", "metadata": {"source": "CRF", "title": "What do We Learn by Semantic Scene Understanding for Remote Sensing imagery in CNN framework?", "authors": ["Haifeng Li", "Jian Peng", "Chao Tao", "Jie Chen", "Min Deng"], "emails": ["lihaifeng@csu.edu.cn,", "kingtaochao@csu.edu.cn*(Corresponding"], "sections": [{"heading": null, "text": "remarkable success and rapidly developed in the field of natural image recognition. Compared with the natural image, the scale of remote sensing image is larger and the scene and the object it represents are more macroscopic. This study inquires whether remote sensing scene and natural scene recognitions differ and raises the following questions: What are the key factors in remote sensing scene recognition? Is the DCNN recognition mechanism centered on object recognition still applicable to the scenarios of remote sensing scene understanding? We performed several experiments to explore the influence of the DCNN structure and the scale of remote sensing scene understanding from the perspective of scene complexity. Our experiment shows that understanding a complex scene depends on an in-depth network and multiple-scale perception. Using a visualization method, we qualitatively and quantitatively analyze the recognition mechanism in a complex remote sensing scene and demonstrate the importance of multi-objective joint semantic support.\nKey words DCNN, remote sensing scene understanding, network depth, multiple-scale\nperception, multi-objective joint semantic support"}, {"heading": "1. Introduction", "text": "Scene is the meaningful combination of multiple objects, environments, and contexts.\nCompared with object recognition, scene understanding not only needs to identify the targets, it should also understand the distribution of targets in a scene. Thus, the recognition pattern is complex. For example, we recognize a cat in the natural image classification task, which only requires describing the cat\u2019s characteristics (texture, color, and shape). Caring about what exists around the cat is unnecessary; however, when adapted in the bedroom, we may have to pay close attention to\nbeds, wardrobes, and other common objects and their spatial interrelationships.\nRemote sensing scene has wider imaging range, farther imaging distance, and larger feature\ndimension (such as mountains, rivers, etc.), and it is susceptible to resolution and atmospheric refraction and other factors. The fine-grained information of the object, which leads to many differences in contrast the natural scene, is difficult to express, and the natural scene likely has a smaller scale and abundant characteristics. For example, the categories in Place 2 (Zhou, Garcia et al. 2014) are mainly indoor scenes (bathroom and office), and these scenes contain a mass of features in rich details. A specific office chair model can also be observed, which is impossible in remote sensing scene. However, understanding remote sensing scenes is never simpler than natural scenes(Desachy 1995, Mattar and Al-Rewihi 1998). Understanding remote sensing scenes is usually plagued by multiple scenes sharing the same objects. For example, there are roads and buildings in business and residential areas, and the differences are often slightly reflected in their density and architectural style. This confusion on spatial distribution pattern makes scene recognition more challenging. Therefore, analyzing the transfer between natural image and remote sensing scene is necessary in understanding the remote sensing scene, and this understanding guides the design of the identification method that captures the specialty of itself.\nAs the basic task of scene understanding, scene recognition plays an important role in the\napplication of computer vision, and related research has greatly progressed recently(Dixit, Chen et al. 2015, Yin, Jiao et al. 2015, Zhong, Cui et al. 2015, Rangel, Cazorla et al. 2016). Traditional scene recognition methods mainly use the manually designed algorithm to extract the feature to construct the bag of words (BOW) model for scene identification; (Lazebnik, Schmid et al. 2006)proposed spatial pyramid matching (SPM) combined with BOW for scene recognition, and (Quattoni and Torralba 2009) modeled the spatial structure of a scene for indoor scene recognition. These methods strongly depend on the extraction of local features and complex coding strategies. Inspired by human visual mechanism, deep convolutional neural network (DCNN) could achieve automatic hierarchical feature extraction, and it has significantly succeeded in image recognition. (Shen, Lin et al. 2016) explored the problem of information attenuation in DCNN depth from the perspective of information theory and proposed the relay network. (Wang, Guo et al. 2016) considered the diversity between scene and object recognition and introduced a priori knowledge to DCNN model. These successes indicate that DCNN can accomplish natural scene-centered identification tasks.\nHowever, does the current network still works well in the tasks of remote sensing scene recognition? Although previous works (Langkvist, Kiselev et al. 2016, Sevo and Avramovic 2016) have proven that DCNN can be applied to remote sensing scenarios well, its actual satisfaction of our needs remains distant. To make a model work better, we must seize the commonality between remote sensing and natural scenes, wherein rich semantic and scale information on remote sensing scene exist, while considering the difference between them.\nHuman visual system understands scene by observing it, obtaining information and then\nabstracting knowledge. In some scenes (such as parks), we need longer observed time and more attention to re-organize information and then extract knowledge; however, for some scenes (such as forest), understanding is completed in an instant. This finding implies that investing on the human visual understanding processes of different scenes is not equitable. To judge which scenes need more visual input to understand them, we propose the concept of scene complexity, which estimates how difficult a scene can be discerned by the human visual system. Limited by the plight that the current dataset is rarely defined from the complexity of the scene, we utilize crowdsourcing to build a mesoscale dataset and make it the basic dataset in the follow-up work.\nIs there a mechanism similar to the human visual system in DCNN? Intuitively, the depth of\nthe network greatly affects model performance (Liu, Shen et al. 2014, Kounalakis and Tsampikos 2015, Song, Dai et al. 2016). Depth means that the information transfer in more levels and longer distance, which corresponds to the visual attention time; the size of receptive field (Gal, Hamori et al. 2004, Coates and Ng 2012) determines the DCNN extracts feature in which scale; the different sizes of the receptive field actually corresponds to how far human observes a scene. Note that complex scene understanding often requires observing multiple fields, whether there exists a mechanism in DCNN that multi-objects jointly support the scene understanding. Using a visualized approach, we analyze it qualitatively and quantitatively.\nIn this paper, we focus on the task of remote sensing scene recognition, revealing the\ndifferences between sensing scene and natural scene recognitions based on DCNN, standing on it to explore the pattern as DCNN identifies the remote sensing scene. The main contributions of this paper are as follows:\n We analyzing how the depth of network and the scale of receptive field influence the\nperformance of remote sensing scene understanding tasks, and reveal that using the same\nfixed depth and scale CNN network for all the types of scene understanding resulting in limited performance. Based on these findings, we suggest that addressing this scale bias from different complexity scene is critical to improve remote sensing scene understanding performance\u3002This finding also inspires us designing a scale-specific network architectures may be a better way for remote sensing image understanding.\n We demonstrate the importance of joint multi-objective semantic support for fine-grained\nremote sensing scene understanding by analyzing the response of class activation maps in CNNs. This finds shows that Scene and object recognition are two closed related visual tasks, and solve these tasks in an integrated fashion may be a better way for remote sensing image understanding."}, {"heading": "2. Construction of Experimental Dataset", "text": "The quality of the training dataset is an important factor facilitating the performance of the\nmodel(Xiao, Hays et al. 2010, Yu, Seff et al. 2015). Superior remote sensing scene dataset not only merely refers to large data, it can also include diverse semantic information (Handa, Patraucean et al. 2015). As especially emphasized, we analyze the influence of network depth and the scale of receptive field from the perspective of scene complexity. For this reason, based on existing remote sensing dataset, we select AID (Xia, Hu et al. 2016) and then sort 22 categories of scene, in which complexity is more distinguishable as our basic dataset (Figure 1). The dataset contains 360 samples per class; each sample is a size of 600\u00d7600 RGB image, noting that in order to balance the image size and hardware limits, we resize the original image to 256 \u00d7 256 in the experiment.\nOn the basis of the remote sensing interpretation knowledge, the dataset is divided into three\nsuperclasses: low, moderate, and high complexities. To objectively evaluate the scene complexity of samples, we invited 10 volunteers to select 10 to 15 samples randomly and score 10\u22121 (Figure 1) according to the level of complexity. In this paper, we defined the score range from 1 to 4 as low complexity, 4 to 7 as moderate complexity, and 8 to 10 as high complexity and then add up the final score to evaluate the grade of the complex scene.\nTable 1 provides the scene complexity evaluation results based on subjective visual scoring. I, II, and \u2162 denote low, middle, and high complexity, respectively."}, {"heading": "Categories Mean Mode frequency Guide by", "text": "mean and then classifies it according to a threshold, whereas the second method classifies it and then takes mode. The results of the two statistical methods are essentially similar in addition to the differences in individual classes (such as playground). The complexity evaluation of major classes is substantially uniform (Figure 2).\n3. How Does Network Depth Influence Scene Semantics Understanding?\nAbundant research (Krizhevsky, Sutskever et al. 2012, Simonyan and Zisserman 2014) has\nshown that the deeper the network, the stronger the representation power of the model. However, empirical evidence shows that merely increasing network depth does not effectively improve model performance(He, Zhang et al. 2015, Srivastava, Greff et al. 2015). Intuitively, the needs of the time human eye to observe would change in scenes with different complexities, that is, there are differences in the degree of difficulty when identifying scenes with different complexities. Therefore, we analyze the influence of network depth on the remote sensing scene recognition task from the scene complexity. Experimental results show that different complexity scenarios require various network depths and that highly complex scene recognition will benefit from depth nevertheless, in\nidentifying the low complexity scene; this benefit is not obvious or even harmful. However, deepening the network generally does contribute in improving network performance.\n3.1 Experiment design in network depth\nTo analyze the influence of DCNN depth on the task of remote sensing scene recognition, we\nchoose Alex 3/4/5 Conv-Net (Krizhevsky, Sutskever et al. 2012) as our benchmark models. These models are the same except for the use of 3/4/5 convolution layers. To exclude our experimental results for only a specific network, analogously, repeating experiments on VGG 16/19 Net (Simonyan and Zisserman 2014), networks are composed of 13/16 convolution layers and 3 fully connected layers. A unified size of convolutional kernel that eliminates the interference of other possible factors is used.\nExperiments use the Caffe (Jia, Yangqing et al. 2014) deep learning framework and train\nmodels on the AID-22 Scene dataset. Note that considering the training difficulties caused by deep network, we pre-train and then fine-tune the VGGNet on the ImageNet dataset to obtain the final model.\n3.2 Experimental analysis and conclusion\nWe used overall accuracy (OA) to evaluate the model. Table 2 shows the results of testing on\nVGG-net after 20 w iterations. The table indicates that in the task of remote sensing scene recognition, model performance is affected by various factors and not only network depth. Simply deepening network depth do not improve model performance. Similar to the AlexNet series network (Table 3), 3conv-layers performs best and 5conv-layers performs worst, and simply increasing network depth damages network performance. The VGG performance is remarkable relative to AlexNet mainly due to the depth of the network and the uniform size of the convolution kernel; therefore, the model has a stronger ability to abstract representation and better captures the details of image features.\nVGG-16 96.15 94.10 VGG-19 96.89 94.91\n\uff08%\uff09 \uff08%\uff09\nAlexnet-3conv 85.32 78.02 Alexnet-4conv 85.67 75.47 Alexnet-5conv 81.58 75.07\nIn the task of remote sensing scene recognition, network performs unequally on various scenes,\nand the performance of different networks is not the same on one-scene recognition. We test VGGnet with different depths to recognize each scene class (Figure 3). Varying model depths perform differently, for example, the VGG-16 has preferable bare land recognition and weak desert scene recognition results, whereas the VGG-19 model works better than the VGG-16 on playground recognition.\nVarious network depths have different recognition abilities on different scenes. To answer\nwhether these scenes have something in common and if model depth and recognition ability are related for these scenes, we further analyze the effect of depth on the recognition of different scenarios from the complexity perspective. Different degrees of complexity of a scene have different sensitivities on network depth, and the change of network depth has minimal effect on the\nrecognition task of simple scenes, but it has great influence on the recognition result of a complex scene. Deepening network depth can effectively improve the ability of the model in recognizing complex scenes, whereas reducing it significantly weakens the ability of the model in identifying complex scenes. Figure 4 shows the overall accuracy of different network depths over the three complexity superclasses. On the AlexNet and VGG networks, a simple scene is better identified in a shallow network, and when deepening the depth, the result worsens; for moderate complex scenes, the overall accuracy increases when the network becomes deeper. Notably, the recognition accuracy of highly complex scenes tested on the VGG network improves as depth increases, but AlexNet begins to decline. This finding intuitively explains that the power of representation in AlexNet is limited on this highly complex scene."}, {"heading": "4. How Does Network Scale Changes Influence Scene Semantic Understanding?", "text": "Remote sensing scenes with large scale and objects in the scene are more ambitious(Wu and\nLi 2009, Blaschke 2010). Therefore, grasping overall characteristics is significant for semantic understanding of remote sensing scene (Benz, Hofmann et al. 2004, Zhang, Xia et al. 2015). Multiple classes share objects, for example, dense residential and center scene are greatly similar, and they both contain roads and buildings; the difference between them is mainly reflected in the building style (Figure 5). This semantic difference under similar content becomes a challenge in remote sensing scene understanding, in which learning the overall characteristics of the model and the spatial distribution pattern between objects are required; thus, it is necessary to consider this\nmulti-scale property. We analyze the influence of scale factors on DCNN in remote sensing scene recognition. The experimental results show that learning characteristics of multiple scales is salutary in the scene recognition process.\nThe scope of a receptive field determines the current layer of the model extract feature in\nwhich scale, and it can be calculated by the size of the convolution kernel. The inception structure (Szegedy, Liu et al. 2015) stacks the convolution layers with different kernel sizes into a module and concatenates the features of multiple-scale input into the next module. Given the structural simplicity, we designed a number of single-scale inception networks excluding other possible factors of interference.\nWe defined the layer input to the inception as layers 1 and 2, which sets uniform convolution\nkernel sizes of 1\u00d71, 3\u00d73, and 5\u00d75. To ensure that each inception structure learns single-scale feature (Table 4), we use zero padding to create the input and output of the feature map consistent."}, {"heading": "Filter concatenation", "text": ""}, {"heading": "Previous layer", "text": "reflects the error reduction of classification results better than random classification. Experimental results show that the DCNN-based remote sensing scene recognition involves learning multiple scale features. The characteristics of the single scale are insufficient to represent all the semantic information of the scene. Differences in the representation ability of different scale features for the remote sensing scene simultaneously exist, and a larger scale of a feature has stronger expressive ability. Table 5 shows the recognition result of the different scale inceptions embedded in the networks. The kappa value of the GoogleNet model with multiple scale features is higher than that of other single-scale models. The inception 5\u00d75 model with a large convolution kernel scale works best in single-scale models; the smallest scale inception 1\u00d71 model recognition has the worst drops. Thus, it can be deduced that remote sensing scene recognition depends on the characteristics of multiple scales, which mainly adapt to large-scale scenes. Although most information are expressed by coarse-grained features, it still requires some fine-grained features.\nrepresentation, and complex scene should be represented from multiple scale features. Figure 6 shows the manner in which different inception models identify different complexity scenes; the overall accuracy of multi-scale inception model is higher than other single-scale models for most of the scene recognition. Moreover, the multi-scale inception model outperforms others in highly complex scene recognition tasks, and the accuracy in some of the scenes even increases more than 30%. In analyzing the performance of different scale models on various complexity scenarios (Figure 7), the small-scale model behaves badly on the highly complex scene. As the scale grows, the precision curve rises, noting that the multi-scale model rises fastest and that a similar phenomenon on mid-complexity scene occurs. The accuracy curve of the simple scene peaks in the smaller scale model indicates that the complex remote sensing scene should be represented in a larger scale. The recognition of complex scenes, which requires multiple scales to support, is critical and which simple scenes do not necessarily need."}, {"heading": "5. How Does Combining Target Objects Support Scene Semantic Understanding?", "text": "People complete the understanding of a scene by extracting information from different\nviewpoints in face of complex scenes (Saeed 2015). The current study (Girshick, Donahue et al. 2013, Zhou, Khosla et al. 2014) proves that the model learns a scene by encoding the distribution of objects; however, the feature representation that the model has learned remains unclear. In this\nfeature representation, we use a visual approach to map the relationship between the output and the original input of image and analyze the feature learning mechanism of DCNN in remote sensing scene recognition. The experimental results show that the recognition of complex remote sensing scene involves the process of learning the distribution of multiple objects. The joint distribution of multiple targets supports scene understanding.\n5.1 CAM Visualized Analysis\nIn this paper, we use CAM (Zhou, Khosla et al. 2015) to visualize objects, which highly\ncontributes to recognition. This method builds the mapping relationship between the feature and probability of the image output in the feature transfer process through the hidden layers and reflects the influence of the object on a heat map (Figure 8). The most important part in it is building class activation maps:\n\ud835\udc40\ud835\udc50 = \u2211 \ud835\udc64\ud835\udc58 \ud835\udc50 \ud835\udc58 \ud835\udc53\ud835\udc58(\ud835\udc65, \ud835\udc66) (1)\nwhere \ud835\udc40\ud835\udc50 is the contribution of spatial position (x, y) to the model predicted class c, \ud835\udc64\ud835\udc58 \ud835\udc50 is\nthe weight of the kth neuron of the last convolution layer corresponding to class c, and \ud835\udc53\ud835\udc58(\ud835\udc65, \ud835\udc66) is the activation of the kth neuron of the last convolution layer at position (x, y).\n5.2 Experimental analysis and conclusion\nWe train the model on the AID-22 Scene dataset using the GoogleNet* proposed in CAM.\nThe network removed the auxiliary loss function based on GoogleNet and replaced the full\nconnection layer with global average pooling (GAP). We test 72 samples of 22 classes and the overall accuracy achieves over 90%, which ensures that the model captures useful features of the sample. Table 4 shows the overall accuracy of the model.\ncategories (Figure 8). We find that the DCNN-based scene recognition actually learns the object and the distribution among them as reflected in the spatial location by capturing the characteristics of single or multiple key objects in the scene. For example, in Figure 9, the area of the car is highlighted for the scene whose ground truth is parking, and this finding indicates that the model focuses on learning the characteristics of the car and the overall distributed characteristics of the cars in general. Similarly, the recognition of port scenarios is determined by waters and its surrounding areas. Figure 10 shows the CAM results of scenes with different complexities.\nWe analyze the relationship between the corresponding features and different output\nprobabilities, and we find a strong correlation between the recognition result and the responsive mode of CAM. The model captures the characteristics of the corresponding target in spatial region, and the difference in spatial position is reflected in a responsive pattern, which ultimately affects scene recognition. Figure 11 shows the top-1 to top-5 output probabilities corresponding to the CAMs of a category, and the coast, sand, and waves all respond to the top-1; this finding indicates that the model abstracts the concept of the beach by learning the characteristics of the three categories of objects. The seawater and seawater edge have more intense responses in top-2 to demonstrate that the model mainly learns both types of objects, resulting in the identification of the sample as port category.\nTo exclude this phenomenon only for a specific category, we randomly select 20 samples from\nper category to obtain the CAMs and statistics of the proportion of samples that respond to multiple targets in each scene (Figure 12). CAMs of 80% of the samples in the 15 categories, of which the total number is 22, responds to multiple objects. Meanwhile, in some scenes, the proportion of the samples which meet the pattern is lower than 50%. This result is mainly because these scenes express semantic information through single object, such as the center scene, which represents specific individual object (building).\nWe conduct similar experiments on the CID256 dataset, mask multiple objects of scene and\nanalyze the relationship between the output probability of the model and responsive pattern of the CAMs (Figure 13). The result shows that the recognition of remote sensing scene has learned the comprehensive features of multiple targets and that a single target is insufficient to semantically represent the scene.\nand CAM responds in the forest and road areas; right: block the area of forest, model incorrectly identifies it as river_bridge, and the corresponding CAM response area is mainly concentrated at the road partition."}, {"heading": "6. Discussion and Future Work", "text": "Our work is inspired by the temporal and spatial imbalance of the attention placed in\nunderstanding the scene and focuses on exploring the depth and scale of the model in the task of remote sensing scene recognition from the perspective of scene complexity. A visualization method is used for quantitative and qualitative analyses of the mechanism in remote sensing scene recognition. The main conclusions are as follows:\n1. Remote sensing scene recognition with different complexities depends on different\nnetwork depths and sale characteristics. This means complex scene concept is the representation of multiple scale and level feature, there is implicit correlation between complexity and feature.\n2. Complex scenes rely on the feature representation of multiple scales and the joint semantic\nsupport of multiple targets. We demonstrate that multiple scale involved model outperforms any other single network; and experiments based on CAMs indicates that the distribution of combing targets play significant role in learning mechanism. In our next work, we will design an effective indicator to evaluate remote sensing scene\ncomplexity, and then design a network with an adaptive depth, which considers multiple scales. We will further explore the manner in which the mechanism of multi-objective joint probability can be combined with the generation model to help us better underst and a scene.\nReference Benz, U. C., P. Hofmann, G. Willhauck, I. Lingenfelder and M. Heynen (2004). \"Multiresolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information.\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.\" International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition. Handa, A., V. Patraucean, V. Badrinarayanan, S. Stent and R. Cipolla (2015). \"SceneNet: Understanding Real World Indoor Scenes With Synthetic Data.\" Computer Science. He, K., X. Zhang, S. Ren and J. Sun (2015). \"Deep Residual Learning for Image Recognition.\" Computer Science. Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long and Jonathan (2014). \"Caffe: Convolutional Architecture for Fast Feature Embedding.\" Eprint Arxiv: 675-678. Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. IEEE Computer Society Conference on Computer Vision & Pattern Recognition. Liu, F., C. Shen and G. Lin (2014). \"Deep convolutional neural fields for depth estimation from a single image.\" 5162-5170. Mattar, E. A. and K. M. Al-Rewihi (1998). Remote Image Segmentation and Understanding via Artificial Convolution Neural Network. The Middle East International GIS Conference and Workshops. Quattoni, A. and A. Torralba (2009). \"Recognizing indoor scenes.\" 413-420. Rangel, J. C., M. Cazorla, I. Garc\u00edavarea, J. Mart\u00ednezg\u00f3mez, \u00c9. Fromont and M. Sebban\n(2016). \"Scene classification based on semantic labeling.\" Advanced Robotics(11-12). Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 1- 9. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.\" Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). \"Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.\" Journal of Applied Remote Sensing 9(1): 095064. Zhou, B., A. L. Garcia, J. Xiao, A. Torralba and A. Oliva (2014). \"Learning Deep Features for Scene Recognition using Places Database.\" Advances in Neural Information Processing Systems 1: 487-495. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2014). \"Object Detectors Emerge in Deep Scene CNNs.\" Computer Science. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2015). \"Learning Deep Features for Discriminative Localization.\" Computer Science."}], "references": [{"title": "Multiresolution, object-oriented fuzzy analysis of remote sensing data for GIS-ready information.\" International Journal of Photogrammetry & Remote Sensing", "author": ["U.C. Benz", "P. Hofmann", "G. Willhauck", "I. Lingenfelder", "M. Heynen"], "venue": "Isprs Journal", "citeRegEx": "Benz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Benz et al\\.", "year": 2004}, {"title": "Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536", "author": ["A. Coates", "A.Y. Ng"], "venue": "Photogrammetry & Remote Sensing", "citeRegEx": "Coates and Ng,? \\Q2012\\E", "shortCiteRegEx": "Coates and Ng", "year": 2012}, {"title": "Scene classification with semantic Fisher vectors", "author": ["M. Dixit", "S. Chen", "D. Gao", "N. Rasiwasia", "N. Vasconcelos"], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "Dixit et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dixit et al\\.", "year": 2015}, {"title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition", "citeRegEx": "Girshick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition.", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Depth-adaptive methodologies for 3D image caregorization.", "author": ["Kounalakis", "Tsampikos"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Kounalakis and Tsampikos,? \\Q2015\\E", "shortCiteRegEx": "Kounalakis and Tsampikos", "year": 2015}, {"title": "Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing", "author": ["M. Langkvist", "A. Kiselev", "M. Alirezaie", "A. Loutfi"], "venue": "IEEE Computer Society", "citeRegEx": "Langkvist et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Langkvist et al\\.", "year": 2016}, {"title": "Deep convolutional neural fields for depth estimation from a single image.\" 5162-5170", "author": ["F. Recognition. Liu", "C. Shen", "G. Lin"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Recognizing indoor scenes.", "author": ["A. Workshops. Quattoni", "A. Torralba"], "venue": null, "citeRegEx": "Quattoni and Torralba,? \\Q2009\\E", "shortCiteRegEx": "Quattoni and Torralba", "year": 2009}, {"title": "Scene classification based on semantic labeling.", "author": ["U. Saeed"], "venue": "Pattern Recognition Letters", "citeRegEx": "Saeed,? \\Q2015\\E", "shortCiteRegEx": "Saeed", "year": 2015}, {"title": "Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks", "author": ["L. 740-744. Shen", "Z. Lin", "Q. Huang"], "venue": "Computer Science", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional", "author": ["X. Song", "Y. Dai", "X. Qin"], "venue": "\"Highway", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution", "author": ["L. Wang", "S. Guo", "W. Huang", "Y. Xiong", "Y. Qiao"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.", "author": ["G.S. Xia", "J. Hu", "F. Hu", "B. Shi", "X. Bai", "Y. Zhong", "L. Zhang"], "venue": "Computer Vision and Pattern", "citeRegEx": "Xia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer", "author": ["F. Yu", "A. Seff", "Y. Zhang", "S. Song", "T. Funkhouser", "J. Xiao"], "venue": "SAE and SVM.\" Expert Systems with Applications", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.", "author": ["M. Cui", "Q. Zhu", "L. Zhang"], "venue": "Image Understanding.\" Journal of Sensors", "citeRegEx": "Y. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Y. et al\\.", "year": 2015}, {"title": "Scene Recognition using Places Database.", "author": ["A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "B. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "B. et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "spatial pyramid matching (SPM) combined with BOW for scene recognition, and (Quattoni and Torralba 2009) modeled the spatial structure of a scene for indoor scene recognition.", "startOffset": 76, "endOffset": 104}, {"referenceID": 9, "context": "People complete the understanding of a scene by extracting information from different viewpoints in face of complex scenes (Saeed 2015).", "startOffset": 123, "endOffset": 135}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.", "startOffset": 70, "endOffset": 97}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.", "startOffset": 70, "endOffset": 241}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.", "startOffset": 70, "endOffset": 370}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors.", "startOffset": 70, "endOffset": 579}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.", "startOffset": 70, "endOffset": 781}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.\" International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.", "startOffset": 70, "endOffset": 948}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.\" International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition. Handa, A., V. Patraucean, V. Badrinarayanan, S. Stent and R. Cipolla (2015). \"SceneNet: Understanding Real World Indoor Scenes With Synthetic Data.", "startOffset": 70, "endOffset": 1148}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.\" International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition. Handa, A., V. Patraucean, V. Badrinarayanan, S. Stent and R. Cipolla (2015). \"SceneNet: Understanding Real World Indoor Scenes With Synthetic Data.\" Computer Science. He, K., X. Zhang, S. Ren and J. Sun (2015). \"Deep Residual Learning for Image Recognition.", "startOffset": 70, "endOffset": 1282}, {"referenceID": 11, "context": "\" International Journal of Photogrammetry & Remote Sensing 58(3\u20134): 239-258. Blaschke, T. (2010). \"Object based image analysis for remote sensing.\" Isprs Journal of Photogrammetry & Remote Sensing 65(1): 2-16. Coates, A. and A. Y. Ng (2012). \"Selecting Receptive Fields in Deep Networks.\" Advances in Neural Information Processing Systems: 2528-2536. Desachy, J. (1995). \"Active fusion for remote sensing image understanding.\" Proceedings of SPIE - The International Society for Optical Engineering 2579: 67-77. Dixit, M., S. Chen, D. Gao, N. Rasiwasia and N. Vasconcelos (2015). Scene classification with semantic Fisher vectors. Computer Vision and Pattern Recognition. Gal, V., J. Hamori, T. Roska, D. Balya, Z. Borostyankoi, M. Brendel, K. Lotz, L. Negyessy and L. Orzo (2004). \"Receptive Field Atlas and Related CNN Models.\" International Journal of Bifurcation & Chaos 14(2): 551-584. Girshick, R., J. Donahue, T. Darrell and J. Malik (2013). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. Computer Vision and Pattern Recognition. Handa, A., V. Patraucean, V. Badrinarayanan, S. Stent and R. Cipolla (2015). \"SceneNet: Understanding Real World Indoor Scenes With Synthetic Data.\" Computer Science. He, K., X. Zhang, S. Ren and J. Sun (2015). \"Deep Residual Learning for Image Recognition.\" Computer Science. Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long and Jonathan (2014). \"Caffe: Convolutional Architecture for Fast Feature Embedding.", "startOffset": 70, "endOffset": 1438}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.", "startOffset": 0, "endOffset": 32}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.", "startOffset": 0, "endOffset": 146}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.", "startOffset": 0, "endOffset": 338}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories.", "startOffset": 0, "endOffset": 506}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. IEEE Computer Society Conference on Computer Vision & Pattern Recognition. Liu, F., C. Shen and G. Lin (2014). \"Deep convolutional neural fields for depth estimation from a single image.", "startOffset": 0, "endOffset": 709}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. IEEE Computer Society Conference on Computer Vision & Pattern Recognition. Liu, F., C. Shen and G. Lin (2014). \"Deep convolutional neural fields for depth estimation from a single image.\" 5162-5170. Mattar, E. A. and K. M. Al-Rewihi (1998). Remote Image Segmentation and Understanding via Artificial Convolution Neural Network.", "startOffset": 0, "endOffset": 839}, {"referenceID": 5, "context": "Kounalakis and Tsampikos (2015). \"Depth-adaptive methodologies for 3D image caregorization.\" Krizhevsky, A., I. Sutskever and G. E. Hinton (2012). \"ImageNet Classification with Deep Convolutional Neural Networks.\" Advances in Neural Information Processing Systems 25(2): 2012. Langkvist, M., A. Kiselev, M. Alirezaie and A. Loutfi (2016). \"Classification and Segmentation of Satellite Orthoimagery Using Convolutional Neural Networks.\" Remote Sensing 8(4): 329. Lazebnik, S., C. Schmid and J. Ponce (2006). Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories. IEEE Computer Society Conference on Computer Vision & Pattern Recognition. Liu, F., C. Shen and G. Lin (2014). \"Deep convolutional neural fields for depth estimation from a single image.\" 5162-5170. Mattar, E. A. and K. M. Al-Rewihi (1998). Remote Image Segmentation and Understanding via Artificial Convolution Neural Network. The Middle East International GIS Conference and Workshops. Quattoni, A. and A. Torralba (2009). \"Recognizing indoor scenes.", "startOffset": 0, "endOffset": 1023}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.", "startOffset": 0, "endOffset": 17}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.", "startOffset": 0, "endOffset": 165}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing.", "startOffset": 0, "endOffset": 344}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.", "startOffset": 0, "endOffset": 501}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.", "startOffset": 0, "endOffset": 624}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.", "startOffset": 0, "endOffset": 784}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.", "startOffset": 0, "endOffset": 874}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.", "startOffset": 0, "endOffset": 969}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.", "startOffset": 0, "endOffset": 1096}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.", "startOffset": 0, "endOffset": 1269}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo.", "startOffset": 0, "endOffset": 1409}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.", "startOffset": 0, "endOffset": 1559}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.", "startOffset": 0, "endOffset": 1738}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.", "startOffset": 0, "endOffset": 1911}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.\" Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). \"Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.", "startOffset": 0, "endOffset": 2046}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.\" Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). \"Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.\" Journal of Applied Remote Sensing 9(1): 095064. Zhou, B., A. L. Garcia, J. Xiao, A. Torralba and A. Oliva (2014). \"Learning Deep Features for Scene Recognition using Places Database.", "startOffset": 0, "endOffset": 2295}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.\" Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). \"Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.\" Journal of Applied Remote Sensing 9(1): 095064. Zhou, B., A. L. Garcia, J. Xiao, A. Torralba and A. Oliva (2014). \"Learning Deep Features for Scene Recognition using Places Database.\" Advances in Neural Information Processing Systems 1: 487-495. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2014). \"Object Detectors Emerge in Deep Scene CNNs.", "startOffset": 0, "endOffset": 2495}, {"referenceID": 9, "context": "Saeed, U. (2015). \"Eye movements during scene understanding for biometric identification.\" Pattern Recognition Letters 82: 190-195. Sevo, I. and A. Avramovic (2016). \"Convolutional Neural Network Based Automatic Object Detection on Aerial Images.\" IEEE Geoscience and Remote Sensing Letters 13(5): 740-744. Shen, L., Z. Lin and Q. Huang (2016). Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks, Springer International Publishing. Simonyan, K. and A. Zisserman (2014). \"Very Deep Convolutional Networks for LargeScale Image Recognition.\" Computer Science. Song, X., Y. Dai and X. Qin (2016). \"Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network.\" Srivastava, R. K., K. Greff and J. Schmidhuber (2015). \"Highway Networks.\" Computer Science. Szegedy, C., W. Liu, Y. Jia and P. Sermanet (2015). \"Going deeper with convolutions.\" 19. Wang, L., S. Guo, W. Huang, Y. Xiong and Y. Qiao (2016). \"Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs.\" Wu, H. and Z. L. Li (2009). \"Scale issues in remote sensing: a review on analysis, processing and modeling.\" Sensors 9(3): 1768. Xia, G. S., J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong and L. Zhang (2016). \"AID: A Benchmark Dataset for Performance Evaluation of Aerial Scene Classification.\" Xiao, J., J. Hays, K. A. Ehinger and A. Oliva (2010). SUN database: Large-scale scene recognition from abbey to zoo. Computer Vision and Pattern Recognition. Yin, H., X. Jiao, Y. Chai and B. Fang (2015). \"Scene classification based on single-layer SAE and SVM.\" Expert Systems with Applications 42(7): 3368-3380. Yu, F., A. Seff, Y. Zhang, S. Song, T. Funkhouser and J. Xiao (2015). \"LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.\" Computer Science. Zhang, L., G. S. Xia, T. Wu, L. Lin and X. C. Tai (2015). \"Deep Learning for Remote Sensing Image Understanding.\" Journal of Sensors 501: 173691. Zhong, Y., M. Cui, Q. Zhu and L. Zhang (2015). \"Scene classification based on multifeature probabilistic latent semantic analysis for high spatial resolution remote sensing images.\" Journal of Applied Remote Sensing 9(1): 095064. Zhou, B., A. L. Garcia, J. Xiao, A. Torralba and A. Oliva (2014). \"Learning Deep Features for Scene Recognition using Places Database.\" Advances in Neural Information Processing Systems 1: 487-495. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2014). \"Object Detectors Emerge in Deep Scene CNNs.\" Computer Science. Zhou, B., A. Khosla, A. Lapedriza, A. Oliva and A. Torralba (2015). \"Learning Deep Features for Discriminative Localization.", "startOffset": 0, "endOffset": 2627}], "year": 2017, "abstractText": "Recently, deep convolutional neural network (DCNN) achieved increasingly remarkable success and rapidly developed in the field of natural image recognition. Compared with the natural image, the scale of remote sensing image is larger and the scene and the object it represents are more macroscopic. This study inquires whether remote sensing scene and natural scene recognitions differ and raises the following questions: What are the key factors in remote sensing scene recognition? Is the DCNN recognition mechanism centered on object recognition still applicable to the scenarios of remote sensing scene understanding? We performed several experiments to explore the influence of the DCNN structure and the scale of remote sensing scene understanding from the perspective of scene complexity. Our experiment shows that understanding a complex scene depends on an in-depth network and multiple-scale perception. Using a visualization method, we qualitatively and quantitatively analyze the recognition mechanism in a complex remote sensing scene and demonstrate the importance of multi-objective joint semantic support.", "creator": "Microsoft\u00ae Word 2016"}}}