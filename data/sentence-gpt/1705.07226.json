{"id": "1705.07226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "RankPL: A Qualitative Probabilistic Programming Language", "abstract": "In this paper we introduce RankPL, a modeling language that can be thought of as a qualitative variant of a probabilistic programming language with a semantics based on Spohn's ranking theory. Broadly speaking, RankPL can be used to represent and reason about processes that exhibit uncertainty expressible by distinguishing \"normal\" from\" surprising\" events. RankPL allows (iterated) revision of rankings over alternative program states and supports various types of reasoning, including abduction and causal inference. For example, while it has been argued that such procedures are less efficient than the traditional techniques of classification, it appears that RankPL can be used to illustrate and explain a problem that could be solved in real-time. This paper was coauthored by David Brounner, and led by J. Brounner.", "histories": [["v1", "Fri, 19 May 2017 23:58:26 GMT  (34kb)", "http://arxiv.org/abs/1705.07226v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["tjitze rienstra"], "accepted": false, "id": "1705.07226"}, "pdf": {"name": "1705.07226.pdf", "metadata": {"source": "CRF", "title": "RankPL: A Qualitative Probabilistic Programming Language", "authors": ["Tjitze Rienstra"], "emails": ["tjitze@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n07 22\n6v 1\n[ cs\n.A I]\n1 9\nM ay"}, {"heading": "1 Introduction", "text": "Probabilistic programming languages (PPLs) are programming languages extended with statements to (1) draw values at random from a given probability distribution, and (2) perform conditioning due to observation. Probabilistic programs yield, instead of a deterministic outcome, a probability distribution over possible outcomes. PPLs greatly simplify representation of, and reasoning with rich probabilistic models. Interest in PPLs has increased in recent years, mainly in the context of Bayesian machine learning. Examples of modern PPLs include Church, Venture and Figaro [4,8,10], while early work goes back to Kozen [6].\nRanking theory is a qualitative abstraction of probability theory in which events receive discrete degrees of surprise called ranks [11]. That is, events are ranked 0 (not surprising), 1 (surprising), 2 (very surprising), and so on, or \u221e if impossible. Apart from being computationally simpler, ranking theory permits meaningful inference without requiring precise probabilities. Still, it provides analogues to powerful notions known from probability theory, like conditioning and independence. Ranking theory has been applied in logic-based AI (e.g. belief revision and non-monotonic reasoning [1,3]) as well as formal epistemology [11].\nIn this paper we develop a language called RankPL. Semantically, the language draws a parallel with probabilistic programming in terms of ranking theory. We start with a minimal imperative programming language (if-then-else, while, etc.) and extend it with statements to (1) draw choices at random from a given ranking function and (2) perform ranking-theoretic conditioning due to observation. Analogous to probabilistic programs, a RankPL programs yields, instead of a deterministic outcome, a ranking function over possible outcomes.\nBroadly speaking, RankPL can be used to represent and reason about processes whose input or behavior exhibits uncertainty expressible by distinguishing normal (rank 0) from surprising (rank > 0) events. Conditioning in RankPL amounts to the (iterated) revision of rankings over alternative program states. This is a form of revision consistent with the well-known AGM and DP postulates for (iterated) revision [1,2]. Various types of reasoning can be modeled, including abduction and causal inference. Like with PPLs, these reasoning tasks can be modeled without having to write inference-specific code.\nThe overview of this paper is as follows. Section 2 deals with the basics of ranking theory. In section 3 we introduce RankPL and present its syntax and formal semantics. In section 4 we discuss two generalized conditioning schemes (L-conditioning and J-conditioning) and show how they can be implemented in RankPL. All the above will be demonstrated by practical examples. In section 5 we discuss our RankPL implementation. We conclude in section 6."}, {"heading": "2 Ranking Theory", "text": "Here we present the necessary basics of ranking theory, all of which is due to Spohn [11]. The definition of a ranking function presupposes a finite set \u2126 of possibilities and a boolean algebra A over subsets of \u2126, which we call events.\nDefinition 1. A ranking function is a function \u03ba : \u2126 \u2192 N\u222a{\u221e} that associates every possibiltiy with a rank. \u03ba is extended to a function over events by defining \u03ba(\u2205) = \u221e and \u03ba(A) = min({\u03ba(w) | w \u2208 A}) for each A \u2208 A \\ \u2205. A ranking function must satisfy \u03ba(\u2126) = 0.\nAs mentioned in the introduction, ranks can be understood as degrees of surprise or, alternatively, as inverse degrees of plausibility. The requirement that \u03ba(\u2126) = 0 is equivalent to the condition that at least one w \u2208 \u2126 receives a rank of 0. We sometimes work with functions \u03bb : \u2126 \u2192 N \u222a {\u221e} that violate this condition. The normalization of \u03bb is a ranking function denoted by ||\u03bb|| and defined by ||\u03bb||(w) = \u03bb(w) \u2212 \u03bb(\u2126). Conditional ranks are defined as follows.\nDefinition 2. Given a ranking function \u03ba, the rank of A conditional on B (denoted \u03ba(A | B) is defined by\n\u03ba(A | B) =\n{\n\u03ba(A \u2229B)\u2212 \u03ba(B) if \u03ba(B) 6= \u221e, \u221e otherwise.\nWe denote by \u03baB the ranking function defined by \u03baB(A) = \u03ba(A | B).\nIn words, the effect of conditioning on B is that the rank of B is shifted down to zero (keeping the relative ranks of the possibilities in B constant) while the rank of its complement is shifted up to \u221e.\nHow do ranks compare to probabilities? An important difference is that ranks of events do not add up as probabilities do. That is, if A and B are disjoint, then \u03ba(A\u222aB) = min(\u03ba(A), \u03ba(B)), while P (A\u222aB) = P (A)+P (B). This is, however,\nconsistent with the interpretation of ranks as degrees of surprise (i.e., A \u222a B is no less surprising than A or B). Furthermore, ranks provide deductively closed beliefs, whereas probabilities do not. More precisely, if we say that A is believed with firmness x (for some x > 0) with respect to \u03ba iff \u03ba(A) > x, then if A and B are believed with firmness x then so is A \u2229B. A similar notion of belief does not exist for probabilities, as is demonstrated by the Lottery paradox [7].\nFinally, note that \u221e and 0 in ranking theory can be thought of as playing the role of 0 and 1 in probability, while min, \u2212 and + play the role, respectively, of +, \u00f7 and \u00d7. Recall, for example, the definition of conditional probability, and compare it with definition 2. This correspondence also underlies notions such as (conditional) independence and ranking nets (the ranking-based counterpart of Bayesian networks) that have been defined in terms of rankings [11]."}, {"heading": "3 RankPL", "text": "We start with a brief overview of the features of RankPL. The basis is a minimal imperative language consisting of integer-typed variables, an if -then-else statement and a while-do construct. We extend it with the two special statements mentioned in the introduction. We call the first one ranked choice. It has the form {s1} \u3008e\u3009 {s2}. Intuitively, it states that either s1 or s2 is executed, where the former is a normal (rank 0) event and the latter a typically surprising event whose rank is the value of the expression e. Put differently, it represents a draw of a statement to be executed, at random, from a ranking function over two choices. Note that we can set e to zero to represent a draw from two equally likely choices, and that larger sets of choices can be represented through nesting.\nThe second special statement is called the observe statement observe b. It states that the condition b is observed to hold. Its semantics corresponds to ranking-theoretic conditioning. To illustrate, consider the program\nx := 10; {y := 1} \u30081\u3009 {{y := 2} \u30081\u3009 {y := 3}}; x := x\u00d7 y;\nThis program has three possible outcomes: x = 10, x = 20 and x = 30, ranked 0, 1 and 2, respectively. Now suppose we extend the program as follows:\nx := 10; {y := 1} \u30081\u3009 {{y := 2} \u30081\u3009 {y := 3}}; observe y > 1; x := x\u00d7 y;\nHere, the observation rules out the event y = 1, and the ranks of the remaining possibilities are shifted down, resulting in two outcomes x = 20 and x = 30, ranked 0 and 1, respectively.\nA third special construct is the rank expression rank b., which evaluates to the rank of the boolean expression b. Its use will be demonstrated later."}, {"heading": "3.1 Syntax", "text": "We fix a set Vars of variables (ranged over by x) and denote by Val the set of integers including \u221e (ranged over by n). We use e, b and s to range over the numerical expressions, boolean expressions, and statements. They are defined by the following BNF rules:\ne := n | x | rank b | (e1 \u2666 e2) (for \u2666 \u2208 {\u2212,+,\u00d7,\u00f7}) b := \u00acb | (b1 \u2228 b2) | (e1 e2) (for \u2208 {=, <}) s := {s0; s1} | x := e | if b then {s1} else {s2} |\nwhile b do {s} | {s1} \u3008e\u3009 {s2} | observe b | skip\nWe omit parentheses and curly brackets when possible and define \u2227 in terms of \u2228 and \u00ac. We write if b then {s} instead of if b then {s} else {skip}, and abbreviate statements of the form {x := e1} \u3008e\u3009 {x := e2} to x := e1 \u3008e\u3009 e2. Note that the skip statement does nothing and is added for technical convenience."}, {"heading": "3.2 Semantics", "text": "The denotational semantics of RankPL defines the meaning of a statement s as a function DJsK that maps prior rankings into posterior rankings. The subjects of these rankings are program states represented by valuations, i.e., functions that assign values to all variables. The initial valuation, denoted by \u03c30, sets all variables to 0. The initial ranking, denoted by \u03ba0, assigns 0 to \u03c30 and\u221e to others. We denote by \u03c3[x \u2192 n] the valuation equivalent to \u03c3 except for assigning n to x.\nFrom now on we associate \u2126 with the set of valuations and denote the set of rankings over \u2126 by K. Intuitively, if \u03ba(\u03c3) is the degree of surprise that \u03c3 is the actual valuation before executing s, then DJsK(\u03ba)(\u03c3) is the degree of surprise that \u03c3 is the actual valuation after executing s. If we refer to the result of running the program s, we refer to the ranking DJsK(\u03ba0). Because s might not execute successfully, DJsK is not a total function over K. There are two issues to deal with. First of all, non-termination of a loop leads to an undefined outcome. Therefore DJsK is a partial function whose value DJsK(\u03ba) is defined only if s terminates given \u03ba. Secondly, observe statements may rule out all possibilities. A program whose outcome is empty because of this is said to fail. We denote failure with a special ranking \u03ba\u221e that assigns \u221e to all valuations. Since \u03ba\u221e 6\u2208 K, we define the range of DJsK by K\u2217 = K \u222a{\u03ba\u221e}. Thus, the semantics of a statement s is defined by a partial function DJsK from K\u2217 to K\u2217.\nBut first, we define the semantics of expressions. A numerical expression is evaluated w.r.t. both a ranking function (to determine values of rank expressions) and a valuation (to determine values of variables). Boolean expressions may also contain rank expressions and therefore also depend on a ranking function. Given a valuation \u03c3 and ranking \u03ba, we denote by \u03c3\u03ba(e) the value of the numerical expression e w.r.t. \u03c3 and \u03ba, and by [b]\u03ba the set of valuations satisfying the boolean expression b w.r.t. \u03ba. These functions are defined as follows.1\n\u03c3\u03ba(n) = n\n\u03c3\u03ba(x) = \u03c3(x)\n\u03c3\u03ba(rank b) = \u03ba([b]\u03ba)\n\u03c3\u03ba(a1 \u2666 a2) = \u03c3\u03ba(a1) \u2666 \u03c3\u03ba(a2)\n[\u00acb]\u03ba = \u2126 \\ [b]\u03ba\n[b1 \u2228 b2]\u03ba = [b1]\u03ba \u222a [b2]\u03ba\n[a1 a2]\u03ba = {\u03c3 \u2208 \u2126 | \u03c3\u03ba(a1) \u03c3\u03ba(a2)}\n1 We omit explicit treatment of undefined operations (i.e. division by zero and some operations involving \u221e). They lead to program termination.\nGiven a boolean expression b we will write \u03ba(b) as shorthand for \u03ba([b]\u03ba) and \u03bab as shorthand for \u03ba[b]\u03ba . We are now ready to define the semantics of statements. It is captured by seven rules, numbered D1 to D7. The first deals with the skip statement, which does nothing and therefore maps to the identity function.\nDJskipK(\u03ba) = \u03ba. (D1)\nThe meaning of s1; s2 is the composition of DJs1K and DJs2K.\nDJs1; s2K(\u03ba) = DJs2K(DJs1K(\u03ba)) (D2)\nThe rank of a valuation \u03c3 after executing an assignment x := e is the minimum of all ranks of valuations that equal \u03c3 after assigning the value of e to x.\nDJx := eK(\u03ba)(\u03c3) = \u03ba({\u03c3\u2032 \u2208 \u2126 | \u03c3 = \u03c3\u2032[x \u2192 \u03c3\u2032\u03ba(e)]}) (D3)\nTo execute if b then {s1} else {s2} we first execute s1 and s2 conditional on b and \u00acb, yielding the rankingsDJs1K(\u03bab) andDJs2K(\u03ba\u00acb). These are adjusted by adding the prior ranks of b and \u00acb and combined by taking the minimum of the two. The result is normalized to account for the case where one branch fails.\nDJif e then {s1} else {s2}K(\u03ba) = ||\u03bb||,\nwhere \u03bb(\u03c3) = min\n(\nDJs1K(\u03bab)(\u03c3) + \u03ba(b), DJs2K(\u03ba\u00acb)(\u03c3) + \u03ba(\u00acb)\n)\n(D4)\nGiven a prior \u03ba, the rank of a valuation after executing s1 \u3008e\u3009 s2 is the minimum of the ranks assigned by DJs1K(\u03ba) and DJs2K(\u03ba), where the latter is increased by e. The result is normalized to account for the case where one branch fails.\nDJ{s1} \u3008e\u3009 {s2}K(\u03ba) = ||\u03bb||, where \u03bb(\u03c3) = min ( DJs1K(\u03ba)(\u03c3), DJs2K(\u03ba)(\u03c3) + \u03c3\u03ba(e) ) (D5)\nThe semantics of observe b corresponds to conditioning on the set of valuations satisfying b, unless the rank of this set is \u221e or the prior ranking equals \u03ba\u221e.\nDJobserve bK(\u03ba) = {\n\u03ba\u221e if \u03ba = \u03ba\u221e or \u03ba(b) = \u221e, or \u03bab otherwise.\n(D6)\nWe define the semantics ofwhile b do {s} as the iterative execution of if b then {s} else {skip} until the rank of b is \u221e (the loop terminates normally) or the result is undefined (s does not terminate). If neither of these conditions is ever met (i.e., if the while statement loops endlessly) then the result is undefined.\nDJwhile b do {s}K(\u03ba) = { Fnb,s(\u03ba) for the first n s.t. F n b,s(\u03ba)(b) = \u221e, or\nundef. if there is no such n, (D7)\nwhere Fb,s : K\u22a5 \u2192 K\u22a5 is defined by Fb,s(\u03ba) = DJif b then {s} else {skip}K(\u03ba). Some remarks. Firstly, the semantics of RankPL can be thought of as a ranking-based variation of the Kozen\u2019s semantics of probabistic programs [6] (i.e.,\nreplacing \u00d7 with + and + with min). Secondly, a RankPL implementation does not need to compute complete rankings. Our implementation discussed in section 5 follows a most-plausible-first execution strategy: different alternatives are explored in ascending order w.r.t. rank, and higher-ranked alternatives need not be explored if knowing the lowest-ranked outcomes is enough, as is often the case.\nExample Consider the two-bit full adder circuit shown in figure 1. It contains two XOR gatesX1, X2, two AND gates A1, A2 and an OR gate O1. The function of this circuit is to generate a binary representation (b1, b2) of the number of inputs among a1, a2, a3 that are high. The circuit diagnosis problem is about explaining observed incorrect behavior by finding minimal sets of gates that, if faulty, cause this behavior.2\nThe listing below shows a RankPL solution. On line 1 we set the constants L and H (representing a low and high signal); and OK and FAIL (to represent the state of a gate). Line 2 encodes the space of possible inputs (L or H, equally likely). The failure variables fa1, fa2, fo1, fx2 and fx2 represent the events of individual gates failing and are set on line 3. Here, we assume that failure is surprising to degree 1. The circuit\u2019s logic is encoded on lines 4-8, where the output of a failing gate is arbitrarily set to L or H. Note that \u2295 stands for XOR. At the end we observe \u03c6.\n1 L := 0; H := 1; OK := 0; FAIL := 1; 2 a1 := (L \u30080\u3009 H); a2 := (L \u30080\u3009 H); a3 := (L \u30080\u3009 H); 3 fx1 := (OK \u30081\u3009 FAIL); fx2 := (OK \u30081\u3009 FAIL); fa1 := (OK \u30081\u3009 FAIL); fa2 := (OK \u30081\u3009 FAIL); fo1 := (OK \u30081\u3009 FAIL); 4 if fx1 = OK then l1 := a1 \u2295 a2 else l1 := (L \u30080\u3009 H); 5 if fa1 = OK then l2 := a1 \u2227 a2 else l2 := (L \u30080\u3009 H); 6 if fa2 = OK then l3 := l1 \u2227 a3 else l3 := (L \u30080\u3009 H); 7 if fx2 = OK then b2 := l1 \u2295 a3 else b2 := (L \u30080\u3009 H); 8 if fo1 = OK then b1 := l3 \u2228 l2 else b1 := (L \u30080\u3009 H); 9 observe \u03c6;\nThe different valuations of the failure variables produced by this program represent explanations for the observation \u03c6, ranked according to plausibility. Suppose we observe that the input (a1, a2, a3) is valued (L,L,H) while the\n2 See Halpern [5, Chapter 9] for a similar treatment of this example.\noutput (b1, b2) is incorrectly valued (H,L) instead of (L,H). Thus, we set \u03c6 to a1 = L \u2227 a2 = L \u2227 a3 = H \u2227 b1 = H \u2227 b2 = L. The program then produces one outcome ranked 0, namely (fa1, fa2, fo1, fx2, fx2) = (OK, OK, OK, FAIL, OK). That is, \u03c6 is most plausibly explained by failure of X1. Other outcomes are ranked higher than 0 and represent explanations involving more than one faulty gate."}, {"heading": "4 Noisy Observation and Iterated Revision", "text": "Conditioning by A means that A becomes believed with infinite firmness. This is undesirable if we have to deal with iterated belief change or noisy or untrustworthy observations, since we cannot, after conditioning on A, condition on events inconsistent with A. J-conditioning [3] is a more general form of conditioning that addresses this problem. It is parametrized by a rank x that indicates the firmness with which the evidence must be believed.\nDefinition 3. Let A \u2208 \u2126, \u03ba a ranking function over \u2126 such that \u03ba(A), \u03ba(A) < \u221e, and x a rank. The J-conditioning of \u03ba by A with strength x, denoted by \u03baA\u2192x, is defined by \u03baA\u2192x(B) = min(\u03ba(B|A), \u03ba(B|A) + x).\nIn words, the effect of J-conditioning by A with strength x is that A becomes believed with firmness x. This permits iterated belief change, because the rank of A is shifted up only by a finite number of ranks and hence can be shifted down again afterwards. Instead of introducing a special statement representing J-conditioning, we show that we can already express it in RankPL, using ranked choice and observation as basic building blocks. Below we write \u03bab\u2192x as shorthand for \u03ba[b]\u03ba\u2192x. Proofs are omitted due to space constraints.\nTheorem 1. Let b be a boolean expression and \u03ba a ranking function s.t. \u03ba(b) < \u221e and \u03ba(\u00acb) < \u221e. We then have \u03bab\u2192x = DJ{observe b} \u3008x\u3009 {observe \u00acbK(\u03ba)}. L-conditioning [3] is another kind of generalized conditioning. Here, the parameter x characterizes the \u2018impact\u2019 of the evidence.\nDefinition 4. Let A \u2208 \u2126, \u03ba a ranking function over \u2126 such that \u03ba(A), \u03ba(A) < \u221e, and x a rank. The L-conditioning of \u03ba is denoted by \u03baA\u2191x and is defined by \u03baA\u2191x(B) = min(\u03ba(A \u2229B)\u2212 y, \u03ba(\u00acA \u2229B) + x\u2212 y), where y = min(\u03ba(A), x).\nThus, L-conditioning by A with strength x means that A improves by x ranks w.r.t. the rank of \u00acA. Unlike J-conditioning, L-conditioning satisfies two properties that are desirable for modeling noisy observation: reversibility ((\u03baA\u2191x)A\u2191x = \u03ba) and commutativity ((\u03baA\u2191x)B\u2191x = (\u03baB\u2191x)A\u2191x) [11]. We can expression Lconditioning in RankPL using ranked choice, observation and the rank expression as basic building blocks. Like before, we write \u03bab\u2191x to denote \u03ba[b]\u03ba\u2191x.\nTheorem 2. Let b be a boolean expression, \u03ba a ranking function over \u2126 such that \u03ba(b), \u03ba(\u00acb) < \u221e, and x a rank. We then have:\n\u03bab\u2191x = D u wwv if (rank(b) \u2264 x) then {observe b} \u3008x\u2212 rank(b) + rank(\u00acb)\u3009 {observe \u00acb}\nelse\n{observe \u00acb} \u3008rank(b)\u2212 x\u3009 {observe b}\n} ~ (\u03ba)\nIn what follows we use the statement observeL(x, b) as shorthand for the statement that represents L-conditioning as defined in theorem 2.\nExample This example involves both iterated revision and noisy observation. A robot navigates a grid world and has to determine its location using a map and two distance sensors. Figure 2 depicts the map that we use. Gray cells represent walls and other cells are empty (ignoring, for now, the red cells and dots). The sensors (oriented north and south) measure the distance to the nearest wall or obstacle. To complicate matters, the sensor readings might not match the map. For example, the X in figure 2 marks an obstacle that affects sensor readings, but as far as the robot knows, this cell is empty.\nThe listing below shows a RankPL solution. The program takes as input: (1) A map, held by an array map of size m \u00d7 n, storing the content of each cell (0 = empty, 1 = wall); (2) An array mv (length k) of movements (N/E/S/W for north/east/south/west) at given time points; and (3) Two arrays ns and ss (length k) with distances read by the north and south sensor at given time points. Note that, semantically, arrays are just indexed variables.\nInput: k: number of steps to simulate Input: mv: array (size \u2265 k) of movements (N/E/S/W) Input: ns and ss: arrays (size \u2265 k) of north and south sensor readings Input: map: 2D array (size m \u00d7 n) encoding the map\n1 t := 0; x := 0 \u30080\u3009 {1 \u30080\u3009 {2 \u30080\u3009 . . . m}}; y := 0 \u30080\u3009 {1 \u30080\u3009 {2 \u30080\u3009 . . . n}}; 2 while (t < k) do { 3 if (mv[t] = N) then y := y+ 1 4 else if (mv[t] = S) then y := y\u2212 1 5 else if (mv[t] = W) then x := x\u2212 1 6 else if (mv[t] = E) then x := x+ 1 else skip; 7 nd := 0; while map[x][y+ nd+ 1] = 0 do nd := nd+ 1; 8 observeL(1, nd = ns[t]); 9 sd := 0; while map[x][y\u2212 sd\u2212 1] = 0 do sd := sd+ 1;\n10 observeL(1, sd = ss[t]); 11 t := t+ 1; 12 }\nThe program works as follows. On line 1 the time point t is set to 0 and the robot\u2019s location (x, y) is set to a randomly chosen coordinate (all equally likely) using nested ranked choice statements. Inside the while loop, which iterates over t, we first process the movement mv[t] (lines 3-6). We then process (lines 7-8) the north sensor reading, by counting empty cells between the robot and nearest wall, the result of which is observed to equal ns[t]\u2014and likewise for the south sensor (lines 9-10). We use L-conditioning with strength 1 to account for possible incorrect observations. On line 11 we update t.\nSuppose we want to simulate a movement from (0, 5) to (4, 5). Thus, we use the inputs mv = {E,E,E,E}, ns = {1, 1, 1, 1}, ss = {2, 1, 2, 3}, while map is set as shown in figure 2 (i.e., 1 for every cell containing a wall, 0 for every other cell). Note that the observed distances stored in ns and ss are consistent with the distances observed along this path, where, at t = 1, the south sensor reads a distance of 1 instead of 2, due to the obstacle X .\nThe different values of x, y generated by this program encode possible locations of the robot, ranked according to plausibility. The dots in figure 2 show the actual locations, while the red cells represent the inferred most plausible (i.e., rank zero) locations generated by the program. Terminating after t = 0 (i.e., setting k to 1) yields four locations, all consistent with the observed distances 1 (north) and 2 (south). If we terminate after t = 1, the robot wrongly believes to be at (6, 4), due to having observed the obstacle. However, if we terminate after t = 3, the program produces the actual location.\nNote that using L-conditioning here is essential. Regular conditioning would cause failure after the third iteration. We could also have used J-conditioning, which gives different rankings of intermediate results."}, {"heading": "5 Implementation", "text": "A RankPL interpreter written in Java can be found at http://github.com/tjitze/RankPL. It runs programs written using the syntax described in this paper, or constructed using Java classes that map to this syntax. The latter makes it possible to embed RankPL programs inside Java code and to make it interact with and use classes and methods written Java. The interpreter is faithful to the semantics described in section 3 and implements the most-plausible-first execution strategy discussed in section 3.2. All examples discussed in this paper are included, as well as a number of additional examples."}, {"heading": "6 Conclusion and Future Work", "text": "We have introduced RankPL, a language semantically similar to probabilistic programming, but based on Spohn\u2019s ranking theory, and demonstrated its utility using examples involving abduction and iterated revision. We believe that the approach has great potential for applications where PPLs are too demanding due to their computational complexity and dependence on precise probability values. Moreover, we hope that our approach will generate a broader and more practical scope for the topics of ranking theory and belief revision which, in the past, have been studied mostly from purely theoretical perspectives.\nA number of aspects were not touched upon and will be addressed in future work. This includes a more fine grained characterization of termination and a\ndiscussion of the relationship with nondeterministic programming, which is a special case of RankPL. Furthermore, we have omitted examples to show that RankPL subsumes ranking networks and can be used to reason about causal rules and actions [3]. We also did not contrast our approach with default reasoning formalisms that use ranking theory as a semantic foundation (see, e.g., [9]).\nEven though we demonstrated that RankPL is expressive enough to solve fairly complex tasks in a compact manner, it is a very basic language that is best regarded as proof of concept. In principle, the approach can be applied to any programming language, whether object-oriented, functional, or LISP-like. Doing so would make it possible to reason about ranking-based models expressed using, for example, recursion and complex data structures. These features are also supported by PPLs such as Church [4], Venture [8] and Figaro [10]."}], "references": [{"title": "On the logic of iterated belief revision", "author": ["A. Darwiche", "J. Pearl"], "venue": "Artificial intelligence 89(1-2), 1\u201329", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Handbook of logic in artificial intelligence and logic programming (vol", "author": ["P. G\u00e4rdenfors", "H. Rott"], "venue": "4). chap. Belief Revision, pp. 35\u2013132. Oxford University Press, Oxford, UK", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Qualitative probabilities for default reasoning, belief revision, and causal modeling", "author": ["M. Goldszmidt", "J. Pearl"], "venue": "Artificial Intelligence 84(1), 57\u2013112", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Church: a language for generative models", "author": ["N.D. Goodman", "V.K. Mansinghka", "D.M. Roy", "K. Bonawitz", "J.B. Tenenbaum"], "venue": "McAllester, D.A., Myllym\u00e4ki, P. (eds.) UAI 2008, Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, Helsinki, Finland, July 9-12, 2008. pp. 220\u2013229. AUAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Reasoning about uncertainty", "author": ["J.Y. Halpern"], "venue": "MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Semantics of probabilistic programs", "author": ["D. Kozen"], "venue": "J. Comput. Syst. Sci. 22(3), 328\u2013 350", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Probability and the logic of rational belief", "author": ["H.E. Kyburg Jr."], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1961}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["V.K. Mansinghka", "D. Selsam", "Y.N. Perov"], "venue": "CoRR abs/1404.0099", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "System Z: A natural ordering of defaults with tractable applications to nonmonotonic reasoning", "author": ["J. Pearl"], "venue": "Parikh, R. (ed.) Proceedings of the 3rd Conference on Theoretical Aspects of Reasoning about Knowledge, Pacific Grove, CA, March 1990. pp. 121\u2013135. Morgan Kaufmann", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Figaro: An object-oriented probabilistic programming language", "author": ["A. Pfeffer"], "venue": "Charles River Analytics Technical Report 137", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "The Laws of Belief - Ranking Theory and Its Philosophical Applications", "author": ["W. Spohn"], "venue": "Oxford University Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Examples of modern PPLs include Church, Venture and Figaro [4,8,10], while early work goes back to Kozen [6].", "startOffset": 59, "endOffset": 67}, {"referenceID": 7, "context": "Examples of modern PPLs include Church, Venture and Figaro [4,8,10], while early work goes back to Kozen [6].", "startOffset": 59, "endOffset": 67}, {"referenceID": 9, "context": "Examples of modern PPLs include Church, Venture and Figaro [4,8,10], while early work goes back to Kozen [6].", "startOffset": 59, "endOffset": 67}, {"referenceID": 5, "context": "Examples of modern PPLs include Church, Venture and Figaro [4,8,10], while early work goes back to Kozen [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 10, "context": "Ranking theory is a qualitative abstraction of probability theory in which events receive discrete degrees of surprise called ranks [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "belief revision and non-monotonic reasoning [1,3]) as well as formal epistemology [11].", "startOffset": 44, "endOffset": 49}, {"referenceID": 2, "context": "belief revision and non-monotonic reasoning [1,3]) as well as formal epistemology [11].", "startOffset": 44, "endOffset": 49}, {"referenceID": 10, "context": "belief revision and non-monotonic reasoning [1,3]) as well as formal epistemology [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "This is a form of revision consistent with the well-known AGM and DP postulates for (iterated) revision [1,2].", "startOffset": 104, "endOffset": 109}, {"referenceID": 1, "context": "This is a form of revision consistent with the well-known AGM and DP postulates for (iterated) revision [1,2].", "startOffset": 104, "endOffset": 109}, {"referenceID": 10, "context": "Here we present the necessary basics of ranking theory, all of which is due to Spohn [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "A similar notion of belief does not exist for probabilities, as is demonstrated by the Lottery paradox [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "This correspondence also underlies notions such as (conditional) independence and ranking nets (the ranking-based counterpart of Bayesian networks) that have been defined in terms of rankings [11].", "startOffset": 192, "endOffset": 196}, {"referenceID": 5, "context": "Firstly, the semantics of RankPL can be thought of as a ranking-based variation of the Kozen\u2019s semantics of probabistic programs [6] (i.", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "J-conditioning [3] is a more general form of conditioning that addresses this problem.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "L-conditioning [3] is another kind of generalized conditioning.", "startOffset": 15, "endOffset": 18}, {"referenceID": 10, "context": "Unlike J-conditioning, L-conditioning satisfies two properties that are desirable for modeling noisy observation: reversibility ((\u03baA\u2191x)A\u2191x = \u03ba) and commutativity ((\u03baA\u2191x)B\u2191x = (\u03baB\u2191x)A\u2191x) [11].", "startOffset": 186, "endOffset": 190}, {"referenceID": 2, "context": "Furthermore, we have omitted examples to show that RankPL subsumes ranking networks and can be used to reason about causal rules and actions [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "These features are also supported by PPLs such as Church [4], Venture [8] and Figaro [10].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "These features are also supported by PPLs such as Church [4], Venture [8] and Figaro [10].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "These features are also supported by PPLs such as Church [4], Venture [8] and Figaro [10].", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "In this paper we introduce RankPL, a modeling language that can be thought of as a qualitative variant of a probabilistic programming language with a semantics based on Spohn\u2019s ranking theory. Broadly speaking, RankPL can be used to represent and reason about processes that exhibit uncertainty expressible by distinguishing \u201cnormal\u201d from \u201csurprising\u201d events. RankPL allows (iterated) revision of rankings over alternative program states and supports various types of reasoning, including abduction and causal inference. We present the language, its denotational semantics, and a number of practical examples. We also discuss an implementation of RankPL that is available for download.", "creator": "LaTeX with hyperref package"}}}