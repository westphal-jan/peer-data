{"id": "1705.07368", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Mixed Membership Word Embeddings for Computational Social Science", "abstract": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. These models have recently risen in popularity due to the performance of scalable algorithms trained in the big data setting. Despite their success, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability.\n\n\n\n\"We believe that by using the tools built into NLP's core framework, NLP will be more effective than traditional NLP, and that this model may improve the performance of NLP tasks, and in the future we could improve that by providing an environment that is more flexible and easy to use in NLP and other languages, which make NLP easier to use.\n\"Our work with NLP can only be performed in real-time, and it is only in very limited circumstances.\"\nMore information on the NLP implementation is available in the following publications:\nThis research paper has also been coauthored by Dr. Andrew G. McNeill and Dr. Paul F. E. Rodden, PhD.", "histories": [["v1", "Sat, 20 May 2017 23:45:54 GMT  (3297kb,D)", "http://arxiv.org/abs/1705.07368v1", null], ["v2", "Thu, 25 May 2017 03:12:35 GMT  (3300kb,D)", "http://arxiv.org/abs/1705.07368v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["james foulds"], "accepted": false, "id": "1705.07368"}, "pdf": {"name": "1705.07368.pdf", "metadata": {"source": "CRF", "title": "Mixed Membership Word Embeddings for Computational Social Science", "authors": ["James Foulds"], "emails": ["jfoulds@ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "Word embedding models, which learn to encode dictionary words with vector space representations, have been shown to be valuable for a variety of NLP tasks such as statistical machine translation (Vaswani et al., 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al., 2011), as they provide a more nuanced representation of words than a simple indicator vector into a dictionary. These models have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, Mikolov et al. (2013a,b) showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents given a fixed computational budget.\nIn this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of Collobert et al. (2011), Mikolov et al. (2013a,b), and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al., 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015). A very standard practice in the literature is to train word embedding models on a generic large corpus such as Wikipedia, and use the\nar X\niv :1\n70 5.\n07 36\n8v 1\n[ cs\n.C L\n] 2\n0 M\nay 2\nembeddings for NLP tasks on the target dataset, cf. (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015). However, as we shall see here, this standard practice might not always be effective, as the size of a dataset does not necessarily correspond to its degree of relevance for a particular analysis. Even very large corpora have idiosyncrasies that can make their embeddings invalid for other domains. For instance, suppose we would like to use word embeddings to analyze scientific articles on machine learning. In Table 1, I report the most similar words to the word \u201clearning\u201d based on word embedding models trained on two corpora. For embeddings trained on articles from the NIPS conference, the most similar words are related to machine learning, as desired, while for Tomas Mikolov\u2019s embeddings trained on the massive, generic Google News corpus, the most similar words relate to learning and teaching in the classroom.\nEven more concerningly, Bolukbasi et al. (2016) have recently shown that word embeddings from standard corpora can encode sexist assumptions implicit in a dataset, such as the analogy \u201cman is to computer programmer as woman is to homemaker.\u201d Based on these results, it is reasonable to expect that they will also typically encode the worldview of the dominant culture, which in the U.S.A. is white, male, and Eurocentric, and is therefore inappropriate for studying, e.g., black female hip-hop artists\u2019 lyrics, or poetry by Syrian refugees. This emphasis on the language norms of the dominant culture could potentially lead to systematic bias against minorities, women, and people of color in NLP applications with real-world consequences, such as automatic essay grading and college admissions. In order to proactively combat these kinds of biases in large generic datasets, there is a need for effective word embedding models for small datasets, so that the most relevant datasets can be used for training, even when they are small.\nIn this paper, I propose a word embedding model that can be trained directly on a small to mediumsized corpus of interest, without the need for a separate big data training set. The primary insight is to use a data-efficient parameter sharing scheme via mixed membership modeling, with inspiration from topic models. Mixed membership models provide a flexible yet efficient latent representation, in which entities are associated with shared, global representations, but to uniquely varying degrees. We can identify the skip-gram word2vec model of Mikolov et al. (2013a,b) as corresponding to a certain naive Bayes topic model, which leads to mixed membership extensions, allowing the use of fewer vectors than words. I show that this leads to better modeling performance without big data, as measured by predictive performance (when the context is leveraged for prediction), as well as to interpretable latent representations that are valuable for computational social science applications."}, {"heading": "2 Background", "text": "In this section, I provide the necessary background on word embeddings, as well as on topic models and mixed membership models. A more detailed discussion of related work is given in the Appendix. Traditional language models aim to predict words given the contexts that they are found in, thereby forming a joint probabilistic model for sequences of words in a language. Bengio et al. (2003) developed improved language models by using distributed representations (Hinton et al., 1986), in which words are represented by neural network synapse weights, or equivalently, vector space embeddings.\nLater authors have noted that these word embeddings are useful for semantic representations of words, independently of whether a full joint probabilistic language model is learned, and that alternative training schemes can be beneficial for learning the embeddings. In particular, Mikolov et al. (2013a,b) proposed the skip-gram model, which inverts the language model prediction task and aims to predict the context given an input word. The skip-gram model is a log-bilinear discriminative probabilistic classifier parameterized by \u201cinput\u201d word embedding vectors vwi for the input words wi, and \u201coutput\u201d word embedding vectors v\u2032wc for context words wc \u2208 context(i), as shown in Table 2, top-left. Topic models such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are another class of probabilistic language models that have been used for semantic representation (Griffiths et al., 2007).\nSkip-gram Skip-gram topic model\nNaive Bayes\nFor each word in the corpus wi For each word wc \u2208 context(i)\nDraw wc|wi via p(wc|wi) \u221d exp(v\u2032wc \u1d40 vwi + bwc)\nFor each word in the corpus wi For each word wc \u2208 context(i)\nDraw wc|wi \u223c Discrete(\u03c6(wi))\nMixed membership\nFor each word in the corpus wi\nDraw a topic zi \u223c Discrete(\u03b8(wi)) For each word wc \u2208 context(i)\nDraw wc|wi via p(wc|wi) \u221d exp(v\u2032wc \u1d40 vzi + bwc)\nFor each word in the corpus wi\nDraw a topic zi \u223c Discrete(\u03b8(wi)) For each word wc \u2208 context(i)\nDraw wc|wi \u223c Discrete(\u03c6(zi))\nTable 2: \u201cGenerative\u201d models. Identifying the skip-gram (top-left)\u2019s word distributions with topics yields analogous topic models (right), and mixed membership modeling extensions (bottom).\nA straightforward way to model text corpora is via unsupervised multinomial naive Bayes, in which a latent cluster assignment for each document selects a multinomial distribution over words, referred to as a topic, with which the documents\u2019 words are assumed to be generated. LDA topic models improve over naive Bayes by using a mixed membership model, in which the assumption that all words in a document d belong to the same topic is relaxed, and replaced with a distribution over topics \u03b8(d). In the model\u2019s assumed generative process, for each word i in document d, a topic assignment zi is drawn via \u03b8(d), then the word is drawn from the chosen topic \u03c6(zi). The mixed membership formalism provides a useful compromise between model flexibility and statistical efficiency: the K topics \u03c6(k) are shared across all documents, thereby sharing statistical strength, but each document is free to use the topics to its own unique degree. Bayesian inference further aids data efficiency, as uncertainty over \u03b8(d) can be managed for shorter documents. Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here."}, {"heading": "3 The Mixed Membership Skip-Gram", "text": "To design our word embedding model for small corpora, we identify connections between word embeddings and topic models, and adapt advances from the topic modeling literature. Although the skip-gram is discriminative, in the sense that it does not jointly model the input words wi, we can equivalently interpret it as encoding a \u201cconditionally generative\u201d process for the context given the words, in order to develop probabilistic models that extend the skip-gram. Following the distributional hypothesis (Harris, 1954), the skip-gram\u2019s word embeddings parameterize discrete probability distributions over words p(wc|wi) which tend to co-occur, and tend to be semantically coherent \u2013 a property leveraged by the Gaussian LDA model of Das et al. (2015). By identifying these discrete distributions with topics \u03c6(wi), we see that the skip-gram can be reinterpreted as a parameterization of a certain supervised naive Bayes topic model (Table 2, top-right). In this topic model, input words wi are fully observed \u201ccluster assignments,\u201d and the words in wi\u2019s contexts are a \u201cdocument.\u201d The skip-gram differs from this supervised topic model only in the parameterization of the \u201ctopics\u201d via word vectors which encode the distribution over words via a log-bilinear model.\nAs in LDA, we can improve on this model by replacing the naive Bayes assumption with a mixed membership assumption. By applying the mixed membership representation to this topic model version of the skip-gram, we obtain the model in the bottom-right of Table 2.1 After once again parameterizing this model with word embeddings, we obtain our final model, the mixed membership skip-gram (MMSG) (Table 2, bottom-left). In the model, each input word has a distribution over topics \u03b8(w). Each topic has a vector-space embedding vk and each output word has an embedding v\u2032w. A topic zi \u2208 {1, . . . ,K} is drawn for each context, and the words in the context are drawn from the log-bilinear model, using the vector for this topic vzi . We expect that the resulting mixed membership word embeddings are beneficial in the small-to-medium data regime for the following reasons:\n1. By using fewer input vectors than words, we can reduce the size of the semantic representation to be learned (output vectors are generally discarded for the purposes of embedding).\n2. The topic vectors are shared across all words, allowing more data to be used per vector. 1The model retains a naive Bayes assumption at the context level, for latent variable count parsimony.\nAlgorithm 1 Training the mixed membership skip-gram via annealed MHW and NCE for j = 1 : maxAnnealingIter do Tj := T0 + \u03bb\u03ba j\nfor i = 1 : N do c \u223c Uniform(|context(wi)|); z(new)i \u223c qwc //using cached samples from alias tables accept or reject z(new)i via Equation 4; If accept, zi := z (new) i\nend for end for \u03b8\u0302 (wi) k :\u221d n (wi)\u00aci k + \u03b1k [V,V\u2032, b] := NCE(inputWords = z, contextWords = w)\n3. Polysemy is automatically addressed by clustering the words into topics, allowing for more topically focused and semantically coherent vector representations.\nOf course, the model also requires more parameters to be learned, namely the mixed membership proportions \u03b8(w). Based on topic modeling, I hypothesized that with care, these added parameters need not adversely affect performance in the small-medium data regime, for two reasons: 1) we can use a Bayesian approach to effectively manage uncertainty in them, and to marginalize them out, which prevents them being a bottleneck during training; and 2) at test time, using the posterior for zi given the context, instead of the \u201cprior\u201d Pr(zi|wi, \u03b8), mitigates the impact of uncertainty in \u03b8(wi):\nPr(zi = k|wi, context(i),V,V\u2032, \u03b8) \u221d \u03b8(wi)k \u220f\nc\u2208context(i) exp(v\u2032\u1d40wcvk + bwc)\u2211V j\u2032=1 exp(v \u2032\u1d40 j\u2032 vk + bj\u2032) . (1)\nIf a single vector v\u0302wi for a word token is desired, we can obtain one from the above, leveraging the context, via the posterior mean, v\u0302wi , \u2211 k vkPr(zi = k|wi, context(i),V,V\u2032, \u03b8). With fewer vectors than words, some model capacity is lost, but the flexibility of the mixed membership representation allows the model to compensate for this. When the number of shared vectors equals the number of words, the mixed membership skip-gram is strictly more representationally powerful than the skip-gram. With more vectors than words, we expect that the increased representational power would be beneficial in the big data regime. As this is not my goal, I leave this for future work."}, {"heading": "4 Learning Mixed Membership Word Embeddings", "text": "To train the mixed membership skip-gram, an online EM algorithm with stochastic gradient M-step updates can readily be derived, similar to that of Tian et al. (2014)\u2019s multi-prototype embedding model, which has multiple vectors per word. However, this is impractical due to aO(KD) complexity for the E-step, and a O(D) complexity for the M-step, where K and D are the number of topics/dictionary words, respectively. Instead, I propose a principled approximate algorithm that is sublinear time in both K and D. The overall strategy is to first impute the z assignments by simulated annealing, using an efficient implementation of a collapsed Gibbs sampler, thereby reducing the learning problem to standard word embedding. Given the imputed z\u2019s, we then learn the topic and word embeddings V, V\u2032 via noise-contrastive estimation, which approximately performs maximum likelihood estimation (or equivalently, MAP estimation with an improper uniform prior). The overall algorithm can be understood as approximately finding a posterior mode, arg maxPr(V,V\u2032, \u03b8, z|w,wc), in the spirit of iterated conditional models (Besag, 1986), but with approximate updates and with iteration avoided, since z can be estimated without the vectors. The overall algorithm is summarized in Algorithm 1."}, {"heading": "4.1 Imputing the z\u2019s: Topic Model Pre-Clustering via Annealed Metropolis-Hastings-Walker", "text": "To engineer such an algorithm, the key insight is that our mixed membership skip-gram model (Table 2, bottom left) is equivalent to the topic model version (Table 2, bottom right), up to the parameterization, and the prior (if any). With sufficiently high dimensional embeddings, the logbilinear model can capture any distribution p(wc|zi), and so the maximum likelihood embeddings would encode the exact same word distributions as the MLE topics for the topic model, \u03c6(zi). However, the topic model admits a collapsed Gibbs sampler (CGS) that efficiently resolves the cluster\nassignments, which cause the bottleneck during the E-step. I therefore propose to reparameterize the mixed membership skipgram as its corresponding topic model for the purposes of imputing the z\u2019s, and run simulated annealing based on the collapsed Gibbs sampler for the topic model to impute the topic assignments. Then, with the z\u2019s fixed, learning the word and topic vectors corresponds to finding the optimal vectors for encoding these fixed \u03c6 distributions via the log-bilinear model.\nThis topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an off-the-shelf clustering algorithm (or LDA) to initially identify different clusters of contexts, and then apply word embedding algorithms on the cluster assignments. However, our clustering is learned based on the word embedding model itself, and clustering at test time is performed via Bayesian reasoning, in Equation 1, rather than via an ad-hoc method. With Dirichlet priors on the parameters, the collapsed Gibbs update is:\np(zi = k|\u00b7) \u221d ( n (wi)\u00aci k + \u03b1k ) |context(i)|\u220f c=1\nn (k)\u00aci wc + \u03b2wc+n(i,c)wc n(k)\u00aci + \u2211 w\u2032 \u03b2w\u2032 + c\u2212 1 , (2)\nwhere \u03b1 and \u03b2 are parameter vectors for Dirichlet priors over the topic and word distributions, n(wi)k and n(k)\u00aciwc are input and output word/topic counts (excluding the current word), and n (i,c) wc is the number of occurrences of word wc before the cth word in the ith context. We scale this algorithm up to thousands of topics using an adapted version of the recently proposed Metropolis-Hastings-Walker algorithm for high-dimensional topic models, which scales sublinearly in K (Li et al., 2014). The method uses a data structure called an alias table, which allows for amortized O(1) time sampling from discrete distributions. A Metropolis-Hastings update is used to correct for approximating the CGS update with a proposal distribution based on these samples. We can interpret the product over the context, which dominates the collapsed Gibbs update, as a product of experts (Hinton, 2002), where each word in the context is an \u201cexpert\u201d which weighs in multiplicatively on the update. In order to approximate this via alias tables, we use proposals which approximate the product of experts with a \u201cmixture of experts.\u201d We select a word wc uniformly from the context, and the proposal qwc draws a candidate topic proportionally to the chosen context word\u2019s contribution to the update:\nc \u223c Uniform(|context(wi)|) , qwc(k) \u221d n (k) wc + \u03b2wc n(k) + \u2211 w\u2032 \u03b2w\u2032 . (3)\nWe expect these proposals to have some resemblance to the target distribution, but to be flatter, which is a property we\u2019d generally like in a proposal distribution. The proposal is implemented efficiently by sampling from the experts via the alias table data structure, in amortized O(1) time, rather than in time linear in the sparsity pattern, as in (Li et al., 2014), since the proposal does not involve the sparse term (which is less important in our case). We perform simulated annealing to optimize over the posterior, which is very natural in a Metropolis-Hastings setting. Interpreting the negative log posterior as the energy function for a Boltzmann distribution at temperature Tj for iteration j, this is achieved by raising the model part of the Metropolis-Hastings acceptance ratio to the power of 1Tj :\nz (new) i \u223c qwc , Pr(accept z (new) i |\u00b7) = min ( 1, (p(zi = z(new)i |\u00b7) p(zi = z (old) i |\u00b7) ) 1 Tj qwc(z (old) i ) qwc(z (new) i ) ) . (4)\nAnnealing also helps with mixing, as the standard Gibbs updates can become nearly deterministic. We use a temperature schedule Tj = T0 + \u03bb\u03baj , where T0 is the target final temperature, \u03ba < 1, and\n\u03bb controls the initial temperature, and therefore mixing in the early iterations. In my experiments, I use T0 = 0.0001, \u03ba = 0.99, and \u03bb = |context|. The acceptance probability can be computed in time constant in K, and sampling is amortized constant time in K, so each iteration is in amortized constant time in K. Rao-Blackwellized estimates of the mixed membership proportions are obtained from the final sample as \u03b8\u0302(wi)k \u221d n (wi)\u00aci k + \u03b1k."}, {"heading": "4.2 Learning the Embeddings: Noise-Contrastive Estimation", "text": "Finally, with the topic assignments imputed and \u03b8 estimated via the topic model, we must learn the embeddings, which is still an expensive O(D) per context via SGD for maximum likelihood estimation. This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noise-contrastive estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012). NCE avoids the expensive normalization step, making the algorithm scale sublinearly in the vocabulary size D. The algorithm solves unsupervised learning tasks by transforming them into the supervised learning task of distinguishing the data from randomly sampled noise. As the number of noise samples tends to infinity, the method increasingly well approximates maximum likelihood estimation, while avoiding explicitly computing the normalization constant or its derivative, as required by a direct optimization of the log-likelihood. We use NCE as a principled approximation to MLE, and hence MAP estimation with a uniform prior, with the overall Algorithm 1 approximately finding a posterior mode. NCE takes as input the log-likelihood of a data point, i.e. a context word given an input word and its topic assignment\nlogPr(wc|~V, wi, zi,b) = logPr0(wc|~V, wi, zi,b) + a = v\u2032\u1d40wcvzi + bwc + a , (5)\nwhere Pr0 refers to an unnormalized distribution, ~V is the vector of all word and topic embeddings, and a is a parameter encoding the corresponding log normalization constant in the current context i. Following Mnih and Teh (2012), we fix a = 0, under the supposition that the NCE procedure will compensate for this by encouraging the distributions to \u201cself-normalize\u201d in order to optimize the NCE objective. NCE performs logistic regression to distinguish between the data samples and the noise samples. Supposing that there are k samples from the noise distribution per word-pair example, the NCE objective function for context i is\nJ (i)(~V,b) , E P\n(i) d\n[log \u03c3(G(wc; ~V, wi, zi,b))]\u2212 kEPn [log(1\u2212 \u03c3(G(wc; ~V, wi, zi,b)))] (6)\nwhere P (i)d is the data distribution for words wc context i, and G(wc; ~V, wi, zi,b) , logPr(wc|~V, wi, zi,b) \u2212 logPn(wc) is the difference in log-likelihood between the model and the noise distributions. We learn the embeddings by stochastic gradient ascent on the NCE objective."}, {"heading": "5 Experiments", "text": "The goals of my experiments were to validate the proposed methods, to study their applicability for computational social science research, and to substantiate my claims vis-\u00e0-vis small versus big data.\nI measured the intrinsic quality of the learned embeddings via their predictive performance, at the skip-gram\u2019s training task, predicting context words wc given input words wi. For each dataset, I held out 10,000 (wc, wi) pairs uniformly at random, where wc \u2208 context(i), |context(i)| = 10, and aimed to predict wc given wi (and optionally, context(i) \\ wc). Since there are a large number of classes, I treat this as a ranking problem, and report the mean reciprocal rank. The experiments were repeated and averaged over 5 train/test splits. I used four datasets of sociopolitical, scientific, and literary\ninterest: the corpus of NIPS articles from 1987 \u2013 1999 (N \u2248 2.3 million), the U.S. presidential state of the Union addresses from 1790 \u2013 2015 (N \u2248 700, 000), the complete works of Shakespeare (N \u2248 240, 000; this version did not contain the Sonnets), and the writings of black scholar and activist W.E.B. Du Bois, as digitized by Project Gutenberg (N \u2248 170, 000). The results are shown in Table 4. I compared to a word frequency baseline, the skip-gram (SG), and Tomas Mikolov/Google\u2019s vectors trained on Google News, N \u2248 100 billion, via CBOW. Simulated annealing was performed for 1,000 iterations, NCE was performed for 1 million minibatches of size 128, and 128-dimensional embeddings were used (300 for Google). I used K = 2, 000 for NIPS, K = 500 for state of the Union, and K = 100 for the two smaller datasets. Methods were able to leverage the remainder of the context, either by adding the context\u2019s vectors, or via the posterior (Equation 1), which helped for all methods except the naive skip-gram. We can identify several noteworthy findings. First, the generic big data vectors (Google+context) were outperformed by the skip-gram on 3 out of 4 datasets (and by the skip-gram topic model on the other), by a large margin, indicating that corpus-specific embeddings are often important. Second, the mixed membership models, using posterior inference, beat or matched their na\u00efve Bayes counterparts, for both the word embedding models and the topic models. As hypothesized, posterior inference on zi at test time was important for good performance. Finally, the topic models beat their corresponding word embedding models at prediction. We therefore consider word embeddings (and topic embeddings) to be primarily valuable for dimensionality reduction, rather than for prediction, at least in the small data regime.\nI also performed several case studies. I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors v\u0302wi for each token (and similarly for author embeddings), and visualized them in two dimensions using t-SNE (Maaten and Hinton, 2008) (all vectors were normalized to unit length). The state of the Union addresses (Figure 1) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period. The embedded topics (in gray) allow us to interpret the latent space. The George W. Bush addresses are embedded near a \u201cwar on terror\u201d topic (\u201cweapons, war...\u201d), and the Barack Obama addresses are embedded near a \u201cstimulus\u201d topic (\u201cpeople, work...\u201d).\nOn the NIPS corpus, for the input word \u201cBayesian\u201d (Table 3), the naive Bayes and skip-gram models learned a topic with words that refer to Bayesian networks, probabilistic models, and neural networks. The mixed membership models are able to separate this into more coherent and specific topics\nincluding Bayesian modeling, Bayesian training of neural networks (for which Sir David MacKay was a strong proponent, and Andreas Weigend wrote an influential early paper), and Monte Carlo methods. By performing the additive composition of word vectors, which we obtain by finding the prior mean vector for each word type w, v\u0304w , \u2211 k vk\u03b8 (w) k (and then normalizing), we obtain relevant topics vk as nearest neighbors (Figure 2). Similarly, we find that vobjectRecognition \u2212 v\u0304object + v\u0304speech \u2248 vspeechRecognition, and vspeechRecognition \u2212 v\u0304speech + v\u0304character \u2248 vcharacterRecognition. The t-SNE visualization of NIPS documents (Figures 2, 3) shows some temporal clustering patterns (blue documents are more recent, red documents are older, and gray points are topics). Zooming in, we see regions of the space corresponding to learning algorithms (bottom), data space and latent space (center), training neural networks (top), and nearest neighbors (bottom-left). I also visualized the authors\u2019 embeddings via t-SNE (Figure 4). We find regions of latent space for reinforcement learning authors (left: \u201cstate, action,...,\u201d Singh, Barto, Sutton), probabilistic methods (right: \u201cmixture, model,\u201d \u201cmonte, carlo,\u201d Bishop, Williams, Barber, Opper, Jordan, Ghahramani, Tresp, Smyth), and evaluation (top-right: \u201cresults, performance, experiments,...\u201d)."}, {"heading": "6 Conclusion", "text": "I have proposed a model-based method for training interpretable corpus-specific word embeddings, without big data, for computational social science. My approach leverages mixed membership representations, the Metropolis-Hastings-Walker algorithm and noise-contrastive estimation. Experimental results for prediction, and real-world case studies on NIPS and state of the Union addresses, indicate that high-quality embeddings and topics can be obtained using the algorithm. I plan to use this approach for substantive social science applications, and to address algorithmic bias/fairness issues. I also plan to extend the methods to leverage big data sets together with a target small data set."}, {"heading": "Acknowledgments", "text": "I thank Eric Nalisnick and Padhraic Smyth for many helpful discussions."}, {"heading": "A Related Work", "text": "In this appendix, I discuss related work in the literature and its relation to the proposed methods.\nA.1 Topic Modeling and Word Embeddings\nThe Gaussian LDA model of Das et al. (2015) improves the performance of topic modeling by leveraging the semantic information encoded in word embeddings. Gaussian LDA modifies the generative process of LDA such that each topic is assumed to generate the vectors via its own Gaussian distribution. Similarly to our MMSG model, in Gaussian LDA each topic is encoded with a vector, in this case the mean of the Gaussian. It takes pre-trained word embeddings as input, rather than learning the embeddings from data within the same model, and does not aim to perform word embedding.\nThe topical word embedding (TWE) models of Liu et al. (2015) reverse this, as they take LDA topic assignments of words as input, and aim to use them to improve the resultant word embeddings. The authors propose three variants, each of which modifies the skip-gram training objective to use LDA topic assignments together with words. In the best performing variant, called TWE-1, a standard skip-gram word embedding model is trained independently with another skip-gram variant, which tries to predict context words given the input word\u2019s topic assignment. The skip-gram embedding and the topic embeddings are concatenated to form the final embedding.\nAt test time, a distribution over topics for the word given the context, p(zi|context(i)) is estimated according to the topic counts over the other context words. Using this as a prior, a posterior over topics given both the input word and the context is calculated, and similarities between pairs of words (with their contexts) are averaged over this posterior, in a procedure inspired by those used by Reisinger and Mooney (2010); Huang et al. (2012). The primary similarity to our MMSG approach is the use of a training algorithm involving the prediction of context words, given a topic. Our method does this\nas part of an overall model-based inference procedure, and we learn mixed membership proportions \u03b8(w) rather than using empirical counts as the prior over topics for a word token. In accordance with the skip-gram\u2019s prediction model, we are thus able to model the context words in the data likelihood term when computing the posterior probability of the topic assignment. TWE-1 requires that topic assignments are available at test time. It provides a mechanism to predict contextual similarity, but not to predict held-out context words, so I was unable to compare to it in my experiments.\nA.2 Multi-Prototype Embedding Models\nMulti-prototype embeddings models are another relevant line of work. These models address lexical ambiguity by assigning multiple vectors to each word type, each corresponding to a different meaning of that word. Reisinger and Mooney (2010) propose to cluster the occurrences of each word type, based on features extracted from its context. Embeddings are then learned for each cluster. Huang et al. (2012) apply a similar approach, but they use initial single-prototype word embeddings to provide the features used for clustering. These clustering methods have some resemblance to our topic model pre-clustering step, although their clustering is applied within instances of a given word type, rather than globally across all word types, as in our methods. This results in models with more vectors than words, while we aim to find fewer vectors than words, to reduce the model\u2019s complexity for small datasets. Rather than employing an off-the-shelf clustering algorithm and then applying an unrelated embedding model to its output, our approach aims to perform model-based clustering within an overall joint model of topic/cluster assignments and word vectors.\nPerhaps the most similar model to ours in the literature is the probabilistic multi-prototype embedding model of Tian et al. (2014), who treat the prototype assignment of a word as a latent variable, assumed drawn from a mixture over prototypes for each word. The embeddings are then trained using EM. Our MMSG model can be understood as the mixed membership version of this model, in which the prototypes (vectors) are shared across all word types, and each word type has its own mixed membership proportions across the shared prototypes. While a similar EM algorithm can be applied to the MMSG, the E-step is much more expensive, as we typically desire many more shared vectors (often in the thousands) than we would prototypes per a single word type (Tian et al. use ten in their experiments). We use the Metropolis-Hastings-Walker algorithm with the topic model reparameterization of our model in order to address this by efficiently pre-solving the E-step."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of Machine Learning Research, 3(Feb):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 259\u2013302.", "citeRegEx": "Besag,? 1986", "shortCiteRegEx": "Besag", "year": 1986}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings", "author": ["T. Bolukbasi", "Chang", "K.-W.", "J.Y. Zou", "V. Saligrama", "A.T. Kalai"], "venue": "Advances in Neural Information Processing Systems 29, pages 4349\u20134357.", "citeRegEx": "Bolukbasi et al\\.,? 2016", "shortCiteRegEx": "Bolukbasi et al\\.", "year": 2016}, {"title": "An efficient, probabilistically sound algorithm for segmentation and word discovery", "author": ["M.R. Brent"], "venue": "Machine Learning, 34(1):71\u2013105.", "citeRegEx": "Brent,? 1999", "shortCiteRegEx": "Brent", "year": 1999}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Gaussian LDA for topic models with word embeddings", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": "ACL, pages 795\u2013804.", "citeRegEx": "Das et al\\.,? 2015", "shortCiteRegEx": "Das et al\\.", "year": 2015}, {"title": "Topics in semantic representation", "author": ["T.L. Griffiths", "M. Steyvers", "J.B. Tenenbaum"], "venue": "Psychological review, 114(2):211.", "citeRegEx": "Griffiths et al\\.,? 2007", "shortCiteRegEx": "Griffiths et al\\.", "year": 2007}, {"title": "A Bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases", "author": ["J. Grimmer"], "venue": "Political Analysis, pages 1\u201335.", "citeRegEx": "Grimmer,? 2010", "shortCiteRegEx": "Grimmer", "year": 2010}, {"title": "The Bayesian echo chamber: Modeling social influence via linguistic accommodation", "author": ["F. Guo", "C. Blundell", "H. Wallach", "K. Heller"], "venue": "Artificial Intelligence and Statistics, pages 315\u2013323.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 13(Feb):307\u2013361.", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? 2012", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, 14(8):1771\u20131800.", "citeRegEx": "Hinton,? 2002", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Distributed representations", "author": ["G.E. Hinton", "J.L. Mcclelland", "D.E. Rumelhart"], "venue": "Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, chapter 3, pages 77\u2013109. MIT Press, Cambridge, MA.", "citeRegEx": "Hinton et al\\.,? 1986", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873\u2013882. Association for Computational Linguistics.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Reducing the sampling complexity of topic models", "author": ["A.Q. Li", "A. Ahmed", "S. Ravi", "A.J. Smola"], "venue": "KDD, pages 891\u2013900. ACM.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "Chua", "T.-S.", "M. Sun"], "venue": "AAAI, pages 2418\u20132424.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Visualizing data using t-SNE", "author": ["Maaten", "L. v. d.", "G. Hinton"], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten et al\\.,? 2008", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Computational historiography: Data mining in a century of classics journals", "author": ["D. Mimno"], "venue": "Journal on Computing and Cultural Heritage (JOCCH), 5(1):3.", "citeRegEx": "Mimno,? 2012", "shortCiteRegEx": "Mimno", "year": 2012}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems, pages 2265\u20132273.", "citeRegEx": "Mnih and Kavukcuoglu,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "Proceedings of the 29th International Conference on Machine Learning.", "citeRegEx": "Mnih and Teh,? 2012", "shortCiteRegEx": "Mnih and Teh", "year": 2012}, {"title": "Modeling topic control to detect influence in conversations using nonparametric topic models", "author": ["Nguyen", "V.-A.", "J. Boyd-Graber", "P. Resnik", "D.A. Cai", "J.E. Midberry", "Y. Wang"], "venue": "Machine Learning, 95(3):381\u2013421.", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["J. Reisinger", "R.J. Mooney"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109\u2013117. Association for Computational Linguistics.", "citeRegEx": "Reisinger and Mooney,? 2010", "shortCiteRegEx": "Reisinger and Mooney", "year": 2010}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["F. Tian", "H. Dai", "J. Bian", "B. Gao", "R. Zhang", "E. Chen", "Liu", "T.-Y."], "venue": "COLING, pages 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP, pages 1387\u20131392.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 19\u201327.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "apply a similar approach, but they use initial single-prototype word embeddings to provide the features used for clustering. These clustering methods have some resemblance to our topic model pre-clustering step, although their clustering is applied within instances of a given word", "author": ["Huang"], "venue": null, "citeRegEx": "Huang,? \\Q2012\\E", "shortCiteRegEx": "Huang", "year": 2012}, {"title": "who treat the prototype assignment of a word as a latent variable, assumed drawn from a mixture over prototypes for each word. The embeddings are then trained using EM. Our MMSG model can be understood as the mixed membership version of this model, in which the prototypes (vectors) are shared across all word types, and each word type", "author": ["Tian"], "venue": null, "citeRegEx": "Tian,? \\Q2014\\E", "shortCiteRegEx": "Tian", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Word embedding models, which learn to encode dictionary words with vector space representations, have been shown to be valuable for a variety of NLP tasks such as statistical machine translation (Vaswani et al., 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al.", "startOffset": 195, "endOffset": 217}, {"referenceID": 5, "context": ", 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al., 2011), as they provide a more nuanced representation of words than a simple indicator vector into a dictionary.", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al.", "startOffset": 282, "endOffset": 297}, {"referenceID": 22, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al.", "startOffset": 317, "endOffset": 330}, {"referenceID": 30, "context": "It should be noted that \u201cbig\u201d datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases (Grimmer, 2010), academic journals (Mimno, 2012), books (Zhu et al., 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al.", "startOffset": 338, "endOffset": 356}, {"referenceID": 4, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 25, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 9, "context": ", 2015), and transcripts of recorded speech (Brent, 1999; Nguyen et al., 2014; Guo et al., 2015).", "startOffset": 44, "endOffset": 96}, {"referenceID": 4, "context": ", 2013), part-of-speech tagging, chunking, and named entity recogition (Collobert et al., 2011), as they provide a more nuanced representation of words than a simple indicator vector into a dictionary. These models have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, Mikolov et al. (2013a,b) showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents given a fixed computational budget. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of Collobert et al. (2011), Mikolov et al.", "startOffset": 72, "endOffset": 810}, {"referenceID": 5, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 20, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 26, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 16, "context": "(Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014; Kiros et al., 2015).", "startOffset": 0, "endOffset": 92}, {"referenceID": 3, "context": "Even more concerningly, Bolukbasi et al. (2016) have recently shown that word embeddings from standard corpora can encode sexist assumptions implicit in a dataset, such as the analogy \u201cman is to computer programmer as woman is to homemaker.", "startOffset": 24, "endOffset": 48}, {"referenceID": 14, "context": "(2003) developed improved language models by using distributed representations (Hinton et al., 1986), in which words are represented by neural network synapse weights, or equivalently, vector space embeddings.", "startOffset": 79, "endOffset": 100}, {"referenceID": 0, "context": "Bengio et al. (2003) developed improved language models by using distributed representations (Hinton et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Topic models such as latent Dirichlet allocation (LDA) (Blei et al., 2003) are another class of probabilistic language models that have been used for semantic representation (Griffiths et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 7, "context": ", 2003) are another class of probabilistic language models that have been used for semantic representation (Griffiths et al., 2007).", "startOffset": 107, "endOffset": 131}, {"referenceID": 6, "context": "Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here.", "startOffset": 74, "endOffset": 110}, {"referenceID": 18, "context": "Some recent papers have aimed to combine topic models and word embeddings (Das et al., 2015; Liu et al., 2015), but they do not aim to address the small data problem for computational social science, which I focus on here.", "startOffset": 74, "endOffset": 110}, {"referenceID": 12, "context": "Following the distributional hypothesis (Harris, 1954), the skip-gram\u2019s word embeddings parameterize discrete probability distributions over words p(wc|wi) which tend to co-occur, and tend to be semantically coherent \u2013 a property leveraged by the Gaussian LDA model of Das et al.", "startOffset": 40, "endOffset": 54}, {"referenceID": 6, "context": "Following the distributional hypothesis (Harris, 1954), the skip-gram\u2019s word embeddings parameterize discrete probability distributions over words p(wc|wi) which tend to co-occur, and tend to be semantically coherent \u2013 a property leveraged by the Gaussian LDA model of Das et al. (2015). By identifying these discrete distributions with topics \u03c6i, we see that the skip-gram can be reinterpreted as a parameterization of a certain supervised naive Bayes topic model (Table 2, top-right).", "startOffset": 269, "endOffset": 287}, {"referenceID": 1, "context": "The overall algorithm can be understood as approximately finding a posterior mode, arg maxPr(V,V\u2032, \u03b8, z|w,wc), in the spirit of iterated conditional models (Besag, 1986), but with approximate updates and with iteration avoided, since z can be estimated without the vectors.", "startOffset": 156, "endOffset": 169}, {"referenceID": 27, "context": "To train the mixed membership skip-gram, an online EM algorithm with stochastic gradient M-step updates can readily be derived, similar to that of Tian et al. (2014)\u2019s multi-prototype embedding model, which has multiple vectors per word.", "startOffset": 147, "endOffset": 166}, {"referenceID": 25, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al.", "startOffset": 67, "endOffset": 95}, {"referenceID": 15, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 15, "context": "This topic model pre-clustering step is reminiscent of the work of Reisinger and Mooney (2010); Huang et al. (2012); Liu et al. (2015), who apply an off-the-shelf clustering algorithm (or LDA) to initially identify different clusters of contexts, and then apply word embedding algorithms on the cluster assignments.", "startOffset": 96, "endOffset": 135}, {"referenceID": 17, "context": "We scale this algorithm up to thousands of topics using an adapted version of the recently proposed Metropolis-Hastings-Walker algorithm for high-dimensional topic models, which scales sublinearly in K (Li et al., 2014).", "startOffset": 202, "endOffset": 219}, {"referenceID": 13, "context": "We can interpret the product over the context, which dominates the collapsed Gibbs update, as a product of experts (Hinton, 2002), where each word in the context is an \u201cexpert\u201d which weighs in multiplicatively on the update.", "startOffset": 115, "endOffset": 129}, {"referenceID": 17, "context": "The proposal is implemented efficiently by sampling from the experts via the alias table data structure, in amortized O(1) time, rather than in time linear in the sparsity pattern, as in (Li et al., 2014), since the proposal does not involve the sparse term (which is less important in our case).", "startOffset": 187, "endOffset": 204}, {"referenceID": 21, "context": "This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noise-contrastive estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012).", "startOffset": 72, "endOffset": 92}, {"referenceID": 21, "context": "This same complexity is also an issue for the standard skip-gram, which Mnih and Teh (2012); Mnih and Kavukcuoglu (2013) have addressed using the noise-contrastive estimation (NCE) algorithm of Gutmann and Hyv\u00e4rinen (2010, 2012).", "startOffset": 93, "endOffset": 121}, {"referenceID": 24, "context": "Following Mnih and Teh (2012), we fix a = 0, under the supposition that the NCE procedure will compensate for this by encouraging the distributions to \u201cself-normalize\u201d in order to optimize the NCE objective.", "startOffset": 10, "endOffset": 30}, {"referenceID": 13, "context": "I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors v\u0302wi for each token (and similarly for author embeddings), and visualized them in two dimensions using t-SNE (Maaten and Hinton, 2008) (all vectors were normalized to unit length). The state of the Union addresses (Figure 1) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period.", "startOffset": 241, "endOffset": 429}], "year": 2017, "abstractText": "Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. These models have recently risen in popularity due to the performance of scalable algorithms trained in the big data setting. Despite their success, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage the notion of mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. Leveraging connections to topic models, I show how to train these models in high dimensions using a combination of state-of-the-art techniques for word embeddings and topic modeling. Experimental results show an improvement in predictive performance of up to 63% in MRR over the skip-gram on small datasets. The models are interpretable, as embeddings of topics are used to encode embeddings for words (and hence, documents) in a model-based way. I illustrate this with two computational social science case studies, on NIPS articles and State of the Union addresses.", "creator": "LaTeX with hyperref package"}}}