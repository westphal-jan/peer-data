{"id": "1705.07661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization", "abstract": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new method for computing binary compressed representations -\\textit{sketches}- of high-dimensional real feature vectors. Given an expected code length $c$ and high-dimensional input data points, our algorithm provides a binary code of $c$ bits aiming at preserving the distance between the points from the original high-dimensional space where the data is encoded as one vector. We introduce a computational and computational process that, if applied in a finite state, can be efficiently partitioned and the data point being captured is created. The algorithm was previously described as a \"compression process\", using an algorithm called NQQM. The algorithms are then partitioned into a set of \"compression layers\" (n-array) using NQQM. We describe the process as the \"compression layer\", as an \"analytic process\", as an \"analytic process\". To make this process work we use NQQM. The process is called a \"synaptic process\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that is \"synaptic\", a process that", "histories": [["v1", "Mon, 22 May 2017 10:54:20 GMT  (444kb,D)", "http://arxiv.org/abs/1705.07661v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anne morvan", "antoine souloumiac", "c\\'edric gouy-pailler", "jamal atif"], "accepted": false, "id": "1705.07661"}, "pdf": {"name": "1705.07661.pdf", "metadata": {"source": "CRF", "title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization", "authors": ["Anne Morvan", "Antoine Souloumiac", "C\u00e9dric Gouy-Pailler", "Jamal Atif"], "emails": ["anne.morvan@cea.fr."], "sections": [{"heading": "1 Introduction", "text": "In large-scale machine learning application fields such as computer vision, or metagenomics, learning similarity-preserving binary codes is critical in order to perform efficient indexing of large-scale highdimensional data. Storage requirements can be reduced and similarity search sped up by embedding high-dimensional data into a compact binary code.\nA classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms.\nIn order to increase the accuracy of the data sketches in the context of nearest neighbors search or classification, this projection can be also learned from data (Weiss et al. [2008], Wang et al. [2012], Gong\n\u2217To whom correspondence should be adressed: anne.morvan@cea.fr. Partly supported by the Direction Ge\u0301ne\u0301rale de l\u2019Armement (French Ministry of Defense).\nar X\niv :1\n70 5.\n07 66\n1v 1\n[ cs\n.L G\n] 2\nand Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al. [2014]). As Principal Component Analysis (PCA) is a common tool for reducing data dimensionality, PCA is often performed: data are then projected onto the first c principal components. But the PCA alone is not sufficient. Indeed, the c first principal components are chosen with a decreasing order of explained variance. It means that principal directions with higher variance carry more information. Consequently, associating each of the c directions to one of the c bits is equivalent to giving more weights to less informative directions and will lead to poor performance of the obtained sketches.\nTo remedy this problem, after data have been projected on the first principal components of the covariance matrix, a solution consists in applying a suitable rotation on the projected data before performing the hashing function, in order to balance variance over the principal components. In work from Je\u0301gou et al. [2010], a random rotation is successfully applied giving quite good results. In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al. [2013]) or in Isotropic Hashing (IsoHash) (Kong and Li [2012]), the rotation is rather learned. In ITQ, the rotation is iteratively computed by an alternating minimization algorithm corresponding to an orthogonal Procrustes problem. This technique is currently the state-of-the-art for computing similarity-preserving binary codes.\nITQ method comes with some drawbacks, though. First, the convergence guarantees are only empirical. The method needs a parameter for the number of iterations required to compute the rotation. Second, the alternative optimization process relies on the computation of the SVD of a c\u00d7 c matrix at each iteration, which even if c is small in comparison with d, corresponds to a time complexity of O(c3). Finally, ITQ is a completely offline process since 1) the whole dataset needs to be stored for computing the c principal components, 2) the computation of the rotation is based on the whole c \u00d7 n projected dataset onto the principal components where n is the number of instances. This can be prohibitive when dealing with lots of high-dimensional data.\nContributions In this paper, our contributions are two-fold: 1) We provide an offline algorithm outperforming ITQ from the following points of view: accuracy, computation time and spatial complexities, and convergence guarantees. 2) We also introduce a streaming adaptation of the algorithm where data is seen only once and principal subspace plus the balancing rotation are updated as new data is seen. Besides the c\u00d7 d principal subspace to return, this requires only the storage of two c\u00d7 c matrices -one is the covariance matrix of the projected data onto the c principal components- instead of the whole initial and projected datasets as it is the case for ITQ. Depending on the availability of data and the expected accuracy of the sketches, the binary compact codes can be computed from one, two or three passes over the data. In any case, for only one pass over the data, our online algorithm gives better results than OSH (Leng et al. [2015a]) while being far less computationally demanding."}, {"heading": "2 Related work", "text": "Two paradigms exist for constructing hash functions (Wang et al. [2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpin\u0303a\u0301n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed. Online Hashing (OKH) (Huang et al. [2013]) learns the hash functions from a stream of pair of data with a \u201dPassive-Aggressive\u201d method. In Online Sketching Hashing (OSH) (Leng et al. [2015a]), the binary embeddings are learned from a maintained sketch of the dataset with a smaller size but preserving the property of interest. The proposed algorithm belongs to this latter category of online unsupervised hyperplanes-based hashing methods."}, {"heading": "3 The proposed offline unsupervised model for binary quantiza-", "text": "tion"}, {"heading": "3.1 Common unsupervised offline binary quantization problem statement", "text": "Let us first introduce some notations. We have a stream of n data points {xt \u2208 Rd}1\u2264t\u2264n which constitute the columns of the data matrix X \u2208 Rd\u00d7n supposed to be zero-centered. Our goal is to learn a binary code matrix B \u2208 {\u22121, 1}c\u00d7n where c denotes the code length such that for each bit k = 1... c, the binary encoding function is defined by hk(xt) = sign(w\u0303 T k xt) where w\u0303k are column vectors of hyperplane coefficients and sign(x) = 1 if x \u2265 0 and \u22121 otherwise which is applied component-wise on coefficients of vectors. B = sign(W\u0303X) where each row k of W\u0303 is w\u0303Tk , with W\u0303 \u2208 Rc\u00d7d.\nIn ITQ and our model, W\u0303 = RW where R is a suitable c\u00d7c orthogonal matrix and the c\u00d7d matrix W represents the linear dimensionality reduction method applied to data. For instance, W can be the matrix whose row vectors wTk correspond to the c first principal components of the covariance matrix C = XX T (an other supervised approach is to perform Canonical Correlation Analysis (Gong et al. [2013])). So the challenge is rather in defining an appropriate orthogonal matrix.\nFor ITQ, R is obtained through the optimization process minimizing the following quantization loss: Q(B,R) = ||B\u2212W\u0303X||2F = ||B\u2212RWX||2F = ||B\u2212RV ||2F where V = WX and ||.||F denotes the Froebenius norm.\nAfter having initialized R with a random orthogonal matrix thanks to a QR decomposition with time complexity O(c3), ITQ alternatively computes B after freezing R and optimizes R according to B by performing the SVD of V TB. Algorithm 2 in appendix shows the procedure for obtaining a locally optimal rotation R1 after K iterations."}, {"heading": "3.2 UnifDiag: the proposed diagonal uniformization-based offline method for learning a suitable rotation", "text": "Let the c \u00d7 c symmetric matrix CovV = V V T be the covariance matrix of projected data V = WX. In our offline model, R is learned to balance the variance over the c directions given by the c principal components of CovV . Let us consider the c diagonal coefficients of CovV : \u03c321 , ..., \u03c3 2 c s.t. \u03c3 2 1 \u2265 ... \u2265 \u03c32c\n2. As CovV is symmetric, Tr(CovV ) = \u2211c i=1 \u03c3 2 i = \u2211c i=1 \u03bbi where Tr stands for the Trace application and \u03bb1, ..., \u03bbc are the c first eigenvalues of C s.t. \u03bb1 \u2265 ... \u2265 \u03bbc. Balancing variance over the c directions can be seen as equalizing the diagonal coefficients of CovV such that \u03c321 = ... = \u03c3 2 c def = \u03c4 . Figure 1 shows the covariance matrices for CIFAR-10 training set (n = 59000, d = 960, c = 32) corresponding to (a) projected data V after basic PCA, (b) rotated and projected data RV with ITQ, and (c) with our algorithm uniformizing the diagonal. One can note that diagonal coefficients for ITQ tend to be identical. So it makes sense to uniformize the diagonal.\nLemma 1. If R is an orthogonal matrix i.e. RRT = RTR = I where I stands for the identity matrix and M is a symmetric matrix, then Tr(R MRT ) = Tr(M). In particular R can be a rotation.\nAccording to Lemma 1, since CovV is symmetric, Tr(R CovV RT ) = Tr(CovV ). So in order to have R CovV RT with equal diagonal coefficients, we should set \u03c4 = Tr(CovV )/c.\nSo, similarly to IsoHash from work in (Kong and Li [2012]), we formulate the problem of finding R as the problem of equalizing the diagonal coefficients of CovV to the value \u03c4 = Tr(CovV )/c."}, {"heading": "3.2.1 Variance uniformization", "text": "We propose to build the optimal orthogonal matrix R as a product of c \u2212 1 Givens rotations G(i, j, \u03b8) described by definition 3.1.\n1In the following, we will use equally the term orthogonal matrix or rotation. 2If W is exactly the c first eigenvectors of C -for instance, if W is obtained through PCA-, \u2200i \u2208 {1, ..., c}, \u03c32i = \u03bbi.\nDefinition 3.1. A Givens rotation G(i, j, \u03b8) is a matrix of the form:\nG(i, j, \u03b8) =  1 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0 ... . . . ... ... ... 0 \u00b7 \u00b7 \u00b7 c \u00b7 \u00b7 \u00b7 \u2212s \u00b7 \u00b7 \u00b7 0 ... ... . . . ... ... 0 \u00b7 \u00b7 \u00b7 s \u00b7 \u00b7 \u00b7 c \u00b7 \u00b7 \u00b7 0 ... ... ... . . . ...\n0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 1  where c = cos(\u03b8) and s = sin(\u03b8) are at the intersections of the i-th and j-th rows and columns. The nonzero elements are consequently: \u2200k 6= i, j, gk,k = 1, gi,i = gj,j = c, gj,i = \u2212s and gi,j = s for i > j.\nThe computation of R follows the iterative Jacobi eigenvalue algorithm known as diagonalization process (Golub and van der Vorst [2000]):\nCovV \u2190 G(i, j, \u03b8t) CovV G(i, j, \u03b8t)T (1) R\u2190 R G(i, j, \u03b8)T . (2)\nNote that left (resp. right) multiplication by G(i, j, \u03b8) only mixes i-th and j-th rows (resp. columns). The update from equation 1 only modifies i-th and j-th rows and columns of CovV . The two updated diagonal coefficients (i, i) and (j, j) only depend on CovVi,j , CovVj,j , CovVj,i and \u03b8 which reduces the optimization of \u03b8 to a 2-dimensional problem, a classical trick when using Givens rotations (Golub and van der Vorst [2000]). Let us define: a def = CovVj,j , d def = CovVi,i, b\ndef = CovVj,i = CovVi,j and:(\na\u2032 b\u2032 b\u2032 d\u2032\n) def = ( c \u2212s s c )( a b b d )( c s \u2212s c ) , (3)\nthen we have the following result:\nTheorem 2. If min(a, d) \u2264 \u03c4 \u2264 max(a, d) (sufficient condition)3 then there exists one \u03b8 \u2208 [\u2212\u03c0/2;\u03c0/2] such that a\u2032 = \u03c4 , d\u2032 = a + d \u2212 \u03c4 and b\u2032 = \u2212s2 \u221a( a\u2212d 2 )2 + b2 with cos(\u03b8) = \u221a 1+c1c2\u2212s1s2 2 and sin(\u03b8) =\n3This theorem uses only a sufficient condition. A weaker necessary and sufficient condition to guarantee |c2| \u2264 1 and s2 \u2208 R+ is a+d2 \u2212 \u221a( a\u2212d 2 )2 + b2 \u2264 \u03c4 \u2264 a+d 2 + \u221a( a\u2212d 2 )2 + b2.\n\u2212 c1s2+c2s12 cos \u03b8 , c1 = ( a\u2212d 2 ) / \u221a( a\u2212d 2 )2 + b2, s1 = b/ \u221a( a\u2212d 2 )2 + b2, c2 = (\u03c4 \u2212 a+d2 )/ \u221a( a\u2212d 2 )2 + b2 and s2 =\u221a\n1\u2212 c22 \u2208 R+.\nProof. See the proof in appendix.\nNote that there is no need to compute explicitly \u03b8, \u03b81 or \u03b82."}, {"heading": "3.2.2 Description of the diagonal uniformization algorithm", "text": "The mean of CovV diagonal coefficients being equal to \u03c4 , the following indices sets are not empty:\nDefinition 3.2. Let iInf def = { l \u2208 {1, ..., c} | CovVl,l < \u03c4} and iSup def = { l \u2208 {1, ..., c} | CovVl,l > \u03c4}.\nTaking one index j from iInf and the other one i from iSup guarantees the condition of Theorem 2, which allows to set CovVj,j to the value \u03c4 . The index j can then be removed from iInf and as CovVi,i is set to a + d \u2212 \u03c4 , the index i reassigned to iInf if \u03c4 > a+d2 or in iSup if \u03c4 < a+d 2 . The number of diagonal coefficients of CovV different from \u03c4 has been decreased by one. Finally, the necessary number of iterations to completely empty iInf and iSup, i.e. uniformizing CovV diagonal, is bounded by c\u2212 1. The method is summarized in algorithm 1 where pop(list) and add(list, e) are subroutines to delete and return the first element of list, resp. to add e in list.\nAlgorithm 1 Diagonal Uniformization algorithm (UnifDiag)\n1: Inputs : the c\u00d7 c symmetric matrix to uniformize : CovV , tolerance: tol 2: // Initialization: 3: R\u2190 Ic // Identity matrix with dimension c 4: \u03c4 \u2190 Tr(CovV )/c ; nIter = 0 5: iInf = { l \u2208 {1, ..., c} | CovVl,l < \u03c4 \u2212 tol} 6: iSup = { l \u2208 {1, ..., c} | CovVl,l > \u03c4 + tol} 7: while nIter < c\u2212 1 & not isEmpty(iInf) & not isEmpty(iSup) do 8: // Givens rotation parameters computation: 9: j \u2190 pop(iInf); i\u2190 pop(iSup); a\u2190 CovV [j, j]; b\u2190 CovV [i, j]; d\u2190 CovV [i, i];\n10: c, s determined in Theorem 2 11: // CovV update: 12: rowj \u2190 CovV [j, :]; rowi \u2190 CovV [i, :] 13: CovV [j, :] = c\u00d7 rowj \u2212 s\u00d7 rowi; CovV [i, :] = s\u00d7 rowj + c\u00d7 rowi 14: CovV [:, j] = CovV [j, :]; CovV [:, i] = CovV [i, :] 15: CovV [j, j] = a\u2032; CovV [i, i] = d\u2032 ; CovV [j, i] = b\u2032 determined in Theorem 2 16: // Rotation update: 17: colj \u2190 R[:, j]; coli \u2190 R[:, i] 18: R[:, j] = c\u00d7 colj \u2212 s\u00d7 coli; R[:, i] = s\u00d7 colj + c\u00d7 coli 19: // Indices list update: 20: if a+d2 < \u03c4 \u2212 tol then 21: add(iInf, i) 22: end if 23: if a+d2 > \u03c4 + tol then 24: add(iSup, i) 25: end if 26: end while 27: return R"}, {"heading": "3.2.3 Relation to existing works regarding complexities", "text": "Our algorithm for computing R requires the storage of only the c\u00d7 c CovV matrix. At most c\u2212 1 Givens rotations are computed, each implying four column or row multiplications i.e. 4c flops. So the final time complexity is O(c2).\nSpatial complexity ITQ requires the storage of two c\u00d7 n matrices: the after-PCA projected dataset V and the corresponding binary encoding matrix B. As ITQ is dependent on the number of training data, it scales poorly w.r.t the size of the datasets and is of limited use for larger-than-memory datasets.\nTime complexity Learning R with our model is computationally cheaper than taking a random rotation as it implies a QR decomposition with a cost of O(c3).\nThis is also the case for ITQ where for a certain number of iterations, the c\u00d7 c product BTV should be computed (O(c2n)), followed by an SVD (O(c3)) corresponding to a final cost: O(Kc2n+Kc3) where K is the number of iterations and set in practice to 50. Moreover, one can remark that in the proposed model, the computation time for R does not depend on the size of the training set which makes it scalable.\nIn IsoHash (Kong and Li [2012]), authors have already formulated the problem as a variance-uniformization one. Two algorithms were proposed: Lift and projection and Gradient Flow, both looking for an orthogonal matrix R corresponding to a matrix with a uniform diagonal and the same eigenvalues as CovV . Lift and projection (LP) consists in alternative projections on the space of matrices with uniform diagonal and the space of matrices with the same eigenvalues as CovV . This costs niter SVD of a c \u00d7 c matrix. So the corresponding time complexity is O(c3 \u00d7 niter) with niter in practice set to 100. Gradient Flow (GF) involves an integration of a differential equation using Adams-Bashforth-Moulton PECE solver, with a cost equal to O(c3). So LP is very slow while GF barely faster. In both cases, even if c is small in comparison to d and the complexities do not either depend on n, our model has the great advantages 1) to have a lower time complexity cost, 2) to be much more simpler than IsoHash."}, {"heading": "4 The proposed streaming unsupervised model for binary quan-", "text": "tization\nIn the streaming context, the goal is to have bt = sign(W\u0303txt) for t = 1...n where bt is computed and returned before xt+1 is seen by using xt and previous update of W\u0303t\u22121. W\u0303t = RtWt where Wt is the linear dimension reduction embedding and Rt is a suitable c\u00d7 c rotation.\nOnline update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c\n2) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix.\nOnline update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting.\nRemark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al. [2000])."}, {"heading": "4.1 Discussion about the number of passes over data", "text": "A crucial point is to determine at which stage data should be projected and rotated. Should we perform a first pass over the data in order to work on a reliable Wt?\nOne pass over the data will give poorer performance relatively to the other approaches with two or three passes since data are directly projected onto Wt which is not properly estimated during the first iterations. Consequently, Rt is also learned from an approximate estimation of projected data covariance at step t: CovVt.\nIf three passes are possible over the data, results will be optimal and far more better than only one pass. One pass can be used for obtaining W and a second for CovV . Then, R can be correctly computed and a third pass is used for getting the final binary embedding: xt is projected onto RtWt and the function sign is applied. This is still a streaming algorithm which reduces the memory storage requirements in comparison to ITQ and there is no loss of accuracy.\nAlternatively, two passes can be done: one for W and CovV and a second for the encoding -rotation and sign applications- leading to results with intermediate quality in comparison with the one and three passes approaches."}, {"heading": "4.2 Complexity analysis", "text": "We compare here the spatial and time complexities of our method and OSH (Leng et al. [2015a]) which is to the best of our knowledge, the only online hashing method the most similar to ours, i.e. unsupervised, hyperplanes-based and reading one data point at a time. Despite what is announced, OSH is fundamentally mini-batch: the stream is divided into chunks of data for which a matrix S \u2208 Rd\u00d7l as a sketch of the whole dataset X \u2208 Rd\u00d7n is maintained. Then the principal components are computed from the updated sketch S. The projection of data followed by the random rotation can be applied only after this step. Therefore there are actually two passes over the data by reading twice data of each chunk.\nSpatial complexity Without counting the projection matrix and the rotation, OSH needs spatially to maintaining the sketch S which costs O(d\u00d7 l) with l d. The SVD decomposition then needs O(ld+ l2). In comparison, we only need O(c2) with c l.\nTime complexity For each round, OSH takes O(dl2 + l3) to learn the principal components. If we consider only one pass over the data, as new data is seen, our method updates with OPAST the estimation of the principal subspace and the covariance of the projected data CovV with only a cost of 4dc+O(c2). The complexity to compute the rotation is O(c2).\nOur method shows clearly advantages in terms of spatial and time complexities. Moreover, binary hash codes can be directly computed as new data is seen, while OSH, as a mini-batch method, has a delay."}, {"heading": "5 Experiments", "text": "All experiments were made with the CIFAR-10 dataset4 which contains 60000 32\u00d732 color images equally divided in 10 classes. 960-dimensional GIST descriptors were extracted from those data."}, {"heading": "5.1 Nearest neighbor search task - Comparison to offline methods", "text": "We evaluate the performance of nearest neighbor search using Euclidean neighbors as a ground truth. A nominal threshold of the average distance to the 50th nearest neighbor is computed and determines the sets of neighbors and non-neighbors called Euclidean ground truth. Based on that, the precision-recall curve is plotted for different hashing methods with the following setting: 1000 queries were randomly\n4http://www.cs.toronto.edu/~kriz/cifar.html\nsampled and the remaining data is used as training set. All the corresponding experiments are averaged over 5 random training/test partitions.\nWe compared our method to four baseline methods that follow the basic hashing scheme H(X) = sgn(W\u0303X), where the projection matrix W\u0303 \u2208 Rc\u00d7d is determined according to the chosen method:\n\u2022 LSH (Andoni and Indyk [2008]): W\u0303 is a Gaussian random matrix.\n\u2022 basic-PCA: W\u0303 whose rows are the c principal components, supposed to perform badly compared to the next method with a random rotation.\n\u2022 RandRot-PCA: W\u0303 = RW where W is the PCA matrix and R a random orthogonal (rotation) matrix. This is the initialization of ITQ.\n\u2022 RotUnifDiagOffline-PCA: W\u0303 = RW where W is the PCA matrix and R the rotation obtained with UnifDiag.\n\u2022 ITQ: as described in part 3.\n\u2022 RandRot-OPAST: W is the PCA matrix obtained through one pass with OPAST and R a random rotation. (2 passes)\n\u2022 RotUnifDiagOffline-OPAST: W is OPAST-principal subspace (1 pass) and R is obtained with UnifDiag through another pass. (3 passes)\n\u2022 RotUnifDiagOnline-OPAST: W is OPAST-principal subspace (1 pass) and R is computed during the same pass with UnifDiag. (2 passes)\nFigure 2 shows the results for CIFAR-10 dataset, varying the length of the code. Because our algorithm cannot produce codes with a length higher than the original data dimension (d = 960), code sizes are evaluated up to 512 bits. For space constraints, results for c = 256 and c = 512 are presented in appendix. The bigger the area under the recall-precision curve, the better is the method. As expected, basic-PCA performs poorly for each c. Then, the random projection holds the second worst results for c = 32 and c = 64 while other methods have similar performance. We remark though, that ITQ and RotUnifDiagOffline-OPAST (our online algorithm with 3 passes) give the best results for all code sizes. As the code length gets higher, they are more closely followed by randRot-PCA, randRot-OPAST (2 passes) and RotUnifDiagOffline-PCA (our offline model). In particular, random projection does not give the second worst results from c = 128 and behaves similarly to the other methods, except RotUnifDiagOfflinePCA which is outperformed from c = 128.\n0 20 40 60 80 100 Rounds\n0.28\n0.29\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\nM e a n A\nv e ra\ng e P\nre ci\nsi o n\nrandom_projection RotUnifDiag-OPAST-1pass OSH randRot-OPAST\n(a) c = 32\n0 20 40 60 80 100 Rounds\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\n0.38\nM e a n A\nv e ra\ng e P\nre ci\nsi o n\nrandom_projection RotUnifDiag-OPAST-1pass OSH randRot-OPAST\n(b) c = 64\nFigure 3: Mean Average Precision on CIFAR-10 dataset.\n0 200 400 600 800 1000\nData\n0.28\n0.29\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\nM e a n A\nv e ra\ng e P\nre ci\nsi o n\nRotUnifDiag-OPAST-1pass\nFigure 4: MAP evolution of our model on the first 1000 points of CIFAR-10 training set for c = 32."}, {"heading": "5.2 Nearest neighbor search task - Comparison to OSH", "text": "In this section, we evaluate our online algorithm in comparison with Online Sketching Hashing (OSH) (Leng et al. [2015a]), the nearest state-of-the-art approach. The training dataset is equally divided into 100 chunks (100 rounds) in order to perform a fair comparison since OSH is mini-batch and therefore performs two passes over the data: one to feed the chunk, an another one to perform the projection on Wt (computed with an SVD on the sketched version of the data chunk) followed by the random rotation application. Figure 3 shows the Mean Average Precision (MAP) on CIFAR-10 dataset with 100 rounds for different value of c < l = 200 averaged on 5 experiments (c = 16 and c = 128 in appendix). Our algorithm outperforms OSH for c \u2208 {16, 32, 64}. Finally, for c = 32, figure 4 illustrates the convergence property of our online algorithm by plotting the MAP evolution (averaged on 5 experiments) as new data is seen. The results are shown for the first 1000 data points of the training set: we do not need more than 400 points to reach the precision of the method."}, {"heading": "6 Conclusion", "text": "In this paper we introduced a novel method for learning distance-preserving binary embeddings of highdimensional data with convergence guarantees. Unlike classical state-of-the-art methods, our algorithm is straightforwardly adaptable to the streaming context where the whole dataset cannot be stored entirely. In the streaming context, our online model enables to obtain without delay a binary code as a new data point is seen. Our approach shows promising results for both offline and online settings, as evidenced by the experimental part. It can achieve accuracy at least similar to if not better than state-of-theart methods while saving considerable computation time and spatial requirements. Further work would be to investigate whether another rotation, not uniformizing the diagonal of the covariance matrix of the projected data, could be more optimal. Another interesting perspective would be to evaluate the performance of the compact binary codes in other machine learning applications: instead of using the original data, one could use directly these binary embeddings to perform unsupervised or supervised learning hopefully without downgrading the accuracy."}, {"heading": "7 Appendix", "text": ""}, {"heading": "7.1 ITQ algorithm", "text": "This section presents ITQ pseudo-code in algorithm 2. randomOrthogonalMatrix(c, c) denotes a subroutine returning a c \u00d7 c random orthogonal matrix which simply generates a c \u00d7 c random matrix and then applies a QR decomposition with time complexity O(c3).\nAlgorithm 2 ITQ algorithm Gong et al. [2013]\n1: Inputs : data : V = WX, V \u2208 Rc\u00d7n PCA-projected X; code length: c; number of iterations: K 2: R\u2190 randomOrthogonalMatrix(c, c) // Initialization 3: for i in 1...K do 4: B = sign(RV ) // Fix R and update B 5: S,\u2126, S\u0303T = SV D(V TB) // Fix B and update R 6: R = S\u0303ST 7: end for"}, {"heading": "7.2 Proof of theorem 2", "text": "In this section Theorem 2 is proven. Theorem 2 If min(a, d) \u2264 \u03c4 \u2264 max(a, d) (sufficient condition5) then there exists one \u03b8 \u2208 [pi/2;\u03c0/2]\nsuch that a\u2032 = \u03c4 with cos(\u03b8) = \u221a\n1+c1c2\u2212s1s2 2 and sin(\u03b8) = \u2212 c1s2+c2s1 2 cos \u03b8 , c1 = ( a\u2212d 2 ) / \u221a( a\u2212d 2 )2 + b2,\ns1 = b/ \u221a( a\u2212d 2 )2 + b2, c2 = (\u03c4 \u2212 a+d2 )/ \u221a( a\u2212d 2 )2 + b2 and s2 = \u221a 1\u2212 c22 \u2208 R+.\nProof. As stated in equation 3, the problem of finding a suitable angle \u03b8 to uniformize the diagonal of CovV can be reduced to the following 2-dimensional problem:(\na\u2032 b\u2032 b\u2032 d\u2032\n) def = ( c \u2212s s c )( a b b d )( c s \u2212s c ) (4)\nwhere a def = CovVj,j , d def = CovVi,i, b def = CovVj,i = CovVi,j . This implies :\na\u2032 = a+ d 2 + ( a\u2212d 2 b ) . ( cos(2\u03b8) \u2212 sin(2\u03b8) ) (5)\nd\u2032 = a+ d 2 \u2212 ( a\u2212d 2 b ) . ( cos(2\u03b8) \u2212 sin(2\u03b8) ) (6)\nb\u2032 = ( a\u2212d 2 b ) . ( sin(2\u03b8) cos(2\u03b8) ) (7)\nAs, the Givens angle \u03b8 should be parameterized so that the diagonal coefficients are set to a same value \u03c4 , the following holds: (\na\u2212d 2 b\n) . ( cos(2\u03b8) \u2212 sin(2\u03b8) ) = \u03c4 \u2212 a+ d 2 (8)\n5This theorem uses only a sufficient condition. A weaker necessary and sufficient condition to guarantee |c2| < 1 and s2 \u2208 R+ is a+d2 \u2212 \u221a( a\u2212d 2 )2 + b2 \u2264 \u03c4 \u2264 a+d 2 + \u221a( a\u2212d 2 )2 + b2 (proof by setting b = 0).\nRecall that:\nc1 def = cos(\u03b81) =\n( a\u2212 d\n2\n) / \u221a( a\u2212 d\n2\n)2 + b2 (9)\ns1 def = sin(\u03b81) = b/\n\u221a( a\u2212 d\n2\n)2 + b2 (10)\nc2 def = cos(\u03b82) = ( \u03c4 \u2212 a+ d\n2\n) / \u221a( a\u2212 d\n2\n)2 + b2 (11)\ns2 def = sin(\u03b82) = \u221a 1\u2212 c22. (12)\nRemark that the condition min(a, d) < \u03c4 < max(a, d) guarantees c2 to be well defined i.e. |c2| \u2264 1 and s2 \u2208 R+. Then, equation 8 becomes: (\nc1 s1 )( cos(2\u03b8) \u2212 sin(2\u03b8) ) = c2. (13)\nThis is clear that a solution of equation 13 is:( cos(2\u03b8) \u2212 sin(2\u03b8) ) = ( c1c2 \u2212 s1s2 c1s2 + c2s1 ) . (14)\nIn that case, one can take:\ncos(\u03b8) =\n\u221a 1 + cos(2\u03b8)\n2 =\n\u221a 1 + c1c2 \u2212 s1s2\n2 (15)\nsin(\u03b8) = sin(2\u03b8) 2 cos(\u03b8) = \u2212c1s2 + c2s1 2 cos \u03b8 (16)\nand the corresponding Givens rotation gives:\na\u2032 = \u03c4 (17) d\u2032 = a+ d\u2212 \u03c4 (18)\nb\u2032 = ( a\u2212d 2 b )(sin(2\u03b8) cos(2\u03b8) ) (19)\n=\n\u221a( a\u2212 d\n2\n)2 + b2 ( c1 s1 )(\u2212c1s2 \u2212 c2s1 c1c2 \u2212 s1s2 ) (20)\n= \u2212s2\n\u221a( a\u2212 d\n2\n)2 + b2 (21)\nwith s2 completely defined by equations 12 and 11."}, {"heading": "7.3 OPAST algorithm", "text": "The pseudo-code of our modified OPAST algorithm 3 tracking online the principal subspace and the covariance matrix CovV of projected data onto this principal subspace is given here."}, {"heading": "7.4 Further experiments", "text": "Experiments have been carried out on a single processor machine (Intel Core i7-5600U CPU @ 2.60GHz, 4 hyper-threads) with 16GB RAM. In this section, we present other experimental results.\nAlgorithm 3 Modified OPAST algorithm\nc 32 64 128 256 512\nITQ 1.8 3.6 9.3 25.8 87.6 UnifDiag 3\u00d7 10\u22123 5\u00d7 10\u22123 1\u00d7 10\u22122 4\u00d7 10\u22122 0.14"}, {"heading": "7.5 Speedups with rotation computation in comparison with ITQ in offline setting", "text": "Table 1 shows the speedup results regarding the rotation time computation with our method UnifDiag based on uniformizing the diagonal of CovV (tol = 10\u221214) instead of ITQ (number of iterations set to K = 50). This empirically confirms that our method for computing R is faster than ITQ (see Section 3.2.3 for exact theoretical time complexities) and is a very interesting alternative to ITQ in the offline setting."}, {"heading": "7.5.1 Nearest neighbor search task in the offline setting", "text": "Figure 5 shows the complete results with CIFAR-10 dataset for the precision-recall curves, varying the length of the code from c = 32 to c = 512. Table 2 contains the corresponding Mean Average Precision values. Both emphasize the competitive results of our method in comparison with ITQ."}, {"heading": "7.5.2 Nearest neighbor search task in the online setting", "text": "Figure 6 shows the complete online MAP results on CIFAR-10 dataset from c = 32 to c = 128."}], "references": [{"title": "Fast orthonormal past algorithm", "author": ["K. Abed-Meraim", "A. Chkeif", "Y. Hua"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Abed.Meraim et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Abed.Meraim et al\\.", "year": 2000}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM,", "citeRegEx": "Andoni and Indyk.,? \\Q2008\\E", "shortCiteRegEx": "Andoni and Indyk.", "year": 2008}, {"title": "Practical and optimal LSH for angular distance", "author": ["A. Andoni", "P. Indyk", "T. Laarhoven", "I. Razenshteyn", "L. Schmidt"], "venue": "In NIPS,", "citeRegEx": "Andoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andoni et al\\.", "year": 2015}, {"title": "Structured adaptive and random spinners for fast machine learning computations", "author": ["M. Bojarski", "A. Choromanska", "K. Choromanski", "F. Fagan", "C. Gouy-Pailler", "A. Morvan", "N. Sakr", "T. Sarlos", "J. Atif"], "venue": "In AISTATS,", "citeRegEx": "Bojarski et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2017}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Online robust pca via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "In NIPS,", "citeRegEx": "Feng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2013}, {"title": "Eigenvalue computation in the 20th century", "author": ["G.H. Golub", "H.A. van der Vorst"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Golub and Vorst.,? \\Q2000\\E", "shortCiteRegEx": "Golub and Vorst.", "year": 2000}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "In CVPR,", "citeRegEx": "Gong and Lazebnik.,? \\Q2011\\E", "shortCiteRegEx": "Gong and Lazebnik.", "year": 2011}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Kernelized locality-sensitive hashing", "author": ["K. Grauman", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Grauman and Kulis.,? \\Q2011\\E", "shortCiteRegEx": "Grauman and Kulis.", "year": 2011}, {"title": "Online hashing", "author": ["L.-K. Huang", "Q. Yang", "W.-S. Zheng"], "venue": "In IJCAI, pages 1422\u20131428,", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Aggregating local descriptors into a compact image representation", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid", "P. P\u00e9rez"], "venue": "In CVPR,", "citeRegEx": "J\u00e9gou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "J\u00e9gou et al\\.", "year": 2010}, {"title": "Isotropic hashing", "author": ["W. Kong", "W.-j. Li"], "venue": "In NIPS, pages 1646\u20131654", "citeRegEx": "Kong and Li.,? \\Q2012\\E", "shortCiteRegEx": "Kong and Li.", "year": 2012}, {"title": "Simultaneous feature learning and hash coding with deep neural networks", "author": ["H. Lai", "Y. Pan", "Y. Liu", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Spherical hashing", "author": ["Y. Lee"], "venue": "In CVPR, pages 2957\u20132964,", "citeRegEx": "Lee.,? \\Q2012\\E", "shortCiteRegEx": "Lee.", "year": 2012}, {"title": "Online sketching hashing", "author": ["C. Leng", "J. Wu", "J. Cheng", "X. Bai", "H. Lu"], "venue": "In CVPR,", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Hashing for distributed data", "author": ["C. Leng", "J. Wu", "J. Cheng", "X. Zhang", "H. Lu"], "venue": "In ICML,", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. fu Chang"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "In CVPR, pages 2074\u20132081,", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "In NIPS,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "In NIPS,", "citeRegEx": "Raginsky and Lazebnik.,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik.", "year": 2009}, {"title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "author": ["R. Raziperchikolaei", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "In NIPS,", "citeRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.,? \\Q2016\\E", "shortCiteRegEx": "Raziperchikolaei and Carreira.Perpi\u00f1\u00e1n.", "year": 2016}, {"title": "Spherical LSH for approximate nearest neighbor search on unit hypersphere", "author": ["K. Terasawa", "Y. Tanaka"], "venue": "In WADS,", "citeRegEx": "Terasawa and Tanaka.,? \\Q2007\\E", "shortCiteRegEx": "Terasawa and Tanaka.", "year": 2007}, {"title": "Semi-supervised hashing for large-scale search", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Learning to hash for indexing big data - a survey", "author": ["J. Wang", "W. Liu", "S. Kumar", "S.-F. Chang"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "In NIPS, pages 1753\u20131760,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Streaming sparse principal component analysis", "author": ["W. Yang", "H. Xu"], "venue": "In ICML, pages 494\u2013503,", "citeRegEx": "Yang and Xu.,? \\Q2015\\E", "shortCiteRegEx": "Yang and Xu.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["F. Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "In ICML,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.", "startOffset": 56, "endOffset": 80}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al.", "startOffset": 56, "endOffset": 450}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise.", "startOffset": 56, "endOffset": 598}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al.", "startOffset": 56, "endOffset": 846}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms.", "startOffset": 56, "endOffset": 870}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms. In order to increase the accuracy of the data sketches in the context of nearest neighbors search or classification, this projection can be also learned from data (Weiss et al. [2008], Wang et al.", "startOffset": 56, "endOffset": 1154}, {"referenceID": 1, "context": "A classical method is Locality-Sensitive Hashing (LSH) (Andoni and Indyk [2008]) for nearest neighbors search: 1) input data with high dimension d are projected onto a lower c-dimensional space through a c\u00d7d random projection with i.i.d. Gaussian entries, 2) then a hashing function is applied to the resulting projected vector to obtain the final binary codes. Two examples for the hashing function are crosspolytope LSH (Terasawa and Tanaka [2007]) which returns the closest vector from the set {\u00b11ei}1\u2264i\u2264c where {ei}1\u2264i\u2264c stands for the canonical basis, and hyperplane LSH (Andoni et al. [2015]) corresponding to the sign function applied pointwise. For reducing the storage cost of the projection and the computation time of the matrix-vector products (O(c\u00d7d)), a pseudo-random matrix with structure can be used instead (Andoni et al. [2015], Bojarski et al. [2017]) leading to a reduced time complexity of O(d log c) thanks to fast Hadamard and Fourier transforms. In order to increase the accuracy of the data sketches in the context of nearest neighbors search or classification, this projection can be also learned from data (Weiss et al. [2008], Wang et al. [2012], Gong \u2217To whom correspondence should be adressed: anne.", "startOffset": 56, "endOffset": 1174}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al.", "startOffset": 21, "endOffset": 60}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al. [2014]).", "startOffset": 21, "endOffset": 78}, {"referenceID": 7, "context": "and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Yu et al. [2014]). As Principal Component Analysis (PCA) is a common tool for reducing data dimensionality, PCA is often performed: data are then projected onto the first c principal components. But the PCA alone is not sufficient. Indeed, the c first principal components are chosen with a decreasing order of explained variance. It means that principal directions with higher variance carry more information. Consequently, associating each of the c directions to one of the c bits is equivalent to giving more weights to less informative directions and will lead to poor performance of the obtained sketches. To remedy this problem, after data have been projected on the first principal components of the covariance matrix, a solution consists in applying a suitable rotation on the projected data before performing the hashing function, in order to balance variance over the principal components. In work from J\u00e9gou et al. [2010], a random rotation is successfully applied giving quite good results.", "startOffset": 21, "endOffset": 994}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al. [2013]) or in Isotropic Hashing (IsoHash) (Kong and Li [2012]), the rotation is rather learned.", "startOffset": 33, "endOffset": 78}, {"referenceID": 7, "context": "In ITerative Quantization (ITQ) (Gong and Lazebnik [2011], Gong et al. [2013]) or in Isotropic Hashing (IsoHash) (Kong and Li [2012]), the rotation is rather learned.", "startOffset": 33, "endOffset": 133}, {"referenceID": 15, "context": "In any case, for only one pass over the data, our online algorithm gives better results than OSH (Leng et al. [2015a]) while being far less computationally demanding.", "startOffset": 98, "endOffset": 118}, {"referenceID": 7, "context": "Two paradigms exist for constructing hash functions (Wang et al. [2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 53, "endOffset": 72}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 51}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 81}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods.", "startOffset": 27, "endOffset": 107}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al.", "startOffset": 27, "endOffset": 267}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al.", "startOffset": 27, "endOffset": 286}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al.", "startOffset": 27, "endOffset": 312}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 332}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 352}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al.", "startOffset": 27, "endOffset": 364}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al.", "startOffset": 27, "endOffset": 383}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al.", "startOffset": 27, "endOffset": 401}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al.", "startOffset": 27, "endOffset": 448}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al.", "startOffset": 27, "endOffset": 589}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]).", "startOffset": 27, "endOffset": 608}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al.", "startOffset": 27, "endOffset": 681}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]).", "startOffset": 27, "endOffset": 701}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al.", "startOffset": 27, "endOffset": 792}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al.", "startOffset": 27, "endOffset": 844}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed.", "startOffset": 27, "endOffset": 865}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed. Online Hashing (OKH) (Huang et al. [2013]) learns the hash functions from a stream of pair of data with a \u201dPassive-Aggressive\u201d method.", "startOffset": 27, "endOffset": 929}, {"referenceID": 1, "context": "[2016]): data-independent (Andoni and Indyk [2008], Raginsky and Lazebnik [2009], Grauman and Kulis [2011]) and data dependent methods. The latter ones learn the hash codes from a training set and perform better. The learning can be unsupervised (Weiss et al. [2008], Liu et al. [2011], Gong and Lazebnik [2011], Gong et al. [2013], Kong and Li [2012], Lee [2012], Liu et al. [2014], Yu et al. [2014], Raziperchikolaei and Carreira-Perpi\u00f1\u00e1n [2016]) aiming at preserving distances in the original space or (semi-)supervised which also tries to preserve label similarity (Wang et al. [2012], Liu et al. [2012]). Some recent hashing functions involve deep learning (Lai et al. [2015], Chen et al. [2015]). When the dataset is too large to be loaded into memory, distributed (Leng et al. [2015b]) and online hashing techniques (Huang et al. [2013], Leng et al. [2015a]) have been developed. Online Hashing (OKH) (Huang et al. [2013]) learns the hash functions from a stream of pair of data with a \u201dPassive-Aggressive\u201d method. In Online Sketching Hashing (OSH) (Leng et al. [2015a]), the binary embeddings are learned from a maintained sketch of the dataset with a smaller size but preserving the property of interest.", "startOffset": 27, "endOffset": 1077}, {"referenceID": 8, "context": "For instance, W can be the matrix whose row vectors w k correspond to the c first principal components of the covariance matrix C = XX T (an other supervised approach is to perform Canonical Correlation Analysis (Gong et al. [2013])).", "startOffset": 213, "endOffset": 232}, {"referenceID": 12, "context": "So, similarly to IsoHash from work in (Kong and Li [2012]), we formulate the problem of finding R as the problem of equalizing the diagonal coefficients of CovV to the value \u03c4 = Tr(CovV )/c.", "startOffset": 39, "endOffset": 58}, {"referenceID": 12, "context": "In IsoHash (Kong and Li [2012]), authors have already formulated the problem as a variance-uniformization one.", "startOffset": 12, "endOffset": 31}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W .", "startOffset": 112, "endOffset": 138}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al.", "startOffset": 112, "endOffset": 1051}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al.", "startOffset": 112, "endOffset": 1071}, {"referenceID": 0, "context": "Online update of the principal subspace Fast Orthonormal PAST (Projection Approximation and Subspace Tracking) (Abed-Meraim et al. [2000]), also named OPAST, is a method to quickly estimate and track the principal subspace of a data stream, corresponding to matrix W . At each iteration t, OPAST guarantees the orthonormality of Wt rows and costs only 4dc + O(c ) flops while storing only W and a c\u00d7 c matrix. The pseudo-code is given by algorithm 3 in appendix. Online update of a suitable rotation The procedure to compute Rt relies exclusively on the c \u00d7 c covariance matrix of projected data CovVt and not on the whole V as for ITQ. CovVt is easy to update dynamically and this is performed while computing Wt with OPAST algorithm, so no need of adaptation for the Rt learning in the streaming setting. Remark also that this stage is completely independent from the first one to compute the principal subspace. One interest of UnifDiag algorithm for learning the rotation is the freedom to plug any other method for online PCA (Feng et al. [2013], Yang and Xu [2015]) or online subspace tracking( Abed-Meraim et al. [2000]).", "startOffset": 112, "endOffset": 1127}, {"referenceID": 15, "context": "2 Complexity analysis We compare here the spatial and time complexities of our method and OSH (Leng et al. [2015a]) which is to the best of our knowledge, the only online hashing method the most similar to ours, i.", "startOffset": 95, "endOffset": 115}, {"referenceID": 1, "context": "We compared our method to four baseline methods that follow the basic hashing scheme H(X) = sgn(W\u0303X), where the projection matrix W\u0303 \u2208 Rc\u00d7d is determined according to the chosen method: \u2022 LSH (Andoni and Indyk [2008]): W\u0303 is a Gaussian random matrix.", "startOffset": 193, "endOffset": 217}, {"referenceID": 15, "context": "2 Nearest neighbor search task - Comparison to OSH In this section, we evaluate our online algorithm in comparison with Online Sketching Hashing (OSH) (Leng et al. [2015a]), the nearest state-of-the-art approach.", "startOffset": 152, "endOffset": 172}], "year": 2017, "abstractText": "In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new method for computing binary compressed representations -sketchesof high-dimensional real feature vectors. Given an expected code length c and high-dimensional input data points, our algorithm provides a binary code of c bits aiming at preserving the distance between the points from the original high-dimensional space. Our offline version of the algorithm outperforms the offline state-of-the-art methods regarding their computation time complexity and have a similar quality of the sketches. It also provides convergence guarantees. Moreover, our algorithm can be straightforwardly used in the streaming context by not requiring neither the storage of the whole dataset nor a chunk. We demonstrate the quality of our binary sketches through extensive experiments on real data for the nearest neighbors search task in the offline and online settings.", "creator": "LaTeX with hyperref package"}}}