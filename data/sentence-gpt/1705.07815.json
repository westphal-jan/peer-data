{"id": "1705.07815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances", "abstract": "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples. This results in a generalized estimate of variance in the probability of errors for both the source and target domain distributions. In the context of the optimal probability of error for both the source and target domains, the optimal probability of errors for both the source and target domain distributions can be estimated from unlabeled samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 22 May 2017 15:50:47 GMT  (16kb)", "http://arxiv.org/abs/1705.07815v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jaeho lee", "maxim raginsky"], "accepted": false, "id": "1705.07815"}, "pdf": {"name": "1705.07815.pdf", "metadata": {"source": "CRF", "title": "Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances\u2217", "authors": ["Jaeho Lee", "Maxim Raginsky"], "emails": ["jlee620@illinois.edu", "maxim@illinois.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n07 81\n5v 1\n[ cs\n.L G\n] 2\n2 M\nay 2"}, {"heading": "1 Introduction and problem set-up", "text": "In the traditional paradigm of statistical learning [1], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z \u2192 R+. Each f \u2208 F quantifies the loss of some decision rule or a hypothesis applied to instances z \u2208 Z, so, with a slight abuse of terminology, we will refer to F as the hypothesis space. The (expected) risk of a hypothesis f on instances generated according to P is given by\nR(P, f) := EP [f(Z)] =\n\u222b\nZ f(z)P (dz).\nGiven an n-tuple Z1, . . . , Zn of i.i.d. training examples drawn from an unknown P \u2208 P, the objective is to find a hypothesis f\u0302 \u2208 F whose risk R(P, f\u0302) is close to the minimum risk\nR\u2217(P,F) := inf f\u2208F R(P, f) (1.1)\nwith high probability. Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:\nR(Pn, f) = 1\nn\nn\u2211\ni=1\nf(Zi) \u2212\u2192 min, f \u2208 F (1.2)\n\u2217This work was supported in part by the NSF grant nos. CIF-1527388 and CIF-1302438, and in part by the NSF\nCAREER award 1254041. \u2020jlee620@illinois.edu \u2021maxim@illinois.edu\nwhere Pn := 1 n \u2211n i=1 \u03b4Zi is the empirical distribution of the training examples.\nRecently, however, an alternative viewpoint has emerged, inspired by ideas from robust statistics and robust stochastic optimization. In this distributionally robust framework, instead of solving the ERM problem (1.2), one aims to solve the minimax problem\nsup Q\u2208A(Pn)\nR(Q, f) \u2212\u2192 min, f \u2208 F (1.3)\nwhere A(Pn) is an ambiguity set containing the empirical distribution Pn and, possibly, the unknown probability law P either with high probability or almost surely. The ambiguity sets serve as a mechanism for compensating for the uncertainty about P that inherently arises due to having only a finite sample to work with, and they can be constructed in a variety of ways, e.g., via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7]. However, with the exception of the recent work by Farnia and Tse [3], the minimizer of (1.3) is still evaluated under the standard statistical risk minimization paradigm. In this work, we instead propose replacing the statistical risk minimization criterion (1.1) with the local minimax risk\ninf f\u2208F sup Q\u2208A(P ) R(Q, f)\nat P , where the ambiguity set A(P ) is taken to be a Wasserstein ball centered at P . (Recently, this modification was also proposed by Farnia and Tse [3] for ambiguity sets defined by a finite number of moment constraints.) As we will argue below, this change of perspective is natural when there is a possibility of domain drift, i.e., when the learned hypothesis is evaluated on a distribution Q which may be different from the distribution P that was used to generate the training data."}, {"heading": "1.1 Wasserstein ambiguity sets and local minimax risk", "text": "We assume that the instance space Z is a Polish space (i.e., a complete separable metric space) with metric dZ . We denote by P(Z) the space of all Borel probability measures on Z, and by Pp(Z) with p \u2265 1 the space of all P \u2208 P(Z) with finite pth moments:\nPp(Z) := { P \u2208 P(Z) : EP [dpZ(Z, z0)] < \u221e for some z0 \u2208 Z } .\nThe metric structure of Z can be used to define a family of metrics on the spaces Pp(Z) [8]:\nDefinition 1.1. For p \u2265 1, the p-Wasserstein distance between P,Q \u2208 Pp(Z) is\nWp(P,Q) := inf M(\u00b7\u00d7Z)=P M(Z\u00d7\u00b7)=Q\n( EM [d p Z(Z,Z \u2032)] )1/p , (1.4)\nwhere the infimum is over all couplings of P and Q, i.e., probability measures M on the product space Z \u00d7 Z with the given marginals P and Q.\nRemark 1.1. Wasserstein metrics arise in the problem of optimal transport : for any coupling M of P and Q, the conditional distribution MZ\u2032|Z can be viewed as a randomized policy for \u2018transporting\u2019 a unit quantity of some material from a random location Z to another location Z \u2032, while satisfying the marginal constraint Z \u2032 \u223c Q. If the cost of transporting a unit of material from z \u2208 Z to z\u2032 \u2208 Z is given by dpZ(z, z \u2032), then W pp (P,Q) is the minimum expected transport cost.\nWe now consider a learning problem (P,F) with P = Pp(Z) for some p \u2265 1. Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P :\nA(P ) = BW\u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,\nwhere the radius \u03b5 > 0 is a tunable parameter. We then define the local worst-case risk of f at P ,\nR\u03b5,p(P, f) := sup Q\u2208BW\u03b5,p(P ) R(Q, f),\nand the local minimax risk at P :\nR\u2217\u03b5,p(P,F) := inf f\u2208F R\u03b5,p(P, f).\nSome inequalities relating the local worst-case risk R\u03b5,p(Q, f) and the statistical risk R(Q, f) are presented in Section 2.\nThe basic problem of interest can now be stated as follows: given an n-tuple Z1, . . . , Zn of i.i.d. training examples drawn from an unknown P \u2208 Pp(Z), find a hypothesis f\u0302 \u2208 F , such that\nR\u03b5,p(P, f\u0302) \u2248 R\u2217\u03b5,p(P,F) with high probability.\nIn Section 3, we show that, under some regularity assumptions, this goal can be achieved via a natural Empirical Risk Minimization (ERM) procedure. In particular, Theorem 3.1 provides a high-probability bound on the excess risk R\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P,F)."}, {"heading": "1.2 Motivating problem: domain adaptation", "text": "One of the attractive features of ambiguity sets based on Wasserstein distances is that, because of their intimate connection to the metric geometry of the instance space, they provide a natural mechanism for handling uncertainty due to (possibly randomized) transformations acting on the problem instances. To illustrate this point, we briefly discuss a motivating example of domain adaptation.\nIn contrast to the standard statistical learning framework, where the risk of the learned hypothesis is evaluated on the same unknown distribution that was used for generating the training examples, the problem of domain adaptation [9] arises when the training data are generated according to an unknown distribution P , but the learned hypothesis is evaluated on another unknown distribution Q. However, it is assumed that these distributions (commonly referred to as problem domains) are somehow related, and some partial information about Q is also available at training time. In the context of supervised learning, the instances are random couples Z = (X,Y ) consisting of features X and labels Y , and the training data consist of n labeled examples Z1 = (X1, Y1), . . . , Zn = (Xn, Yn) drawn from the source domain P and m unlabeled features X \u20321, . . . ,X \u2032 m drawn from the target domain Q. The goal is to learn a hypothesis that would perform well on the target domain Q. In a recent paper, Courty et al. [10] have introduced an algorithmic framework for domain adaptation based on optimal transport. Their approach revolves around a particular generative model for the drift between the source and the target domains. Let us disintegrate the source domain distribution as P = \u00b5\u2297 PY |X , where \u00b5 \u2208 P(X ) is the marginal distribution of the features and PY |X is the conditional distribution of the labels given the features. Then there exists an unknown deterministic transformation T : X \u2192 X of the feature space, such that a sample (X \u2032, Y \u2032)\nfrom the target domain distribution Q = \u03bd \u2297 QY |X can be generated using the following two-step procedure:\nX T\u2212\u2212\u2212\u2212\u2192 X \u2032 PY |X\u2212\u2212\u2212\u2212\u2212\u2192 Y \u2032,\nwhere the input X is drawn from \u00b5. In other words, the domain drift is due solely to an unknown deterministic transformation of the features. If we further assume that T is the optimal transport map from \u00b5 to \u03bd, i.e., that W pp (\u00b5, \u03bd) = E\u00b5[d p X (X,T (X))] under some metric dX on X , and if Wp(P,Q) = Wp(\u00b5, \u03bd), then it is natural to cast the problem of domain adaptation as that of learning a hypothesis f \u2208 F that would approximately achieve the local minimax risk R\u03b5,p(P,F), where \u03b5 is some estimate of Wp(\u00b5, \u03bd) from source and target training data. In Section 4, we present an algorithm based on this idea and provide a quantitative analysis of its performance. In particular, Theorem 4.1 gives a high-probability bound on the excess risk of the learned classifier with respect to the target domain distribution Q. We note that, in contrast to the original methodology of Courty et al. [10], our approach completely bypasses the problem of estimating the transport map T ."}, {"heading": "2 Local worst-case risk vs. statistical risk", "text": "In some situations (see, e.g., Section 4), it is of interest to convert back and forth between local worst-case (or local minimax) risks and the usual statistical risks. In this section, we give a couple of inequalities relating these quantities. The first one is a simple consequence of the Kantorovich duality theorem from the theory of optimal transport [8]:\nProposition 2.1. Suppose that f is L-Lipschitz, i.e., |f(z) \u2212 f(z\u2032)| \u2264 LdZ(z, z\u2032) for all z, z\u2032 \u2208 Z. Then, for any Q \u2208 BW\u03b5,p(P ),\nR(Q, f) \u2264 R\u03b5,p(P, f) \u2264 R(Q, f) + 2L\u03b5.\nAs an example, consider the problem of binary classification with hinge loss: Z = X \u00d7Y, where X is an arbitrary feature space, Y = {\u22121,+1}, and the hypothesis space F consists of all functions of the form f(z) = f(x, y) = max{0, 1\u2212 yf0(x)}, where f0 : X \u2192 R is a candidate predictor. Then, since the function u 7\u2192 max{0, 1 \u2212 u} is Lipschitz-continuous with constant 1, we can write\n|f(x, y)\u2212 f(x\u2032, y\u2032)| \u2264 |yf0(x)\u2212 y\u2032f0(x\u2032)| \u2264 2\u2016f0\u2016X1{y 6= y\u2032}+ |f0(x)\u2212 f0(x\u2032)|,\nwhere \u2016f0\u2016X := supx\u2208X |f0(x)|. If \u2016f0\u2016X < \u221e and if f0 is L0-Lipschitz with respect to some metric dX on X , then it follows that f is Lipschitz with constant max{2\u2016f0\u2016X , L0} with respect to the product metric\ndZ(z, z \u2032) = dZ((x, y), (x \u2032, y\u2032)) := dX (x, x \u2032) + 1{y 6= y\u2032}.\nNext we consider the case when the function f is smooth but not Lipschitz-continuous. Since we are working with general metric spaces that may lack an obvious differentiable structure, we need to first introduce some concepts from metric geometry [11]. A metric space (Z,dZ) is a geodesic space if for every two points z, z\u2032 \u2208 Z there exists a path \u03b3 : [0, 1] \u2192 Z, such that \u03b3(0) = z, \u03b3(1) = z\u2032, and dZ(\u03b3(s), \u03b3(t)) = (t\u2212 s) \u00b7 dZ(\u03b3(0), \u03b3(1)) for all 0 \u2264 s \u2264 t \u2264 1 (such a path is called a constant-speed\ngeodesic). A functional F : Z \u2192 R is geodesically convex if for any pair of points z, z\u2032 \u2208 Z there is a constant-speed geodesic \u03b3, so that\nF (\u03b3(t)) \u2264 (1\u2212 t)F (\u03b3(0)) + tF (\u03b3(1)) = (1\u2212 t)F (z) + tF (z\u2032), \u2200t \u2208 [0, 1].\nAn upper gradient of a Borel function f : Z \u2192 R is a functional Gf : Z \u2192 R+, such that for any pair of points z, z\u2032 \u2208 Z there exists a constant-speed geodesic \u03b3 obeying\n|f(z\u2032)\u2212 f(z)| \u2264 \u222b 1\n0 Gf (\u03b3(t))dt \u00b7 dZ(z, z\u2032). (2.1)\nWith these definitions at hand, we have the following:\nProposition 2.2. Suppose that f has a geodesically convex upper gradient Gf . Then\nR(Q, f) \u2264 R\u03b5,p(P, f) \u2264 R(Q, f) + 2\u03b5 sup Q\u2208BW\u03b5,p(P ) \u2016Gf (Z)\u2016Lq(Q),\nwhere 1/p + 1/q = 1, and \u2016 \u00b7 \u2016Lq(Q) := ( EQ| \u00b7 |q )1/q .\nAs a simple example, consider the setting of regression with quadratic loss: let X be a convex subset of Rd, let Y = [\u2212B,B] for some 0 < B < \u221e, and equip Z = X \u00d7 Y with the Euclidean metric\ndZ(z, z \u2032) = \u221a \u2016x\u2212 x\u2032\u201622 + |y \u2212 y\u2032|2, z = (x, y), z\u2032 = (x\u2032, y\u2032).\nSuppose that the functions f \u2208 F are of the form f(z) = f(x, y) = (y\u2212f0(x))2 with f0 \u2208 C1(Rd,R), such that \u2016f0\u2016X \u2264 M < \u221e and \u2016\u2207f0(x)\u20162 \u2264 L\u2016x\u20162 for some 0 < L < \u221e. Then Proposition 2.2 leads to the following:\nProposition 2.3.\nR(Q, f) \u2264 R\u03b5,2(P, f) \u2264 R(Q, f) + 4\u03b5(B +M) ( 1 + L sup\nQ\u2208BW\u03b5,2(P )\n\u03c3Q,X\n) ,\nwhere \u03c3Q,X := EQ\u2016X\u20162 for Z = (X,Y ) \u223c Q."}, {"heading": "3 Guarantees for empirical risk minimization", "text": "Let Z1, . . . , Zn be an n-tuple of i.i.d. training examples drawn from P . In this section, we analyze the performance of the ERM procedure\nf\u0302 := argmin f\u2208F\nR\u2217\u03b5,p(Pn, f). (3.1)\nThe following strong duality result due to Gao and Kleywegt [7] will be instrumental:\nProposition 3.1. For any upper semicontinuous function f : Z \u2192 R and for any Q \u2208 Pp(Z),\nR\u03b5,p(Q, f) = min \u03bb\u22650\n{ \u03bb\u03b5p +EQ[\u03d5\u03bb,f (Z)] } ,\nwhere \u03d5\u03bb,f (z) := supz\u2032\u2208Z { f(z\u2032)\u2212 \u03bb \u00b7 dpZ(z, z\u2032) } .\nWe begin by imposing some regularity assumptions:\nAssumption 3.1. The instance space Z is bounded: diam(Z) := supz,z\u2032\u2208Z dZ(z, z\u2032) < \u221e. Assumption 3.2. The functions in F are upper semicontinuous and uniformly bounded: 0 \u2264 f(z) \u2264 M < \u221e for all f \u2208 F and z \u2208 Z. Assumption 3.3. There exists a hypothesis f0 \u2208 F , such that, for all z \u2208 Z, f0(z) \u2264 C0dpZ(z, z0) for some C0 \u2265 0 and z0 \u2208 Z.\nWe can now give a performance guarantee for the ERM procedure (3.1):\nTheorem 3.1. If Assumptions 3.1\u20133.3 are satisfied, then, with probability at least 1\u2212 \u03b4,\nR\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P,F) \u2264 24\u221a n\n\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du\n+ 24C0(2 diam(Z))p\u221a\nn\n( 1 + ( diam(Z)\n\u03b5\n)p) + 3M \u221a log(2/\u03b4)\n2n , (3.2)\nwhere N (F , \u2016 \u00b7 \u2016\u221e, \u00b7) are the covering numbers of F with respect to the sup norm. Remark 3.1. The first term on the right-hand side of (3.1) is the Dudley entropy integral [12]. We conjecture that this term can be replaced by the expected Rademacher average of the hypothesis class F , but have been unable to prove it. Remark 3.2. The second term on the right-hand side of (3.1) increases as \u03b5 \u2192 0. The excess risk bound of Farnia and Tse [3] has the same behavior, where in that case \u03b5 is the slack in the moment constraints defining the ambiguity set. This (1/\u03b5)p scaling can be eliminated if more refined bounds on the optimum dual \u03bb are available.\nProof. Let f\u2217 \u2208 F be any achiever of the local minimax risk R\u2217\u03b5,p(P,F). We start by decomposing the excess risk:\nR\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P,F) = R\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P, f\u2217) \u2264 R\u03b5,p(P, f\u0302)\u2212R\u03b5,p(Pn, f\u0302) +R\u03b5,p(Pn, f\u2217)\u2212R\u03b5,p(P, f\u2217),\nwhere the last step follows from the definition of f\u0302 . Define\n\u03bb\u0302 := argmin \u03bb\u22650\n{ \u03bb\u03b5p +EPn [\u03d5\u03bb,f\u0302 (Z)] } , \u03bb\u2217 := argmin\n\u03bb\u22650\n{ \u03bb\u03b5p +EP [\u03d5\u03bb,f\u2217(Z)] } .\nThen, using Proposition 3.1, we can write\nR\u03b5,p(P, f\u0302)\u2212R\u03b5,p(Pn, f\u0302) = min \u03bb\u22650\n{ \u03bb\u03b5p + \u222b\nZ \u03d5 \u03bb,f\u0302 (z)P (dz)\n} \u2212 ( \u03bb\u0302\u03b5p + \u222b\nZ \u03d5 \u03bb\u0302,f\u0302 (z)Pn(dz)\n)\n\u2264 \u222b\nZ \u03d5 \u03bb\u0302,f\u0302\n(z)(P \u2212 Pn)(dz)\nand, following similar logic,\nR\u03b5,p(Pn, f \u2217)\u2212R\u03b5,p(P, f\u2217) \u2264\n\u222b\nZ \u03d5\u03bb\u2217,f\u2217(z)(Pn \u2212 P )(dz). (3.3)\nBy Lemma 3.1 given in Section 3.2, \u03bb\u0302 \u2208 \u039b := [0, C02p\u22121(1 + (diam(Z)/\u03b5)p)]. Hence, defining the function class \u03a6 := { \u03d5\u03bb,f : \u03bb \u2208 \u039b, f \u2208 F } , we have\nR\u03b5,p(P, f\u0302)\u2212R\u03b5,p(Pn, f\u0302) \u2264 sup \u03d5\u2208\u03a6\n[\u222b\nZ \u03d5d(P \u2212 Pn)\n] . (3.4)\nSince all f \u2208 F take values in [0,M ], the same holds for all \u03d5 \u2208 \u03a6. Therefore, by a standard symmetrization argument,\nR\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P, f\u0302) \u2264 2Rn(\u03a6) +M \u221a 2 log(2/\u03b4)\nn\nwith probability at least 1\u2212 \u03b4/2, where\nRn(\u03a6) := E  sup \u03d5\u2208\u03a6 1 n n\u2211\ni=1\n\u03c3i\u03d5(Zi)\n \nis the expected Rademacher average of \u03a6, with i.i.d. Rademacher random variables \u03c31, . . . , \u03c3n independent of Z1, . . . , Zn. Moreover, from (3.3) and from Hoeffding\u2019s inequality it follows that\nR\u03b5,p(Pn, f \u2217)\u2212R\u03b5,p(P, f\u2217) \u2264 M\n\u221a log(2/\u03b4)\n2n\nwith probability at least 1\u2212 \u03b4/2. Consequently,\nR\u03b5,p(P, f\u0302)\u2212R\u2217\u03b5,p(P,F) \u2264 2Rn(\u03a6) + 3M \u221a log(2/\u03b4)\n2n (3.5)\nwith probability at least 1\u2212 \u03b4. Using the bound of Lemma 3.2 from Section 3.2 in (3.5), we obtain the statement of the theorem."}, {"heading": "3.1 Example bounds", "text": "In this subsection, we illustrate the use of Theorem 3.1 when (upper bounds on) the covering numbers for the hypothesis class F are available. Throughout this section, we let Z = X \u00d7Y, where the feature space X = {x \u2208 Rd : \u2016x\u20162 \u2264 r0} is a ball of radius r0 in Rd with center at the origin, the label space Y \u2286 [\u2212B,+B] for some 0 < B < \u221e. We equip Z with the Euclidean metric\ndZ(z, z \u2032) = dZ((x, y), (x \u2032, y\u2032)) := \u221a \u2016x\u2212 x\u2032\u201622 + |y \u2212 y\u2032|2\nand take p = 2. We first consider a simple neural network class F consisting of functions of the form f(z) = f(x, y) = (y\u2212 s(fT0 x))2, where s : R \u2192 R is a bounded smooth nonlinearity with s(0) = 0 and with bounded first derivative, and where f0 takes values in the unit ball in R d.\nCorollary 3.1. For any P \u2208 P(Z), with probability at least 1\u2212 \u03b4,\nR\u03b5,2(P, f\u0302)\u2212R\u2217\u03b5,2(P,F)\n\u2264 C1\u221a n +\n384(r0 +B) 2\n\u221a n\n( 1 + 4(r20 +B 2)\n\u03b52\n) + 6(\u2016s\u20162\u221e +B2) \u221a\nlog(2/\u03b4)\u221a 2n\nwhere C1 is a constant dependent only on d, r0, s:\nC1 = 288 \u221a dr0(B + \u2016s\u2016\u221e)\u2016s\u2032\u2016\u221e.\nWe also consider the case of a massive nonparametric class. Let (HK , \u2016 \u00b7 \u2016K) be the Gaussian reproducing kernel Hilbert space (RKHS) with the kernel K(x1, x2) = exp { \u2212\u2016x1 \u2212 x2\u201622/\u03c32 } for\nsome \u03c3 > 0, and let Br := { h \u2208 HK : \u2016h\u2016K \u2264 r } be the radius-r ball in HK . Let F be the class of all functions of the form f(z) = f(x, y) = (y \u2212 f0(x))2, where the predictors f0 : X \u2192 R belong to IK(Br), an embedding of Br into the space C(X ) of continuous real-valued functions on X equipped with the sup norm \u2016f\u2016X := supx\u2208X |f(x)|.\nIn order to apply Theorem 3.1, we need to control the covering numbers N (F , \u2016 \u00b7 \u2016\u221e, \u00b7). To that end, we need the following estimate due to Cucker and Zhou [13, Thm 5.1] (which was later shown by K\u00fchn [14] to be asymptotically exact up to the double logarithmic factor):\nProposition 3.2. For compact X \u2286 Rd,\nlogN (IK(Br), \u2016 \u00b7 \u2016X , u) \u2264 d ( 32 +\n640d(diam(X ))2 \u03c32\n)d+1 ( log r\nu\n)d+1\nholds for all 0 < u \u2264 r/2. Using Proposition 3.2, we can prove the following generalization bound for Gaussian RKHS.\nCorollary 3.2. With probability at least 1\u2212 \u03b4, for any P \u2208 P(Z), R\u03b5,2(P, f\u0302)\u2212R\u2217\u03b5,2(P,F) \u2264 C1\u221a n (r2 +Br) + 384(r0 +B) 2 \u221a n ( 1 + 4(r20 +B 2) \u03b52 ) + 6(r2 +B2) \u221a log(2/\u03b4)\u221a 2n\nwhere C1 is a constant dependent only on d, r0, \u03c3:\nC1 = 48 \u221a d ( 2\u0393 ( d+ 3\n2 , log 2\n) + (log 2) d+1 2 )( 32 +\n2560dr20 \u03c32\n) d+1 2\n,\nand \u0393(s, v) := \u222b\u221e v u s\u22121e\u2212udu is the incomplete gamma function."}, {"heading": "3.2 Technical lemmas for the proof of Theorem 3.1", "text": "Lemma 3.1. Fix some Q \u2208 Pp(Z). Define f\u0303 \u2208 F and \u03bb\u0303 \u2265 0 via\nf\u0303 := argmin f\u2208F R\u03b5,p(Q, f) and \u03bb\u0303 := argmin \u03bb\u22650\n{ \u03bb\u03b5p +EQ[\u03d5\u03bb,f\u0303 (Z)] } .\nThen\n\u03bb\u0303 \u2264 C02p\u22121 ( 1 + ( diam(Z)\n\u03b5\n)p) . (3.6)\nLemma 3.2. The expected Rademacher complexity of the function class \u03a6 satisfies\nRn(\u03a6) \u2264 12\u221a n\n\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du+ 12C0(2 diam(Z))p\u221a n\n( 1 + ( diam(Z)\n\u03b5\n)p) ."}, {"heading": "4 Application to domain adaptation", "text": "As discussed in Section 1.2, the problem of domain adaptation arises when we want to transfer the data or knowledge from a source domain P \u2208 P(Z) to a different but related target domain Q \u2208 P(Z) [9]. Suppose that it is possible to estimate the Wasserstein distance Wp(P,Q) between the two domain distributions. Then, as we show below, we can provide a generalization bound for the target domain by combining estimation guarantees for Wp(P,Q) with risk inequalities of Section 2.\nWe work in the setting considered by Courty et al. [10]: Let Z = X \u00d7 Y, where (X , dX ) is the feature space and (Y, dY ) is the label space. We endow Z with the \u2113p product metric\ndZ(z, z \u2032) = dZ((x, y), (x \u2032, y\u2032)) := ( dpX (x, x \u2032) + dpY(y, y \u2032) )1/p .\nLet P = \u00b5\u2297PY |X and Q = \u03bd\u2297QY |X be the source and the target domain distributions, respectively. We assume that domain drift is due to an unknown (possibly nonlinear) transformation T : X \u2192 X of the feature space that preserves the conditional distribution of the labels given the features. That is, \u03bd = T#\u00b5, the pushforward of \u00b5 by T , and for any x \u2208 X and any measurable set B \u2286 Y\nPY |X(B|x) = QY |X(B|T (x)). (4.1)\nThis assumption leads to the following lemma, which enables us to estimate Wp(P,Q) only from unlabeled source domain data and unlabeled target domain data:\nLemma 4.1. Suppose there exists a deterministic and invertible optimal transport map T : X \u2192 X such that \u03bd = T#\u00b5, i.e., W p p (\u00b5, \u03bd) = E\u00b5[d p X (X,T (X))]. Then\nWp(P,Q) = Wp(\u00b5, \u03bd). (4.2)\nRemark 4.1. If X is a convex subset of Rd endowed with the \u2113p metric dX (x, x\u2032) = \u2016x \u2212 x\u2032\u2016p for p \u2265 2, then, under the assumption that \u00b5 and \u03bd have positive densities with respect to the Lebesgue measure, the (unique) optimal transport map from \u00b5 to \u03bd is deterministic and a.e. invertible \u2013 in fact, its inverse is equal to the optimal transport map from \u03bd to \u00b5 [8].\nNow suppose that we have n labeled examples (X1, Y1), . . . , (Xn, Yn) from P and m unlabeled examples X \u20321, . . . ,X \u2032 m from \u03bd. Define the empirical distributions\n\u00b5n = 1\nn\nn\u2211\ni=1\n\u03b4Xi , \u03bdm = 1\nm\nm\u2211\nj=1\n\u03b4X\u2032j .\nNotice that, by the triangle inequality, we have\nWp(\u00b5, \u03bd) \u2264 Wp(\u00b5, \u00b5n) +Wp(\u00b5n, \u03bdm) +Wp(\u03bd, \u03bdm). (4.3)\nHere, Wp(\u00b5n, \u03bdm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(\u00b5, \u00b5n) and Wp(\u03bd, \u03bdm) rapidly converge to zero as n,m \u2192 \u221e:\nProposition 4.1. Let \u00b5 be a probability distribution on a bounded set X \u2282 Rd, where d > 2p. Let \u00b5n denote the empirical distribution of X1, . . . ,Xn i.i.d.\u223c \u00b5. Then, for any r \u2208 (0,\u221e),\nP(Wp(\u00b5n, \u00b5) \u2265 r) \u2264 Ca exp(\u2212Cbnrd/p) (4.4)\nwhere Ca, Cb are constants depending on p, d, diam(X ) only.\nBased on these considerations, we propose the following domain adaptation scheme:\n1. Compute the p-Wasserstein distance Wp(\u00b5n, \u03bdm) between the empirical distributions of the features in the labeled training set from the source domain P and the unlabeled training set from the target domain Q.\n2. Set the desired confidence parameter \u03b4 \u2208 (0, 1) and the radius\n\u03b5\u0302(\u03b4) := Wp(\u00b5n, \u03bdm) +\n( log(4Ca/\u03b4)\nCbn\n)p/d + ( log(4Ca/\u03b4)\nCbm\n)p/d . (4.5)\n3. Compute the empirical risk minimizer\nf\u0302 = argmin f\u2208F R\u03b5\u0302(\u03b4),p(Pn, f), (4.6)\nwhere Pn is the empirical distribution of the n labeled samples from P .\nWe can give the following target domain generalization bound for the hypothesis generated according to (4.6):\nTheorem 4.1. Suppose that the feature space X is a bounded subset of Rd with d > 2p, take dX (x, x\n\u2032) = \u2016x\u2212 x\u2032\u2016p, and let F be a family of hypotheses with Lipschitz constant at most L. Then with probability at least 1\u2212 \u03b4, the empirical risk minimizer f\u0302 from (4.6) satisfies\nR(Q, f\u0302)\u2212R(Q,F) \u2264 2L\u03b5\u0302(\u03b4) + C1\u221a n + C2\u221a n\n( 1 + ( diam(Z) \u03b5\u0302(\u03b4) )p) + 3M \u221a log(4/\u03b4)\u221a 2n ,\nwhere\nC1 = 24\n\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du and C2 = 24C0(2diam(Z))p.\nProof. For simplicity, we assume that there exists a hypothesis f\u2217 \u2208 F that achieves R(Q,F). Then, for any \u03b5 > 0 such that Wp(P,Q) \u2264 \u03b5, Proposition 2.1 implies that\nR(Q, f\u0302)\u2212R(Q, f\u2217) \u2264 R\u03b5,p(P, f\u0302)\u2212R\u03b5,p(P, f\u2217) + 2L\u03b5 \u2264 R\u03b5,p(P, f\u0302)\u2212R\u03b5,p(P,F) + 2L\u03b5.\nFrom Theorem 3.1, we know that\nR\u03b5,p(P, f\u0302)\u2212R\u03b5,p(P,F) \u2264 C1\u221a n + C2\u221a n\n( 1 + ( diam(Z)\n\u03b5\n)p) + 3M \u221a\nlog(4/\u03b4)\u221a 2n\nholds with probability at least 1\u2212\u03b4/2. Thus, it remains to find the right \u03b5, such that thatWp(P,Q) \u2264 \u03b5 holds with high probability. From Proposition 4.1, we see that each of the following two statements holds with probability at least 1\u2212 \u03b4/4:\nWp(\u00b5n, \u00b5) \u2264 ( log(4Ca/\u03b4)\nCbn\n)p/d , Wp(\u03bdm, \u03bd) \u2264 ( log(4Ca/\u03b4)\nCbm\n)p/d .\nSince Wp(P,Q) = Wp(\u00b5, \u03bd) by Lemma 4.1, we see that Wp(P,Q) \u2264 \u03b5\u0302(\u03b4) with probability at least 1\u2212\u03b4/2, where \u03b5\u0302(\u03b4) is given by Eq. (4.5). The claim of the theorem follows from the union bound."}, {"heading": "A Proofs", "text": "A.1 Proofs for Section 2\nProof of Proposition 2.1. For p = 1, the result follows immediately from the Kantorovich dual representation of W1(\u00b7, \u00b7) [8]:\nW1(Q,Q \u2032) = sup    |EQF \u2212EQ\u2032F | : sup\nz,z\u2032\u2208Z z 6=z\u2032\n|F (z) \u2212 F (z\u2032)| dZ(z, z\u2032) \u2264 1   \nand from the fact that, for Q,Q\u2032 \u2208 BW\u03b5,1(P ), W1(Q,Q\u2032) \u2264 2\u03b5 by the triangle inequality. For p > 1, the result follows from the fact that W1(Q,Q \u2032) \u2264 Wp(Q,Q\u2032) for all Q,Q\u2032 \u2208 Pp(Z).\nProof of Proposition 2.2. Fix some Q,Q\u2032 \u2208 BW\u03b5,p(P ) and let M \u2208 P(Z \u00d7 Z) achieve the infimum in (1.4) for Wp(Q,Q \u2032). Then for (Z,Z \u2032) \u223c M we have\nf(Z \u2032)\u2212 f(Z) \u2264 \u222b 1\n0 Gf (\u03b3(t))dt \u00b7 dZ(Z,Z \u2032)\n\u2264 1 2\n( Gf (Z) +Gf (Z \u2032) ) dZ(Z,Z \u2032),\nwhere the first inequality is from (2.1) and the second one is by the assumed geodesic convexity of Gf . Taking expectations of both sides with respect to M and using H\u00f6lder\u2019s inequality, we obtain\nR(Q\u2032, f)\u2212R(Q, f) \u2264 1 2\n( EM \u2223\u2223Gf (Z) +Gf (Z \u2032) \u2223\u2223q )1/q ( EMd p Z(Z,Z \u2032) )1/p\n= 1\n2\n\u2225\u2225Gf (Z) +Gf (Z \u2032) \u2225\u2225 Lq(M) Wp(Q,Q \u2032),\nwhere we have used the p-Wasserstein optimality of M for Q and Q\u2032. By the triangle inequality, and since Z \u223c Q and Z \u2032 \u223c Q,\n\u2016Gf (Z) +Gf (Z \u2032)\u2016Lq(M) \u2264 \u2016Gf (Z)\u2016Lq(Q) + \u2016Gf (Z)\u2016Lq(Q\u2032) \u2264 2 sup\nQ\u2208BW\u03b5,p(P )\n\u2016Gf (Z)\u2016Lq(Q).\nInterchanging the roles of Q and Q\u2032 and proceeding with the same argument, we obtain the estimate\nsup Q,Q\u2032\u2208BW\u03b5,p(P ) |R(Q, f)\u2212R(Q\u2032, f)| \u2264 2\u03b5 sup Q\u2208BW\u03b5,p(P ) \u2016Gf (Z)\u2016Lq(Q),\nfrom which it follows that\nR(Q, f) \u2264 R\u03b5,p(P, f) = sup\nQ\u2032\u2208BW\u03b5,p(P )\n[R(Q\u2032, f)\u2212R(Q, f) +R(Q, f)]\n\u2264 R(Q, f) + 2\u03b5 sup Q\u2208BW\u03b5,p(P ) \u2016Gf (Z)\u2016Lq(Q).\nProof of Proposition 2.3. As a subset of Rd+1, Z is a geodesic space: for any pair z, z\u2032 \u2208 Z there is a unique constant-speed geodesic \u03b3(t) = (1 \u2212 t)z + tz\u2032. We claim that Gf (z) = Gf (x, y) = 2(B+M)(1+L\u2016\u2207f0(x)\u20162) is a geodesically convex upper gradient for f(z) = f(x, y) = (y\u2212f0(x))2. In this flat Euclidean setting, geodesic convexity coincides with the usual definition of convexity, and the map z 7\u2192 Gf (z) is evidently convex:\nGf ((1 \u2212 t)z + tz\u2032) \u2264 (1\u2212 t)Gf (z) + tGf (z\u2032).\nNext, by the mean-value theorem,\nf(z\u2032)\u2212 f(z) = \u222b 1\n0 \u3008z\u2032 \u2212 z,\u2207f((1\u2212 t)z + t\u2032z)\u3009dt\n\u2264 \u222b 1\n0 \u2016\u2207f((1\u2212 t)z + tz\u2032)\u20162 dt \u00b7 \u2016z \u2212 z\u2032\u20162\n=\n\u222b 1\n0 \u2016\u2207f((1\u2212 t)z + tz\u2032)\u20162 dt \u00b7 dZ(z, z\u2032),\nand a simple calculation shows that\n\u2016\u2207f(z)\u201622 = \u2016\u2207f(x, y)\u201622 = 4f(z) ( 1 + \u2016\u2207f0(z)\u201622 )\n\u2264 4(B +M)2(1 + L2\u2016x\u201622).\nTherefore, \u2016\u2207f(z)\u20162 \u2264 Gf (z) for z = (x, y), as claimed. Thus, by Proposition (2.2),\nR(Q, f) \u2264 R\u2217\u03b5,2(P, f) \u2264 R(Q, f) + 2 sup\nQ\u2208BW\u03b5,2(P )\n\u2016Gf (Z)\u2016L2(Q)\u03b5\n= R(Q, f) + 4(B +M) ( 1 + L sup\nQ\u2208BW\u03b5,2(P )\nEQ\u2016X\u20162 ) \u03b5\n= R(Q, f) + 4(B +M) ( 1 + L sup\nQ\u2208BW\u03b5,2(P )\n\u03c3Q,X\n) \u03b5.\nA.2 Proofs for Section 3\nProof of Corollary 3.1. We first verify the regularity assumptions. Assumption 3.1 is evidently satisfied since diam(Z)2 = diam(X )2 + diam(Y)2 \u2264 4r20 + 4B2. Each f \u2208 F is continuous, and Assumption 3.2 holds with M = 2(\u2016s\u20162\u221e +B2). To verify Assumption 3.3, take f0 = 0 and pick an arbitrary x0 \u2208 X . Then\nf(x, y) = (y \u2212 s(0))2 = y2 \u2264 \u2016x\u2212 x0\u20162 + y2 = d2Z(z, z0)\nwith z0 = (x0, 0). Thus, Assumption 3.3 holds with C0 = 1 and z0 = 0.\nTo evaluate the Dudley entropy integral in (3.1), we need to estimate the covering numbers N (F , \u2016 \u00b7 \u2016\u221e, \u00b7). First observe that, for any two f, g \u2208 F corresponding to f0, g0 \u2208 Rd, we have\nsup x\u2208X sup y\u2208Y |f(x, y)\u2212 g(x, y)| = sup x\u2208X sup y\u2208Y\n\u2223\u2223\u2223\u2223 ( y \u2212 s(fT0 x) )2 \u2212 ( y \u2212 s(gT0 x) )2\u2223\u2223\u2223\u2223\n\u2264 2B sup x\u2208X |s(fT0 x)\u2212 s(gT0 x)|+ sup x\u2208X |s2(fT0 x)\u2212 s2(gT0 x)| \u2264 2r0 ( B + \u2016s\u2016\u221e\n) \u2016s\u2032\u2016\u221e\ufe38 \ufe37\ufe37 \ufe38\n:=D\n\u2016f0 \u2212 g0\u20162.\nSince f0, g0 belong to the unit ball in R d,\nN (F , \u2016 \u00b7 \u2016\u221e, u/2) \u2264 ( 6D\nu\n)d\nfor 0 < u < 2D, and N (F , \u2016 \u00b7 \u2016\u221e, u/2) = 1 for u \u2265 2D, which gives \u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du \u2264\n\u222b 2D\n0\n\u221a d log ( 6D\nu\n) du\n= 6D \u221a d\n\u222b 1/3\n0\n\u221a log ( 1\nu\n) du\n= 6cD \u221a d,\nwhere c = 16 ( 2 \u221a log 3 + 3 \u221a \u03c0erfc( \u221a log 3) ) < 1. Substituting this into the bound (3.1), we get the desired estimate.\nProof of Corollary 3.2. We start by presenting the following technical lemma.\nLemma A.1. For any f, g \u2208 F we have: \u2016f\u2016\u221e \u2264 2(r2 +B2) \u2016f \u2212 g\u2016\u221e \u2264 2(r +B)\u2016f0 \u2212 g0\u2016X , where f0, g0 \u2208 IK(Br) are the predictors satisfying f(x, y) = (y\u2212 f0(x))2 and g(x, y) = (y\u2212 g0(x))2. Proof. First note that for Gaussian kernels, supx\u2208X \u221a K(x, x) \u2264 1 by definition. Then, we have the first claim by\n|f(z)| = (f0(x)\u2212 y)2 \u2264 2f20 (x) + 2y2 \u2264 2r2 + 2B2, where the first inequality is due to convexity of square function and the second inequality is due to the reproducing kernel property of K and the Cauchy-Schwarz inequality in HK . Second claim is established similarly:\n\u2223\u2223f(z)\u2212 g(z) \u2223\u2223 = \u2223\u2223(f0(x)\u2212 y)2 \u2212 (g0(x)\u2212 y)2 \u2223\u2223\n= \u2223\u2223f0(x) + g0(x)\u2212 2y \u2223\u2223\u2223\u2223f0(x)\u2212 g0(x) \u2223\u2223 \u2264 ( 2 sup \u2016h\u2016K\u2264r |h(x)| + 2|y| )\u2223\u2223f0(x)\u2212 g0(x) \u2223\u2223\n\u2264 2(r +B)\u2016f0 \u2212 g0\u2016X , where the last inequality is due to Cauchy-Schwarz inequality again.\nWe now return to the proof of Corollary 3.2. Assumption 3.1 holds since diam(Z)2 = diam(X )2+ diam(Y)2 \u2264 4r20 + 4B2. The functions in F are continuous, and Assumption 3.2 holds with M = 2(r2+B2) by virtue of the first estimate of Lemma A.1. To verify Assumption 3.3, take f0 = 0 and pick an arbitrary x0 \u2208 X . Then\nf(x, y) = y2 \u2264 \u2016x\u2212 x0\u20162 + y2 = d2Z(z, z0)\nwith z0 = (x0, 0). Thus, Assumption 3.3 holds with C0 = 1 and z0 = 0. Now we proceed to upper-bound the Dudley entropy integral for F :\n\u222b \u221e\n0\n\u221a logN ( F , \u2016 \u00b7 \u2016\u221e, u\n2\n) du\n\u2264 \u222b \u221e\n0\n\u221a logN ( IK(Br), \u2016 \u00b7 \u2016X ,\nu\n4(r +B)\n) du\n=\n\u222b 4(r+B)r\n0\n\u221a logN ( IK(Br), \u2016 \u00b7 \u2016X ,\nu\n4(r +B)\n) du\n\u2264 \u222b 2(r2+Br)\n0\n\u221a logN ( IK(Br), \u2016 \u00b7 \u2016X ,\nu\n4(r +B)\n) du\n\ufe38 \ufe37\ufe37 \ufe38 :=T1\n+\n\u222b 4(r2+Br)\n2(r2+Br)\n\u221a logN ( IK(Br), \u2016 \u00b7 \u2016X , r\n2\n) du\n\ufe38 \ufe37\ufe37 \ufe38 :=T2\nwhere we used the second claim of Lemma A.1 for the first inequality and the monotonicity of covering numbers for the second inequality. Plugging in the estimate from Proposition 3.2,\nT1 \u2264 \u221a d ( 32 +\n2560dr20 \u03c32\n) d+1 2 \u222b 2(r2+Br)\n0\n( log 4(r2 +Br)\nu\n) d+1 2\ndu\n= 4 \u221a d ( 32 +\n2560dr20 \u03c32\n) d+1 2\n(r2 +Br)\n\u222b +\u221e\nlog 2 e\u2212uu\nd+1 2 du\n= 4 \u221a d ( 32 +\n2560dr20 \u03c32\n) d+1 2\n(r2 +Br)\u0393\n( d+ 3\n2 , log 2\n) .\nT2 \u2264 2(r2 +Br) \u00b7 \u221a d ( 32 +\n2560dr20 \u03c32\n) d+1 2\n(log 2) d+1 2 ,\nand hence T1 + T2 \u2264 C148 (r2 +Br).\nProof of Lemma 3.1. Since \u03d5\u03bb,f \u2265 0 for all \u03bb, f , we arrive at\n\u03bb\u0303 \u2264 R\u03b5,p(Q,F) \u03b5p . (A.1)\nWe proceed to upper-bound the local minimax risk R\u03b5,p(Q,F):\nR\u03b5,p(Q,F) = inf f\u2208F min \u03bb\u22650\n{ \u03bb\u03b5p + \u222b\nZ sup z\u2032\u2208Z\n[ f(z\u2032)\u2212 \u03bbdpZ(z, z\u2032) ] Q(dz\u2032)\n}\n\u2264 min \u03bb\u22650\n{ \u03bb\u03b5p + \u222b\nZ sup z\u2032\u2208Z\n[ f0(z \u2032)\u2212 \u03bbdpZ(z, z\u2032) ] Q(dz\u2032)\n}\n\u2264 min \u03bb\u22650\n{ \u03bb\u03b5p + \u222b\nZ sup z\u2032\u2208Z\n[ C0d p Z(z \u2032, z0)\u2212 \u03bbdpZ(z, z\u2032) ] Q(dz\u2032) } .\nFor \u03bb \u2265 C02p\u22121, the integrand can be upper-bounded as follows:\nsup z\u2032\u2208Z\n[ C0d p Z(z \u2032, z0)\u2212 \u03bbdpZ(z, z\u2032) ] \u2264 sup\nz\u2032\u2208Z\n[ C02 p\u22121dpZ(z, z0) + (C02 p\u22121 \u2212 \u03bb)dpZ(z, z\u2032) ]\n\u2264 C02p\u22121dpZ(z, z0).\nTherefore,\nR\u03b5,p(Q,F) \u2264 min \u03bb\u2265C02p\u22121\n{ \u03bb\u03b5p +C02 p\u22121 \u222b\nZ dpZ(z, z0)Q(dz)\n}\n\u2264 C02p\u22121 ( \u03b5p + (diam(Z))p ) .\nSubstituting this estimate into (A.1), we obtain (3.6)\nProof of Lemma 3.2. Define the \u03a6-indexed process X = (X\u03d5)\u03d5\u2208\u03a6 via\nX\u03d5 := 1\u221a n\nn\u2211\ni=1\n\u03c3i\u03d5(Zi),\nwhich is clearly zero-mean: E[X\u03d5] = 0 for all \u03d5 \u2208 \u03a6. To upper-bound the Rademacher average Rn(\u03a6), we first show that X is a subgaussian process with respect to a suitable pseudometric. For \u03d5 = \u03d5\u03bb,f and \u03d5 \u2032 = \u03d5\u03bb\u2032,f \u2032 , define\nd\u03a6(\u03d5,\u03d5 \u2032) := \u2016f \u2212 f \u2032\u2016\u221e + (diam(Z))p|\u03bb\u2212 \u03bb\u2032|,\nand it is not hard to show that \u2016\u03d5 \u2212 \u03d5\u2032\u2016\u221e \u2264 d\u03a6(\u03d5,\u03d5\u2032). Then, for any t \u2208 R, using Hoeffding\u2019s lemma and the fact that (\u03c3i, Zi) are i.i.d., we arrive at\nE [ exp(t(X\u03d5 \u2212X\u03d5\u2032)) ] = E  exp   t\u221a\nn\nn\u2211\ni=1\n\u03c3i(\u03d5(Zi)\u2212 \u03d5\u2032(Zi))\n   \n=  E [ exp\n( t\u221a n \u03c31 ( \u03d5(Z1)\u2212 \u03d5\u2032(Z1)\n)) ]  n\n\u2264 exp ( t2d2\u03a6(\u03d5,\u03d5 \u2032)\n2\n) .\nHence, X is subgaussian with respect to d\u03a6, and therefore the Rademacher average Rn(\u03a6) can be upper-bounded by the Dudley entropy integral [12]:\nRn(\u03a6) \u2264 12\u221a n\n\u222b \u221e\n0\n\u221a logN (\u03a6, d\u03a6, u)du,\nwhere N (\u03a6, d\u03a6, \u00b7) are the covering numbers of (\u03a6, d\u03a6). From the definition of d\u03a6, it follows that\nN (\u03a6, d\u03a6, u) \u2264 N (F , \u2016 \u00b7 \u2016\u221e, u/2) \u00b7 N (\u039b, | \u00b7 |, u/2(diam(Z))p),\nand therefore\nRn(\u03a6) \u2264 12\u221a n\n(\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du+\n\u222b \u221e\n0\n\u221a logN (\u039b, | \u00b7 |, u/2(diam(Z))p)du ) .\nSince \u039b is a compact interval, it is straightforward to upper-bound the second integral:\n\u222b \u221e\n0\n\u221a logN (\u039b, | \u00b7 |, u/2(diam(Z))p)du \u2264 2|\u039b|(diam(Z))p\n\u222b 1/2\n0\n\u221a log(1/u)du\n= 2c|\u039b|(diam(Z))p,\nwhere |\u039b| = C02p\u22121(1 + (diam(Z)/\u03b5)p) is the length of \u039b and c = 12 (\u221a log 2 + \u221a \u03c0erfc( \u221a log 2) ) < 1. Consequently,\nRn(\u03a6) \u2264 12\u221a n\n(\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du+ 2|\u039b|(diam(Z))p\n)\n\u2264 12\u221a n\n\u222b \u221e\n0\n\u221a logN (F , \u2016 \u00b7 \u2016\u221e, u/2)du+ 12C0(2 diam(Z))p\u221a n\n( 1 + ( diam(Z)\n\u03b5\n)p) .\nA.3 Proofs for Section 4\nProof of Lemma 4.1. First we prove that Wp(P,Q) \u2264 Wp(\u00b5, \u03bd). Define the mapping T\u0303 : Z \u2192 Z by T\u0303 := T \u2297 idZ , i.e., T\u0303 (z) = T\u0303 (x, y) = (T (x), y), and let Q\u0303 = T\u0303#P , the pushforward of P by T\u0303 . We claim that Q\u0303 \u2261 Q. Indeed, for any measurable sets A \u2286 X and B \u2286 Y,\nQ\u0303(A\u00d7B) = T\u0303#P (A\u00d7B) = P (T\u22121(A) \u00d7B)\n=\n\u222b\nT\u22121(A) \u00b5(dx)PY |X(B|x)\n=\n\u222b\nA T#\u00b5(dx)PY |X(B|T (x))\n=\n\u222b\nA \u03bd(dx)QY |X(B|x),\nwhere we have used the relation (4.1) and the invertibility of T . Thus,\nW pp (P,Q) \u2264 EP [dpZ(Z, T\u0303 (Z)))] = EP [d p X (X,T (X))] = W p p (\u00b5, \u03bd).\nFor the reverse inequality, let M \u2208 P(Z \u00d7 Z) be the optimal coupling of P and Q. Then, for Z = (X,Y ) and Z \u2032 = (X \u2032, Y \u2032) with (Z,Z \u2032) \u223c M , the marginal MXX\u2032 is evidently a coupling of the marginals \u00b5 and \u03bd, and therefore\nW pp (P,Q) = EM [d p Z(Z,Z \u2032)]\n= EM [d p X (X,X \u2032)] +EM [d p Y(Y, Y \u2032)] \u2265 EM [dpX (X,X \u2032)] \u2265 W pp (\u00b5, \u03bd)."}], "references": [{"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["V. Koltchinskii"], "venue": "volume 2033 of Lecture Notes in Mathematics. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A minimax approach to supervised learning", "author": ["F. Farnia", "D. Tse"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Statistics of robust optimization: a generalized empirical likelihood approach", "author": ["J.C. Duchi", "P.W. Glynn", "H. Namkoong"], "venue": "arXiv preprint 1610.03425", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations", "author": ["P. Mohajerin Esfahani", "D. Kuhn"], "venue": "arXiv preprint 1505.05116", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributionally robust logistic regression", "author": ["S. Shafieezadeh-Abadeh", "P. Mohajerin Esfahani", "D. Kuhn"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Distributionally robust stochastic optimization with Wasserstein distance", "author": ["R. Gao", "A.J. Kleywegt"], "venue": "arXiv preprint 1604.02199", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Topics in optimal transportation", "author": ["C. Villani"], "venue": "American Mathematics Society, Providence, RI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning, 79:151\u2013175", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal transport for domain adaptation", "author": ["N. Courty", "R. Flamary", "D. Tuia", "A. Rakotomamonjy"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures", "author": ["L. Ambrosio", "N. Gigli", "G. Savar\u00e9"], "venue": "Birkh\u00e4user", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Upper and Lower Bounds for Stochastic Processes: Modern Methods and Classical Problems", "author": ["M. Talagrand"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning theory: an approximation theory viewpoint", "author": ["F. Cucker", "D. Zhou"], "venue": "Cambridge University Press, Cambridge, MA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Covering numbers of gaussian reproducing kernel hilbert spaces", "author": ["T. K\u00fchn"], "venue": "Journal of Complexity, pages 489\u2013499", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "On the rate of convergence in wasserstein distance of the empirical measure", "author": ["N. Fournier", "A. Guillin"], "venue": "Probability Theory and Related Fields, pages 1\u201332", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction and problem set-up In the traditional paradigm of statistical learning [1], we have a class P of probability measures on a measurable instance space Z and a class F of measurable functions f : Z \u2192 R+.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:", "startOffset": 112, "endOffset": 118}, {"referenceID": 1, "context": "Under suitable regularity assumptions, this objective can be accomplished via Empirical Risk Minimization (ERM) [1, 2]:", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 5, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 6, "context": ", via moment constraints [3], f -divergence balls [4], and Wasserstein balls [5, 6, 7].", "startOffset": 77, "endOffset": 86}, {"referenceID": 2, "context": "However, with the exception of the recent work by Farnia and Tse [3], the minimizer of (1.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "(Recently, this modification was also proposed by Farnia and Tse [3] for ambiguity sets defined by a finite number of moment constraints.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "The metric structure of Z can be used to define a family of metrics on the spaces Pp(Z) [8]: Definition 1.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 5, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 6, "context": "Following [5, 6, 7], we let the ambiguity set A(P ) be the p-Wasserstein ball of radius \u03b5 \u2265 0 centered at P : A(P ) = B \u03b5,p(P ) := { Q \u2208 Pp(Z) : Wp(P,Q) \u2264 \u03b5 } ,", "startOffset": 10, "endOffset": 19}, {"referenceID": 8, "context": "In contrast to the standard statistical learning framework, where the risk of the learned hypothesis is evaluated on the same unknown distribution that was used for generating the training examples, the problem of domain adaptation [9] arises when the training data are generated according to an unknown distribution P , but the learned hypothesis is evaluated on another unknown distribution Q.", "startOffset": 232, "endOffset": 235}, {"referenceID": 9, "context": "[10] have introduced an algorithmic framework for domain adaptation based on optimal transport.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10], our approach completely bypasses the problem of estimating the transport map T .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The first one is a simple consequence of the Kantorovich duality theorem from the theory of optimal transport [8]: Proposition 2.", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": "Since we are working with general metric spaces that may lack an obvious differentiable structure, we need to first introduce some concepts from metric geometry [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 0, "context": "A metric space (Z,dZ) is a geodesic space if for every two points z, z\u2032 \u2208 Z there exists a path \u03b3 : [0, 1] \u2192 Z, such that \u03b3(0) = z, \u03b3(1) = z\u2032, and dZ(\u03b3(s), \u03b3(t)) = (t\u2212 s) \u00b7 dZ(\u03b3(0), \u03b3(1)) for all 0 \u2264 s \u2264 t \u2264 1 (such a path is called a constant-speed", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "A functional F : Z \u2192 R is geodesically convex if for any pair of points z, z\u2032 \u2208 Z there is a constant-speed geodesic \u03b3, so that F (\u03b3(t)) \u2264 (1\u2212 t)F (\u03b3(0)) + tF (\u03b3(1)) = (1\u2212 t)F (z) + tF (z), \u2200t \u2208 [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 6, "context": "The following strong duality result due to Gao and Kleywegt [7] will be instrumental: Proposition 3.", "startOffset": 60, "endOffset": 63}, {"referenceID": 11, "context": "1) is the Dudley entropy integral [12].", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "The excess risk bound of Farnia and Tse [3] has the same behavior, where in that case \u03b5 is the slack in the moment constraints defining the ambiguity set.", "startOffset": 40, "endOffset": 43}, {"referenceID": 13, "context": "1] (which was later shown by K\u00fchn [14] to be asymptotically exact up to the double logarithmic factor): Proposition 3.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "2, the problem of domain adaptation arises when we want to transfer the data or knowledge from a source domain P \u2208 P(Z) to a different but related target domain Q \u2208 P(Z) [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 9, "context": "[10]: Let Z = X \u00d7 Y, where (X , dX ) is the feature space and (Y, dY ) is the label space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "invertible \u2013 in fact, its inverse is equal to the optimal transport map from \u03bd to \u03bc [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "Here, Wp(\u03bcn, \u03bdm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(\u03bc, \u03bcn) and Wp(\u03bd, \u03bdm) rapidly converge to zero as n,m \u2192 \u221e:", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "Here, Wp(\u03bcn, \u03bdm) can be computed from unlabeled data by solving a finite-dimensional linear program [8], and the following convergence result of Fournier and Guillin [15] implies that, with high probability, both Wp(\u03bc, \u03bcn) and Wp(\u03bd, \u03bdm) rapidly converge to zero as n,m \u2192 \u221e:", "startOffset": 166, "endOffset": 170}, {"referenceID": 7, "context": "For p = 1, the result follows immediately from the Kantorovich dual representation of W1(\u00b7, \u00b7) [8]: W1(Q,Q ) = sup \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 |EQF \u2212EQ\u2032F | : sup z,z\u2032\u2208Z z 6=z\u2032 |F (z) \u2212 F (z\u2032)| dZ(z, z\u2032) \u2264 1 \uf8fc \uf8f4\uf8fd \uf8f4\uf8fe and from the fact that, for Q,Q\u2032 \u2208 BW \u03b5,1(P ), W1(Q,Q) \u2264 2\u03b5 by the triangle inequality.", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "Hence, X is subgaussian with respect to d\u03a6, and therefore the Rademacher average Rn(\u03a6) can be upper-bounded by the Dudley entropy integral [12]: Rn(\u03a6) \u2264 12 \u221a n \u222b \u221e", "startOffset": 139, "endOffset": 143}], "year": 2017, "abstractText": "As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples.", "creator": "LaTeX with hyperref package"}}}