{"id": "1705.08722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Open-Category Classification by Adversarial Sample Generation", "abstract": "In real-world classification tasks, it is difficult to collect samples of all possible categories of the environment in the training stage. Therefore, the classifier should be prepared for unseen classes. When an instance of an unseen class appears in the prediction stage, a robust classifier should have the ability to tell it is unseen, instead of classifying it to be any known category. In this paper, adopting the idea of adversarial learning, we propose the ASG framework for open-category classification. ASG generates positive and negative samples of seen categories in the unsupervised manner via an adversarial learning strategy. With the generated samples, ASG then learns to tell seen from unseen in the supervised manner. Experiments performed on several datasets show the effectiveness of ASG. In the first two experiments, we used the ANOVA to predict the validity of the classifier by providing an inverse-squared test for the probability of a given given category. The results showed that the classifier was more likely to predict the reliability of a given category than to predict the reliability of the given classification. Finally, the training session showed that the classification performance of the ASG classifier in the supervised manner is a significant improvement over the prediction of the supervised approach for training of the target classes (with a higher likelihood for the predictions of the predicted classifier than for the prediction of the predicted classifier). We also observed that the ASG is more effective for predicting the reliability of the given classification than for predictions of the predicted classifier (with a higher probability for the predictions of the predicted classifier). Finally, the training session showed that the ASG is more effective for predicting the reliability of the given classification than for predictions of the predicted classifier (with a higher probability for the predictions of the predicted classifier). Finally, the training session showed that the ASG is more effective for predicting the reliability of the given classification than for predictions of the predicted classifier (with a higher probability for the predictions of the predicted classifier). Finally, the training session showed that the ASG is more effective for predicting the reliability of the given classification than for predictions of the predicted classifier (with a higher likelihood for the predictions of the predicted classifier). Finally, the training session showed that the ASG is more effective for predicting the reliability of the given classification than for predictions of the predicted classifier (with a higher probability for the predictions of the predicted classifier). Finally, the training session showed that the ASG is more effective for predicting the reliability of the given classification than", "histories": [["v1", "Wed, 24 May 2017 12:27:06 GMT  (473kb,D)", "https://arxiv.org/abs/1705.08722v1", "Published in IJCAI 2017"], ["v2", "Sat, 17 Jun 2017 09:08:34 GMT  (473kb,D)", "http://arxiv.org/abs/1705.08722v2", "Published in IJCAI 2017"]], "COMMENTS": "Published in IJCAI 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang yu", "wei-yang qu", "nan li", "zimin guo"], "accepted": false, "id": "1705.08722"}, "pdf": {"name": "1705.08722.pdf", "metadata": {"source": "CRF", "title": "Open-Category Classification by Adversarial Sample Generation\u2217", "authors": ["Yang Yu", "Wei-Yang Qu", "Nan Li", "Zimin Guo"], "emails": ["yuy@nju.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "As machine learning techniques are adopted in increasing applications, it is appealing that they can be applied in environments that are open and non-stationary, where unseen situations can emerge unexpectedly. For classification, a typical learning task, classical methods implicitly assume that the data is i.i.d. even for the future test ones. This assumption no longer holds in open environments, which drastically weaken the robustness of classical classification methods.\nIn this work, we consider the open-category classification (OCC) problem, where there are novel classes that none of their instances were observed during the training phase, but in the test phase their instances could be encountered. Classical approaches can only predict instances from unseen classes as one of the seen classes. An open-environment aware classifier, on the contrary, should be able to tell at first if an instance belongs to a seen class.\nDifferent directions have been explored related to the OCC problem. In class incremental-learning [Fink et al., 2006; Muhlbaier et al., 2009; Kuzborskij et al., 2013], new classes\n\u2217This research was supported by the NSFC (61375061, 61333014), Jiangsu SF (BK20160066), and Foundation for the Author of National Excellent Doctoral Dissertation of China (201451). \u2020Part of the work was done when Z. Guo was visiting Nanjing University as an undergraduate student.\nare assumed to appear incremental. However, these studies mainly focused on how to enable the system to incorporate later coming training instances from new classes, but did not address the problem of recognizing unseen classes. In learning with a rejection option [Chow, 1970], the classifier rejects to recognize an instance if its confidence is low, which, however, does not take open-environment into consideration. Note that an instance close to the seen class boundary can have a low confidence but belongs to a seen class, while an unseen class instance can have a high confidence if it is far from the seen class boundary. Outlier detection techniques [Hodge and Austin, 2004] could be employed if we treat unseen class instances as outliers. However, samples from seen class could also contains outliers, meanwhile, a cluster of unseen class instances may not be treated as outliers. Zero-shot learning [Palatucci et al., 2009] is close to the OCC problem, but they commonly assume that a high-level attribute set is available for all classes including the unseen one, which provides useful information for recognizing unseen class instances. In our problem, however, we consider learning in an unfamiliar open environment and thus do not assume the availability of such high-level attributes.\nOnly a few previous studies addressed the OCC problem. For examples, in [Scheirer et al., 2013] the open-set cost is considered, but it is hard to measure that cost without seeing open-set data; in [Da et al., 2014], a set of unlabeled data of all classes is employed, but sufficient unlabeled data from all potential classes may not always available. We are thus more interested in solving the problem without extra information.\nIn this paper, we propose the adversarial sample generation (ASG) framework for the OCC problem. Inspired by the adversarial learning [Goodfellow et al., 2014], ASG generates negative instances of seen classes by finding data points that are close to the training instances, given that they can be separated from the seen data by a discriminator. When the training data is small, ASG also generates positive instances that cannot be discriminated from the seen class instances, in order to enlarge the dataset. Using the supervision of the generated negative and positive samples of seen classes, it is then straightforward to train an open-category classifier to tell seen from unseen. We conduct experiments on several domains with open categories but no extra training information. The results show that the ASG achieves significant better performance than the compared methods.\nar X\niv :1\n70 5.\n08 72\n2v 2\n[ cs\n.L G\n] 1\n7 Ju\nn 20\n17\nThe rest of this paper is organized in four sections that presents the background, the proposed method, the experiment results, and the conclusion, respectively."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Related Problems", "text": "This paper studies the open-category classification problem, where no information about the unseen classes, neither instances nor attributes, is available. Some studies are related to this problem.\nThe incremental learning requires a proper adaptation of traditional machine learning approaches to deal with the dynamic and open environment, in addition, class-incremental learning(C-IL) [Zhou and Chen, 2002] is an important branch of it, which mainly concerned with the addition of new classes. In [Fink et al., 2006; Kuzborskij et al., 2013], each new class has a binary classifier which distinguishes between existing categories and new categories, and the new category in the classifier shares the hypothesis with the existing category to train. However, the method still need a few instances of new categories, when no instances of new categories are available in the training phase, these methods can not be applied. To address this limitation, a new method is introduced in [Da et al., 2014], which processes new categories by exploring unlabeled instances, this requires reliable unlabeled data, but the data availability and quality is difficult to guarantee in practice.\nThe open set recognition problem is mainly concerned by the community of pattern recognition, and has been applied to face recognition and speech recognition, etc. In [Phillips et al., 2011], the main concern is the operation of the threshold, only instances where the confidence is above the threshold are classified as seen classes. In [Scheirer et al., 2013; Bendale and Boult, 2015], both the new decision boundary and the risk over open space are considered to limit the regions for seen categories.\nThe outlier detection problem [Hodge and Austin, 2004] requires the identification of anomaly instances from a given data set. The outlier detection methods can be applied in open-category problems as long as the abnormal instances is predicted as novel classes. However, the difference is that outlier detection is only concerned with the discovery of abnormal instances, which is limited to specific applications. Besides, it does not take into account the classification error of existing categories\nThe class discovery problem tries to identify the instances of rare categories which are not known in advance, but are known to exist in the training data [Pelleg and Moore, 2005; Hospedales et al., 2013]. The open-category problem is different from class discovery problem, because the unseen class is not necessarily a rare class. On the other hand, it is possible to find examples of a rare class in the training data, while the instance of novel classes only appear in the test data.\nThe zero-shot problem tries to identify the unseen classes with the assumption that some high-level attributes of all classes are known as a prior. For example, in [Xian et al., 2016], a latent embedding model was presented which learns\na compatibility function in the high-level attributes space considered between image and class embeddings. In this work, we do not assume the availability of such information."}, {"heading": "2.2 Adversarial Learning", "text": "The adversarial learning employs a generative model and a discriminative model, where the generative model learns to generate instances that can fool the discriminative model as a non-generated instance, which is also called Turing Learning in [Li et al., 2016]. Besides, the Generative adversarial nets (GAN) combines the two models as a whole neural network for end-to-end training, resulting in a model that is consistent with the original data distribution. Further improvements include studies on the stability of GAN. For example, in [Nowozin et al., 2016], the F-divergence is introduced from the perspective of distance measurement, and GAN has been shown to be a special case of F-divergence when it comes to a particular metric.\nIn our work, we will employ the adversarial learning principle that a generative model fights to generate instances judged by a discriminator. Different with the studies on GAN, our framework can apply to various learning models besides neural networks. Moreover, to solve the open-category classification problem, we do not only need to generate seen class data, but more importantly need to generate unseen class data."}, {"heading": "2.3 Derivative-free Optimization", "text": "Previously learning approaches commonly employed gradient-based optimization methods. However, the optimization problems may not always simple enough to fit the gradient-based methods. Often, a complex optimization has to be relaxed to a convex problem, sacrificing the faithfulness to the original problem.\nAncient derivative-free optimization methods include representatives such as genetic algorithms [Goldberg, 1989], which are mostly heuristic methods. Recently, the derivativefree optimization methods have made significant progress in both theoretical foundation and practical usage, including Bayesian optimization methods [Brochu et al., 2010], optimistic optimization methods [Munos, 2014], and modelbased optimization [Yu et al., 2016]. A derivative-free optimization method considers an optimization formalized as argmaxx\u2208X f(x), where X is domain. The method, instead of calculating gradients of f , samples solutions x and learns from their feedbacks f(x) for finding better solutions. Therefore, derivative-free optimization methods can be more suitable for problem with bad mathematical properties, including non-convexity, non-differentiability, and having many local optima. We thus employ the state-of-the-art derivative-free methods in our approach."}, {"heading": "3 Proposed Method", "text": "The open-category classification (OCC) problem can be described as follows. Given a training dataset D = {(xi, yi)}Li=1, where xi \u2208 Rd is a training instance and yi \u2208 Y = {1, 2, ...,K} is the corresponding category label. In the test phase, we need to predict the categories of an open dataset Do = {(xi, yi)}\u221ei=1, where yi \u2208 Yo = {1, 2, ...,K,K +\n1, ...,M} withM > K. Since there are classes which are not observed in the training phase, the goal of OCC is to learn a model f(x) : X \u2192 Y \u2032 = {1, 2, ...,K, novel}, where the option novel indicates that the category was unseen in the training phase, so as to minimize the expected risk as follows\nf\u2217 = argminf\u2208H E(x,y)\u223cDoerr(y, f(x)) (1) whereH is the hypnosis space and err is defined as follows\nerr(y, f(x)) = { I(f(x) 6= y), y \u2208 Y I(f(x) 6= novel), y 6\u2208 Y (2)\nThe I(expression) is an indicator function, it equals 1 when the expression holds and 0 otherwise.\nThe OCC problem is difficult to solve because no information about the novel class is available. Our overall idea is that, if we can generate the instances of the class novel and put them into the training set (denoted the augmented training set as D\u0303), then the problem will be easily solved by standard supervised learning\nf\u2217 = argminf\u2208H E(x,y)\u223cD\u0303I(y 6= f(x)) (3)\nThe label of instance x can be predicted as argmaxk=1,...,K,novel fk(x). The problem is then how to generate data of the novel class.\nThe idea of adversarial learning [Goodfellow et al., 2014] is employed for data generation. In the adversarial learning, a generative model is trained to generate samples that are thought to be appropriate according to a discriminator. For example, to generate data consisting with the training data, the objective is that the discriminator cannot distinguish the generated data from the training data. Using this idea, we directly generate the data of unseen class for the OCC problem.\nGenerate Unseen Class Instances To generate an instance of the unseen classes, ASG searches in the instance space, such that the instance should not be recognized as the seen class by the discriminator, which is a classifier trained to separate generated samples and seen class instances. However, there are too many such instances. For the purpose of distinguishing seen from unseen, we only need the samples that are around the boundary between seen and unseen classes. Therefore, ASG tries to find an instance that is close to the seen class instances, but is recognized as unseen class by the discriminator.\nASG considers each class separately, and for each class, it generates samples one by one. Let PD(x;D,D\u2212) denote the probability of x to be positive by the discriminator, trained with positive data D and negative data D\u2212. For class k, denote the current generated samples as D\u2212k , which is empty initially. The objective that a generated sample does not belong to the seen class is\nargminx PD(x;Dk, D \u2212 k \u222a {x}) (4)\nIntuitively, we evaluate a generated sample by adding it to the negative data set and train the model to see if the generated sample is not classified as positive (seen class).\nEq.(4) alone cannot generate all the boundary samples of the seen class, but only data samples that do not belong to the\nAlgorithm 1 Generation of negative instances of seen classes Input:\nD: Training instances{(xi, yi)}Li=1 T : Number of generated instances per class L: Learning algorithm for the discriminant model Opt: A derivative-free optimization method\nOutput: D\u2212: Negative samples of all class 1, 2, ..K\n1: for each k \u2208 {1, ...,K} do 2: D\u2212k = \u2205 3: for t = 1, 2, ..., T do 4: x\u2212 = solve Eq.(7) by Opt with discriminator L 5: Update D\u2212k = D \u2212 k \u222a {x\u2212} 6: end for 7: end for 8: return D\u2212 = {D\u22121 , D \u2212 2 , . . . , D \u2212 K}\nseen class. To generate boundary samples, we further require that the generated samples are close to the seen class data. Therefore, it is natural to consider that the distance between the generated sample and the original data set Dk should be small in a measure, which is enforced by the penalty term as\nP1(x,Dk) = max{0, argminx\u2032\u2208Dk dist(x, x \u2032)\u2212 C1} (5)\nwhere the radius parameter C1 is a positive constant, and dist(x, x\u2032) is a distance measure that can be Euclidean distance or other distance measures.\nFurthermore, since ASG generates samples one by one, Eq.(4) and Eq.(5) cannot prevent it from generating many identical samples. However, we want the generated samples to be scattered around the boundary. Therefore, we force the generated samples to be different, i.e., a newly generated sample should be far away from the previously generated samples, which is expressed as\nP2(x,D \u2212 k ) = max{0, C2 \u2212 argminx\u2032\u2208D\u2212k dist(x, x \u2032)} (6)\nHere the radius parameter C2 is a positive constant. Combining the loss and the penalty terms, the overall objective function is\nargmin x\nPD(x;Dk, D \u2212 k \u222a{x})+\u03bb1P1(x,Dk)+\u03bb2P2(x,D \u2212 k )\n(7) where \u03bb1 and \u03bb2 are hyper-parameters of the penalty terms. Note that our penalty terms aims only at are pushing samples out of the radius distance, the hyper-parameters will not have a great impact on the learning result.\nNote that Eq.(7) is non-convex, particularly when models other than neural networks are considered. It is hard to rely on the gradient-based method. Thus, we employ the derivativefree method to solve the optimization. Since most derivativefree methods are generally applicable, we denote such an algorithm as Opt in Algorithm 1. The concrete algorithm will be disclosed in the experiment section.\nAlgorithm 1 shows the overall procedure of generating negative instances. The algorithm takes input of labeled training dataset D and parameter T to indicate the number of instances to be generated per class. For each class\nk \u2208 1, 2, ...,K, it generates a set of instances, D\u2212k , in turn, and the instances are obtained by optimizing Eq.(7).\nGenerate Seen Class Instances On the other hand, if the class k is a rare class that has only a small number of samples, the trained classification model may be inaccurate. Our sample generation method can also be used to generate positive/seen class data, in order to improve the prediction accuracy. We only need to change the optimization function in step 6 of Algorithm 1 to complete this goal.\nFor the goal of generating a seen class instance, the loss is argmaxx PD(x;Dk, D + k \u222a {x}) (8) where D+k is the previously generated positive data set. Again, we want to generate scattered samples, thus we force the generated instance to be distant to the previously generated ones, by the penalty: P3(x,D\n+ k ) = max{0, C3 \u2212 argminx\u2032\u2208D+k dist(x, x \u2032)} (9) where the dist is a distance measure function, and C3 is a positive constant. Then the overall objective is:\nargmaxx PD(x;Dk, D + k \u222a {x})\u2212 \u03b7P3(x,D + k ) (10) where \u03b7 is the coefficient of regular entry P3. Let the Eq.(10) replace the Eq.(7) in the step 6 of Algorithm 1, the D+k set can be obtained for each class k.\nOverall Procedure After the data of unseen and seen classes are generated, it is straightforward to train a classifier for distinguishing between them. In ASG, we prefer to train such classifier for each class, i.e., train foock from Dk \u222aD + k and D \u2212 k .\nAnother issue needs to be considered is the learning capacity of the discriminator L. Note that when the capacity is too high, every generated sample can be discriminated from the original data; while when the capacity is too low, the boundary of the seen classes cannot be well captured. Unlike unsupervised learning, in the OCC problem we have some data of the seen classes. Using this data, we can fine tune the hyper-parameter (e.g. the structure of a neural network) of the learning algorithm for a proper capacity.\nOverall, given any learning algorithm L, the procedure of the ASG framework is as follows: (1) On the seen data D, fine tune the hyper-parameters of L; (2) Generate negative instancesD\u2212 of the seen classes by Algorithm 1, as well as positive instances D+ if necessary; (3) For each class k, train a classifier foock from positive data Dk \u222aD+k and negative data D \u2212 k by L; In the prediction stage, for a test instance x, it is tested by each of foock . If all f ooc k classify x as negative, then x is predicted as a novel class. Otherwise, x is predicted as the class k with foock has the highest confident."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Comparison Methods", "text": "As discussed above, the hyper-parameters of the learning algorithm should be determined at the first step. In the experiment, we employ SVM with RBF kernel as the learning algorithm and for the discriminant model too. And the binary\nclassifier trained after the generation is also set as RBF-SVM. In order to verify the validity of the ASG framework, we conducted experiments on several benchmark datasets, comparing with:\nOC-SVM: One-Class SVM [Scho\u0308lkopf et al., 2001] is a state-of-the-art outlier detector [Ma and Perkins, 2003], which computes a binary function supposed to capture regions in the input space where the probability density lives.\nMOC-SVM: The OC-SVM can hardly find local outliers since it seeks for a hyperplane to separate the data and the origin in essence. Therefore, for this comparison method, we train multiple one-class SVMs, one OC-SVM for each seen class in the training set, for outlier detection.\n1-vs-Set: 1-vs-Set Machine [Scheirer et al., 2013] introduces extra decision boundaries for the seen classes in order to minimize the risk over open space.\nOVR-SVM: One-vs-rest SVM is a powerful multi-class classification scheme [Rifkin and Klautau, 2004]. In the original OVR-SVM, an instance x is predicted as category y, where y = argmaxk=1,...,K fk(x) where fk is a binary SVM trained for class k. In order to adapt OVR-SVM for the prediction of open-category problem, we let the x be predicted as class y only when maxkf(x) > 0, otherwise x is considered to be the instance of novel class.\nNNO: Nearest Non-Outlier(NNO) [Bendale and Boult, 2015] is a theoretical guaranteed method which deal with the open world recognition by introduce a measurable function in Nearest Class Mean method.\nIn experiments, we use the implementations of OC-SVM, MOC-SVM and OVR-SVM in the LIBSVM software [Chang and Lin, 2011], and the implementation of 1-vs-Set Machine from the code released by the authors. And we implement the NNO by ourselves due to the miss of the code. The coefficient C in SVM is determined by cross validation on the training dataset using the original OVR-SVM. The width of Gaussian kernel \u03b3 is fixed to 1/d, where d is the size of the feature. For the ASG algorithm, the distance measure dist is set to be the Euclidean distance. When generating negative samples of class k, the parameter C1,C2 is set as minx1,x2\u2208Dk;x1 6=x2 dist(x1, x2), \u03bb1 and \u03bb2 are both set to 0.1, and T = 200. When generating positive samples for class k, the parameter C3 is also set to minx1,x2\u2208Dk;x1 6=x2 dist(x1, x2), \u03b7 is set to 0.3, and also T = 200. For the derivative-free optimization method, we employ the recently developed RACOS algorithm1 [Yu et al., 2016] with default parameters."}, {"heading": "4.2 Results", "text": "Three Moons: We first illustrate the behaviors of ASG using a 2-dimensional synthetic data as in Figure 1. The dataset consists of three classes, but only two of them are seen in the training stage, as in Figure 1 (a). With the generated data, the boundary of the seen classes can be well characterized, as in Figure 1 (b). The decision boundaries in Figure 1 (c) show that ASG can correctly exclude the unseen moon as seen data, while the classical SVM will classify the unseen moon as blue.\n1using the implementation in https://github.com/eyounx/ZOOpt\nHandwritten Digit Image Classification: We conducted the second experiment on the MNIST handwritten digit dataset. First, seen categories were randomly selected from 10 total categories and the test data contains all 10 categories. The number of training data, generated data and test data are 100, 300 and 100 for each class. We have 10 random selections for the seen categories and repeat 10 times for each se-\nlection, both mean and standard deviation are recorded. Figure 2 shows the results on 3 and 5 seen classes in training. It can be observed that ASG-SVM achieves the best performance on both configurations, while the OVR-SVM is the runner up. Besides, the 1-vs-set machine shows to outperform the OC-SVM and MOC-SVM, and the outlier detection methods, OC-SVM and MOC-SVM, produce the worst per-\nformance. Besides, the NNO seems not perform well in this dataset.\nTo break down the performance to seen and unseen data separately, we set the class 2, 3, 5 as seen categories, and training data size for each category is set to 100 and 1000, respectively. The experiments are repeated for 10 times, and both mean and standard variance of the measure are reported. The results of F1, precision and recall for both seen classes and unseen classes are shown in Table 1. For seen classes, the ASG-SVM gets the best performance on F1 and precision when the instance number for each category is set to 100 and 1000, while the recall of OVR-SVM and 1-vs-set is the highest when the number of instance is set to 100 and 1000. For unseen classes, the ASG-SVM obtains the highest performance on F1 and recall, while the precision of OVR-SVM and 1-vs-set is the best when the instance number of each class is 100 and 1000.\nTable 2 shows the improvement ratio of ASG-SVM over the comparing methods in 4 configurations of the number of seen classes and training set size. The cases in the first column represent different situations, for example, 3-100 represent there are 3 seen classes in the training data and 100 instances for each class. It can be observed that, when the training data has a smaller size, ASG-SVM has larger improvements. This mainly due to that ASG also generates samples of seen classes to improve the prediction accuracy.\nDocument Classification: In the third experiment, we conduct experiments on the 20 Newsgroups dataset, which is a popular text dataset consists of documents from 20 different topics. Note that this data set has some topics that are highly similar, such as comp.sys.ibm.pc.hardware and comp.sys.mac.hardware. When one belongs to the seen classes and the other is unseen, the classification will be quite difficult. We set the number of seen classes as 5. The number of training data, generated positive data, generated negative data and test data are set to 3000, 2000, 2000 and 3000, respectively. We randomly sample seen classes and the training data to form 20 data sets, and for each data sets, the experiment is repeat for 10 times.\nFigure 3 shows the Macro-F1 performance with the increase of test classes. It can be observed that the curves in general decreases as expected. Among these methods, ASG-SVM consistently achieves the best performance, and the OVR-SVM is the runner-up. The outlier detection methods, OC-SVM and MOC-SVM, fail in this data, which further confirms that the instance of unseen classes should not be simply treated as outliers in the OCC problem. The win/tie/loss table on all algorithm pairs is in the appendix due\nto the space limit, which will show that ASG framework is significantly better than the other methods.\nSmall Datasets: We also conducted experiments on five small datasets, where the size for each seen class is no more than 100 in the training datasets, which makes the classification tasks become more challenging.\nAs the result shown in Table 3, ASG-SVM obtains the best performance in four datasets compared to the other five methods; NNO has the best performance in the flag dataset, but can be worse than some of the comparing methods. In addition to ASG-SVM, NNO and OVR-SVM are runner-ups that have better performance in several cases, but have lower Macro-F1 in the glass dataset than MOC-SVM, where the number of each class is just 15 in the training set. Sort the algorithms with their average ranks on all data sets, the best is ASG-SVM (average rank 1.2), and the remaining are NNO (2.6), OVR-SVM (3.0), MOC-SVM (3.6), 1-vs-Set (4.6), and OC-SVM (6)."}, {"heading": "5 Conclusion", "text": "Open-category classification problem often occurs in practical problems, where a system needs to predict the data in an open environment. In this paper, we propose the ASG framework to address the problem by adversarial data generation. In experiments, we demonstrate that ASG can successfully generate boundary data around the seen classes, which makes the recognition of unseen classes can be easily done by supervised learning. On several datasets, ASG shows to be more effective than several state-of-the-art methods. In the future, besides the current SVM model, we would like to apply ASG with state-of-the-art multi-class learning method, and develop a theoretical grounded method for the OCC problem."}], "references": [{"title": "In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition", "author": ["Abhijit Bendale", "Terrance E. Boult. Towards open world recognition"], "venue": "pages 1893\u20131902, Boston, MA,", "citeRegEx": "Bendale and Boult. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "with application to active user modeling and hierarchical reinforcement learning", "author": ["Eric Brochu", "Vlad M. Cora", "Nando de Freitas. A tutorial on bayesian optimization of expensive cost functions"], "venue": "CoRR, abs/1012.2599,", "citeRegEx": "Brochu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, 2(3):27", "citeRegEx": "Chang and Lin. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C Chow"], "venue": "IEEE Transactions on Information Theory, 16(1):41\u201346", "citeRegEx": "Chow. 1970", "shortCiteRegEx": null, "year": 1970}, {"title": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence", "author": ["Qing Da", "Yang Yu", "Zhi-Hua Zhou. Learning with augmented class by exploiting unlabeled data"], "venue": "pages 1760\u20131766, Quebec, Canada,", "citeRegEx": "Da et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 23rd International Conference on Machine learning", "author": ["Michael Fink", "Shai Shalev-Shwartz", "Yoram Singer", "Shimon Ullman. Online multiclass learning by interclass hypothesis sharing"], "venue": "pages 313\u2013320, Pittsburgh, PA,", "citeRegEx": "Fink et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Genetic Algorithms in Search Optimization and Machine Learning", "author": ["David E. Goldberg"], "venue": "AddisonWesley,", "citeRegEx": "Goldberg. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Generative adversarial nets", "author": ["Goodfellow"], "venue": null, "citeRegEx": "Goodfellow,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow", "year": 2014}, {"title": "Artificial Intelligence Review", "author": ["Victoria J Hodge", "Jim Austin. A survey of outlier detection methodologies"], "venue": "22(2):85\u2013126,", "citeRegEx": "Hodge and Austin. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Finding rare classes: Active learning with generative and discriminative models", "author": ["Timothy M Hospedales", "Shaogang Gong", "Tao Xiang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 25(2):374\u2013386,", "citeRegEx": "Hospedales et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "From N to N+1: Multiclass transfer incremental learning", "author": ["I. Kuzborskij", "F. Orabona", "B. Caputo"], "venue": "Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, pages 3358\u2013 3365, Portland, OR", "citeRegEx": "Kuzborskij et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Turing learning: a metric-free approach to inferring behavior and its application to swarms", "author": ["Wei Li", "Melvin Gauci", "Roderich Gro"], "venue": "Swarm Intelligence, 10(3):211\u2013243,", "citeRegEx": "Li et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of the 2003 IEEE International Joint Conference on Neural Networks", "author": ["Junshui Ma", "Simon Perkins. Time-series novelty detection using one-class support vector machines"], "venue": "volume 3, pages 1741\u20131745,", "citeRegEx": "Ma and Perkins. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Learn++ NC: Combining ensemble of classifiers with dynamically weighted consult-and-vote for efficient incremental learning of new classes", "author": ["M.D. Muhlbaier", "A. Topalis", "R. Polikar"], "venue": "IEEE Transactions on Neural Networks, 20(1):152\u2013168", "citeRegEx": "Muhlbaier et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning", "author": ["R\u00e9mi Munos"], "venue": "Foundations and Trends in Machine Learning, 7(1):1\u2013129,", "citeRegEx": "Munos. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin et al", "2016] Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Zero-shot learning with semantic output codes", "author": ["Palatucci et al", "2009] Mark Palatucci", "Dean Pomerleau", "Geoffrey E. Hinton", "Tom M. Mitchell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "editors", "author": ["Dan Pelleg", "Andrew W. Moore. Active learning for anomaly", "rare-category detection. In L.K. Saul", "Y. Weiss", "L. Bottou"], "venue": "Advances in Neural Information Processing Systems 17, pages 1073\u20131080.", "citeRegEx": "Pelleg and Moore. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "In Handbook of Face Recognition", "author": ["P Jonathon Phillips", "Patrick Grother", "Ross Micheals. Evaluation methods in face recognition"], "venue": "pages 551\u2013574.", "citeRegEx": "Phillips et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Journal of Machine Learning Research", "author": ["Ryan Rifkin", "Aldebaro Klautau. In defense of one-vs-all classification"], "venue": "5:101\u2013141,", "citeRegEx": "Rifkin and Klautau. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Walter J Scheirer", "Terrance E Boult", "Anderson de Rezende Rocha", "Archana Sapkota. Toward open set recognition"], "venue": "35(7):1757\u20131772,", "citeRegEx": "Scheirer et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural computation", "author": ["Bernhard Sch\u00f6lkopf", "John C Platt", "John Shawe-Taylor", "Alex J Smola", "Robert C Williamson. Estimating the support of a high-dimensional distribution"], "venue": "13(7):1443\u20131471,", "citeRegEx": "Sch\u00f6lkopf et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Latent embeddings for zero-shot classification", "author": ["Xian et al", "2016] Yongqin Xian", "Zeynep Akata", "Gaurav Sharma", "Quynh N. Nguyen", "Matthias Hein", "Bernt Schiele"], "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "author": ["Yang Yu", "Hong Qian", "Yi-Qi Hu. Derivative-free optimization via classification"], "venue": "pages 2286\u20132292, Phoenix, AZ,", "citeRegEx": "Yu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge-based systems", "author": ["Zhi-Hua Zhou", "Zhao-Qian Chen. Hybrid decision tree"], "venue": "15(8):515\u2013528,", "citeRegEx": "Zhou and Chen. 2002", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "In class incremental-learning [Fink et al., 2006; Muhlbaier et al., 2009; Kuzborskij et al., 2013], new classes", "startOffset": 30, "endOffset": 98}, {"referenceID": 13, "context": "In class incremental-learning [Fink et al., 2006; Muhlbaier et al., 2009; Kuzborskij et al., 2013], new classes", "startOffset": 30, "endOffset": 98}, {"referenceID": 10, "context": "In class incremental-learning [Fink et al., 2006; Muhlbaier et al., 2009; Kuzborskij et al., 2013], new classes", "startOffset": 30, "endOffset": 98}, {"referenceID": 3, "context": "In learning with a rejection option [Chow, 1970], the classifier rejects to recognize an instance if its confidence is low, which, however, does not take open-environment into consideration.", "startOffset": 36, "endOffset": 48}, {"referenceID": 8, "context": "Outlier detection techniques [Hodge and Austin, 2004] could be employed if we treat unseen class instances as outliers.", "startOffset": 29, "endOffset": 53}, {"referenceID": 20, "context": "For examples, in [Scheirer et al., 2013] the open-set cost is considered, but it is hard to measure that cost without seeing open-set data; in [Da et al.", "startOffset": 17, "endOffset": 40}, {"referenceID": 4, "context": ", 2013] the open-set cost is considered, but it is hard to measure that cost without seeing open-set data; in [Da et al., 2014], a set of unlabeled data of all classes is employed, but sufficient unlabeled data from all potential classes may not always available.", "startOffset": 110, "endOffset": 127}, {"referenceID": 24, "context": "The incremental learning requires a proper adaptation of traditional machine learning approaches to deal with the dynamic and open environment, in addition, class-incremental learning(C-IL) [Zhou and Chen, 2002] is an important branch of it, which mainly concerned with the addition of new classes.", "startOffset": 190, "endOffset": 211}, {"referenceID": 5, "context": "In [Fink et al., 2006; Kuzborskij et al., 2013], each new class has a binary classifier which distinguishes between existing categories and new categories, and the new category in the classifier shares the hypothesis with the existing category to train.", "startOffset": 3, "endOffset": 47}, {"referenceID": 10, "context": "In [Fink et al., 2006; Kuzborskij et al., 2013], each new class has a binary classifier which distinguishes between existing categories and new categories, and the new category in the classifier shares the hypothesis with the existing category to train.", "startOffset": 3, "endOffset": 47}, {"referenceID": 4, "context": "To address this limitation, a new method is introduced in [Da et al., 2014], which processes new categories by exploring unlabeled instances, this requires reliable unlabeled data, but the data availability and quality is difficult to guarantee in practice.", "startOffset": 58, "endOffset": 75}, {"referenceID": 18, "context": "In [Phillips et al., 2011], the main concern is the operation of the threshold, only instances where the confidence is above the threshold are classified as seen classes.", "startOffset": 3, "endOffset": 26}, {"referenceID": 20, "context": "In [Scheirer et al., 2013; Bendale and Boult, 2015], both the new decision boundary and the risk over open space are considered to limit the regions for seen categories.", "startOffset": 3, "endOffset": 51}, {"referenceID": 0, "context": "In [Scheirer et al., 2013; Bendale and Boult, 2015], both the new decision boundary and the risk over open space are considered to limit the regions for seen categories.", "startOffset": 3, "endOffset": 51}, {"referenceID": 8, "context": "The outlier detection problem [Hodge and Austin, 2004] requires the identification of anomaly instances from a given data set.", "startOffset": 30, "endOffset": 54}, {"referenceID": 17, "context": "The class discovery problem tries to identify the instances of rare categories which are not known in advance, but are known to exist in the training data [Pelleg and Moore, 2005; Hospedales et al., 2013].", "startOffset": 155, "endOffset": 204}, {"referenceID": 9, "context": "The class discovery problem tries to identify the instances of rare categories which are not known in advance, but are known to exist in the training data [Pelleg and Moore, 2005; Hospedales et al., 2013].", "startOffset": 155, "endOffset": 204}, {"referenceID": 11, "context": "The adversarial learning employs a generative model and a discriminative model, where the generative model learns to generate instances that can fool the discriminative model as a non-generated instance, which is also called Turing Learning in [Li et al., 2016].", "startOffset": 244, "endOffset": 261}, {"referenceID": 6, "context": "Ancient derivative-free optimization methods include representatives such as genetic algorithms [Goldberg, 1989], which are mostly heuristic methods.", "startOffset": 96, "endOffset": 112}, {"referenceID": 1, "context": "Recently, the derivativefree optimization methods have made significant progress in both theoretical foundation and practical usage, including Bayesian optimization methods [Brochu et al., 2010], optimistic optimization methods [Munos, 2014], and modelbased optimization [Yu et al.", "startOffset": 173, "endOffset": 194}, {"referenceID": 14, "context": ", 2010], optimistic optimization methods [Munos, 2014], and modelbased optimization [Yu et al.", "startOffset": 41, "endOffset": 54}, {"referenceID": 23, "context": ", 2010], optimistic optimization methods [Munos, 2014], and modelbased optimization [Yu et al., 2016].", "startOffset": 84, "endOffset": 101}, {"referenceID": 21, "context": "In order to verify the validity of the ASG framework, we conducted experiments on several benchmark datasets, comparing with: OC-SVM: One-Class SVM [Sch\u00f6lkopf et al., 2001] is a state-of-the-art outlier detector [Ma and Perkins, 2003], which computes a binary function supposed to capture regions in the input space where the probability density lives.", "startOffset": 148, "endOffset": 172}, {"referenceID": 12, "context": ", 2001] is a state-of-the-art outlier detector [Ma and Perkins, 2003], which computes a binary function supposed to capture regions in the input space where the probability density lives.", "startOffset": 47, "endOffset": 69}, {"referenceID": 20, "context": "1-vs-Set: 1-vs-Set Machine [Scheirer et al., 2013] introduces extra decision boundaries for the seen classes in order to minimize the risk over open space.", "startOffset": 27, "endOffset": 50}, {"referenceID": 19, "context": "OVR-SVM: One-vs-rest SVM is a powerful multi-class classification scheme [Rifkin and Klautau, 2004].", "startOffset": 73, "endOffset": 99}, {"referenceID": 0, "context": "NNO: Nearest Non-Outlier(NNO) [Bendale and Boult, 2015] is a theoretical guaranteed method which deal with the open world recognition by introduce a measurable function in Nearest Class Mean method.", "startOffset": 30, "endOffset": 55}, {"referenceID": 2, "context": "In experiments, we use the implementations of OC-SVM, MOC-SVM and OVR-SVM in the LIBSVM software [Chang and Lin, 2011], and the implementation of 1-vs-Set Machine from the code released by the authors.", "startOffset": 97, "endOffset": 118}, {"referenceID": 23, "context": "For the derivative-free optimization method, we employ the recently developed RACOS algorithm1 [Yu et al., 2016] with default parameters.", "startOffset": 95, "endOffset": 112}], "year": 2017, "abstractText": "In real-world classification tasks, it is difficult to collect training samples from all possible categories of the environment. Therefore, when an instance of an unseen class appears in the prediction stage, a robust classifier should be able to tell that it is from an unseen class, instead of classifying it to be any known category. In this paper, adopting the idea of adversarial learning, we propose the ASG framework for open-category classification. ASG generates positive and negative samples of seen categories in the unsupervised manner via an adversarial learning strategy. With the generated samples, ASG then learns to tell seen from unseen in the supervised manner. Experiments performed on several datasets show the effectiveness of ASG.", "creator": "LaTeX with hyperref package"}}}