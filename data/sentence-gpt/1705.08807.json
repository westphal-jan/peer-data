{"id": "1705.08807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "When Will AI Exceed Human Performance? Evidence from AI Experts", "abstract": "Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI.\n\n\n\n\n\n\nThe views expressed in this article belong to the authors and do not necessarily represent the views of AI. In the context of the study, the author assumes that humans, AI and humans must be educated on the value and limits of technology. This can be interpreted as saying that we cannot accept technological advances as merely technological advances; that we must consider the value of human life as a whole. We have been making strides in the past several years to make sure that our society is not a place where people do not feel that way.\nThe survey was conducted from January-November 2013.\nResults\n\n\n\n\n\n\nThe findings suggest that technology is not a safe future, and that it can become less powerful when it comes to learning from past mistakes, and that it is not a safe future. In general, human error could mean a failure in a way that is a result of future mistakes. For example, a recent study found that most American children were less likely to read books about social science or technology than children in schools and had fewer books about technology in high school. This result has serious consequences in children and adults of all ages.\nThe researchers conclude that even in the early 1970s, this lack of education, coupled with strong expectations that education would lead to improved life, is not a good way to change society. The data suggests that as the technology advances, the value of technology has increased to unprecedented levels.\nThe authors recommend that we have to make sure that technology is maintained in a balanced and equitable way. This should also be a major concern in the future. The authors suggest that people who are able to afford to go to colleges and colleges, which are traditionally run by individuals and their own companies, should be willing to pay a premium for a certain level of education.\nThe authors warn that it can be difficult for young people to pursue an education in which they are not educated. They also advise that their education should be based on a combination of research and knowledge acquired from the past, including, for example, education, and knowledge gained from research that has been completed in many areas of life. However, they warn that such learning is not necessary to prevent some individuals from gaining a", "histories": [["v1", "Wed, 24 May 2017 15:00:20 GMT  (744kb,D)", "http://arxiv.org/abs/1705.08807v1", null], ["v2", "Tue, 30 May 2017 21:36:40 GMT  (744kb,D)", "http://arxiv.org/abs/1705.08807v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["katja grace", "john salvatier", "allan dafoe", "baobao zhang", "owain evans"], "accepted": false, "id": "1705.08807"}, "pdf": {"name": "1705.08807.pdf", "metadata": {"source": "CRF", "title": "When Will AI Exceed Human Performance? Evidence from AI Experts", "authors": ["Katja Grace", "John Salvatier", "Allan Dafoe", "Baobao Zhang", "Owain Evans"], "emails": [], "sections": [{"heading": "Introduction", "text": "Advances in artificial intelligence (AI) will have massive social consequences. Self-driving technology might replace millions of driving jobs over the coming decade. In addition to possible unemployment, the transition will bring new challenges, such as rebuilding infrastructure, protecting vehicle cyber-security, and adapting laws and regulations [5]. New challenges, both for AI developers and policy-makers, will also arise from applications in law enforcement, military technology, and marketing [6]. To prepare for these challenges, accurate forecasting of transformative AI would be invaluable.\nSeveral sources provide objective evidence about future AI advances: trends in computing hardware [7], task performance [8], and the automation of labor [9]. The predictions of AI experts provide crucial additional information. We survey a larger and more representative sample of AI experts than any study to date [10, 11]. Our questions cover the timing of AI advances (including both practical applications of AI and the automation of various human jobs), as well as the social and ethical impacts of AI."}, {"heading": "Survey Method", "text": "Our survey population was all researchers who published at the 2015 NIPS and ICML conferences (two of the premier venues for peer-reviewed research in machine learning). A total of 352 researchers responded to our survey invitation (21% of the 1634 authors we contacted). Our questions concerned the timing of specific AI capabilities (e.g. folding laundry, language translation), superiority at specific occupations (e.g. truck driver, surgeon), superiority over humans at all tasks, and the social impacts of advanced AI. See Survey Content for details."}, {"heading": "Time Until Machines Outperform Humans", "text": "AI would have profound social consequences if all tasks were more cost effectively accomplished by machines. Our survey used the following definition:\n\u201cHigh-level machine intelligence\u201d (HLMI) is achieved when unaided machines can accomplish every task better and more cheaply than human workers.\nar X\niv :1\n70 5.\n08 80\n7v 1\n[ cs\n.A I]\n2 4\nM ay\n2 01\n7\nEach individual respondent estimated the probability of HLMI arriving in future years. Taking the mean over each individual, the aggregate forecast gave a 50% chance of HLMI occurring within 45 years and a 10% chance of it occurring within 9 years. Figure 1 displays the probabilistic predictions for a random subset of individuals, as well as the mean predictions. There is large inter-subject variation: Figure 3 shows that Asian respondents expect HLMI in 30 years, whereas North Americans expect it in 74 years.\nWhile most participants were asked about HLMI, a subset were asked a logically similar question that emphasized consequences for employment. The question defined full automation of labor as:\nwhen all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers.\nForecasts for full automation of labor were much later than for HLMI: the mean of the individual beliefs assigned a 50% probability in 122 years from now and a 10% probability in 20 years.\nRespondents were also asked when 32 \u201cmilestones\u201d for AI would become feasible. The full descriptions of the milestone are in Table S5. Each milestone was considered by a random subset of respondents (n\u226524). Respondents expected (mean probability of 50%) 20 of the 32 AI milestones to be reached within ten years. Fig. 2 displays timelines for a subset of milestones."}, {"heading": "Intelligence Explosion, Outcomes, AI Safety", "text": "The prospect of advances in AI raises important questions. Will progress in AI become explosively fast once AI research and development itself can be automated? How will high-level machine intelligence (HLMI) affect economic growth? What are the chances this will lead to extreme outcomes (either positive or negative)? What should be done to help ensure AI progress is beneficial? Table\nS4 displays results for questions we asked on these topics. Here are some key findings:\n1. Researchers believe the field of machine learning has accelerated in recent years. We asked researchers whether the rate of progress in machine learning was faster in the first or second half of their career. Sixty-seven percent (67%) said progress was faster in the second half of their career and only 10% said progress was faster in the first half. The median career length among respondents was 6 years.\n2. Explosive progress in AI after HLMI is seen as possible but improbable. Some authors have argued that once HLMI is achieved, AI systems will quickly become vastly superior to humans in all tasks [3, 12]. This acceleration has been called the \u201cintelligence explosion.\u201d We asked respondents for the probability that AI would perform vastly better than humans in all tasks two years after HLMI is achieved. The median probability was 10% (interquartile range: 1-25%). We also asked respondents for the probability of explosive global technological improvement two years after HLMI. Here the median probability was 20% (interquartile range 5-50%).\n3. HLMI is seen as likely to have positive outcomes but catastrophic risks are possible. Respondents were asked whether HLMI would have a positive or negative impact on humanity over the long run. They assigned probabilities to outcomes on a five-point scale. The median probability was 25% for a \u201cgood\u201d outcome and 20% for an \u201cextremely good\u201d outcome. By contrast, the probability was 10% for a bad outcome and 5% for an outcome described as \u201cExtremely Bad (e.g., human extinction).\u201d\n4. Society should prioritize research aimed at minimizing the potential risks of AI. Forty-eight percent of respondents think that research on minimizing the risks of AI should be prioritized by society more than the status quo (with only 12% wishing for less).\nAsians expect HLMI 44 years before North Americans Figure 3 shows big differences between individual respondents in when they predict HLMI will arrive. Both citation count and seniority were not predictive of HLMI timelines (see Fig. S1 and the results of a regression in Table S2). However, respondents from different regions had striking differences in HLMI predictions. Fig. 3 shows an aggregate prediction for HLMI of 30 years for Asian respondents and 74 years for North Americans. Fig. S1 displays a similar gap between the two countries with the most respondents in the survey: China (median 28 years) and USA (median 76 years). Similarly, the aggregate year for a 50% probability for automation of each job we asked about (including truck driver and surgeon) was predicted to be earlier by Asians than by North Americans (Table S2). Note that we used respondents\u2019 undergraduate institution as a proxy for country of origin and that many Asian respondents now study or work outside Asia."}, {"heading": "Was our sample representative?", "text": "One concern with any kind of survey is non-response bias; in particular, researchers with strong views may be more likely to fill out a survey. We tried to mitigate this effect by making the survey short (12 minutes) and confidential, and by not mentioning the survey\u2019s content or goals in our invitation email. Our response rate was 21%. To investigate possible non-response bias, we collected demographic data for both our respondents (n=406) and a random sample (n=399) of NIPS/ICML researchers who did not respond. Results are shown in Table S3. Differences between the groups in citation count, seniority, gender, and country of origin are small. While we cannot rule out non-response biases due to unmeasured variables, we can rule out large bias due to the demographic variables we measured. Our demographic data also shows that our respondents included many highly-cited researchers (mostly in machine learning but also in statistics, computer science theory, and neuroscience) and came from 43 countries (vs. a total of 52 for everyone we sampled). A majority work in academia (82%), while 21% work in industry."}, {"heading": "Discussion", "text": "Why think AI experts have any ability to foresee AI progress? In the domain of political science, a long-term study found that experts were worse than crude statistical extrapolations at predicting political outcomes [13]. AI progress, which relies on scientific breakthroughs, may appear intrinsically harder to predict. Yet there are reasons for optimism. While individual breakthroughs are unpredictable, longer term progress in R&D for many domains (including computer hardware, genomics, solar energy) has been impressively regular [14]. Such regularity is also displayed by trends [8] in AI performance in SAT problem solving, games-playing, and computer vision and could be exploited by AI experts in their predictions. Finally, it is well established that aggregating individual predictions can lead to big improvements over the predictions of a random individual [15]. Further work could use our data to make optimized forecasts. Moreover, many of the AI milestones (Fig. 2) were forecast to be achieved in the next decade, providing ground-truth evidence about the reliability of individual experts."}, {"heading": "Survey Content", "text": "We developed questions through a series of interviews with Machine Learning researchers. Our survey questions were as follows:\n1. Three sets of questions eliciting HLMI predictions by different framings: asking directly about HLMI, asking about the automatability of all human occupations, and asking about recent progress in AI from which we might extrapolate.\n2. Three questions about the probability of an \u201cintelligence explosion\u201d.\n3. One question about the welfare implications of HLMI.\n4. A set of questions about the effect of different inputs on the rate of AI research (e.g., hardware progress).\n5. Two questions about sources of disagreement about AI timelines and \u201cAI Safety.\u201d\n6. Thirty-two questions about when AI will achieve narrow \u201cmilestones\u201d.\n7. Two sets of questions on AI Safety research: one about AI systems with non-aligned goals, and one on the prioritization of Safety research in general.\n8. A set of demographic questions, including ones about how much thought respondents have given to these topics in the past. The questions were asked via an online Qualtrics survey. (The Qualtrics file will be shared to enable replication.) Participants were invited by email and were offered a financial reward for completing the survey. Questions were asked in roughly the order above and respondents received a randomized subset of questions. Surveys were completed between May 3rd 2016 and June 28th 2016.\nOur goal in defining \u201chigh-level machine intelligence\u201d (HLMI) was to capture the widely-discussed notions of \u201chuman-level AI\u201d or \u201cgeneral AI\u201d (which contrasts with \u201cnarrow AI\u201d) [3]. We consulted all previous surveys of AI experts and based our definition on that of an earlier survey [11]. Their definition of HLMI was a machine that \u201ccan carry out most human professions at least as well as a typical human.\u201d Our definition is more demanding and requires machines to be better at all tasks than humans (while also being more cost-effective). Since earlier surveys often use less demanding notions of HLMI, they should (all other things being equal) predict earlier arrival for HLMI.\nDemographic Information The demographic information on respondents and non-respondents (Table S3) was collected from public sources, such as academic websites, LinkedIn profiles, and Google Scholar profiles. Citation count and seniority (i.e. numbers of years since the start of PhD) were collected in February 2017."}, {"heading": "Elicitation of Beliefs", "text": "Many of our questions ask when an event will happen. For prediction tasks, ideal Bayesian agents provide a cumulative distribution function (CDF) from time to the cumulative probability of the event. When eliciting points on respondents\u2019 CDFs, we framed questions in two different ways, which we call \u201cfixed-probability\u201d and \u201cfixed-years\u201d. Fixed-probability questions ask by which year an event has an p% cumulative probability (for p=10%, 50%, 90%). Fixed-year questions ask for the cumulative probability of the event by year y (for y=10, 25, 50). The former framing was used in recent surveys of HLMI timelines; the latter framing is used in the psychological literature on forecasting [16, 17]. With a limited question budget, the two framings will sample different points on the CDF; otherwise, they are logically equivalent. Yet our survey respondents do not treat them as logically equivalent. We observed effects of question framing in all our prediction questions, as well as in pilot studies. Differences in these two framings have previously been documented in the forecasting literature [16, 17] but there is no clear guidance on which framing leads to more accurate predictions. Thus we simply average over the two framings when computing CDF estimates for HLMI and for tasks. HLMI predictions for each framing are shown in Fig. S2."}, {"heading": "Statistics", "text": "For each timeline probability question (see Figures 1 and 2), we computed an aggregate distribution by fitting a gamma CDF to each individual\u2019s responses using least squares and then taking the mixture distribution of all individuals. Reported medians and quantiles were computed on this summary distribution. The confidence intervals were generated by bootstrapping (clustering on respondents with 10,000 draws) and plotting the 95% interval for estimated probabilities at each year. The time-in-field and citations comparisons between respondents and non-respondents (Table S3) were done using two-tailed t-tests. The region and gender proportions were done using twosided proportion tests. The significance test for the effect of region on HLMI date (Table S2) was done using robust linear regression using the R function rlm from the MASS package to do the regression and then the f.robtest function from the sfsmisc package to do a robust F-test significance."}, {"heading": "Supplementary Figures", "text": "(a) Top 4 Undergraduate Country HLMI CDFs\nChina (n=36)\nFrance (n=16)\nIndia (n=20)\nUnited States (n=53)\n0.00\n0.25\n0.50\n0.75\n1.00\n0 25 50 75 100 Years from 2016\nPr ob\nab ili\nty o\nf H LM\nI\nTop 4 Undergrad Country HLMI CDFs (b) Time in Field Quantile HLMI CDFs\nQ[1] (n=57) Q[2] (n=40)\nQ[3] (n=55)\nQ[4] (n=48)\n0.00\n0.25\n0.50\n0.75\n1.00\n0 25 50 75 100 Years from 2016\nPr ob\nab ili\nty o\nf H LM\nI\nTime in Field Quartile HLMI CDFs\n(c) Citation Count Quartile HLMI CDFs\nQ[1] (n=53) Q[2] (n=57)\nQ[3] (n=65) Q[4] (n=49)\n0.00\n0.25\n0.50\n0.75\n1.00\n0 25 50 75 100 Years from 2016\nPr ob\nab ili\nty o\nf H LM\nI\nHLMI CDF By Citation Count Quartile\nFigure S1: Aggregate subjective probability of HLMI arrival by demographic group. Each graph curve is an Aggregate Forecasts CDF, computed using the procedure described in Figure 1 and in \u201cElicitation of Beliefs.\u201d Figure S1a shows aggregate HLMI predictions for the four countries with the most respondents in our survey. Figure S1b shows predictions grouped by quartiles for seniority (measured by time since they started a PhD). Figure S1c shows predictions grouped by quartiles for citation count. \u201cQ4\u201d indicates the top quartile (i.e. the most senior researchers or the researchers with most citations).\n0.00\n0.25\n0.50\n0.75\n1.00\n0 25 50 75 100\nYears from 2016\nP ro\nb ab\nili ty\no f\nH L\nM I\nFraming Fixed Probabilities Fixed Years Combined\nFraming CDFs\nFigure S2: Aggregate subjective probability of HLMI arrival for two framings of the question. The \u201cfixed probabilities\u201d and \u201cfixed years\u201d curves are each an aggregate forecast for HLMI predictions, computed using the same procedure as in Fig. 1. These two framings of questions about HLMI are explained in \u201cElicitation of Beliefs\u201d above. The \u201ccombined\u201d curve is an average over these two framings and is the curve used in Fig. 1."}, {"heading": "Supplementary Tables", "text": ""}, {"heading": "S1: Automation Predictions by Researcher Region", "text": "This question asked when automation of the job would become feasible, and cumulative probabilities were elicited as in the HLMI and milestone prediction questions. The definition of \u201cfull automation\u201d is given above (p.1). For the \u201cNA/Asia gap\u201d, we subtract the Asian from the N. American median estimates.\nTable S1: Median estimate (in years from 2016) for automation of human jobs by region of undergraduate institution\nQuestion Europe N. America Asia NA/Asia gap Full Automation 130.8 168.6 104.2 +64.4 Retail salesperson 13.2 10.6 10.2 +0.4 Truck driver 46.4 41.0 31.4 +9.6 Surgeon 18.8 20.2 10.0 +10.2 AI researcher 80.0 123.6 109.0 +14.6"}, {"heading": "S2: Regression of HLMI Prediction on Demographic Features", "text": "We standardized inputs and regressed the log of the median years until HLMI for respondents on gender, log of citations, seniority (i.e. numbers of years since start of PhD), question framing (\u201cfixed-probability\u201d vs. \u201cfixed-years\u201d) and region where the individual was an undergraduate. We used a robust linear regression.\nTable S2: Robust linear regression for individual HLMI predictions\nterm Estimate SE t-statistic p-value Wald F - statistic (Intercept) 3.65038 0.17320 21.07635 0.00000 458.0979 Gender = \u201cfemale\u201d -0.25473 0.39445 -0.64578 0.55320 0.3529552 log(citation_count) -0.10303 0.13286 -0.77546 0.44722 0.5802456 Seniority (years) 0.09651 0.13090 0.73728 0.46689 0.5316029 Framing = \u201cfixed_probabilities\u201d -0.34076 0.16811 -2.02704 0.04414 4.109484 Region = \u201cEurope\u201d 0.51848 0.21523 2.40898 0.01582 5.93565 Region = \u201cM.East\u201d -0.22763 0.37091 -0.61369 0.54430 0.3690532 Region = \u201cN.America\u201d 1.04974 0.20849 5.03496 0.00000 25.32004 Region = \u201cOther\u201d -0.26700 0.58311 -0.45788 0.63278 0.2291022"}, {"heading": "S3: Demographics of Respondents vs. Non-respondents", "text": "There were (n=406) respondents and (n=399) non-respondents. Non-respondents were randomly sampled from all NIPS/ICML authors who did not respond to our survey invitation. Subjects with\nmissing data for region of undergraduate institution or for gender are grouped in \u201cNA\u201d. Missing data for citations and seniority is ignored in computing averages. Statistical tests are explained in section \u201cStatistics\u201d above.\nTable S3: Demographic differences between respondents and non-respondents\nUndergraduate region\nRespondent proportion Non-respondent proportion\np-test p-value\nAsia 0.305 0.343 0.283 Europe 0.271 0.236 0.284 Middle East 0.071 0.063 0.721 North America 0.254 0.221 0.307 Other 0.015 0.013 1.000 NA 0.084 0.125 0.070\nGender Respondent proportion Non-respondent proportion p-test p-value female 0.054 0.100 0.020 male 0.919 0.842 0.001 NA 0.027 0.058 0.048\nVariable Respondent estimate Non-respondent estimate statistic p-value Citations 2740.5 4528.0 2.55 0.010856 log(Citations) 5.9 6.4 3.19 0.001490 Years in field 8.6 11.1 4.04 0.000060"}, {"heading": "S4: Survey responses on AI progress, intelligence explosions, and AI Safety", "text": "The argument by Stuart Russell, referred to in one of the questions below, can be found at www. edge.org/conversation/the-myth-of-ai#26015.\nExtremely good On balance good Neutral On balance bad Extremely bad (e.g human extinction)\nChance HLMI has positive or negative long run impact on humanity (median answers)\n20% 25% 20% 10% 5%\n10% chance 50% chance 90% chance\nTime until 'full automation of labor' 50 years 100 years 200 years\nFirst half (decelerating) About equal Second half (accelerating)\nProgress faster in 1st or 2nd half of your career?\n11% 24% 65%\n2 years after 30 years after Chance global technological progress dramatically increases after HLMI 20% 80%\nQuite likely (81-100%)\nLikely (61-80%)\nAbout even (41-60%)\nUnlikely (21-40%) Quite unlikely (0-20%)\nChance intelligence explosion argument is broadly correct\n12% 17% 21% 24% 26%\nNo, not a real problem.\nNo, not an important problem.\nYes, a moderately important problem.\nYes, an important problem.\nYes, among the most important\nproblems in the field.\nDoes Stuart Russell's argument for why highly advanced AI might pose a risk point at an important problem?\n11% 19% 31% 34% 5%\nMuch less valuable Less valuable As valuable as other problems More valuable Much more valuable\nValue of working on this problem now, compared to other problems in the field 22% 41% 28% 7% 1.4%\nMuch easier Easier As hard as\nother problems Harder Much harder Difficulty of problem, relative to other problems in the field 7% 19% 42% 23% 10%\nMuch less Less About the same\nas it is now More Much more How much should society prioritize 'AI Safety Research'? (included capabilities vs. minimizing potential risks definition) 5% 6% 41% 35% 12%\nVery little A little A moderate\namount A lot A great deal How much have you thought about when HLMI (or similar) will be developed? 6% 27% 28% 31% 8%\nTable S4: Median survey responses for AI progress and safety questions"}, {"heading": "S5: Description of AI Milestones", "text": "The timelines in Figure 2 are based on respondents\u2019 predictions about the achievement of various milestones in AI. Beliefs were elicited in the same way as for HLMI predictions (see \u201cElicitation of Beliefs\u201d above). We chose a subset of all milestones to display in Figure 2 based on which milestones could be accurately described with a short label.\nTable S5: Descriptions of AI Milestones\nMilestone Name Description n In Fig. 2 median (years) Translate New Language with \u2019Rosetta Stone\u2019\nTranslate a text written in a newly discovered language into English as well as a team of human experts, using a single other document in both languages (like a Rosetta stone). Suppose all of the words in the text can be found in the translated document, and that the language is a difficult one. 35 16.6\nTranslate Speech Based on Subtitles Translate speech in a new language given only unlimited films with subtitles in the new language. Suppose the system has access to training data for other languages, of the kind used now (e.g., same text in two languages for many languages and films with subtitles in many languages). 38 10 Translate (vs. amateur human)\nPerform translation about as good as a human who is fluent in both languages but unskilled at translation, for most types of text, and for most popular languages (including languages that are known to be difficult, like Czech, Chinese and Arabic). 42 X 8\nTelephone Banking Operator Provide phone banking services as well as human operators can, without annoying customers more than humans. This includes many one-off tasks, such as helping to order a replacement bank card or clarifying how to use part of the bank website to a customer. 31 X 8.2\nMake Novel Categories Correctly group images of previously unseen objects into classes, after training on a similar labeled dataset containing completely different classes. The classes should be similar to the ImageNet classes. 29 7.4 One-Shot Learning One-shot learning: see only one labeled image of a new object, and then be able to recognize the object in real world scenes, to the extent that a typical human can (i.e. including in a wide variety of settings). For example, see only one image of a platypus, and then be able to recognize platypuses in nature photos. The system may train on labeled images of other objects. Currently, deep networks often need hundreds of examples in classification tasks[1], but there has been work on one-shot learning for both classification[2] and generative tasks[3]. [1] Lake et al. (2015). Building Machines That Learn and Think Like People [2] Koch (2015) Siamese Neural Networks for One-Shot Image Recognition [3] Rezende et al. (2016). One-Shot Generalization in Deep Generative Models 32 9.4 Generate Video from New Direction\nSee a short video of a scene, and then be able to construct a 3D model of the scene good enough to create a realistic video of the same scene from a substantially different angle. For example, constructing a short video of walking through a house from a video taking a very different path through the house. 42 11.6\nTranscribe Speech Transcribe human speech with a variety of accents in a noisy environment as well as a typical human can. 33 X 7.8 Read Text Aloud (text-tospech)\nTake a written passage and output a recording that can\u2019t be distinguished from a voice actor, by an expert listener. 43 X 9\nMath Research Routinely and autonomously prove mathematical theorems that are publishable in top mathematics journals today, including generating the theorems to prove. 31 X 43.4 Putnam Math Competition Perform as well as the best human entrants in the Putnam competition\u2014a math contest whose questions have known solutions, but which are difficult for the best young mathematicians. 45 X 33.8 Go (same training as human) Defeat the best Go players, training only on as many games as the best Go players have played. For reference, DeepMind\u2019s AlphaGo has probably played a hundred million games of self-play, while Lee Sedol has probably played 50,000 games in his life[1]. [1] Lake et al. (2015). Building Machines That Learn and Think Like People 42 X 17.6 Starcraft Beat the best human Starcraft 2 players at least 50 Starcraft 2 is a real time strategy game characterized by:\n\u2022 Continuous time play\n\u2022 Huge action space\n\u2022 Partial observability of enemies\n\u2022 Long term strategic play, e.g. preparing for and then hiding surprise attacks.\n24 X 6\nQuick Novice Play at Random Game Play a randomly selected computer game, including difficult ones, about as well as a human novice, after playing the game less than 10 minutes of game time. The system may train on other games. 44 12.4\nAngry Birds Play new levels of Angry Birds better than the best human players. Angry Birds is a game where players try to efficiently destroy 2D block towers with a catapult. For context, this is the goal of the IJCAI Angry Birds AI competition. 39 X 3 All Atari Games Outperform professional game testers on all Atari games using no gamespecific knowledge. This includes games like Frostbite, which require planning to achieve sub-goals and have posed problems for deep Q-networks[1][2]. [1] Mnih et al. (2015). Human-level control through deep reinforcement learning. [2] Lake et al. (2015). Building Machines That Learn and Think Like People 38 X 8.8 Novice Play at half of Atari Games in 20 Minutes\nOutperform human novices on 50% of Atari games after only 20 minutes of training play time and no game specific knowledge. For context, the original Atari playing deep Q-network outperforms professional game testers on 47% of games[1], but used hundreds of hours of play to train[2]. [1] Mnih et al. (2015). Human-level control through deep reinforcement learning. [2] Lake et al. (2015). Building Machines That Learn and Think Like People 33 6.6\nFold Laundry Fold laundry as well and as fast as the median human clothing store employee. 30 X 5.6 5km Race in City (bipedal robot vs. human)\nBeat the fastest human runners in a 5 kilometer race through city streets using a bipedal robot body. 28 X 11.8\nAssemble any LEGO Physically assemble any LEGO set given the pieces and instructions, using non- specialized robotics hardware. For context, Fu 2016[1] successfully joins single large LEGO pieces using model based reinforcement learning and online adaptation. [1] Fu et al. (2016). OneShot Learning of Manipulation Skills with Online Dynamics Adaptation and Neural Network Priors 35 X 8.4 Learn to Sort Big Numbers Without Solution Form\nLearn to efficiently sort lists of numbers much larger than in any training set used, the way Neural GPUs can do for addition[1], but without being given the form of the solution. For context, Neural Turing Machines have not been able to do this[2], but Neural Programmer-Interpreters[3] have been able to do this by training on stack traces (which contain a lot of information about the form of the solution). [1] Kaiser & Sutskever (2015). Neural GPUs Learn Algorithms [2] Zaremba & Sutskever (2015). Reinforcement Learning Neural Turing Machines [3] Reed & de Freitas (2015). Neural ProgrammerInterpreters 44 6.2\nPython Code for Simple Algorithms Write concise, efficient, human-readable Python code to implement simple algorithms like quicksort. That is, the system should write code that sorts a list, rather than just being able to sort lists. Suppose the system is given only:\n\u2022 A specification of what counts as a sorted list\n\u2022 Several examples of lists undergoing sorting by quicksort\n36 8.2\nAnswer Factoid Questions via Internet Answer any \u201ceasily Googleable\u201d factoid questions posed in natural language better than an expert on the relevant topic (with internet access), having found the answers on the internet. Examples of factoid questions:\n\u2022 \u201cWhat is the poisonous substance in Oleander plants?\u201d\n\u2022 \u201cHow many species of lizard can be found in Great Britain?\u201d\n46 7.2\nAnswer Open-Ended Factual Questions via Internet Answer any \u201ceasily Googleable\u201d factual but open ended question posed in natural language better than an expert on the relevant topic (with internet access), having found the answers on the internet. Examples of open ended questions:\n\u2022 \u201cWhat does it mean if my lights dim when I turn on the microwave?\u201d\n\u2022 \u201cWhen does home insurance cover roof replacement?\"\n38 9.8\nAnswer Questions Without Definite Answers Give good answers in natural language to factual questions posed in natural language for which there are no definite correct answers. For example: \u201cWhat causes the demographic transition?\u201d, \u201cIs the thylacine extinct?\u201d, \u201cHow safe is seeing a chiropractor?\u201d 47 10 High School Essay Write an essay for a highschool history class that would receive high grades and pass plagiarism detectors. For example answer a question like \u201cHow did the whaling industry affect the industrial revolution?\u201d 42 X 9.6 Generate Top 40 Pop Song Compose a song that is good enough to reach the US Top 40. The system should output the complete song as an audio file. 38 X 11.4 Produce a Song Indistinguishable from One by a Specific Artist\nProduce a song that is indistinguishable from a new song by a particular artist, e.g., a song that experienced listeners can\u2019t distinguish from a new song by Taylor Swift. 41 10.8\nWrite New York Times BestSeller Write a novel or short story good enough to make it to the New York Times best-seller list. 27 X 33 Explain Own Actions in Games\nFor any computer game that can be played well by a machine, explain the machine\u2019s choice of moves in a way that feels concise and complete to a layman. 38 X 10.2\nWorld Series of Poker Play poker well enough to win the World Series of Poker. 37 X 3.6 Output Physical Laws of Virtual World\nAfter spending time in a virtual world, output the differential equations governing that world in symbolic form. For example, the agent is placed in a game engine where Newtonian mechanics holds exactly and the agent is then able to conduct experiments with a ball and output Newton\u2019s laws of motion. 52 14.8"}, {"heading": "Acknowledgments", "text": "We thank Connor Flexman for collecting demographic information. We also thank Nick Bostrom for inspiring this work, and Michael Webb and Andreas Stuhlm\u00fcller for helpful comments. We thank the Future of Humanity Institute (Oxford), the Future of Life Institute, and the Open Philanthropy Project for supporting this work."}], "references": [{"title": "One hundred year study on artificial intelligence: Report of the 2015-2016 study panel", "author": ["Peter Stone", "Rodney Brooks", "Erik Brynjolfsson", "Ryan Calo", "Oren Etzioni", "Greg Hager", "Julia Hirschberg", "Shivaram Kalyanakrishnan", "Ece Kamar", "Sarit Kraus"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "The Master Algorithm : How the Quest for the Ultimate Learning Machine Will Remake", "author": ["Pedro Domingos"], "venue": "Our World. Basic Books,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies", "author": ["Erik Brynjolfsson", "Andrew McAfee"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Robotics and the lessons of cyberlaw", "author": ["Ryan Calo"], "venue": "California Law Review,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Self-driving cars: Disruptive or incremental", "author": ["Tao Jiang", "Srdjan Petrovic", "Uma Ayyer", "Anand Tolani", "Sajid Husain"], "venue": "Applied Innovation Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Two centuries of productivity growth in computing", "author": ["William D. Nordhaus"], "venue": "The Journal of Economic History,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Algorithmic progress in six domains", "author": ["Katja Grace"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Race Against the Machine: How the Digital Revolution Is Accelerating Innovation, Driving Productivity, and Irreversibly Transforming Employment and the Economy", "author": ["Erik Brynjolfsson", "Andrew McAfee"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "How long until human-level ai? results from an expert assessment", "author": ["Seth D. Baum", "Ben Goertzel", "Ted G. Goertzel"], "venue": "Technological Forecasting and Social Change,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Speculations concerning the first ultraintelligent machine", "author": ["Irving John Good"], "venue": "Advances in computers,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1966}, {"title": "Expert political judgment: How good is it? How can we know", "author": ["Philip Tetlock"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "How predictable is technological progress", "author": ["J Doyne Farmer", "Fran\u00e7ois Lafond"], "venue": "Research Policy,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "The good judgment project: A large scale test", "author": ["Lyle Ungar", "Barb Mellors", "Ville Satop\u00e4\u00e4", "Jon Baron", "Phil Tetlock", "Jaime Ramos", "Sam Swift"], "venue": "Technical report, Association for the Advancement of Artificial Intelligence Technical Report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Eliciting and modeling probability forecasts of continuous quantities", "author": ["Joe W. Tidwell", "Thomas S. Wallsten", "Don A. Moore"], "venue": "Paper presented at the 27th Annual Conference of Society for Judgement and Decision Making,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Abstract Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3].", "startOffset": 149, "endOffset": 158}, {"referenceID": 1, "context": "Abstract Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3].", "startOffset": 149, "endOffset": 158}, {"referenceID": 2, "context": "Abstract Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3].", "startOffset": 149, "endOffset": 158}, {"referenceID": 3, "context": "To adapt public policy, we need to better anticipate these advances [4, 5].", "startOffset": 68, "endOffset": 74}, {"referenceID": 4, "context": "To adapt public policy, we need to better anticipate these advances [4, 5].", "startOffset": 68, "endOffset": 74}, {"referenceID": 4, "context": "In addition to possible unemployment, the transition will bring new challenges, such as rebuilding infrastructure, protecting vehicle cyber-security, and adapting laws and regulations [5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 5, "context": "New challenges, both for AI developers and policy-makers, will also arise from applications in law enforcement, military technology, and marketing [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "Several sources provide objective evidence about future AI advances: trends in computing hardware [7], task performance [8], and the automation of labor [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Several sources provide objective evidence about future AI advances: trends in computing hardware [7], task performance [8], and the automation of labor [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Several sources provide objective evidence about future AI advances: trends in computing hardware [7], task performance [8], and the automation of labor [9].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "We survey a larger and more representative sample of AI experts than any study to date [10, 11].", "startOffset": 87, "endOffset": 95}, {"referenceID": 2, "context": "Some authors have argued that once HLMI is achieved, AI systems will quickly become vastly superior to humans in all tasks [3, 12].", "startOffset": 123, "endOffset": 130}, {"referenceID": 10, "context": "Some authors have argued that once HLMI is achieved, AI systems will quickly become vastly superior to humans in all tasks [3, 12].", "startOffset": 123, "endOffset": 130}, {"referenceID": 11, "context": "Why think AI experts have any ability to foresee AI progress? In the domain of political science, a long-term study found that experts were worse than crude statistical extrapolations at predicting political outcomes [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": "While individual breakthroughs are unpredictable, longer term progress in R&D for many domains (including computer hardware, genomics, solar energy) has been impressively regular [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 7, "context": "Such regularity is also displayed by trends [8] in AI performance in SAT problem solving, games-playing, and computer vision and could be exploited by AI experts in their predictions.", "startOffset": 44, "endOffset": 47}, {"referenceID": 13, "context": "Finally, it is well established that aggregating individual predictions can lead to big improvements over the predictions of a random individual [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "[1] Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Pedro Domingos.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Nick Bostrom.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Erik Brynjolfsson and Andrew McAfee.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Ryan Calo.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Tao Jiang, Srdjan Petrovic, Uma Ayyer, Anand Tolani, and Sajid Husain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] William D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Katja Grace.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Erik Brynjolfsson and Andrew McAfee.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Seth D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Irving John Good.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Philip Tetlock.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] J Doyne Farmer and Fran\u00e7ois Lafond.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Lyle Ungar, Barb Mellors, Ville Satop\u00e4\u00e4, Jon Baron, Phil Tetlock, Jaime Ramos, and Sam Swift.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Joe W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Our goal in defining \u201chigh-level machine intelligence\u201d (HLMI) was to capture the widely-discussed notions of \u201chuman-level AI\u201d or \u201cgeneral AI\u201d (which contrasts with \u201cnarrow AI\u201d) [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 14, "context": "The former framing was used in recent surveys of HLMI timelines; the latter framing is used in the psychological literature on forecasting [16, 17].", "startOffset": 139, "endOffset": 147}, {"referenceID": 14, "context": "Differences in these two framings have previously been documented in the forecasting literature [16, 17] but there is no clear guidance on which framing leads to more accurate predictions.", "startOffset": 96, "endOffset": 104}, {"referenceID": 0, "context": "Q[1] (n=57) Q[2] (n=40)", "startOffset": 1, "endOffset": 4}, {"referenceID": 1, "context": "Q[1] (n=57) Q[2] (n=40)", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Q[3] (n=55) Q[4] (n=48)", "startOffset": 1, "endOffset": 4}, {"referenceID": 3, "context": "Q[3] (n=55) Q[4] (n=48)", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Q[1] (n=53) Q[2] (n=57)", "startOffset": 1, "endOffset": 4}, {"referenceID": 1, "context": "Q[1] (n=53) Q[2] (n=57)", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Q[3] (n=65) Q[4] (n=49)", "startOffset": 1, "endOffset": 4}, {"referenceID": 3, "context": "Q[3] (n=65) Q[4] (n=49)", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Currently, deep networks often need hundreds of examples in classification tasks[1], but there has been work on one-shot learning for both classification[2] and generative tasks[3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Currently, deep networks often need hundreds of examples in classification tasks[1], but there has been work on one-shot learning for both classification[2] and generative tasks[3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "Currently, deep networks often need hundreds of examples in classification tasks[1], but there has been work on one-shot learning for both classification[2] and generative tasks[3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 0, "context": "[1] Lake et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Building Machines That Learn and Think Like People [2] Koch (2015) Siamese Neural Networks for One-Shot Image Recognition [3] Rezende et al.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Building Machines That Learn and Think Like People [2] Koch (2015) Siamese Neural Networks for One-Shot Image Recognition [3] Rezende et al.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "For reference, DeepMind\u2019s AlphaGo has probably played a hundred million games of self-play, while Lee Sedol has probably played 50,000 games in his life[1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "[1] Lake et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "This includes games like Frostbite, which require planning to achieve sub-goals and have posed problems for deep Q-networks[1][2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "This includes games like Frostbite, which require planning to achieve sub-goals and have posed problems for deep Q-networks[1][2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "[1] Mnih et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Lake et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "For context, the original Atari playing deep Q-network outperforms professional game testers on 47% of games[1], but used hundreds of hours of play to train[2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "For context, the original Atari playing deep Q-network outperforms professional game testers on 47% of games[1], but used hundreds of hours of play to train[2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "[1] Mnih et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Lake et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "For context, Fu 2016[1] successfully joins single large LEGO pieces using model based reinforcement learning and online adaptation.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "[1] Fu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Learn to Sort Big Numbers Without Solution Form Learn to efficiently sort lists of numbers much larger than in any training set used, the way Neural GPUs can do for addition[1], but without being given the form of the solution.", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "For context, Neural Turing Machines have not been able to do this[2], but Neural Programmer-Interpreters[3] have been able to do this by training on stack traces (which contain a lot of information about the form of the solution).", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "For context, Neural Turing Machines have not been able to do this[2], but Neural Programmer-Interpreters[3] have been able to do this by training on stack traces (which contain a lot of information about the form of the solution).", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "[1] Kaiser & Sutskever (2015).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Neural GPUs Learn Algorithms [2] Zaremba & Sutskever (2015).", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "Reinforcement Learning Neural Turing Machines [3] Reed & de Freitas (2015).", "startOffset": 46, "endOffset": 49}], "year": 2017, "abstractText": "Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3]. To adapt public policy, we need to better anticipate these advances [4, 5]. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.", "creator": "LaTeX with hyperref package"}}}