{"id": "1705.09045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Cross-Domain Perceptual Reward Functions", "abstract": "In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem with the goal and reward. In our example, if we set up an agent training a task with reward goals, we often want to set goals at once. This is a great idea because of how it can help us learn how to identify and describe the rewards and rewards as it goes. In this example, we define the rewards with a function called this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 25 May 2017 04:54:36 GMT  (1768kb,D)", "https://arxiv.org/abs/1705.09045v1", "A shorter version of this paper was accepted to RLDM (this http URL)"], ["v2", "Wed, 7 Jun 2017 15:44:37 GMT  (1732kb,D)", "http://arxiv.org/abs/1705.09045v2", "A shorter version of this paper was accepted to RLDM (this http URL)"], ["v3", "Tue, 25 Jul 2017 15:40:28 GMT  (1732kb,D)", "http://arxiv.org/abs/1705.09045v3", "A shorter version of this paper was accepted to RLDM (this http URL)"]], "COMMENTS": "A shorter version of this paper was accepted to RLDM (this http URL)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ashley d edwards", "srijan sood", "charles l isbell jr"], "accepted": false, "id": "1705.09045"}, "pdf": {"name": "1705.09045.pdf", "metadata": {"source": "CRF", "title": "Cross-Domain Perceptual Reward Functions", "authors": ["Ashley D. Edwards", "Srijan Sood", "Charles L. Isbell"], "emails": ["srijansood}@gatech.edu,", "isbell@cc.gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Rewards often act as the sole feedback for Reinforcement Learning (RL) problems. This signal is surprisingly powerful\u2014it can motivate agents to solve tasks without any further guidance for how to accomplish them. Nevertheless, rewards do not come for free, and are typically hand-engineered for each problem. Furthermore, they must traditionally be defined in terms of the agent\u2019s environment, which may be difficult or tedious. Such reward specifications can be practical for single-goal problems, yet challenging in environments consisting of multiple goal configurations. We may be willing, for example, to specify rewards based on the numerous cuts, holes, and bindings that are necessary to construct a table, but developing rewards for each type of furniture could be intractable.\nLearning from demonstration approaches, such as inverse reinforcement learning and imitation learning [3], aim to alleviate the engineering effort by utilizing examples of how a task should be solved. These techniques can be valuable when training an agent how to complete a task, but are not always necessary when we wish to specify what the task is. Alternatively, goals may be specified through target images, but an underlying concern remains with this and each of the former approaches. That is, we must represent the problem in the agent\u2019s environment.\nOne common technique humans use when approaching problems is to study replicas of the solved instance. We may, for example, attempt to build a table by examining an image of one that is already configured. This is analogous in RL terms to specifying a reward based on the agent\u2019s environment, either through the parameters of the agent\u2019s state or target images. But we may also use other resources to guide our solutions\u2014written instructions, spoken words, diagrams, etc. We aim to allow specifying tasks in similar manners, across domains, through cross-domain goal instances that are defined in environments outside of the agent\u2019s. Rather than hand-specifying rewards based on internal parameters of the agent, or providing target images from the agent\u2019s environment, our approach allows specifying goals in surrogate environments, where one may more easily find or construct solutions.\nar X\niv :1\n70 5.\n09 04\n5v 3\n[ cs\n.A I]\n2 5\nJu l 2\n01 7\nWe introduce Cross-Domain Perceptual Reward (CDPR) functions, which produce rewards that represent deeply learned similarities between a cross-domain goal image and states from an agent\u2019s environment. We empirically demonstrate that CDPRs are a general approach for providing accurate rewards between these cross-domain and intra-domain representations.\nWe provide evidence of generality by showing that CDPRs can be used without modification across multiple goal instantiations. We demonstrate accuracy by introducing a goal retrieval metric and additionally showing that CDPRs can successfully train an agent to complete a task. We describe two novel tasks with two distinct cross-domain goal specifications for each. We show that by using CDPRs as a replacement for hand-specified reward functions, we can successfully train an agent to solve these tasks with deep reinforcement learning, with the added benefit that we can specify the goals on our own terms.\nThe rest of the paper is organized as follows. We begin by providing the background for our approach in Section 2. In Section 3 we discuss related work and then in Section 4 we describe a deep neural network architecture that learns the CDPRs. We provide results in Section 5 followed by the conclusion in Section 6."}, {"heading": "2 Background", "text": "Reinforcement Learning (RL) problems are traditionally specified by a Markov Decision Process \u3008S,A, P,R\u3009 [35]. The set S consists of states s \u2208 S that correspond to the agent\u2019s environment. An agent takes actions a \u2208 A and receives rewards r \u2208 R(s) that depend on the current state. In general, an agent\u2019s task is defined solely by the rewards specified in the MDP. Each time the task changes, the reward function must be redefined. The transition function P (s, a, s\u2032) represents the probability that the agent will land in state s\u2032 after taking action a in state s. A policy \u03c0(s, a) represents the probability of taking action a in state s. We typically are interested policies that maximize the long-term expected reward over time. In our problem, we are not given rewards as inputs to the MDP. Rather, we aim to use deep learning to learn a reward function that reflects the similarity between an image of the agent\u2019s state and an image from another environment. There has been a large body of work in deep learning for finding similarities across domains, generally applied to image classification, generation, and retrieval problems (e.g. [6, 7, 14, 18, 23]). Our work explores the use of such techniques for cross-domain goal specifications."}, {"heading": "3 Related work", "text": "We now describe several approaches for representing goals in RL. Throughout the literature, we have found that goals are often defined in terms of the agent\u2019s environment. Our own approach introduces a general mechanism for specifying goals on our own terms. We discuss two broad techniques for representing goals, either by explicitly instantiating them through engineering or direct feedback, or implicitly expressing them through demonstrations."}, {"heading": "3.1 Explicit goal instantiations", "text": "Arguably, one of the most common approaches for representing goals for RL problems is to provide hand-specified rewards that are based on internal parameters of an agent\u2019s environment. There is a wealth of literature that describes such task-specific rewards, from those that indicate when an agent has reached a desired location [34], to more complex ones that can be used as feedback for training neural networks [43]. Rewards are largely where we instill domain knowledge, and so are inherently specialized. Even if a reward is commonly used for multiple problems in the same domain, it often remains a function of the configurations of each agent in the environment.\nFor problems consisting of visual inputs, a more general solution for representing goals is to use target images. With this approach, rewards are based on pixels alone, thus allowing one to abstract away the task specification from the parameters of the agent\u2019s configuration. This approach is similar to visuo-servoing, which uses target images to guide robots to some goal [15]. Such solutions often require extracting known objects from camera images. A more recent RL approach learns the relevant features from the agent\u2019s state space [11]. However, this method still requires defining the goal in terms of the agent\u2019s environment. Another approach uses motion templates to transform the visually\ndissimilar motions of humans and a simulated robot into a similar structure, but this correspondence is hand-specified [10]. Our approach aims to learn such correspondences.\nWhen multiple goals or MDP settings exist, it may be necessary to produce more general solutions. Early work aimed to find rewards that generalized across multiple environments [30]. Recently Finn et al. [12] described an approach that allowed generalizing skills learned from \u201clabeled\u201d MDPs with known reward functions to similar \u201cunlabeled\u201d MDPs with unspecified reward functions. Our work also uses a semi-supervised approach for learning reward functions, but we do not assume the labeled and unlabeled distributions are similar. Finally, we have seen approaches that aim learn a value function that generalizes over states and goals, but these approaches also assume that the two representations share a similar structure [29, 42].\nSpecifying the goal itself may not always be difficult, but training an agent to actually reach the goal may be. Training in simulation and then transferring the knowledge to the real-world can often be more efficient than training there directly (e.g. [26, 27, 38, 39, 41]). Such specifications may also be considered cross-domain, although the motivation for these works is different from our own. In particular, such \u201csim-to-real\u201d approaches tend to focus on transferring across separate realizations of similar domains.\nMany methods utilize human feedback to guide an agent\u2019s behavior. For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37]. Alternatively, policy shaping takes a more hands-off approach, and provides policy \u201cadvice\u201d, as opposed to explicit rewards [13]. Another approach aims to interactively teach an agent goals [21]. Finally, rather than providing direct target images for where an object should be, Finn et al. [11] introduced an approach that allowed humans to select the pixel locations for where a robot should move an object."}, {"heading": "3.2 Implicit goal instantiations", "text": "Learning from Demonstration (LfD) is often used when specifying a goal is difficult, or when a problem is too challenging for an agent to solve on its own. Our approach differs from LfD, as we do not aim to portray how to solve a task, only what the task is. Nevertheless, we do share some motivation with these works, which we now discuss.\nInverse RL aims to infer a reward function from expert demonstrations [1]. This approach typically requires multiple demonstrations for a single task. We also aim to learn a reward function in our approach, but we use a single sample to represent the goal. A similar approach is imitation learning, which aims to learn a policy directly from demonstrations [28]. Both of these techniques often assume that the student, i.e., agent, and the teacher, or demonstrator, share a common environment. That is, the observed states and actions are shared between the teacher and student. When the observations are dissimilar, a correspondence problem must be solved [3]. These correspondences are often hand-specified, and may require equipment such as sensors on the teacher\u2019s body. Our own approach aims to learn such correspondences between different representations.\nRecent works have proposed solving the correspondence problem automatically. For example, one approach used deep learning to train an agent that had a first-person perspective through third-person samples [32]. Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9]. Our own approach does not assume we have examples of desired behaviors.\nFinally, there has been a breadth of recent work that aims to specify goals in different modalities. Such approaches have similar a motivation to our own. That is, they aim to provide goal specifications in more natural settings than the agent\u2019s. For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40]. Recently, an approach learned correspondences between written instructions and visual states in Atari games [19]. This approach is more similar to our own as it learns the correspondences between two different domain representations. It is clear that there is much momentum in this area of research. Still, we have not yet seen an approach that learns a visual correspondence for cross-domain goal representations."}, {"heading": "4 Approach", "text": "We now formalize our approach for developing Cross-Domain Perceptual Reward (CDPR) functions. We focus on solving RL problems consisting of MDPs with visual states and unspecified reward functions. We are interested in problems where an agent has a task with multiple potential goal configurations. An agent\u2019s overall task may be to build furniture, for example, but its goal could be to build a chair, or table, or bench. Put succinctly, each goal is a specific instance of a task. Typically, an agent\u2019s task is defined by a reward function. This remains true for our approach as well, but now we require the reward to be a function of both the state and a goal. Therefore, for a single task, this general reward function remains fixed when the goal changes\u2014only the inputs vary. We now describe how we develop such rewards, or CDPRs.\nWe aim to specify goals through images obtained from alternative environments. Our work is inspired by an approach that uses deep learning to find cross-domain similarities between images and their corresponding spoken captions, which are represented through spectrograms [14]. We hypothesize that this approach can be used in RL to represent cross-domain goals. The joint audio-visual approach used a network tailored to the spoken captions, but we aim to use a more general network that can accept any image as input. A diagram of our network is shown in Figure 1. The inputs to the network are states from the agent\u2019s environment, Gi, and cross-domain goal images, G\u0302j . We use this notation for the agent\u2019s state from now on, as we consider each state image to be a potential goal image. The role of the CDPR is to determine this by outputting similarities between the two.\nWe construct two separate networks with identical architectures to encode the intra-domain features, \u03a6Gi , and the cross-domain features, \u03a6G\u0302j , respectively. The CDPR is obtained by taking the dot product of these two outputs:\nR(Gi, G\u0302j) = \u03a6 T Gi\u03a6G\u0302j (1)\nThis reward then acts as the similarity between an agent\u2019s state and a cross-domain goal.\nWe train the network with a semi-supervised approach. Assume that for a given task, that we have k pairs consisting of an intra-domain goal specification and a corresponding cross-domain goal: {(G1, G\u03021), . . . , (Gk, G\u0302k)}. After training, we only use cross-domain images to instantiate goals. We aim to learn a reward function that makes R(Gi, G\u0302j) large when i = j and small otherwise. As such, we use the loss function from the previously described approach, which optimizes for exactly this target, modified appropriately for learning reward functions. Given the parameters \u03b8 of the network\ndescribed in Figure 1, the loss can be formulated as: L(\u03b8) = max ( 0, R\u03b8(Gj , G\u0302i)\u2212R\u03b8(Gi, G\u0302i) + 1 ) + max ( 0, R\u03b8(Gi, G\u0302j)\u2212R\u03b8(Gi, G\u0302i) + 1 ) (2)\nWe use gradient descent to optimize this loss function. For each training batch, we randomly sample two goal pairs, (Gi, G\u0302i) and (Gj , G\u0302j). The loss aims to make the reward for the matching pair, R(Gi, G\u0302i), be larger than the rewards for mismatched pairs, R(Gj , G\u0302i) and R(Gi, G\u0302j). Otherwise, the outputs for the loss function will be greater than 0 and so the gradients will be penalized. Once we obtain the CDPRs, we can use them to instantiate the reward function for the MDP, and use standard RL approaches to solve the task. We should note that while the CDPR remains fixed across goal specifications, it is necessary to learn a new CDPR for each type of cross-domain goal representation."}, {"heading": "5 Experiments and results", "text": "We aim to demonstrate the generality and accuracy of CDPRs. We compare against a standard hand-specified reward, and a state encoding reward that learns features in the agent\u2019s domain and represents the goal as an intra-domain target image. This reward is represented by the negative euclidean distance between the agent\u2019s state and the intra-domain goal representation. For each of the tasks, we automatically generated the cross-domain goal representations, as we aim to evaluate first if our approach works. Future work would entail using hand-specified goal specifications. We now describe the tasks we used to evaluate our approach.\nWe now describe a metric for measuring accuracy in reward functions. While evaluating with RL can suffice, it may be intractable to iterate over many goal instantiations. Therefore, we introduce the Goal Retrieval Accuracy (GRA) metric, which is motivated by image retrieval approaches. Given a set of states that were not seen during training time, we aim to find if the accurate cross-domain goal can be retrieved. In particular, this metric measures if the highest reward given to an unseen state matches the correct cross-domain goal. This measure gives an indication for how accurate the CDPRs are, and additionally indicates how well the learned rewards can generalize across multiple goal specifications.\nWe additionally measure how well the CDPRs work with RL. To further measure accuracy and generality, we used two unseen goal instantiations for each task, as shown in Figure 2. We used Deep RL to solve the tasks with the architecture described in the paper [24]. We trained the network with an Adam optimizer with an initial learning rate of 10\u22124 and a batch size of 32. To evaluate the agent\u2019s performance, we ran its learned policy on the task every 100 episodes and measure a task-dependent evaluation accuracy."}, {"heading": "5.1 Maze task", "text": "The first domain we evaluate our approach on is a Maze, as shown in Fig 2a. We randomly generated mazes consisting of 1-3 green, blue, or yellow colored rooms. The agent\u2019s actions are to move up, down, left or right, and its task is to navigate the maze. Goals designate the specific room the agent needs to reach. In order to specify the desired room using standard RL approaches, we would need to know its coordinates. Rather, we utilize two cross-domain goal representations: a sign language handshape indicating the first letter of the desired room color, and a spectrogram of a spoken command stating \u201cGo to the [color] room.\u201d We use a dataset of sign language gestures from [4] for the handshape specification. For the speech specification, we recorded one sample of the spoken command for each colored room, and then varied the pitch to produce multiple samples. We set the reward for the standard approach as 1 if the agent is located in the desired room and 0 otherwise. The agent\u2019s episode resets after 1000 steps, as a terminal state requires knowledge of the goal location. During training, the agent could be initialized in any location on the map. During evaluation, it is initialized in the green room in the top maze in Figure 2a and in the yellow room in the bottom maze.\nIn order to train the CDPRs for the handshape and the speech specifications, we used a momentum optimizer with an initial learning rate of 10\u22124 and momentum value of .9. We used a batch size of 32. The embedding networks for the CDPR have the following architecture: Conv1\u2192 maxpool\u2192 Conv2\u2192 maxpool\u2192 FC1\u2192 FC2, where Conv1 consists of 64 11x11 filters with stride 4, Conv2 consists of 192 5x5 filters with stride 2, FC1 outputs 400 features, and FC2 outputs 100. Each maxpool consists of a 3x3 filter with stride 2. Conv1, Conv2, and FC1 are each followed by batch normalization [16] and then ELU [8]."}, {"heading": "5.2 Music playing task", "text": "The next domain we evaluate our approach on allows an agent to play synthesized piano notes by selecting keys and note durations. The agent can play 7 keys on a single scale of a-g, and each key can be a whole or half note. When the agent selects a note, a .wav file is generated that gets converted into a spectrogram, which represents its state. The task is for the agent to play a song, and the goals designate which song to play. In order to specify the desired song, we would need to know how to play the notes on the piano. Rather, we again use two cross-domain goal representations: a spectrogram of the desired song played on a synthesized guitar and the sheet music of the song. To obtain the samples of both specifications, we randomly generated notes and automatically created the sheet music and spectrograms for the guitar. We set the reward for the standard approach as the total percentage of notes the agent has played correctly. The agent\u2019s episode resets after it plays notes for four complete bars. To avoid positive loops with the standard specification and CDPRs, the agent only receives a reward when it reaches this terminal state. During training and evaluation, the agent is initialized without any notes played.\nIn order to train the CDPRs for this task, we used an Adam optimizer [20] with an initial learning rate of 10\u22124. We used a batch size of 32. The embedding networks for the CDPR have the following architecture: Conv1\u2192 maxpool\u2192 Conv2\u2192 maxpool\u2192 FC1, where Conv1 consists of 32 11x11 filters with stride 4, Conv2 consists of 64 5x5 filters with stride 2, and FC1 outputs 2048 features.\nEach maxpool consists of a 3x3 filter with stride 2. Conv1 and Conv2 are both followed by batch normalization and then ELU."}, {"heading": "5.3 Results", "text": "We now discuss results for training the CDPRs for each task and running RL with the learned rewards."}, {"heading": "5.4 Maze results", "text": "We first give qualitative results for the Maze task. Figure 3 shows a heat map of the rewards obtained from each location the agent could be spawned in. We do not show results for the standard reward, since this would just give rewards of 1 in the correct room and 0 otherwise. It is clear that CDPRs have learned to accurately reward the correct rooms for both the handshape and speech goal specifications. The largest reward values are given when the agent is in the correct room. We report the rewards for the feature encoding reward only for comparison, as these rewards are not directly learned. It is clear that the correct room will receive higher rewards than the rest, since the reward will be 0 when the state and goal are equal.\nThe next result we report is the GRA, shown in Table 1. Again, we show the encoding results for completeness, but the rewards will always yield a goal retrieval accuracy of 1. The handshape and speech CDPRs achieve high accuracy for retrieving the correct goal, demonstrating the accuracy of the approach and that it can generalize across new, unseen, goals.\nFinally, we report the results for using the CDPRs with deep RL, as shown in Figure 4. We were interested in finding not only if the agent reached the goal, but also if it remained there, since this domain did not have terminal states. The desired goals were for the agent to reach the blue and green rooms, respectively, in the rooms displayed in Figure 2a. The CDPR with the spoken command was able to perform well on both tasks. Interestingly, the CDPR with the handshape only performed well one one task. This may be surprising, since this goal representation achieved a high GRA. It is clear that the GRA should not be the only metric for evaluating rewards. Rather, we should utilize multiple evaluation metrics, as incorrectly specified rewards have been known to lead to strange behavior.\nWe have one explanation for why the handshape goal representation does not work for the first goal specification. If we again study the qualitative results from Figure 3, we observe that the handshape CDPR actually gives intermediate rewards in the hallways. This may lead to suboptimal policies, so even though the GRA is high, these intermediate rewards may lead to locally suboptimal policies. The feature encoding network also gives intermediate rewards, but these rewards will not introduce positive cycles since none of the rewards are greater than 0. Future work then should ensure we train on both goal images and random states obtained from the domain to ensure \u201cfalse positives\u201d such as this do not occur."}, {"heading": "5.5 Music results", "text": "We now report results for the music domain. We do not have qualitative results for this task as the rewards are more difficult to visualize. The first result we report then is the GRA, shown in Table 1. The guitar and sheet specifications also achieved high accuracy, but the results are clearly not as good as the Maze results. Just as in classification problems, we will need to determine how much error we are willing to tolerate to avoid providing solutions for every problem. Nevertheless, future work will aim to develop more sophisticated architectures that produce higher GRAs.\nWe use the RL results to further examine the correctness of the CDPRs, as shown in Figure 5. The desired goals were for the agent to play the songs depicted in Figure 2b: {(\u2018a\u2019, W), (\u2018b\u2019, W), (\u2018b\u2019, H), (\u2018c\u2019, H), (\u2018d\u2019, W)} and {(\u2018f\u2019, W), (\u2018a\u2019, H), (\u2018e\u2019, H), (\u2018a\u2019, H), (\u2018c\u2019, H), (\u2018e\u2019, W)}, where W and H are whole and half notes, respectively. Both the guitar and sheet music goal specifications yielded results with high accuracy on the first song, each achieving accuracies of up to .75. The second song was slightly harder, as it had more notes. Both specifications achieved lower accuracy on the second song. In fact, the sheet music encoding learned a policy worse than random. The correspondence for this goal representation is likely more difficult than the guitar representation, which we see some evidence of in Figure 1. As we make improvements to the GRAs, we also expect the accuracy in RL to improve as well."}, {"heading": "6 Discussion and conclusion", "text": "Our results indicate that we can accurately learn general reward functions specified through CDPRs. As discussed in the related work section, there is clearly evidence of momentum for this area of research. Further research will entail improving the accuracy of the CDPRs and their performance on RL tasks. Additionally, our initial work focused on determining that CDPRs could effectively be used to represent cross-domain goal representations. Future work will aim to utilize real-world samples of goals, perhaps obtained from crowd-sourcing. As such, it will be necessary to improve the sample complexity required to learn the CDPRs, which we did not examine in this study.\nIn summary, we have shown how goals can be defined in alternative environments than the agents. We introduced a general reward function, CDPRs, and showed that our approach could achieve strong performance in tasks specified through cross-domain goal specifications."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 1. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment", "author": ["H.B. Ammar", "E. Eaton", "P. Ruvolo", "M.E. Taylor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, 57(5):469\u2013483,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A new 2d static hand gesture colour image dataset for asl gestures", "author": ["A.L.C. Barczak", "N.H. Reyes", "M. Abastillas", "A. Piccio", "T. Susnjak"], "venue": "Research Letters in the Information and Mathematical Sciences, 15:12\u201320,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain separation networks", "author": ["K. Bousmalis", "G. Trigeorgis", "N. Silberman", "D. Krishnan", "D. Erhan"], "venue": "CoRR, abs/1608.06019,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 539\u2013546. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning modular neural network policies for multi-task and multi-robot transfer", "author": ["C. Devin", "A. Gupta", "T. Darrell", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1609.07088,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Perceptual reward functions", "author": ["A. Edwards", "C. Isbell", "A. Takanishi"], "venue": "arXiv preprint arXiv:1608.03824,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep spatial autoencoders for visuomotor learning", "author": ["C. Finn", "X.Y. Tan", "Y. Duan", "T. Darrell", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Generalizing skills with semi-supervised reinforcement learning", "author": ["C. Finn", "T. Yu", "J. Fu", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1612.00429,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy shaping: Integrating human feedback with reinforcement learning", "author": ["S. Griffith", "K. Subramanian", "J. Scholz", "C. Isbell", "A.L. Thomaz"], "venue": "Advances in Neural Information Processing Systems, pages 2625\u20132633,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning of spoken language with visual context", "author": ["D. Harwath", "A. Torralba", "J. Glass"], "venue": "Advances in Neural Information Processing Systems, pages 1858\u20131866,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A tutorial on visual servo control", "author": ["S. Hutchinson", "G.D. Hager", "P. Corke"], "venue": "Robotics and Automation, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A social reinforcement learning agent", "author": ["C. Isbell", "C.R. Shelton", "M. Kearns", "S. Singh", "P. Stone"], "venue": "Proceedings of the fifth international conference on Autonomous agents, pages 377\u2013384. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CoRR, abs/1611.07004,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Beating atari with natural language guided reinforcement learning", "author": ["R. Kaplan", "C. Sauer", "A. Sosa"], "venue": "arXiv preprint arXiv:1704.05539,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning task goals interactively with visual demonstrations", "author": ["J. Kirk", "A. Mininger", "J. Laird"], "venue": "Biologically Inspired Cognitive Architectures, 18:1\u20138,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance", "author": ["W.B. Knox", "P. Stone"], "venue": "Artificial Intelligence, 225:24\u201350,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1512.09300,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Robot gains social intelligence through multimodal deep reinforcement learning", "author": ["A.H. Qureshi", "Y. Nakamura", "Y. Yoshikawa", "H. Ishiguro"], "venue": "Humanoid Robots (Humanoids), 2016 IEEE-RAS 16th International Conference on, pages 745\u2013751. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)2rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Is imitation learning the route to humanoid robots", "author": ["S. Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Universal value function approximators", "author": ["T. Schaul", "D. Horgan", "K. Gregor", "D. Silver"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1312\u20131320,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Where do rewards come from", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto"], "venue": "Proceedings of the annual conference of the cognitive science society, pages 2601\u20132606,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting navigation states from a hand-drawn map", "author": ["M. Skubic", "P. Matsakis", "B. Forrester", "G. Chronis"], "venue": "Robotics and Automation, 2001. Proceedings 2001 ICRA. IEEE International Conference on, volume 1, pages 259\u2013264. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Third-person imitation learning", "author": ["B.C. Stadie", "P. Abbeel", "I. Sutskever"], "venue": "arXiv preprint arXiv:1703.01703,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards the grounding of abstract words: a neural network model for cognitive robots", "author": ["F. Stramandinoli", "A. Cangelosi", "D. Marocco"], "venue": "Neural Networks (IJCNN), The 2011 International Joint Conference on, pages 467\u2013474. IEEE,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Proceedings of the seventh international conference on machine learning, pages 216\u2013224,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1990}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge Univ Press,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, 10(Jul):1633\u20131685,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "Artificial Intelligence, 172(6-7):716\u2013737,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR),", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Face valuing: Training user interfaces with facial expressions and reinforcement learning", "author": ["V. Veeriah", "P.M. Pilarski", "R.S. Sutton"], "venue": "arXiv preprint arXiv:1606.02807,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "arXiv preprint arXiv:1511.03791,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "arXiv preprint arXiv:1611.01578,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Learning from demonstration approaches, such as inverse reinforcement learning and imitation learning [3], aim to alleviate the engineering effort by utilizing examples of how a task should be solved.", "startOffset": 102, "endOffset": 105}, {"referenceID": 33, "context": "Reinforcement Learning (RL) problems are traditionally specified by a Markov Decision Process \u3008S,A, P,R\u3009 [35].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "[6, 7, 14, 18, 23]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 32, "context": "There is a wealth of literature that describes such task-specific rewards, from those that indicate when an agent has reached a desired location [34], to more complex ones that can be used as feedback for training neural networks [43].", "startOffset": 145, "endOffset": 149}, {"referenceID": 41, "context": "There is a wealth of literature that describes such task-specific rewards, from those that indicate when an agent has reached a desired location [34], to more complex ones that can be used as feedback for training neural networks [43].", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "This approach is similar to visuo-servoing, which uses target images to guide robots to some goal [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "A more recent RL approach learns the relevant features from the agent\u2019s state space [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "dissimilar motions of humans and a simulated robot into a similar structure, but this correspondence is hand-specified [10].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Early work aimed to find rewards that generalized across multiple environments [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "[12] described an approach that allowed generalizing skills learned from \u201clabeled\u201d MDPs with known reward functions to similar \u201cunlabeled\u201d MDPs with unspecified reward functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Finally, we have seen approaches that aim learn a value function that generalizes over states and goals, but these approaches also assume that the two representations share a similar structure [29, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 40, "context": "Finally, we have seen approaches that aim learn a value function that generalizes over states and goals, but these approaches also assume that the two representations share a similar structure [29, 42].", "startOffset": 193, "endOffset": 201}, {"referenceID": 24, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 36, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 37, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 39, "context": "[26, 27, 38, 39, 41]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 15, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 20, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 35, "context": "For example, some approaches allow humans to provide rewards to agents in real-time [17, 22, 37].", "startOffset": 84, "endOffset": 96}, {"referenceID": 11, "context": "Alternatively, policy shaping takes a more hands-off approach, and provides policy \u201cadvice\u201d, as opposed to explicit rewards [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Another approach aims to interactively teach an agent goals [21].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "[11] introduced an approach that allowed humans to select the pixel locations for where a robot should move an object.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Inverse RL aims to infer a reward function from expert demonstrations [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 26, "context": "A similar approach is imitation learning, which aims to learn a policy directly from demonstrations [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "When the observations are dissimilar, a correspondence problem must be solved [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 30, "context": "For example, one approach used deep learning to train an agent that had a first-person perspective through third-person samples [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 34, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 7, "context": "Another somewhat related RL approach is transfer learning, which aims to transfer learned behaviors from one domain to another [36], for example by initializing the parameters of policies in unsolved tasks [2], or by transferring skills across untrained robots [9].", "startOffset": 261, "endOffset": 264}, {"referenceID": 29, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 108, "endOffset": 115}, {"referenceID": 31, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 175, "endOffset": 179}, {"referenceID": 23, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 223, "endOffset": 227}, {"referenceID": 38, "context": "For example, we have seen sketches of maps used for representing desired trajectories in navigational tasks [5, 31], correspondences learned between words and robotic actions [33], rewards based on the touch of a handshake [25], and value functions learned from facial expressions [40].", "startOffset": 281, "endOffset": 285}, {"referenceID": 17, "context": "Recently, an approach learned correspondences between written instructions and visual states in Atari games [19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "Our work is inspired by an approach that uses deep learning to find cross-domain similarities between images and their corresponding spoken captions, which are represented through spectrograms [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "We used Deep RL to solve the tasks with the architecture described in the paper [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "\u201d We use a dataset of sign language gestures from [4] for the handshape specification.", "startOffset": 50, "endOffset": 53}, {"referenceID": 14, "context": "Conv1, Conv2, and FC1 are each followed by batch normalization [16] and then ELU [8].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Conv1, Conv2, and FC1 are each followed by batch normalization [16] and then ELU [8].", "startOffset": 81, "endOffset": 84}, {"referenceID": 18, "context": "In order to train the CDPRs for this task, we used an Adam optimizer [20] with an initial learning rate of 10\u22124.", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agent\u2019s environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agent\u2019s. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agent\u2019s state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.", "creator": "LaTeX with hyperref package"}}}