{"id": "1705.09231", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Neural Attribute Machines for Program Generation", "abstract": "Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 25 May 2017 15:35:16 GMT  (185kb,D)", "https://arxiv.org/abs/1705.09231v1", null], ["v2", "Wed, 31 May 2017 04:00:12 GMT  (185kb,D)", "http://arxiv.org/abs/1705.09231v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["matthew amodio", "swarat chaudhuri", "thomas reps"], "accepted": false, "id": "1705.09231"}, "pdf": {"name": "1705.09231.pdf", "metadata": {"source": "CRF", "title": "Neural Attribute Machines for Program Generation\u2217", "authors": ["Matthew Amodio", "Swarat Chaudhuri"], "emails": ["mamodio@cs.wisc.edu", "swarat@rice.edu", "reps@cs.wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3]. Less extensively studied have been neural models for generation of highly structured artifacts, for example the source code of programs. Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6]. However, a fundamental difficulty in this problem domain is that to be acceptable to a compiler, a program must satisfy a rich set of constraints such as \u201cnever use undeclared variables\u201d or \u201conly use variables in a type-safe way\u201d. Learning such constraints automatically from data is a difficult task.\nIn this paper, we present a new generative model, called Neural Attribute Machines (NAMs), for programs that satisfy constraints like the above. The key insight of our approach is that the constraints enforced by a programming language are known in full detail a priori. Accordingly, they can be incorporated into training, and we propose a framework for doing so. We demonstrate that this approach has significant benefits: training existing architectures on samples that unfailingly abide by a constraint still produces a generative model that often violates the constraint; in contrast, the NAM model significantly outperforms these models at sampling from the space of constrained programs.\n\u2217Supported, in part, by a gift from Rajiv and Ritu Batra; by NSF under grant CCF-1162076; by AFRL under DARPA MUSE award FA8750-14-2-0270 and DARPA STAC award FA8750-15-C-0082; and by the UWMadison Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors, and do not necessarily reflect the views of the sponsoring agencies.\nar X\niv :1\n70 5.\n09 23\n1v 2\n[ cs\n.A I]\n3 1\nWe use the formalism of attribute grammars [7] as the language for expressing rich structural constraints over programs. Our model composes such a grammar with a recurrent neural network (RNN) that generates the nodes of a program\u2019s abstract-syntax tree, and uses it to constrain the output of the RNN at each point in time. Our training framework builds off of the observation that in the setting of generating constrained samples, there is a correct prediction, and then there are two categories of incorrect predictions. Incorrect predictions that are nevertheless legal under the constraint are more desirable than predictions that violate the constraint. The NAM addresses this multifaceted problem in two ways. First, it augments the input sequence with a fixed-length representation of a structural context that the attribute grammar uses to check constraints. The context provides information that the current input sequence is just one particular instantiation of a more general structural constraint. Second, NAMs optimize a three-valued loss function that penalizes correct, incorrect-but-legal, and incorrect-and-illegal predictions differently.\nThe main contributions of this paper can be summarized as follows:\n\u2022 We present a new neural network and logical architecture that incorporates background knowledge in the form of attribute-grammar constraints. \u2022 We give a framework to train this new architecture that uses a three-valued loss function to penalize correct, incorrect-but-legal, and incorrect-and-illegal predictions differently. \u2022 Our experiments show the difficulties existing neural models have with learning in constrained domains, and the advantages of using the proposed framework."}, {"heading": "2 Methodology", "text": ""}, {"heading": "2.1 Background on Attribute Grammars", "text": "Definition 2.1 An attribute grammar (AG) is a context-free grammar extended by attaching attributes to the terminal and nonterminal symbols of the grammar, and by supplying attribute equations to define attribute values [7]. Each production can also be equipped with an attribute constraint to specify that some relationship must hold among the values of the production\u2019s attributes.\nIn every production X0 \u2192 X1, . . . , Xk, each Xi denotes an occurrence of one of the grammar symbols; associated with each such symbol occurrence is a set of attribute occurrences corresponding to the symbol\u2019s attributes.\nThe attributes of a symbol X , denoted by A(X), are divided into two disjoint classes: synthesized attributes and inherited attributes. A production\u2019s output attributes are the synthesized-attribute occurrences of the left-hand-side nonterminal plus the inherited-attribute occurrences of the righthand-side symbols; its input attributes are the inherited-attribute occurrences of the left-hand-side nonterminal plus the synthesized-attribute occurrences of the right-hand-side symbols.\nEach production has a set of attribute equations, each of which defines the value of one of the production\u2019s output attributes as a function of the production\u2019s input attributes. We assume that the terminal symbols of the grammar have no synthesized attributes, and that the root symbol of the grammar has no inherited attributes. Noncircularity is a decidable property of AGs [8], and hence we can assume that no derivation tree exists in which an attribute instance is defined transitively in terms of itself.\nAn AG is L-attributed [9] if, in each production X0 \u2192 X1, . . . , Xk, the attribute equation for each inherited-attribute occurrence of a right-hand-side symbol Xi only depends on (i) inherited-attribute occurrences of X0, and (ii) synthesized-attribute occurrences of X1, . . . , Xi\u22121.\nWith reasonable assumptions about the computational power of attribute equations and attribute constraints, L-attributed AGs capture the class NP [10] (modulo a technical \u201cpadding\u201d adjustment).\nExample 2.2 To illustrate L-attributed AGs, we use a simple language of binary numerals. The abstract-syntax trees for binary numerals are defined using the following operator/operand declarations:2\n2 The notation used above is a variant of context-free grammars in which the operator names (Numeral, Pair, Zero, and One) serve to identify the productions uniquely. For example, the declaration \u201cnumeral: Numeral(bits);\u201d is the analog of the production \u201cnumeral \u2192 bits.\u201d (The notation is adapted from the Synthesizer Generator [11].)\nnumeral : Numeral(bits); bits : Pair(bits bits) | Zero() | One();\nTwo integer-valued attributes\u2014\u201dpositionIn\u201d and \u201cpositionOut\u201d\u2014will be used to determine a bit\u2019s position in a numeral:\nbits { inherited int positionIn; synthesized int positionOut; };\nThese attributes are used to define a left-to-right pattern of information flow through each derivation tree (also known as \u201cleft-to-right threading\u201d). In particular, with the attribute equations given in Fig. 1(a), at each leaf of the tree, the value of bits.positionOut is the position of the bit that the leaf represents with respect to the left end of the numeral (where the leftmost bit is considered to be at position 1).3 Fig. 1(b) shows the left-to-right pattern of dependences among attribute dependences in a six-node tree.\nOur ultimate goal is the creation of new trees de novo, with generation proceeding top-down, leftto-right. The latter characteristic motivated the choice that constraints be expressible using an L-attributed AG, because they propagate information left-to-right in an AST.\nL-attributed AGs also offer substantially increased expressive power over context-free grammars\u2014in particular, an L-attributed AG G can express non-context-free constraints on the trees in the language L(G). There are a large number of such constraints involved in any grammar that produces only programs that pass a C compiler. In the study presented in Section 3, we experimented with two constraints in isolation:\nDeclared-variable constraint: Each use of a variable must be preceded by a declaration of the variable. Typesafe-variable constraint: Each use of a variable must satisfy the type requirements of the position of the variable in the tree.\nBy working with a corpus of C programs that all compile, all of the training examples satisfy both constraints."}, {"heading": "2.2 From an AST to a sequence", "text": "While other attempts at learning models that can be used to generate trees include performing convolutions over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm to adopt for presenting a tree to a neural model is a single-traversal, top-down, left-to-right sequentialization of the tree. An AST T is represented by a depth-first sequence of pairs: each pair (ni, pi), for 1 \u2264 i \u2264 |Nodes(T)|, consists of a nonterminal ni \u2208 N and a production pi \u2208 P that has ni on the left-hand side. Depth information in the tree is conveyed by interspersing pop indicators when moving up the tree. As a preprocessing step, all variable names are aliased by their order of use in the program so that the ith distinct variable used is named Vari. This approach\n3 In the attribute equations for a production such as \u201cbits: Pair(bits bits);\u201d we distinguish between the three different occurrences of nonterminal \u201cbits\u201d via the symbols \u201cbits$1,\u201d \u201cbits$2,\u201d and \u201cbits$3,\u201d which denote the leftmost occurrence, the next-to-leftmost occurrence, etc. (In this case, the leftmost occurrence is the left-hand-side occurrence.)\nprevents difficulties associated with rarely used token names, and does not lose any meaningful information about structural constraints. Fig. 2 depicts a trivial example to illustrate the process.\nLet |N | and |P | denote the input and output dimensions, respectively. If n is a nonterminal in N , we use Pn to denote the subset of the productions P with n on the left-hand side. Pn is the set of legal outputs under the context-free constraint. The two context-sensitive constraints considered here further narrow the set of legal outputs. We use Pcd and Pct to denote the set of legal outputs at some (unspecified) point in the linearized tree, under the declared-variable constraint cd and the type constraint ct, respectively. Let c be either cd or ct; at a prediction step at an instance of nonterminal n, the possible output can be partitioned into three sets: {pi}, Pc \u2212 {pi}, and Pn \u2212 Pc."}, {"heading": "2.3 Challenges", "text": "Large and variably sized AST sequences present several problems for traditional neural sequencelearners. The first is the existence of distant non-local dependences, such as a declaration of variable v near the beginning of a file that might be many hundreds of steps away before the RNN needs to predict the use of v. Another is the existence of complex relationships between nodes that are difficult to express in a linear sequentialization, like when the distinction between a great-grandparent and a great-uncle node is important. Third, due to only approximating the sequence distribution, it is very likely that while generating large ASTs under randomized sampling, novel contexts will be encountered. In such a case, there would not be any information explicitly contained in the training set that can guide further generation after entering \u201cuncharted territory.\u201d With other approaches, one relies on the ability of the learner to generalize from the training set. However, imperfect learning of the constraints of L could cause poor generalization. Because NAMs are able to learn from the constraints, they have the potential to generalize based on the constraints, and hence have the potential to perform much better when they enter uncharted territory. The new idea pursued here is to leverage constraints that are defined unambiguously even in novel situations. If (an approximation to) the constraints can be learned, they will provide additional guidance about what to do in these situations."}, {"heading": "2.4 NAM", "text": "NAMs are equipped with a deterministic automaton, referred to hereafter as the logical machine. The logical machine provides assistance to the NAM in two ways: it augments the input vector with a fixed-length vector that represents the context, and it imposes its knowledge of the output partitions {pi}, Pc \u2212 {pi}, and Pn \u2212 Pc to add an extra loss term for constraint-violating predictions.\nAugmented input. The logical machine outputs a fixed-length binary vector C that represents the context of the current node with respect to the constraint desired. All variable names are known from the grammar G, and each corresponds to one production rule p \u2208 P . Let pvi be the production rule that chooses variable name vi and Pv be the collection of all production rules for variables. For the context vector Cd for the declared-variable constraint, which is binary and of length |Pv|, the 1-valued entries are the pvi positions of declared variables.\nFor the context vector Ct for the typesafe-variables constraint, each variable and type combination (vi, ti) has an entry in Ct, which is 1 if vi is of type ti. There is then one additional entry for each type ti, which is 1 if the current prediction must be of type ti.\nDeveloping fixed-length context vectors that are informative for the constraint at hand could be daunting for constraints that are more conceptually complex, but even these simple representations had a profound positive impact on the model learned. Different representations of the same constraint were not tried. An interesting direction for future work would be to test how robust the NAM framework is to different ways of building the context vector.\nThree-level loss function. In addition to being presented with augmented input by the logical machine, NAMs are also trained with additional reinforcement from the logical machine. The standard cross-entropy loss function that measures the distance between the model\u2019s predicted probability distribution y\u0302 and the true observation y suffers from an undesirable consequence in the setting of learning to generate constrained sequences. The one-hot encoding of y means that probabilities assigned to all y\u2032 6= y are penalized equally. However, in the presence of constraints, there are really three categories of predictions: the partitions previously mentioned. Having a three-level loss function that punishes the partition of illegal predictions more than the legal-but-incorrect prediction could be interpreted in the vein of methods that artificially increase the training-set size, such as left-right reversing of images of scenes in a classification task where it is assumed a priori that orientation could not possibly affect the classification. These methods are most effective in situations like images or trees, where the input is high-dimensional but lies on a lower-dimensional manifold. In our work, the logical machine provides some feedback to the NAM that even though certain sequences were not actually in the training data, they have the possibility of existing, while others do not.\nThe objective that the NAM optimizes can be written as follows:\nLoss = Lxe + \u03a3i\u03bbiLci (1)\nwhere Lxe is the traditional cross-entropy loss function and Lci is the additional penalty for violating constraint ci, whose magnitude is controlled by \u03bbi. We say that Equation 1 defines a three-level loss function because predictions that are both wrong and illegal are penalized by both terms, while predictions that are wrong but legal are only penalized by the first. The tradeoff between NAMs learning the specific (xi, yi) training sequences versus the constraint more generally\u2014without caring which legal sequences are more realistic\u2014can be controlled with the hyperparameter \u03bbi.\nAlgorithms. The algorithms for training and generation are given as Algs. 1 and 2, respectively. During training, the trees in the corpus are traversed and the NAM\u2019s parameters are updated via stochastic gradient descent. Generation then samples trees from the learned model.\nAlgorithm 1: Training 1 trees := SequentializeTrainingTrees(); 2 repeat 3 tree := nextTrainingTree(trees); 4 trueNonterminal := curNonterminal; 5 trueOp := curOp 6 repeat 7 predictedOp := ChooseOperator(trueNonterminal, curContext); 8 gradients := Loss(trueOp,predictedOp); 9 UpdateWeights(gradients);\n10 (trueNonterminal,trueOp) := next position in left-right threading of sequentialized tree; 11 curContext := update curContext according to the values of the attributes at trueNonterminal; 12 until all nonterminals in tree are processed; 13 until all trees are processed;"}, {"heading": "3 Experiments", "text": "All of our experiments used the following models: a vanilla RNN, a NAM with just the augmented input, a NAM with just the new objective function, and the full NAM with both. The chosen version\nAlgorithm 2: Generation 1 tree := Root([hole]); 2 curFocus := tree.child[1]; 3 curNonterminal := nonterminal of curFocus; 4 curContext := the values of the attributes at curFocus; 5 repeat 6 op := ChooseOperator(curNonterminal, curContext); 7 Insert op([hole1], . . ., [holearity(op)]) into tree at curFocus; 8 curFocus := next hole in preorder after curFocus; 9 curNonterminal := nonterminal of curFocus;\n10 curContext := update curContext according to the values of the attributes at curFocus; 11 until there are no more holes;\nof RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17]. For the entire experiment, two stacked LSTMs are used, the number of neurons in each layer is 200, and backpropagation through time is truncated after 50 steps. The Adam optimizer [18] is used with learning rate .001, dropout [19] with a keep probability of .9 is applied to all layers except the softmax output, and both L1-norm and L2-norm regularization is applied to weights but not biases in all matrices with \u03bbL1 = \u03bbL2 = .0001. The NAM\u2019s \u03bbi values were set to .1, chosen so that the order of magnitude of the gradients for each term were roughly equal. As de novo generation is the goal in mind, all models were trained until their generation performance no longer improved as measured by the evaluation criteria discussed below.\nAn artificial corpus created for the work here is a set of 1,500 simple C programs containing elementary arithmetic operations, variable manipulations, and control-flow operations, 15% of which were held out for testing. There are an average of 7.01 unique variables, 3.29 unique types, 6.47 procedures, and 101.82 lines of code per program, providing a challenge for both constraints by having numerous declarations, multiple types in the same program, and changing scopes. The full corpus is available in the supplementary material. Programs in the corpus are translated to an abstract syntax tree (AST) in the C Intermediate Language (CIL) [20]. The set of these ASTs can be interpreted as a sample from an L-attributed attribute grammar G."}, {"heading": "3.1 Evaluation criteria", "text": "Our experiments were designed to answer the following questions:\nA. What is the quality of simulated samples? B. How well do the models represent the training data? C. At what rate are constraints violated while sampling?\nMethods for evaluating the quality of the learned model in generation tasks can be more subjective than in prediction tasks, where performance on held-out test sets is relevant. In our case, however, we can make various measurements of error rates when internal nodes are generated, as well as test\nwhether a generated tree satisfies the constraint as a whole, which provides an overall measure of success.\nThree measurements are used throughout\n1. The ability to learn the corpus as measured by the average negative log-likelihood of the training samples under each model. 2. The number of predictions made that violate the constraint while generating new samples. 3. The number of trees that are entirely legal under the specific constraint under consideration. (In\neach generated tree, one constraint violation is sufficient to make the whole tree illegal.)"}, {"heading": "3.2 Declared-variable constraint", "text": "Our first experiment imposed the constraint that every variable used must be declared (see columns 2-5 of Fig. 4, and Fig. 5). As shown by Fig. 4, even though the vanilla RNN gets to see the whole tree prior to the node requiring a prediction, it still makes many mistakes. For comparison, a stochastic context-free grammar that has been given the same augmented input is shown in Fig. 5. Since it now includes the context vector, it is referred to hereafter as a stochastic grammar with context (SGWC). This model will never choose to use variable v if the element of the context vector corresponding to v is not set to 1, and thus never violates the constraint. However, it does not specialize to the corpus very well, motivating the use of neural models that capture richer patterns in the training-set. The NAM\u2019s modified objective function offers a modest improvement, but augmenting the input with the context vector provides a much more significant improvement. Moreover, the latter effect does not dominate the former: the full NAM with both improvements performs the best overall.\nSome insight into how the models differ can be gleaned from the average negative log-likelihoods (see Fig. 5). As expected, the NAM\u2019s loss term acts as a regularization term, and even though training-set trees are less likely, the result is improved generation ability (see Fig. 4). The better generation ability strongly suggests that the excess fitting that the vanilla RNN did to the training-set compared to the NAM w/ 3-level loss is best described as overfitting.\nThe average number of unique variables and average number of procedures in each generated program gives one measurement of each model\u2019s fidelity to the training corpus. Training-set trees varied, but averaged 7.0 unique variables and 6.5 procedures. The vanilla RNN uses more unique variables and has fewer procedures than programs in the training-set, corroborating the likelihood numbers\u2019 indication that they did not learn the corpus as well as the other models. The NAM with context and the NAM with both improvements yielded samples that resembled the corpus much more faithfully by these measurements (see Fig. 4).\nAugmenting the input with the context vector makes the representation of the input at each step much richer. Thus, the NAM with context is able to learn more valuable patterns of the training data that exist in this higher-dimensional space. The number of legal trees increases 2.8-fold over the baseline vanilla RNN. This drastic improvement still has some degree of overfitting that can be alleviated with the regularizing loss term. The full NAM thus has a slightly higher average negative log-likelihood than the one without the regularization, but it has the best results during generation (see Fig. 4)"}, {"heading": "3.3 Typesafe-variable constraint", "text": "The same relative performance is seen when we work with the second constraint. The SGWC generalizes to the test set especially poorly in this case, because the context vector varies more and\nthus there are rare or completely novel situations that the SGWC struggles with. The vanilla RNN is the same model as in Section 3.2, because it does not take the constraint into consideration in any way. The raw number of violations is lower in this setting because the tests for this constraint occur less frequently: not all variable uses involve multiple types that must agree. Even so, the simpler models produce fewer legal trees than in the experiment in Section 3.2."}, {"heading": "4 Related Work", "text": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24]. Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6]. The most closely related piece of work is by Maddison and Tarlow [21], who use log-bilinear tree-traversal models, a class of probabilistic pushdown automata, for program generation. Their model also addresses the problems of declarations of names and type consistency, and they use \u201ctraversal variables\u201d to propagate information from parts of the already-produced tree to influence what production is selected at a node. However, the state-transition function of their generator admits a simple \u201ctabular parameterization,\u201d whereas memory updates in our approach involve complex interactions between a neural and a logical machine. Also, their training process does not have an analog of our three-valued loss function.\nProgram generation is closely related to the problem of program synthesis, the problem of producing programs in a high-level language that implement a user-given specification. A recent body of work uses neural techniques [26, 27, 28] to solve this problem. Of these efforts, Balog et al. [28] and Murali et al. [27] use combinatorial search, guided by a neural network, to generate programs that satisfy language-level constraints. However, this literature has not studied neural architectures whose training predisposes them toward satisfying such constraints.\nThe work presented here can be related to several key concepts in the theory of grammars. Breaking down the generation of a tree into a series of non-terminals, terminals, and production rules is the same methodology used with stochastic context-free-grammars. As an automaton that sees an input stream that contains occurrences of \u201cpop\u201d and produces an output, a NAM is a form of transducer, namely a visibly-pushdown transducer [29].\nNeural stack machines like those in [30] augment an RNN with a stack, which the RNN must learn how to operate through differentiable approximations. In contrast, a NAM only needs to learn how to make use of data values generated by the logical machine, rather than additionally needing to learn how to mimic the logical machine\u2019s operations.\nThe new term that was introduced in the objective function can be thought of as a way to perform regularization. Many attempts at customized regularization have been demonstrated [31, 32]. Our regularization term allows NAMs to learn the set of all legal production rules without penalty, but regularizes the learning of the specific singleton relative to the set of legal production rules."}, {"heading": "5 Discussion", "text": "Learning to generate sequences with strong structural constraints would ideally be as easy as presenting an RNN exclusively with sequences that are members of the constrained space. As our experiments show, this can be difficult to achieve in practice. In some cases, though, aspects of the structure can be explicitly represented. NAMs provide a framework for incorporating the knowledge of these constraints into RNNs. They significantly outperform RNNs without the constraint when trained on the same data.\nWe have demonstrated the utility of NAMs for two L-attributed AG problems. The work here allows for the possibility of creating a generator of NAM systems: from a specification of a desired constrained language, one could generate the corresponding form for training. Moreover, these are just two of the possible types of mistakes that would prevent a program from passing a C compiler\u2019s many checks. A topic for future work is to incorporate enough C constraints so that generated programs would have a high probability of being compilable.\nBecause other sequences have a natural underlying parse tree and associated constraints that can be expressed using an L-attributed AG, another topic for future work is to explore the application of NAMs to other structured sequences, such as proof trees of a logic."}], "references": [{"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A neural representation of sketch drawings", "author": ["David Ha", "Douglas Eck"], "venue": "arXiv preprint arXiv:1704.03477,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Fuzzing with code fragments", "author": ["Christian Holler", "Kim Herzig", "Andreas Zeller"], "venue": "In USENIX Security Symposium,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On the naturalness of software", "author": ["Abram Hindle", "Earl T Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu"], "venue": "In Software Engineering (ICSE),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Code completion with statistical language models", "author": ["Veselin Raychev", "Martin Vechev", "Eran Yahav"], "venue": "In PLDI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Semantics of context-free languages", "author": ["D.E. Knuth"], "venue": "Mathematical Systems Theory, 2(2):127\u2013145, June", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1968}, {"title": "Semantics of context-free languages: Correction", "author": ["D.E. Knuth"], "venue": "Mathematical Systems Theory, 5(1):95\u201396, March", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1971}, {"title": "Attributed translations", "author": ["P.M. Lewis", "D.J. Rosenkrantz", "R.E. Stearns"], "venue": "J. Comput. Syst. Sci., 9(3):279\u2013307, December", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1974}, {"title": "Complexity characterizations of attribute grammar languages", "author": ["Sophocles Efremidis", "Christos H. Papadimitriou", "Martha Sideris"], "venue": "Inf. and Comp.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "The Synthesizer Generator: A System for Constructing Language- Based Editors", "author": ["T. Reps", "T. Teitelbaum"], "venue": "Springer-Verlag, NY,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1504.01106,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["Bing Shuai", "Zhen Zuo", "Bing Wang", "Gang Wang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "A batch-normalized recurrent network for sentiment classification", "author": ["Horia Margarit", "Raghav Subramaniam"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation", "author": ["Marijn F. Stollenga", "Wonmin Byeon", "Marcus Liwicki", "Juergen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1929}, {"title": "CIL: Intermediate language and tools for analysis and transformation of C programs", "author": ["George C. Necula", "Scott McPeak", "Shree P. Rahul", "Westley Weimer"], "venue": "In International Conference on Compiler Construction,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Structured generative models of natural source code", "author": ["C.J. Maddison", "D. Tarlow"], "venue": "ICML,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A statistical semantic language model for source code", "author": ["Tung Thanh Nguyen", "Anh Tuan Nguyen", "Hoan Anh Nguyen", "Tien N. Nguyen"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Graph-based statistical language model for code", "author": ["Anh Tuan Nguyen", "Tien N. Nguyen"], "venue": "In Proceedings of the 37th International Conference on Software Engineering - Volume 1,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "PHOG: Probabilistic model for code", "author": ["Pavol Bielik", "Veselin Raychev", "Martin Vechev"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Mining idioms from source code", "author": ["Miltiadis Allamanis", "Charles Sutton"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Neuro-symbolic program synthesis", "author": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1611.01855,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Bayesian sketch learning for program synthesis", "author": ["Vijayaraghavan Murali", "Swarat Chaudhuri", "Chris Jermaine"], "venue": "arXiv preprint arXiv:1703.05698,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Visibly pushdown transducers", "author": ["J.-F. Raskin", "F. Servais"], "venue": "ICALP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Bayesian recurrent neural network for language modeling", "author": ["Jen-Tzung Chien", "Yuan-Chu Ku"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "SAE-RNN deep learning for RGB-D based object recognition", "author": ["Jing Bai", "Yan Wu"], "venue": "In International Conference on Intelligent Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Neural networks have been applied successfully to many generative modeling tasks, from images with pixel-level detail [1] to strokes corresponding to crude sketches [2] to natural language in automated responses to user questions [3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 3, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 159, "endOffset": 165}, {"referenceID": 5, "context": "Program generation has many potential applications, including automatically testing programming tools [4] and assisting humans as they solve programming tasks [5, 6].", "startOffset": 159, "endOffset": 165}, {"referenceID": 6, "context": "We use the formalism of attribute grammars [7] as the language for expressing rich structural constraints over programs.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "1 An attribute grammar (AG) is a context-free grammar extended by attaching attributes to the terminal and nonterminal symbols of the grammar, and by supplying attribute equations to define attribute values [7].", "startOffset": 207, "endOffset": 210}, {"referenceID": 7, "context": "Noncircularity is a decidable property of AGs [8], and hence we can assume that no derivation tree exists in which an attribute instance is defined transitively in terms of itself.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "An AG is L-attributed [9] if, in each production X0 \u2192 X1, .", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "With reasonable assumptions about the computational power of attribute equations and attribute constraints, L-attributed AGs capture the class NP [10] (modulo a technical \u201cpadding\u201d adjustment).", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "\u201d (The notation is adapted from the Synthesizer Generator [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "While other attempts at learning models that can be used to generate trees include performing convolutions over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm to adopt for presenting a tree to a neural model is a single-traversal, top-down, left-to-right sequentialization of the tree.", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "While other attempts at learning models that can be used to generate trees include performing convolutions over nodes in binary trees [12] or stacking multiple RNNs in fixed directions [13], a natural paradigm to adopt for presenting a tree to a neural model is a single-traversal, top-down, left-to-right sequentialization of the tree.", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "child[1]; 3 curNonterminal := nonterminal of curFocus; 4 curContext := the values of the attributes at curFocus; 5 repeat 6 op := ChooseOperator(curNonterminal, curContext); 7 Insert op([hole1], .", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 15, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 16, "context": "of RNN is the Long Short Term Memory (LSTM) cell [14], which has been favored in recent literature for its ability to learn long-term dependences, although the NAM framework is general to any type of RNN cell [15, 16, 17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 17, "context": "The Adam optimizer [18] is used with learning rate .", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "001, dropout [19] with a keep probability of .", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Programs in the corpus are translated to an abstract syntax tree (AST) in the C Intermediate Language (CIL) [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 5, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 22, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 23, "context": "The problem of corpus-driven program generation has been studied before [21, 6, 22, 23, 24].", "startOffset": 72, "endOffset": 91}, {"referenceID": 21, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 206, "endOffset": 210}, {"referenceID": 5, "context": "Statistical models used in this task include n-gram topic models [22], probabilistic tree-substitution grammars [25], a generalization of probabilistic grammars known as probabilistic higher-order grammars [24], and recurrent neural networks [6].", "startOffset": 242, "endOffset": 245}, {"referenceID": 20, "context": "The most closely related piece of work is by Maddison and Tarlow [21], who use log-bilinear tree-traversal models, a class of probabilistic pushdown automata, for program generation.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 26, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 27, "context": "A recent body of work uses neural techniques [26, 27, 28] to solve this problem.", "startOffset": 45, "endOffset": 57}, {"referenceID": 27, "context": "[28] and Murali et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] use combinatorial search, guided by a neural network, to generate programs that satisfy language-level constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "As an automaton that sees an input stream that contains occurrences of \u201cpop\u201d and produces an output, a NAM is a form of transducer, namely a visibly-pushdown transducer [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 29, "context": "Neural stack machines like those in [30] augment an RNN with a stack, which the RNN must learn how to operate through differentiable approximations.", "startOffset": 36, "endOffset": 40}, {"referenceID": 30, "context": "Many attempts at customized regularization have been demonstrated [31, 32].", "startOffset": 66, "endOffset": 74}, {"referenceID": 31, "context": "Many attempts at customized regularization have been demonstrated [31, 32].", "startOffset": 66, "endOffset": 74}], "year": 2017, "abstractText": "Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar. Neural Attribute Machines (NAMs) are equipped with a logical machine that represents the underlying grammar, which is used to teach the constraints to the neural machine by (i) augmenting the input sequence, and (ii) optimizing a custom loss function. Unlike traditional RNNs, NAMs are exposed to the grammar, as well as samples from the language of the grammar. During generation, NAMs make significantly fewer violations of the constraints of the underlying grammar than RNNs trained only on samples from the language of the grammar.", "creator": "LaTeX with hyperref package"}}}