{"id": "1705.09322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Convergent Tree-Backup and Retrace with Function Approximation", "abstract": "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples of optimization and performance optimisation in the field. We then show that the Tree Backup and Retrace algorithms are highly efficient in their general purpose, and that when we include the algorithm's complexity in the tree, the cost of re-reinforcing a tree will be the same.\n\n\n\n\n\nWe want to thank Thomas G. S. Kuznoynihan, Michael E. M. Tausman, James F. Naughton, Thomas H. Schumacher and Timothy B. Nieder, William A. Van Gogh, and T. Kuznoynihan.\nFor more information, please check out our previous review:\nhttp://tos.co/R6UqYQ6b6x", "histories": [["v1", "Thu, 25 May 2017 18:37:55 GMT  (1095kb,D)", "http://arxiv.org/abs/1705.09322v1", "NIPS 2017 submission"]], "COMMENTS": "NIPS 2017 submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmed touati", "pierre-luc bacon", "doina precup", "pascal vincent"], "accepted": false, "id": "1705.09322"}, "pdf": {"name": "1705.09322.pdf", "metadata": {"source": "CRF", "title": "Convergent Tree-Backup and Retrace with Function Approximation", "authors": ["Ahmed Touati"], "emails": ["ahmed.touati@umontreal.ca", "pbacon@cs.mcgill.ca", "dprecup@cs.mcgill.ca", "vincentp@iro.umontreal.ca"], "sections": [{"heading": null, "text": "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds."}, {"heading": "1 Introduction", "text": "Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al. (2011). Therefore, the design and analysis of off-policy algorithms using all the features of reinforcement learning, e.g. bootstrapping, multi-step updates (eligibility traces), and function approximation has been explored extensively over three decades. While off-policy learning and function approximation have been understood in isolation, their combination with multi-steps bootstrapping produces a so-called deadly triad (Sutton and Barto, 2017), i.e., many algorithms in this category are unstable.\nA convergent approach to this triad is provided by importance sampling, which \u201cbends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001). However, as the length of the trajectories increases, the variance of importance sampling corrections tends to become very large. An alternative approach which was developed for tabular representations of the value function is the tree backup algorithm (Precup, 2000) which, remarkably, does not rely on importance sampling directly. Tree Backup has recently been revisited by (Munos et al., 2016), who used its intuitions to develop the Retrace(\u03bb) algorithm.\nar X\niv :1\n70 5.\n09 32\n2v 1\n[ cs\n.L G\n] 2\n5 M\nBoth Tree Backup and Retrace(\u03bb) were only shown to converge with a tabular value function representation, and whether they would also converge with function approximation was an open question, which we tackle in this paper.\nFirst, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(\u03bb), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al. (2015), which also allows us to provide sample complexity bounds. Our algorithm can be implemented with both classical accumulating traces (Sutton and Barto, 1998) as well as Dutch traces (van Hasselt et al., 2014). We also provide empirical evidence of its good performance."}, {"heading": "2 Background and notation", "text": "In reinforcement learning, an agent interacts with its environment, assumed to be a discounted Markov Decision Process (S,A, \u03b3, P, r) with state space S , action spaceA, discount factor \u03b3 \u2208 [0, 1), transition probabilities P : S\u00d7A \u2192 (S \u2192 [0, 1]) mapping state-action pairs to distributions over next states, and reward function r : (S\u00d7A)\u2192 R. For simplicity, we assume the state and action space are finite, but our analysis can be extended to the countable or continuous case. We denote by \u03c0(a | s) the probability of choosing action a in state s under the policy \u03c0 : S \u2192 (A \u2192 [0, 1]). The action-value function for policy \u03c0, denoted Q\u03c0 : S \u00d7A \u2192 R, represents the expected sum of discounted rewards along the trajectories induced by the MDP and \u03c0: Q\u03c0(s, a) = E [ \u2211\u221e t=0 \u03b3\ntrt | (s0, a0) = (s, a), \u03c0]. Q\u03c0 can be obtained as the fixed point of the Bellman operator on action-value functions T \u03c0Q = r + \u03b3P\u03c0Q where r is the expected immediate reward and P\u03c0 is defined as:\n(P\u03c0Q)(s, a) := \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P (s\u2032 | s, a)\u03c0(a\u2032 | s\u2032)Q(s\u2032, a\u2032) .\nIn this paper, we are concerned with the policy evaluation problem (Sutton and Barto, 1998) under model-free off-policy learning. That is, we will evaluate a target policy \u03c0 using sampled trajectories (i.e. sequences of states, actions and rewards) drawn by following a different behavior policy \u00b5.\nIn order to obtain generalization between different state-action pairs, Q\u03c0 should be represented in a functional form. We focus on linear function approximation of the form:\nQ(s, a) := \u03b8>\u03c6(s, a) ,\nwhere \u03b8 \u2208 \u0398 \u2282 Rd is a weight vector and \u03c6 : S \u00d7A \u2192 Rd is a feature map from a state-action pairs to a given d-dimensional feature space.\nOff-policy learning Munos et al. (2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(\u03bb)\u03c0 (Harutyunyan et al., 2016) and Tree-backup (TB(\u03bb)) (Precup, 2000). It was shown that all these methods in fact share the following general form of the \u03bb-return (Sutton and Barto, 2017) for some coefficients \u03bai:\nG\u03bbk := Q(sk, ak) + \u221e\u2211 t=k (\u03bb\u03b3)t\u2212k\n( t\u220f\ni=k+1\n\u03bai ) (rt + \u03b3E\u03c0Q(st+1, \u00b7)\u2212Q(st, at))\n= Q(sk, ak) + \u221e\u2211 t=k (\u03bb\u03b3)t\u2212k\n( t\u220f\ni=k+1\n\u03bai ) \u03b4t ,\nwhere E\u03c0Q(st+1, .) := \u2211 a\u2208A \u03c0(a | st+1)Q(st+1, a) and \u03b4t = rt + \u03b3E\u03c0Q(st+1, .) \u2212 Q(st, at) is the temporal-difference (TD) error. The coefficients \u03bai determine how the TD errors would be scaled in order to correct for the discrepancy between target and behavior policies. From this unified representation, Munos et al. (2016) derived the Retrace(\u03bb) algorithm. Both TB(\u03bb) and Retrace(\u03bb) consider this form of return, but set \u03bai differently. The TB(\u03bb) updates correspond to the choice \u03bai = \u03c0(ai | si) while Retrace(\u03bb) sets \u03bai = min ( 1, \u03c0(ai | si)\u00b5(ai | si) ) , which is intended to allow learning from full returns when the target and behavior policies are very close. The importance sampling\napproach (Precup, 2000) converges in the tabular case, as it warps the behavior data distribution to the distribution that would be induced by the target policy \u03c0, but it also suffers from high variance. As for Q(\u03bb)\u03c0 , the behavior and target policies must be sufficiently close to guarantee convergence in the tabular case.\nThe analysis provided in this paper concerns TB(\u03bb) and Retrace(\u03bb), which are convergent in the tabular case, but have not been analyzed in the function approximation case. We start by noting that the Bellman operator 1 R underlying these these algorithms can be written in the following form:\n(RQ)(s, a) := Q(s, a) + E\u00b5 [ \u221e\u2211 t=0 (\u03bb\u03b3)t ( t\u220f i=1 \u03bai ) (rt + \u03b3E\u03c0Q(st+1, \u00b7)\u2212Q(st, at)) ] = Q(s, a) + (I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(T \u03c0Q\u2212Q)(s, a) ,\nwhere E\u00b5 is the expectation over the behavior policy and MDP transition probabilities and P\u03ba\u00b5 is the operator defined by:\n(P\u03ba\u00b5Q)(s, a) := \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A P (s\u2032 | s, a)\u00b5(a\u2032 | s\u2032)\u03ba(s\u2032, a\u2032)Q(s\u2032, a\u2032) .\nIn the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016). In this paper, we focus on what happens to these operators when combined with linear function approximation."}, {"heading": "3 Off-policy instability with function approximation", "text": "When combined with function approximation, the temporal difference updates corresponding to the \u03bb-return G\u03bbk are given by\n\u03b8k+1 = \u03b8k + \u03b1k ( G\u03bbk \u2212Q(sk, ak) ) \u2207\u03b8Q(sk, ak)\n= \u03b8k + \u03b1k ( \u221e\u2211 t=k (\u03bb\u03b3)t\u2212k ( t\u220f i=k+1 \u03bai ) \u03b4kt ) \u03c6(sk, ak) (1)\nwhere \u03b4kt = rt + \u03b3\u03b8 > k E\u03c0\u03c6(st+1, \u00b7)\u2212 \u03b8>k \u03c6(st, at) and \u03b1k are positive non-increasing step sizes. The updates (1) implies off-line updating as G\u03bbk is a quantity which depends on future rewards. This will be addressed later using eligibility traces: a mechanism to transform the off-line updates into efficient on-line ones. Since (1) describes stochastic updates, the following standard assumption is necessary: Assumption 1. The Markov chain induced by the behavior policy \u00b5 is ergodic and admits a unique stationary distribution, denoted by \u03be, over state-action pairs. We write \u039e for the diagonal matrix whose diagonal entries are (\u03be(s, a))s\u2208S,a\u2208A.\nOur first proposition establishes the expected behavior of the parameters in the limit. Proposition 1. If the behavior policy satisfies Assumption 1 and (\u03b8k)k\u22640 is the Markov process defined by (1) then: E[\u03b8k+1 | \u03b80] = (I + \u03b1kA)E[\u03b8k | \u03b80] + b , where matrix A and vector b are defined as follows:\nA := \u03a6>\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(\u03b3P\u03c0 \u2212 I)\u03a6. b := \u03a6>\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r .\nSketch of Proof (The full proof is in the appendix).\n\u03b8k+1 = \u03b8k + \u03b1k ( \u221e\u2211 t=k (\u03bb\u03b3)t\u2212k ( t\u220f i=k+1 \u03bai ) \u03c6(sk, ak) ( [\u03b3E\u03c0\u03c6(xt+1, \u00b7)\u2212 \u03c6(xt, at)]>\u03b8k + rt )) = \u03b8k + \u03b1k (Ak\u03b8k + bk)\nSo, E[\u03b8k+1 | \u03b8k] = (I + \u03b1kA)\u03b8k + b where A = E[Ak] and b = E[bk] 1We overload our notation over linear operators and their corresponding matrix representation.\nThe ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, \u03b8k converges to the unique solution \u03b8\u2217 of the system A\u03b8\u2217 + b = 0. This crucially relies on the matrix A being negative definite i.e y>Ay < 0,\u2200y 6= 0. In the on-policy case, when \u00b5 = \u03c0, we rely on the fact that the stationary distribution is invariant by the the transition matrix P\u03c0 i.e d>P\u03c0 = d> (Tsitsiklis et al., 1997; Sutton et al., 2015). However, this is no longer true for off-policy learning with arbitrary target/behavior policies and the matrix A may not be negative definite: the series \u03b8k may then diverge. We will now see that the same phenomenon may occur with TB(\u03bb) and Retrace(\u03bb).\nCounterexample: We extend the two-states MDP of Tsitsiklis et al. (1997), originally proposed to show the divergence of off-policy TD(0), to function approximation over state-action pairs. This environment has only two states, as shown in Figure 1, and two actions: left or right.\nIn this particular case, both TB(\u03bb) and Retrace(\u03bb) share the same matrix P\u03ba\u00b5:\nP\u03c0 = 0 1 0 00 1 0 01 0 0 0 1 0 0 0  , P\u03ba\u00b5 = 0.5P\u03c0, (P\u03c0)n = 0 1 0 00 1 0 00 1 0 0 0 1 0 0  \u2200n \u2265 2 . If we set \u03b2 := 0.5\u03b3\u03bb, we then have:\n(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121 =  1 \u03b21\u2212\u03b2 0 0 0 11\u2212\u03b2 0 0 \u03b2 \u03b2 2\n1\u2212\u03b2 1 0\n\u03b2 \u03b2 2\n1\u2212\u03b2 0 1\n , A = ( 6\u03b3\u2212\u03b2\u22125 1\u2212\u03b2 0\n3(\u03b3\u03b2\u2212\u03b22\u2212\u03b2\u2212\u03b3) 1\u2212\u03b2 \u22125\n) .\nTherefore, \u2200 \u03b3 \u2208 ( 56 , 1) and \u2200\u03bb \u2208 [0,min(1, 12\u03b3\u221210 \u03b3 )), the first eigenvalue e1 = 6\u03b3\u2212\u03b2\u22125\n1\u2212\u03b2 of A is positive. The basis vectors (1, 0)> and (0, 1)> are eigenvectors of A associated with e1 and -5, then if \u03b80 = (\u03b71, \u03b72)>, we obtain E[\u03b8k | \u03b80] = (\u03b71 \u220fk\u22121 i=0 (1 + \u03b1ie1), \u03b72 \u220fk\u22121 i=0 (1\u2212 5\u03b1i))> implying that\n||E[\u03b8k | \u03b80]|| \u2265 |\u03b71| \u220fk\u22121 i=0 (1 + \u03b1ie1). Hence, as \u2211 k \u03b1k \u2192\u221e, ||E[\u03b8k | \u03b80]|| \u2192 \u221e if \u03b71 6= 0."}, {"heading": "4 Convergent gradient off-policy algorithms", "text": "If A were negative definite, Retrace(\u03bb) or TB(\u03bb) with function approximation would converge to \u03b8\u2217 = \u2212A\u22121b. It is known (Bertsekas, 2011) that \u03a6\u03b8\u2217 is the fixed point of the projected Bellman operator : \u03a6\u03b8\u2217 = \u03a0\u00b5R(\u03a6\u03b8\u2217) , where \u03a0\u00b5 = \u03a6(\u03a6>\u039e\u03a6)\u22121\u03a6>\u039e is the orthogonal projection onto the space S = {\u03a6\u03b8|\u03b8 \u2208 Rd} with respect to the weighted Euclidean norm ||.||\u039e. Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding \u03b8\u2217 is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE):\nMSPBE(\u03b8) = 1\n2 ||\u03a0\u00b5R(\u03a6\u03b8)\u2212 \u03a6\u03b8||2\u039e .\nThis is the route that we take in this paper to derive convergent forms of TB(\u03bb) and Retrace(\u03bb). To do so, we first define our objective function in terms of A and b which we introduced in Proposition 1. Proposition 2. Let M := \u03a6>\u039e\u03a6 = E[\u03a6\u03a6>] be the covariance matrix of features. We have:\nMSPBE(\u03b8) = 1\n2 ||A\u03b8 + b||2M\u22121\n(The proof is provided in the appendix.)\nIn order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations. The implied double sampling makes it not straightforward to obtain an unbiased estimator of the gradient. Sutton et al. (2009b) addressed this problem with a two-timescale stochastic approximations. However, the algorithm obtained in this way is no longer a true stochastic gradient method with respect to the original objective. Liu et al. (2015) suggested an alternative which converts the original minimization problem into a primal-dual saddle point problem. This is the approach that we chose in this paper.\nThe convex conjugate of a real-valued function f is defined as: f\u2217(y) = sup\nx\u2208X (\u3008y, x\u3009 \u2212 f(x)) , (2)\nand f is convex, we have f\u2217\u2217 = f . Also, if f(x) = 12 ||x||M\u22121 , then f \u2217(x) = 12 ||x||M . Note that by going to the convex conjugate, we do not need to invert matrix M . We now go back to the original minimization problem:\nmin \u03b8\u2208\u0398 MSPBE(\u03b8)\u21d4 min \u03b8\u2208\u0398\n1 2 ||A\u03b8 + b||2M\u22121 \u21d4 min \u03b8\u2208\u0398 max \u03c9\u2208\u2126\n( \u3008A\u03b8 + b, \u03c9\u3009 \u2212 1\n2 ||\u03c9||2M ) We now apply the projected gradient updates for saddle-point problems (ascent in \u03c9 and descent in \u03b8)\n\u03c9k+1 = \u03a0\u2126 (\u03c9k + \u03b1k(A\u03b8k + b\u2212M\u03c9k)) , \u03b8k+1 = \u03a0\u0398 ( \u03b8k \u2212 \u03b1kA>\u03c9k ) . (3)\nwhere \u03a0\u0398 and \u03a0\u2126 are the orthogonal projections respectively on \u0398 and \u2126. As the A, b and M are all expectations, we could derive stochastic updates by drawing samples, which would yield unbiased estimates of the gradient.\nOn-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e\u22121 = 0 and\nek = \u03bb\u03b3\u03ba(sk, ak)ek\u22121 + \u03c6(sk, ak) \u2200k \u2265 0 We define: A\u0302k = ek(\u03b3E\u03c0[\u03c6(sk+1, .)] \u2212 \u03c6(sk, ak)])>, b\u0302k = r(sk, ak)ek, M\u0302k = \u03c6(sk, ak)\u03c6(sk, ak)\n>. Then, we have E[A\u0302k] = A, E[b\u0302k] = b and E[M\u0302k] = M . (The proof is provided in the appendix.)\nThis proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1\nTrue on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(\u03bb) that empirically performed better than GTD(\u03bb) with eligibility traces. Based on this work, we derive true on-line updates for our algorithm. The gradient off-policy algorithm was derived by turning the expected forward view into an expected backward view which can be sampled. In order to derive a true on-line update, we sample instead the forward view and then we turn the sampled forward view to an exact backward view using Theorem 1 in van Hasselt et al. (2014). If k denotes the time horizon, we consider the sampled truncated interim forward return:\n\u2200t < k, Y kt = k\u22121\u2211 i=t (\u03bb\u03b3)i\u2212t  i\u220f j=t+1 \u03baj  \u03b4i where \u03b4i = ri + \u03b8>t E\u03c0\u03c6(st+1, \u00b7)\u2212 \u03b8>t \u03c6(st, at), which gives us the sampled forward update of \u03c9:\n\u2200k < t, \u03c9kt+1 = \u03c9kt + \u03b1t(Y kt \u2212 \u03c6(xt, at)>\u03c9kt )\u03c6(xt, at) (4)\nAlgorithm 1 Gradient Off-policy with eligibility traces\nGiven: target policy \u03c0, behavior policy \u00b5 Initialize \u03b80 and \u03c90 \u03a3\u03b8 = \u03b10\u03b80, \u03a3\u03b1 = \u03b10 for n = 0 . . . do\nset e0 = 0 for k = 0 . . . end of episode do\nObserve sk, ak, rk, sk+1 according to \u00b5 Update traces ek = \u03bb\u03b3\u03ba(sk, ak)ek\u22121 + \u03c6(sk, ak) Update parameters \u03b4k = rk + \u03b3\u03b8 > k E\u03c0\u03c6(sk+1, .)\u2212 \u03b8>k \u03c6(sk, ak)\n\u03c9k+1 = \u03a0\u2126 ( \u03c9k + \u03b1k ( \u03b4kek \u2212 w>k \u03c6(sk, ak)\u03c6(sk, ak) )) \u03b8k+1 = \u03a0\u0398 ( \u03b8k \u2212 \u03b1kw>k ek (\u03b3E\u03c0\u03c6(sk+1, .)\u2212 \u03c6(sk, ak))\n) \u03a3\u03b8 = \u03a3\u03b8 + \u03b1k\u03b8k, \u03a3\u03b1 = \u03a3\u03b1 + \u03b1k\nend for end for Output: Polyak-average : \u03b8\u0304 = \u03a3\u03b8/\u03a3\u03b1\nProposition 4. For any k, the parameter \u03c9kk defined by the forward view (4) is equal to \u03c9k defined by the following backward view:\ne\u03c9\u22121 = 0\ne\u03c9k = \u03bb\u03b3\u03bake \u03c9 k\u22121 + \u03b1k(1\u2212 \u03bb\u03b3\u03bak\u03c6(sk, ak)>e\u03c9k\u22121)\u03c6(sk, ak) \u2200k \u2265 0\n\u03c9k+1 = \u03c9k + \u03b4ke \u03c9 k \u2212 \u03b1t\u03c6(sk, ak)>\u03c9k\u03c6(sk, ak)\nSketch of Proof (the full proof is in the appendix). We show that the temporal differences Y k+1t \u2212Y kt are related through: \u2200t < k, Y k+1t \u2212 Y kt = \u03bb\u03b3\u03bat+1 ( Y k+1t+1 \u2212 Y kt+1 ) and then we apply Theorem 1 in van Hasselt et al. (2014).\nThe resulting detailed procedure is provided in Algorithm 2.\nNote that when \u03bb is equal to zero, the Algorithm 1 and 2 both reduce to the same update: \u03c9k+1 = \u03a0\u2126 ( \u03c9k + \u03b1k(\u03b4k \u2212 \u03c6(sk, ak)>\u03c9k)\u03c6(sk, ak) ) \u03b8k+1 = \u03a0\u0398 ( \u03b8k \u2212 \u03b1k\u03c6(sk, ak)>wk(\u03b3E\u03c0[\u03c6(sk+1, .)]\u2212 \u03c6(sk, ak)])\n) Convergence analysis: Our algorithm is an instance of the mirror stochastic approximation described in Nemirovski et al. (2009). We need the following assumptions to prove convergence.\nAssumption 2. Matrices A and M are non singular. This implies that the unconstrained saddle point problem admits a unique solution (\u03b8\u2217, \u03c9\u2217) = (\u2212A\u22121b, 0). Assumption 3. The features and reward functions are uniformly bounded. Assumption 4. The feasible sets \u0398 and \u2126 are bounded closed convex sets and (\u03b8\u2217, \u03c9\u2217) \u2208 \u0398\u00d7 \u2126. Proposition 5. We consider the Polyak average \u03b8\u0304n = \u2211n i=0 \u03b1i\u03b8i/ \u2211n i=0 \u03b1i where n is the total number of updates and \u03b8i are iterates of Algorithm (1) or (2). If assumptions 2, 3 and 4 are satisfied, there exists a constant B > 0, such that if \u03b1i = B/ \u221a n \u2200i = 0 . . . n then:\nE [ MSPBE(\u03b8\u0304n) ] \u2264 O(1/ \u221a n)\nSketch of Proof (the full proof is provided in the appendix). The proof is similar to the one in Liu et al. (2015) that gives a bound with high probability for GTD(0)/GTD2(0) algorithm. Our proof however provides a bound in expectation as we do not want to use more restrictive assumptions on the distributions of the A\u0302k, b\u0302k and M\u0302k estimates.\nAlgorithm 2 Gradient Off-policy with eligibility/Dutch traces\nGiven: target policy \u03c0, behavior policy \u00b5 Initialize \u03b80 and \u03c90 \u03a3\u03b8 = \u03b10\u03b80, \u03a3\u03b1 = \u03b10 for n = 0 . . . do\nset e\u03b8\u22121 = e \u03c9 \u22121 = 0 for k = 0 . . . end of episode do Observe sk, ak, rk, sk+1 according to \u00b5 Update traces ek = \u03bb\u03b3\u03ba(sk, ak)ek\u22121 + \u03c6(sk, ak) Update Dutch traces e\u03c9k = \u03bb\u03b3\u03bake \u03c9 k\u22121 + \u03b1k ( 1\u2212 \u03bb\u03b3\u03bak\u03c6(sk, ak)>e\u03c9k\u22121 ) \u03c6(sk, ak)\nUpdate parameters \u03b4k = rk + \u03b3\u03b8 > k E\u03c0\u03c6(sk+1, .)\u2212 \u03b8>k \u03c6(sk, ak) \u03c9k+1 = \u03a0\u2126 ( \u03c9k + \u03b4ke \u03c9 k \u2212 \u03b1k\u03c6(sk, ak)>\u03c9k\u03c6(sk, ak) ) \u03b8k+1 = \u03a0\u0398 ( \u03b8k \u2212 \u03b1kw>k ek (\u03b3E\u03c0[\u03c6(sk+1, .)]\u2212 \u03c6(sk, ak))\n) \u03a3\u03b8 = \u03a3\u03b8 + \u03b1k\u03b8k, \u03a3\u03b1 = \u03a3\u03b1 + \u03b1k\nend for end for Output: Polyak-average : \u03b8\u0304 = \u03a3\u03b8/\u03a3\u03b1"}, {"heading": "5 Experimental Results", "text": "To validate our theoretical results about instability, we test the TB(\u03bb), Retrace(\u03bb) and their gradient versions GTB(\u03bb) and GRetrace(\u03bb) in two environments. The first one is the 2-state counterexample that we detailed in the third section and the second is the 7-state versions of Baird\u2019s counterexample (Baird et al. (1995)). Figures 2 and 3 show the MSBPE (averaged over 20 runs) as a function of the number of iterations. We can see that the gradient methods converge in these two counterexamples whereas TB(\u03bb) and Retrace(\u03bb) diverge.\nWe also evaluated the efficiency and robustness of Retrace for higher values of \u03bb in a grid world environment with 4 \u00d7 4 cells and two terminal states. The state-action features were of the form: \u03c6(s, a) = (0 . . . \u03c6(s) . . . 0)> with \u03c6(s) located in the ath position. We represented state by its Cartesian coordinates. Figure 4 shows the RMSE (averaged over 20 runs) as a function of the step size, with each line corresponding to a different value of \u03bb. We notice that for higher value of \u03bb, GRetrace(\u03bb) and true on-line GRtrace(\u03bb) can reach a lower value of RMSE but their performance deteriorates faster for higher steps sizes in comparison with GTB(\u03bb) and true on-line GTB(\u03bb). The true on-line versions diverge only slightly slower which might be due to the fact that the updates are not fully true on-line as they combine accumulating and Dutch traces."}, {"heading": "6 Discussion", "text": "Our analysis highlighted for the first time the difficulties of combining the Tree Backup and Retrace algorithms with function approximation. We addressed these issues by formulating gradient-based algorithm versions of these algorithms which minimize the mean-square projected Bellman error. Using a saddle-point formulation, we were also able to provide convergence guarantees and characterize the convergence rate of our algorithms. Furthermore, we provided versions of these algorithms in both the true on-line (van Hasselt et al., 2014) setting with Dutch traces as well as for classical accumulating traces.\nMahmood et al. (2017) has recently introduced the ABQ(\u03b6) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(\u03b6) which is related to Retrace(\u03bb). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015). The saddle-point formulation also provides a rich literature on acceleration methods which could be incorporated in our algorithms. In particular, the stochastic extra-gradient method or proximal methods (Balamurugan and Bach, 2016) seem to be promising future directions."}, {"heading": "A Proof of Proposition 1", "text": "We compute E[Ak] and E[bk] where expectation are over trajectories drawn by executing the behavior policy: sk, ak, rk, ak+1, . . . st, at, rt, st+1 . . . where sk, ak \u223c d, rt = r(st, at), st+1 \u223c p(\u00b7 | st, at). We note that under stationarity of d, E[Ak] = E[A0] and E[bk] = E[b0]. Let \u03b8, \u03b8\u2032 \u2208 Rd and let Q = \u03a6\u03b8 and Q\u2032 = \u03a6\u03b8\u2032 their respective Q-functions.\n\u03b8\u2032>E[Ak]\u03b8 = E [ \u221e\u2211 t=0 (\u03bb\u03b3)t ( t\u220f i=1 \u03bai ) Q\u2032(s0, a0)[\u03b3E\u03c0Q(st+1, .)\u2212Q(st, at)]> ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t+1 a0:t\n[ Q\u2032(s0, a0) ( t\u220f i=1 \u03bai ) [\u03b3E\u03c0Q(st+1, .)\u2212Q(st, at)]> ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t a0:t\n[ Q\u2032(s0, a0) ( t\u220f i=1 \u03bai ) (\u03b3Est+1 [E\u03c0Q(st+1, .)|st, at]\u2212Q(st, at)) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t a0:t\n[ Q\u2032(s0, a0) ( t\u220f i=1 \u03bai ) (\u03b3 \u2211 s\u2032\u2208S \u2211 a\u2032\u2208A p(s\u2032|st, at)\u03c0(a\u2032|s\u2032)Q(s\u2032, a\u2032)\u2212Q(st, at)) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t a0:t\n[ Q\u2032(s0, a0) ( t\u220f i=1 \u03bai ) (\u03b3P\u03c0Q(st, at)\u2212Q(st, at)) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t\u22121 a0:t\u22121\n[ Q\u2032(s0, a0) ( t\u22121\u220f i=1 \u03bai )\u2211 s\u2032\u2208S \u2211 a\u2032\u2208A p(s\u2032|st\u22121, at\u22121)\u03ba(a\u2032, s\u2032)\u00b5(a\u2032|s\u2032)(\u03b3P\u03c0Q(s\u2032, a\u2032)\u2212Q(s\u2032, a\u2032)) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t\u22121 a0:t\u22121\n[ Q\u2032(s0, a0) ( t\u22121\u220f i=1 \u03bai ) P\u03ba\u00b5(\u03b3P\u03c0 \u2212 I)Q(st\u22121, at\u22121) ]\n= Es0,a0 [ Q\u2032(s0, a0) \u221e\u2211 t=0 (\u03bb\u03b3)t(P\u03ba\u00b5)t(\u03b3P\u03c0 \u2212 I)Q(x0, a0) ] = Es0,a0 [ Q\u2032(s0, a0)(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(\u03b3P\u03c0 \u2212 I)Q(s0, a0)\n] = \u2211 s\u2208S \u2211 a\u2208A \u03be(s, a)Q\u2032(s, a)(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(\u03b3P\u03c0 \u2212 I)Q(s, a)\n= Q\u2032\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(\u03b3P\u03c0 \u2212 I)Q So, \u03b8\u2032>E[Ak]\u03b8 = \u03b8\u2032>\u03a6>\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(\u03b3P\u03c0 \u2212 I)\u03a6\u03b8 \u2200\u03b8, \u03b8\u2032 \u2208 Rd, which implies that:\nE[Ak] = \u03a6>\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121(P\u03c0 \u2212 I)\u03a6\n\u03b8>E[bk] = E[ \u221e\u2211 t=0 (\u03bb\u03b3)t ( t\u220f i=1 \u03bai ) rtQ(s0, a0)] = \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t a0:t [ Q(s0, a0) ( t\u220f i=1 \u03bai ) r(st, at) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t\u22121 a0:t\u22121\n[ Q(s0, a0) ( t\u22121\u220f i=1 \u03bai )\u2211 s\u2032\u2208S \u2211 a\u2032\u2208A p(s\u2032|st\u22121, at\u22121)\u03ba(a\u2032, s\u2032)\u00b5(a\u2032|s\u2032)r(s\u2032, a\u2032) ]\n= \u221e\u2211 t=0 (\u03bb\u03b3)tEs0:t\u22121 a0:t\u22121\n[ Q(s0, a0) ( t\u22121\u220f i=1 \u03bai ) P\u03ba\u00b5r(s\u2032, a\u2032) ] = Es0,a0 [ Q(s0, a0)(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r(s0, s0) ] = \u2211 s\u2208S \u2211 a\u2208A \u03be(s, a)Q(s, a)(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r(s, a) = Q\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r\nSo, \u03b8>E[bk] = \u03b8>\u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r \u2200\u03b8 \u2208 Rd, which implies that:\nE[bk] = \u039e(I \u2212 \u03bb\u03b3P\u03ba\u00b5)\u22121r"}, {"heading": "B Proof of Proposition 2", "text": "MSPBE(\u03b8) = 1\n2 ||\u03a0\u00b5R(\u03a6\u03b8)\u2212 \u03a6\u03b8||2\u039e =\n1 2 ||\u03a0\u00b5 (R(\u03a6\u03b8)\u2212 \u03a6\u03b8) ||2\u039e\n= 1 2 (\u03a0\u00b5 (R(\u03a6\u03b8)\u2212 \u03a6\u03b8))> \u039e (\u03a0\u00b5 (R(\u03a6\u03b8)\u2212 \u03a6\u03b8))\n= 1\n2\n( \u03a6>\u039e (R(\u03a6\u03b8)\u2212 \u03a6\u03b8) )> (\u03a6>\u039e\u03a6)\u22121\u03a6>\u039e ( \u03a6(\u03a6>\u039e\u03a6)\u22121\u03a6>\u039e(R(\u03a6\u03b8)\u2212 \u03a6\u03b8) ) = 1\n2 ||\u03a6>\u039e (R(\u03a6\u03b8)\u2212 \u03a6\u03b8) ||2M\u22121\n= 1\n2 ||\u03a6>\u039e\n( (I \u2212 \u03bb\u03b3P\u00b5\u03c0)\u22121(T \u03c0 \u2212 \u03bb\u03b3P\u00b5\u03c0)\u03a6\u03b8 \u2212 \u03a6\u03b8 ) ||2M\u22121\n= 1\n2 ||\u03a6>\u039e(I \u2212 \u03bb\u03b3P\u00b5\u03c0)\u22121(\u03b3P\u03c0 \u2212 I)\u03a6\u03b8 + \u03a6>\u039e(I \u2212 \u03bb\u03b3P\u00b5\u03c0)\u22121r||2M\u22121\n= 1\n2 ||A\u03b8 + b||2M\u22121"}, {"heading": "C Proof of Proposition 3", "text": "Let\u2019s show that E[A\u0302k] = A. Let\u2019s \u2206t denotes [\u03b3E\u03c0\u03c6(st+1, .)> \u2212 \u03c6(st, at)>]\nA = E [ \u221e\u2211 t=k (\u03bb\u03b3)t\u2212k ( t\u220f i=k+1 \u03bai ) \u03c6(sk, ak)\u2206t ]\n= E [ \u03c6(sk, ak)\u2206k +\n\u221e\u2211 t=k+1 (\u03bb\u03b3)t\u2212k\n( t\u220f\ni=k+1\n\u03bai ) \u03c6(sk, ak)\u2206t ]\n= E [ \u03c6(sk, ak)\u2206k +\n\u221e\u2211 t=k (\u03bb\u03b3)t\u2212k+1 ( t+1\u220f i=k+1 \u03bai ) \u03c6(sk, ak)\u2206t+1 ]\n= E [ \u03c6(sk, ak)\u2206k + \u03bb\u03b3\u03ba(sk+1, ak+1)\u03c6(sk, ak)\u2206k+1 +\n\u221e\u2211 t=k+1 (\u03bb\u03b3)t\u2212k+1 ( t+1\u220f i=k+1 \u03bai ) \u03c6(sk, ak)\u2206t+1 ]\n(?) = E [ \u03c6(sk, ak)\u2206k + \u03bb\u03b3\u03ba(sk, ak)\u03c6(sk\u22121, ak\u22121)\u2206k +\n\u221e\u2211 t=k+1 (\u03bb\u03b3)t\u2212k+1 ( t+1\u220f i=k+1 \u03bai ) \u03c6(sk, ak)\u2206t+1 ] = E [ \u2206k(\u03c6(sk, ak) + \u03bb\u03b3\u03ba(sk, ak)\u03c6(sk\u22121, ak\u22121) + (\u03bb\u03b3) 2\u03ba(sk, ak)\u03ba(sk\u22121, ak\u22121)\u03c6(sk\u22122, ak\u22122) + ...) ]\n= E \u2206k  k\u2211 i=0 (\u03bb\u03b3)k\u2212i  k\u220f j=i+1 \u03baj \u03c6(xi, ai) \n= E[\u2206kek] = E[A\u0302k]\nwe have used in the line (?) the fact that E[\u03ba(sk+1, ak+1)\u03c6(skak)\u2206k+1] = E[\u03ba(sk, ak)\u03c6(sk\u22121ak\u22121)\u2206k] thanks to the stationarity of the distribution d.\nwe have also denote by ek the following vector:\nek = k\u2211 i=0 (\u03bb\u03b3)k\u2212i  k\u220f j=i+1 \u03baj \u03c6(si, ai) = \u03bb\u03b3\u03bak\nk\u22121\u2211 i=0 (\u03bb\u03b3)k\u22121\u2212i  k\u22121\u220f j=i+1 \u03baj \u03c6(si, ai) + \u03c6(sk, ak)\n= \u03bb\u03b3\u03bakek\u22121 + \u03c6(sk, ak)\nVector ek corresponds to the eligibility traces defined in the proposition. Similarly, we could show that E\u00b5[b\u0302k] = b."}, {"heading": "D Proof of Proposition 4", "text": "The return\u2019s temporal difference Y k+1t \u2212 Y kt are related through:\n\u2200t < k, Y k+1t \u2212 Y kt = k\u2211 i=t (\u03bb\u03b3)i\u2212t( i\u220f j=t+1 \u03baj)\u03b4i \u2212 k\u22121\u2211 i=t (\u03bb\u03b3)i\u2212t( i\u220f j=t+1 \u03baj)\u03b4i\n= (\u03bb\u03b3)k\u2212t  k\u220f j=t+1 \u03baj  \u03b4k = \u03bb\u03b3\u03bak+1\n(\u03bb\u03b3)k\u2212(t+1)  k\u220f j=t+2 \u03baj  \u03b4k \n= \u03bb\u03b3\u03bak+1 ( Y k+1t+1 \u2212 Y kt+1 ) We could then apply Theorem 1 of van Hasselt et al. (2014) that give us the following backward view:\ne0 = \u03b10\u03c6(x0, a0)\net = \u03bb\u03b3\u03batet\u22121 + \u03b1t(1\u2212 \u03bb\u03b3\u03bak\u03c6(st, at)>et\u22121)\u03c6(st, at) \u2200t > 0 \u03c9t+1 = \u03c9t + (Y t+1 t \u2212 Y tt )et + \u03b1t(Y tt \u2212 \u03c6(st, at)>\u03c9t)\u03c6(st, at)\n(?) = \u03c9t + \u03b4tet \u2212 \u03b1t\u03c6(st, at)>\u03c9t\u03c6(st, at)\nWe used in the line (?) that Y t+1t = \u03b4t and Y t t = 0"}, {"heading": "E Proof of Proposition 5", "text": "We prove here the convergence rate of the gradient algorithm with eligibility traces (1). The proof for the algorithm (2) with dutch traces is similar. The algorithm (1) is an instance of the Mirror Stochastic Approximation algorithm described in Nemirovski et al. (2009). For clarity, we propose to rephrase here the result of the article with simpler notation. Proposition 6. We consider a convex-concave function f defined on \u0398\u00d7 \u2126 \u2282 Rd \u00d7 Rd, where \u0398 and \u2126 are two bounded closed convex sets whose diameters are upper bounded by D\u03b8, D\u03c9 > 0. we assume that we have an increasing sequence of \u03c3-fields {Fk} such that, \u03b80, \u03c90 are F0 measurable and such that for k \u2265 1,\n\u03b8k = \u03a0\u0398(\u03b8k\u22121 \u2212 \u03b1kg\u03b8k)) (5) \u03c9k = \u03a0\u2126 (\u03c9k\u22121 + \u03b1kg \u03c9 k )) (6)\noutput : \u03b8\u0304K = \u2211K k=0 \u03b1k\u03b8k\u2211K k=0 \u03b1k , \u03c9\u0304K = \u2211K k=0 \u03b1k\u03c9k\u2211K k=0 \u03b1k\nwhere\n\u2022 \u03a0\u0398 and \u03a0\u2126 are orthogonal projection respectively on \u0398 and \u2126.\n\u2022 E(g\u03b8k|Fk\u22121) = \u2202xf(\u03b8k\u22121, \u03c9k\u22121) and E(g\u03c9k |Fk\u22121) = \u2202yf(\u03b8k\u22121, \u03c9k\u22121)\n\u2022 Its exists G\u03b8, G\u03c9 \u2265 0 such that, E(||g\u03b8k||2) \u2264 G2\u03b8 and E(||g\u03c9k ||2) \u2264 G2\u03c9\nIf \u2200k, \u03b1k = 2G\u221a5K where G 2 = 2D2\u03b8G 2 \u03b8 + 2D 2 \u03c9G 2 \u03c9 , then we have:\nmax \u03c9\u2208\u2126 f(\u03b8, \u03c9)\u2212min \u03b8\u2208\u0398\nf(\u03b8, \u03c9) \u2264 2G \u221a 5\nK (7)\nIn our case,\n\u2022 f(\u03b8, \u03c9) = \u3008A\u03b8 + b, \u03c9\u3009 \u2212 12 ||\u03c9|| 2 M .\n\u2022 g\u03c9k = A\u0302k\u03b8 + b\u0302k \u2212 M\u0302k\u03c9 \u2022 g\u03b8k = A\u0302>k \u03c9\nAs our sub/super gradient estimates are unbiased as shown in Proposition (3), we need only to prove that they have bounded variance. We denote by Rmax the uniform bound on rewards and by B the uniform bound on features and L = max(max\u03b8\u2208\u0398 \u2016\u03b8\u2016,max\u03c9\u2208\u2126 \u2016\u03c9\u2016). \u2016 \u00b7 \u2016 denotes the Euclidean norm and we recall that \u2016 \u00b7 \u2016 \u2264 \u221a d\u2016 \u00b7 \u2016\u221e.\n\u2016ek\u20162 = \u2016 k\u2211 i=0 (\u03bb\u03b3)k\u2212i( k\u220f j=i+1 \u03baj)\u03c6(si, ai)\u20162 \u2264 B2d 1\u2212 (\u03bb\u03b3)2\nE [ \u2016g\u03c9k \u20162 ] = E [ \u2016ek (\u03b3E\u03c0[\u03c6(sk+1, \u00b7)]\u2212 \u03c6(sk, ak))> \u03b8 + ekrk \u2212 \u03c6(sk, ak)>\u03c9\u03c6(sk, ak)\u20162 ] \u2264 E [ \u2016ek\u20162| (\u03b3E\u03c0[\u03c6(sk+1, \u00b7)]\u2212 \u03c6(sk, ak))> \u03b8|2 + \u2016ek\u20162|rk|2 + \u2016\u03c6(sk, ak)\u20162|\u03c6(sk, ak)>\u03c9|2\n] (?)\n\u2264 B 2d\n1\u2212 (\u03bb\u03b3)2 (1 + \u03b3)2B2dL2 +\nB2d\n1\u2212 (\u03bb\u03b3)2 R2max +B 4d2L2 := G2\u03c9.\nwhere we used Cauchy-Schwarz inequality in (?). Similarly, we prove that:\nE [ \u2016g\u03b8k\u20162 ] \u2264 B 2d\n1\u2212 (\u03bb\u03b3)2 (1 + \u03b3)2B2dL2 := G2\u03b8.\nWe get then the inequality (7). Now, we have that:\nmin \u03b8\u2208\u0398 f(\u03b8, \u03c9) \u2264 max \u03c9\u2208\u2126 min \u03b8\u2208\u0398 f(\u03b8, \u03c9) \u2264 min \u03b8\u2208\u0398 max \u03c9\u2208\u2126 f(\u03b8, \u03c9) = MSPBE(\u03b8?) = 0\nimplying that max \u03c9\u2208\u2126 f(\u03b8, \u03c9)\u2212min \u03b8\u2208\u0398 f(\u03b8, \u03c9) \u2265 max \u03c9\u2208\u2126 f(\u03b8, \u03c9)\n= max \u03c9\u2208\u2126 \u3008A\u03b8 + b, \u03c9\u3009 \u2212 1 2 ||\u03c9||2M = 1 2 \u2016A\u03b8 + b\u20162M\u22121\n= MSPBE(\u03b8)\nwhich gives the desired inequality in Proposition (5)"}, {"heading": "F Additional experimental results", "text": "The random walk problem is a unidimensional Markov chain with five states plus two absorbing terminal states as each end. The state-action features are in the from: \u03c6(s, a) = (0 . . . \u03c6(s)\ufe38\ufe37\ufe37\ufe38\nathposition\n. . . 0)>.\nWe used for \u03c6(s) two representations: the tabular representation (Figure 5) and the dependant representations (Figure 6). In the dependant representations, we have \u03c6(1) = (1, 0, 0)>, \u03c6(2) = ( 1\u221a\n2 , 1\u221a 2 , 0)>, \u03c6(3) =\n( 1\u221a 3 , 1\u221a 3 , 1\u221a 3 )>, \u03c6(4) = (0, 1\u221a 2 , 1\u221a 2 )>and \u03c6(5) = (0, 0, 1)>."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L Baird"], "venue": "Proceedings of the twelfth international conference on machine learning, pages 30\u201337.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Stochastic variance reduction methods for saddle-point problems", "author": ["P. Balamurugan", "F. Bach"], "venue": "arXiv preprint arXiv:1605.06398.", "citeRegEx": "Balamurugan and Bach,? 2016", "shortCiteRegEx": "Balamurugan and Bach", "year": 2016}, {"title": "Temporal difference methods for general projected equations", "author": ["D.P. Bertsekas"], "venue": "IEEE Transactions on Automatic Control, 56(9):2128\u20132139.", "citeRegEx": "Bertsekas,? 2011", "shortCiteRegEx": "Bertsekas", "year": 2011}, {"title": "Neuro-dynamic programming: an overview", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages 560\u2013564. IEEE.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1995", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1995}, {"title": "The o.d.e. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Borkar and Meyn,? \\Q2000\\E", "shortCiteRegEx": "Borkar and Meyn", "year": 2000}, {"title": "Q(\u03bb) with off-policy corrections", "author": ["A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos"], "venue": "CoRR, abs/1602.04951.", "citeRegEx": "Harutyunyan et al\\.,? 2016", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "L.-J."], "venue": "Machine Learning, 8(3-4):293\u2013321.", "citeRegEx": "Lin and L..J.,? 1992", "shortCiteRegEx": "Lin and L..J.", "year": 1992}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["B. Liu", "J. Liu", "M. Ghavamzadeh", "S. Mahadevan", "M. Petrik"], "venue": "UAI, pages 504\u2013513. Citeseer.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Gradient temporal-difference learning algorithms", "author": ["H.R. Maei"], "venue": null, "citeRegEx": "Maei,? \\Q2011\\E", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Multi-step off-policy learning without importance sampling ratios", "author": ["A.R. Mahmood", "H. Yu", "R.S. Sutton"], "venue": "arXiv preprint arXiv:1702.03006.", "citeRegEx": "Mahmood et al\\.,? 2017", "shortCiteRegEx": "Mahmood et al\\.", "year": 2017}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "Balcan, M. F. and Weinberger, K. Q., editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1928\u20131937, New York, New York, USA. PMLR.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["R. Munos", "T. Stepleton", "A. Harutyunyan", "M. Bellemare"], "venue": "Advances in Neural Information Processing Systems, pages 1046\u20131054.", "citeRegEx": "Munos et al\\.,? 2016", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on optimization, 19(4):1574\u20131609.", "citeRegEx": "Nemirovski et al\\.,? 2009", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup"], "venue": "Computer Science Department Faculty Publication Series, page 80.", "citeRegEx": "Precup,? 2000", "shortCiteRegEx": "Precup", "year": 2000}, {"title": "Off-policy temporal difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 417\u2013424, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Introduction to reinforcement learning, volume 135", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press Cambridge.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Second Edition. MIT Press.", "citeRegEx": "Sutton and Barto,? 2017", "shortCiteRegEx": "Sutton and Barto", "year": 2017}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning - ICML. ACM Press.", "citeRegEx": "Sutton et al\\.,? 2009a", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A convergent o(n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "C. Szepesv\u00e1ri"], "venue": "Advances in neural information processing systems, pages 1609\u20131616.", "citeRegEx": "Sutton et al\\.,? 2009b", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "The Journal of Machine Learning Research, 17:1\u201329.", "citeRegEx": "Sutton et al\\.,? 2015", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "The 10th International Conference on Autonomous Agents and Multiagent Systems Volume 2, AAMAS \u201911, pages 761\u2013768, Richland, SC. International Foundation for Autonomous Agents and Multiagent Systems.", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence, 112(1-2):181\u2013211.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal-difference networks", "author": ["R.S. Sutton", "B. Tanner"], "venue": "Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1377\u20131384.", "citeRegEx": "Sutton and Tanner,? 2004", "shortCiteRegEx": "Sutton and Tanner", "year": 2004}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "Off-policy td (\u03bb) with a true online equivalence", "author": ["H. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "In Proceedings of the 30th Conference on Uncertainty", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "Sample efficient actor-critic with experience", "author": ["Z. Wang", "V. Bapst", "N. Heess", "V. Mnih", "R. Munos", "K. Kavukcuoglu", "N. de Freitas"], "venue": "replay. CoRR,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "Gr\u00fcnwald, P., Hazan, E., and Kale, S., editors, Proceedings of The 28th Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 1724\u20131751, Paris, France. PMLR.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}, {"title": "For clarity, we propose to rephrase here the result of the article with simpler notation", "author": ["Nemirovski"], "venue": "Proposition 6. We consider a convex-concave function f defined on \u0398\u00d7 \u03a9 \u2282 R \u00d7 R, where \u0398 and \u03a9 are two bounded closed convex sets whose diameters are upper bounded by D\u03b8, D\u03c9 > 0. we assume that we have an increasing sequence of \u03c3-fields", "citeRegEx": "Nemirovski,? 2009", "shortCiteRegEx": "Nemirovski", "year": 2009}], "referenceMentions": [{"referenceID": 21, "context": "Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al.", "startOffset": 193, "endOffset": 214}, {"referenceID": 16, "context": "While off-policy learning and function approximation have been understood in isolation, their combination with multi-steps bootstrapping produces a so-called deadly triad (Sutton and Barto, 2017), i.", "startOffset": 171, "endOffset": 195}, {"referenceID": 13, "context": "A convergent approach to this triad is provided by importance sampling, which \u201cbends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001).", "startOffset": 139, "endOffset": 174}, {"referenceID": 14, "context": "A convergent approach to this triad is provided by importance sampling, which \u201cbends\" the behavior policy distribution onto the target one (Precup, 2000; Precup et al., 2001).", "startOffset": 139, "endOffset": 174}, {"referenceID": 13, "context": "An alternative approach which was developed for tabular representations of the value function is the tree backup algorithm (Precup, 2000) which, remarkably, does not rely on importance sampling directly.", "startOffset": 123, "endOffset": 137}, {"referenceID": 11, "context": "Tree Backup has recently been revisited by (Munos et al., 2016), who used its intuitions to develop the Retrace(\u03bb) algorithm.", "startOffset": 43, "endOffset": 63}, {"referenceID": 10, "context": "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al.", "startOffset": 264, "endOffset": 283}, {"referenceID": 10, "context": "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner).", "startOffset": 264, "endOffset": 303}, {"referenceID": 10, "context": "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al.", "startOffset": 264, "endOffset": 862}, {"referenceID": 10, "context": "1 Introduction Rather than being confined to their own stream of experience, off-policy learning algorithms are capable of leveraging data from a different behavior than the one being followed, which can provide many benefits: efficient parallel exploration as in Mnih et al. (2016); Wang et al. (2016), reuse of past experience with experience replay (Lin, 1992) and, in many practical contexts, learning form data produced by policies that are currently deployed, but which we want to improve (as in many scenarios of working with an industrial or health care partner). Moreover, a single stream of experience can be used to learn about a variety of different targets which may take the form of value functions corresponding to different policies and time scales (Sutton et al., 1999) or to predicting different reward functions as in Sutton and Tanner (2004); Sutton et al. (2011). Therefore, the design and analysis of off-policy algorithms using all the features of reinforcement learning, e.", "startOffset": 264, "endOffset": 884}, {"referenceID": 4, "context": "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(\u03bb), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample).", "startOffset": 60, "endOffset": 83}, {"referenceID": 15, "context": "Our algorithm can be implemented with both classical accumulating traces (Sutton and Barto, 1998) as well as Dutch traces (van Hasselt et al.", "startOffset": 73, "endOffset": 97}, {"referenceID": 4, "context": "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(\u03bb), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al.", "startOffset": 61, "endOffset": 510}, {"referenceID": 4, "context": "First, by studying the ordinary differential equation (ODE) (Borkar and Meyn, 2000) associated with Tree Backup and Retrace(\u03bb), we show that their combination with linear function approximation is in fact unstable (a point which we also illustrate with a counterexample). Insights gained from this analysis allow us to derive a new gradient-based algorithm which provably converges to the right solution. Instead of adapting the blueprint from gradient-based temporal difference learning Sutton et al. (2009b), we rely on the primal-dual saddle point formulation of Liu et al. (2015), which also allows us to provide sample complexity bounds.", "startOffset": 61, "endOffset": 584}, {"referenceID": 15, "context": "In this paper, we are concerned with the policy evaluation problem (Sutton and Barto, 1998) under model-free off-policy learning.", "startOffset": 67, "endOffset": 91}, {"referenceID": 13, "context": "(2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(\u03bb) (Harutyunyan et al.", "startOffset": 109, "endOffset": 123}, {"referenceID": 5, "context": "(2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(\u03bb) (Harutyunyan et al., 2016) and Tree-backup (TB(\u03bb)) (Precup, 2000).", "startOffset": 141, "endOffset": 167}, {"referenceID": 13, "context": ", 2016) and Tree-backup (TB(\u03bb)) (Precup, 2000).", "startOffset": 32, "endOffset": 46}, {"referenceID": 16, "context": "It was shown that all these methods in fact share the following general form of the \u03bb-return (Sutton and Barto, 2017) for some coefficients \u03bai: Gk := Q(sk, ak) + \u221e \u2211", "startOffset": 93, "endOffset": 117}, {"referenceID": 10, "context": "Off-policy learning Munos et al. (2016) provided a unified perspective on several off-policy learning algorithms, namely: importance sampling (Precup, 2000), off-policy Q(\u03bb) (Harutyunyan et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 11, "context": "From this unified representation, Munos et al. (2016) derived the Retrace(\u03bb) algorithm.", "startOffset": 34, "endOffset": 54}, {"referenceID": 13, "context": "approach (Precup, 2000) converges in the tabular case, as it warps the behavior data distribution to the distribution that would be induced by the target policy \u03c0, but it also suffers from high variance.", "startOffset": 9, "endOffset": 23}, {"referenceID": 13, "context": "In the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016).", "startOffset": 109, "endOffset": 143}, {"referenceID": 11, "context": "In the tabular case, these operators can be shown to yield contraction mappings with respect to the max norm (Precup, 2000; Munos et al., 2016).", "startOffset": 109, "endOffset": 143}, {"referenceID": 4, "context": "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 3, "context": "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997).", "startOffset": 152, "endOffset": 209}, {"referenceID": 19, "context": "e d>P\u03c0 = d> (Tsitsiklis et al., 1997; Sutton et al., 2015).", "startOffset": 12, "endOffset": 58}, {"referenceID": 2, "context": "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, \u03b8k converges to the unique solution \u03b8\u2217 of the system A\u03b8\u2217 + b = 0.", "startOffset": 153, "endOffset": 285}, {"referenceID": 2, "context": "The ODE (Ordinary Differential Equations) approach (Borkar and Meyn, 2000) is the main tool to establish convergence in the function approximation case (Bertsekas and Tsitsiklis, 1995; Tsitsiklis et al., 1997). In particular, we use Proposition 4.8 in (Bertsekas and Tsitsiklis (1995)), which states that under some conditions, \u03b8k converges to the unique solution \u03b8\u2217 of the system A\u03b8\u2217 + b = 0. This crucially relies on the matrix A being negative definite i.e y>Ay < 0,\u2200y 6= 0. In the on-policy case, when \u03bc = \u03c0, we rely on the fact that the stationary distribution is invariant by the the transition matrix P i.e d>P\u03c0 = d> (Tsitsiklis et al., 1997; Sutton et al., 2015). However, this is no longer true for off-policy learning with arbitrary target/behavior policies and the matrix A may not be negative definite: the series \u03b8k may then diverge. We will now see that the same phenomenon may occur with TB(\u03bb) and Retrace(\u03bb). Counterexample: We extend the two-states MDP of Tsitsiklis et al. (1997), originally proposed to show the divergence of off-policy TD(0), to function approximation over state-action pairs.", "startOffset": 153, "endOffset": 998}, {"referenceID": 2, "context": "It is known (Bertsekas, 2011) that \u03a6\u03b8\u2217 is the fixed point of the projected Bellman operator : \u03a6\u03b8\u2217 = \u03a0\u03bcR(\u03a6\u03b8\u2217) , where \u03a0 = \u03a6(\u03a6>\u039e\u03a6)\u22121\u03a6>\u039e is the orthogonal projection onto the space S = {\u03a6\u03b8|\u03b8 \u2208 R} with respect to the weighted Euclidean norm ||.", "startOffset": 12, "endOffset": 29}, {"referenceID": 17, "context": "Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding \u03b8\u2217 is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE): MSPBE(\u03b8) = 1 2 ||\u03a0R(\u03a6\u03b8)\u2212 \u03a6\u03b8||\u039e .", "startOffset": 144, "endOffset": 184}, {"referenceID": 7, "context": "Rather than computing the sequence of iterates given by the projected Bellman operator, another approach for finding \u03b8\u2217 is to directly minimize (Sutton et al., 2009a; Liu et al., 2015) the Mean Squared Projected Bellman Error (MSPBE): MSPBE(\u03b8) = 1 2 ||\u03a0R(\u03a6\u03b8)\u2212 \u03a6\u03b8||\u039e .", "startOffset": 144, "endOffset": 184}, {"referenceID": 16, "context": ") In order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations.", "startOffset": 108, "endOffset": 130}, {"referenceID": 16, "context": ") In order to derive parameter updates, we could compute gradients of the above expression explicitly as in Sutton et al. (2009b), but we would then obtain a gradient that is a product of expectations. The implied double sampling makes it not straightforward to obtain an unbiased estimator of the gradient. Sutton et al. (2009b) addressed this problem with a two-timescale stochastic approximations.", "startOffset": 108, "endOffset": 330}, {"referenceID": 7, "context": "Liu et al. (2015) suggested an alternative which converts the original minimization problem into a primal-dual saddle point problem.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011).", "startOffset": 142, "endOffset": 154}, {"referenceID": 8, "context": "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e\u22121 = 0 and ek = \u03bb\u03b3\u03ba(sk, ak)ek\u22121 + \u03c6(sk, ak) \u2200k \u2265 0 We define: \u00c2k = ek(\u03b3E\u03c0[\u03c6(sk+1, .)] \u2212 \u03c6(sk, ak)]), b\u0302k = r(sk, ak)ek, M\u0302k = \u03c6(sk, ak)\u03c6(sk, ak) >. Then, we have E[\u00c2k] = A, E[b\u0302k] = b and E[M\u0302k] = M . (The proof is provided in the appendix.) This proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1 True on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(\u03bb) that empirically performed better than GTD(\u03bb) with eligibility traces.", "startOffset": 143, "endOffset": 683}, {"referenceID": 8, "context": "On-line updates: We derive now on-line updates by exploiting equivalences in expectation between forward views and backward views outlined in (Maei, 2011). Proposition 3. Let ek b the eligibility traces vector, defined as e\u22121 = 0 and ek = \u03bb\u03b3\u03ba(sk, ak)ek\u22121 + \u03c6(sk, ak) \u2200k \u2265 0 We define: \u00c2k = ek(\u03b3E\u03c0[\u03c6(sk+1, .)] \u2212 \u03c6(sk, ak)]), b\u0302k = r(sk, ak)ek, M\u0302k = \u03c6(sk, ak)\u03c6(sk, ak) >. Then, we have E[\u00c2k] = A, E[b\u0302k] = b and E[M\u0302k] = M . (The proof is provided in the appendix.) This proposition allows us to replace the expectations in Eq. (3) by corresponding unbiased estimates. The resulting detailed procedure is provided in Algorithm 1 True on-line equivalence: In van Hasselt et al. (2014), the authors derived a true on-line update for GTD(\u03bb) that empirically performed better than GTD(\u03bb) with eligibility traces. Based on this work, we derive true on-line updates for our algorithm. The gradient off-policy algorithm was derived by turning the expected forward view into an expected backward view which can be sampled. In order to derive a true on-line update, we sample instead the forward view and then we turn the sampled forward view to an exact backward view using Theorem 1 in van Hasselt et al. (2014). If k denotes the time horizon, we consider the sampled truncated interim forward return:", "startOffset": 143, "endOffset": 1204}, {"referenceID": 24, "context": "We show that the temporal differences Y k+1 t \u2212Y k t are related through: \u2200t < k, Y k+1 t \u2212 Y k t = \u03bb\u03b3\u03bat+1 ( Y k+1 t+1 \u2212 Y k t+1 ) and then we apply Theorem 1 in van Hasselt et al. (2014). The resulting detailed procedure is provided in Algorithm 2.", "startOffset": 166, "endOffset": 188}, {"referenceID": 12, "context": "Convergence analysis: Our algorithm is an instance of the mirror stochastic approximation described in Nemirovski et al. (2009). We need the following assumptions to prove convergence.", "startOffset": 103, "endOffset": 128}, {"referenceID": 7, "context": "The proof is similar to the one in Liu et al. (2015) that gives a bound with high probability for GTD(0)/GTD2(0) algorithm.", "startOffset": 35, "endOffset": 53}, {"referenceID": 0, "context": "The first one is the 2-state counterexample that we detailed in the third section and the second is the 7-state versions of Baird\u2019s counterexample (Baird et al. (1995)).", "startOffset": 124, "endOffset": 168}, {"referenceID": 1, "context": "In particular, the stochastic extra-gradient method or proximal methods (Balamurugan and Bach, 2016) seem to be promising future directions.", "startOffset": 72, "endOffset": 100}, {"referenceID": 8, "context": "Mahmood et al. (2017) has recently introduced the ABQ(\u03b6) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Mahmood et al. (2017) has recently introduced the ABQ(\u03b6) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(\u03b6) which is related to Retrace(\u03bb). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation.", "startOffset": 0, "endOffset": 417}, {"referenceID": 8, "context": "Mahmood et al. (2017) has recently introduced the ABQ(\u03b6) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(\u03b6) which is related to Retrace(\u03bb). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015).", "startOffset": 0, "endOffset": 722}, {"referenceID": 8, "context": "Mahmood et al. (2017) has recently introduced the ABQ(\u03b6) algorithm which uses an action-dependent bootstrapping parameter that leads to off-policy multi-step learning without importance sampling ratios. They also derived a gradient-based algorithm called AB-Trace(\u03b6) which is related to Retrace(\u03bb). However, the resulting update is different from ours, as they use the two-timescale approach of Sutton et al. (2009a) as basis for their derivation. In contrast, our approach uses the saddle point formulation, avoiding the need for double sampling or different learning rates. Another benefit of this formulation is that it allows us to provide a bound of the convergence rate (proposition 5) whereas Mahmood et al. (2017) is restricted to a more general two-timescale asymptotic result from Yu (2015). The saddle-point formulation also provides a rich literature on acceleration methods which could be incorporated in our algorithms.", "startOffset": 0, "endOffset": 801}], "year": 2017, "abstractText": "Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds.", "creator": "LaTeX with hyperref package"}}}