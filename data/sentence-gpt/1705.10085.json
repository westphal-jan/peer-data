{"id": "1705.10085", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Temporal anomaly detection: calibrating the surprise", "abstract": "We propose a hybrid approach to temporal anomaly detection in user-database access data -- or more generally, any kind of subject-object co-occurrence data. Our methodology allows identifying anomalies based on a single stationary model, instead of requiring a full temporal one, which would be prohibitive in our setting. We learn our low-rank stationary model from the high-dimensional training data, and then fit a regression model for predicting the expected likelihood score of normal access patterns in the future. The disparity between the predicted and the observed likelihood scores is used to assess the \"surprise\". This approach enables calibration of the anomaly score so that time-varying normal behavior patterns are not considered anomalous. We provide a detailed description of the algorithm, including a convergence analysis, and report encouraging empirical results. One of the datasets we tested is new for the public domain. It consists of two months' worth of database access records from a live system. This dataset will be made publicly available, and is provided in the supplementary material. For more information, please visit http://www.paradigm.com/e-mw_index/. Our paper provides a comprehensive description of the methodology used.", "histories": [["v1", "Mon, 29 May 2017 09:16:34 GMT  (199kb,D)", "http://arxiv.org/abs/1705.10085v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["eyal gutflaish", "aryeh kontorovich", "sivan sabato", "ofer biller", "oded sofer"], "accepted": false, "id": "1705.10085"}, "pdf": {"name": "1705.10085.pdf", "metadata": {"source": "CRF", "title": "Temporal anomaly detection: calibrating the surprise", "authors": ["Eyal Gutflaish", "Aryeh Kontorovich", "Sivan Sabato", "Ofer Biller", "Oded Sofer"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Consider a security analyst examining user access logs of a large database system. A blatant security breach might involve a user with insufficient clearance attempting to access a restricted database table. However, there could be more subtle indicators of suspicious activity, such as users accessing database tables that are atypical of their past behavioral pattern, or at unusual times. Moreover, in a distributed attack, perhaps no single user has done anything particularly out of the ordinary, but the general pattern of access to different database tables is atypical in terms of frequency, time of day, etc.\nThe difficulty stemming from the nebulous definition of potentially suspicious activity is compounded by the fact that severe anomalies, by their very nature, are extremely rare occurrences, and when the goal is to learn to identify them, we generally do not expect the available training data to contain any examples. Furthermore, due to the large number of users and database tables in a typical system, a naive solution, which classifies all previously unseen access events as anomalies, will tend to trigger many false alarms. The latter issue is exacerbated further by the problem of \u2217Ben-Gurion University of the Negev \u2020IBM Security Division, Israel\nar X\niv :1\n70 5.\n10 08\n5v 1\n[ cs\n.C R\n] 2\n9 M\ncold start [Su and Khoshgoftaar, 2009] \u2014 that is, the activity of previously unseen users (say, new employees). This activity should not automatically be classified as an anomaly, or else too many false alarms will be issued. Thus, a main challenge of anomaly detection of temporal events from a complex system is to calibrate the surprise level associated with incoming events \u2014 and this is the central challenge that we address.\nProblem description. In this paper we address the problem of unsupervised anomaly detection in a temporal sequence of user-object access events. The events might be employees accessing different database tables, users interacting with a website, customers executing transactions, and so on. We assume that the users and the objects are atomic (that is, known by a name or identification number only). Time is discretized into fixed units (such as hours), and for each time unit, the access activity is recorded in a binary user-object incidence matrix.\nOur goal in this work is to develop an anomaly detection method which allows identifying distributed attacks. Such attacks cannot be characterized by a single suspicious access event \u2014 the latter can be handled by more direct means, such as a permission system; instead, they are characterized by system-wide suspicious access patterns. Thus, our goal is to detect anomalous time intervals, i.e. segments of activity that contain atypical behavior. This presents a host of novel issues, not present in static problems. We detect anomalies at the application-specified time scale, since it is intuitively clear that what might be considered anomalous at the scale of a minute is rather different from the same notion at the scale of hours or days. While we focus in this work on deciding whether a given time interval is anomalous, the proposed method could be adapted also to diagnosing a specific user\u2019s behavior, or a specific object\u2019s access pattern, as anomalous within a time interval. We leave the study of this adaptation to future work.\nFrom a bird\u2019s eye view, our approach consists of two orthogonal components: (1) fitting a generative model to the data and (2) assigning an anomaly score to new time segments based on the model we learned. Conceptually, the first component is fairly standard in anomaly detection applications \u2014 though the choice of generative model must be carried out judiciously. As we explain in the sequel, some natural candidates (such as Markov models or HMMs) are not appropriate for our setting. Given the high number, and atomicity, of users and objects in our application, we propose a model based on a low-rank assumption. Such a model allows decomposing users and objects into latent factors [Srebro et al., 2004], and discovering abnormal behavior patterns based on the latent factors of a user or object. We extend to our setting previous theoretical results on the sample complexity of learning such a model.\nOur main methodological innovation is in step (2). Here, the likelihood assigned to an observed behavior pattern, based on the fitted model, is converted into an anomaly score. We propose to learn the expected likelihood score from the training data, as an independent regression problem, and then to use the disparity between the predicted and observed likelihood scores as a measure of surprise. Thus, we compensate for characteristic discrepancies between the learned model and the observed behavior at certain times. For instance, it is expected that the activity will be different during different hours of the day. However, learning separate models for each possible activity type is prohibitive, for both statistical and computational reasons. Our approach enables us to learn a single model, yet adapt the anomaly score to time-varying normal behavior patterns.\nThis work was motivated by studying a real database monitoring system, with anomaly detection among its goals. We provide a new data set, which we call TDA (Temporal Database Accesses). It was generated by recording user accesses in a live database system (of the kind often monitored for anomalous behavior) over a two-month period. The data recorded thousands of users and database tables, and the rate of events was approximately 20, 000 per hour. The data is\nprovided in the form of binary access matrices indicating accesses over one-hour-long intervals. The full dataset will be made publicly available, and is provided in the supplementary material."}, {"heading": "2 Related Work", "text": "Intrusion detection methods can be roughly clustered into two main categories: rules-based and learning-based methods [Santos et al., 2014]. This paper deals with the latter, in a temporal setting. Temporal analysis [Lee et al., 2000] models the timing and frequencies of user accesses to the database. Approaches based on dependency and relation analysis look for a systematic redundancy in the event co-occurrence log [Chung et al., 1999, Kamra et al., 2008]. Sequence alignment methods [Srivastava et al., 2006] seek common sequences of events, then monitor the system behavior by checking for repetition of these sequences. Some methods gather various statistical information about the user accesses to the database, and use it to model the normal behavior [Spalka and Lehnhardt, 2005, Mathew et al., 2010]. Lee et al. [2002] analyzed SQL patterns in such an application. These approaches are inapplicable to our setting, since they rely on sequential data, while we have only aggregated data from each time interval.\nThe problem of change-point detection, while not strictly a subset of anomaly or intrusion detection, is of some relevance to the temporal setting [Takeuchi and Yamanishi, 2006, Tartakovsky et al., 2006, H\u00f6hle, 2010, Khaleghi and Ryabko, 2012, 2014]. A natural approach is to model the temporal process via a Markov or Hidden Markov Model, as was done in G\u00f6rnitz et al. [2015]. These approaches, however, cannot be feasibly applied to our setting, because of the high dimensionality of the aggregated access matrices. Additionally, our data is aggregated over time intervals rather than being sequential, which precludes a clean application of Markovian techniques. Lakhina et al. [2005] employ a low rank concatenation of entropy matrices for every feature at different intervals, achieved by PCA. Chan et al. [2003], Das and Schneider [2007], Das et al. [2008] use a variant of association rules based on groups to detect anomalies. The main disadvantage of association rules in our context is the inability to cope with new instances.\nOur use of low-rank matrix completion in the context of anomaly detection appears to be novel. However, the technique is well-established in areas of semi-supervised learning and collaborative filtering (e.g., Srebro et al. 2004, Mazumder et al. 2010, Sindhwani et al. 2010, Shamir and Shalev-Shwartz 2014, Hsieh et al. 2015)."}, {"heading": "3 Our approach", "text": "To detect anomalous access patterns, we begin by defining a probabilistic model for normal access patterns. Our approach is based on learning a baseline low-rank stationary model, and then modeling the deviation of the temporal model from the stationary one. This approach enables learning and detection using a feasible number of parameters. Denote the number of different users by n and the number of different objects by m. For simplicity of notation we fix m and n, however in practice they need not be known to the algorithm in advance. We assume that the data is provided as a sequence of consecutive time intervals, where for each time interval t an access matrix Bt \u2208 {0, 1}n\u00d7m is provided, where Bt(i, j) = I[user i accessed object j]. The length of a time interval is an application-specific parameter.\nThe goal of the algorithm is to assign an anomaly score to each new access matrix Bt which is observed after the training phase. Quite generally, the distribution of Bt might be modeled using a matrix \u03c0t \u2208 [0, 1]n\u00d7m, where \u03c0t(i, j) is the probability that user i accesses object j during time\ninterval t, and different entries in Bt are assumed statistically independent. Thus, at time interval t, any possible observation matrix G \u2208 {0, 1}n\u00d7m would be assigned a probability of\nP[Bt = G] = P\u03c0t [Bt = G] := \u220f\ni\u2208[n],j\u2208[m]\n\u03c0t(i, j) G(i,j)(1\u2212 \u03c0t(i, j))1\u2212G(i,j). (1)\nThis model allocates a separate set of parameters for each time interval, and is incapable of extrapolating beyond past observations. Hence, we instead posit a single baseline matrix \u03c0\u0304 \u2208 [0, 1]n\u00d7m, which approximates a stationary (time-independent) distribution. This matrix can be thought of as a rough approximation of \u03c0t for all time intervals t. It induces a distribution on observation matrices in a manner analogous to Eq. (1): P[Bt = G] = P\u03c0\u0304[Bt = G]. This model is similar to the one proposed in Davenport et al. [2014] for a non-temporal variant of matrix completion from probabilistic binary observations. We take the standard approach of assuming that \u03c0\u0304 is low-rank, motivated by the intuition that the relevance of a user to an object can be explained by a small number of latent factors (see, e.g., Su and Khoshgoftaar [2009], Sindhwani et al. [2010], Leskovec et al. [2014]).\nHaving obtained an approximation \u03c0\u0302 to \u03c0\u0304 based on the training set, we can calculate the log-likelihood of an observation matrix G at time interval t, as induced by the parameters \u03c0\u0302:\nLL(G, \u03c0\u0302) := logP\u03c0\u0302[G] = \u2211 i,j (G(i, j) log \u03c0\u0302(i, j) + (1\u2212G(i, j)) log(1\u2212 \u03c0\u0302(i, j))) .\nAt this point, one might consider assigning time interval t an anomaly score based on the value LL(Bt, \u03c0\u0302), where Bt is the actual matrix observed at time t: a lower log-likelihood value would indicate a higher anomaly level. The problem with this proposal is that it is likely that some time intervals will systematically exhibit behavior that deviates significantly from that of \u03c0\u0304, and these systematic deviations should not be classified as anomalies. For instance, it is expected that access patterns will differ between night and day, weekdays and weekends, holidays and workdays, and so on, as well as being affected by application-specific circumstances. Thus, if the application monitors a software company\u2019s database accesses, scheduled days of major version updates will likely have patterns different from other days. Thus, we need some way of accounting for systematic, non-anomalous, differences between time intervals.\nWe address this issue by proposing a compromise between the overly constraining stationary model defined by \u03c0\u0304 and the overly rich model in Eq. (1). Our approach is to model the similarity between \u03c0t and \u03c0\u0304 in terms of properties of the time interval t. This similarity can be formalized using the cross-entropy between \u03c0\u0304 and \u03c0t. Recall that the cross-entropy between two discrete distributions p, q isH(p, q) := \u2212 \u2211 i pi log(qi). For two distributions defined as above by matrices \u03c01, \u03c02 \u2208 [0, 1]m\u00d7n, we have\nH(\u03c01, \u03c02) = \u2211 i,j HBer(\u03c01(i, j), \u03c02(i, j)),\nwhere HBer(a, b) for a, b \u2208 [0, 1] is the cross-entropy between the distributions Bernoulli(a) and Bernoulli(b). The expected value of the measured log-likelihood score LL(Bt, \u03c0\u0302) satisfies\nEBt\u223c\u03c0t [LL(Bt, \u03c0\u0302)] = \u2212H(\u03c0t, \u03c0\u0302).\nTherefore, if the actual log-likelihood score LL(Bt, \u03c0\u0302) is far from \u2212H(\u03c0t, \u03c0\u0302), this can be considered an anomalous behavior. The value of H(\u03c0t, \u03c0\u0302) cannot be computed, since \u03c0t is unknown.\nInstead, we train a predictor in order to estimate it. Each time-interval t is represented by a vector vt \u2208 Rd of d time-dependent real-valued features, such as time-of-day, day-of-weak, day-ofmonth, auto-regressive features (such as the log-likelihood in a previous interval) and possibly application-specific features (such as a binary indicator major-version-delivery). Using these time dependent features, we fit a linear regression model vt 7\u2192 \u3008w\u0302, vt\u3009 parametrized by w\u0302 \u2208 Rd, where LL(Bt, \u03c0\u0302) \u2248 \u3008w\u0302, vt\u3009 on the training set.1 We then define the anomaly score of a new observed time interval t by\nDeviation(\u03c0\u0302, Bt, t) = |LL(Bt, \u03c0\u0302)\u2212 \u3008w\u0302, vt\u3009|. (2)\nThis approach enables identifying anomalous behavior, while avoiding many of the false-alarms resulting from normal differences between time intervals.\nLastly, we address the issue of cold-start [Su and Khoshgoftaar, 2009]. So far, we have assumed that each user and object appears at least once in the training data. This is unlikely in many scenarios. For instance, in the database-access setting, new employees and new database tables can be added over time. If the application monitors an open environment such as a public web site, then the users and objects modeled in \u03c0\u0304 could be a small minority of the set of users and objects observed during the deployment of the system. We address this issue by applying a process commonly known as folding (e.g., Deerwester et al. [1990], Manning and Sch\u00fctze [1999]) to incorporate the new users or objects into the model on the fly.\nIn the next section we give a detailed account of the full anomaly detection algorithm."}, {"heading": "4 The Algorithm", "text": "We describe the two phases of the algorithm: the training phase and the anomaly-detection phase. In the training phase, the algorithm has access to a training set S = (B1, . . . , BT ), where Bt are binary access matrices of consecutive time intervals. The algorithm splits the training set into two parts, S1 = (B1, . . . , BT1), S2 = (BT1+1, . . . , BT ). S1 is used to find an estimator \u03c0\u0302 for the probabilistic stationary model \u03c0\u0304, as described in Section 4.1. S2 is used to fit the log-likelihood regressor w\u0302. The full training phase is given in Section 4.2.\nIn the anomaly-detection phase, an access matrix is provided as input for each time interval, and the algorithm outputs an anomaly score for each such matrix using Eq. (2). The system could now present a ranking of all the intervals by anomaly score in a specific time frame, or display the top few, as specified by the desired user interface.\nFor simplicity of presentation, we describe the two phases under the assumption that no new users or objects appear after the model \u03c0\u0302 is estimated. We explain in Section 4.3 how the algorithm can be seamlessly adapted to handle new users or objects.\nThe most computationally expensive step in the algorithm is the SVD procedure in Alg. 1 below, which, naively, is cubic in the matrix dimensions. However, since in many applications the matrices are sparse, a sparse SVD algorithm could speed up computation (e.g., Larsen 2000). All other procedures are linear in m and/or n, as should be clear from context. Below we use several matrix norms. For a real-valued matrix A \u2208 Rm\u00d7n, denote the nuclear (trace) norm by \u2016A\u2016tr = \u2211 i \u03c3i, where \u03c3i are the singular values of A. Denote the Frobenius norm by\n\u2016A\u2016F = ( \u2211m i=1 \u2211n j=1A 2 i,j) 1 2 . Denote the spectral norm of by \u2016A\u2016sp = max\u03c3i.\n1Central to this approach is the assumption that anomalous intervals are very rare, and so the model is trained almost exclusively on non-anomalous behavior."}, {"heading": "4.1 Estimating the matrix model", "text": "Our probabilistic estimation problem in the first part of the training process is to estimate a low-rank probability matrix \u03c0\u0302 based on the sequence of matrices S1 = (B1, . . . , BT1), where Bt records accesses in time interval t. In our simplified probabilistic model, the Bt are assumed to be drawn i.i.d. according to some low-rank matrix \u03c0\u0304. As discussed in Section 3, this i.i.d. assumption is a deliberate simplification, which ignores possible time-dependencies in the probabilistic model.\nOur probabilistic model, and our low-rank assumption on \u03c0\u0304, are similar to those studied in Davenport et al. [2014], Hsieh et al. [2015]. However, in these works it is assumed that only one access matrix drawn from \u03c0\u0304 is available, while we have several matrices available at training time. A standard approach for finding a low-rank estimate [Fazel, 2002] is to minimize the mean-squared error of the matrix difference, and regularize using the trace norm, which is a convex relaxation of the low-rank constraint:\nF (B, \u03bb) := min \u03c0\u0302 \u2208[0,1]n\u00d7m\n\u2016\u03c0\u0302 \u2212B\u20162F + \u03bb\u2016\u03c0\u0302\u2016tr. (3)\nHere \u03bb > 0 balances the trade-off between fidelity to B and the low-rank structure. Existing generalization bounds [Davenport et al., 2014, Shamir and Shalev-Shwartz, 2014, Hsieh et al., 2015] give guarantees for the mean-squared-loss of the solution to the optimization problem in Eq. (3), for the case where B is a single matrix drawn according to \u03c0\u0304. Our approach to finding the low-rank structure is similar to the above, however instead of considering a single binary matrix B, we use the average matrix B\u0304 = 1T1 \u2211T1 t=1Bt. In Appendix A in the supplementary we prove a generalization bound for F (B\u0304, \u03bb), which converges to the true low-rank model as T1 increases. An efficient solution to the unconstrained version of Eq. (3), with \u03c0\u0302 \u2208 Rn\u00d7m, is given in Mazumder et al. [2010]: For a real-valued matrix A, let SVD(A) be the Singular Value Decomposition ofA. Let r be the rank of the matrix B\u0304, and let (U,D, V T) = SVD(B\u0304). Mazumder et al. [2010] show that\nargmin \u03c0\u0302 \u2016\u03c0\u0302 \u2212 B\u0304\u20162F + \u03bb\u2016\u03c0\u0302\u2016tr = UD\u03bb/2V T, (4)\nwhereD\u03bb = [max(d1\u2212\u03bb, 0), ...,max(dr\u2212\u03bb, 0)]. In practice, Shamir and Shalev-Shwartz [2014] observed that the solutions to Eq. (4) typically nearly satisfy the constraint \u03c0\u0302 \u2208 [0, 1]n\u00d7m, with few values slightly outside [0, 1]. For these values, they propose a simple clipping to fit them into [0, 1]. This approach reportedly results in solutions that are very similar to those obtained by solving the full constrained minimization; this matches our empirical observations. Hence, we follow the same approach. The full procedure for finding a model matrix \u03c0\u0302 given a regularization parameter \u03bb and a training set S is given in Alg. 1.\nAlgorithm 1 FindModel(\u03bb, S): Find model matrix Require: \u03bb > 0, training data S = (B1, . . . , BK) Ensure: \u03c0\u0302\n1: B\u0304 \u2190 1K \u2211K i=1Bt. 2: (U,D, V T)\u2190 SVD(B\u0304). 3: \u03c0\u2032 \u2190 UD\u03bb/2V T. 4: For each i \u2208 [m], j \u2208 [n], set \u03c0\u0302(i, j)\u2190 min(1,max(\u03c0\u0302\u2032(i, j), 0)). 5: Return \u03c0\u0302.\nNote that while in the rest of our algorithm we use log-likelihood as a measure of fit between the model \u03c0\u0302 and the observed matrix Bt, in Eq. (3) the Frobenius norm is used instead, and our generalization bound, given in Appendix A, bounds the mean-squared error. The reason is two-fold: first, as evident in our analysis, the Frobenius norm lends itself to a stable low-rank matrix approximation because of its Lipschitz property. Such a property does not hold for the loglikelihood loss near zero. Second, optimizing \u03c0\u0302 based on the log-likelihood requires significantly more computational resources. In particular, because of the sensitivity of the log-likelihood near 0, optimizing this objective without constraints tends to push the solution far outside the [0, 1] range, and clipping no longer helps. Instead, one is faced with solving a much more heavily constrained optimization problem, which presents a prohibitive computational cost for a run-of-the-mill system. Our experiments show that despite this apparent mismatch between the loss functions, the empirical results are promising. In future work, we plan to study efficient methods for directly optimizing the log-likelihood."}, {"heading": "4.2 The full training algorithm", "text": "In the first step of the training algorithm, the value of the regularization parameter \u03bb is selected by cross-validation, where the selected \u03bb is the one that maximizes the average log-likelihood score of the matrices in the validation set. Then, the selected \u03bb is used to find the estimate \u03c0\u0302. Recall that for this step, the first part of the training set, S1, is used.\n1. A set of values \u039b is initialized for cross validation. We used the set {\u2016B\u0304\u2016sp/2i}Ki=0, where K was selected adaptively, by identifying when further decreasing \u03bb did not improve the log-likelihood on the validation set.\n2. k-fold cross validation (e.g., k = 10) is performed to select \u03bb \u2208 \u039b: In fold i, S1 is divided to a training part St1(i) and a validation part S v 1 (i), and a model \u03c0\u0302\u03bb(i) is calculated by\n\u03c0\u0302\u03bb(i)\u2190 FindModel(\u03bb, St1(i)). The score of \u03bb is set to the average\nL(\u03bb) = 1\nk k\u2211 i=1 1 |Sv1 (i)| \u2211\nBt\u2208Sv1 (i)\nLL(Bt, \u03c0\u0302\u03bb(i)).\n3. The regularization parameter is set to \u03bb\u2217 \u2190 argmaxL(\u03bb).\n4. The estimated model is set to \u03c0\u0302 \u2190 FindModel(\u03bb\u2217, S1)\nHaving found a model estimate \u03c0\u0302, the second learning step is to find a regressor for predicting the expected log-likelihood for a time interval t.\n1. A training set {(vt, yt)}Tt=T1+1 for regression is calculated from S2 as follows:\n(a) The vector of time-dependent features vt is calculated using the definition of the features for t (e.g., time-of-day, day-of-weak, etc.)\n(b) yt \u2190 LL(Bt, \u03c0\u0302).\n2. w\u0302 is calculated by empirical least-squares: w\u0302 \u2190 argminw\u2208Rd \u2211T t=T1+1 (yt \u2212 \u3008w, vt\u3009)2.\nThe outputs of the training phase are \u03c0\u0302 and w\u0302, where \u03c0\u0302 is given as a low-rank matrix decomposition U, S, V of some rank k, where U \u2208 Rn\u00d7k, S \u2208 Rk\u00d7k, V \u2208 Rm\u00d7k, S is diagonal, and \u03c0\u0302 = USV T."}, {"heading": "4.3 Unseen objects: cold start", "text": "In the algorithm description above, we avoided the issue of users or objects that are observed for the first time only after the first phase of training, that is, after \u03c0\u0302 has been estimated. Such new elements might be observed for the first time during the regression phase, or later during the deployment phase, as anomaly scores are being computed. In the context of collaborative filtering, this issue is commonly known as the cold start problem [Su and Khoshgoftaar, 2009]. In our setting, the challenge is to assign likelihood scores to matrices Bt which include new rows or columns that do no appear in \u03c0\u0302.\nPrevious solutions to the cold start problem in the context of collaborative filtering have suggested finding an existing user whose pattern of accesses most resembles that of the new user, and assigning the new user the same prediction as the existing user or a weighted score of the most similar users [Shardanand and Maes, 1995]. Our setup is slightly different, since we are not attempting to predict the values of specific matrix entries. Our solution relies on similar ideas but adapts them to our setting. To calculate the log-likelihood of a matrix Bt which includes rows or columns not present in \u03c0\u0302, we calculate a new version of \u03c0\u0302 which extends to these rows and columns. The process is based on finding similar users/objects in latent space, the low-rank space spanned by \u03c0\u0302.\nEach new user or object is projected onto latent space, via a process commonly known as folding [Manning and Sch\u00fctze, 1999]. Then, we find its nearest neighbor in the existing \u03c0\u0302, based on the distances in latent space. Finally, we assign the new row/column the same probability vector as its nearest neighbor. Using distances in latent space carries two advantages: first, these are less liable to overfit. Second, this allows storing and searching over smaller matrices, instead of the original training matrix B\u0304, which is usually significantly larger.\nMore formally, let B\u03041 = 1T1 \u2211T1 i=1Bi \u2208 Rn\u00d7m be the matrix representing the aggregate access data from the training set S1. Let \u03c0\u0302 = USV T be the rank-k model estimated in the first step of the training phase. Let G = B\u03041V \u2208 Rn\u00d7k and H = B\u0304T1 U \u2208 Rm\u00d7k. Let Gi be the i\u2019th row in G, and let Hj be the j\u2019th row in H . These are the latent representations of user i and object j from B\u03041, respectively. Alg. 2 gives the procedure FoldedLL for calculating the folded log-likelihood of a new observation matrix Bt, assuming for simplicity that all new users/objects appear in the last rows/columns of Bt and its dimensions are n\u2032,m\u2032. The training algorithm and the procedure for assigning an anomaly score, described above, can be easily adapted to handle cold start, by replacing LL in the regression learning step and in the anomaly score step with FoldedLL. It should be noted that this approach will not work if the observed time interval has no shared users or objects with the model \u03c0\u0302, or very few of them. A possible remedy is to attempt to identify when this situation is about to occur, and to retrain the stationary model \u03c0\u0302. We leave this issue to future work."}, {"heading": "5 Experiments", "text": "We tested our algorithm on several datasets. The first dataset is TDA, described in Section 1, and provided in the supplementary material. TDA depicts accesses of users to a database in one-hour intervals over a two-month period. Here, each object is a database table. The other two datasets are the movie-rating datasets NetFlix [Netflix, 2009, Bennett et al., 2007] and MovieLens [Harper and Konstan, 2016]. The rating time stamps were used to generate the aggregated time-interval matrices. Here the objects are movies, and an access occurs when a user rates a movie. None of the available datasets contained known anomalous accesses. Thus, to evaluate the success of\nAlgorithm 2 FoldedLL(Bt, \u03c0\u0302, G,H,U, V ) 1: \u03c0\u0302fold \u2190 \u03c0\u0302 2: for Each row ul in Bt, for l \u2208 {n+ 1, . . . , n\u2032} do 3: u\u2032 \u2190 ul(1 : m) \u00b7 V 4: i\u2190 argmini \u2016u\u2032 \u2212Gi\u20162. 5: Append row i of \u03c0\u0302fold to the end of \u03c0\u0302fold. 6: end for 7: for each column vl in Bt for l \u2208 {m+ 1, . . . ,m\u2032} do 8: v\u2032 \u2190 vTl (1 : n) \u00b7 U 9: j \u2190 argminj \u2016v\u2032 \u2212Hj\u20162.\n10: Append column j of \u03c0\u0302fold to the end of \u03c0\u0302fold. 11: end for 12: Return LL(Bt, \u03c0\u0302fold)\nour algorithm, we injected anomalous behavior into random intervals, as explained below, and checked the percentile of the anomaly score of the anomalous interval, compared to the score of other intervals in the test data. A perfect result for the anomalous interval would be the maximal percentile of 100%. The results of our algorithm are compared to a baseline algorithm, which uses an anomaly score based only on the deviation from the mean log-likelihood in the regression training set, without adjusting for temporal differences using regression. This algorithm is labeled MEAN in the plots.\nTable 1 specifies the dataset paramters. For Netflix we used the timespan between 11-Mar-99 and 19-Apr-04, since data was incomplete for later periods. For Movielens we used years 2010 to 2014, inclusive. The following time-dependent features were used for regression: A binary \u201cweekend\u201d feature, the log-likelihood of the previous interval and of the one 24 hours ago, the number of total accesses in the current interval, and the length of time (in time-interval units) since the time of the last interval in the training set S1. In addition, in TDA we used the hour of the day h \u2208 {1, . . . , 24} and the shifted hour h+ 12 mod 24. In Netflix and Movielens, we used a day-of-the-week feature.\nAccuracy of regression We plot, for each dataset, the true log-likelihood of each test interval against the predicted log-likelihood based on the learned regressor w\u0302. A straight diagonal line would indicate a perfect prediction. We observe that the regression-based prediction is quite successful for these datasets, indicating that the use of a linear regressor here is reasonable. The plots and the correlation coefficients (\u03c1) are provided in Figure 1 (Top).\nRandom accesses In this experiment, an anomalous interval was simulated by adding random accesses to a random test interval: Each bit in the interval\u2019s access matrix was changed to 1 with an independent probability of > 0. The average anomaly score percentile of our algorithm and MEAN, over a 100 random experiments, against the value of log10( ), are plotted in Figure 1 (Bottom). The regression model improved the identification of the anomaly in a wide range of noise levels.\nAccesses at an anomalous time In this experiment, we simulated a behavior which is normal at one time, but possibly anomalous at a different time: two random intervals t1 and t2 were selected from the test set, and we set Bt1 \u2190 Bt2 . We repeated this experiment 100 times. Table 2 lists the percent of changed intervals that were identified in the 95% anomaly percentile or above. The full distribution of both algorithm\u2019s outputs is provided in Figure 2. The histograms depict the percent of anomalous intervals in each anomaly score percentile, in the MEAN algorithm and in our algorithm.\nIndeed, our algorithm identifies a significant proportion of the intervals as anomalous, while the baseline algorithm does not. Note that we do not expect a 100% success here: if t1 and t2 happen to have a similar pattern, e.g., they are at the same hour of the day, the new Bt1 should not be considered anomalous."}, {"heading": "A Convergence analysis", "text": "For two real-valued matrices X,Y \u2208 Rn\u00d7m, define the mean squared error between X and Y by\nMSE(X,Y ) := 1\nmn \u2016X \u2212 Y \u20162F .\nRecall that \u03c0\u0304 is the assumed stationary model, while \u03c0\u0302 is our estimate of that stationary model. Below we show that by finding F (B\u0304, \u03bb), as defined in Eq. (3), we are effectively minimizing MSE(\u03c0\u0302, \u03c0\u0304) up to a term that decays to zero. Eq. (3) minimizes the squared Frobenius norm \u2016\u03c0\u0302 \u2212 B\u20162F with an additional regularization term \u03bb\u2016\u03c0\u0302\u2016tr. This is equivalent to minimizing \u2016\u03c0\u0302\u2212B\u20162F subject to a constraint on \u2016\u03c0\u0302\u2016tr. Moreover, if S = (B1, . . . , BT ) and B\u0304 = 1T \u2211T t=1Bt, we have\n\u2016\u03c0\u0302 \u2212 B\u0304\u20162F = \u2016\u03c0\u0302\u20162F \u2212 2\u3008\u03c0\u0302, B\u0304\u3009+ \u2016B\u0304\u20162F = 1\nT T\u2211 t=1 \u2016\u03c0\u0302 \u2212Bt\u20162F \u2212 1 T T\u2211 t=1 \u2016Bt\u20162F + \u2016B\u0304\u20162F .\nTherefore,\n\u2016\u03c0\u0302 \u2212 B\u0304\u20162F = 1\nT T\u2211 t=1 \u2016\u03c0\u0302 \u2212Bt\u20162F + C = mn \u00b7 1 T T\u2211 t=1 MSE(\u03c0\u0302, Bt) + C,\nwhere C is a constant independent of \u03c0\u0302. By a similar derivation, for another constant C \u2032 independent of \u03c0\u0302,\nEB\u223c\u03c0\u0304[MSE(\u03c0\u0302, B)] = MSE(\u03c0\u0302, \u03c0\u0304) + C \u2032. Therefore, if |EB\u223c\u03c0\u0304[MSE(\u03c0\u0302, B)] \u2212 1T \u2211T t=1 MSE(\u03c0\u0302, Bt)| is small, finding F (B\u0304, \u03bb) is a good proxy for minimizing MSE(\u03c0\u0302, \u03c0\u0304). We prove a more general claim, which bounds the diffrence between the empirical loss and the true loss of a general Lipschitz loss, which we define below. Our result generalizes a result from Hsieh et al. [2015] to the case of a training sample with several matrices. The proof employs techniques from Hsieh et al. [2015] and Shamir and Shalev-Shwartz [2014].\nLet ` : [0, 1]\u00d7 [0, 1]\u2192 R+ be a loss function. For two matrices A,B \u2208 [0, 1]n\u00d7m, let\n`(A,B) := 1\nnm \u2211 i\u2208[n],j\u2208[m] `(A(i, j), B(i, j)).\nFor L > 0, ` is L-Lipschitz in the first argument if\n\u2200x, x\u2032, y \u2208 R, |`(x, y)\u2212 `(x\u2032, y)| \u2264 L|x\u2212 x\u2032|,\nwith an analogous definition for the second argument. For a matrix X \u2208 [0, 1]n\u00d7m and a distribution P over matrices [0, 1]n\u00d7m; denote the true loss of X by `(X,P ) := EY \u2208P [`(X,Y )]. For a (multi)set S \u2286 [0, 1]n\u00d7m, denote the empirical loss of X on S by `(X,S) := 1|S| \u2211 Y \u2208S [`(X,Y )].\nThe theorem below gives a guarantee for L-Lipschitz losses. Note that for the squared-loss ` := MSE, where MSE(x, y) := (x\u2212y)2, ` is 2-Lipschitz in both arguments: For x, x\u2032, y \u2208 [0, 1],\n|(x\u2212 y)2 \u2212 (x\u2032 \u2212 y)2| = |x2 \u2212 x\u20322 \u2212 2xy + 2x\u2032y| = |(x\u2212 x\u2032)(x+ x\u2032)\u2212 2y(x\u2212 x\u2032)| = |(x\u2212 x\u2032)(x+ x\u2032 \u2212 2y)| = |(x\u2212 x\u2032)||(x+ x\u2032 \u2212 2y)| \u2264 2|x\u2212 x\u2032|.\nThus the theorem below holds for the MSE loss with L = 2, proving that as the size of the training set grows, minimizing Eq. (3) converges to a minimization of MSE(\u03c0\u0302, \u03c0\u0304) with high probability. In the following theorem and proof, we use c to indicate a universal constant, whose value can change from line to line.\nTheorem A.1. Let `(x, y) be anL-Lipschitz loss. Let \u03c0\u0304 \u2208 [0, 1]n\u00d7m, and letD\u03c0\u0304 be the distribution satisfying the probabilistic model defined in Section 3. Assume w.l.o.g. that m \u2265 n. Let S = (B1, B2, . . . , BT ) \u223c Dm\u03c0\u0304 be an i.i.d. sample from D\u03c0\u0304 . With a probability of at least 1\u2212 \u03b4, for all \u03c0 \u2208 [0, 1]n\u00d7m such that \u2016\u03c0\u2016tr \u2264 \u03b3,\n|`(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S)| \u2264 c L\u03b3\nn \u221a mT + L\n\u221a ln( 2\u03b4 )\n2Tmn .\nProof. Let \u03a0 \u2286 [0, 1]n\u00d7m. Denote \u03c8(S) := sup\u03c0\u2208\u03a0(`(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S)). By definition,\n\u2200\u03c0 \u2208 \u03a0, `(\u03c0,D\u03c0\u0304) \u2264 `(\u03c0, S) + \u03c8(S). (5)\nNote that the sample S has Tnm independent (though not identically distributed) entries. Thus, we upper-bound \u03c8(S) with high probability using McDiarmid\u2019s inequality [McDiarmid, 1989], which states that if for every two samples S, S\u2032 which differ by a single entry, |\u03c8(S)\u2212\u03c8(S\u2032)| \u2264 \u03b1, then\nP [\u03c8(S)\u2212 E[\u03c8(S)] \u2265 ] \u2264 e \u22122 2 Tnm\u03b12 .\nTo bound \u03b1, consider two samples, S, S\u2032, where the matrices in S are denoted Bt and the matrices in S\u2032 are denoted B\u2032t. Suppose that S\n\u2032 differs from S by a single entry (i, j) in the matrix Bto , such that Bto(i, j) = 1 and B \u2032 to(i, j) = 0. We have\n|\u03c8(S)\u2212 \u03c8(S\u2032)| = | sup \u03c0 (`(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S))\u2212 sup \u03c0 (`(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S\u2032))|.\nFor bounded f, g : R \u2192 R, it holds that | supx f(x) \u2212 supx g(x)| \u2264 | supx(f(x) \u2212 g(x))|. Therefore\n|\u03c8(S)\u2212 \u03c8(S\u2032)| \u2264 1 T | sup \u03c0 \u2211 t (`(\u03c0,Bt)\u2212 `(\u03c0,B\u2032t))|\n= 1\nTmn | sup \u03c0 (`(\u03c0(i, j), Bto(i, j))\u2212 `(\u03c0(i, j), B\u2032to(i, j))|\n\u2264 L Tmn |Bto(i, j)\u2212B\u2032to(i, j)]| \u2264 L Tmn ,\nWhere we used the fact that ` is L-Lipschitz in the second argument. Setting \u03b1 = L/(Tnm) and applying McDiarmid\u2019s inequality, we get\nP[\u03c8(S)\u2212 E[\u03c8(S)] \u2265 ] \u2264 e\u22122 2Tmn/L2 .\nSetting the latter to \u03b42 , we obtain that with a probability of at least 1\u2212 \u03b4 2 ,\n\u03c8(S) \u2264 E[\u03c8(S)] + L\n\u221a ln( 2\u03b4 )\n2Tmn . (6)\nIt remains to bound E[\u03c8(S)]. Let S\u2032 = (B\u20321, . . . , B\u2032T ) \u223c DT\u03c0\u0304 be a sample which is independent from S. Following a standard symmetrization argument as in Hsieh et al. [2015], we have\nE[\u03c8(S)] = ES [sup \u03c0 (`(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S))]]\n= ES [sup \u03c0\n(ES\u2032 [`(\u03c0, S\u2032)\u2212 `(\u03c0, S)])]\n\u2264 ES,S\u2032 [sup \u03c0\n(`(\u03c0, S\u2032)\u2212 `(\u03c0, S))]\n= 1\nmnT ES,S\u2032 sup \u03c0 \u2211 i,j,t `(\u03c0(i, j), B\u2032t(i, j))\u2212 `(\u03c0(i, j), Bt(i, j))  . Letting \u03c3i,j,t \u223c Uniform{\u22121, 1} for i \u2208 [n], j \u2208 [m], t \u2208 [T ] be independent Rademacher variables, it follows that\nE[\u03c8(S)] = 1\nmnT ES,S\u2032,\u03c3 sup \u03c0 \u2211 i,j,t \u03c3i,j,t ( `(\u03c0(i, j), B\u2032t(i, j))\u2212 `(\u03c0(i, j), Bt(i, j)) ) \u2264 2 mnT ES,\u03c3 sup \u03c0 \u2211 i,j,t \u03c3i,j,t`(\u03c0(i, j), Bt(i, j))\n \u2264 2L mnT E\u03c3  sup \u03c0(i,j) \u2211 i,j,t \u03c3i,j,t\u03c0(i, j).\n (7) The last inequality follows from Talagrand\u2019s contraction principle [Ledoux and Talagrand, 1991], together with the fact that `(x, y) is L-Lipschitz in both arguments.\nNow, denoting \u03bdi,j := \u2211 t \u03c3i,j,t, we observe that\nE\u03c3 sup \u03c0 \u2211 i,j,t \u03c3i,j,t\u03c0(i, j)  = E\u03c3 sup \u03c0 \u2211 i,j \u03c0(i, j) \u2211 t \u03c3i,j,t  = E\u03c3 sup \u03c0 \u2211 i,j \u03c0(i, j)\u03bdi,j  . Since the nuclear and spectral norms are dual [e.g., Horn and Johnson, 2013, p. 362], a matrix\nH\u00f6lder inequality holds, where \u03bd is the matrix with entries \u03bdi,j :\u2211 i,j \u03c0(i, j)\u03bdi,j \u2264 \u2016\u03c0\u2016tr\u2016\u03bd\u2016sp .\nTherefore, combining with Eq. (7), and assuming sup\u03c0\u2208\u03a0 \u2016\u03c0\u2016tr \u2264 \u03b3, it follows that\nE[\u03c8(S)] \u2264 2L mnT E\u03c3[sup \u03c0\u2208\u03a0 \u2016\u03c0\u2016tr\u2016\u03bd\u2016sp ] \u2264 2L\u03b3 mnT E\u03c3[\u2016\u03bd\u2016sp ]. (8)\nIt remains to bound E\u03c3 [ ||\u03bd||\nsp\n] . To this end, recall the following result of Lata\u0142a [2005]:\nThere is a universal constant c > 0 such that for any random matrix Z \u2208 Rn\u00d7m with independent mean-zero entries, we have\nE [ ||Z||\nsp\n] \u2264 c max i \u221a\u2211 j E [Z(i, j)2] + max j \u221a\u2211 i E [Z(i, j)2] + 4 \u221a E [Z(i, j)4]  .\nNow \u03bdi,j is a sum of T i.i.d. Rademacher random variables, and thus E[\u03bdi,j ] = 0 and E[\u03bd2i,j ] = T . To bound the 4th moment, we appeal to Khinchine\u2019s inequality [Boucheron et al., 2013, Exercise 5.10], which states that E[\u03bd4i,j ] \u2264 4!2\u00b722E[\u03bd 2 i,j ]\n2 = 3T 2. Substituting these moment bounds into Lata\u0142a\u2019s bound, we get\nE [ ||\u03bd||sp ] \u2264 c (\u221a nT + \u221a mT + 4 \u221a nm \u221a T ) .\nSince 4 \u221a nm \u2264 12 ( \u221a n + \u221a m) and m \u2265 n, we have E [ ||\u03bd||sp ] \u2264 c \u221a Tm. Substituting this into Eq. (8), we get\nE[\u03c8(S)] \u2264 c L\u03b3 n \u221a mT .\nCombining this with Eq. (6) yields that with probability of at least 1\u2212 \u03b42 , if\n\u03c8(S) \u2264 c L\u03b3 n \u221a mT + L\n\u221a ln( 2\u03b4 )\n2Tmn .\nCombining this with Eq. (5) and the observation that\n\u2200\u03c0 \u2208 \u03a0 `(\u03c0,D\u03c0) \u2264 `(\u03c0, S) + sup \u03c0\u2032\u2208\u03a0 (`(\u03c0\u2032,D\u03c0)\u2212 `(\u03c0\u2032, S)) \u2264 `(\u03c0, S) + \u03c8(S),\nwe get that if sup\u03c0\u2208\u03a0 \u2016\u03c0\u2016tr \u2264 \u03b3, then with probability of at least 1\u2212 \u03b42 ,\n\u2200\u03c0 \u2208 \u03a0, `(\u03c0,D\u03c0\u0304)\u2212 `(\u03c0, S) \u2264 c L\u03b3\nn \u221a mT + L\n\u221a ln( 2\u03b4 )\n2Tmn .\nAn analogous argument yields the same bound for `(\u03c0, S)\u2212 `(\u03c0,D\u03c0\u0304), which implies the statement of the theorem via a union bound."}], "references": [{"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "In Proceedings of KDD cup and workshop,", "citeRegEx": "Bennett and Lanning,? \\Q2007\\E", "shortCiteRegEx": "Bennett and Lanning", "year": 2007}, {"title": "Concentration inequalities", "author": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": null, "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "A machine learning approach to anomaly detection", "author": ["P.K. Chan", "M.V. Mahoney", "M.H. Arshad"], "venue": "Department of Computer Sciences, Florida Institute of Technology, Melbourne,", "citeRegEx": "Chan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2003}, {"title": "DEMIDS: A misuse detection system for database", "author": ["C.Y. Chung", "M. Gertz", "KarlLevitt"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chung et al\\.", "year": 1999}, {"title": "Detecting anomalous records in categorical datasets", "author": ["K. Das", "J. Schneider"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Das and Schneider.,? \\Q2007\\E", "shortCiteRegEx": "Das and Schneider.", "year": 2007}, {"title": "Anomaly pattern detection in categorical datasets", "author": ["K. Das", "J. Schneider", "D.B. Neill"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Das et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Das et al\\.", "year": 2008}, {"title": "1-bit matrix completion", "author": ["M.A. Davenport", "Y. Plan", "E. van den Berg", "M. Wootters"], "venue": "Information and Inference,", "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis,", "citeRegEx": "Fazel.,? \\Q2002\\E", "shortCiteRegEx": "Fazel.", "year": 2002}, {"title": "Hidden markov anomaly detection", "author": ["N. G\u00f6rnitz", "M.L. Braun", "M. Kloft"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "G\u00f6rnitz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00f6rnitz et al\\.", "year": 2015}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS),", "citeRegEx": "Harper and Konstan.,? \\Q2016\\E", "shortCiteRegEx": "Harper and Konstan.", "year": 2016}, {"title": "Online change-point detection in categorical time series. In Statistical modelling and regression structures, pages 377\u2013397", "author": ["M. H\u00f6hle"], "venue": null, "citeRegEx": "H\u00f6hle.,? \\Q2010\\E", "shortCiteRegEx": "H\u00f6hle.", "year": 2010}, {"title": "Matrix analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "Horn and Johnson.,? \\Q2013\\E", "shortCiteRegEx": "Horn and Johnson.", "year": 2013}, {"title": "Pu learning for matrix completion", "author": ["C.-J. Hsieh", "N. Natarajan", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Hsieh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2015}, {"title": "Detecting anomalous access patterns in relational databases", "author": ["A. Kamra", "E. Terzi", "E. Bertino"], "venue": "VLDB J,", "citeRegEx": "Kamra et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kamra et al\\.", "year": 2008}, {"title": "Locating changes in highly dependent data with unknown number of change points", "author": ["A. Khaleghi", "D. Ryabko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Khaleghi and Ryabko.,? \\Q2012\\E", "shortCiteRegEx": "Khaleghi and Ryabko.", "year": 2012}, {"title": "Asymptotically consistent estimation of the number of change points in highly dependent time series", "author": ["A. Khaleghi", "D. Ryabko"], "venue": "In ICML,", "citeRegEx": "Khaleghi and Ryabko.,? \\Q2014\\E", "shortCiteRegEx": "Khaleghi and Ryabko.", "year": 2014}, {"title": "Mining anomalies using traffic feature distributions", "author": ["A. Lakhina", "M. Crovella", "C. Diot"], "venue": "In ACM SIGCOMM Computer Communication Review,", "citeRegEx": "Lakhina et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lakhina et al\\.", "year": 2005}, {"title": "Computing the svd for large and sparse matrices", "author": ["R.M. Larsen"], "venue": "SCCM, Stanford University, June,", "citeRegEx": "Larsen.,? \\Q2000\\E", "shortCiteRegEx": "Larsen.", "year": 2000}, {"title": "Some estimates of norms of random matrices", "author": ["R. Lata\u0142a"], "venue": "Proceedings of the American Mathematical Society,", "citeRegEx": "Lata\u0142a.,? \\Q2005\\E", "shortCiteRegEx": "Lata\u0142a.", "year": 2005}, {"title": "Learning fingerprints for a database intrusion detection system", "author": ["S.Y. Lee", "W.L. Low", "P.Y. Wong"], "venue": "In ESORICS: European Symposium on Research in Computer Security. LNCS, SpringerVerlag,", "citeRegEx": "Lee et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2002}, {"title": "Intrusion detection in real-time database systems via time signatures", "author": ["V.C.S. Lee", "J.A. Stankovic", "S.H. Son"], "venue": "In Proceedings of the Sixth IEEE Real-Time Technology and Applications Symposium (RTAS", "citeRegEx": "Lee et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2000}, {"title": "Mining of massive datasets", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "Leskovec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2014}, {"title": "Foundations of statistical natural language processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning and Sch\u00fctze.,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "A data-centric approach to insider attack detection in database systems", "author": ["S. Mathew", "M. Petropoulos", "H.Q. Ngo", "S.J. Upadhyaya"], "venue": "Recent Advances in Intrusion Detection, 13th International Symposium,", "citeRegEx": "Mathew et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mathew et al\\.", "year": 2010}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["R. Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of machine learning research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "Surveys in combinatorics,", "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Approaches and challenges in database intrusion detection", "author": ["R.J. Santos", "J. Bernardino", "MarcoVieira"], "venue": "SIGMOD Record,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Matrix completion with the trace norm: learning, bounding, and transducing", "author": ["O. Shamir", "S. Shalev-Shwartz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shamir and Shalev.Shwartz.,? \\Q2014\\E", "shortCiteRegEx": "Shamir and Shalev.Shwartz.", "year": 2014}, {"title": "Social information filtering: algorithms for automating \u201cword of mouth", "author": ["U. Shardanand", "P. Maes"], "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems,", "citeRegEx": "Shardanand and Maes.,? \\Q1995\\E", "shortCiteRegEx": "Shardanand and Maes.", "year": 1995}, {"title": "One-class matrix completion with low-density factorizations", "author": ["V. Sindhwani", "S.S. Bucak", "J. Hu", "A. Mojsilovic"], "venue": "In 2010 IEEE International Conference on Data Mining,", "citeRegEx": "Sindhwani et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2010}, {"title": "A comprehensive approach to anomaly detection in relational databases", "author": ["A. Spalka", "J. Lehnhardt"], "venue": "Data and Applications Security XIX, 19th Annual IFIP WG 11.3 Working Conference on Data and Applications Security,", "citeRegEx": "Spalka and Lehnhardt.,? \\Q2005\\E", "shortCiteRegEx": "Spalka and Lehnhardt.", "year": 2005}, {"title": "Generalization error bounds for collaborative prediction with low-rank matrices", "author": ["N. Srebro", "N. Alon", "T.S. Jaakkola"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Srebro et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2004}, {"title": "Database intrusion detection using weighted sequence mining", "author": ["A. Srivastava", "S. Sural", "A.K. Majumdar"], "venue": "JCP, 1(4):8\u201317,", "citeRegEx": "Srivastava et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2006}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in artificial intelligence,", "citeRegEx": "Su and Khoshgoftaar.,? \\Q2009\\E", "shortCiteRegEx": "Su and Khoshgoftaar.", "year": 2009}, {"title": "A unifying framework for detecting outliers and change points from time series", "author": ["J.-i. Takeuchi", "K. Yamanishi"], "venue": "IEEE transactions on Knowledge and Data Engineering,", "citeRegEx": "Takeuchi and Yamanishi.,? \\Q2006\\E", "shortCiteRegEx": "Takeuchi and Yamanishi.", "year": 2006}, {"title": "A novel approach to detection of intrusions in computer networks via adaptive sequential and batch-sequential change-point detection methods", "author": ["A.G. Tartakovsky", "B.L. Rozovskii", "R.B. Blazek", "H. Kim"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tartakovsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tartakovsky et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 34, "context": "cold start [Su and Khoshgoftaar, 2009] \u2014 that is, the activity of previously unseen users (say, new employees).", "startOffset": 11, "endOffset": 38}, {"referenceID": 32, "context": "Such a model allows decomposing users and objects into latent factors [Srebro et al., 2004], and discovering abnormal behavior patterns based on the latent factors of a user or object.", "startOffset": 70, "endOffset": 91}, {"referenceID": 27, "context": "Intrusion detection methods can be roughly clustered into two main categories: rules-based and learning-based methods [Santos et al., 2014].", "startOffset": 118, "endOffset": 139}, {"referenceID": 21, "context": "Temporal analysis [Lee et al., 2000] models the timing and frequencies of user accesses to the database.", "startOffset": 18, "endOffset": 36}, {"referenceID": 33, "context": "Sequence alignment methods [Srivastava et al., 2006] seek common sequences of events, then monitor the system behavior by checking for repetition of these sequences.", "startOffset": 27, "endOffset": 52}, {"referenceID": 2, "context": "Approaches based on dependency and relation analysis look for a systematic redundancy in the event co-occurrence log [Chung et al., 1999, Kamra et al., 2008]. Sequence alignment methods [Srivastava et al., 2006] seek common sequences of events, then monitor the system behavior by checking for repetition of these sequences. Some methods gather various statistical information about the user accesses to the database, and use it to model the normal behavior [Spalka and Lehnhardt, 2005, Mathew et al., 2010]. Lee et al. [2002] analyzed SQL patterns in such an application.", "startOffset": 118, "endOffset": 527}, {"referenceID": 2, "context": "Approaches based on dependency and relation analysis look for a systematic redundancy in the event co-occurrence log [Chung et al., 1999, Kamra et al., 2008]. Sequence alignment methods [Srivastava et al., 2006] seek common sequences of events, then monitor the system behavior by checking for repetition of these sequences. Some methods gather various statistical information about the user accesses to the database, and use it to model the normal behavior [Spalka and Lehnhardt, 2005, Mathew et al., 2010]. Lee et al. [2002] analyzed SQL patterns in such an application. These approaches are inapplicable to our setting, since they rely on sequential data, while we have only aggregated data from each time interval. The problem of change-point detection, while not strictly a subset of anomaly or intrusion detection, is of some relevance to the temporal setting [Takeuchi and Yamanishi, 2006, Tartakovsky et al., 2006, H\u00f6hle, 2010, Khaleghi and Ryabko, 2012, 2014]. A natural approach is to model the temporal process via a Markov or Hidden Markov Model, as was done in G\u00f6rnitz et al. [2015]. These approaches, however, cannot be feasibly applied to our setting, because of the high dimensionality of the aggregated access matrices.", "startOffset": 118, "endOffset": 1096}, {"referenceID": 2, "context": "Approaches based on dependency and relation analysis look for a systematic redundancy in the event co-occurrence log [Chung et al., 1999, Kamra et al., 2008]. Sequence alignment methods [Srivastava et al., 2006] seek common sequences of events, then monitor the system behavior by checking for repetition of these sequences. Some methods gather various statistical information about the user accesses to the database, and use it to model the normal behavior [Spalka and Lehnhardt, 2005, Mathew et al., 2010]. Lee et al. [2002] analyzed SQL patterns in such an application. These approaches are inapplicable to our setting, since they rely on sequential data, while we have only aggregated data from each time interval. The problem of change-point detection, while not strictly a subset of anomaly or intrusion detection, is of some relevance to the temporal setting [Takeuchi and Yamanishi, 2006, Tartakovsky et al., 2006, H\u00f6hle, 2010, Khaleghi and Ryabko, 2012, 2014]. A natural approach is to model the temporal process via a Markov or Hidden Markov Model, as was done in G\u00f6rnitz et al. [2015]. These approaches, however, cannot be feasibly applied to our setting, because of the high dimensionality of the aggregated access matrices. Additionally, our data is aggregated over time intervals rather than being sequential, which precludes a clean application of Markovian techniques. Lakhina et al. [2005] employ a low rank concatenation of entropy matrices for every feature at different intervals, achieved by PCA.", "startOffset": 118, "endOffset": 1407}, {"referenceID": 2, "context": "Chan et al. [2003], Das and Schneider [2007], Das et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Chan et al. [2003], Das and Schneider [2007], Das et al.", "startOffset": 0, "endOffset": 45}, {"referenceID": 2, "context": "Chan et al. [2003], Das and Schneider [2007], Das et al. [2008] use a variant of association rules based on groups to detect anomalies.", "startOffset": 0, "endOffset": 64}, {"referenceID": 6, "context": "This model is similar to the one proposed in Davenport et al. [2014] for a non-temporal variant of matrix completion from probabilistic binary observations.", "startOffset": 45, "endOffset": 69}, {"referenceID": 6, "context": "This model is similar to the one proposed in Davenport et al. [2014] for a non-temporal variant of matrix completion from probabilistic binary observations. We take the standard approach of assuming that \u03c0\u0304 is low-rank, motivated by the intuition that the relevance of a user to an object can be explained by a small number of latent factors (see, e.g., Su and Khoshgoftaar [2009], Sindhwani et al.", "startOffset": 45, "endOffset": 381}, {"referenceID": 6, "context": "This model is similar to the one proposed in Davenport et al. [2014] for a non-temporal variant of matrix completion from probabilistic binary observations. We take the standard approach of assuming that \u03c0\u0304 is low-rank, motivated by the intuition that the relevance of a user to an object can be explained by a small number of latent factors (see, e.g., Su and Khoshgoftaar [2009], Sindhwani et al. [2010], Leskovec et al.", "startOffset": 45, "endOffset": 406}, {"referenceID": 6, "context": "This model is similar to the one proposed in Davenport et al. [2014] for a non-temporal variant of matrix completion from probabilistic binary observations. We take the standard approach of assuming that \u03c0\u0304 is low-rank, motivated by the intuition that the relevance of a user to an object can be explained by a small number of latent factors (see, e.g., Su and Khoshgoftaar [2009], Sindhwani et al. [2010], Leskovec et al. [2014]).", "startOffset": 45, "endOffset": 430}, {"referenceID": 34, "context": "Lastly, we address the issue of cold-start [Su and Khoshgoftaar, 2009].", "startOffset": 43, "endOffset": 70}, {"referenceID": 7, "context": ", Deerwester et al. [1990], Manning and Sch\u00fctze [1999]) to incorporate the new users or objects into the model on the fly.", "startOffset": 2, "endOffset": 27}, {"referenceID": 7, "context": ", Deerwester et al. [1990], Manning and Sch\u00fctze [1999]) to incorporate the new users or objects into the model on the fly.", "startOffset": 2, "endOffset": 55}, {"referenceID": 8, "context": "A standard approach for finding a low-rank estimate [Fazel, 2002] is to minimize the mean-squared error of the matrix difference, and regularize using the trace norm, which is a convex relaxation of the low-rank constraint:", "startOffset": 52, "endOffset": 65}, {"referenceID": 6, "context": "Our probabilistic model, and our low-rank assumption on \u03c0\u0304, are similar to those studied in Davenport et al. [2014], Hsieh et al.", "startOffset": 92, "endOffset": 116}, {"referenceID": 6, "context": "Our probabilistic model, and our low-rank assumption on \u03c0\u0304, are similar to those studied in Davenport et al. [2014], Hsieh et al. [2015]. However, in these works it is assumed that only one access matrix drawn from \u03c0\u0304 is available, while we have several matrices available at training time.", "startOffset": 92, "endOffset": 137}, {"referenceID": 6, "context": "Existing generalization bounds [Davenport et al., 2014, Shamir and Shalev-Shwartz, 2014, Hsieh et al., 2015] give guarantees for the mean-squared-loss of the solution to the optimization problem in Eq. (3), for the case where B is a single matrix drawn according to \u03c0\u0304. Our approach to finding the low-rank structure is similar to the above, however instead of considering a single binary matrix B, we use the average matrix B\u0304 = 1 T1 \u2211T1 t=1Bt. In Appendix A in the supplementary we prove a generalization bound for F (B\u0304, \u03bb), which converges to the true low-rank model as T1 increases. An efficient solution to the unconstrained version of Eq. (3), with \u03c0\u0302 \u2208 Rn\u00d7m, is given in Mazumder et al. [2010]: For a real-valued matrix A, let SVD(A) be the Singular Value Decomposition ofA.", "startOffset": 32, "endOffset": 702}, {"referenceID": 6, "context": "Existing generalization bounds [Davenport et al., 2014, Shamir and Shalev-Shwartz, 2014, Hsieh et al., 2015] give guarantees for the mean-squared-loss of the solution to the optimization problem in Eq. (3), for the case where B is a single matrix drawn according to \u03c0\u0304. Our approach to finding the low-rank structure is similar to the above, however instead of considering a single binary matrix B, we use the average matrix B\u0304 = 1 T1 \u2211T1 t=1Bt. In Appendix A in the supplementary we prove a generalization bound for F (B\u0304, \u03bb), which converges to the true low-rank model as T1 increases. An efficient solution to the unconstrained version of Eq. (3), with \u03c0\u0302 \u2208 Rn\u00d7m, is given in Mazumder et al. [2010]: For a real-valued matrix A, let SVD(A) be the Singular Value Decomposition ofA. Let r be the rank of the matrix B\u0304, and let (U,D, V ) = SVD(B\u0304). Mazumder et al. [2010] show that", "startOffset": 32, "endOffset": 871}, {"referenceID": 28, "context": "In practice, Shamir and Shalev-Shwartz [2014] observed that the solutions to Eq.", "startOffset": 13, "endOffset": 46}, {"referenceID": 34, "context": "In the context of collaborative filtering, this issue is commonly known as the cold start problem [Su and Khoshgoftaar, 2009].", "startOffset": 98, "endOffset": 125}, {"referenceID": 29, "context": "Previous solutions to the cold start problem in the context of collaborative filtering have suggested finding an existing user whose pattern of accesses most resembles that of the new user, and assigning the new user the same prediction as the existing user or a weighted score of the most similar users [Shardanand and Maes, 1995].", "startOffset": 304, "endOffset": 331}, {"referenceID": 23, "context": "Each new user or object is projected onto latent space, via a process commonly known as folding [Manning and Sch\u00fctze, 1999].", "startOffset": 96, "endOffset": 123}, {"referenceID": 10, "context": ", 2007] and MovieLens [Harper and Konstan, 2016].", "startOffset": 22, "endOffset": 48}], "year": 2017, "abstractText": "We propose a hybrid approach to temporal anomaly detection in user-database access data \u2014 or more generally, any kind of subject-object co-occurrence data. Our methodology allows identifying anomalies based on a single stationary model, instead of requiring a full temporal one, which would be prohibitive in our setting. We learn our low-rank stationary model from the high-dimensional training data, and then fit a regression model for predicting the expected likelihood score of normal access patterns in the future. The disparity between the predicted and the observed likelihood scores is used to assess the \u201csurprise\u201d. This approach enables calibration of the anomaly score so that time-varying normal behavior patterns are not considered anomalous. We provide a detailed description of the algorithm, including a convergence analysis, and report encouraging empirical results. One of the datasets we tested is new for the public domain. It consists of two months\u2019 worth of database access records from a live system. This dataset will be made publicly available, and is provided in the supplementary material.", "creator": "LaTeX with hyperref package"}}}