{"id": "1705.10479", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets", "abstract": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at https://www.youtube.com/watch?v=vSfqMtF0zK9", "histories": [["v1", "Tue, 30 May 2017 07:15:11 GMT  (2769kb,D)", "http://arxiv.org/abs/1705.10479v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["karol hausman", "yevgen chebotar", "stefan schaal", "gaurav sukhatme", "joseph lim"], "accepted": true, "id": "1705.10479"}, "pdf": {"name": "1705.10479.pdf", "metadata": {"source": "CRF", "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets", "authors": ["Karol Hausman", "Yevgen Chebotar", "Stefan Schaal", "Gaurav Sukhatme", "Joseph J. Lim"], "emails": ["limjj}@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "One of the key factors to enable deployment of robots in unstructured real-world environments is their ability to learn from data. In recent years, there have been multiple examples of robot learning frameworks that present promising results. These include: reinforcement learning [29] - where a robot learns a skill based on its interaction with the environment and imitation learning [2, 5] - where a robot is presented with a demonstration of a skill that it should imitate. In this work, we focus on the latter learning setup.\nTraditionally, imitation learning has focused on using isolated demonstrations of a particular skill [27]. The demonstration is usually provided in the form of kinesthetic teaching, which requires the user to spend sufficient time to provide the right training data. This constrained setup for imitation learning is difficult to scale to real world scenarios, where robots have to be able to execute a combination of different skills. To learn these skills, the robots would require a large number of robot-tailored demonstrations, since at least one isolated demonstration has to be provided for every individual skill.\nIn order to improve the scalability of imitation learning, we propose a framework that can learn to imitate skills from a set of unstructured and unlabeled demonstrations of various tasks.\nAs a motivating example, consider a highly unstructured data source, e.g. a video of a person cooking a meal. A complex activity, such as cooking, involves a set of simpler skills such as grasping, reaching, cutting, pouring, etc. In order to learn from such data, three components are required: i) the ability to map the image stream to state-action pairs that can be executed by a robot, ii) the ability to segment the data into simple skills, and iii) the ability to imitate each of the segmented skills. In this work, we tackle the latter two components, leaving the first one for future work. We believe that the capability proposed here of learning from unstructured, unlabeled demonstrations is an important step towards scalable robot learning systems.\n\u2217Equal contribution\nar X\niv :1\n70 5.\n10 47\n9v 1\n[ cs\n.R O\n] 3\n0 M\nay 2\nIn this paper, we present a novel imitation learning method that learns a multi-modal stochastic policy, which is able to imitate a number of automatically segmented tasks using a set of unstructured and unlabeled demonstrations. Our results indicate that the presented technique can separate the demonstrations into sensible individual skills and imitate these skills using a learned multi-modal policy. We show applications of the presented method to the tasks of skill segmentation, hierarchical reinforcement learning and multi-modal policy learning."}, {"heading": "2 Related Work", "text": "Imitation learning is concerned with learning skills from demonstrations. Approaches that are suitable for this setting can be split into two categories: i) behavioral cloning [25], and ii) inverse reinforcement learning (IRL) [22]. While behavioral cloning aims at replicating the demonstrations exactly, it suffers from the covariance shift [26]. IRL alleviates this problem by learning a reward function that explains the behavior shown in the demonstrations. The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.\nMore recently, there has been work that tackles a problem similar to the one presented in this paper, where the authors consider a setting where there is a large set of tasks with many instantiations [10]. In their work, the authors assume a way of communicating a new task through a single demonstration. We follow the idea of segmenting and learning different skills jointly so that learning of one skill can accelerate learning to imitate the next skill. In our case, however, the goal is to separate the mix of expert demonstrations into single skills and learn a policy that can imitate all of them, which eliminates the need of new demonstrations at test time.\nThe method presented here belongs to the field of multi-task inverse reinforcement learning. Examples from this field include [9] and [4]. In [9], the authors present a Bayesian approach to the problem, while the method in [4] is based on an EM approach that clusters observed demonstrations. Both of these methods show promising results on relatively low-dimensional problems, whereas our approach scales well to higher dimensional domains due to the representational power of neural networks.\nThere has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21]. In contrast, we do not assume access to the expert reward function, which is required to perform reinforcement learning in the later stages of the above algorithms.\nThere has been much work on the problem of skill segmentation and option discovery for hierarchical tasks. Examples include [23, 18, 14, 31, 13]. In this work, we consider a possibility to discover different skills that can all start from the same initial state, as opposed to hierarchical reinforcement learning where the goal is to segment a task into a set of consecutive subtasks. We demonstrate, however, that our method may be used to discover the hierarchical structure of a task similarly to the hierarchical reinforcement learning approaches.\nGenerative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20]. More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16]. In this work, we expand on some of the ideas presented in these works and provide a novel framework that exploits this connection.\nThe works that are most closely related to this paper are [16] and [7]. In [7], the authors show a method that is able to learn disentangled representations and apply it to the problem of image generation. In this work, we provide an alternative derivation of our method that extends their work and applies it to multi-modal policies. In [16], the authors present an imitation learning GAN approach that serves as a basis for the development of our method. We provide an extensive evaluation of the hereby presented approach compared to the work in [16], which shows that our method, as opposed to [16], can handle unstructured demonstrations of different skills."}, {"heading": "3 Preliminaries", "text": "LetM = (S,A, P,R, p0, \u03b3, T ) be a finite-horizon Markov Decision Process (MDP), where S and A are state and action spaces, P : S \u00d7A\u00d7 S \u2192 R+ is a state-transition probability function or system\ndynamics, R : S \u00d7A\u2192 R a reward function, p0 : S \u2192 R+ an initial state distribution, \u03b3 a reward discount factor, and T a horizon. Let \u03c4 = (s0, a0, . . . , sT , aT ) be a trajectory of states and actions and R(\u03c4) = \u2211T t=0 \u03b3\ntR(st, at) the trajectory reward. The goal of reinforcement learning methods is to find parameters \u03b8 of a policy \u03c0\u03b8(a|s) that maximizes the expected discounted reward over trajectories induced by the policy: E\u03c0\u03b8 [R(\u03c4)] where s0 \u223c p0, st+1 \u223c P (st+1|st, at) and at \u223c \u03c0\u03b8(at|st). In an imitation learning scenario, the reward function is unknown. However, we are given a set of demonstrated trajectories, which presumably originate from some optimal expert policy distribution \u03c0E1 that optimizes an unknown reward function RE1 . Thus, by trying to estimate the reward function RE1 and optimizing the policy \u03c0\u03b8 with respect to it, we can recover the expert policy. This approach is known as inverse reinforcement learning (IRL) [1]. In order to model a variety of behaviors, it is beneficial to find a policy with the highest possible entropy that optimizes RE1 . We will refer to this approach as the maximum-entropy IRL [33] with the optimization objective\nmax R ( max \u03c0\u03b8 H(\u03c0\u03b8) + E\u03c0\u03b8R(s, a) ) \u2212 E\u03c0E1R(s, a), (1)\nwhere H(\u03c0\u03b8) is the entropy of the policy \u03c0\u03b8.\nHo and Ermon [16] showed that it is possible to redefine the maximum-entropy IRL problem with multiple demonstrations sampled from a single expert policy \u03c0E1 as an optimization of GANs [15]. In this framework, the policy \u03c0\u03b8(a|s) plays the role of a generator, whose goal is to make it difficult for a discriminator network Dw(s, a) (parameterized by w) to differentiate between imitated samples from \u03c0\u03b8 (labeled 0) and demonstrated samples from \u03c0E1 (labeled 1). Accordingly, the joint optimization goal can be defined as\nmax \u03b8 min w E(s,a)\u223c\u03c0\u03b8 [log(Dw(s, a))] + E(s,a)\u223c\u03c0E1 [log(1\u2212Dw(s, a))] + \u03bbHH(\u03c0\u03b8). (2)\nThe discriminator and the generator policy are both represented as neural networks and optimized by repeatedly performing alternating gradient updates. The discriminator is trained on the mixed set of expert and generator samples and outputs probabilities that a particular sample has originated from the generator or the expert policies. This serves as a reward signal for the generator policy that tries to maximize the probability of the discriminator confusing it with an expert policy. The generator can be trained using the trust region policy optimization (TRPO) algorithm [28] with the cost function log(Dw(s, a)). At each iteration, TRPO takes the following gradient step:\nE(s,a)\u223c\u03c0\u03b8 [\u2207\u03b8 log \u03c0\u03b8(a|s) log(Dw(s, a))] + \u03bbH\u2207\u03b8H(\u03c0\u03b8), (3)\nwhich corresponds to minimizing the objective in Eq. (2) with respect to the policy \u03c0\u03b8."}, {"heading": "4 Multi-modal Imitation Learning", "text": "The traditional imitation learning scenario described in Sec. 3 considers a problem of learning to imitate one skill from demonstrations. The demonstrations represent samples from a single expert policy \u03c0E1. In this work, we focus on an imitation learning setup where we learn from unstructured and unlabelled demonstrations of various tasks. In this case, the demonstrations come from a set of expert policies \u03c0E1 , \u03c0E2 , . . . , \u03c0Ek , where k can be unknown, that optimize different reward functions/tasks. We will refer to this set of unstructured expert policies as a mixture of policies \u03c0E . We aim to segment the demonstrations of these policies into separate tasks and learn a multi-modal policy that will be able to imitate all of the segmented tasks.\nIn order to be able to learn multi-modal policy distributions, we augment the policy input with a latent intention i distributed by a categorical or uniform distribution p(i), similar to [7]. The goal of the intention variable is to select a specific mode of the policy, which corresponds to one of the skills presented in the demonstrations. The resulting policy can be expressed as:\n\u03c0i(a|s, i) = p(i|s, a)\u03c0 i(a|s) p(i) . (4)\nWe augment the trajectory to include the latent intention as \u03c4i = (s0, a0, i0, ...sT , aT , iT ). The resulting reward of the trajectory with the latent intention is R(\u03c4i) = \u2211T t=0 \u03b3 tR(st, at, it). R(a, s, i)\nis a reward function that depends on the latent intention i as we have multiple demonstrations that optimize different reward functions for different tasks. The expected discounted reward is equal to: E\u03c0i\u03b8 [R(\u03c4i)] = \u222b R(\u03c4i)\u03c0 i \u03b8(\u03c4i)d\u03c4i where \u03c0\u03b8(\u03c4i) = p0(s0) \u220fT\u22121 t=0 P (st+1|st, at)\u03c0i\u03b8(at|st, it)p(it).\nHere, we show an extension of the derivation presented in [16] (Eqs. (1, 2)) for a policy \u03c0i(a|s, i) augmented with the latent intention variable i, which uses demonstrations from a set of expert policies \u03c0E , rather than a single expert policy \u03c0E1 . We are aiming at maximum entropy policies that can be determined from the latent intention variable i. Accordingly, we transform the original IRL problem to reflect this goal:\nmax R ( max \u03c0i H(\u03c0i(a|s))\u2212H(\u03c0i(a|s, i)) + E\u03c0iR(s, a, i) ) \u2212 E\u03c0ER(s, a, i), (5)\nwhere \u03c0i(a|s) = \u2211 i \u03c0i(a|s, i)p(i), which results in the policy averaged over intentions (since the p(i) is constant). This goal reflects our objective: we aim to obtain a multi-modal policy that has a high entropy without any given intention, but it collapses to a particular task when the intention is specified. Analogously to the solution for a single expert policy, this optimization objective results in the optimization goal of the generative adversarial imitation learning network, with the exception that the state-action pairs (s, a) are sampled from a set of expert policies \u03c0E :\nmax \u03b8 min w Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 [log(Dw(s, a))] + E(s,a)\u223c\u03c0E [1\u2212 log(Dw(s, a))] (6)\n+\u03bbHH(\u03c0 i \u03b8(a|s))\u2212 \u03bbIH(\u03c0i\u03b8(a|s, i)),\nwhere \u03bbI , \u03bbH correspond to the weighting parameters on the respective objectives. The resulting entropy H(\u03c0i\u03b8(a|s, i)) term can be expressed as:\nH(\u03c0i\u03b8(a|s, i)) = Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 (\u2212 log(\u03c0 i \u03b8(a|s, i)) (7) = \u2212Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log ( p(i|s, a)\u03c0 i \u03b8(a|s) p(i) ) = \u2212Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log(p(i|s, a))\u2212 Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log \u03c0 i \u03b8(a|s) + Ei\u223cp(i) log p(i)\n= \u2212Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log(p(i|s, a)) +H(\u03c0 i \u03b8(a|s))\u2212H(i),\nwhich results in the final objective:\nmax \u03b8 min w Ei\u223cp(i),(s,a)\u223c\u03c0i\u03b8 [log(Dw(s, a))] + E(s,a)\u223c\u03c0E [1\u2212 log(Dw(s, a))] (8)\n+(\u03bbH \u2212 \u03bbI)H(\u03c0i\u03b8(a|s)) + \u03bbIEi\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log(p(i|s, a)) + \u03bbIH(i),\nwhere H(i) is a constant that does not influence the optimization. This results in the same optimization objective as for the single expert policy (see Eq. (2)) with an additional term \u03bbIEi\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log(p(i|s, a)) responsible for rewarding state-action pairs that make the latent intention inference easier. We refer to this cost as the latent intention cost and represent p(i|s, a) with a neural network. The final reward function for the generator is:\nEi\u223cp(i),(s,a)\u223c\u03c0i\u03b8 [log(Dw(s, a))] + \u03bbIEi\u223cp(i),(s,a)\u223c\u03c0i\u03b8 log(p(i|s, a)) + \u03bbH\u2032H(\u03c0 i \u03b8(a|s)). (9)"}, {"heading": "4.1 Relation to InfoGAN", "text": "In this section, we provide an alternative derivation of the optimization goal in Eq. (8) by extending the InfoGAN approach presented in [7]. Following [7], we introduce the latent variable c as a means to capture the semantic features of the data distribution. In this case, however, the latent variables are used in the imitation learning scenario, rather than the traditional GAN setup, which prevents us from using additional noise variables (z in the InfoGAN approach) that are used as noise samples to generate the data from.\nSimilarly to [7], to prevent collapsing to a single mode, the policy optimization objective is augmented with mutual information I(c;G(\u03c0c\u03b8, c)) between the latent variable and the state-action pairs generator G dependent on the policy distribution \u03c0c\u03b8. This encourages the policy to produce behaviors that are\ninterpretable from the latent code, and given a larger number of possible latent code values leads to an increase in the diversity of policy behaviors. The corresponding generator goal can be expressed as:\nEc\u223cp(c),(s,a)\u223c\u03c0c\u03b8 [log(Dw(s, a))] + \u03bbII(c;G(\u03c0 c \u03b8, c)) + \u03bbHH(\u03c0 c \u03b8) (10)\nIn order to compute I(c;G(\u03c0c\u03b8, c)), we follow the derivation from [7] that introduces a lower bound:\nI(c;G(\u03c0c\u03b8, c)) = H(c)\u2212H(c|G(\u03c0c\u03b8, c)) (11) = E(s,a)\u223cG(\u03c0c\u03b8,c)[Ec\u2032\u223cP (c|s,a)[logP (c\n\u2032|s, a)]] +H(c) = E(s,a)\u223cG(\u03c0c\u03b8,c)[DKL(P (\u00b7|s, a)||Q(\u00b7|s, a)) + Ec\u2032\u223cP (c|s,a)[logQ(c\n\u2032|s, a)]] +H(c) \u2265 E(s,a)\u223cG(\u03c0c\u03b8,c)[Ec\u2032\u223cP (c|s,a)[logQ(c\n\u2032|s, a)]] +H(c) = Ec\u223cP (c),(s,a)\u223cG(\u03c0c\u03b8,c)[logQ(c|s, a)] +H(c)\nBy maximizing this lower bound we maximize I(c;G(\u03c0c\u03b8, c)). The auxiliary distribution Q(c|s, a) can be parametrized by a neural network.\nThe resulting optimization goal is\nmax \u03b8 min w Ec\u223cp(c),(s,a)\u223c\u03c0c\u03b8 [log(Dw(s, a))] + E(s,a)\u223c\u03c0E [1\u2212 log(Dw(s, a))] (12)\n+ \u03bbIEc\u223cP (c),(s,a)\u223cG(\u03c0c\u03b8,c)[logQ(c|s, a)] + \u03bbHH(\u03c0 c \u03b8)\nwhich results in the generator reward function:\nEc\u223cp(c),(s,a)\u223c\u03c0c\u03b8 [log(Dw(s, a))] + \u03bbIEc\u223cP (c),(s,a)\u223cG(\u03c0c\u03b8,c)[logQ(c|s, a)] + \u03bbHH(\u03c0 c \u03b8). (13)\nThis corresponds to the same objective that was derived in Section 4. The auxiliary distribution over the latent variables Q(c|s, a) is analogous to the intention distribution p(i|s, a)."}, {"heading": "5 Implementation", "text": "In this section, we discuss implementation details that can alleviate instability of the training procedure of our model. The first indicator that the training has become unstable is a high classification accuracy of the discriminator. In this case, it is difficult for the generator to produce a meaningful policy as the reward signal from the discriminator is flat and the TRPO gradient of the generator vanishes. In an extreme case, the discriminator assigns all the generator samples to the same class and it is impossible for TRPO to provide a useful gradient as all generator samples receive the same reward. Previous work suggests several ways to avoid this behavior. These include leveraging the Wasserstein distance metric to improve the convergence behavior [3] and adding instance noise to the inputs of the discriminator to avoid degenerate generative distributions [30]. We find that adding the Gaussian noise helped us the most to control the performance of the discriminator and to produce a smooth reward signal for the generator policy. During our experiments, we anneal the noise similar to [30], as the generator policy improves towards the end of the training.\nAn important indicator that the generator policy distribution has collapsed to a uni-modal policy is a high or increasing loss of the intention-prediction network p(i|s, a). This means that the prediction of the latent variable i is difficult and consequently, the policy behavior can not be categorized into separate skills. Hence, the policy executes the same skill for different values of the latent variable. To prevent this, one can increase the weight of the latent intention cost \u03bbI in the generator loss or add more instance noise to the discriminator, which makes its reward signal relatively weaker.\nIn this work, we employ both categorical and continuous latent variables to represent the latent intention. The advantage of using a continuous variable is that we do not have to specify the number of possible values in advance as with the categorical variable and it leaves more room for interpolation between different skills. We use a softmax layer to represent categorical latent variables, and use a uniform distribution for continuous latent variables as proposed in [7]."}, {"heading": "6 Experiments", "text": "Our experiments aim to answer the following questions: (1) Can we segment unstructured and unlabelled demonstrations into skills and learn a multi-modal policy that imitates them? (2) What is the influence of the introduced intention-prediction cost on the resulting policies? (3) Can we autonomously discover the number of skills presented in the demonstrations, and even accomplish\nthem in different ways? (4) Does the presented method scale to high-dimensional policies? (5) Can we use the proposed method for learning hierarchical policies? We evaluate our method on a series of challenging simulated robotics tasks described below. The performance of our method can be seen in our supplementary video2."}, {"heading": "6.1 Task setup", "text": "Reacher The Reacher environment is depicted in Fig. 2 (left). The actuator is a 2-DoF arm attached at the center of the scene. There are several targets placed at random positions throughout the environment. The goal of the task is, given a data set of reaching motions to random targets, to discover the dependency of the target selection on the intention and learn a policy that is capable of reaching different targets based on the specified intention input. We evaluate the performance of our framework on environments with 1, 2 and 4 targets.\nWalker-2D The Walker-2D (Fig. 1 left) is a 6-DoF bipedal robot consisting of two legs and feet attached to a common base. The goal of this task is to learn a policy that can switch between three different behaviors dependent on the discovered intentions: running forward, running backward and jumping. We use TRPO to train single expert policies and create a combined data set of all three behaviors that is used to train a multi-modal policy using our imitation framework.\nHumanoid Humanoid (Fig. 1 right) is a high-dimensional robot with 17 degrees of freedom. Similar to Walker-2D the goal of the task is to be able to discover three different policies: running forward, running backward and balancing, from the combined expert demonstrations of all of them.\nGripper-pusher This task involves controlling a 4-DoF arm with an actuated gripper to push a sliding block to a specified goal area (Fig. 2 right). We provide separate expert demonstrations of grasping the object, and pushing it towards the goal starting from the object already being inside the hand. The initial positions of the arm, block and the goal area are randomly sampled at the beginning of each episode. The goal of our framework is to discover both intentions and the hierarchical structure of the task from a combined set of demonstrations."}, {"heading": "6.2 Multi-Target Imitation Learning", "text": "Our goal here is to analyze the ability of our method to segment and imitate policies that perform the same task for different targets. To this end, we first evaluate the influence of the latent intention cost on the Reacher task with 2 and 4 targets. For both experiments, we use either a categorical intention distribution with the number of categories equal to the number of targets or a continuous, uniformly-distributed intention variable, which means that the network has to discover the number of intentions autonomously. Fig. 3 top shows the results of the reaching tasks using the latent intention cost for 2 and 4 targets with different latent intention distributions. For the continuous latent variable, we show a span of different intentions between -1 and 1 in the 0.2 intervals. The colors indicate the intention \u201cvalue\u201d. In the categorical distribution case, we are able to learn a multi-modal policy that can reach all the targets dependent on the given latent intention (Fig. 3-1 and Fig. 3-3 top). The continuous latent intention is able to discover two modes in case of two targets (Fig. 3-2 top) but\n2http://sites.google.com/view/nips17intentiongan\nit collapses to only two modes in the four targets case (Fig. 3-4 top) as this is a significantly more difficult task.\nAs a baseline, we present the results of the Reacher task achieved by the standard GAN imitation learning presented in [16] without the latent intention cost. The obtained results are presented in Fig. 3 bottom. Since the network is not encouraged to discover different skills through the intention learning cost, it collapses to a single target for 2 targets in both the continuous and discrete latent intention variables. In the case of 4 targets, the network collapses to 2 modes, which can be explained by the fact that even without the latent intention cost the imitation network tries to imitate most of the presented demonstrations. Since the demonstration set is very diverse in this case, the network learned two modes without the explicit instruction (latent intention cost) to do so.\nTo demonstrate the development of different intentions, in Fig. 4 (left) we present the Reacher rewards over training iterations for different intention variables. When the latent intention cost is included, (Fig. 4-1), the separation of different skills for different intentions starts to emerge around the 1000-th iteration and leads to a multi-modal policy that, given the intention value, consistently reaches the target associated with that intention. In the case of the standard imitation learning GAN setup (Fig. 4-2), the network learns how to imitate reaching only one of the targets for both intention values.\nIn order to analyze the ability to discover different ways to accomplish the same task, we use our framework with the categorical latent intention in the Reacher environment with a single target. Since we only have a single set of expert trajectories that reach the goal in one, consistent manner, we subsample the expert state-action pairs to ease the intention learning process for the generator. Fig. 4 (right) shows two examples of a heatmap of the visited end-effector states accumulated for two different values of the intention variable. For both cases, the task is executed correctly, the robot reaches the target, but it achieves it using different trajectories. These trajectories naturally emerged through the latent intention cost as it encourages different behaviors for different latent intentions. It is worth noting that the presented behavior can be also replicated for multiple targets if the number of categories in the categorical distribution of the latent intention exceeds the number of targets."}, {"heading": "6.3 Multi-Task Imitation Learning", "text": "We also seek to further understand whether our model extends to segmenting and imitating policies that perform different tasks. In particular, we evaluate whether our framework is able to learn a multi-modal policy on the Walker-2D task. We mix three different policies \u2013 running backwards, running forwards, and jumping \u2013 into one expert policy \u03c0E and try to recover all of them through our method. The results are depicted in Fig. 5 (left). The additional latent intention cost results in a policy that is able to autonomously segment and mimic all three behaviors and achieve a similar performance to the expert policies (Fig. 5-1). Different intention variable values correspond to different expert policies: 0 - running forwards, 1 - jumping, and 2 - running backwards. The imitation learning GAN method is shown as a baseline in Fig. 5-2. The results show that the policy collapses to a single mode, where all different intention variable values correspond to the jumping behavior, ignoring the demonstrations of the other two skills.\nTo test if our multi-modal imitation learning framework scales to high-dimensional tasks, we evaluate it in the Humanoid environment. The expert policy is constructed using three expert policies: running backwards, running forwards, and balancing while standing upright. Fig. 5 (right) shows the rewards obtained for different values of the intention variable. Similarly to Walker-2D, the latent intention cost enables the neural network to segment the tasks and learn a multi-modal imitation policy. In this case, however, due to the high dimensionality of the task, the resulting policy is able to mimic running forwards and balancing policies almost as well as the experts, but it achieves a suboptimal performance on the running backwards task (Fig. 5-3). The imitation learning GAN baseline collapses to a uni-modal policy that maps all the intention values to a balancing behavior (Fig. 5-4).\nFinally, we evaluate the ability of our method to discover options in hierarchical IRL tasks. In order to test this, we collect expert policies in the Gripper-pusher environment that consist of grasping and pushing when the object is grasped demonstrations. The goal of this task is to check whether our method will be able to segment the mix of expert policies into separate grasping and pushing-whengrasped skills. Since the two sub-tasks start from different initial conditions, we cannot present the results in the same form as for the previous tasks. Instead, we present a time-lapse of the learned multi-modal policy (see Fig. 6) that presents the ability to change in the intention during the execution. The categorical intention variable is manually changed after the block is grasped. The intention change results in switching to a pushing policy that brings the block into the goal region. We present this setup as an example of extracting different options from the expert policies that can be further used in an hierarchical reinforcement learning task to learn the best switching strategy."}, {"heading": "7 Conclusions", "text": "We present a novel imitation learning method that learns a multi-modal stochastic policy, which is able to imitate a number of automatically segmented tasks using a set of unstructured and unlabeled demonstrations. The presented approach learns the notion of intention and is able to perform different tasks based on the policy intention input. We evaluated our method on a set of simulation scenarios\nwhere we show that it is able to segment the demonstrations into different tasks and to learn a multi-modal policy that imitates all of the segmented skills. We also compared our method to a baseline approach that performs imitation learning without explicitly separating the tasks.\nIn the future work, we plan to focus on autonomous discovery of the number of tasks in the given pool of demonstrations as well as evaluating this method on real robots."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proc. ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and autonomous systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Apprenticeship learning about multiple intentions", "author": ["Monica Babes", "Vukosi Marivate", "Kaushik Subramanian", "Michael L Littman"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Robot programming by demonstration", "author": ["Aude Billard", "Sylvain Calinon", "Ruediger Dillmann", "Stefan Schaal"], "venue": "In Springer handbook of robotics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Path integral guided policy search", "author": ["Yevgen Chebotar", "Mrinal Kalakrishnan", "Ali Yahya", "Adrian Li", "Stefan Schaal", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.00529,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets, 2016", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Bayesian multitask inverse reinforcement learning", "author": ["Christos Dimitrakakis", "Constantin A Rothkopf"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "One-shot imitation learning", "author": ["Yan Duan", "Marcin Andrychowicz", "Bradly Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1703.07326,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "author": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.03852,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["Carlos Florensa", "Yan Duan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1704.03012,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Multi-level discovery of deep options", "author": ["Roy Fox", "Sanjay Krishnan", "Ion Stoica", "Ken Goldberg"], "venue": "arXiv preprint arXiv:1703.08294,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Generative adversarial imitation learning", "author": ["Jonathan Ho", "Stefano Ermon"], "venue": "CoRR, abs/1606.03476,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning force control policies for compliant manipulation", "author": ["Mrinal Kalakrishnan", "Ludovic Righetti", "Peter Pastor", "Stefan Schaal"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Towards learning hierarchical skills for multi-phase manipulation tasks", "author": ["Oliver Kroemer", "Christian Daniel", "Gerhard Neumann", "Herke Van Hoof", "Jan Peters"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Sergey Levine", "Zoran Popovic", "Vladlen Koltun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["Katharina M\u00fclling", "Jens Kober", "Oliver Kroemer", "Jan Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "In Icml,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Incremental semantically grounded learning from demonstration", "author": ["Scott Niekum", "Sachin Chitta", "Andrew G Barto", "Bhaskara Marthi", "Sarah Osentoski"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Connecting generative adversarial networks and actor-critic methods", "author": ["David Pfau", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1610.01945,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Dean A Pomerleau"], "venue": "Neural Computation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Efficient reductions for imitation learning", "author": ["St\u00e9phane Ross", "Drew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Is imitation learning the route to humanoid robots", "author": ["Stefan Schaal"], "venue": "Trends in cognitive sciences,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz"], "venue": "ICML, volume 37 of JMLR Workshop and Conference Proceedings,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Amortised map inference for image super-resolution", "author": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Feudal networks for hierarchical reinforcement learning", "author": ["Alexander Sasha Vezhnevets", "Simon Osindero", "Tom Schaul", "Nicolas Heess", "Max Jaderberg", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1703.01161,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2017}, {"title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "author": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1703.10593,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D. Ziebart", "Andrew L. Maas", "J. Andrew Bagnell", "Anind K. Dey"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "These include: reinforcement learning [29] - where a robot learns a skill based on its interaction with the environment and imitation learning [2, 5] - where a robot is presented with a demonstration of a skill that it should imitate.", "startOffset": 143, "endOffset": 149}, {"referenceID": 3, "context": "These include: reinforcement learning [29] - where a robot learns a skill based on its interaction with the environment and imitation learning [2, 5] - where a robot is presented with a demonstration of a skill that it should imitate.", "startOffset": 143, "endOffset": 149}, {"referenceID": 25, "context": "Traditionally, imitation learning has focused on using isolated demonstrations of a particular skill [27].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Approaches that are suitable for this setting can be split into two categories: i) behavioral cloning [25], and ii) inverse reinforcement learning (IRL) [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Approaches that are suitable for this setting can be split into two categories: i) behavioral cloning [25], and ii) inverse reinforcement learning (IRL) [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 24, "context": "While behavioral cloning aims at replicating the demonstrations exactly, it suffers from the covariance shift [26].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 30, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 0, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 17, "context": "The majority of IRL works [16, 33, 1, 12, 19] introduce algorithms that can imitate a single skill from demonstrations thereof but they do not readily generalize to learning a multi-task policy from a set of unstructured demonstrations of various tasks.", "startOffset": 26, "endOffset": 45}, {"referenceID": 8, "context": "More recently, there has been work that tackles a problem similar to the one presented in this paper, where the authors consider a setting where there is a large set of tasks with many instantiations [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 7, "context": "Examples from this field include [9] and [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Examples from this field include [9] and [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "In [9], the authors present a Bayesian approach to the problem, while the method in [4] is based on an EM approach that clusters observed demonstrations.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [9], the authors present a Bayesian approach to the problem, while the method in [4] is based on an EM approach that clusters observed demonstrations.", "startOffset": 84, "endOffset": 87}, {"referenceID": 15, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 4, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 19, "context": "There has also been a separate line of work on learning from demonstration, which is then iteratively improved through reinforcement learning [17, 6, 21].", "startOffset": 142, "endOffset": 153}, {"referenceID": 21, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 28, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "Examples include [23, 18, 14, 31, 13].", "startOffset": 17, "endOffset": 37}, {"referenceID": 13, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 6, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "Generative Adversarial Networks (GANs) [15] have enjoyed success in various domains including image generation [8], image-image translation [32] and video prediction [20].", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 9, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "More recently, there have been works connecting GANs and other reinforcement learning and IRL methods [24, 11, 16].", "startOffset": 102, "endOffset": 114}, {"referenceID": 14, "context": "The works that are most closely related to this paper are [16] and [7].", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "The works that are most closely related to this paper are [16] and [7].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In [7], the authors show a method that is able to learn disentangled representations and apply it to the problem of image generation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [16], the authors present an imitation learning GAN approach that serves as a basis for the development of our method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "We provide an extensive evaluation of the hereby presented approach compared to the work in [16], which shows that our method, as opposed to [16], can handle unstructured demonstrations of different skills.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "We provide an extensive evaluation of the hereby presented approach compared to the work in [16], which shows that our method, as opposed to [16], can handle unstructured demonstrations of different skills.", "startOffset": 141, "endOffset": 145}, {"referenceID": 0, "context": "This approach is known as inverse reinforcement learning (IRL) [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 30, "context": "We will refer to this approach as the maximum-entropy IRL [33] with the optimization objective", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "Ho and Ermon [16] showed that it is possible to redefine the maximum-entropy IRL problem with multiple demonstrations sampled from a single expert policy \u03c0E1 as an optimization of GANs [15].", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Ho and Ermon [16] showed that it is possible to redefine the maximum-entropy IRL problem with multiple demonstrations sampled from a single expert policy \u03c0E1 as an optimization of GANs [15].", "startOffset": 185, "endOffset": 189}, {"referenceID": 26, "context": "The generator can be trained using the trust region policy optimization (TRPO) algorithm [28] with the cost function log(Dw(s, a)).", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "In order to be able to learn multi-modal policy distributions, we augment the policy input with a latent intention i distributed by a categorical or uniform distribution p(i), similar to [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 14, "context": "Here, we show an extension of the derivation presented in [16] (Eqs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 5, "context": "(8) by extending the InfoGAN approach presented in [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Following [7], we introduce the latent variable c as a means to capture the semantic features of the data distribution.", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Similarly to [7], to prevent collapsing to a single mode, the policy optimization objective is augmented with mutual information I(c;G(\u03c0 \u03b8, c)) between the latent variable and the state-action pairs generator G dependent on the policy distribution \u03c0 \u03b8.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "In order to compute I(c;G(\u03c0 \u03b8, c)), we follow the derivation from [7] that introduces a lower bound: I(c;G(\u03c0 \u03b8, c)) = H(c)\u2212H(c|G(\u03c0 \u03b8, c)) (11)", "startOffset": 66, "endOffset": 69}, {"referenceID": 27, "context": "These include leveraging the Wasserstein distance metric to improve the convergence behavior [3] and adding instance noise to the inputs of the discriminator to avoid degenerate generative distributions [30].", "startOffset": 203, "endOffset": 207}, {"referenceID": 27, "context": "During our experiments, we anneal the noise similar to [30], as the generator policy improves towards the end of the training.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "We use a softmax layer to represent categorical latent variables, and use a uniform distribution for continuous latent variables as proposed in [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 14, "context": "As a baseline, we present the results of the Reacher task achieved by the standard GAN imitation learning presented in [16] without the latent intention cost.", "startOffset": 119, "endOffset": 123}], "year": 2017, "abstractText": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at http://sites.google.com/view/nips17intentiongan.", "creator": "LaTeX with hyperref package"}}}