{"id": "1705.10498", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Zonotope Hit-and-run for Efficient Sampling from Projection DPPs", "abstract": "Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers and a better estimation of the number of units involved in such sampling. In the absence of a DPP, the average time for sampling was often limited to an appropriate number of units, which in turn increased the number of units that are needed for sampling. The complexity of DPP algorithms in this analysis is compounded by the significant time spent with individual units. One factor that has been particularly important is the complexity of the measurement, which takes into account the fact that it is often not possible to distinguish between an average sample of a given sample and a standard sample of a given sample. The difficulty in identifying the average sample of a given sample is one of the primary reasons for this uncertainty.\n\n\n\n\nA key consideration for estimating DPP algorithms is the complexity of the task. A DPP algorithm is defined as a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task is a continuous task that requires an accurate representation of the set of unit and data. The task", "histories": [["v1", "Tue, 30 May 2017 08:07:19 GMT  (3376kb,D)", "https://arxiv.org/abs/1705.10498v1", "19 pages, accepted to ICML 2017"], ["v2", "Thu, 15 Jun 2017 08:36:00 GMT  (3783kb,D)", "http://arxiv.org/abs/1705.10498v2", "12 pages, 12 figures, 2 columns, accepted to ICML 2017"]], "COMMENTS": "19 pages, accepted to ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO", "authors": ["guillaume gautier", "r\u00e9mi bardenet", "michal valko"], "accepted": true, "id": "1705.10498"}, "pdf": {"name": "1705.10498.pdf", "metadata": {"source": "META", "title": "Zonotope Hit-and-run for Efficient Sampling from Projection DPPs", "authors": ["Guillaume Gautier", "R\u00e9mi Bardenet", "Michal Valko"], "emails": ["<g.gautier@inria.fr>."], "sections": [{"heading": "1. Introduction", "text": "Determinantal point processes (DPPs) are distributions over configurations of points that encode diversity through a kernel function. DPPs were introduced by Macchi (1975) and have then found applications in fields as diverse as probability (Hough et al., 2006), number theory (Rudnick & Sarnak, 1996), statistical physics (Pathria & Beale, 2011), Monte Carlo methods (Bardenet & Hardy, 2016), and spatial statistics (Lavancier et al., 2015). In machine learning, DPPs over finite sets have been used as a model of diverse sets of items, where the kernel function takes the\n1Univ. Lille, CNRS, Centrale Lille, UMR 9189 \u2014 CRIStAL 2INRIA Lille \u2014 Nord Europe, SequeL team. Correspondence to: Guillaume Gautier <g.gautier@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nform of a finite matrix, see Kulesza & Taskar (2012) for a comprehensive survey. Applications of DPPs in machine learning (ML) since this survey also include recommendation tasks (Kathuria et al., 2016; Gartrell et al., 2017), text summarization (Dupuy & Bach, 2016), or models for neural signals (Snoek et al., 2013).\nSampling generic DPPs over finite sets is expensive. Roughly speaking, it is cubic in the number r of items in a DPP sample. Moreover, generic DPPs are sometimes specified through an n \u00d7 n kernel matrix that needs diagonalizing before sampling, where n is the number of items to pick from. In text summarization, r would be the desired number of sentences for a summary, and n the number of sentences of the corpus to summarize. Thus, sampling quickly becomes intractable for large-scale applications (Kulesza & Taskar, 2012). This has motivated research on fast sampling algorithms. While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al., 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b). In particular, there are polynomial bounds on the mixing rates of natural MCMC chains with arbitrary DPPs as their limiting measure; see Anari et al. (2016) for cardinality-constrained DPPs, and Li et al. (2016b) for the general case.\nIn this paper, we contribute a non-obvious MCMC chain to approximately sample from projection DPPs, which are DPPs with a fixed sample cardinality. Leveraging a combinatorial geometry result by Dyer & Frieze (1994), we show that sampling from a projection DPP over a finite set can be relaxed into an easier continuous sampling problem with a lot of structure. In particular, the target of this continuous sampling problem is supported on the volume spanned by the columns of the feature matrix associated to the projection DPP, a convex body also called a zonotope. This zonotope can be partitioned into tiles that uniquely correspond to DPP realizations, and the relaxed target distribution is flat on each tile. Previous MCMC approaches to sampling projections DPPs can be viewed as attempting ar X\niv :1\n70 5.\n10 49\n8v 2\n[ st\nat .M\nL ]\n1 5\nJu n\n20 17\nmoves between neighboring tiles. Using linear programming, we propose an MCMC chain that moves more freely across this tiling. Our chain is a natural transformation of a fast mixing hit-and-run Markov chain (Lova\u0301sz & Vempala, 2003) on the underlying zonotope; this empirically results in more uncorrelated MCMC samples than previous work. While the results of Anari et al. (2016) and their generalization by Li et al. (2016b) apply to projection DPPs, our experiments support the fact that our chain mixes faster.\nThe rest of the paper is organized as follows. In Section 2, we introduce projection DPPs and review existing approaches to sampling. In Section 3, we introduce zonotopes and we tailor the hit-and-run algorithm to our needs. In Section 4, we empirically investigate the performance of our MCMC kernel on synthetic graphs and on a summary extraction task, before concluding in Section 5."}, {"heading": "2. Sampling Projections DPPs", "text": "In this section, we introduce projection DPPs in two equivalent ways, respectively following Hough et al. (2006), Kulesza & Taskar (2012), and Lyons (2003). Both definitions shed a different light on the algorithms in Section 3."}, {"heading": "2.1. Projection DPPs as Particular DPPs", "text": "Let E = [n] , {1, . . . n}. Let also K be a real symmetric positive semidefinite n \u00d7 n matrix, and for I \u2282 E, write KI for the square submatrix of K obtained by keeping only rows and columns indexed by I \u2282 E. The random subset X \u2282 E is said to follow a DPP on E = {1, . . . , n} with kernel K if\nP [I \u2282 X] = detKI , \u2200I \u2282 E. (1)\nExistence of the DPP described by (1) is guaranteed provided K has all its eigenvalues in [0, 1], see e.g., Kulesza & Taskar (2012, Theorem 2.3). Note that (1) encodes the repulsiveness of DPPs. In particular, for any distinct i, j \u2208 [n],\nP [{i, j} \u2282 X] = \u2223\u2223\u2223\u2223Kii KijKji Kjj \u2223\u2223\u2223\u2223 = P [{i} \u2208 X]P [{j} \u2208 X]\u2212K2ij \u2264 P [{i} \u2208 X]P [{j} \u2208 X] .\nIn other words, Kij encodes departure from independence. Similarly, for constant Kii,Kjj , the larger K2ij , the less likely it is to have items i and j co-occur in a sample.\nProjection DPPs are the DPPs such that the eigenvalues of K are either 0 or 1, that is, K is the matrix of an orthogonal projection. Projection DPPs are also sometimes called elementary DPPs (Kulesza & Taskar, 2012). One can show\nthat samples from a projection DPP with kernel matrix K almost surely contain r = Tr(K) points and that general DPPs are mixtures of projection DPPs, see e.g., Kulesza & Taskar (2012, Theorem 2.3)."}, {"heading": "2.2. Building Projection DPPs from Linear Matroids", "text": "Let r < n, and let A be a full-rank r \u00d7 n real matrix with columns (aj)j\u2208[n]. The linear matroid M [A] is defined as the pair (E,B), with E = [n] and\nB = { B\u2282 [n] : |B| = r, {aj}j\u2208B are independent } . (2)\nA set of indices B \u2282 [n] is in B if and only if it indexes a basis of the columnspace of A. Because of this analogy, elements of B are called bases of the matroid M [A]. Note that elementary algebra yields that for all B1, B2 \u2208 B and x \u2208 B1 \\B2, there exists an element y \u2208 B2 \\B1 such that\n(B1 \\ {x}) \u222a {y} \u2208 B. (3)\nProperty (3) is known as the basis-exchange property. It is used in the definition of general matroids (Oxley, 2003).\nLyons (2003) defines a projection DPP as the probability measure on B that assigns to B \u2208 B a mass proportional to |detB|2, where B , A:B is the square matrix formed by the r columns of A indexed byB. Note that this squared determinant is also the squared volume of the parallelotope spanned by the columns indexed by B. In this light, sampling a projection DPP is akin to volume sampling (Deshpande & Rademacher, 2010). Finally, observe that the Cauchy-Binet formula gives the normalization\u2211\nB\u2208B |detA:B |2 = detAAT,\nso that the probability mass assigned to B is\ndetATB: detA:B detAAT\n= det [ AT [AAT] \u22121 A ] B .\nLetting\nK = AT [AAT] \u22121 A, (4)\ngives the equivalence between Sections 2.1 and 2.2.\nA fundamental example of DPP defined by a matroid is the random set of edges obtained from a uniform spanning tree (Lyons, 2003). Let G be a connected graph with r+ 1 vertices and n edges {ei}i\u2208[n]. Let now A be the first r rows of the vertex-edge incidence matrix of G. Then B \u2282 [n] is a basis of M [A] if and only if {ei}i\u2208B form a spanning tree of G (Oxley, 2003). The transfer current theorem of Burton & Pemantle (1993) implies that the uniform distribution on B is a projection DPP, with kernel matrix (4).\n2.3. On Projection DPPs and k-DPPs in ML\nProjection DPPs are DPPs with realizations of constant cardinality k = r, where r is the rank of K. This constant cardinality is desirable when DPPs are used in summary extraction (Kulesza & Taskar, 2012; Dupuy & Bach, 2016) and the size of the required output is predefined. Another way of constraining the cardinality of a DPP is to condition on the event |X| = k, which leads to the so-called k-DPPs (Kulesza & Taskar, 2012). Projection DPPs and k-DPPs are in general different objects. In particular, a k-DPP is not a DPP in the sense of (1) unless its kernel matrix K is a projection. In that sense, k-DPPs are non-DPP objects that generalize projection DPPs. In this paper, we show that projection DPPs can benefit from fast sampling methods. It is not obvious how to generalize our algorithm to k-DPPs.\nIn ML practice, using projection DPPs is slightly different from using a k-DPP. In some applications, typically with graphs, the DPP is naturally a projection, such as uniform spanning trees described in Section 2.2. But quite often, kernels are built feature-by-feature. That is, for each data item i \u2208 [n], a normalized vector of features \u03c6i \u2208 Rr is chosen, a marginal relevance qi is assigned to item i, and a matrix L is defined as\nLij = \u221a qi\u03c6i\u03c6j \u221a qj . (5)\nIn text summarization, for instance, items i, j could be sentences, qi the marginal relevance of sentence i to the user\u2019s query, and \u03c6i features such as tf-idf frequencies of a choice of words, and one could draw from a k-DPP associated to L through P [X = I] \u221d detLI , see e.g., Kulesza & Taskar (2012, Section 4.2.1). Alternately, let A be the matrix with columns ( \u221a qi\u03c6i)i\u2208[r], and assume r < n and A is full-rank. The latter can be ensured in practice by adding a small i.i.d. Gaussian noise to each entry of A. The projection DPP with kernel K in (4) will yield samples of cardinality r, almost surely, and such that the corresponding columns of A span a large volume, hence feature-based diversity. Thus, if the application requires an output of length p, one can pick r = p, as we do in Section A. Alternatively, if we want an output of size approximately p, we can pick r \u2265 p and independently thin the resulting sample, which preserves the DPP structure (Lavancier et al., 2015)."}, {"heading": "2.4. Exact Sampling of Projection DPPs", "text": "Hough et al. (2006) give an algorithm to sample general DPPs, which is based on a subroutine to sample projection DPPs. Consider a projection DPP with kernel K such that Tr(K) = r, Hough et al.\u2019s (2006) algorithm follows the chain rule to sample a vector (x1, . . . , xr) \u2208 [n]r with successive conditional densities\np (x`+1 = i|x1 = i1, . . . , x` = i`) \u221d Kii\u2212Ki,I`K \u22121 I` KI`,i,\nwhere I` = {i1, . . . , i`}. Forgetting order, {x1, . . . , xr} are a draw from the DPP (Hough et al., 2006, Proposition 19), see also Kulesza & Taskar (2012, Theorem 2.3) for a detailed treatment of DPPs on [n].\nWhile exact, this algorithm runs in O(nr3) operations and requires computing and storing the n\u00d7nmatrix K. Storage can be diminished if one has access to A in (4), through QR decomposition of AT. Still, depending on n and r, sampling can become intractable. This has sparked interest in fast approximate sampling methods for DPPs, which we survey in Section 2.5.\nInterestingly, there exist fast and exact methods for sampling some specific DPPs, which are not based on the approach of Hough et al. (2006). We introduced the DPP behind uniform spanning trees on a connected graph G in Section 2.2. Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r3) and can be o(r3) (Levin et al., 2009), where G has r + 1 vertices. This compares favorably with the algorithm of Hough et al. (2006) above, since each sample contains r edges. The Aldous-Broder algorithm, for instance, starts from an empty set T = \u2205 and an arbitrary node x0, and samples a simple random walk (Xt)t\u2208N on the edges of G, starting from X0 = x0, and adding edge [Xt, Xt+1] to T the first time it visits vertex Xt+1. The algorithm stops when each vertex has been seen at least once, that is, at the cover time of the graph."}, {"heading": "2.5. Approximate Sampling of Projection DPPs", "text": "There are two main sets of methods for approximate sampling from general DPPs. The first set uses the generalpurpose tools from numerical algebra and the other is based on MCMC sampling.\nConsider K = CTC with C of size d \u00d7 n, for some d n (Kulesza & Taskar, 2011), but still too large for exact sampling using the method of Hough et al. (2006), then Gillenwater et al. (2012) show how projecting C can give an approximation with bounded error. When this decomposition of the kernel is not possible, Affandi et al. (2013) adapt Nystro\u0308m sampling (Williams & Seeger, 2001) to DPPs and bound the approximation error for DPPs and k-DPPs, which thus applies to projection DPPs.\nApart from general purpose approximate solvers, there exist MCMC-based methods for approximate sampling from projection DPPs. In Section 2.2, we introduced the basisexchange property, which implies that once we remove an element from a basis B1 of a linear matroid, any other basis B2 has an element we can take and add to B1 to make it a basis again. This means we can construct a connected\nAlgorithm 1 basisExchangeSampler Input: Either A or K Initialize i\u2190 0 and pick B0 \u2208 B as defined in (2) while Not converged do\nDraw u \u223c U [0,1] if u < 12 then\nDraw s \u223c UBi and t \u223c U [n]\\Bi P \u2190 (Bi \\ {s}) \u222a {t} Draw u\u2032 \u223c U [0,1] if u\u2032 < Vol\n2(A:P ) Vol2(Bi)+Vol2(A:P ) = detKPdetKBi+detKP then\nBi+1 \u2190 P else Bi+1 \u2190 Bi\nend if else Bi+1 \u2190 Bi end if i\u2190 i+ 1\nend while\ngraphGbe withB as vertex set, and we add an edge between two bases if their symmetric difference has cardinality 2. Gbe is called the basis-exchange graph. Feder & Mihail (1992) show that the simple random walk on Gbe has limiting distribution the uniform distribution on B and mixes fast, under conditions that are satisfied by the matroids involved by DPPs.\nIf the uniform distribution on B is not the DPP we want to sample from,1 we can add an accept-reject step after each move to make the desired DPP the limiting distribution of the walk. Adding such an acceptance step and a probability to stay at the current basis, Anari et al. (2016); Li et al. (2016b) give precise polynomial bounds on the mixing time of the resulting Markov chains. This Markov kernel on B is given in Algorithm 1. Note that we use the acceptance ratio of Li et al. (2016b). In the following, we make use of the notation Vol defined as follows. For any P \u2282 [n],\nVol2(A:P ) , detA T P :A:P \u221d detKP , (6)\nwhich corresponds to the squared volume of the parallelotope spanned by the columns of A indexed by P . In particular, for subsets P such that |P | > r or such that |P | = r, P /\u2208 B we have Vol2(A:P ) = 0. However, for B \u2208 B, Vol2(B) = |detA:B |2 > 0.\nWe now turn to our contribution, which finds its place in this category of MCMC-based approximate DPP samplers.\n1It may not even be a DPP (Lyons, 2003, Corollary 5.5)."}, {"heading": "3. Hit-and-run on Zonotopes", "text": "Our main contribution is the construction of a fast-mixing Markov chain with limiting distribution a given projection DPP. Importantly, we assume to know A in (4).\nAssumption 1. We know a full-rank r \u00d7 n matrix A such that K = AT(AAT)\u22121A.\nAs discussed in Section 2.3, this is not an overly restrictive assumption, as many ML applications start with building the feature matrix A rather than the similarity matrix K."}, {"heading": "3.1. Zonotopes", "text": "We define the zonotope Z(A) of A as the r-dimensional volume spanned by the column vectors of A,\nZ(A) = A[0, 1]n. (7)\nAs an affine transformation of the unit hypercube, Z(A) is a r-dimensional polytope. In particular, for a basis B \u2208 B of the matroid M [A], the corresponding Z(B) is a rdimensional parallelotope with volume Vol(B) = |detB|, see Figure 1(a). On the contrary, any P \u2282 [n], such that |P | = r, P /\u2208 B also yields a parallelotope Z(A:P ), but its volume is null. In the latter case, the exchange move in Algorithm 1 will never be accepted and the state space of the corresponding Markov chain is indeed B.\nOur algorithm relies on the proof of the following.\nProposition 1 (see Dyer & Frieze, 1994 for details).\nVol(Z(A)) = \u2211 B\u2208B Vol(B) = \u2211 B\u2208B |detB| (8)\nProof. In short, for a good choice of c \u2208 Rn, Dyer & Frieze (1994) consider for any x \u2208 Z(A), the following linear program (LP) noted Px(A, c),\nmin y\u2208Rn cTy\ns.t. Ay = x 0 \u2264 y \u2264 1.\n(9)\nStandard LP results (Luenberger & Ye, 2008) yield that the unique optimal solution y\u2217 of Px(A, c) takes the form\ny\u2217 = A\u03be(x) +Bxu, (10)\nwith u \u2208 [0, 1]r and \u03be(x) \u2208 {0, 1}n such that \u03be(x)i = 0 for i \u2208 Bx. In case the choice of Bx is ambiguous, Dyer & Frieze (1994) take the smallest in the lexicographic order. Decomposition (10) allows locating any point x \u2208 Z(A) as falling inside a uniquely defined parallelotope Z(Bx) shifted by \u03be(x). Manipulating the optimality conditions of (9), Dyer & Frieze (1994) prove that each basis B can be realized as aBx for some x, and that x\u2032 \u2208 Z(Bx)\u21d2 Bx =\nBx\u2032 . This allows to write Z(A) as the tiling of all Z(B), B \u2208 B, with disjoint interiors. This leads to Proposition 1.\nNote that c is used to fix the tiling of the zonotope, but the map x 7\u2192 Bx depends on this linear objective. Therefore, the tiling of Z(A) is may not be unique. An arbitrary c gives a valid tiling, as long as there are no ties when solving (9). Dyer & Frieze (1994) use a nonlinear mathematical trick to fix c. In practice (Section 4.1), we generate a random Gaussian c once and for all, which makes sure no ties appear during the execution, with probability 1.\nRemark 1. We propose to interpret the proof of Proposition 1 as a volume sampling algorithm: if one manages to sample an x uniformly on Z(A), and then extracts the corresponding basis B = Bx by solving (9), then B is drawn with probability proportional to Vol(B) = |detB|.\nRemark 1 is close to what we want, as sampling from a projection DPP under Assumption 1 boils down to sampling a basis B of M [A] proportionally to the squared volume |detB|2 (Section 2.2). In the rest of this section, we explain how to efficiently sample x uniformly on Z(A), and how to change the volume into its square."}, {"heading": "3.2. Hit-and-run and the Simplex Algorithm", "text": "Z(A) is a convex set. Approximate uniform sampling on large-dimensional convex bodies is one of the core questions in MCMC, see e.g., Cousins & Vempala (2016) and references therein. The hit-and-run Markov chain (Turc\u030cin, 1971; Smith, 1984) is one of the preferred practical and theoretical solutions (Cousins & Vempala, 2016).\nWe describe the Markov kernel P (x, z) of the hit-and-run Markov chain for a generic target distribution \u03c0 supported\non a convex set C. Sample a point y uniformly on the unit sphere centered at x. Letting d = y \u2212 x, this defines the line Dx , {x+ \u03b1d ; \u03b1 \u2208 R}. Then, sample z from any Markov kernel Q(x, \u00b7) supported on Dx that leaves the restriction of \u03c0 to Dx invariant. In particular, MetropolisHastings kernel (MH, Robert & Casella 2004) is often used with uniform proposal on Dx, which favors large moves across the support C of the target, see Figure 1(b). The resulting Markov kernel leaves \u03c0 invariant, see e.g., Andersen & Diaconis (2007) for a general proof. Furthermore, the hit-and-run Markov chain has polynomial mixing time for log concave \u03c0 (Lova\u0301sz & Vempala, 2003, Theorem 2.1).\nTo implement Remark 1, we need to sample from \u03c0u \u221d 1Z(A). In practice, we can choose the secondary Markov kernel Q(x, \u00b7) to be MH with uniform proposal on Dx, as long as we can determine the endpoints x+\u03b1m(y\u2212x) and x+\u03b1M (y\u2212x) ofDx\u2229Z(A). In fact, zonotopes are tricky convex sets, as even an oracle saying whether a point belongs to the zonotope requires solving LPs (basically, it is Phase I of the simplex algorithm). As noted by Lova\u0301sz & Vempala (2003, Section 4.4), hit-and-run with LP is the state-of-the-art for computing the volume of large-scale zonotopes. Thus, by definition of Z(A), this amounts to solving two more LPs: \u03b1m is the optimal solution to the linear program\nmin \u03bb\u2208Rn,\u03b1\u2208R \u03b1\ns.t. x+ \u03b1d = A\u03bb 0 \u2264 \u03bb \u2264 1,\n(11)\nwhile \u03b1M is the optimal solution of the same linear program with objective \u2212\u03b1. Thus, a combination of hit-andrun and LP solvers such as Dantzig\u2019s simplex algorithm (Luenberger & Ye, 2008) yields a Markov kernel with invariant distribution 1Z(A), summarized in Algorithm 2.\nAlgorithm 2 unifZonoHitAndRun Input: A Initialization: i\u2190 0 x0 \u2190 Au with u \u223c U [0,1]n while Not converged do\nDraw d \u223c USr\u22121 and let Dxi , xi + Rd Draw x\u0303 \u223c UDxi\u2229Z(A) #Solve 2 LPs, see (11) xi+1 \u2190 x\u0303 i\u2190 i+ 1\nend while\nAlgorithm 3 extractBasis Input: A, c, x \u2208 Z(A) Compute y\u2217 the opt. solution of Px(A, c) #1 LP, see (9) B \u2190 {i ; y\u2217i \u2208]0, 1[} return B\nThe acceptance in MH is 1 due to our choice of the proposal and the target. By the proof of Proposition 1, running Algorithm 2, taking the output chain (xi) and extracting the bases (Bxi) with Algorithm 3, we obtain a chain on B with invariant distribution proportional to the volume of B.\nIn terms of theoretical performance, this Markov chain inherits Lova\u0301sz & Vempala\u2019s (2003) mixing time as it is a simple transformation of hit-and-run targeting the uniform distribution on a convex set. We underline that this is not a pathological case and it already covers a range of applications, as changing the feature matrix A yields another zonotope, but the target distribution on the zonotope stays uniform. Machine learning practitioners do not use volume sampling for diversity sampling yet, but nothing prevents it, as it already encodes the same feature-based diversity as squared volume sampling (i.e., DPPs). Nevertheless, our initial goal was to sample from a projection DPP with kernel K under Assumption 1. We now modify the Markov chain just constructed to achieve that."}, {"heading": "3.3. From Volume to Squared Volume", "text": "Consider the probability density function on Z(A)\n\u03c0v(x) = |detBx| detAAT 1Z(A)(x),\nrepresented on our example in Figure 1(c). Observe, in particular, that \u03c0v is constant on each Z(B). Running the hit-and-run algorithm with this target instead of \u03c0u in Section 3.2, and extracting bases using Algorithm 3 again, we obtain a Markov chain on B with limiting distribution \u03bd(B) proportional to the squared volume spanned by column vectors of B, as required. To see this, note that \u03bd(B) is the volume of the \u201cskyscraper\u201d built on top of Z(B) in Figure 1(c), that is Vol(B)\u00d7Vol(B).\nAlgorithm 4 volZonoHitAndRun Input: A, c, x,B Draw d \u223c USr\u22121 and let Dx , x+ Rd Draw x\u0303 \u223c UDx\u2229Z(A) #Solve 2 LPs, see (11) B\u0303 \u2190 extractBasis(A, c, x\u0303) #Solve 1 LP, see (9) Draw u \u223c U [0,1] if u < Vol(B\u0303)Vol(B) =\n\u2223\u2223\u2223detA:B\u0303detA:B \u2223\u2223\u2223 then return x\u0303, B\u0303\nelse return x,B end if\nAlgorithm 5 zonotopeSampler Input: A, c Initialization: i\u2190 0 xi \u2190 Au, with u \u223c U [0,1]n Bi \u2190 extractBasis(A, c, xi) while Not converged do xi+1, Bi+1 \u2190 volZonoHitAndRun(A, c, xi, Bi) i\u2190 i+ 1\nend while\nThe resulting algorithm is shown in Algorithm 5. Note the acceptance ratio in the subroutine Algorithm 4 compared to Algorithm 2, since the target of the hit-and-run algorithm is not uniform anymore."}, {"heading": "3.4. On Base Measures", "text": "As described in Section 2.3, it is common in ML to specify a marginal relevance qi of each item i \u2208 [n], i.e., the base measure of the DPP. Compared to a uniform base measure, this means replacing A by A\u0303 with columns a\u0303i = \u221a qiai. Contrary to A, in Algorithm 4, both the zonotope and the acceptance ratio are scaled by the corresponding products of \u221a qis. We could equally well define A\u0303 by multiplying each column of A by qi instead of its square root, and leave the acceptance ratio in Algorithm 4 use columns of the original A. By the arguments in Section 3.3, the chain (Bi) would leave the same projection DPP invariant. In particular, we have some freedom in how to introduce the marginal relevance qi, so we can choose the latter solution that simply scales the zonotope and its tiles to preserve outer angles, while using unscaled volumes to decide acceptance. This way, we do not create harder-to-escape or sharper corners for hit-and-run, which could lead the algorithm to be stuck for a while (Cousins & Vempala, 2016, Section 4.2.1). Finally, since hit-and-run is efficient at moving across convex bodies (Lova\u0301sz & Vempala, 2003), the rationale is that if hit-and-run was empirically mixing fast before scaling, its performance should not decrease."}, {"heading": "4. Experiments", "text": "We investigate the behavior of our Algorithm 5 on synthetic graphs in Section 4.1, in summary extraction in Section 4.2, and on MNIST in Appendix A."}, {"heading": "4.1. Non-uniform Spanning Trees", "text": "We compare Algorithm 1 studied by Anari et al. (2016); Li et al. (2016b) and our Algorithm 5 on two types of graphs, in two different settings. The graphs we consider are the complete graph K10 with 10 vertices (and 45 edges) and a realization BA(20, 2) of a Baraba\u0301si-Albert graph with 20 vertices and parameter 2. We chose BA as an example of structured graph, as it has the preferential attachment property present in social networks (Baraba\u0301si & Albert, 1999). The input matrix A is a weighted version of the vertex-edge incidence matrix of each graph for which we keep only the 9 (resp. 19) first rows, so that it satisfies Assumption 1. For more generality, we introduce a base measure, as described in Section 2.3 and 3.4, by reweighting the columns of A with i.i.d. uniform variables in [0, 1]. Samples from the corresponding projection DPP are thus spanning trees drawn proportionally to the products of their edge weights.\nFor Algorithm 5, a value of the linear objective c is drawn once and for all, for each graph, from a standard Gaussian distribution. This is enough to make sure no ties appear during the execution, as mentioned in Section 3.1. This linear objective is kept fixed throughout the experiments so that the tiling of the zonotope remains the same. We run both algorithms for 70 seconds, which corresponds to roughly 50 000 iterations of Algorithm 5. Moreover, we run 100 chains in parallel for each of the two algorithms. For each of the 100 repetitions, we initialize the two algorithms with the same random initial basis, obtained by solving (9) once, with x = Au and u \u223c U [0,1]n . For both graphs, the total number |B| of bases is of order 108, so computing total variation distances is impractical. We instead compare Algorithms 1 and 5 based on the estimation of inclusion probabilities P [S \u2282 B] for various subsets S \u2282 [n] of size 3. We observed similar behaviors across 3-subsets, so we display here the typical behavior on a 3-subset.\nThe inclusion probabilities are estimated via a running average of the number of bases containing the subsets S. Figures 2(a) and 3(a) show the behavior of both algorithms vs. MCMC iterations for the complete graph K10 and a realization of BA(20, 2), respectively. Figures 2(b) and 3(b) show the behavior of both algorithms vs. wall-clock time for the complete graph K10 and a realization of BA(20, 2), respectively. In these four figures, bold curves correspond to the median of the relative errors, whereas the frontiers of colored regions indicate the first and last deciles of the relative errors.\nIn Figures 2(c) and 3(c) we compute the Gelman-Rubin statistic (Gelman & Rubin, 1992), also called the potential scale reduction factor (PSRF). We use the PSRF implementation of CODA (Plummer et al., 2006) in R, on the 100 binary chains indicating the presence of the typical 3-subset in the current basis.\nIn terms of number of iterations, our Algorithm 5 clearly mixes faster. Relatedly, we observed typical acceptance rates for our algorithm an order of magnitude larger than Algorithm 1, while simultaneously attempting more global moves than the local basis-exchange moves of Algorithm 1. The high acceptance is partly due to the structure of the zonotope: the uniform proposal in the hit-and-run algorithm already favors bases with large determinants, as the length of the intersection of Dx in Algorithm 4 with any Z(B) is an indicator of its volume, see also Figure 1(b).\nUnder the time-horizon constraint, see Figures 2(b) and 3(b), Algorithm 1 has time to perform more than 106 iterations compared to roughly 50 000 steps for our chain. The acceptance rate of Algorithm 5 is still 10 times larger, but the time required to solve the linear programs at each MCMC iteration clearly hinders our algorithm in terms of CPU time. Both algorithms are comparable in performance, but given its large acceptance, we would expect our algorithm to perform better if it was allowed to do even only 10 times more iterations. Now this is implementationdependent, and our current implementation of Algorithm 5 is relatively naive, calling the simplex algorithm in the GLPK (Oki, 2012) solver with CVXOPT (Andersen et al., 2008) from Python. We think there are big potential speedups to realize in the integration of linear programming solvers in our code. Moreover, we initialize our simplex algorithms randomly, while the different LPs we solve are related, so there may be additional smart mathematical speed-ups in using the path followed by one simplex instance to initialize the next.\nFinally, we note that the performance of our Algorithm 5 seems stable and independent of the structure of the graph, while the performance of the basis-exchange Algorithm 1 seems more graph-dependent. Further investigation is needed to make stronger statements."}, {"heading": "4.2. Text Summarization", "text": "Looking at Figures 2 and 3, our algorithm will be most useful when the bottleneck is mixing vs. number of iterations rather than CPU time. For instance, when integrating a costly-to-evaluate function against a projection DPP, the evaluation of the integrand may outweigh the cost of one iteration. To illustrate this, we adapt an experiment of Kulesza & Taskar (2012, Section 4.2.1) on minimum Bayes risk decoding for summary extraction. The idea is to find a\nsubset Y of sentences of a text that maximizes\n1\nR R\u2211 r=1 ROUGE-1F (Y, Yr) , (12)\nwhere (Yr)r are sampled from a projection DPP. ROUGE1F is a measure of similarity of two sets of sentences. We summarize this 64-sentence article as a subset of 11 sentences. In this setting, evaluating once ROUGE-1F in the sum (12) takes 0.1s on a modern laptop, while one iteration of our algorithm is 10\u22123s. Our Algorithm 5 can thus compute (12) for R = 10 000 in about the same CPU time as Algorithm 1, an iteration of which costs 10\u22125s. We show in Figure 4 the value of (12) for 3 possible summaries( Y (i) )3 i=1\nchosen uniformly at random in B, over 50 independent runs. The variance of our estimates is smaller, and the number of different summaries explored is about 50%, against 10% for Algorithm 1. Evaluating (12) using our algorithm is thus expected to be closer to the maximum of the underlying integral. Details are given in Appendix B."}, {"heading": "5. Discussion", "text": "We proposed a new MCMC kernel with limiting distribution being an arbitrary projection DPP. This MCMC kernel leverages optimization algorithms to help making global moves on a convex body that represents the DPP. We provided empirical results supporting its fast mixing when compared to the state-of-the-art basis-exchange chain of Anari et al. (2016); Li et al. (2016b). Future work will focus on an implementation: while our MCMC chain mixes faster, when compared based on CPU time our algorithm suffers from having to solve linear programs at each iter-\nation. We note that even answering the question whether a given point belongs to a zonotope involves linear programming, so that chord-finding procedures used in slice sampling (Neal, 2003, Sections 4 and 5) would not provide significant computational savings.\nWe also plan to investigate theoretical bounds on the mixing time of our Algorithm 4. We can build upon the work of Anari et al. (2016), as our Algorithm 4 is also a weighted extension of our Algorithm 2, and the polynomial bounds for the vanilla hit-and-run algorithm (Lova\u0301sz & Vempala, 2003) already apply to the latter. Note that while not targeting a DPP, our Algorithm 2 already samples items with feature-based repulsion, and could be used independently if the determinantal aspect is not crucial to the application.\nAcknowledgments The research presented was supported by French Ministry of Higher Education and Research, CPER NordPas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, and French National Research Agency projects EXTRA-LEARN (n.ANR-14-CE24-0010-01) and BOB (n.ANR-16-CE23-0003)."}, {"heading": "A. Landmarks on MNIST", "text": "To illustrate our Algorithm 5 on a non-synthetic dataset, we take a thousand images of handwritten 1s from MNIST. We consider the problem of finding landmarks among these 1s, see, for example, Liang & Paisley (2015). To obtain a set of r = 10 landmarks from a projection DPP, we design r-dimensional feature vectors of our images, the inner products of which indicate similarity. Specifically, we take A to be the coordinates of the projections of our dataset onto the first r principal directions given by a standard PCA, so that A has rank r if the whole variance is not explained by these first r eigenvalues. Note that our choice of using standard PCA is arbitrary: For manifold landmarking, we could take the coordinates output by a manifold learning algorithm such as ISOMAP.\nRunning our Algorithm 5 for 10 000 time steps, we obtain bases with squared volumes spanning three orders of magnitude. The basis in our MCMC sample with maximum squared volume is shown in the first row of Figure 5. The bottom three rows show bases drawn uniformly at random from our initial dataset, for visual comparison. The left column gives the log of the ratio of the squared volume of the corresponding basis by that of the top row.\nVisual inspection reveals that the projection DPP and our MCMC sample successfully pick up diversity among 1s: Different angles, thicknesses, and shapes are selected, while uniform random sampling exhibits less coverage. This is confirmed by the log ratios, which are far from the three orders of magnitude within our chain."}, {"heading": "B. Extractive Text Summarization", "text": "We consider the article2 entitled Scientists, Stop Thinking Explaining Science Will Fix Things. In order to generate an 11-sentences summary, we build 11 features as follows. For each sentence, we compute its number of characters and its number of words. Then, we apply a Porter stemmer (Loper & Bird, 2002) and count again the number of characters and words in each sentence. In addition, we sum the tf-idf values of the words in each sentence and compute the average cosine distance to all other sentences. Finally, we compute the position of the sentence in the original article and generate binary features indicating positions 15. We end up with a feature matrix A of size 11\u00d7 64.\nNext, we display 3 summaries of the article, the first one is drawn using Algorithm 5 while the 2 others are constructed by picking 11 sentences uniformly at random. In both cases, we sort the sentences in the order they originally appear. Samples from Algorithm 5 induce a better coverage of the article than uniform draws.\n2http://www.slate.com/articles/health_and_science/science/2017/04/explaining_science_won_t_fix_ information_illiteracy.html\nOne summary drawn with Algorithm 5:\nIf you are a scientist, this disregard for evidence probably drives you crazy.\nSo what do you do about it?\nAcross the country, science communication and advocacy groups report upticks in interest.\nIn 2010, Dan Kahan, a Yale psychologist, essentially proved this theory wrong.\nIf the deficit model were correct, Kahan reasoned, then people with increased scientific literacy, regardless of worldview, should agree with scientists that climate change poses a serious risk to humanity.\nScientific literacy, it seemed, increased polarization.\nThis lumps scientists in with the nebulous \u201dleft\u201d and, as Daniel Engber pointed out here in Slate about the upcoming March for Science, rebrands scientific authority as just another form of elitism.\nIs it any surprise, then, that lectures from scientists built on the premise that they simply know more (even if it\u2019s true) fail to convince this audience?\nWith that in mind, it may be more worthwhile to figure out how to talk about science with people they already know, through, say, local and community interactions, than it is to try to publish explainers on national news sites.\nGoldman also said scientists can do more than just educate the public: The Union of Concerned Scientists, for example, has created a science watchdogteam that keeps tabs on the activities of federal agencies.\nThere\u2019s also a certain irony that, right here in this article, I\u2019m lecturing scientists about what they might not know-in other words, I\u2019m guilty of following the deficit model myself.\nTwo summaries drawn uniformly at random:\nIf you consider yourself to have even a passing familiarity with science, you likely find yourself in a state of disbelief as the president of the United States calls climate scientists \u201dhoaxsters\u201d and pushes conspiracy theories about vaccines.\nIn fact, it\u2019s so wrong that it may have the opposite effect of what they\u2019re trying to achieve.\nRespondents who knew more about science generally, regardless of political leaning, were better able to identify the scientific consensusin other words, the polarization disappeared.\nIn fact, well-meaning attempts by scientists to inform the public might even backfire.\nPsychologists, aptly, dubbed this the \u201dbackfire effect.\u201d\nBut if scientists are motivated to change minds-and many enrolled in science communication workshops do seem to have this goal-they will be sorely disappointed.\nThat\u2019s not to say scientists should return to the bench and keep their mouths shut.\nGoldman also said scientists can do more than just educate the public: The Union of Concerned Scientists, for example, has created a science watchdogteam that keeps tabs on the activities of federal agencies.\nBut I\u2019m learning to better challenge scientists\u2019 assumptions about how communication works.\nIt\u2019s very logical, and my hunch is that it comes naturally to scientists because most have largely spent their lives in school-whether as students, professors, or mentors-and the deficit model perfectly explains how a scientist learns science.\nSo in the spirit of doing better, I\u2019ll not just write this article but also take the time to talk to scientists in person about how to communicate science strategically and to explain why it matters.\nAnd it\u2019s not just Trump-plenty of people across the political spectrum hold bizarre and inaccurate ideas about science, from climate change and vaccines to guns and genetically modified organisms.\nIt seems many scientists would take matters into their own hands by learning how to better communicate their subject to the masses.\nI\u2019ve always had a handful of intrepid graduate students, but now, fueled by the Trump administration\u2019s Etch A Sketch relationship to facts, record numbers of scientists are setting aside the pipette for the pen.\nThis is because the way most scientists think about science communication-that just explaining the real science better will help-is plain wrong.\nBefore getting fired up to set the scientific record straight, scientists would do well to first considerthe science of science communication.\nIf the deficit model were correct, Kahan reasoned, then people with increased scientific literacy, regardless of worldview, should agree with scientists that climate change poses a serious risk to humanity.\nScientific literacy, it seemed, increased polarization.\nPresenting facts that conflict with an individual\u2019s worldview, it turns out, can cause people to dig in further.\nI spoke with Gretchen Goldman, research director of the Union of Concerned Scientists\u2019 Center for Science and Democracy, which offers communication and advocacy workshops.\nCommunication that appeals to values, not just intellect, research shows, can be far more effective.\nI hope they end up doing the same."}], "references": [{"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["R.H. Affandi", "A. Kulesza", "E.B. Fox", "B. Taskar"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Affandi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Affandi et al\\.", "year": 2013}, {"title": "The random walk construction of uniform spanning trees and uniform labelled trees", "author": ["D.J. Aldous"], "venue": "SIAM Journal on Discrete Mathematics,", "citeRegEx": "Aldous,? \\Q1990\\E", "shortCiteRegEx": "Aldous", "year": 1990}, {"title": "Monte-Carlo Markov chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Anari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anari et al\\.", "year": 2016}, {"title": "Hit and run as a unifying device", "author": ["H.C. Andersen", "P.W. Diaconis"], "venue": "Journal de la Socie\u0301te\u0301 Franc\u0327aise de Statistique,", "citeRegEx": "Andersen and Diaconis,? \\Q2007\\E", "shortCiteRegEx": "Andersen and Diaconis", "year": 2007}, {"title": "CVXOPT: A python package for convex optimization", "author": ["M. Andersen", "J. Dahl", "L. Vandenberghe"], "venue": null, "citeRegEx": "Andersen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andersen et al\\.", "year": 2008}, {"title": "Emergence of scaling in random networks", "author": ["Barab\u00e1si", "A.-L", "R. Albert"], "venue": "Science, 286:11,", "citeRegEx": "Barab\u00e1si et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barab\u00e1si et al\\.", "year": 1999}, {"title": "Monte-Carlo with determinantal point processes", "author": ["R. Bardenet", "A. Hardy"], "venue": "arXiv preprint arXiv:1605.00361,", "citeRegEx": "Bardenet and Hardy,? \\Q2016\\E", "shortCiteRegEx": "Bardenet and Hardy", "year": 2016}, {"title": "Generating random spanning trees", "author": ["A. Broder"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Broder,? \\Q1989\\E", "shortCiteRegEx": "Broder", "year": 1989}, {"title": "Local characteristics, entropy and limit theorems for spanning trees and domino tilings via transfer impedances", "author": ["R. Burton", "R. Pemantle"], "venue": "The Annals of Probability,", "citeRegEx": "Burton and Pemantle,? \\Q1993\\E", "shortCiteRegEx": "Burton and Pemantle", "year": 1993}, {"title": "A practical volume algorithm", "author": ["B. Cousins", "S. Vempala"], "venue": "Mathematical Programming Computation,", "citeRegEx": "Cousins and Vempala,? \\Q2016\\E", "shortCiteRegEx": "Cousins and Vempala", "year": 2016}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["A. Deshpande", "L. Rademacher"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Deshpande and Rademacher,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher", "year": 2010}, {"title": "Learning determinantal point processes in sublinear time", "author": ["C. Dupuy", "F. Bach"], "venue": "arXiv preprint arXiv:1610.05925,", "citeRegEx": "Dupuy and Bach,? \\Q2016\\E", "shortCiteRegEx": "Dupuy and Bach", "year": 2016}, {"title": "Random walks, totally unimodular matrices, and a randomised dual simplex algorithm", "author": ["M. Dyer", "A. Frieze"], "venue": "Mathematical Programming,", "citeRegEx": "Dyer and Frieze,? \\Q1994\\E", "shortCiteRegEx": "Dyer and Frieze", "year": 1994}, {"title": "Balanced matroids", "author": ["T. Feder", "M. Mihail"], "venue": "Proceedings of the twenty-fourth annual ACM,", "citeRegEx": "Feder and Mihail,? \\Q1992\\E", "shortCiteRegEx": "Feder and Mihail", "year": 1992}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D.B. Rubin"], "venue": "Statist. Sci.,", "citeRegEx": "Gelman and Rubin,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin", "year": 1992}, {"title": "Discovering diverse and salient threads in document collections", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Gillenwater et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 2012}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability surveys,", "citeRegEx": "Hough et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hough et al\\.", "year": 2006}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["B. Kang"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Kang,? \\Q2013\\E", "shortCiteRegEx": "Kang", "year": 2013}, {"title": "Batched gaussian process bandit optimization via determinantal point processes", "author": ["T. Kathuria", "A. Deshpande", "P. Kohli"], "venue": "Neural Information Processing Systems, pp. pp", "citeRegEx": "Kathuria et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kathuria et al\\.", "year": 2016}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Kulesza and Taskar,? \\Q2012\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2012}, {"title": "k-dpps: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Kulesza and Taskar,? \\Q2011\\E", "shortCiteRegEx": "Kulesza and Taskar", "year": 2011}, {"title": "Determinantal point process models and statistical inference", "author": ["F. Lavancier", "J. M\u00f8ller", "E. Rubak"], "venue": "Journal of the Royal Statistical Society. Series B: Statistical Methodology,", "citeRegEx": "Lavancier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lavancier et al\\.", "year": 2015}, {"title": "Efficient sampling for kdeterminantal point processes", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fast sampling for strongly rayleigh measures with application to determinantal point processes", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Landmarking manifolds with gaussian processes", "author": ["D. Liang", "J. Paisley"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liang and Paisley,? \\Q2015\\E", "shortCiteRegEx": "Liang and Paisley", "year": 2015}, {"title": "NLTK: The natural language toolkit", "author": ["E. Loper", "S. Bird"], "venue": "In Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,", "citeRegEx": "Loper and Bird,? \\Q2002\\E", "shortCiteRegEx": "Loper and Bird", "year": 2002}, {"title": "Hit and run is fast and fun", "author": ["L. Lov\u00e1sz", "S. Vempala"], "venue": "Technical Report MSR-TR-2003-05,", "citeRegEx": "Lov\u00e1sz and Vempala,? \\Q2003\\E", "shortCiteRegEx": "Lov\u00e1sz and Vempala", "year": 2003}, {"title": "Linear and nonlinear programming", "author": ["D.G. Luenberger", "Y. Ye"], "venue": null, "citeRegEx": "Luenberger and Ye,? \\Q2008\\E", "shortCiteRegEx": "Luenberger and Ye", "year": 2008}, {"title": "Determinantal probability measures", "author": ["R. Lyons"], "venue": "Publications Mathe\u0301matiques de l\u2019Institut des Hautes E\u0301tudes Scientifiques,", "citeRegEx": "Lyons,? \\Q2003\\E", "shortCiteRegEx": "Lyons", "year": 2003}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability,", "citeRegEx": "Macchi,? \\Q1975\\E", "shortCiteRegEx": "Macchi", "year": 1975}, {"title": "Gnu linear programming kit, version 4.61", "author": ["E. Oki"], "venue": null, "citeRegEx": "Oki,? \\Q2012\\E", "shortCiteRegEx": "Oki", "year": 2012}, {"title": "What is a matroid", "author": ["J. Oxley"], "venue": "Cubo Matema\u0301tica Educacional,", "citeRegEx": "Oxley,? \\Q2003\\E", "shortCiteRegEx": "Oxley", "year": 2003}, {"title": "Coda: Convergence diagnosis and output analysis for MCMC", "author": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"], "venue": "R News,", "citeRegEx": "Plummer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2006}, {"title": "Fast mixing for discrete point processes", "author": ["P. Rebeschini", "A. Karbasi"], "venue": "In Conference on Learning Theory, pp", "citeRegEx": "Rebeschini and Karbasi,? \\Q2015\\E", "shortCiteRegEx": "Rebeschini and Karbasi", "year": 2015}, {"title": "Monte-Carlo Statistical Methods", "author": ["C.P. Robert", "G. Casella"], "venue": null, "citeRegEx": "Robert and Casella,? \\Q2004\\E", "shortCiteRegEx": "Robert and Casella", "year": 2004}, {"title": "Zeros of principal L-functions and random matrix theory", "author": ["Z. Rudnick", "P. Sarnak"], "venue": "Duke Mathematical Journal,", "citeRegEx": "Rudnick and Sarnak,? \\Q1996\\E", "shortCiteRegEx": "Rudnick and Sarnak", "year": 1996}, {"title": "Efficient Monte-Carlo procedures for generating points uniformly distributed over bounded regions", "author": ["R.L. Smith"], "venue": "Operations Research,", "citeRegEx": "Smith,? \\Q1984\\E", "shortCiteRegEx": "Smith", "year": 1984}, {"title": "A determinantal point process latent variable model for inhibition in neural spiking data", "author": ["J. Snoek", "R. Zemel", "R.P. Adams"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Snoek et al\\.,? \\Q1932\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 1932}, {"title": "On the computation of multidimensional integrals by the monte-carlo method", "author": ["V.F. Tur\u010din"], "venue": "Theory of Probability & Its Applications,", "citeRegEx": "Tur\u010din,? \\Q1971\\E", "shortCiteRegEx": "Tur\u010din", "year": 1971}], "referenceMentions": [{"referenceID": 16, "context": "DPPs were introduced by Macchi (1975) and have then found applications in fields as diverse as probability (Hough et al., 2006), number theory (Rudnick & Sarnak, 1996), statistical physics (Pathria & Beale, 2011), Monte Carlo methods (Bardenet & Hardy, 2016), and spatial statistics (Lavancier et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 21, "context": ", 2006), number theory (Rudnick & Sarnak, 1996), statistical physics (Pathria & Beale, 2011), Monte Carlo methods (Bardenet & Hardy, 2016), and spatial statistics (Lavancier et al., 2015).", "startOffset": 163, "endOffset": 187}, {"referenceID": 27, "context": "DPPs were introduced by Macchi (1975) and have then found applications in fields as diverse as probability (Hough et al.", "startOffset": 24, "endOffset": 38}, {"referenceID": 18, "context": "Applications of DPPs in machine learning (ML) since this survey also include recommendation tasks (Kathuria et al., 2016; Gartrell et al., 2017), text summarization (Dupuy & Bach, 2016), or models for neural signals (Snoek et al.", "startOffset": 98, "endOffset": 144}, {"referenceID": 1, "context": "While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al.", "startOffset": 83, "endOffset": 133}, {"referenceID": 7, "context": "While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al.", "startOffset": 83, "endOffset": 133}, {"referenceID": 15, "context": "While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al., 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al.", "startOffset": 288, "endOffset": 360}, {"referenceID": 0, "context": "While fast exact algorithms exist for specific DPPs such as uniform spanning trees (Aldous, 1990; Broder, 1989; Propp & Wilson, 1998), generic DPPs have so far been addressed with approximate sampling algorithms, using random projections (Kulesza & Taskar, 2012), low-rank approximations (Kulesza & Taskar, 2011; Gillenwater et al., 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al.", "startOffset": 288, "endOffset": 360}, {"referenceID": 17, "context": ", 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b).", "startOffset": 54, "endOffset": 150}, {"referenceID": 2, "context": ", 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b).", "startOffset": 54, "endOffset": 150}, {"referenceID": 0, "context": ", 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b). In particular, there are polynomial bounds on the mixing rates of natural MCMC chains with arbitrary DPPs as their limiting measure; see Anari et al. (2016) for cardinality-constrained DPPs, and Li et al.", "startOffset": 8, "endOffset": 331}, {"referenceID": 0, "context": ", 2012; Affandi et al., 2013), or using Markov chain Monte Carlo techniques (Kang, 2013; Li et al., 2016a; Rebeschini & Karbasi, 2015; Anari et al., 2016; Li et al., 2016b). In particular, there are polynomial bounds on the mixing rates of natural MCMC chains with arbitrary DPPs as their limiting measure; see Anari et al. (2016) for cardinality-constrained DPPs, and Li et al. (2016b) for the general case.", "startOffset": 8, "endOffset": 387}, {"referenceID": 2, "context": "While the results of Anari et al. (2016) and their generalization by Li et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 2, "context": "While the results of Anari et al. (2016) and their generalization by Li et al. (2016b) apply to projection DPPs, our experiments support the fact that our chain mixes faster.", "startOffset": 21, "endOffset": 87}, {"referenceID": 16, "context": "In this section, we introduce projection DPPs in two equivalent ways, respectively following Hough et al. (2006), Kulesza & Taskar (2012), and Lyons (2003).", "startOffset": 93, "endOffset": 113}, {"referenceID": 16, "context": "In this section, we introduce projection DPPs in two equivalent ways, respectively following Hough et al. (2006), Kulesza & Taskar (2012), and Lyons (2003).", "startOffset": 93, "endOffset": 138}, {"referenceID": 16, "context": "In this section, we introduce projection DPPs in two equivalent ways, respectively following Hough et al. (2006), Kulesza & Taskar (2012), and Lyons (2003). Both definitions shed a different light on the algorithms in Section 3.", "startOffset": 93, "endOffset": 156}, {"referenceID": 31, "context": "It is used in the definition of general matroids (Oxley, 2003).", "startOffset": 49, "endOffset": 62}, {"referenceID": 28, "context": "A fundamental example of DPP defined by a matroid is the random set of edges obtained from a uniform spanning tree (Lyons, 2003).", "startOffset": 115, "endOffset": 128}, {"referenceID": 31, "context": "Then B \u2282 [n] is a basis of M [A] if and only if {ei}i\u2208B form a spanning tree of G (Oxley, 2003).", "startOffset": 82, "endOffset": 95}, {"referenceID": 28, "context": "A fundamental example of DPP defined by a matroid is the random set of edges obtained from a uniform spanning tree (Lyons, 2003). Let G be a connected graph with r+ 1 vertices and n edges {ei}i\u2208[n]. Let now A be the first r rows of the vertex-edge incidence matrix of G. Then B \u2282 [n] is a basis of M [A] if and only if {ei}i\u2208B form a spanning tree of G (Oxley, 2003). The transfer current theorem of Burton & Pemantle (1993) implies that the uniform distribution on B is a projection DPP, with kernel matrix (4).", "startOffset": 116, "endOffset": 425}, {"referenceID": 21, "context": "Alternatively, if we want an output of size approximately p, we can pick r \u2265 p and independently thin the resulting sample, which preserves the DPP structure (Lavancier et al., 2015).", "startOffset": 158, "endOffset": 182}, {"referenceID": 14, "context": "Interestingly, there exist fast and exact methods for sampling some specific DPPs, which are not based on the approach of Hough et al. (2006). We introduced the DPP behind uniform spanning trees on a connected graph G in Section 2.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": "Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r) and can be o(r) (Levin et al.", "startOffset": 43, "endOffset": 57}, {"referenceID": 1, "context": "Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r) and can be o(r) (Levin et al.", "startOffset": 43, "endOffset": 72}, {"referenceID": 1, "context": "Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r) and can be o(r) (Levin et al.", "startOffset": 43, "endOffset": 99}, {"referenceID": 1, "context": "Random walk algorithms such as the ones by Aldous (1990), Broder (1989), and Propp & Wilson (1998) sample uniform spanning trees in time bounded by the cover time of the graph, for instance, which is O(r) and can be o(r) (Levin et al., 2009), where G has r + 1 vertices. This compares favorably with the algorithm of Hough et al. (2006) above, since each sample contains r edges.", "startOffset": 43, "endOffset": 337}, {"referenceID": 14, "context": "Consider K = CC with C of size d \u00d7 n, for some d n (Kulesza & Taskar, 2011), but still too large for exact sampling using the method of Hough et al. (2006), then Gillenwater et al.", "startOffset": 136, "endOffset": 156}, {"referenceID": 14, "context": "(2006), then Gillenwater et al. (2012) show how projecting C can give an approximation with bounded error.", "startOffset": 13, "endOffset": 39}, {"referenceID": 0, "context": "When this decomposition of the kernel is not possible, Affandi et al. (2013) adapt Nystr\u00f6m sampling (Williams & Seeger, 2001) to DPPs and bound the approximation error for DPPs and k-DPPs, which thus applies to projection DPPs.", "startOffset": 55, "endOffset": 77}, {"referenceID": 2, "context": "Adding such an acceptance step and a probability to stay at the current basis, Anari et al. (2016); Li et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "Adding such an acceptance step and a probability to stay at the current basis, Anari et al. (2016); Li et al. (2016b) give precise polynomial bounds on the mixing time of the resulting Markov chains.", "startOffset": 79, "endOffset": 118}, {"referenceID": 2, "context": "Adding such an acceptance step and a probability to stay at the current basis, Anari et al. (2016); Li et al. (2016b) give precise polynomial bounds on the mixing time of the resulting Markov chains. This Markov kernel on B is given in Algorithm 1. Note that we use the acceptance ratio of Li et al. (2016b). In the following, we make use of the notation Vol defined as follows.", "startOffset": 79, "endOffset": 308}, {"referenceID": 38, "context": "The hit-and-run Markov chain (Tur\u010din, 1971; Smith, 1984) is one of the preferred practical and theoretical solutions (Cousins & Vempala, 2016).", "startOffset": 29, "endOffset": 56}, {"referenceID": 36, "context": "The hit-and-run Markov chain (Tur\u010din, 1971; Smith, 1984) is one of the preferred practical and theoretical solutions (Cousins & Vempala, 2016).", "startOffset": 29, "endOffset": 56}, {"referenceID": 2, "context": "We compare Algorithm 1 studied by Anari et al. (2016); Li et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 2, "context": "We compare Algorithm 1 studied by Anari et al. (2016); Li et al. (2016b) and our Algorithm 5 on two types of graphs, in two different settings.", "startOffset": 34, "endOffset": 73}, {"referenceID": 32, "context": "We use the PSRF implementation of CODA (Plummer et al., 2006) in R, on the 100 binary chains indicating the presence of the typical 3-subset in the current basis.", "startOffset": 39, "endOffset": 61}, {"referenceID": 30, "context": "Now this is implementationdependent, and our current implementation of Algorithm 5 is relatively naive, calling the simplex algorithm in the GLPK (Oki, 2012) solver with CVXOPT (Andersen et al.", "startOffset": 146, "endOffset": 157}, {"referenceID": 4, "context": "Now this is implementationdependent, and our current implementation of Algorithm 5 is relatively naive, calling the simplex algorithm in the GLPK (Oki, 2012) solver with CVXOPT (Andersen et al., 2008) from Python.", "startOffset": 177, "endOffset": 200}, {"referenceID": 2, "context": "We provided empirical results supporting its fast mixing when compared to the state-of-the-art basis-exchange chain of Anari et al. (2016); Li et al.", "startOffset": 119, "endOffset": 139}, {"referenceID": 2, "context": "We provided empirical results supporting its fast mixing when compared to the state-of-the-art basis-exchange chain of Anari et al. (2016); Li et al. (2016b). Future work will focus on an implementation: while our MCMC chain mixes faster, when compared based on CPU time our algorithm suffers from having to solve linear programs at each iterFigure 4.", "startOffset": 119, "endOffset": 158}, {"referenceID": 2, "context": "We can build upon the work of Anari et al. (2016), as our Algorithm 4 is also a weighted extension of our Algorithm 2, and the polynomial bounds for the vanilla hit-and-run algorithm (Lov\u00e1sz & Vempala, 2003) already apply to the latter.", "startOffset": 30, "endOffset": 50}], "year": 2017, "abstractText": "Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costlyto-evaluate functions, such as summary extraction in our experiments.", "creator": "LaTeX with hyperref package"}}}