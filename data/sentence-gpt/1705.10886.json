{"id": "1705.10886", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "High Dimensional Structured Superposition Models", "abstract": "High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. (the basic assumption is that the model itself will be relatively simple, but still highly complex.) For example, in the second half of the experiment, only the sample sample size of the sample size was used, while the sample size of the sample size in the initial study was used to determine the length of the sample. (See Note 4.) The following model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\n\n\n\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter. The following model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter. The following model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter. The following model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter. The following model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter.\nThe final model uses the same model: a small, small and larger sample size. Each set of parameter parameters is an exact sum of the parameters for each parameter. The following model uses the same model: a small, small and larger sample size. Each set of parameter", "histories": [["v1", "Tue, 30 May 2017 23:00:34 GMT  (358kb,D)", "http://arxiv.org/abs/1705.10886v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["qilong gu", "arindam banerjee"], "accepted": true, "id": "1705.10886"}, "pdf": {"name": "1705.10886.pdf", "metadata": {"source": "CRF", "title": "High Dimensional Structured Superposition Models", "authors": ["Qilong Gu", "Arindam Banerjee"], "emails": ["guxxx396@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": "1 Introduction", "text": "For high-dimensional structured estimation problems [7, 27], considerable advances have been made in accurately estimating a sparse or structured parameter \u03b8 \u2208 Rp even when the sample size n is far smaller than the ambient dimensionality of \u03b8\u2217, i.e., n p. Instead of a single structure, such as sparsity or low rank, recent years have seen interest in parameter estimation when the parameter \u03b8\u2217 is a superposition or sum of multiple different structures, i.e., \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].\nIn this paper, we substantially generalize the non-asymptotic estimation error analysis for such superposition models such that (i) the parameter \u03b8 can be the superposition of any number of component parameters \u03b8\u2217i , and (ii) the structure in each \u03b8\u2217i can be captured by any suitable norm Ri(\u03b8 \u2217 i ). We will analyze the following linear measurement based superposition model\ny = X k\u2211 i=1 \u03b8\u2217i + \u03c9 , (1)\nwhere X \u2208 Rn\u00d7p is a random sub-Gaussian design or compressive matrix, k is the number of components,\nar X\niv :1\n70 5.\n10 88\n6v 1\n[ cs\n.L G\n] 3\n0 M\n\u03b8\u2217i is one component of the unknown parameters, y \u2208 Rn is the response vector, and \u03c9 \u2208 Rn is random noise independent of X . The structure in each component \u03b8\u2217i is captured by any suitable norm Ri(\u00b7), such that Ri(\u03b8 \u2217 i ) has a small value, e.g., sparsity captured by \u2016\u03b8\u2217i \u20161, low-rank (for matrix \u03b8\u2217i ) captured by the nuclear norm \u2016\u03b8\u2217i \u2016\u2217, etc. Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).\nThe superposition estimation problem can be posed as follows: Given (y,X) generated following (1), estimate component parameters {\u03b8\u0302i} such that all the component-wise estimation errors \u2206i = \u03b8\u0302i \u2212 \u03b8\u2217i , where \u03b8\u2217i is the population mean, are small. Ideally, we want to obtain high-probability non-asymptotic bounds on the total componentwise error measured as \u2211k i=1 \u2016\u03b8\u0302i \u2212 \u03b8\u2217i \u20162, with the bound improving (getting smaller) with increase in the number n of samples.\nWe propose the following estimator for the superposition model in (1):\nmin {\u03b81,...,\u03b8k} \u2225\u2225\u2225\u2225\u2225y \u2212X k\u2211 i=1 \u03b8i \u2225\u2225\u2225\u2225\u2225 2\n2\ns.t. Ri(\u03b8i) \u2264 \u03b1i , i = 1, . . . , k , (2)\nwhere \u03b1i are suitable constants. In this paper, we focus on the case where \u03b1i = Ri(\u03b8\u2217i ), noting that recent advances [22] can be used to extend our results to more general settings.\nThe superposition estimator in (2) succeeds if a certain geometric condition, which we call structural coherence (SC), is satisfied by certain sets (cones) associated with the component norms Ri(\u00b7). Since the estimate \u03b8\u0302i = \u03b8\u2217i + \u2206i is in the feasible set of the optimization problem (2), the error vector \u2206i satisfies the constraint Ri(\u03b8\u2217i + \u2206i) \u2264 \u03b1i where \u03b1i = Ri(\u03b8\u2217i ). The SC condition of is a geometric relationship between the corresponding error cones Ci = cone{\u2206i|Ri(\u03b8\u2217i + \u2206i) \u2264 Ri(\u03b8\u2217i )} (see Section 3). If SC is satisfied, then we can show that the sum of componentwise estimation error can be bounded with high probability, and the bound takes the form:\nk\u2211 i=1 \u2016\u03b8\u0302i \u2212 \u03b8\u2217i \u20162 \u2264 c maxiw(Ci \u2229Bp) + \u221a log k\u221a n , (3)\nwhere n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp. Interestingly, the estimation error converges at the rate of 1\u221a\nn , similar to the case of single parameter estimators [21, 3], and depends only\nlogarithmically on the number of components k. Further, while dependency of the error on Gaussian width of the error has been shown in recent results involving a single parameter [3, 30], the bound in (3) depends on the maximum of the Gaussian width of individual error cones, not their sum. The analysis thus gives a general way to construct estimators for superposition problems along with high-probability non-asymptotic upper bounds on the sum of componentwise errors. To show the generality of our work, we provide a detailed review and comparison with related work in Appendix 8.\nNotation: In this paper, we use \u2016.\u2016 to denote vector norm, and |||.||| to denote operator norm. For example, \u2016.\u20162 is the Euclidean norm for a vector or matrix, and |||.|||\u2217 is the nuclear norm of a matrix. We denote cone{E} as the smallest closed cone that contains a given set E . We denote \u3008., .\u3009 as the inner product.\nThe rest of this paper is organized as follows: We start with an optimization algorithm in Section 6 and a deterministic estimation error bound in Section 2, while laying down the key geometric and statistical quantities involved in the analysis. In Section 3, we discuss the geometry of the structural coherence (SC) condition, and show that the geometric SC condition implies statistical restricted eigenvalue (RE) condition. In Section 5, we develop the main error bound on the sum of componentwise errors which hold with high probability for sub-Gaussian designs and noise. In Section 7, we compare an estimator using \u201cinfimal convolution\u201d[25] of norms with our estimator (2) for the noiseless case. We discuss related work in Section 8. We apply our error bound to practical problems in Section 9, present experimental results in Section 10, and conclude in Section 11. The proofs of all technical results are in the Appendix."}, {"heading": "2 Error Structure and Recovery Guarantees", "text": "In this section, we start with some basic results and, under suitable assumptions, provide a deterministic bound for the componentwise estimation error in superposition models. Subsequently, we will show that the assumptions made here hold with high probability as long as a purely geometric non-probabilistic condition characterized by structural coherence (SC) is satisfied. Let {\u03b8\u0302i} be a solution to the superposition estimation problem in (2), {\u03b8\u2217i } be the optimal (population) parameters involved in the true data generation process. Let \u2206i = \u03b8\u0302i\u2212 \u03b8\u2217i be the error vector for component i of the superposition. Our goal is to provide a preliminary understanding of the structure of error sets where \u2206i live, identify conditions under which a bound on the total componentwise error \u2211k i=1 \u2016\u03b8\u0302i \u2212 \u03b8\u2217i \u20162 will hold, and provide a preliminary version of such a bound, which will be subsequently refined to the form in (3) in Section 5. Since \u03b8\u0302i = \u03b8\u2217i + \u2206i lies in the feasible set of (2), as discussed in Section 1, the error vectors \u2206i will lie in the error sets Ei = {\u2206i \u2208 Rp|Ri(\u03b8\u2217i + \u2206i) \u2264 Ri(\u03b8\u2217i )} respectively. For the analysis, we will be focusing on the cone of such error sets, given by\nCi = cone{\u2206i \u2208 Rp|Ri(\u03b8\u2217i + \u2206i) \u2264 Ri(\u03b8\u2217i )} . (4)\nLet \u03b8\u2217 = \u2211k\ni=1 \u03b8 \u2217 i , \u03b8\u0302 = \u2211k i=1 \u03b8\u0302i, and \u2206 = \u2211k i=1 \u2206i, so that \u2206 = \u03b8\u0302 \u2212 \u03b8\u2217. From the optimality of \u03b8\u0302 as a\nsolution to (2), we have\n\u2016y \u2212X\u03b8\u0302\u20162 \u2264 \u2016y \u2212X\u03b8\u2217\u20162 \u21d2 \u2016X\u2206\u20162 \u2264 2\u03c9TX\u2206 , (5)\nusing \u03b8\u0302 = \u03b8\u2217 + \u2206 and y = X\u03b8\u2217 + \u03c9. In order to establish recovery guarantees, under suitable assumptions we construct a lower bound to \u2016X\u2206\u20162, the left hand side of (5). The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24]. We also construct an upper bound to \u03c9TX\u2206, the right hand side of (5), which needs to carefully analyze the noise-design (ND) interaction, i.e., between the noise \u03c9 and the design X .\nWe start by assuming that a generalized form of RE condition is satisfied by the superposition of errors:\nthere exists a constant \u03ba > 0 such that for all \u2206i \u2208 Ci, i = 1, 2, . . . , k:\n(RE) 1\u221a n \u2225\u2225\u2225\u2225\u2225X k\u2211 i=1 \u2206i \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03ba k\u2211 i=1 \u2016\u2206i\u20162 . (6)\nThe above RE condition considers the following set:\nH = { k\u2211 i=1 \u2206i : \u2206i \u2208 Ci, k\u2211 i=1 \u2016\u2206i\u20162 = 1 } . (7)\nwhich involves all the k error cones, and the lower bound is over the sum of norms of the component wise errors. If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved. If we set all components but \u2206i to zero, then (6) becomes the RE condition only for component i. We also note that the general RE condition as explicitly stated in (6) has been implicitly used in [1] and [32]. For subsequent analysis, we introduce the set H\u0304 defined as\nH\u0304 = { k\u2211 i=1 \u2206i : \u2206i \u2208 Ci, k\u2211 i=1 \u2016\u2206i\u20162 \u2264 1 } . (8)\nnoting thatH \u2282 H\u0304. The general RE condition in (6) depends on the random design matrix X , and is hence an inequality which will hold with certain probability depending on X and the set H. For superposition problems, the probabilistic RE condition as in (6) is intimately related to the following deterministic structural coherence (SC) condition on the interaction of the different component cones Ci, without any explicit reference to the random design matrix X: there is a constant \u03c1 > 0 such that for all \u2206i \u2208 Ci, i = 1, . . . , k,\n(SC)\n\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u2206i \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03c1 k\u2211 i=1 \u2016\u2206i\u20162 . (9)\nIf k = 1, the SC condition is trivially satisfied with \u03c1 = 1. Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully. For k > 1, the SC condition (9) implies a non-trivial relationship among the component cones. In particular, if the SC condition is true, then the sum \u2211k i=1 \u2206i being zero implies that each component \u2206i must also be zero. As presented in (9), the SC condition comes across as an algebraic condition. In Section 3, we present a geometric characterization of the SC condition [18], and illustrate that the condition is both necessary and sufficient for accurate recovery of each component. In Section 4, we show that for sub-Gaussian design matrices X , the SC condition in (9) in fact implies that the RE condition in (6) will hold with high probability, after the number of samples crosses a certain sample complexity, which depends on the Gaussian width of the component cones. For now, we assume the RE condition in (6) to hold, and proceed with the error bound analysis.\nTo establish recovery guarantee, following (5), we need an upper bound on the interaction between noise \u03c9\nand design X [3, 20]. In particular, we consider the noise-design (ND) interaction\n(ND) sn(\u03b3) = inf s>0\n{ s : sup\nu\u2208sH\n1\u221a n \u03c9TXu \u2264 \u03b3s2 \u221a n\n} , (10)\nwhere \u03b3 > 0 is a constant, and sH is the scaled version ofH where the scaling factor is s > 0. Here, sn(\u03b3) denotes the minimal scaling needed onH such that one obtains a uniform bound over \u2206 \u2208 sH of the form: 1 n\u03c9\nTX\u2206 \u2264 \u03b3s2n(\u03b3). Then, from the basic inequality in (5), with the bounds implied by the RE condition and the ND interaction, we have\n1\u221a n \u2016X\u2206\u20162 \u2264 1\u221a n\n\u221a \u03c9TX\u2206 \u21d2 \u03ba k\u2211 i=1 \u2016\u2206i\u20162 \u2264 \u221a \u03b3sn(\u03b3) , (11)\nwhich implies a bound on the component-wise error. The main deterministic bound below states the result formally:\nTheorem 1 (Deterministic bound) Assume that the RE condition in (6) is satisfied inH with parameter \u03ba. Then, if \u03ba2 > \u03b3, we have \u2211k i=1 \u2016\u2206i\u20162 6 2sn(\u03b3).\nThe above bound is deterministic and holds only when the RE condition in (6) is satisfied with constant \u03ba such that \u03ba2 > \u03b3. In the sequel, we first give a geometric characterization of the SC condition in Section 3, and show that the SC condition implies the RE condition with high probability in Section 4. Further, we give a high probability characterization of sn(\u03b3) based on the noise \u03c9 and design X in terms of the Gaussian widths of the component cones, and also illustrate how one can choose \u03b3 in Section 5. With these characterizations, we will obtain the desired component-wise error bound of the form (3)."}, {"heading": "3 Geometry of Structural Coherence", "text": "In this section, we give a geometric characterization of the structural coherence (SC) condition in (9). We start with the simplest case of two vectors x, y. If they are not reflections of each other, i.e., x 6= \u2212y, then the following relationship holds:\nProposition 2 If there exists a \u03b4 < 1 such that \u2212\u3008x, y\u3009 \u2264 \u03b4\u2016x\u20162\u2016y\u20162, then\n\u2016x+ y\u20162 \u2265 \u221a\n1\u2212 \u03b4 2 (\u2016x\u20162 + \u2016y\u20162) . (12)\nNext, we generalize the condition of Proposition 2 to vectors in two different cones C1 and C2. Given the cones, define\n\u03b40 = sup x\u2208C1\u2229Sp\u22121,y\u2208C2\u2229Sp\u22121\n\u2212 \u3008x, y\u3009 . (13)\nBy construction, \u2212\u3008x, y\u3009 \u2264 \u03b40\u2016x\u20162\u2016y\u20162 for all x \u2208 C1 and y \u2208 C2. If \u03b40 < 1, then (12) continues to hold for all x \u2208 C1 and y \u2208 C2 with constant \u221a (1\u2212 \u03b40)/2 > 0. Note that this corresponds to the SC condition\nwith k = 2 and \u03c1 = \u221a\n(1\u2212 \u03b40)/2. We can interpret this geometrically as follows: first reflect cone C1 to get \u2212C1, then \u03b4 is the cosine of the minimum angle between \u2212C1 and C2. If \u03b40 = 1, then \u2212C1 and C2 share a ray, and structural coherence does not hold. Otherwise, \u03b40 < 1, implying \u2212C1 \u2229 C2 = {0}, i.e., the two cones intersect only at the origin, and structural coherence holds.\nFor the general case involving k cones, denote\n\u03b4i = sup u\u2208\u2212Ci\u2229Sp\u22121,v\u2208 \u2211 j 6=i Cj\u2229Sp\u22121 \u3008u, v\u3009 . (14)\nIn recent work, [18] concluded that if \u03b4i < 1 for each i = 1, . . . , k then \u2212Ci and \u2211\nj 6=i Cj does not share a ray, and the original signal can be recovered in noiseless case. We show that the condition above in fact implies \u03c1 > 0 for the SC condition in (9), which is sufficient for accurate recovery even in the noisy case. In particular, with \u03b4 := maxi \u03b4i, we have the following result:\nTheorem 3 (Structural Coherence (SC) Condition) Let \u03b4 := maxi \u03b4i with \u03b4i as defined in (14). If \u03b4 < 1, there exists a \u03c1 > 0 such that for any \u2206i \u2208 Ci, i = 1, . . . , k, the SC condition in (9) holds, i.e.,\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u2206i \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03c1 k\u2211 i=1 \u2016\u2206i\u20162 . (15)\nThus, the SC condition is satisfied in the general case as long as the reflection \u2212Ci of any cone Ci does not intersect, i.e., share a ray, with the Minkowski sum \u2211 j 6=i Cj of the other cones."}, {"heading": "4 Restricted Eigenvalue Condition for Superposition Models", "text": "Assuming that the SC condition is satisfied by the error cones {Ci}, i = 1, . . . , k, in this section we show that the general RE condition in (6) will be satisfied with high probability when the number of samples n in the sub-Gaussian design matrix X \u2208 Rn\u00d7p crosses the sample complexity n0. We give a precise characterization of the sample complexity n0 in terms of the Gaussian width of the setH. Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques. We start with a restricted eigenvalue condition on H. For a random vector Z \u2208 Rp, we define marginal tail function for an arbitrary set E as\nQ\u03be(E;Z) = inf u\u2208E\nP (|\u3008Z, u\u3009| \u2265 \u03be) , (16)\nnoting that it is deterministic given the set E \u2286 Rp. Let i, i = 1, . . . , n, be independent Rademacher random variables, i.e., random variable with probability 12 of being either +1 or\u22121, and letXi, i = 1, . . . , n, be independent copies of Z. We define empirical width of E as\nWn(E;Z) = sup u\u2208E \u3008h, u\u3009, where h = 1\u221a n n\u2211 i=1 iXi . (17)\nWith this notation, we recall the following result from [28, Proposition 5.1]:\nLemma 1 Let X \u2208 Rn\u00d7p be a random design matrix with each row the independent copy of sub-Gaussian random vector Z. Then for any \u03be, \u03c1, t > 0, we have\ninf u\u2208H \u2016Xu\u20162 \u2265 \u03c1\u03be\n\u221a nQ2\u03c1\u03be(H;Z)\u2212 2Wn(H;Z)\u2212 \u03c1\u03bet (18)\nwith probability at least 1\u2212 e\u2212 t2 2 .\nFrom Lemma 1, in order to obtain lower bound of \u03ba in RE condition (6), we need to lower boundQ2\u03c1\u03be(H;Z) and upper bound Wn(H;Z). To lower bound Q2\u03c1\u03be(H;Z), we consider the spherical cap\nA = ( \u2211k\ni=1 Ci) \u2229 Sp\u22121 . (19)\nFrom [28, 20], one can obtain a lower bound to Q\u03be(A;Z) based on the Paley-Zygmund inequality. The Paley-Zygmund inequality lower bound the tail distribution of a random variable by its second momentum. Let u be an arbitrary vector, we use the following version of the inequality.\nP (|\u3008Z, u\u3009| \u2265 2\u03be) \u2265 [E|\u3008Z, u\u3009| \u2212 2\u03be]2+\nE|\u3008Z, u\u3009|2 (20)\nIn the current context, the following result is a direct consequence of SC condition, which shows that Q2\u03c1\u03be(H;Z) is lower bounded by Q\u03be(A;Z), which in turn is strictly bounded away from 0. The proof of Lemma 2 is given in Appendix C.1.\nLemma 2 Let setsH andA be as defined in (7) and (19) respectively. If the SC condition in (9) holds, then the marginal tail functions of the two sets have the following relationship:\nQ\u03c1\u03be(H;Z) \u2265 Q\u03be(A;Z). (21)\nNext we discuss how to upper bound the empirical width Wn(H;Z). Let set E be arbitrary, and random vector g \u223c N (0, Ip) be a standard Gaussian random vector in Rp. The Gaussian width [3] of E is defined as\nw(E) = E sup u\u2208E \u3008g, u\u3009. (22)\nEmpirical width Wn(H;Z) can be seen as the supremum of a stochastic process. One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width. As we can bound Q2\u03c1\u03be(H;Z) and Wn(H;Z), we come to the conclusion on RE condition. Let X \u2208 Rn\u00d7p be a random matrix where each row is an independent copy of the sub-Gaussian random vector Z \u2208 Rp, and where Z has sub-Gaussian norm |||Z|||\u03c82 \u2264 \u03c3x [29]. Let \u03b1 = infu\u2208Sp\u22121 E[|\u3008Z, u\u3009|] so that \u03b1 > 0 [20, 28]. We have the following lower bound of the RE condition. The proof of Theorem 4 is based on the proof of [28, Theorem 6.3], and we give it in appendix C.2.\nTheorem 4 (Restricted Eigenvalue Condition) LetX be the sub-Gaussian design matrix that satisfies the assumptions above. If the SC condition (9) holds with a \u03c1 > 0, then with probability at least 1\u2212exp(\u2212t2/2), we have\ninf u\u2208H \u2016Xu\u20162 \u2265 c1\u03c1\n\u221a n\u2212 c2w(H)\u2212 c3\u03c1t (23)\nwhere c1, c2 and c3 are positive constants determined by \u03c3x, \u03c3\u03c9 and \u03b1.\nTo get a \u03ba > 0 in (6), one can simply choose t = (c1\u03c1 \u221a n \u2212 c2w(H))/2c3\u03c1. Then as long as n > c4w 2(H)/\u03c12 for c4 = c22/c21, we have\n\u03ba = inf u\u2208H 1\u221a n \u2016Xu\u20162 \u2265 1 2\n( c1\u03c1\u2212 c2\nw(H)\u221a n\n) > 0,\nwith high probability.\nFrom the discussion above, if SC condition holds and the sample size n is large enough, then we can find a matrix X such that RE condition holds. On the other hand, once there is a matrix X such that RE condition holds, then we can show that SC must also be true. Its proof is give in Appendix C.3.\nProposition 5 If X is a matrix such that the RE condition (6) holds for \u2206i \u2208 Ci, then the SC condition (9) holds.\nProposition 5 demonstrates that SC condition is a necessary condition for the possibility of RE. If SC condition does not hold, then there is {\u2206i} such that \u2206i 6= 0 for some i = 1, . . . , k, but \u2016 \u2211k i=1 \u2206i\u20162 = 0\nwhich implies \u2211k i=1 \u2206i = 0. Then for every matrix X , we have X \u2211k\ni=1 \u2206i = 0, and RE condition is not possible."}, {"heading": "5 General Error Bound", "text": "Recall that the error bound in Theorem 1 is given in terms of the noise-design (ND) interaction\nsn(\u03b3) = inf s>0\n{ s : sup\nu\u2208sC\n1\u221a n \u03c9TXu \u2264 \u03b3s2 \u221a n\n} . (24)\nIn this section, we give a characterization of the ND interaction, which yields the final bound on the componentwise error as long as n \u2265 n0, i.e., the sample complexity is satisfied. Let \u03c9 be a centered sub-Gaussian random vector, and its sub-Gaussian norm |||\u03c9|||\u03c82 \u2264 \u03c3\u03c9. Let X be a row-wise i.i.d. sub-Gaussian random matrix, for each row Z, its sub-Gaussian norm |||Z|||\u03c82 \u2264 \u03c3x. The ND interaction can be bounded by the following conclusion, and the proof of lemma 3 is given in Appendix D.1.\nLemma 3 Let design X \u2208 Rn\u00d7p be a row-wise i.i.d. sub-Gaussian random matrix, and noise \u03c9 \u2208 Rn be a centered sub-Gaussian random vector. Then sn(\u03b3) \u2264 cw(H\u0304)\u03b3\u221an . for some constant c > 0 with probability at least 1\u2212 c1 exp(\u2212c2w2(H\u0304))\u2212 c3 exp(\u2212c4n). Constant c depends on \u03c3x and \u03c3\u03c9.\nIn lemma 3 and theorem 6, we need the Gaussian width of H\u0304 and H respectively. From definition, both H\u0304 and H is related to the union of different cones; therefore bounding the width of H\u0304 and H may be difficult. We have the following bound of w(H) and w(H\u0304) in terms of the width of the component spherical caps. The proof of Lemma 4 is given in Appendix D.2.\nLemma 4 (Gaussian width bound) Let H and H\u0304 be as defined in (7) and (8) respectively. Then, we have w(H) = O ( maxiw(Ci \u2229 Sp\u22121) + \u221a log k ) and w(H\u0304) = O ( maxiw(Ci \u2229Bp) + \u221a log k ) .\nBy applying lemma 4, we can derive the error bound using the Gaussian width of individual error cone. From our conclusion on deterministic bound in theorem 1, we can choose an appropriate \u03b3 such that \u03ba2 > \u03b3. Then, by combining the result of theorem 1, theorem 4, lemma 3 and lemma 4, we have the final form of the bound, as originally discussed in (3):\nTheorem 6 For estimator (3), let Ci = cone{\u2206 : Ri(\u03b8\u2217i + \u2206) \u2264 Ri(\u03b8\u2217i )}, design X be a random matrix with each row an independent copy of sub-Gaussian random vector Z, noise \u03c9 be a centered sub-Gaussian random vector, and Bp \u2286 Rp be the centered unit euclidean ball. Suppose SC condition holds with\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u2206i \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03c1 k\u2211 i=1 \u2016\u2206i\u20162 .\nAlgorithm 1 Accelerated Proximal Algorithm Inputs: X , y. Initialize: {\u03b80i }ki=1 = {0}, \u03b10 = 0, \u03b7 > 0, 0 < \u03b2 < 1. for t = 0, 1, . . . , T do\nSet \u03b7t+1 = \u03b7. while true do\nfor i = 1, . . . , k do \u03b8\u0303t+1i = \u03a0\u2126i(\u03b8 t i \u2212 \u03b7t+1\u2207f\u03b8i(\u03b8t)) end for if f(\u03b8\u0303t+1) \u2264 f(\u03b8t) +\u2207T f(\u03b8t)(\u03b8\u0303t+1 \u2212 \u03b8t) + 1 2\u03b7t+1 ( \u2211k i=1 \u2016\u03b8\u0303 t+1 i \u2212 \u03b8ti\u201622) then\nbreak end if \u03b7t+1 = \u03b2\u03b7t+1\nend while \u03b1t+1 = 1+ \u221a 1+4\u03b12t 2 , \u03b8 t+1 i = \u03b8\u0303 t+1 i + \u03b1t\u22121 \u03b1t+1\n(\u03b8\u0303t+1i \u2212 \u03b8ti) end for\nfor any \u2206i \u2208 Ci ans a constant \u03c1 > 0. If sample size n > c(maxiw2(Ci \u2229 Sp\u22121) + log k)/\u03c12, then with high probability,\nk\u2211 i=1 \u2016\u03b8\u0302i \u2212 \u03b8\u2217i \u20162 \u2264 C maxiw(Ci \u2229Bp) + \u221a log k \u03c12 \u221a n , (25)\nfor constants c, C > 0 that depend on sub-Gaussian norms |||Z|||\u03c62 and |||\u03c9|||\u03c62 .\nThus, assuming the SC condition in (9) is satisfied, the sample complexity and error bound of the estimator depends on the largest Gaussian width, rather than the sum of Gaussian widths. The result can be viewed as a direct generalization of existing results for k = 1, when the SC condition is always satisfied, and the sample complexity and error is given by w2(C1 \u2229 Sp\u22121) and w(C1 \u2229Bp) [3, 10]."}, {"heading": "6 Accelerated Proximal Algorithm", "text": "In this section, we propose a general purpose algorithm for solving problem (2). For convenience, with \u03b8 = \u2211k i=1 \u03b8i, we set f(\u03b8) = f( \u2211k i=1 \u03b8i) = \u2016y \u2212 X\u03b8\u201622 and \u2126i = {\u03b8i|Ri(\u03b8i) \u2264 Ri(\u03b8\u2217i )}. While the norms Ri(.) may be non-smooth, one can design a general algorithm as long as the proximal operators \u03a0\u2126i(v) = argminu\u2208\u2126i \u2016u \u2212 v\u2016 2 2 for each set \u2126i can be efficiently computed. The algorithm is simply the proximal gradient method [23], where each component \u03b8i is cyclically updated in each iteration (see Algorithm 1):\n\u03b8\u0303t+1i = argmin \u03b8i\u2208\u2126i \u3008\u2207\u03b8if(\u03b8 t), \u03b8i \u2212 \u03b8ti\u3009+\n1\n2\u03b7t+1 \u2016\u03b8i \u2212 \u03b8ti\u201622 ,\n= argmin \u03b8i\u2208\u2126i\n\u2016\u03b8i \u2212 (\u03b8ti \u2212 \u03b7t+1\u2207\u03b8if(\u03b8 t))\u201622 ,\n(26)\nwhere \u03b7t+1 is the learning rate. To determine a proper \u03b7t+1, we use a backtracking step [4]. Starting from a constant \u03b7t+1 = \u03b7, in each step we first update \u03b8\u0303t+1i ; then we decide whether \u03b8\u0303 t+1 i satisfies condition:\nf(\u03b8\u0303t+1) \u2264 f(\u03b8t) +\u2207T f(\u03b8t)(\u03b8\u0303t+1 \u2212 \u03b8t) + 1 2\u03b7t+1 ( k\u2211 i=1 \u2016\u03b8\u0303t+1i \u2212 \u03b8 t i\u201622). (27)\nIf the condition (27) does not hold, then we decrease \u03b7t+1 till (27) is satisfied. Based on existing results [4], the basic method can be accelerated by setting the starting point of the next iteration \u03b8t+1i as a proper combination of \u03b8\u0303t+1i and \u03b8 t i . By [4], one can use the updates:\n\u03b8t+1i = \u03b8\u0303 t+1 i + \u03b1t \u2212 1 \u03b1t+1 (\u03b8\u0303t+1i \u2212 \u03b8 t i) , where \u03b1t+1 =\n1 + \u221a\n1 + 4\u03b12t 2 . (28)\nConvergence of Algorithm 1 has been studied in [4]. The backtracking step ensures that the convergence of algorithm 1. The work [4] also give the convergence rate of Algorithm 1, which is O(1/t2). Therefore, we can always reach a stationary point of problem (2) using Algorithm 1."}, {"heading": "7 Noiseless Case: Comparing Estimators", "text": "In this section, we present a comparative analysis of estimator\nmin {\u03b8i} k\u2211 i=1 \u03bbiRi(\u03b8i) s.t. X k\u2211 i=1 \u03b8i = y (29)\nwith the proposed estimator (2) in the noiseless case, i.e., \u03c9 = 0. In essence, we show that the two estimators have similar recovery conditions, but the existing estimator (29) needs additional structure for unique decomposition of \u03b8 into the components {\u03b8\u0302i}. The estimator (29) needs to consider the so-called \u201cinfimal convolution\u201d [25, 32] over different norms to get a (unique) decomposition of \u03b8 in terms of the components {\u03b8\u0302i}. Denote\nR(\u03b8) = min {\u03b8i}: \u2211 i \u03b8i=\u03b8 k\u2211 i=1 \u03bbiRi(\u03b8i) . (30)\nResults in [25] show that (30) is also a norm. Thus estimator (29) can be rewritten as\nmin \u03b8 R(\u03b8) s.t. X\u03b8 = y. (31)\nInterestingly, the above discussion separates the estimation problem in (29) into two parts\u2014solving (31) to get \u03b8\u0302, and then solving (30) to get the components {\u03b8\u0302i}. The problem (31) is a simple structured recovery problem, and is well studied [10, 28]. Using infimal convolution based decomposition problem (30) to get the components {\u03b8\u0302i} will be our focus in the sequel. To get some properties of decomposition (30), we consider the unit norm balls for normR(.) and component\nnorms Ri(.):\n\u2126R = {\u03b8 \u2208 Rp : R(\u03b8) \u2264 1} and \u2126iR = {\u03b8i \u2208 Rp : Ri(\u03b8i) \u2264 1} , i = 1, . . . , k .\nThe norm balls are related by the following result, we give the proof in appendix F.1. Lemma 5 For a given set {\u03bbi}, the infimal convolution norm ball \u2126R is the convex hull of \u22c3k i=1 1 \u03bbi \u2126iR, i.e.,\n\u2126R = conv( \u22c3k i=1 1 \u03bbi \u2126iR).\nLemma 5 illustrates what the decomposition (30) should be like. If \u03b8 is a point on the surface of the norm ball \u2126R, then the value of R(\u03b8) is the convex combination of some \u03b8i on the surface of 1\u03bbi\u2126 i R such that Ri(\u03b8i) = R(\u03b8). Hence if \u03b8 can be successfully decomposed into different components along the direction of \u03b8i, then we should be able to connect \u03b8i and \u03b8 by a surface on the R(.) norm ball, or they have to be \u201cclose\u201d. Interestingly, the above intuition of \u201ccloseness\u201d between different components \u03b8i can be described in the language of cones, in a way similar to the structural coherence property discussed in Section 3.\nGiven the intuition above, we state the main result in this section below. Its proof is given in appendix F.2.\nTheorem 7 Given \u03b8\u03021, . . . , \u03b8\u0302k and define\nC0 = \u2211 \u03b8i 6=0 ( c\u2032i ci \u2212 1 ) \u03b8i | c\u2032i \u2265 0, k\u2211 i=1 c\u2032i = 1  . (32) Suppose dim(span{\u03b8i}) = k, then there exist \u03bb1, . . . , \u03bbk such that \u2211k i=1 \u03b8\u0302i = \u03b8 are unique solutions of\n(30) if and only if there are c1, . . . , ck with ci \u2265 0 and \u2211k\ni=1 ci = 1 such that for the corresponding error cone Ci of \u03b8\u0302i and C0 defined above, \u2212Ci \u2229 \u2211 j 6=i Cj = {0}, for i = 0, 1, . . . , k.\nTheorem 7 illustrate that the successful decomposition of (30) requires an additional condition, i.e., \u2212C0 \u2229\u2211k i=1 Ci = {0} beyond that is needed by the SC condition (see Section 3). The additional condition needs us to choose parameters {\u03bbi} properly. Theorem 7 shows that {\u03bbi} depends on both {\u03b8\u2217i } and {ci}. For appropriate {\u03b8\u2217i }, there may be a range of {ci} such that the solution is unique. Therefore, in noiseless situation, if we know {Ri(\u03b8\u2217i )}, then solving estimator (29) would be a better idea, because it requires less condition to recover the true value and we do not need to choose parameters {\u03bbi}."}, {"heading": "8 Related Work", "text": "Structured superposition models have been studied in recent literatures. Early work focus on the case when k=2 and noise \u03c9 = 0, and assume specific structures such as sparse+sparse [14], and low-rank+sparse [11]. [16] analyze error bound for low-rank and sparse matrix decomposition with noise. Recent work have considered more generalized models and structures. [1] analyze the decomposition of a low-rank matrix plus another matrix with generalized structure. [15] propose an estimator for the decomposition of two generalize structured matrices, while one of them has a random rotation. Because of the increase in practical application and non-trivial of such problem, people have begun to work on unified frameworks for superposition model. In [31], the authors generalize the noiseless matrix decomposition problem to arbitrary number of superposition under random orthogonal measurement. [32] consider the superposition of structures of structures captured by decomposable norm, while [18] consider general norms but with a different measurement model, involving componentwise random rotations. These two papers are similar in spirit to our work, so we briefly discuss and differentiate our work from these papers.\n[32] consider a general framework for superposition model, and give a high-probability bound for the fol-\nlowing estimation problem:\nmin \u03b8i,i=1,...,k \u2225\u2225\u2225\u2225\u2225y \u2212X k\u2211 i=1 \u03b8i \u2225\u2225\u2225\u2225\u2225 2\n2\n+ k\u2211 i=1 \u03bbiRi(\u03b8i) (33)\nthey assume each Ri() to be a special kind of norm called decomposable norm. the authors used a different approach for RE condition. They decompose \u2016X \u2211k i=1 \u2206i\u20162 into two parts. One is\n1 n\u2016X\u2206i\u2016 2 2 \u2265 \u03ba\u2016\u2206i\u201622, (34)\nwhich characterizes the restricted eigenvalue of each error cone. The other is\n2 n | \u2211 i<j\u3008X\u2206i, X\u2206j\u3009| \u2264 \u03ba 2 \u2211k i=1 \u2016\u2206i\u201622, (35)\nwhich characterizes the interaction between different error cones. (35) is a strong assumption, and RE condition can hold without it. If \u2206i and \u2206j are positively correlated, then large interaction terms will make our RE condition stronger. Therefore their results are restricted.\n[19] consider an estimator like (2), which is\nmin \u03b8i,i=1,...,k \u2225\u2225\u2225\u2225\u2225y \u2212X k\u2211 i=1 Qi\u03b8i \u2225\u2225\u2225\u2225\u2225 2\n2\ns.t. Ri(\u03b8i) \u2264 Ri(\u03b8\u2217i ), i = 1, . . . , k, (36)\nwhereQi are known random rotations. Problem (36) is then transformed into a geometric problem: whether k random cones intersect. The componentwise random rotation can ensure that any kind of combination can be recovered with high probability. However, in practical problems, we need not have such random rotations available as part of the measurements. Further, their analysis is primarily focused on the noiseless case."}, {"heading": "9 Application of General Bound", "text": "In this section, we instantiate the general error bounds on Morphological Component Analysis (MCA), and low-rank and sparse matrix decomposition. The proofs are provided in appendix E."}, {"heading": "9.1 Morphological Component Analysis Using l1 Norm", "text": "In Morphological Component Analysis [14], we consider the following linear model\ny = X(\u03b8\u22171 + \u03b8 \u2217 2) + \u03c9,\nwhere vector \u03b81 is sparse and vector \u03b82 is sparse under a rotation Q. In [14], the authors introduced a quantity\nM = max i,j |Qij |. (37)\nFor small enough M , if the sum of their sparsity is lower than a constant related to M , we can recovery them. We show that for two given sparse vectors, our SC condition is more general.\nConsider the following estimator\nmin \u03b81,\u03b82 \u2016y \u2212X(\u03b81 + \u03b82)\u201622 s.t. \u2016\u03b81\u20161 \u2264 \u2016\u03b8\u22171\u20161, \u2016Q\u03b82\u20161 \u2264 \u2016Q\u03b8\u22172\u20161, (38)\nwhere vector y \u2208 Rn is the observation, vectors \u03b81, \u03b82 \u2208 Rp are the parameters we want to estimate, matrix X \u2208 Rn\u00d7p is a sub-Gaussian random design, matrix Q \u2208 Rp\u00d7p is orthogonal. We assume \u03b81 and Q\u03b82 are s1-sparse and s2-sparse vectors respectively. Function \u2016Q.\u20161 is still a norm. Suppose s1 = 1, s2 = 1, and the i-th entry of \u03b81 and the j-th entry of Q\u03b82 are non-zero. If\nQijsign(\u03b8\u22171)isign(Q\u03b8 \u2217 2) > 0,\nthen we have\n\u03c1 \u2265 \u221a (1\u2212 \u221a\n1\u2212Q2ij)/2. (39)\nThus we will have chance to separate \u03b81 and \u03b82 successfully. It is easy to see that M is lower bounded by \u03b8T1 Q\u03b82. Large \u03b8 T 1 Q\u03b82 leads to larger M , but also leads to larger \u03c1, which is better for separating \u03b81 and \u03b82. The proof of above bound of \u03c1 is given in Appendix E.1.\nIn general, it is difficult for us to derive a lower bound of \u03c1 like 39. Instead, we can derive the following sufficient condition in terms of M :\nTheorem 8 If M \u2264 18\u221as1s2 , then for problem (38) with high probability\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log p\nn ,\n\u221a s2 log p\nn\n}) .\nWhen s1 = s2 = 1, this condition M \u2264 18\u221as1s2 is much stronger than (39), because every entry of Q has to be smaller than 1/8;\n9.2 Morphological Component Analysis Using k-support Norm\nk-support norm [2] is another way to induce sparse solution instead of l1 norm. Recent works [2, 12] have shown that k-support norm has better statistical guarantee than l1 norm. For arbitrary \u03b8 \u2208 Rp, its k-support norm \u2016\u03b8\u2016spk is defined as\n\u2016\u03b8\u2016spk = inf \u2211 I\u2208Gk \u2016uI\u20162 : supp(uI) \u2286 I, \u2211 I\u2208Gk uI = \u03b8  . For the superposition of an s1 sparse vector and an s2 sparse vector, the best choice is to use s1-support\nnorm and s2-support norm. The new problem is\nmin \u03b81,\u03b82 \u2016y \u2212X(\u03b81 + \u03b82)\u201622 s.t. \u2016\u03b81\u2016sps1 \u2264 \u2016\u03b8 \u2217 1\u2016sps1 , \u2016Q\u03b82\u2016 sp s2 \u2264 \u2016Q\u03b8 \u2217 2\u2016sps2 . (40)\nDenote \u03c3s1,s2(Q) as the set of all the largest singular values ofQ\u2019s s1\u00d7s2 submatrices. Let \u03c3 = max\u03c3s1,s2(Q). In this case, we have the following sufficient condition and high probability error bound:\nTheorem 9 If \u03c3 \u2264 1 4(1+\n\u03b81max \u03b81min )(1+ \u03b82max \u03b82min\n) where \u03b8max = maxi\u2208supp(\u03b8) |\u03b8i| and \u03b8min = mini\u2208supp(\u03b8) |\u03b8i|,\nthen we have for problem (40) with high probability\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log(p\u2212 s1)\nn ,\n\u221a s2 log(p\u2212 s2)\nn\n}) .\nIn the problem setting of theorem 9, both norms are not decomposable. Therefore we can not apply the framework of [32] for this problem."}, {"heading": "9.3 Low-rank and Sparse Matrix Decomposition", "text": "To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank. These two kinds of norm ensure that the sparsity and the rank of the estimated matrices are small. If we have k > 1, the framework in [32] is not applicable, because k-support norm is not decomposable. When k = 1, the component norms simplify to \u2016 \u00b7 \u20161 for sparsity. Suppose we have a rank-r matrix L\u2217 and a sparse matrix S\u2217 with s nonzero entries, S\u2217, L\u2217 \u2208 Rd1\u00d7d2 . Our observation Y comes from the following problem\nYi = \u3008Xi, L\u2217 + S\u2217\u3009+ Ei, i = 1, . . . , n,\nwhere each Xi \u2208 Rd1\u00d7d2 is a sub-Gaussian random design matrix. Ei is the noise matrix. We want to recover S\u2217 and L\u2217 using s-support norm and nuclear norm respectively, so that the estimator takes the form:\nmin L,S n\u2211 i=1 (Yi \u2212 \u3008Xi, L+ S\u3009)2 s.t. |||L|||\u2217 \u2264 |||L \u2217|||\u2217, \u2016S\u2016 sp s \u2264 \u2016S\u2217\u2016sps . (41)\nBy using Theorem 6, and existing results on Gaussian widths, the error bound is given by\nTheorem 10 If there is a \u03c1 > 0 for problem (41), then with high probability\n\u2016L\u2212 L\u2217\u20162 + \u2016S \u2212 S\u2217\u20162 = O ( max {\u221a s log(d1d2 \u2212 s)\nn ,\n\u221a r(d1 + d2 \u2212 r)\nn\n}) .\nTheorem 10 requires SC condition to hold. When will SC condition for (41) holds? Early work have shown that to successfully estimate both L and S, the low-rank matrix L should satisfy \u201cincoherence\u201d condition\n[8]. From example in Appendix E.5, we can recovery matrix L and S even incoherence condition does not hold."}, {"heading": "10 Experimental Results", "text": "In this section, we confirm the theoretical results in this paper with some simple experiments. We show our experimental results under different settings. In our experiments we focus on MCA when k = 2. The design matrixX are generated from Gaussian distribution such that every entry ofX subjects toN (0, 1). The noise \u03c9 is generated from Gaussian distribution such that every entry of \u03c9 subjects toN (0, 1). We implement our algorithm 1 in MATLAB. We use synthetic data in all our experiments, and let the true signal\n\u03b81 = (1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 s1 , 0 . . . , 0), Q\u03b82 = (1, . . . , 1\ufe38 \ufe37\ufe37 \ufe38 s2 , 0 . . . , 0)\nWe generate our data in different ways for our three experiments."}, {"heading": "10.1 Recovery From Noisy Observation", "text": "In our first experiment, we test the impact of \u03c1 on the estimation error. We choose three different matrices Q, and \u03c1 is determined by the choice of Q. The first Q is given by random sampling: we sample a random\northogonal matrix Q such that Qij > 0, and \u03c1 is lower bounded by (39). The second and third Q is given by identity matrix I and its negative \u2212I; therefore \u03c1 = 1/ \u221a 2 and \u03c1 = 0 respectively. We choose dimension p = 1000, and let s1 = s2 = 1. The number of samples n varied between 1 and 1000. Observation y is given by y = X(\u03b8\u22171 + \u03b8 \u2217 2) + \u03c9. In this experiment, given Q, for each n, we generate 100 pairs of X and w. For each (X,w) pair, we get a solution \u03b8\u03021 and \u03b8\u03022. We take the average over all \u2016\u03b8\u03021 \u2212 \u03b8\u22171\u20162 + \u2016\u03b8\u03022 \u2212 \u03b8\u22172\u20162. Figure 4 shows the plot of number of samples vs the average error. From figure 4, we can see that the error curve given by random Q lies between curves given by two extreme cases, and larger \u03c1 gives lower curve."}, {"heading": "10.2 Recovery From Noiseless Observation", "text": "In our second experiment, we test how the dimension p affects the successful recovery of true value. In this experiment, we choose different dimension p with p = 20, p = 40, p = 80, and p = 160. We let s1 = s2 = 1. To avoid the impact of \u03c1, for each sample size n, we sample 100 random orthogonal matrices Q. Observation y is given by y = X(\u03b8\u22171 + \u03b8 \u2217 2). For each solution \u03b8\u03021 and \u03b8\u03022 of (38), we calculate the proportion of Q such that \u2016\u03b8\u03021 \u2212 \u03b8\u22171\u20162 + \u2016\u03b8\u03022 \u2212 \u03b8\u22172\u20162 \u2264 10\u22124. We increase n from 1 to 40, and the plot we get is figure 5. From figure 5 we can find that the sample complexity required to recover \u03b8\u22171 and \u03b8 \u2217 2 increases with dimension p.\n10.3 Recovery using k-support norm\nIn our last experiment, we test the impact of sparsity on the estimation error. In this experiment we solve problem (40), and let both s1 and s2 vary from 2 to 3. We set the matrix Q to be a p \u00d7 p discrete cosine transformation (DCT) matrix [14]. We use different problem size with p = 100 and p = 150. Number"}, {"heading": "11 Conclusions", "text": "We present a simple estimator for general superposition models and give a purely geometric characterization, based on structural coherence, of when accurate estimation of each component is possible. Further, we establish sample complexity of the estimator and upper bounds on componentwise estimation error and show that both, interestingly, depend on the largest Gaussian width among the spherical caps induced by the error cones corresponding to the component norms. Going forward, it will be interesting to investigate specific component structures which satisfy structural coherence, and also extend our results to allow more general measurement models.\nAcknowledgements: The research was also supported by NSF grants IIS-1563950, IIS-1447566, IIS1447574, IIS-1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo."}, {"heading": "Appendix", "text": ""}, {"heading": "A Proof of Theorem 1", "text": "Theorem 11 (Deterministic bound) Assume that the RE condition in (6) is satisfied in C with parameter \u03ba. Then, if \u03ba2 > \u03b3, we have \u2211k i=1 \u2016\u2206i\u20162 6 2sn(\u03b3).\nProof: By feasibility of \u03b8\u2217 and optimality of \u03b8\u0302, we have\n\u2016Y \u2212X\u03b8\u0302\u201622 \u2264 \u2016Y \u2212X\u03b8\u2217\u201622 .\nIf \u03b8\u0302 = \u2211\ni \u03b8\u0302i is an optimum of (2), we have\n\u2016Y \u2212X\u03b8\u0302\u201622 = \u2016X(\u03b8\u0302 \u2212 \u03b8\u2217)\u201622 \u2212 2\u03c9TX(\u03b8\u0302 \u2212 \u03b8\u2217) + \u2016\u03c9\u201622.\nWith \u2206 = \u03b8\u0302 \u2212 \u03b8\u2217, \u2206i = \u03b8\u0302i \u2212 \u03b8\u2217i , we have\n\u2016Y \u2212X\u03b8\u0302\u201622 \u2212 \u2016Y \u2212X\u03b8\u2217\u201622 = \u2016X\u2206\u201622 \u2212 2\u03c9TX\u2206 6 0. (42)\nFor any \u2206 = \u2211k i=1 \u2206i,\u2206i \u2208 Ci, for the sake of contraction let \u2211k i=1 \u2016\u2206i\u20162 \u2265 2sn(\u03b3). Then we have\n1 n(\u2016X\u2206\u2016 2 2 \u2212 2\u03c9TX\u2206) \u2265 (\u2211k i=1 \u2016\u2206i\u20162 )2 (\u03ba2 \u2212 \u03b3) > 0. (43)\nsince \u03ba2 > \u03b3. However, the inequality contradicts (42). Therefore \u2211k\ni=1 \u2016\u2206i\u20162 \u2264 2sn(\u03b3)."}, {"heading": "B Geometry of Structural Coherence", "text": "In this section, our goal is to characterize the geometric property of our SC condition. We start from a simple case when k = 2."}, {"heading": "B.1 Proof of Lemma 2", "text": "Lemma 6 If there exists a \u03b4 < 1 such that \u2212\u3008x, y\u3009 \u2264 \u03b4\u2016x\u20162\u2016y\u20162, then\n\u2016x+ y\u20162 \u2265 \u221a\n1\u2212 \u03b4 2 (\u2016x\u20162 + \u2016y\u20162) . (44)\nProof: We know from [18] that\n\u2016x+ y\u201622 \u2265 (1\u2212 \u03b4)(\u2016x\u201622 + \u2016y\u201622)\nand (\u2016x\u20162 + \u2016y\u20162)2 \u2264 2(\u2016x\u201622 + \u2016y\u201622)\nCombine them and we will get the conclusion."}, {"heading": "B.2 Proof of Theorem 3", "text": "Theorem 12 (Structural Coherence (SC) Condition) Let \u03b4 := maxi \u03b4i with \u03b4i as defined in (14). If \u03b4 < 1, there exists a \u03c1 > 0 such that for any \u2206i \u2208 Ci, i = 1, . . . , k, the SC condition in (9) holds, i.e.,\u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u2206i \u2225\u2225\u2225\u2225\u2225 2 \u2265 \u03c1 k\u2211 i=1 \u2016\u2206i\u20162 . (45)\nProof: We have by lemma (2)\n\u2016 \u2211 i \u2206i\u20162 \u2265 \u221a 1\u2212\u03b4 2 ( \u2016\u2206i\u2032\u20162 + \u2225\u2225\u2225\u2211j 6=i\u2032 \u2206k\u2225\u2225\u2225 2 ) .\nSum over all possible combinations, we get\nk \u2016 \u2211 i \u2206i\u20162 \u2265 \u221a 1\u2212\u03b4 2 \u2211 i\u2032 ( \u2016\u2206i\u2032\u20162 + \u2225\u2225\u2225\u2211j 6=i\u2032 \u2206k\u2225\u2225\u2225 2 ) \u2265 \u221a 1\u2212\u03b4 2 \u2211 i\u2032 \u2016\u2206i\u2032\u20162.\nTherefore \u2225\u2225\u2225\u2211ki=1 \u2206i\u2225\u2225\u2225 2 \u2265 1k \u221a 1\u2212\u03b4 2 \u2211k i=1 \u2016\u2206i\u20162."}, {"heading": "C Restricted Eigenvalue Condition", "text": ""}, {"heading": "C.1 Proof of Lemma 2", "text": "Lemma 7 Let sets C and A be as defined in (7) and (19) respectively. If the SC condition in (9) holds, then the marginal tail functions of the two sets have the following relationship:\nQ\u03c1\u03be(H;Z) \u2265 Q\u03be(A;Z). (46)\nProof: By definition, for any u \u2208 H, we can find ui \u2208 Ci, i = 1, 2, . . . , k and \u2211k i=1 ui = u. Then\n|\u3008Z, u\u3009| = \u2225\u2225\u2225\u2211ki=1 ui\u2225\u2225\u2225\n2 \u2223\u2223\u2223\u2329Z, u\u2016\u2211ki=1 ui\u20162\u232a \u2223\u2223\u2223 \u2265 \u03c1 \u2223\u2223\u2223\u2329Z, u\u2016\u2211ki=1 ui\u20162\u232a \u2223\u2223\u2223 . Let v = u\n\u2016 \u2211k i=1 ui\u20162 , from definition we know that v \u2208 A. Hence we also have\nP (|\u3008Z, u\u3009| \u2265 \u03c1\u03be) = P (\n1 \u03c1 |\u3008Z, u\u3009| \u2265 \u03be\n) \u2265 P (|\u3008Z, v\u3009| \u2265 \u03be).\nTherefore taking the infimum over all v \u2208 A and then all u \u2208 C, the conclusion holds."}, {"heading": "C.2 Proof of Theorem 4", "text": "Theorem 13 (Restricted Eigenvalue Condition) Let X be the sub-Gaussian design matrix that satisfies the assumptions above. If the SC condition (9) holds with a \u03c1 > 0, then with probability at least 1 \u2212 exp(\u2212t2/2), we have\ninf u\u2208H \u2016Xu\u20162 \u2265 c1\u03c1\n\u221a n\u2212 c2w(H)\u2212 c3\u03c1t (47)\nwhere c1, c2 and c3 are positive constants determined by \u03c3x, \u03c3\u03c9 and \u03b1.\nProof: Let two sets C and A be as defined previously. From Lemma (1) and lemma (2) we know that for any \u03be > 0, with probability at least 1\u2212 e\u2212t2/2\ninf u\u2208H \u2016Xu\u20162 \u2265 \u03c1\u03be\n\u221a nQ2\u03be(A;Z)\u2212 2W (H;Z)\u2212 \u03c1\u03bet. (48)\nWe use the \u201dBowling scheme\u201d in [28], let v be any vector in A, by Paley-Zygmund inequality [6], one can get\nP (|\u3008x, v\u3009| \u2265 2\u03be) \u2265 [E|\u3008x, v\u3009| \u2212 2\u03be]2+ E|\u3008x, v\u3009|2 \u2265 (\u03b1\u2212 2\u03be) 2 4\u03c32x . (49)\nFrom the proof of [28, Theorem 6.3], empirical width can be bounded by\nW (H;Z) \u2264 L\u03c3xw(H) (50)\nSelect \u03be = \u03b1/6, combine (48), (49), (50) to discover that:\ninf u\u2208C \u2016Xu\u20162 \u2265\n1 9 \u03c1\u03b13\u03c3\u22122x \u221a n\u2212 2L\u03c3xw(H)\u2212 \u03c1 \u03b1 6 t\nwhich completes the proof.\nFrom the conclusion above, the right hand side contains three parts. The first part is a constant times the square root of sample size, and the second part is a measure of the complexity of error sets. Therefore, when the number of samples is large enough or the error set has low complexity, the right terms will be larger than zero."}, {"heading": "C.3 Proof of Proposition 5", "text": "Proposition 14 If there is a matrix X such that condition (6) holds for \u2206i \u2208 Ci, then SC (9) holds.\nProof: If such \u03c1 does not exist, then there are some \u2206i \u2208 Ci, i = 1, . . . , k not all zero such that\u2225\u2225\u2225\u2211ki=1 \u2206i\u2225\u2225\u2225 2 = 0\u21d2 \u2211k i=1 \u2206i = 0,\nwhich implies \u2016X \u2211k\ni=1 \u2206i\u20162 = 0 for every X . This is a contradiction."}, {"heading": "D Error Bound", "text": ""}, {"heading": "D.1 Proof of Lemma 3", "text": "Lemma 8 Let design X \u2208 Rn\u00d7p be a row-wise i.i.d. sub-Gaussian random matrix, and noise \u03c9 \u2208 Rn be a centered sub-Gaussian random vector. Then sn(\u03b3) \u2264 cw(H\u0304)\u03b3\u221an . for some constant c > 0 with probability at least 1\u2212 c1 exp(\u2212c2w2(H\u0304))\u2212 c3 exp(\u2212c4n). Constant c depends on \u03c3x and \u03c3\u03c9.\nProof: First notice that\n\u03c9TX\u2206 = \u2016\u03c9\u20162. \u03c9X\u2206\n\u2016\u03c9\u20162 .\nWe can first bound \u03c9X\u2206\u2016\u03c9\u20162 then bound \u2016\u03c9\u20162. (a). Bound \u03c9X\u2206\u2016\u03c9\u20162 : Note that \u03c9X\u2206 \u2016\u03c9\u20162 is not centered. In the first step we center it using\n1\n\u2016\u03c9\u20162 i\u03c9ix\nT i \u2206,\nwhere i is a Radmacher random variable, its probability of being +1 and \u22121 are both half. xi \u2208 Rp is the i-th row of X . By assumption we know different xi are independent and have same distribution.\nHere we fix \u03c9. By proposition 5.10 in [29], the following bound holds:\nP (\u2223\u2223\u2223\u2223\u03c9X\u2206\u2016\u03c9\u20162 \u2223\u2223\u2223\u2223 \u2265 t) \u2264 e. exp(\u2212 c1t2\u03c32x\u2016\u2206\u20162 ) . (51)\nThrough simple transform we know that\nP (\u2223\u2223\u2223\u2223\u2329 \u03c9X\u2016\u03c9\u20162 , u\u2212 v \u232a\u2223\u2223\u2223\u2223 \u2265 t) \u2264 e.e\u2212c1t2/(\u03c32x\u2016u\u2212v\u201622)\nfor any u, v \u2208 Rp. Then use [26, Theorem 2.2.27]\nP ( sup \u2206\u2208H\u0304 \u2223\u2223\u2223\u2223\u2223 1\u2016\u03c9\u20162 \u2211i i\u03c9ixTi \u2206 \u2223\u2223\u2223\u2223\u2223 \u2265 c2w(H\u0304) + 2c3t ) \u2264 c4 exp ( \u22124c1t 2 \u03c32x )\n(b). Bound \u03c9: We first notice that \u2016\u03c9\u201622 is a sub-exponential random variable. Therefore if the sub-gaussian norm of \u03c9 is \u2016\u03c9\u2016\u03c62 , then for each entry \u03c9i the sub-expoential norm \u2016\u03c92i \u2016\u03c61 \u2264 2\u2016\u03c9\u20162\u03c62 . Through definition we reach:\nE\u2016\u03c9\u20162 \u2264 \u221a E\u2016\u03c9\u201622 \u2264 \u03c3\u03c9 \u221a 2n.\nApplying proposition 5.16 in [29] to \u2016\u03c9\u201622, we obtain\nP (|\u2016\u03c9\u201622 \u2212 E\u2016\u03c9\u201622| \u2265 t) \u2264 2 exp [ \u2212c5 min ( t2\n4\u03c34\u03c9 , t 2\u03c32\u03c9\n)] .\nReplace t with \u03c32\u03c9n gives P (\u2016\u03c9\u20162 \u2265 2\u03c3x \u221a n) \u2264 2 exp(\u2212c5n).\nCombine (a) and (b): First we have\nP ( sup \u2206\u2208H\u0304 1\u221a n \u2223\u2223\u2223\u2223\u2223\u2211 i i\u03c9ix T i \u2206 \u2223\u2223\u2223\u2223\u2223 \u2264 2c2\u03c3xw(H\u0304) + 4c3\u03c3xt )\n\u2265P ( sup \u2206\u2208H\u0304 \u2223\u2223\u2223\u2223\u2223 1\u2016\u03c9\u20162 \u2211i i\u03c9ixTi \u2206 \u2223\u2223\u2223\u2223\u2223 \u2265 c2w(H\u0304) + 2c3t \u2227 \u2016\u03c9\u20162\u221an \u2265 2\u03c3x )\n\u2265P ( sup \u2206\u2208H\u0304 \u2223\u2223\u2223\u2223\u2223 1\u2016\u03c9\u20162 \u2211i i\u03c9ixTi \u2206 \u2223\u2223\u2223\u2223\u2223 \u2265 c2w(H\u0304) + 2c3t|\u03c9 ) P ( \u2016\u03c9\u20162\u221a n \u2265 2\u03c3x ) \u22651\u2212 2 exp(\u2212c5n)\u2212 c4 exp ( \u22124c1t 2\n\u03c32x ) Then choose t = c22c3w(H\u0304),\nP ( sup \u2206\u2208H\u0304 1\u221a n \u2223\u2223\u2223\u2223\u2223\u2211 i i\u03c9ix T i \u2206 \u2223\u2223\u2223\u2223\u2223 \u2264 4c2\u03c3\u03c9w(H\u0304) ) \u2265 1\u2212 c4. exp ( \u2212c1c 2 2w(H\u0304) c3\u03c32x ) \u2212 2 exp(\u2212c5n)\nNow we have bounded the symmetrized \u03c9TX\u2206, then \u03c9TX\u2206 can be bounded using symmetrization of probability [17]:\nP ( sup \u2206\u2208H\u0304 1\u221a n |\u03c9TX\u2206\u2212 E\u03c9TX\u2206| > 16c5\u03c3\u03c9w(H\u0304) )\n\u22644P ( sup \u2206\u2208H\u0304 1\u221a n \u2223\u2223\u2223\u2223\u2223\u2211 i i\u03c9ix T i \u2206 \u2223\u2223\u2223\u2223\u2223 > 4c5\u03c3\u03c9w(H\u0304) ) \u2264 4c4. exp ( \u2212c1c 2 2w(H\u0304) c3\u03c32x ) + 8 exp(\u2212c5n).\nBecause \u03c9 has zero mean, there the above inequality give us:\nsup \u2206\u2208H\u0304 |\u03c9TX\u2206|\u221a n \u2264 16c5\u03c3\u03c9w(H\u0304)\nwith high probability. Hence by definition\nsn(\u03b3) \u2264 16c5\u03c3\u03c9w(H\u0304)\n\u03b3 \u221a n\nwith probability at least 1\u2212 4c4. exp ( \u2212 c1c\n2 2w(H\u0304) c3\u03c32x\n) \u2212 8 exp(\u2212c5n).\nNow as we have the high probability bound of both \u03ba and sn(\u03b3), we can derive our error bound for random case."}, {"heading": "D.2 Proof of Lemma 4", "text": "Lemma 9 (Gaussian width bound) Let H and H\u0304 be as defined in (7) and (8) respectively. Then, we have w(H) = O ( maxiw(Ci \u2229 Sp\u22121) + \u221a log k ) and w(H\u0304) = O ( maxiw(Ci \u2229Bp) + \u221a log k ) .\nProof: By definition and the fact that the Gaussian width of convex hull of sets is equal to the Gaussian width of their union [10]\nw(H) = E sup u\u2208H \u3008u, g\u3009 = Emax i sup ui\u2208Ci\u2229Sp\u22121 \u3008ui, g\u3009\nBy concentration inequality for Lipschitz functions [17], for each i = 1, . . . , k\nP ( sup ui\u2208Ci\u2229Sp\u22121 \u3008ui, g\u3009 \u2265 E sup ui\u2208Ci\u2229Sp\u22121 \u3008ui, g\u3009+ r) \u2264 exp(\u2212r2/2).\nThen denote Di = supui\u2208Ci\u2229Sp\u22121\u3008ui, g\u3009, we have\nw(H) =Emax i Di\n\u2264max i EDi + \u03b4 + k\u2211 i=1 \u222b \u221e \u03b4 P ( Di \u2265 max i EDi + r ) dr\n\u2264max i EDi + \u03b4 + k \u222b \u221e \u03b4 exp(\u2212r2/2)dr \u2264max i EDi + \u03b4 + \u03b7 \u2032k exp(\u2212\u03b42/2).\nLet \u03b4 = \u221a log k, we get\nw(H) \u2264 max i EDi +\n\u221a log k + \u03b7 = O ( max i w(Ci \u2229 Sp\u22121) + \u221a log k ) .\nthe conclusion holds. The conclusion for w(H\u0304) can be proved the same as above."}, {"heading": "D.3 Proof of Theorem 6", "text": "Theorem 15 For estimator (3), let Ci = cone{\u2206 : Ri(\u03b8\u2217i + \u2206) \u2264 Ri(\u03b8\u2217i )}, design X be a random matrix with each row an independent copy of sub-Gaussian random vector Z, noise \u03c9 be a centered sub-Gaussian random vector, and Bp \u2286 Rp be the a centered unit euclidean ball. If sample size n > c(maxiw2(Ci \u2229 Sp\u22121) + log k)/\u03c1 2, then with high probability,\nk\u2211 i=1 \u2016\u03b8\u0302i \u2212 \u03b8\u2217i \u20162 \u2264 C maxiw(Ci \u2229Bp) + \u221a log k \u03c12 \u221a n , (52)\nfor constants c, C > 0 that depend on sub-Gaussian norms |||Z|||\u03c62 and |||\u03c9|||\u03c62 .\nProof: Firstly, we choose\nt = 1\n3 \u03b12\u03c3\u22122x\n\u221a n\u2212 6L\u03c3x\u03c1\u22121\u03b1\u22121w(H).\nFrom theorem 4, RE condition holds for\n\u03ba \u2265 1 2\n( 1\n9 \u03c1\u03b13\u03c3\u22122x \u2212 2L\u03c3x w(H)\u221a n ) with probability at least\n1\u2212 exp ( \u2212(1\n3 \u03b12\u03c3\u22122x\n\u221a n\u2212 6L\u03c3x\u03c1\u22121\u03b1\u22121w(H))2/2 ) .\nNext we choose \u03b3 = \u03c1 2\u03b16\n1296\u03c34x , and let sn(\u03b3) be defined as above. Thus from theorem (1) and our discussion\nabove, if 1\n4\n( 1\n9 \u03c1\u03b13\u03c3\u22122x \u2212 2L\u03c3x w(H)\u221a n\n)2 > \u03c12\u03b16\n1296\u03c34x \u21d2 n > c1w2(H),\nfor some constant c1 > 0, then \u03ba > 2\u03b3. Using theorem 1, we have\nk\u2211 i=1 \u2016\u2206i\u20162 \u2264 c2 w(H\u0304)\u221a n .\nwith probability at least\n1\u2212 c3 exp(\u2212c4w(H\u0304))\u2212 c5 exp(\u2212c6n)\u2212 exp(\u2212(c7 \u221a n\u2212 c8w(H))2),\nwhich completes the proof."}, {"heading": "E Examples", "text": ""}, {"heading": "E.1 Structural Coherence For 1-sparse + 1-sparse MCA", "text": "Proposition 16 Suppose both vector \u03b81 and vector Q\u03b82 are one sparse, and the i-th entry of \u03b81 and the j-th entry of Q\u03b82 are non-zero. If\nQijsign(\u03b8\u22171)isign(Q\u03b8 \u2217 2) > 0,\nthen we have\n\u03c1 \u2265 \u221a (1\u2212 \u221a\n1\u2212Q2ij)/2. (53)\nProof: Denote \u2206\u2212i as a vector whose ith entry is 0, and other entries are equal to those of \u2206. Suppose both \u03b81 and Q\u03b82 are 1-sparse vectors, and the ith entry of \u03b81 jth entry of Q\u03b82 are nonzero. Then error vector\n\u22061 and \u22062 satisfy the following inequalities:\n\u2212\u3008sign(\u03b81i),\u22061i\u3009 \u2265 \u2016\u22061\u2212i\u20161,\u2212\u3008sign(\u03b82j),\u22062j\u3009 \u2265 \u2016\u22062\u2212j\u20161.\nTherefore \u2212\u3008sign(\u03b81i),\u22061i\u3009 \u2016\u22061\u20162 \u2265 1\u221a\n1 + \u2016\u22061\u2212i\u201622\n\u220621i\n\u2265 1\u221a 1 +\n\u2016\u22061\u2212i\u201622 \u2016\u22061\u2212i\u201621\n\u2265 1\u221a 2 .\nThe same holds for Q\u22062. Then when Qijsign(\u03b81i)sign(Q\u03b82j) > 0 we have from geometry that\n\u2212\u3008\u22061,\u22062\u3009 \u2264 \u2212 cos(2 arccos( 1\u221a 2\n) + arccos(Qijsign(\u03b81i)sign(Q\u03b82j))) \u2264 \u221a 1\u2212Q2ij\nTherefore \u03c1 \u2265\n\u221a 1\u2212\n\u221a 1\u2212Q2ij 2 by proposition 2."}, {"heading": "E.2 Proof of Theorem 8", "text": "Theorem 17 If M \u2264 18\u221as1s2 , then for problem (38) with high probability\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log p\nn ,\n\u221a s2 log p\nn\n}) .\nProof: Let \u2206i = \u03b8i \u2212 \u03b8\u2217i for i = 1, 2, then we have\n\u3008\u22061,\u22062\u3009 = \u3008\u22061, QTQ\u22062\u3009 \u2264 max ij |Qij |\u2016\u22061\u20161\u2016Q\u22062\u20161\n\u2264M(\u2016P\u2126\u22061\u20161 + \u2016P\u2126c1\u22061\u20161)(\u2016P\u21262Q\u22062\u20161 + \u2016P\u2126c2Q\u22062\u20161) \u2264 4M\u2016P\u2126\u22061\u20161\u2016P\u21262Q\u22062\u20161 \u2264 4M \u221a s1s2\u2016\u22061\u20162\u2016Q\u22062\u20162\nLet 4M \u221a s1s2 \u2264 12 , we get M \u2264 1 8 \u221a s1s2 and \u03c1 \u2265 12 .\nFrom the result of [10], we know that the Gaussian width for the error cone of a s-sparse vector isO( \u221a s log(ps )). Therefore by theorem 6, if n \u2265 C maxs\u2208{s1,s2} s log( p s ) for some C > 0, then\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log p\nn ,\n\u221a s2 log p\nn\n}) ."}, {"heading": "E.3 Proof of Theorem 9", "text": "Theorem 18 If \u03c3 \u2264 1 4(1+\n\u03b81max \u03b81min )(1+ \u03b82max \u03b82min\n) where \u03b8max = maxi\u2208supp(\u03b8) |\u03b8i| and \u03b8min = mini\u2208supp(\u03b8) |\u03b8i|,\nthen we have for problem (40) with high probability\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log(p\u2212 s1)\nn ,\n\u221a s2 log(p\u2212 s2)\nn\n}) .\nProof: We first characterize the interaction between cones:\n\u3008\u22061,\u22062\u3009 = \u3008\u22061, QTQ\u22062\u3009.\nbecause k-support norm is an atomic norm, and its atomic set is all unit k-sparse vectors. Therefore we can decompose \u22061 into combination of unit s1-sparse vectors \u22061 = \u2211 i \u03b1iui and Q\u22062 into combination of unit\ns2-sparse vectors Q\u22062 = \u2211 j \u03b2jvj . Then\n\u3008\u22061, QTQ\u22062\u3009 \u2264 \u2211 i |\u03b1i| \u2211 j |\u03b2j |max ij |uTi QT vj | \u2264 \u03c3\u2016\u22061\u2016sps1\u2016\u22062\u2016 sp s2 .\nBy Theorem 9 in [13], we have\n\u2016\u22061\u2016sps1 \u2264 \u221a\n2(1 + \u03b81max \u03b81min\n)\u2016\u22061\u20162 and \u2016\u22062\u2016sps2 \u2264 \u221a\n2(1 + \u03b82max \u03b82min )\u2016\u22061\u20162.\nTherefore \u3008\u22061,\u22062\u3009 \u2264 2\u03c3(1 +\n\u03b81max \u03b81min )(1 + \u03b82max \u03b82min )\u2016\u22061\u20162\u2016\u22062\u20162.\nLet 2\u03c3(1 + \u03b81max\u03b81min )(1 + \u03b82max \u03b82min ) \u2264 12 , then \u03c3 \u2264 1\n4(1+ \u03b81max \u03b81min )(1+ \u03b82max \u03b82min\n) and \u03c1 \u2265 12 .\nFrom [13], when we set k to be the sparsity s, the corresponding Gaussian width of tangent cone is O(s log(p\u2212 s)), therefore plus this result in Theorem 6 we get\n\u2016\u03b81 \u2212 \u03b8\u22171\u20162 + \u2016\u03b82 \u2212 \u03b8\u22172\u20162 = O\n( max {\u221a s1 log(p\u2212 s1)\nn ,\n\u221a s2 log(p\u2212 s2)\nn\n}) ."}, {"heading": "E.4 Proof of Theorem 10", "text": "Theorem 19 If there is a \u03c1 > 0 for problem (41), then with high probability\n\u2016L\u2212 L\u2217\u20162 + \u2016S \u2212 S\u2217\u20162 = O ( max {\u221a s log(d1d2 \u2212 s)\nn ,\n\u221a r(d1 + d2 \u2212 r)\nn\n}) .\nProof: From [10], we know that for error cone of d1 \u00d7 d2 rank-r matrix, its Gaussian width is O(r(d1 + d2 \u2212 r)). Therefore if \u03c1 > 0, then by applying theorem 6, the error bound is\n\u2016L\u2212 L\u2217\u20162 + \u2016S \u2212 S\u2217\u20162 = O ( max {\u221a s log(d1d2 \u2212 s)\nn ,\n\u221a r(d1 + d2 \u2212 r)\nn\n}) ."}, {"heading": "E.5 Additional Example for Low-rank ans Sparse Matrix Decomposition", "text": "Example 1 Suppose noise \u03c9 = 0,\nS0 = L0 =  1 0 . . . 0 0 0 . . . 0 ... ... . . .\n... 0 0 . . . 0  , (54) and M = S0 + L0, then the SC condition of problem (41) holds.\nProof: Suppose the singular value decomposition of L\u22170 is U\u03a3V T . Denote\nC\u2032S = cl{\u2206| \u2212 \u3008sign(S\u20320),\u2206\u3009 \u2265 \u2016P\u2126c(\u2206)\u20161}, C\u2032L = cl{\u2206| \u2212 \u3008UV T ,\u2206\u3009 \u2265 \u2016PT\u22a5(\u2206)\u2016\u2217}.\nFrom [18] we know that SC is equivalent to \u2212C\u2032S \u22c2 C\u2032L = {0}. To prove \u2212C\u2032S \u22c2 C\u2032L = {0}, we need the following inequalities:\n\u3008sign(S\u20320),\u2206\u3009 \u2265 \u2016P\u2126c(\u2206)\u20161, \u2212 \u3008UV T ,\u2206\u3009 \u2265 \u2016PT\u22a5(\u2206)\u2016\u2217.\nhas unique solution 0. It is easy to notice that \u3008sign(S\u20320),\u2206\u3009 = \u3008UV T ,\u2206\u3009 = \u220611. As the value of norms is non-negative, we have \u220611 \u2265 0 and \u2212\u220611 \u2265 0. Therefore \u220611 = 0. Besides,\n\u2016P\u2126c(\u2206)\u20161 = \u2211 (i,j) 6=(1,1) |\u2206ij | \u2264 0,\nwhich leads to \u2206ij = 0 for (i, j) 6= (1, 1). Finally, \u2206 = 0 and the conclusion holds.\nPeople tend to think that we cannot obtain the correct decomposition in this situation. Note that the cone CS is centered at one point, and the cone CL contains CS but their surface contacts only at the origin. Therefore the reflection of one cone will touch the other cone only at the origin. As a result, for M = M0 = S0 + L0, i.e., SC condition holds."}, {"heading": "F Noiseless Case: Comparing Estimators", "text": "In this section we try to explore the structures that are different between problem (2) and problem (29) and the structures that they share."}, {"heading": "F.1 Proof of Lemma 5", "text": "Lemma 10 For a given set {\u03bbi}, the infimal convolution norm ball \u2126R is the convex hull of \u22c3k i=1 1 \u03bbi \u2126iR,\ni.e., \u2126R = conv( \u22c3k i=1 1 \u03bbi \u2126iR). Proof: If \u03b8 \u2208 \u2126R, then from definition there are \u2211k i=1 \u03b8i = \u03b8 such that R(\u03b8) = \u2211k\ni=1 \u03bbiRi(\u03b8i). Without loss of generalization, suppose \u03b8i 6= 0 for each i, then we have the following decomposition:\nk\u2211 i=1 \u03bbiRi(\u03b8i) R(\u03b8) R(\u03b8) \u03bbiRi(\u03b8i) \u03b8i = \u03b8. (55)\nIt is easy to know that \u2211k\ni=1 \u03bbiRi(\u03b8i) R(\u03b8) = 1 andRi( R(\u03b8) \u03bbiRi(\u03b8i) \u03b8i) = 1 \u03bbi R(\u03b8) \u2264 1\u03bbi . Therefore R(\u03b8) \u03bbiRi(\u03b8i) \u03b8i \u2208 1\u03bbi\u2126 i R, and \u03b8 \u2208 conv( \u22c3k i=1 1 \u03bbi \u2126iR).\nIf \u03b8 \u2208 conv( \u22c3k i=1 1 \u03bbi \u2126iR), then we can find \u03b8i \u2208 1 \u03bbi \u2126iR and ci > 0, \u2211k i=1 ci = 1 such that\nk\u2211 i=1 ci\u03b8i = \u03b8.\nThen\nR(\u03b8) \u2264 k\u2211 i=1 \u03bbiRi(ci\u03b8i) = k\u2211 i=1 ci\u03bbiRi(\u03b8i) \u2264 1.\nTherefore \u03b8 \u2208 \u2126R which completes the proof."}, {"heading": "F.2 Proof of theorem 7", "text": "Theorem 20 Given \u03b8\u03021, . . . , \u03b8\u0302k and define\nC0 = \u2211 \u03b8i 6=0 ( c\u2032i ci \u2212 1 ) \u03b8i | c\u2032i \u2265 0, k\u2211 i=1 c\u2032i = 1  . (56) Suppose dim(span{\u03b8i}) = k, then there exist \u03bb1, . . . , \u03bbk such that \u2211k i=1 \u03b8\u0302i = \u03b8 are unique solutions of\n(30) if and only if there are c1, . . . , ck with ci \u2265 0 and \u2211k\ni=1 ci = 1 such that for the corresponding error cone Ci of \u03b8\u0302i and C0 defined above, \u2212Ci \u2229 \u2211 j 6=i Cj = {0}, for i = 0, 1, . . . , k.\nProof: Before proofing the main result we need the following lemma, it is proved in appendix F.3.\nLemma 11 For fixed \u03bb1, . . . , \u03bbk, suppose \u2211k\ni=1 \u03b8i = \u03b8 is a solution of decomposition (30) under this set of {\u03bbi}, and dim(span{\u03b8i}) = k. Let Ci, i = 1, 2, . . . , k be the corresponding error cones of {\u03b8i}, ci = \u03bbiRi(\u03b8i)/R(\u03b8) and C0 be as defined in (56). The decomposition (30) for \u03b8 is unique if and only if for any i = 0, 1, . . . , k,\n\u2212 Ci \u2229 \u2211 j 6=i Cj = {0}. (57)\nWe come to the main result and the necessity is obvious from lemma 11;\nWithout loss of generality, suppose ci \u2265 0. If such c1, . . . , ck exist for \u03b8\u03021, . . . , \u03b8\u0302k, let \u03bbi = ciRi(\u03b8\u0302i) . Suppose \u03b81, . . . , \u03b8k is a set of optimal solution under the \u03bb defined above. From 55 we can write the decomposition as \u03b8 = \u2211k i=1 c \u2032 i\u03b8 \u2032 i where c \u2032 i is a coefficient of convex combination, c \u2032 i\u03b8 \u2032 i = \u03b8i and \u03bbiRi(\u03b8 \u2032 i) = R(\u03b8) under coefficient \u03bbi.\nIf \u03b8i 6= \u03b8\u0302i for some i, then as \u2211k i=1 \u03bbiRi(\u03b8\u0302i) = \u2211k i=1 ci = 1, we have\n\u03bbiRi(\u03b8i) \u2264 1\u21d2 Ri(ci\u03b8i) \u2264 Ri(\u03b8\u0302i).\nTherefore ci\u03b8i \u2212 \u03b8\u0302i \u2208 Ci by definition. We also have\nk\u2211 i=1 \u03b8\u0302i + k\u2211 i=1 ( c\u2032i ci \u2212 1)\u03b8\u0302i + k\u2211 i=1 c\u2032i ci (ci\u03b8i \u2212 \u03b8i) = \u03b8 \u21d2 c\u2032i ci (ci\u03b8i \u2212 \u03b8i) = \u2212 k\u2211 i=1 ( c\u2032i ci \u2212 1)\u03b8\u0302i\nwhich is contradict to our condition. Therefore \u03b8\u0302i is a solution. Uniqueness is a direct conclusion of lemma 11.\nNote that in this proof we set \u03bbi = ciRi(\u03b8i) . This is also a general way to choose the parameter {\u03bbi}."}, {"heading": "F.3 Proof of Lemma 11", "text": "Proof: Without loss of generality, assume \u03b8i 6= 0.According to (55), let\nci = \u03bbiRi(\u03b8i) R(\u03b8) , and \u03b8\u2032i = 1 ci \u03b8i.\nSuppose \u03b8i is a unique decomposition, for any \u2206i \u2208 Ci, if \u2211k i=1 c \u2032 i\u03bbiRi(\u03b8 \u2032 i + \u2206i) \u2264 R(\u03b8) and \u2211k i=1 c \u2032 i(\u03b8 \u2032 i +\n\u2206i) = \u03b8 for some c\u2032i \u2265 0, \u2211k i=1 c \u2032 i = 1. Therefore we obtain the following decomposition of \u03b8:\n\u03b8 = k\u2211 i=1 ci\u03b8 \u2032 i + k\u2211 i=1 (c\u2032i \u2212 ci)\u03b8i + k\u2211 i=1 c\u2032i\u2206i.\nIt is obvious from observation that \u2211k\ni=1(c \u2032 i\u2212ci)\u03b8i+ \u2211k i=1 c \u2032 i\u2206i = 0 and \u2211k i=1(c \u2032 i\u2212ci)\u03b8i \u2208 C0, \u2211k i=1 c\n\u2032 i\u2206i \u2208\u2211k\ni=1 Ci. By minimal ofR(\u03b8), \u2211k i=1 c \u2032 i\u03bbiRi(\u03b8 \u2032 i+\u2206i) = R(\u03b8). By our assumption, such decomposition of \u03b8 is unique,\nthus k\u2211 i=1 (c\u2032i \u2212 ci)\u03b8i = k\u2211 i=1 c\u2032i\u2206i = 0.\nwhich implies\u2212C0\u2229 \u2211k\ni=1 Ci = {0}. Uniqueness also give that c\u2032i\u2206i = 0 for each i = 1, 2, . . . , k. Therefore \u2212Ci \u2229 \u2211 j 6=i Cj = {0}.\nIf \u03b8i is not a unique decomposition then there are some \u2206i 6= 0 such that \u2211k i=1 \u2206i = 0 and\nk\u2211 i=1 \u03bbiRi(\u03b8i + \u2206i) = R(\u03b8).\nLet\nc\u2032\u2032i = \u03bbiRi(\u03b8i + \u2206i) R(\u03b8) , and \u03b8\u2032\u2032i = 1\nc\u2032\u2032i (\u03b8i + \u2206i),\nwe have \u03bbiRi(\u03b8 \u2032\u2032 i ) = R(\u03b8),\nand hence \u03b8\u2032\u2032i \u2212 \u03b8\u2032i \u2208 Ci for \u03bbiRi(\u03b8\u2032i) = R(\u03b8). Unfold \u03b8\u2032i and \u03b8\u2032\u2032i gives\nc\u2032\u2032i (\u03b8 \u2032\u2032 i \u2212 \u03b8\u2032i) = c\u2032\u2032i (\n1 c\u2032\u2032i \u2212 1 c\u2032i )\u03b8i + \u2206i.\nSum over all i, we get k\u2211 i=1 c\u2032\u2032i (\u03b8 \u2032\u2032 i \u2212 \u03b8\u2032i) = k\u2211 i=1 c\u2032\u2032i ( 1 c\u2032\u2032i \u2212 1 c\u2032i )\u03b8i,\nwhich is contradict to our assumption that \u2212C0 \u2229 \u2211k i=1 Ci = {0}. Therefore the conclusion holds."}], "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "author": ["Alekh Agarwal", "Sahand Negahban", "Martin J. Wainwright"], "venue": "The Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Sparse Prediction with the k-Support Norm", "author": ["Andreas Argyriou", "Rina Foygel", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Estimation with Norm Regularization", "author": ["Arindam Banerjee", "Sheng Chen", "Farideh Fazayeli", "Vidyashankar Sivakumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Peter J Bickel", "Yaacov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Concentration Inequalities", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Statistics for High Dimensional Data: Methods, Theory and Applications", "author": ["Peter Buhlmann", "Sara van de Geer"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Latent variable graphical model selection via convex optimization", "author": ["Venkat Chandrasekaran", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "The Annals of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1935}, {"title": "The Convex Geometry of Linear Inverse Problems", "author": ["Venkat Chandrasekaran", "Benjamin Recht", "Pablo A. Parrilo", "Alan S. Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Rank-Sparsity Incoherence for Matrix Decomposition", "author": ["Venkat Chandrasekaran", "Sujay Sanghavi", "Pablo a. Parrilo", "Alan S. Willsky"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Generalized dantzig selector: Application to the k-support norm", "author": ["Soumyadeep Chatterjee", "Sheng Chen", "Arindam Banerjee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Structured estimation with atomic norms: General bounds and applications", "author": ["Sheng Chen", "Arindam Banerjee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Uncertainty principles and ideal atomic decomposition", "author": ["David L. Donoho", "Xiaoming Huo"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Corrupted Sensing: Novel Guarantees for Separating Structured Signals", "author": ["R Foygel", "L Mackey"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes, volume 23 of A Series of Modern Surveys in Mathematics", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The achievable performance of convex demixing", "author": ["M B McCoy", "J A Tropp"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A geometric analysis of convex demixing", "author": ["Michael B McCoy"], "venue": "Ph.D. Thesis, Caltech,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Learning without concentration", "author": ["Shahar Mendelson"], "venue": "J. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "A Unified Framework for High-Dimensional Analysis ofM -Estimators with Decomposable Regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Sharp Time\u2013Data Tradeoffs for Linear Inverse Problems", "author": ["Samet Oymak", "Benjamin Recht", "Mahdi Soltanolkotabi"], "venue": "ArXiv e-prints,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Restricted Eigenvalue Properties for Correlated Gaussian Designs", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Convex Analysis", "author": ["R. Tyrrell Rockafellar"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1970}, {"title": "Upper and Lower Bounds for Stochastic Processes. A Series of Modern Surveys in Mathematics", "author": ["Michel Talagrand"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Regression Shrinkage and Selection via the Lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements", "author": ["Joel A Tropp"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Estimation in high dimensions: a geometric perspective", "author": ["Roman Vershynin"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Compressive principal component pursuit", "author": ["John Wright", "Arvind Ganesh", "Kerui Min", "Yi Ma"], "venue": "IEEE International Symposium on Information Theory,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Dirty statistical models", "author": ["Eunho Yang", "Pradeep Ravikumar"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "For high-dimensional structured estimation problems [7, 27], considerable advances have been made in accurately estimating a sparse or structured parameter \u03b8 \u2208 Rp even when the sample size n is far smaller than the ambient dimensionality of \u03b8\u2217, i.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "For high-dimensional structured estimation problems [7, 27], considerable advances have been made in accurately estimating a sparse or structured parameter \u03b8 \u2208 Rp even when the sample size n is far smaller than the ambient dimensionality of \u03b8\u2217, i.", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 7, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 8, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 10, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 14, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 15, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 17, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 18, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 29, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 30, "context": ", \u03b8 = \u2211k i=1 \u03b8 \u2217 i , where \u03b8 \u2217 1 may be sparse, \u03b8 \u2217 2 may be low rank, and so on [1, 8, 9, 11, 15, 16, 18, 19, 31, 32].", "startOffset": 81, "endOffset": 118}, {"referenceID": 13, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 82, "endOffset": 89}, {"referenceID": 10, "context": "Popular models such as Morphological Component Analysis (MCA) [14] and Robust PCA [8, 11] can be viewed as a special cases of this framework (see Section 9).", "startOffset": 82, "endOffset": 89}, {"referenceID": 21, "context": "In this paper, we focus on the case where \u03b1i = Ri(\u03b8 i ), noting that recent advances [22] can be used to extend our results to more general settings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 9, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 28, "context": "where n is the sample size, k is the number of components, and w(Ci\u2229Bp) is the Gaussian width [3, 10, 30] of the intersection of the error cone Ci with the unit Euclidean ball Bp \u2286 Rp.", "startOffset": 94, "endOffset": 105}, {"referenceID": 20, "context": "Interestingly, the estimation error converges at the rate of 1 \u221a n , similar to the case of single parameter estimators [21, 3], and depends only logarithmically on the number of components k.", "startOffset": 120, "endOffset": 127}, {"referenceID": 2, "context": "Interestingly, the estimation error converges at the rate of 1 \u221a n , similar to the case of single parameter estimators [21, 3], and depends only logarithmically on the number of components k.", "startOffset": 120, "endOffset": 127}, {"referenceID": 2, "context": "Further, while dependency of the error on Gaussian width of the error has been shown in recent results involving a single parameter [3, 30], the bound in (3) depends on the maximum of the Gaussian width of individual error cones, not their sum.", "startOffset": 132, "endOffset": 139}, {"referenceID": 28, "context": "Further, while dependency of the error on Gaussian width of the error has been shown in recent results involving a single parameter [3, 30], the bound in (3) depends on the maximum of the Gaussian width of individual error cones, not their sum.", "startOffset": 132, "endOffset": 139}, {"referenceID": 23, "context": "In Section 7, we compare an estimator using \u201cinfimal convolution\u201d[25] of norms with our estimator (2) for the noiseless case.", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 6, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 22, "context": "The lower bound is a generalized form of the restricted eigenvalue (RE) condition studied in the literature [5, 7, 24].", "startOffset": 108, "endOffset": 118}, {"referenceID": 4, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 22, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 2, "context": "If k = 1, the RE condition in (6) above simplifies to the widely studied RE condition in the current literature on Lasso-type and Dantzig-type estimators [5, 24, 3] where only one error cone is involved.", "startOffset": 154, "endOffset": 164}, {"referenceID": 0, "context": "We also note that the general RE condition as explicitly stated in (6) has been implicitly used in [1] and [32].", "startOffset": 99, "endOffset": 102}, {"referenceID": 30, "context": "We also note that the general RE condition as explicitly stated in (6) has been implicitly used in [1] and [32].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 22, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 2, "context": "Since most existing literature on high-dimensional structured models focus on the k = 1 setting [5, 24, 3], there was no reason to study the SC condition carefully.", "startOffset": 96, "endOffset": 106}, {"referenceID": 17, "context": "In Section 3, we present a geometric characterization of the SC condition [18], and illustrate that the condition is both necessary and sufficient for accurate recovery of each component.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "and design X [3, 20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 19, "context": "and design X [3, 20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 17, "context": "In recent work, [18] concluded that if \u03b4i < 1 for each i = 1, .", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 55, "endOffset": 63}, {"referenceID": 19, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 55, "endOffset": 63}, {"referenceID": 2, "context": "Our analysis is based on the results and techniques in [28, 20], and we note that [3] has related results using mildly different techniques.", "startOffset": 82, "endOffset": 85}, {"referenceID": 26, "context": "(19) From [28, 20], one can obtain a lower bound to Q\u03be(A;Z) based on the Paley-Zygmund inequality.", "startOffset": 10, "endOffset": 18}, {"referenceID": 19, "context": "(19) From [28, 20], one can obtain a lower bound to Q\u03be(A;Z) based on the Paley-Zygmund inequality.", "startOffset": 10, "endOffset": 18}, {"referenceID": 2, "context": "The Gaussian width [3] of E is defined as w(E) = E sup u\u2208E \u3008g, u\u3009.", "startOffset": 19, "endOffset": 22}, {"referenceID": 24, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 2, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 26, "context": "One way to upper bound the supremum of a stochastic process is by generic chaining [26, 3, 28], and by using generic chaining we can upper bound the stochastic process by a Gaussian process, which is the Gaussian width.", "startOffset": 83, "endOffset": 94}, {"referenceID": 27, "context": "Let X \u2208 Rn\u00d7p be a random matrix where each row is an independent copy of the sub-Gaussian random vector Z \u2208 Rp, and where Z has sub-Gaussian norm |||Z|||\u03c82 \u2264 \u03c3x [29].", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "Let \u03b1 = infu\u2208Sp\u22121 E[|\u3008Z, u\u3009|] so that \u03b1 > 0 [20, 28].", "startOffset": 44, "endOffset": 52}, {"referenceID": 26, "context": "Let \u03b1 = infu\u2208Sp\u22121 E[|\u3008Z, u\u3009|] so that \u03b1 > 0 [20, 28].", "startOffset": 44, "endOffset": 52}, {"referenceID": 2, "context": "The result can be viewed as a direct generalization of existing results for k = 1, when the SC condition is always satisfied, and the sample complexity and error is given by w(C1 \u2229 Sp\u22121) and w(C1 \u2229Bp) [3, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 9, "context": "The result can be viewed as a direct generalization of existing results for k = 1, when the SC condition is always satisfied, and the sample complexity and error is given by w(C1 \u2229 Sp\u22121) and w(C1 \u2229Bp) [3, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 3, "context": "To determine a proper \u03b7t+1, we use a backtracking step [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Based on existing results [4], the basic method can be accelerated by setting the starting point of the next iteration \u03b8 i as a proper combination of \u03b8\u0303 i and \u03b8 t i .", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "By [4], one can use the updates: \u03b8 i = \u03b8\u0303 t+1 i + \u03b1t \u2212 1 \u03b1t+1 (\u03b8\u0303 i \u2212 \u03b8 t i) , where \u03b1t+1 = 1 + \u221a 1 + 4\u03b12 t 2 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Convergence of Algorithm 1 has been studied in [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "The work [4] also give the convergence rate of Algorithm 1, which is O(1/t2).", "startOffset": 9, "endOffset": 12}, {"referenceID": 23, "context": "The estimator (29) needs to consider the so-called \u201cinfimal convolution\u201d [25, 32] over different norms to get a (unique) decomposition of \u03b8 in terms of the components {\u03b8\u0302i}.", "startOffset": 73, "endOffset": 81}, {"referenceID": 30, "context": "The estimator (29) needs to consider the so-called \u201cinfimal convolution\u201d [25, 32] over different norms to get a (unique) decomposition of \u03b8 in terms of the components {\u03b8\u0302i}.", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "Results in [25] show that (30) is also a norm.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "The problem (31) is a simple structured recovery problem, and is well studied [10, 28].", "startOffset": 78, "endOffset": 86}, {"referenceID": 26, "context": "The problem (31) is a simple structured recovery problem, and is well studied [10, 28].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "Early work focus on the case when k=2 and noise \u03c9 = 0, and assume specific structures such as sparse+sparse [14], and low-rank+sparse [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Early work focus on the case when k=2 and noise \u03c9 = 0, and assume specific structures such as sparse+sparse [14], and low-rank+sparse [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "[16] analyze error bound for low-rank and sparse matrix decomposition with noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] analyze the decomposition of a low-rank matrix plus another matrix with generalized structure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] propose an estimator for the decomposition of two generalize structured matrices, while one of them has a random rotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "In [31], the authors generalize the noiseless matrix decomposition problem to arbitrary number of superposition under random orthogonal measurement.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[32] consider the superposition of structures of structures captured by decomposable norm, while [18] consider general norms but with a different measurement model, involving componentwise random rotations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[32] consider the superposition of structures of structures captured by decomposable norm, while [18] consider general norms but with a different measurement model, involving componentwise random rotations.", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "[32] consider a general framework for superposition model, and give a high-probability bound for the fol-", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] consider an estimator like (2), which is", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "1 Morphological Component Analysis Using l1 Norm In Morphological Component Analysis [14], we consider the following linear model y = X(\u03b8\u2217 1 + \u03b8 \u2217 2) + \u03c9, where vector \u03b81 is sparse and vector \u03b82 is sparse under a rotation Q.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In [14], the authors introduced a quantity M = max i,j |Qij |.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "2 Morphological Component Analysis Using k-support Norm k-support norm [2] is another way to induce sparse solution instead of l1 norm.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Recent works [2, 12] have shown that k-support norm has better statistical guarantee than l1 norm.", "startOffset": 13, "endOffset": 20}, {"referenceID": 11, "context": "Recent works [2, 12] have shown that k-support norm has better statistical guarantee than l1 norm.", "startOffset": 13, "endOffset": 20}, {"referenceID": 30, "context": "Therefore we can not apply the framework of [32] for this problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 105, "endOffset": 112}, {"referenceID": 10, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 105, "endOffset": 112}, {"referenceID": 1, "context": "3 Low-rank and Sparse Matrix Decomposition To recover a sparse matrix and low-rank matrix from their sum [8, 11], one can use k-support norm [2] to induce sparsity and nuclear norm to induce low-rank.", "startOffset": 141, "endOffset": 144}, {"referenceID": 30, "context": "If we have k > 1, the framework in [32] is not applicable, because k-support norm is not decomposable.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We set the matrix Q to be a p \u00d7 p discrete cosine transformation (DCT) matrix [14].", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.", "creator": "LaTeX with hyperref package"}}}