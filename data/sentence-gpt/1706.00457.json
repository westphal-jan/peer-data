{"id": "1706.00457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems", "abstract": "In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM's top-ranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017. As an independent research partner, we use a library of over 100,000 Python libraries and libraries to generate nmtpy modules. This means that nmtpy's implementation and development tools are based on a general Python architecture.\n\n\n\nNmtpy's training tools are a new toolkit based on the popular RNG and RNG languages (i.e. RNG, Python, C). The Python interpreter is the most used, with no Python libraries or libraries to write and distribute, and its Python library is implemented as an abstraction framework for programming programming in parallel, distributed, distributed systems.\nThe Python interpreter consists of two libraries:\nThe original is a version of Theano for training neural sequence-to-sequence architectures in Python, built upon a new and improved approach by using both Python and C library as examples for the training of neural sequence-to-sequence architectures. This allows the training to be executed with the following code:\nThe second version is a Python library written specifically for learning C programming languages. The code is not written specifically for learning Python.\nThe third version is a Python library written specifically for learning C programming languages. The code is not written specifically for learning C programming languages.\nNmtpy's goal is to teach neural sequence-to-sequence architectures in Python, using its own libraries and libraries. The interpreter contains several techniques for learning Python and C libraries. The most popular is the nmtpy-interpreter module.\nNmtpy's goal is to teach neural sequence-to-sequence architectures in Python, using its own libraries and libraries. The code is not written specifically for learning C programming languages.\nThe Python interpreter is built on a new and improved approach by using both Python and C library as examples for the training of neural sequence-to-sequence architectures. This allows the training to be executed with the following code:\nThe Python interpreter is a version of the original, with no Python libraries or libraries to write and distribute, and its Python library is implemented", "histories": [["v1", "Thu, 1 Jun 2017 18:57:39 GMT  (638kb,D)", "http://arxiv.org/abs/1706.00457v1", "10 pages, 3 figures"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ozan caglayan", "mercedes garc\\'ia-mart\\'inez", "adrien bardet", "walid aransa", "fethi bougares", "lo\\\"ic barrault"], "accepted": false, "id": "1706.00457"}, "pdf": {"name": "1706.00457.pdf", "metadata": {"source": "CRF", "title": "NMTPY: A FLEXIBLE TOOLKIT FOR ADVANCED NEURAL MACHINE TRANSLATION SYSTEMS", "authors": ["Ozan Caglayan", "Mercedes Garc\u00eda-Mart\u00ednez", "Adrien Bardet", "Walid Aransa", "Fethi Bougares", "Lo\u00efc Barrault"], "emails": [], "sections": [{"heading": "1 OVERVIEW", "text": "nmtpy is a refactored, extended and Python 3 only version of dl4mt-tutorial 1, a Theano (Theano Development Team, 2016) implementation of attentive Neural Machine Translation (NMT) (Bahdanau et al., 2014).\nThe development of nmtpy project which has been open-sourced2 under MIT license in March 2017, started in March 2016 as an effort to adapt dl4mt-tutorial to multimodal translation models. nmtpy has now become a powerful toolkit where adding a new model is as simple as deriving from an abstract base class to fill in a set of fundamental methods and (optionally) implementing a custom data iterator. The training and inference utilities are as model-agnostic as possible allowing one to use them for different sequence generation networks such as multimodal NMT and image captioning to name a few. This flexibility and the rich set of provided architectures (Section 3) is what differentiates nmtpy from Nematus (Sennrich et al., 2017) another NMT software derived from dl4mt-tutorial."}, {"heading": "2 WORKFLOW", "text": "Figure 1 describes the general workflow of a training session. An experiment in nmtpy is described with a configuration file (Appendix A) to ensure reusability and reproducibility. A training experiment can be simply launched by providing this configuration file to nmt-train which sets up the environment and starts the training. Specifically nmt-train automatically selects a free GPU, sets the seed for all random number generators and finally creates a model (model_type option) instance. Architecture-specific steps like data loading, weight initialization and graph construction are delegated to the model instance. The corresponding log file and model checkpoints are named in a way to reflect the experiment options determined by the configuration file (Example: model_type-e<embdim>-r<rnndim>-<opt>_<lrate>...).\nOnce everything is ready, nmt-train starts consuming mini-batches of data from the model\u2019s iterator to perform forward/backward passes along with the weight updates. A validation on held-out corpus is periodically performed to evaluate the generalization performance of the model. Specifically, after each valid_freq updates, nmt-train calls the nmt-translate utility which will perform beam-\n1https://github.com/nyu-dl/dl4mt-tutorial 2https://github.com/lium-lst/nmtpy\nar X\niv :1\n70 6.\n00 45\n7v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n7\nsearch decoding, compute the requested metrics and return the results back so that nmt-train can track the progress and save best checkpoints to disk.\nSeveral examples regarding the usage of the utilities are given in Appendix B."}, {"heading": "2.1 ADDING NEW ARCHITECTURES", "text": "New architectures can be defined by creating a new file under nmtpy/models/ using a copy of an existing architecture and modifying the following predefined methods:\n\u2022 __init__(): Instantiates a model. Keyword arguments can be used to add options specific to the architecture that will be automatically gathered from the configuration file by nmt-train.\n\u2022 init_params(): Initializes the layers and weights. \u2022 build(): Defines the Theano computation graph that will be used during training. \u2022 build_sampler(): Defines the Theano computation graph that will be used during\nbeam-search. This is generally very similar to build() but with sequential RNN steps and non-masked tensors.\n\u2022 load_valid_data(): Loads the validation data for perplexity computation. \u2022 load_data(): Loads the training data."}, {"heading": "2.2 BUILDING BLOCKS", "text": "In this section, we introduce the currently available components and features of nmtpy that one can use to design their architecture.\nTraining nmtpy provides Theano implementations of stochastic gradient descent (SGD) and its adaptive variants RMSProp (Tieleman & Hinton, 2012), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014) to optimize the weights of the trained network. A preliminary support for gradient noise (Neelakantan et al., 2015) is available for Adam. Gradient norm clipping (Pascanu et al., 2013) is enabled by default with a threshold of 5 to avoid exploding gradients. Although the provided architectures all use the cross-entropy objective by their nature, any arbitrary differentiable objective function can be used since the training loop is agnostic to the architecture being trained.\nRegularization A dropout (Srivastava et al., 2014) layer which can be placed after any arbitrary feed-forward layer in the architecture is available. This layer works in inverse mode where the magnitudes are scaled during training instead of testing. Additionally, L2 regularization loss with a scalar factor defined by decay_c option in the configuration can be added to the training loss.\nInitialization The weight initialization is governed by the weight_init option and supports Xavier (Glorot & Bengio, 2010) and He (He et al., 2015) initialization methods besides orthogonal (Saxe et al., 2013) and random normal.\nLayers The following layers are available in the latest version of nmtpy:\n\u2022 Feed-forward and highway layer (Srivastava et al., 2015) \u2022 Gated Recurrent Unit (GRU) (Chung et al., 2014) \u2022 Conditional GRU (CGRU) (Firat & Cho, 2016) \u2022 Multimodal CGRU (Caglayan et al., 2016a;b)\nLayer normalization (Ba et al., 2016), a method that adaptively learns to scale and shift the incoming activations of a neuron, can be enabled for GRU and CGRU blocks.\nIteration Parallel and monolingual text iterators with compressed (.gz, .bz2, .xz) file support are available under the names TextIterator and BiTextIterator. Additionally, the multimodal WMTIterator allows using image features and source/target sentences at the same time for multimodal NMT (Section 3.3). We recommend using shuffle_mode:trglen when implemented to speed up the training by efficiently batching same-length sequences.\nPost-processing All decoded translations will be post-processed if filter option is given in the configuration file. This is useful in the case where one would like to compute automatic metrics on surface forms instead of segmented. Currently available filters are bpe and compound for cleaning subword BPE (Sennrich et al., 2016) and German compound-splitting (Sennrich & Haddow, 2015) respectively.\nMetrics nmt-train performs a patience based early-stopping using either validation perplexity or one of the following external evaluation metrics:\n\u2022 bleu: Wrapper around Moses multi-bleu BLEU (Papineni et al., 2002) \u2022 bleu_v13a: A Python reimplementation of Moses mteval-v13a.pl BLEU \u2022 meteor: Wrapper around METEOR (Lavie & Agarwal, 2007)\nThe above metrics are also available for nmt-translate to immediately score the produced hypotheses. Other metrics can be easily added and made available as early-stopping metrics."}, {"heading": "3 ARCHITECTURES", "text": ""}, {"heading": "3.1 NMT", "text": "The default NMT architecture (attention) is based on the original dl4mt-tutorial implementation which differs from Bahdanau et al. (2014) in the following major aspects:\n\u2022 CGRU decoder which consists of two GRU layers interleaved with attention mechanism. \u2022 The hidden state of the decoder is initialized with a non-linear transformation applied to\nmean bi-directional encoder state in contrast to last bi-directional encoder state.\n\u2022 The Maxout (Goodfellow et al., 2013) hidden layer before the softmax operation is removed.\nIn addition, nmtpy offers the following configurable options for this NMT:\n\u2022 layer_norm Enables/disables layer normalization for bi-directional GRU encoder. \u2022 init_cgru Allows initializing CGRU with all-zeros instead of mean encoder state. \u2022 n_enc_layers Number of additional unidirectional GRU encoders to stack on top of bi-\ndirectional encoder.\n\u2022 tied_emb Allows sharing feedback embeddings and output embeddings (2way) or all embeddings in the network (3way) (Inan et al., 2016; Press & Wolf, 2016).\n\u2022 *_dropout Dropout probabilities for three dropout layers placed after source embeddings (emb_dropout), encoder hidden states (ctx_dropout) and pre-softmax activations (out_dropout)."}, {"heading": "3.2 FACTORED NMT", "text": "Factored NMT (FNMT) is an extension of NMT which is able to generate two output symbols. The architecture of such a model is presented in Figure 2. In contrast to multi-task architectures, FNMT outputs share the same recurrence and output symbols are generated in a synchronous fashion3.\nTwo FNMT variants which differ in how they handle the output layer are currently available:\n\u2022 attention_factors: the lemma and factor embeddings are concatenated to form a single feedback embedding.\n\u2022 attention_factors_seplogits: the output path for lemmas and factors are kept separate with different pre-softmax transformations applied for specialization.\nFNMT with lemmas and linguistic factors has been successfully used for IWSLT\u201916 English\u2192French (Garc\u00eda-Mart\u00ednez et al., 2016) and WMT\u2019174 English\u2192Latvian and English\u2192Czech evaluation campaigns."}, {"heading": "3.3 MULTIMODAL NMT & CAPTIONING", "text": "We provide several multimodal architectures (Caglayan et al., 2016a;b) where the probability of a target word is conditioned on source sentence representations and convolutional image features (Figure 3). More specifically, these architectures extends monomodal CGRU into a multimodal one where the attention mechanism can be shared or separate between input modalities. A late fusion of attended context vectors are done using either by summing or concatenating the modality-specific representations.\nOur attentive multimodal system for Multilingual Image Description Generation track of WMT\u201916 Multimodal Machine Translation surpassed the baseline architecture (Elliott et al., 2015) by +1.1 METEOR and +3.4 BLEU and ranked first among multimodal submissions (Specia et al., 2016)."}, {"heading": "3.4 LANGUAGE MODELING", "text": "A GRU-based language model architecture (rnnlm) is available in the repository which can be used with nmt-test-lm to obtain language model scores.\n3FNMT currently uses a dedicated nmt-translate-factors utility though it will probably be merged in the near future.\n4http://matrix.statmt.org/\nDecoder with Multimodal Attention\nTextual Encoder\nImage\nVisual Encoder (CNN)\nUnder review as a conference paper at ICLR 2016\nCNN RNN\nchildren sitting in a classroom\n+ RNN\nSchulkinder sitz n in einem Klassenzimmer\nFigure 1: An illustration of the multilingual multimodal language model. Descriptions are generated by combining features from source- and target-language multimodal language models. The dashed lines denote variants of the model: removing the CNN features from a source model would create language-only conditioning vectors; whereas removing the CNN input in the decoder assumes the source feature vectors know enough about the image to generate a good description.\npicts the overall approach, illustrating the way we transfer feature representations between models. Image description models generally use a fixed representation of the visual input taken from a object detection model (e.g., a CNN). In this work we add fixed features extracted from a source language model (which may itself be a multimodal image description model) to our image description model. This is distinct from neural machine translation models which train source language feature representations specifically for target decoding in a joint model (Cho et al., 2014; Sutskever et al., 2014). Our composite model pipeline is more flexible than a joint model, allowing the reuse of models for other tasks (e.g., monolingual image description, object recognition) and not requiring retraining for each different language pair. We show that the representations extracted from source language models, despite not being trained to translate between languages, are nevertheless highly successful in transferring additional informative features to the target language image description model.\nIn a series of experiments on the IAPR-TC12 dataset of images described in English and German, we find that models that incorporate source language features substantially outperform target monolingual image description models. The best English-languagemodel improves upon the state-of-theart by 2.3 BLEU4 points for this dataset. In the first results reported on German image description, our model achieves a 8.8 Meteor point improvement compared to a monolingual image description baseline. The implication is that linguistic and visual features offer orthogonal improvements in multimodal modelling (a point also made by Silberer & Lapata (2014) and Kiela & Bottou (2014)). The models that include visual features also improve over our translation baselines, although to a lesser extent; we attribute this to the dataset being exact translations rather than independently elicited descriptions, leading to high performance for the translation baseline. Our analyses show that the additional features improve mainly lower-quality sentences, indicating that our best models successfully combine multiple noisy input modalities."}, {"heading": "2 MODELS", "text": "Our multilingual image description models are neural sequence generation models, with additional inputs from either visual or linguistic modalities, or both. We present a family of models in sequence of increasing complexity to make their compositional character clear, beginning with a neural sequence model over words and concluding with the full model using both image and source features. See Figure 2 for a depiction of the model architecture."}, {"heading": "2.1 RECURRENT LANGUAGE MODEL (LM)", "text": "The core of our model is a Recurrent Neural Network model over word sequences, i.e., a neural language model (LM) (Mikolov et al., 2010). The model is trained to predict the next word in the sequence, given the current sequence seen so far. At each timestep i for input sequence w0...n, the input word wi, represented as a one-hot vector over the vocabulary, is embedded into a highdimensional continuous vector using the learned embedding matrixWeh (Eqn 1). A nonlinear function f is applied to the embedding combined with the previous hidden state to generate the hidden state hi (Eqn 2). At the output layer, the next word oi is predicted via the softmax function over the\n2\nFeature Maps\nAnnotation Vectors\nSource words\nTarget words\nFigure 3: The architecture f multimodal attention (Caglayan et al., 2016b)."}, {"heading": "3.5 IMAGE CAPTIONING", "text": "A GRU-based r imple entation of Show, Attend and Tell archit cture (Xu t al., 2015) which learns to generate a natural language description by applying soft attention over convolutional image features is available under th nam i g2t t. This architecture is c ntly u ed 5 as a baseline system for the Multilingual Image Description Generation track of WMT\u201917 Multimodal Machine Translation shared task."}, {"heading": "4 TOOLS", "text": "In this section we present translation and rescoring utilities nmt-translate and nmt-rescore. Other auxiliary utiliti s are briefly de cribed in Ap endix C."}, {"heading": "4.1 NMT-TRANSLATE", "text": "nmt-translate is responsible for translation decoding using the beam-search method defined by NMT architecture. This default beam-search supports single and ensemble decoding for both monomodal and multimodal translation models. If a g ven architecture reimpleme ts the beam-search method in its class, that one will be used instead.\nSince the number of CPUs in a single machine is 2x-4x higher than the number of GPUs and we mainly reserve the GPUs for training, nmt-translate makes use of CPU workers for maximum efficiency. More specifically, each worker rec ives a m del in tan (or i st nces when ensembling) and performs the beam-search on samples that it continuously fetches from a shared queue. This queue is filled by the mast r proce s using the it rator provided by the model.\nOne thing to note for parallel CPU decoding is that if the numpy is linked against a BLAS implementation with threading support enabled (as in the case with Anaconda & Intel MKL), each spawned process attempts to use all available threads in the machine leading to a resource conflict. In order for nmt-translate to benefit correctly from parallelism, the number of threads per process is thus limited 6 to 1. The impact of this setting and the overall decoding speed in terms of words/sec (wps) are reported in (Table 1) for a medium-sized En\u2192Tr NMT with \u223c10M parameters."}, {"heading": "4.2 NMT-RESCORE", "text": "A 1-best plain text or n-best hypotheses file can be rescored with nmt-rescore using either a single or an ensemble of models. Since rescoring of a given hypothesis simply means computing the negative\n5http://www.statmt.org/wmt17/multimodal-task.html 6This is achieved by setting X_NUM_THREADS=1 environment variable where X is one of\nOPENBLAS,OMP,MKL depending on the numpy installation.\nlog-likelihood of it given the source sentence, nmt-rescore uses a single GPU to efficiently compute the scores in batched mode. See Appendix B for examples."}, {"heading": "5 CONCLUSION", "text": "We have presented nmtpy, an open-source sequence-to-sequence framework based on dl4mt-tutorial and refined in many ways to ease the task of integrating new architectures. The toolkit has been internally used in our team for tasks ranging from monomodal, multimodal and factored NMT to image captioning and language modeling to help achieving top-ranked submissions during campaigns like IWSLT and WMT."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the French National Research Agency (ANR) through the CHIST-ERA M2CR project, under the contract number ANR-15-CHR2-0006-017."}, {"heading": "A CONFIGURATION FILE EXAMPLE", "text": "# Options in this section are consumed by nmt-train [training] model_type: attention # Model type without .py patience: 20 # early-stopping patience valid_freq: 1000 # Compute metrics each 1000 updates valid_metric: meteor # Use meteor during validations valid_start: 2 # Start validations after 2nd epoch valid_beam: 3 # Decode with beam size 3 valid_njobs: 16 # Use 16 processes for beam-search valid_save_hyp: True # Save validation hypotheses decay_c: 1e-5 # L2 regularization factor clip_c: 5 # Gradient clip threshold seed: 1235 # Seed for numpy and Theano RNG save_best_n: 2 # Keep 2 best models on-disk device_id: auto # Pick 1st available GPU snapshot_freq: 10000 # Save a resumeable snapshot max_epochs: 100\n# Options below are passed to model instance [model] tied_emb: 2way # weight-tying mode (False,2way,3way) layer_norm: True # layer norm in GRU encoder shuffle_mode: trglen # Shuffled/length-ordered batches filter: bpe # post-processing filter(s) n_words_src: 0 # limit src vocab if > 0 n_words_trg: 0 # limit trg vocab if > 0 save_path: ~/models # Where to store checkpoints rnn_dim: 100 # Encoder and decoder RNN dim embedding_dim: 100 # All embedding dim weight_init: xavier batch_size: 32 optimizer: adam lrate: 0.0004 emb_dropout: 0.2 # Set dropout rates ctx_dropout: 0.4 out_dropout: 0.4\n# Dictionary files produced by nmt-build-dict [model.dicts] src: ~/data/train.norm.max50.tok.lc.bpe.en.pkl trg: ~/data/train.norm.max50.tok.lc.bpe.de.pkl\n# Training and validation data [model.data] train_src : ~/data/train.norm.max50.tok.lc.bpe.en train_trg : ~/data/train.norm.max50.tok.lc.bpe.de valid_src : ~/data/val.norm.tok.lc.bpe.en valid_trg : ~/data/val.norm.tok.lc.bpe.de valid_trg_orig: ~/data/val.norm.tok.lc.de"}, {"heading": "B USAGE EXAMPLES", "text": "# Launch an experiment $ nmt-train -c wmt-en-de.conf\n# Launch an experiment with different architecture $ nmt-train -c wmt-en-de.conf \u2019model_type:my_amazing_nmt\u2019\n# Change dimensions $ nmt-train -c wmt-en-de.conf \u2019rnn_dim:500\u2019 \u2019embedding_dim:300\u2019\n# Force specific GPU device $ nmt-train -c wmt-en-de.conf \u2019device_id:gpu5\u2019\nListing 1: Example usage patterns for nmt-train.\n# Decode on 30 CPUs with beam size 10, compute BLEU/METEOR # Language for METEOR is set through source file suffix (.en) $ nmt-translate -j 30 -m best_model.npz -S val.tok.bpe.en \\\n-R val.tok.de -o out.tok.de -M bleu meteor -b 10\n# Generate n-best list with an ensemble of checkpoints $ nmt-translate -m model*npz -S val.tok.de \\\n-o out.tok.50best.de -b 50 -N 50\n# Generate json file with alignment weights (-e) $ nmt-translate -m best_model.npz -S val.tok.bpe.en \\\n-R val.tok.de -o out.tok.de -e\nListing 2: Example usage patterns for nmt-translate.\n# Rescore 50-best list with ensemble of models $ nmt-rescore -m model*npz -s val.tok.bpe.en \\\n-t out.tok.50best.de \\ -o out.tok.50best.rescored.de\nListing 3: Example usage patterns for nmt-rescore."}, {"heading": "C DESCRIPTION OF THE PROVIDED TOOLS", "text": "nmt-build-dict Generates .pkl vocabulary files from preprocessed corpus. A single/combined vocabulary for two or more languages can be created with -s flag.\nnmt-extract Extracts arbitrary weights from a model snapshot which can further be used as pretrained weights of a new experiment or analyzed using visualization techniques (especially for embeddings).\nnmt-coco-metrics A stand-alone utility which computes multi-reference BLEU, METEOR, CIDE-r (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) using MSCOCO evaluation tools (Chen et al., 2015). Multiple systems can be given with -s flag to produce a table of scores.\nnmt-bpe-(learn,apply) Copy of subword utilities 8 (Sennrich et al., 2016) which are used to first learn a BPE segmentation model over a given corpus file and then apply it to new sentences.\nnmt-test-lm Computes language model perplexity of a given corpus.\n8https://github.com/rsennrich/subword-nmt"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u00eda-Mart\u00ednez", "Fethi Bougares", "Lo\u00efc Barrault", "Joost van de Weijer"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Multimodal attention for neural machine translation", "author": ["Ozan Caglayan", "Lo\u00efc Barrault", "Fethi Bougares"], "venue": "arXiv preprint arXiv:1609.03976,", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler"], "venue": "CoRR, abs/1510.04709,", "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Conditional gated recurrent unit with attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Firat and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Firat and Cho.", "year": 2016}, {"title": "Factored neural machine translation architectures", "author": ["Mercedes Garc\u00eda-Mart\u00ednez", "Lo\u00efc Barrault", "Fethi Bougares"], "venue": "In Proceedings of the International Workshop on Spoken Language Translation,", "citeRegEx": "Garc\u00eda.Mart\u00ednez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garc\u00eda.Mart\u00ednez et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher"], "venue": "arXiv preprint arXiv:1611.01462,", "citeRegEx": "Inan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Inan et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Lavie and Agarwal.,? \\Q2007\\E", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf"], "venue": "arXiv preprint arXiv:1608.05859,", "citeRegEx": "Press and Wolf.,? \\Q2016\\E", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "A joint dependency model of morphological and syntactic structure for statistical machine translation", "author": ["Rico Sennrich", "Barry Haddow"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Sennrich and Haddow.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Nematus: a Toolkit for Neural Machine Translation, pp. 65\u201368", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch-Mayne", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": "Association for Computational Linguistics (ACL),", "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1 OVERVIEW nmtpy is a refactored, extended and Python 3 only version of dl4mt-tutorial 1, a Theano (Theano Development Team, 2016) implementation of attentive Neural Machine Translation (NMT) (Bahdanau et al., 2014).", "startOffset": 192, "endOffset": 215}, {"referenceID": 21, "context": "This flexibility and the rich set of provided architectures (Section 3) is what differentiates nmtpy from Nematus (Sennrich et al., 2017) another NMT software derived from dl4mt-tutorial.", "startOffset": 114, "endOffset": 137}, {"referenceID": 27, "context": "Training nmtpy provides Theano implementations of stochastic gradient descent (SGD) and its adaptive variants RMSProp (Tieleman & Hinton, 2012), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2014) to optimize the weights of the trained network.", "startOffset": 154, "endOffset": 168}, {"referenceID": 14, "context": "A preliminary support for gradient noise (Neelakantan et al., 2015) is available for Adam.", "startOffset": 41, "endOffset": 67}, {"referenceID": 16, "context": "Gradient norm clipping (Pascanu et al., 2013) is enabled by default with a threshold of 5 to avoid exploding gradients.", "startOffset": 23, "endOffset": 45}, {"referenceID": 9, "context": "Initialization The weight initialization is governed by the weight_init option and supports Xavier (Glorot & Bengio, 2010) and He (He et al., 2015) initialization methods besides orthogonal (Saxe et al.", "startOffset": 130, "endOffset": 147}, {"referenceID": 18, "context": ", 2015) initialization methods besides orthogonal (Saxe et al., 2013) and random normal.", "startOffset": 50, "endOffset": 69}, {"referenceID": 4, "context": ", 2015) \u2022 Gated Recurrent Unit (GRU) (Chung et al., 2014) \u2022 Conditional GRU (CGRU) (Firat & Cho, 2016) \u2022 Multimodal CGRU (Caglayan et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 20, "context": "Currently available filters are bpe and compound for cleaning subword BPE (Sennrich et al., 2016) and German compound-splitting (Sennrich & Haddow, 2015) respectively.", "startOffset": 74, "endOffset": 97}, {"referenceID": 15, "context": "\u2022 bleu: Wrapper around Moses multi-bleu BLEU (Papineni et al., 2002) \u2022 bleu_v13a: A Python reimplementation of Moses mteval-v13a.", "startOffset": 45, "endOffset": 68}, {"referenceID": 0, "context": "The default NMT architecture (attention) is based on the original dl4mt-tutorial implementation which differs from Bahdanau et al. (2014) in the following major aspects:", "startOffset": 115, "endOffset": 138}, {"referenceID": 10, "context": "\u2022 tied_emb Allows sharing feedback embeddings and output embeddings (2way) or all embeddings in the network (3way) (Inan et al., 2016; Press & Wolf, 2016).", "startOffset": 115, "endOffset": 154}, {"referenceID": 7, "context": "FNMT with lemmas and linguistic factors has been successfully used for IWSLT\u201916 English\u2192French (Garc\u00eda-Mart\u00ednez et al., 2016) and WMT\u2019174 English\u2192Latvian and English\u2192Czech evaluation campaigns.", "startOffset": 95, "endOffset": 125}, {"referenceID": 5, "context": "Our attentive multimodal system for Multilingual Image Description Generation track of WMT\u201916 Multimodal Machine Translation surpassed the baseline architecture (Elliott et al., 2015) by +1.", "startOffset": 161, "endOffset": 183}, {"referenceID": 22, "context": "4 BLEU and ranked first among multimodal submissions (Specia et al., 2016).", "startOffset": 53, "endOffset": 74}], "year": 2017, "abstractText": "In this paper, we present nmtpy, a flexible Python toolkit based on Theano for training Neural Machine Translation and other neural sequence-to-sequence architectures. nmtpy decouples the specification of a network from the training and inference utilities to simplify the addition of a new architecture and reduce the amount of boilerplate code to be written. nmtpy has been used for LIUM\u2019s topranked submissions to WMT Multimodal Machine Translation and News Translation tasks in 2016 and 2017. 1 OVERVIEW nmtpy is a refactored, extended and Python 3 only version of dl4mt-tutorial 1, a Theano (Theano Development Team, 2016) implementation of attentive Neural Machine Translation (NMT) (Bahdanau et al., 2014). The development of nmtpy project which has been open-sourced2 under MIT license in March 2017, started in March 2016 as an effort to adapt dl4mt-tutorial to multimodal translation models. nmtpy has now become a powerful toolkit where adding a new model is as simple as deriving from an abstract base class to fill in a set of fundamental methods and (optionally) implementing a custom data iterator. The training and inference utilities are as model-agnostic as possible allowing one to use them for different sequence generation networks such as multimodal NMT and image captioning to name a few. This flexibility and the rich set of provided architectures (Section 3) is what differentiates nmtpy from Nematus (Sennrich et al., 2017) another NMT software derived from dl4mt-tutorial.", "creator": "LaTeX with hyperref package"}}}