{"id": "1706.00536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Latent Attention Networks", "abstract": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network's inner mechanisms and their power and limitations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 2 Jun 2017 02:10:39 GMT  (246kb,D)", "http://arxiv.org/abs/1706.00536v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["christopher grimm", "dilip arumugam", "siddharth karamcheti", "david abel", "lawson l s wong", "michael l littman"], "accepted": false, "id": "1706.00536"}, "pdf": {"name": "1706.00536.pdf", "metadata": {"source": "CRF", "title": "Latent Attention Networks", "authors": ["Christopher Grimm", "Dilip Arumugam", "Siddharth Karamcheti", "David Abel", "Lawson L.S. Wong", "Michael L. Littman"], "emails": ["michael_littman}@brown.edu"], "sections": [{"heading": null, "text": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network\u2019s inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \u201cattention masks\u201d support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network\u2019s underlying decision-making process irrespective of the data modality."}, {"heading": "1 Introduction", "text": "Machine-learning systems are becoming ubiquitous, even in safety-critical areas. Trained models used in self-driving cars, healthcare, and environmental science must not only strive to be error free but, in the face of failures, must be amenable to rapid diagnosis and recovery. This trend toward real-world applications is largely being driven by recent advances in the area of deep learning. Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20]. Unlike traditional linear models, deep neural networks offer the significant advantage of being able to learn their own feature representation for the completion of a given task. While learning such a representation removes the need for manual feature engineering and generally boosts performance, the resulting models are often hard to interpret, making it significantly more difficult to assign credit (or blame) to the model\u2019s behaviors. The use of deep learning models in increasingly important application areas underscores the need for techniques to gain insight into their failure modes, limitations, and decision-making mechanisms.\nSubstantial prior work investigates methods for increasing interpretability of these systems. One body of work focuses on visualizing various aspects of networks or their relationship to each datum they take as input [28, 29]. Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24]. A third line of work, of which our method is most aligned, seeks to capture and understand what networks focus on and what they ignore through attention mechanisms.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n00 53\n6v 1\n[ cs\n.A I]\n2 J\nun 2\nAttention-based approaches focus on network architectures that specifically attend to regions of their input space. These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15]. Crucially, these explicit attention mechanisms act as filters on the input. As such, the filtered components of the input could be replaced with reasonably generated noise without dramatically affecting the final network output. The ability to selectively replace irrelevant components of the input space is a direct consequence of the explicit attention mechanism. The insight at the heart of the present work is that it is possible to evaluate the property of \u201cselective replaceability\u201d to better understand a network that lacks any explicit attention mechanism. An architecture without explicit attention may still depend more on specific facets of its input data when constructing its learned, internal representation, resulting in a \u201clatent\u201d attention mechanism.\nIn this work, we propose a novel approach for indirectly measuring latent attention mechanisms in arbitrary neural networks using the notion of selective replaceability. Concretely, we learn an auxiliary, \u201cLatent Attention Network\u201d (LAN), that consumes an input data sample and generates a corresponding mask (of the same shape) indicating the degree to which each of the input\u2019s components are replaceable with noise. We train this LAN by corrupting the inputs to a pre-trained network according to generated LAN masks and observing the resulting corrupted outputs. We define a loss function that trades off maximizing the corruption of the input while minimizing the deviation between the outputs generated by the pre-trained network using the true and corrupted inputs, independently. The resultant LAN masks must learn to identify the components of the input data that are most critical to producing the existing network\u2019s output (i.e. those regions that are given the most attention by the existing network.)\nWe empirically demonstrate that the LAN framework can provide unique insights into the inner workings of various pre-trained networks. Specifically, we show that classifiers trained on a Translated MNIST domain learn a two-stage process of first localizing a digit within the image before determining its class. We use this interpretation to predict regions on the screen where digits are less likely to be properly classified. Additionally, we use our framework to visualize the latent attention mechanisms of classifiers on both image classification (to learn the visual features most important to the network\u2019s prediction), and natural language document classification domains (to identify the words most relevant to certain output classes). Finally, we show that an autoencoder trained on frames of Atari Pong learn to encode static components of the input space without specifically attending to objects on the screen. We use this interpretation to predict the behavior of the autoencoder on previously unseen samples."}, {"heading": "2 Related Work", "text": "We now survey relevant literature focused on understanding deep neural networks, with a special focus on approaches employing attention in some capacity.\nAttention has primarily been applied to neural networks to improve performance [19, 10, 2]. Typically, the added attention scheme provides an informative prior that can ease the burden of learning a complex, highly structured output space (as in machine translation). For instance, Cho et al. [6] survey existing content-based attention models to improve performance in a variety of supervised learning tasks, including speech recognition, machine translation, image caption generation, and more. Similarly, Yang et al. [27] apply stacked attention networks to better answer natural language questions about images, and Goyal et al. [9] investigate a complementary method for networks specifically designed to answer questions about visual content; their approach visualizes which content in the image is used to inform the network\u2019s answer. They use a strategy similar to that of attention to visualize what a network focuses on when tasked with visual question answering problems.\nYosinski et al. [28] highlight an important distinction for techniques that visualize aspects of networks: dataset-centric methods, which require a trained network and data for that network, and networkcentric methods, which target visualizing aspects of the network independent of any data. In general, dataset-centric methods for visualization have the distinct advantange of being network agnostic. Namely, they can treat the network to visualize entirely as a black box. All prior work for visualizing networks, of both dataset-centric and network-centric methodologies, is specific to particular network architectures (such as convolutional networks). For example, Zeiler and Fergus [29] introduce a\nvisualization method for convolutional neural networks (CNNs) that illustrates which input patterns activate feature maps at each layer of the network. Their core methodology is to project activations of nodes at any layer of the network back to the input pixel space using a Deconvolutional Network introduced by Zeiler et al. [30], resulting in highly interpretable feature visualizations. An exciting line of work has continued advancing these methods, as in Nguyen et al. [21], Simonyan et al. [25], building on the earlier work of Erhan et al. [7] and Berkes and Wiskott [5].\nA different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24]. Lei et al. [16] forces networks to output a short \u201crationale\u201d that (ideally) justifies the network\u2019s decision in Natural Language Processing tasks. Bahdanau et al. [2] advance a similar technique in which neural translation training is augmented by incentivizing networks to jointly align and translate source texts. Lastly, Zintgraf et al. [31] describe a method for eliciting visualizations that offer explanation for decisions made by networks by highlighting regions of the input that are considered evidence for or against a particular decision.\nIn contrast to all of the discussed methods, we develop a dataset-centric method for visualizing attention in an arbitrary network architecture. To the best of our knowledge, the approach we develop is the first of its kind in this regard."}, {"heading": "3 Method", "text": "A key distinguishing feature of our approach is that we assume minimal knowledge about the network to be visualized. We only require that the network F : Rd 7\u2192 R` be provided as a black-box function (that is, we can provide input x to F and obtain output F (x)) through which gradients can be computed. Since we do not have access to the network architecture, we can only probe the network either at its input or its output. In particular, our strategy is to modify the input by selectively replacing components via an attention mask, produced by a learned Latent Attention Network (LAN)."}, {"heading": "3.1 Latent Attention Network Framework", "text": "A Latent Attention Network is a function A : Rd 7\u2192 [0, 1]d that, given an input x (for the original network F ), produces an attention mask A(x) of the same shape as x. The attention mask seeks to identify input components of x that are critical to producing the output F (x). Equivalently, the attention mask determines the degree to which each component of x can be corrupted by noise while minimally affecting F (x). To formalize this notion, we need two additional design components:\nLF : R` \u00d7 R` 7\u2192 R a loss function in the output space of F , H : Rd 7\u2192 R a noise probability density over the input space of F .\n(1)\nWe can now complete the specification of the LAN framework. As illustrated in Figure 1, given an input x, we draw a noisy vector \u03b7 \u223c H and corrupt x according to A(x) as follows:\nx\u0303 = A(x) \u00b7 \u03b7 + (1\u2212A(x)) \u00b7 x, (2)\nwhere 1 denotes a tensor of ones with the same shape as A(x), and all operations are performed element-wise. Under this definition of x\u0303, the components of A(x) that are close to 0 indicate that the corresponding components of x represent signal/importance, and those close to 1 represent noise/irrelevance. Finally, we can apply the black-box network F to x\u0303 and compare the output F (x\u0303) to the original F (x) using the loss function LF . An ideal attention mask A(x) replaces/corrupts as many input components as possible (it has A(x) components close to 1), while minimally distorting the original output F (x), as measured by LF . Hence we train the LAN A by minimizing the following training objective for each input x:\nLLAN(x) = E\u03b7\u223cH [ LF (F (x\u0303), F (x))\u2212 \u03b2A(x) ] , (3)\nwhere A(x) denotes the mean value of the attention mask for a given input, and \u03b2 > 0 is a hyperparameter for weighting the amount of corruption applied to the input against the reproducibility error with respect to LF . As in Equation 2, x\u0303 is a function of both \u03b7 and A(x)."}, {"heading": "3.2 Latent Attention Network Design", "text": "To specify a LAN, we need to provide two components: the loss functionLF and the noise distribution H . The choice of these two components depends on the particular visualization task. Typically, the loss function LF is the same as the one used to train F itself, although it is not necessary. For example, if a network F was pre-trained and applied as a black-box function for a slightly different task, one may wish to visualize the latent attention with respect to the new task\u2019s loss, which can be used to verify that F is considering expected parts of the input.\nThe noise distribution H should reflect the expected space of inputs to F , since input components\u2019 importance is measured with respect to variation determined by H . In the general setting, H could be a uniform distribution over Rd; however, we often operate in significantly more structured spaces (e.g. images, text). In these structured cases, we suspect it is important to ensure that the noise vector \u03b7 lies near the manifold of the input samples.\nBased on this principle, we propose two methods of defining H via the generating process for \u03b7:\n\u2022 Constant noise \u03b7 const: In domains where input features represent quantities with default value c (e.g. 0 word counts in a bag of words, 0 binary valued images), set \u03b7 = c1, where 1 is a tensor of ones with the appropriate shape and c \u2208 R. \u2022 Bootstrapped noise \u03b7 boot: Draw uniform random samples from the training dataset. We\nexpect that this approach is particularly effective in domains where the data occupies a small manifold of the input space. For example, consider that that the set of natural images is much smaller than the set of possible images. Randomly selecting an image guarantees that we will be near that manifold, whereas other basic forms of randomness are unlikely to have this property."}, {"heading": "4 Experiments", "text": "To illustrate the wide applicability of the LAN framework, we conduct experiments in a variety of typical learning tasks, including digit classification, object classification in natural images, and image reconstruction. The goal of these experiments is to demonstrate the effectiveness of LANs to visualize latent attention mechanisms of different network types. Additionally, we conduct an exploratory experiment in a topic-modeling task to demonstrate the flexibility of LANs across multiple modalities. While LANs can be implemented with arbitrary network architectures, we here restrict our focus to fully-connected LANs to demonstrate the efficacy of our framework. Specifically, our LAN implementations range from 2\u20135 fully-connected layers each with fewer than 1000 hidden units. At a high level, these tasks are as follows (see supplementary material for training details):\nTranslated MNIST\nData : A dataset of 28\u00d7 28 grayscale images with MNIST digits, scaled down to 12\u00d7 12, are placed in random locations. No modifications are made to the orientation of the digits.\nTask : We train a standard deep network for digit classification.\nCIFAR-10\nData : A dataset of 3-channel 32\u00d7 32 color images of objects or animals, each belonging to one of ten unique classes. The images are typically centered around the classes they depict.\nTask : We train a standard CNN for object detection.\nNewsgroup-20\nData : A dataset consisting of news articles belonging to one of twenty different topics. The list of topics includes politics, electronics, space, and religion, amongst others.\nTask : We train a bag-of-words neural network, similar to the Deep Averaging Network (DAN) of Iyyer et al. [12] to classify documents into one of the twenty different categories.\nAtari Pong\nData : A dataset of 3-channel 64\u00d7 64 color images of screens from the classic Atari game: Pong. These screens are generated by the Atari Learning Environment [3] emulator played by an agent that takes random actions at each time step.\nTask : We train a standard autoencoder for image reconstruction.\nFor each experiment, we train a network F (designed for the given task) to convergence. Then, we train a Latent Attention Network, A on F . For all experiments conducted with image data, we used bootstrapped noise while our exploratory experiment with natural language used constant noise. Since LANs capture attention in the input space, the result of the latter training procedure is to visualize the attention mechanism of F on any sample in the input. For a detailed description of all experiments and associated network architectures, please consult the supplementary material."}, {"heading": "5 Results", "text": "We next describe the results of our experiments."}, {"heading": "5.1 Translated MNIST Results", "text": "Results are shown in Figure 2. We provide side-by-side visualizations of samples from the Translated MNIST dataset and their corresponding attention maps produced by the LAN network. In these attention maps, there are two striking features: (1) a blob of attention surrounding the digit and (2) an unchanging grid pattern across the background. This grid pattern is depicted in Figure 3a.\nIn what follows, we defend an interpretation of the grid effect illustrated in Figure 3a. Namely, we claim that our attention masks have illustrated that the classifier network operates in two distinct phases:\n1. Detect the presence of a digit somewhere in the input space.\n2. Direct attention to the region in which the digit was found to determine its class.\nUnder our hypothesis, one would expect classification accuracy to decrease in regions not spanned by the constant grid pattern. To test this idea, we estimated the error of the classifier on digits centered at various locations in the image. We rescaled the digits to 7\u00d7 7 pixels to make it easier to fit them in the regions not spanned by the constant grid. Visualizations of the resulting accuracies are displayed in Figure 3b. Notice how the normalized accuracy falls off around the edges of the image (where the constant grid is least present). This effect is particularly pronounced with smaller digits, which would be harder to detect with a fixed detection grid.\nTo further corroborate our hypothesis, we conducted an additional experiment with a modified version of the Translated MNIST domain. In this new domain, digits are scaled to 12 \u00d7 12 pixels and never occur in the bottom right 12\u00d7 12 region of the image. Under these conditions, we retrained our classifier and LAN, obtaining the visualization of the constant grid pattern and probability representation presented in Figure 3(c-d). Notice how the grid pattern is absent from the bottom right-hand corner where digits never appeared at training time. Expectedly, the accuracy of the classifier falls off if tested on digits in this region.\nThese results illustrate the ability of the attention masks generated by LANs to provide insights into the methods employed by networks to complete tasks. Additionally, through interpreting these methods via attention masks, we were able to predict likely failure modes in a pre-trained network."}, {"heading": "5.2 CIFAR-10 CNN", "text": "In Figure 4, we provide samples of original images from the CIFAR-10 dataset alongside the corresponding attention masks produced by the LAN. Notice that, for images belonging to the same class, the resulting masks capture common visual features such as tail feathers for birds or hulls/masts for ships. The presence of these features in the mask suggests that the underlying classifier learns a\ncanonical representation of each class to discriminate between images and to confirm its classification. We further note that, in addition to revealing high level concepts in the learned classifier, the LAN appears to demonstrate the ability to compose those concepts so as to discriminate between classes. This property is most apparent between the horse and deer classes, both of which show extremely similar regions of attention for capturing legs while deviating in their structure to confirm the presence of a heads or antlers, respectively."}, {"heading": "5.3 Newsgroup-20 Document Classification Results", "text": "Tables 1 and 2 contrast words present in documents against the 15 most important words, as determined by the corresponding attention mask, for topic classification. We note that these important words generally tend to be either in the document itself (highlighted in yellow) or closely associated with the category that the document belongs to. The absence of important words from other classes is explained by our choice of \u03b70-noise, which produces more visually appealing attention-masks, but doesn\u2019t penalize the LAN for ignoring such words. We suspect that category-associated words not present in the document occur due to the capacity limitations on the fully-connected LAN architecture on a high dimensional and poorly structured bag-of-words input space. Future work will further explore the use of LANs in natural language tasks."}, {"heading": "5.4 Pong Autoencoder", "text": "Results are shown in Figure 5. We investigate the attention masks learned from an autoencoder trained on frames from Atari Pong under a random policy. In this experiment, we observe that the LAN learns a static attention mask. Note that the highlighted areas correspond to regions where the paddles and ball are commonly found. In particular, there is a high degree of uniformity within the mask along the paddles\u2019 columns. This suggests that rather than tracking the paddles\u2019 positions, the autoencoder stores pixel values in the columns where the paddles are likely to be.\nTo support this claim, we illustrate the behavior of the autoencoder when provided an input with a duplicated player paddle placed in various positions on the screen. As we see in Figure 6, the\nautoencoder is unable to reconstruct duplicate paddles placed in arbitrary locations, often mistaking them for the ball (left two columns). However, when the duplicate paddle is placed on the same column as the player paddle, the autoencoder successfully reconstructs both (right two columns). The right-most example is particularly interesting because the autoencoder has never seen such input during training, and represents an example of the power of LANs to reveal the underlying method by which trained networks accomplish their goals\u2014in this case by encoding an entire column of pixels."}, {"heading": "6 Conclusion", "text": "As deep neural networks continue to find application to a growing collection of tasks, understanding their decision-making processes becomes increasingly important. Furthermore, as this space of tasks grows to include areas where there is a small margin for error, the ability to explore and diagnose problems within erroneous models becomes crucial.\nIn this work, we proposed Latent Attention Networks as a framework for capturing the latent attention mechanisms of arbitrary neural networks that draws parallels between noise-based input corruption and attention. We have shown that the analysis of these attention measurements can effectively diagnose failure modes in pre-trained networks and provide unique perspectives on the mechanism by which arbitrary networks perform their designated tasks.\nWe believe there are several interesting research directions that arise from our framework. First, there are interesting parallels between this work and the popular Generative Adversarial Networks [8]. It may be possible to simultaneously train F andA as adversaries. Since both F andA are differentiable, one could potentially exploit this property and use A to encourage a specific attention mechanism on F , speeding up learning in challenging domains and otherwise allowing for novel interactions between deep networks. Furthermore, we explored two types of noise for input corruption: \u03b7 const and \u03b7 boot. It may be possible to make the process of generating noise a part of the network itself\nby learning a nonlinear transformation and applying it to some standard variety of noise (such as Normal or Uniform). Since our method depends on being able to sample noise that is similar to the \u201cbackground noise\u201d of the domain, better mechanisms for capturing noise could potentially enhance the LAN\u2019s ability to pick out regions of attention and eliminate the need for choosing a specific type of noise at design time. Doing so would allow the LAN to pick up more specific features of the input space that are relevant to the decision-making process of arbitrary classifier networks."}, {"heading": "A Translated MNIST Handwritten Digit Classifier", "text": "Here we investigate the attention masks produced by a LAN trained on a digit classifier. We show how LANs provide intuition about the particular method a neural network uses to complete its task and highlight failure-modes. Specifically, we construct a \u201ctranslated MNIST\u201d domain, where the original digits are scaled down from 28\u00d7 28 to 12\u00d7 12 and positioned in random locations in the original 28\u00d7 28 image. The network F is a classifier, outputting the probability of each digit being present in a given image.\nThe pre-trained network, F has the following architecture: Conv(10, 2, (4 \u00d7 4), `-ReLU), Conv(20, 2, (4\u00d7 4), `-ReLU), FC(10, softmax). F is trained with the Adam Optimizer for 100, 000 iterations with a learning rate of 0.001 and with LF = \u2212 \u2211 i yi logF (x)i where y \u2208 R10 is a one-hot vector indicating the digit class.\nThe latent attention network, A has the following architecture: FC(100, `-ReLU), FC(784, sigmoid), with its output being reshaped to a 28\u00d7 28 image. A is trained with the Adam Optimizer for 100, 000 iterations with a learning rate of 0.0001. We use \u03b2 = 5.0 and \u03b7 = \u03b7boot for this experiment."}, {"heading": "B CIFAR-10 CNN", "text": "In this experiment we demonstrate that the LAN framework can illuminate the decision making of classifier (based on the Alexnet architecture) on natural images. To avoid overfitting, we augment the CIFAR-10 dataset by applying small random affine transformations to the images at train time. We used \u03b2 = 5.0 for this experiment.\nThe pre-trained network, F has the following architecture: Conv(64, 2, (5 \u00d7 5), `-ReLU), Conv(64, 2, (5 \u00d7 5), `-ReLU), Conv(64, 1, (3 \u00d7 3), `-ReLU), Conv(64, 1, (3 \u00d7 3), `-ReLU), Conv(32, 2, (3 \u00d7 3), `-ReLU), FC(384, tanh), FC(192, tanh), FC(10, softmax), where dropout and local response normalization is applied at each layer. F is trained with the Adam Optimizer for 250, 000 iterations with a learning rate of 0.0001 and with LF = \u2212 \u2211 i yi logF (x)i where y \u2208 R20 is a one-hot vector indicating the image class.\nThe latent attention network, A has the following architecture: FC(500, `-ReLU), FC(500, `-ReLU), FC(500, `-ReLU), FC(1024, sigmoid), with its output being reshaped to a 32 \u00d7 32 \u00d7 1 image and tiled 3 times on the channel dimension to produce a mask over the pixels. A is trained with the Adam Optimizer for 250, 000 iterations with a learning rate of 0.0005. We used \u03b2 = 7.0 and \u03b7 = \u03b7boot for this experiment.\nC 20 Newsgroups Document Classification\nIn this experiment, we extend the LAN framework for use on non-visual tasks. Namely, we show that it can be used to provide insight into the decision-making process of a bag-of-words document classifier, and identify individual words in a document that inform its predicted class label.\nTo do this, we train a Deep Averaging Network (DAN) [12] for classifying documents from the Newsgroup-20 dataset. The 20 Newsgroups Dataset consists of 18,821 total documents, partioned into a training set of 11,293 documents, and a test set of 7,528 documents. Each document belongs to 1 of 20 different categories, including topics in religion, sports, computer hardware, and politics,\nto name a few. In our experiments, we utilize the version of the dataset with stop words (common words like \u201cthe\u201d, \u201chis\u201d, \u201cher\u201d) removed.\nThe DAN Architecture is very simple - each document is represented with a bag-of-words histogram vector, with dimension equal to the number of unique words in the dataset (the size of the vocabulary). This bag of words vector is then multiplied with an embedding matrix and divided by the number of words in the document, to generate a low-dimension normalized representation. This vector is then passed through two separate hidden layers (with dropout), and then a final softmax layer, to produce a distribution over the 20 possible classes. In our experiments we use an embedding size of 50, and hidden layer sizes of 200 and 150 units, respectively. We train the model for 1,000,000 mini-batches, with a batch size of 32. Like with our previous experiments, we utilize the Adam Optimizer Kingma and Ba [13], with a learning rate of 0.00005.\nThe latent attention network,A has the following architecture: FC(100, `-ReLU), FC(1000, `-ReLU), FC(vocab-size, sigmoid). A is trained with the Adam Optimizer for 100, 000 iterations with a learning rate of 0.001. We used \u03b2 = 50.0 and \u03b7 = \u03b7const with a constant value of 0."}, {"heading": "D Pong Autoencoder", "text": "Here we explore the properties of latent attention masks produced from an autoencoder reconstructing images collected by an agent playing the popular Atari game, Pong. We collected 3.2 million frames of an agent executing a random policy in order to train a convolutional autoencoder. The choice to operate under a random policy was made to maximize the overall number of objects the model (and, consequently, LAN) would have to pay attention to in order to succeed; notice that an optimal agent playing the game to maximize its total score need not place any attention on the score pixels itself in order to perform well.\nThe pre-trained network, F has the following architecture: Conv(16, 2, (5 \u00d7 5), `-ReLU), Conv(32, 2, (5 \u00d7 5), `-ReLU), Conv(64, 2, (5 \u00d7 5), `-ReLU), ConvTrans(32, 2, (5 \u00d7 5), `-ReLU), ConvTrans(16, 2, (5\u00d7 5), `-ReLU), ConvTrans(3, 2, (5\u00d7 5), sigmoid). F is trained with the Adam Optimizer for 100, 000 iterations with a learning rate of 0.00005.\nThe latent attention network,A has the following architecture: FC(500, `-ReLU), FC(4096, sigmoid), with its output being reshaped to a 64\u00d7 64\u00d7 1 image and tiled 3 times on the channel dimension to produce a mask over the pixels. A is trained with the Adam Optimizer for 30, 000 iterations with a learning rate of 0.00001. We use \u03b2 = 0.0005 and \u03b7 = \u03b7boot."}], "references": [{"title": "M\u00c3\u017eller. How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael H. Bowling"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Slow feature analysis yields a rich repertoire of complex cell properties", "author": ["Pietro Berkes", "Laurenz Wiskott"], "venue": "Journal of vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University of Montreal,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Towards transparent ai systems: Interpreting visual question answering models", "author": ["Yash Goyal", "Akrit Mohapatra", "Devi Parikh", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1608.08974,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan L. Boyd-Graber", "Hal Daum\u00e9"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Rationalizing Neural Predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "Naacl,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin A. Riedmiller", "Andreas Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "author": ["Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder P. Singh", "Honglak Lee"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Why should i trust you?: Explaining the predictions of any classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Explaining classifications for individual instances", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["Matthew Zeiler", "Rob Fergus"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 3, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 17, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 19, "context": "Deep neural networks have achieved state-of-the-art performance on fundamental domains such as image classification [14], language modeling [4, 18], and reinforcement learning from raw pixels [20].", "startOffset": 192, "endOffset": 196}, {"referenceID": 27, "context": "One body of work focuses on visualizing various aspects of networks or their relationship to each datum they take as input [28, 29].", "startOffset": 123, "endOffset": 131}, {"referenceID": 28, "context": "One body of work focuses on visualizing various aspects of networks or their relationship to each datum they take as input [28, 29].", "startOffset": 123, "endOffset": 131}, {"referenceID": 22, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 0, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 23, "context": "Other work investigates algorithms for eliciting an explanation from trained machine-learning systems for each decision they make [23, 1, 24].", "startOffset": 130, "endOffset": 141}, {"referenceID": 25, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 10, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 21, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 14, "context": "These \u201cexplicit\u201d attention mechanisms were developed primarily to improve network behavior, but additionally offer increased interpretability of network decision making through highlighting key attributes of the input data [26, 11, 22, 15].", "startOffset": 223, "endOffset": 239}, {"referenceID": 18, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 9, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 1, "context": "Attention has primarily been applied to neural networks to improve performance [19, 10, 2].", "startOffset": 79, "endOffset": 90}, {"referenceID": 5, "context": "[6] survey existing content-based attention models to improve performance in a variety of supervised learning tasks, including speech recognition, machine translation, image caption generation, and more.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27] apply stacked attention networks to better answer natural language questions about images, and Goyal et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] investigate a complementary method for networks specifically designed to answer questions about visual content; their approach visualizes which content in the image is used to inform the network\u2019s answer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] highlight an important distinction for techniques that visualize aspects of networks: dataset-centric methods, which require a trained network and data for that network, and networkcentric methods, which target visualizing aspects of the network independent of any data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, Zeiler and Fergus [29] introduce a", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "[30], resulting in highly interpretable feature visualizations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], Simonyan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25], building on the earlier work of Erhan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] and Berkes and Wiskott [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] and Berkes and Wiskott [5].", "startOffset": 27, "endOffset": 30}, {"referenceID": 22, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 0, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 23, "context": "A different line of work focuses on strategies for eliciting explanations from machine learning systems to increase interpretability [23, 1, 24].", "startOffset": 133, "endOffset": 144}, {"referenceID": 15, "context": "[16] forces networks to output a short \u201crationale\u201d that (ideally) justifies the network\u2019s decision in Natural Language Processing tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] advance a similar technique in which neural translation training is augmented by incentivizing networks to jointly align and translate source texts.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1 Latent Attention Network Framework A Latent Attention Network is a function A : R 7\u2192 [0, 1] that, given an input x (for the original network F ), produces an attention mask A(x) of the same shape as x.", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "[12] to classify documents into one of the twenty different categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "These screens are generated by the Atari Learning Environment [3] emulator played by an agent that takes random actions at each time step.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "First, there are interesting parallels between this work and the popular Generative Adversarial Networks [8].", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "Deep neural networks are able to solve tasks across a variety of domains and modalities of data. Despite many empirical successes, we lack the ability to clearly understand and interpret the learned internal mechanisms that contribute to such effective behaviors or, more critically, failure modes. In this work, we present a general method for visualizing an arbitrary neural network\u2019s inner mechanisms and their power and limitations. Our dataset-centric method produces visualizations of how a trained network attends to components of its inputs. The computed \u201cattention masks\u201d support improved interpretability by highlighting which input attributes are critical in determining output. We demonstrate the effectiveness of our framework on a variety of deep neural network architectures in domains from computer vision, natural language processing, and reinforcement learning. The primary contribution of our approach is an interpretable visualization of attention that provides unique insights into the network\u2019s underlying decision-making process irrespective of the data modality.", "creator": "LaTeX with hyperref package"}}}