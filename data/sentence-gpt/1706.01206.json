{"id": "1706.01206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "One-step and Two-step Classification for Abusive Language Detection on Twitter", "abstract": "Automatic abusive language detection is a difficult but important task for online social media. Our research explores a two-step approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.01% with both type-based and non-social social media.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 5 Jun 2017 06:20:23 GMT  (286kb)", "http://arxiv.org/abs/1706.01206v1", "ALW1: 1st Workshop on Abusive Language Online to be held at the annual meeting of the Association of Computational Linguistics (ACL) 2017 (Vancouver, Canada), August 4th, 2017"]], "COMMENTS": "ALW1: 1st Workshop on Abusive Language Online to be held at the annual meeting of the Association of Computational Linguistics (ACL) 2017 (Vancouver, Canada), August 4th, 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ji ho park", "pascale fung"], "accepted": false, "id": "1706.01206"}, "pdf": {"name": "1706.01206.pdf", "metadata": {"source": "CRF", "title": "One-step and Two-step Classification for Abusive Language Detection on Twitter", "authors": ["Ji Ho Park"], "emails": ["jhpark@connect.ust.hk,", "pascale@ece.ust.hk"], "sections": [{"heading": null, "text": "Automatic abusive language detection is a difficult but important task for online social media. Our research explores a twostep approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 Fmeasure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps."}, {"heading": "1 Introduction", "text": "Fighting abusive language online is becoming more and more important in a world where online social media plays a significant role in shaping people\u2019s minds (Perse and Lambe, 2016). Nevertheless, major social media companies like Twitter find it difficult to tackle this problem (Meyer, 2016), as the huge number of posts cannot be mediated with only human resources.\nWarner and Hirschberg (2012) and Burnap and Williams (2015) are one of the early researches to use machine learning based classifiers for detecting abusive language. Djuric et al., (2015) incorporated representation word embeddings (Mikolov et al., 2013). Nobata et al. (2016) combined pre-defined language elements and word embedding to train a regression model. Waseem (2016) used logistic regression with n-grams and user-specific features such as gender and location. Davidson et al. (2017) conducted a deeper investigation on different types of abusive language. Badjatiya et al. (2017) experimented with\ndeep learning-based models using ensemble gradient boost classifiers to perform multi-class classification on sexist and racist language. All approaches have been on one step.\nMany have addressed the difficulty of the definition of abusive language while annotating the data, because they are often subjective to individuals (Ross et al. 2016) and lack of context (Waseem and Hovy, 2016; Schmidt & Wiegand, 2017). This makes it harder for non-experts to annotate without having a certain amount of domain knowledge (Waseem, 2016).\nIn this research, we aim to experiment a twostep approach of detecting abusive language first and then classifying into specific types and compare with a one-step approach of doing one multiclass classification on sexist and racist language.\nMoreover, we explore applying a convolutional neural network (CNN) to tackle the task of abusive language detection. We use three kinds of CNN models that use both character-level and word-level inputs to perform classification on different dataset segmentations. We measure the performance and ability of each model to capture characteristics of abusive language."}, {"heading": "2 Methodology", "text": "We propose to implement three CNN-based models to classify sexist and racist abusive language: CharCNN, WordCNN, and HybridCNN. The major difference among these models is whether the input features are characters, words, or both.\nThe key components are the convolutional layers that each computes a one-dimensional convolution over the previous input with multiple filter sizes and large feature map sizes. Having different filter sizes is the same as looking at a sentence with different windows simultaneously. Maxpooling is performed after the convolution to\ncapture the feature that is most significant to the output."}, {"heading": "2.1 CharCNN", "text": "CharCNN is a modification of the character-level convolutional network in (Zhang et al. 2015). Each character in the input sentence is first transformed into a one-hot encoding of 70 characters, including 26 English letters, 10 digits, 33 other characters, and a newline character (punctuations and special characters). All other non-standard characters are removed.\nZhang et al. (2015) uses 7 layers of convolutions and max-pooling layers, 2 fully-connected layers, and 1 softmax layer, but we also designed a shallow version with 2 convolutions and maxpooling layers, 1 fully-connected layers, and 1 softmax layers with dropout, due to the relatively small size of our dataset to prevent overfitting."}, {"heading": "2.2 WordCNN", "text": "WordCNN is a CNN-static version proposed by Kim (2014). The input sentence is first segmented into words and converted into a 300-dimensional embedding word2vec trained on 100 billion words from Google News (Mikolov et al., 2013). Incorporating pre-trained vectors is a widely-used method to improve performance, especially when using a relatively small dataset. We set the embedding to be non-trainable since our dataset is small.\nWe propose to segment some out-ofvocabulary phrases as well. Since the Twitter tweets often contain hashtags such as #womenagainstfeminism and #feminismisawful we use a wordsegment library (Segaran and Hammerbacher, 2009) to capture more words."}, {"heading": "2.3 HybridCNN", "text": "We design HybridCNN, a variation of WordCNN, since WordCNN has the limitation of only taking word features as input. Abusive language often contains either purposely or mistakenly misspelled words and made-up vocabularies such as #feminazi. Therefore, since CharCNN and WordCNN do not use character and word inputs at the same time, we design the HybridCNN to experiment whether the model can capture features from both levels of inputs.\nHybridCNN has two input channels. Each channel is fed into convolutional layers with three filter windows of different size. The output of the convolution are concatenated into one vector after 1-max-pooling. The vector is then fed into the final softmax layer to perform classification (See Figure 1)."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Datasets", "text": "We used the two English Twitter Datasets (Waseem and Hovy, 2016; Waseem, 2016) published as unshared tasks for the 1st Workshop on Abusive Language Online(ALW1). It contains tweets with sexist and racist comments. Waseem and Hovy (2016) created a list of criteria based on a critical race theory and let an expert annotate the corpus. First, we concatenated the two datasets into one and then divided that into three datasets for one-step and two-step classification (Table 1). One-step dataset is a segmentation for multi-class classification. For two-step classification, we merged the sexism and racism labels into one abusive label. Finally, we created another dataset with abusive languages to experiment a second classifier to distinguish \u201csexism\u201d and \u201cracism\u201d, given that the instance is classified as \u201cabusive\u201d."}, {"heading": "3.2 Training and Evaluation", "text": "We performed two classification experiments:\n1. Detecting \u201cnone\u201d, \u201csexist\u201d, and \u201cracist\u201d language (one-step)\n2. Detecting \u201cabusive language\u201d, then further classifying into \u201csexist\u201d or \u201cracist\u201d (twostep)\nThe purpose of these experiments was to see whether dividing the problem space into two steps makes the detection more effective. We trained the models using mini-batch stochastic gradient descent with AdamOptimizer (Kingma and Ba, 2014). For more efficient training in an unbalanced dataset, the mini-batch with a size of 32 had been sampled with equal distribution for all labels. The training continued until the evaluation set loss did not decrease any longer. All the results are average results of 10-fold cross validation.\nAs evaluation metric, we used F1 scores with precision and recall score and weighted averaged the scores to consider the imbalance of the labels. For this reason, total average F1 might not between average precision and recall.\nAs baseline, we used the character n-gram logistic regression classifier (indicated as LR on\nTable 2-4) from Waseem and Hovy (2016), Support Vector Machines (SVM) classifier, and FastText (Joulin et al., 2016) that uses average bag-of-words representations to classify sentences. It was the second best single model on the same dataset after CNN (Badjatiya et al., 2017)."}, {"heading": "3.3 Hyperparameters", "text": "For hyperparameter tuning, we evaluated on the validation set. These are the hyperparmeters used for evaluation. \u2022 CharCNN: Shallow model with 1024 fea-\nture units for convolution layers with filter size 4, max-pooling size 3, and L2 regularization constant 1 and 2048 units for the fully-connected layer\n\u2022 WordCNN: Convolution layers with 3 filters with the size of [1,2,3] and feature map size 50, max-pooling, and L2 regularization constant 1\n\u2022 HybridCNN: For the character input channel, convolution layers with 3 filters with size of [3,4,5] and for word input\nchannel, 3 filters with size of [1,2,3]. Both channels had feature map size of 50, maxpooling, and L2 regularization constant 1."}, {"heading": "4 Result and Discussions", "text": ""}, {"heading": "4.1 One-step Classification", "text": "The results of the one-step multi-class classification are shown in the top part of Table 2. Our newly proposed HybridCNN performs the best, giving an improvement over the result from WordCNN. We expected the additional character input channel improves the performance. We assumed that the reason CharCNN performing worse than WordCNN is that the dataset is too small for the character-based model to capture word-level features by itself.\nBaseline methods tend to have high averaged F1 but low scores on racism and sexism labels due to low recall scores."}, {"heading": "4.2 Two-step Classification", "text": "The two-step approach that combines two binary classifiers shows comparable results with one-step approach. The results of combining the two are shown in the bottom part of Table 3.\nCombining two logistic regression classifiers in the two-step approach performs about as well as one-step HybridCNN and outperform one-step logistic regression classifier by more than 10 F1 points. This is surprising since logistic regression takes less features than the HybridCNN.\nFurthermore, using HybridCNN on the first step to detect abusive language and logistic regression on the second step to classify racism and sexism worked better than just using HybridCNN.\nTable 4 shows the results of abusive language classification. HybridCNN also performs best for abusive language detection, followed by WordCNN and logistic regression.\nTable 5 shows the results of classifying into sexism and racism given that it is abusive. The second classifier has significant performance in predicting a specific type (in this case, sexism\nDataset One-step Two-step-1 Two-step-2 Label None Racism Sexism None Abusive Sexism Racism\n# 12,427 2,059 3,864 12,427 5,923 2,059 3,864\nTable 1: Dataset Segmentation\nand racism) of an abusive language. We can deduce that sexist and racist comments have obvious discriminating features that are easy for all classifiers to capture.\nSince the precision and recall scores of the \u201cabusive\u201d label is higher than those of \u201cracism\u201d and \u201csexism\u201d in the one-step approach, the twostep approach can perform as well as the one-step approach."}, {"heading": "5 Conclusion and Future work", "text": "We explored a two-step approach of combining two classifiers - one to classify abusive language and another to classify a specific type of sexist and racist comments given that the language is abusive. With many different machine learning classifiers including our proposed HybridCNN, which takes both character and word features as input, we showed the potential in the two-step approach compared to the one-step approach which is simply a multi-class classification. In this way, we can boost the performance of simpler models like logistic regression, which is faster and easier to train, and combine different types of classifiers like convolutional neural network and logistic regression together depending on each of its performance on different datasets.\nWe believe that two-step approach has potential in that large abusive language datasets with specific label such as profanity, sexist, racist, homophobic, etc. is more difficult to acquire than those simply flagged as abusive.\nFor this reason, in the future we would like to explore training the two-step classifiers on separate datasets (for example, a large dataset with abusive language for the first-step classifier and smaller specific-labelled dataset for the secondstep classifier) to build a more robust and detailed abusive language detector.\nNone Racism Sexism Total Method Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1\nLR .824 .945 .881 .810 .598 .687 .835 .556 .668 .825 .824 .814\nSVM .802 .956 .872 .815 .531 .643 .851 .483 .616 .814 .808 .793 FastText .828 .922 .882 .759 .630 .685 .777 .557 .648 .810 .812 .804\nCharCNN .861 .867 .864 .693 .746 .718 .713 .666 .688 .801 .811 .811 WordCNN .870 .868 .868 .704 .762 .731 .712 .686 .694 .818 .816 .816\nHybridCNN .872 .882 .877 .713 .766 .736 .743 .679 .709 .827 .827 .827 LR (two) .841 .933 .895 .800 .664 .731 .809 .590 .683 .828 .831 .824\nSVM (two) .816 .945 .876 .811 .605 .689 .823 .511 .630 .816 .815 .803 HybridCNN\n(two) .877 .864 .869 .690 .759 .721 .705 .701 .699 .807 .809 .807\nHybridCNN + LR(two) .880 .859 .869 .722 .751 .735 .683 .717 .699 .821 .817 .818\nTable 2. Experiment Results: upper part is the one-step methods that perform multi-class classification and lower methods with (two) indicate two-step that combines two binary classifiers. HybridCNN is our newly created model.\nModel Prec. Rec. F1 LR .816 .640 .711\nSVM .839 .560 .668 FastText .765 .616 .683\nCharCNN .743 .674 .707 WordCNN .731 .722 .726\nHybridCNN .719 .754 .734\nTable 3. Results on Abusive Language Detection\nModel Prec. Rec. F1 LR .954 .953 .952\nSVM .954 .953 .952 FastText .937 .937 .937\nCharCNN .941 .941 .941 WordCNN .952 .952 .952\nHybridCNN .951 .950 .950 Table 4. Results on Sexist/Racist\nClassification"}, {"heading": "Acknowledgements", "text": "This work is partially funded by CERG16214415 of the Hong Kong Research Grants Council and ITS170 of the Innovation and Technology Commission."}], "references": [{"title": "Deep learning for hate speech detection in tweets", "author": ["P. Badjatiya", "S. Gupta", "M. Gupta", "V. Varma"], "venue": "Proceedings of the 26th International Conference on World Wide Web Companion,", "citeRegEx": "Badjatiya et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Badjatiya et al\\.", "year": 2017}, {"title": "Hate speech detection with comment embeddings", "author": ["N. Djuric", "J. Zhou", "R. Morris", "M. Grbovic", "V. Radosavljevic", "N. Bhamidipati"], "venue": "Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Djuric et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Djuric et al\\.", "year": 2015}, {"title": "Bag of tricks for efficient text classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of EMNLP.,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Convolutional networks and applications in vision. Circuits and Systems (ISCAS)", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "Proceedings of 2010 IEEE International Symposium on,", "citeRegEx": "LeCun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Abusive language detection in online user content", "author": ["C. Nobata", "J. Tetreault", "A. Thomas", "Y. Mehdad", "Y. Chang"], "venue": "Proceedings of the 25th International Conference on World Wide Web,", "citeRegEx": "Nobata et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nobata et al\\.", "year": 2016}, {"title": "Media effects and society", "author": ["E.M. Perse", "J. Lambe"], "venue": null, "citeRegEx": "Perse and Lambe,? \\Q2016\\E", "shortCiteRegEx": "Perse and Lambe", "year": 2016}, {"title": "Measuring the reliability of hate speech annotations: The case of the european refugee crisis", "author": ["B. Ross", "M. Rist", "G. Carbonell", "B. Cabrera", "N. Kurowsky", "M. Wojatzki"], "venue": "In Proceedings of the Workshop on Natural Language", "citeRegEx": "Ross et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2017}, {"title": "A survey on hate speech detection using natural language processing", "author": ["A. Schmidt", "M. Wiegand"], "venue": "Socialnlp 2017,", "citeRegEx": "Schmidt and Wiegand,? \\Q2017\\E", "shortCiteRegEx": "Schmidt and Wiegand", "year": 2017}, {"title": "Beautiful data: The stories behind elegant data solutions \" O'Reilly Media, Inc.", "author": ["T. Segaran", "J. Hammerbacher"], "venue": null, "citeRegEx": "Segaran and Hammerbacher,? \\Q2009\\E", "shortCiteRegEx": "Segaran and Hammerbacher", "year": 2009}, {"title": "Harnessing twitter\" big data\" for automatic emotion identification", "author": ["W. Wang", "L. Chen", "K. Thirunarayan", "A.P. Sheth"], "venue": "Privacy, Security, Risk and Trust (PASSAT),", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Detecting hate speech on the world wide web", "author": ["W. Warner", "J. Hirschberg"], "venue": "Proceedings of the Second Workshop on Language in Social Media,", "citeRegEx": "Warner and Hirschberg,? \\Q2012\\E", "shortCiteRegEx": "Warner and Hirschberg", "year": 2012}, {"title": "Are you a racist or am I seeing things? annotator influence on hate speech detection on twitter", "author": ["Z. Waseem"], "venue": "Proceedings of the 1st Workshop on Natural Language Processing and Computational Social Science,", "citeRegEx": "Waseem,? \\Q2016\\E", "shortCiteRegEx": "Waseem", "year": 2016}, {"title": "Hateful symbols or hateful people? predictive features for hate speech detection on twitter", "author": ["Z. Waseem", "D. Hovy"], "venue": "Proceedings of NAACL-HLT,", "citeRegEx": "Waseem and Hovy,? \\Q2016\\E", "shortCiteRegEx": "Waseem and Hovy", "year": 2016}, {"title": "Characterlevel convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Fighting abusive language online is becoming more and more important in a world where online social media plays a significant role in shaping people\u2019s minds (Perse and Lambe, 2016).", "startOffset": 157, "endOffset": 180}, {"referenceID": 6, "context": ", (2015) incorporated representation word embeddings (Mikolov et al., 2013).", "startOffset": 53, "endOffset": 75}, {"referenceID": 15, "context": "2016) and lack of context (Waseem and Hovy, 2016; Schmidt & Wiegand, 2017).", "startOffset": 26, "endOffset": 74}, {"referenceID": 14, "context": "This makes it harder for non-experts to annotate without having a certain amount of domain knowledge (Waseem, 2016).", "startOffset": 101, "endOffset": 115}, {"referenceID": 4, "context": "Fighting abusive language online is becoming more and more important in a world where online social media plays a significant role in shaping people\u2019s minds (Perse and Lambe, 2016). Nevertheless, major social media companies like Twitter find it difficult to tackle this problem (Meyer, 2016), as the huge number of posts cannot be mediated with only human resources. Warner and Hirschberg (2012) and Burnap and Williams (2015) are one of the early researches to use machine learning based classifiers for detecting abusive language.", "startOffset": 158, "endOffset": 397}, {"referenceID": 4, "context": "Fighting abusive language online is becoming more and more important in a world where online social media plays a significant role in shaping people\u2019s minds (Perse and Lambe, 2016). Nevertheless, major social media companies like Twitter find it difficult to tackle this problem (Meyer, 2016), as the huge number of posts cannot be mediated with only human resources. Warner and Hirschberg (2012) and Burnap and Williams (2015) are one of the early researches to use machine learning based classifiers for detecting abusive language.", "startOffset": 158, "endOffset": 428}, {"referenceID": 0, "context": "Djuric et al., (2015) incorporated representation word embeddings (Mikolov et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Djuric et al., (2015) incorporated representation word embeddings (Mikolov et al., 2013). Nobata et al. (2016) combined pre-defined language elements and word embedding to train a regression model.", "startOffset": 0, "endOffset": 111}, {"referenceID": 0, "context": "Djuric et al., (2015) incorporated representation word embeddings (Mikolov et al., 2013). Nobata et al. (2016) combined pre-defined language elements and word embedding to train a regression model. Waseem (2016) used logistic regression with n-grams and user-specific features such as gender and location.", "startOffset": 0, "endOffset": 212}, {"referenceID": 0, "context": "Djuric et al., (2015) incorporated representation word embeddings (Mikolov et al., 2013). Nobata et al. (2016) combined pre-defined language elements and word embedding to train a regression model. Waseem (2016) used logistic regression with n-grams and user-specific features such as gender and location. Davidson et al. (2017) conducted a deeper investigation on different types of abusive language.", "startOffset": 0, "endOffset": 329}, {"referenceID": 0, "context": "Badjatiya et al. (2017) experimented with deep learning-based models using ensemble gradient boost classifiers to perform multi-class classification on sexist and racist language.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "CharCNN is a modification of the character-level convolutional network in (Zhang et al. 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 16, "context": "CharCNN is a modification of the character-level convolutional network in (Zhang et al. 2015). Each character in the input sentence is first transformed into a one-hot encoding of 70 characters, including 26 English letters, 10 digits, 33 other characters, and a newline character (punctuations and special characters). All other non-standard characters are removed. Zhang et al. (2015) uses 7 layers of convolutions and max-pooling layers, 2 fully-connected layers, and 1 softmax layer, but we also designed a shallow version with 2 convolutions and maxpooling layers, 1 fully-connected layers, and 1 softmax layers with dropout, due to the relatively small size of our dataset to prevent overfitting.", "startOffset": 75, "endOffset": 387}, {"referenceID": 6, "context": "The input sentence is first segmented into words and converted into a 300-dimensional embedding word2vec trained on 100 billion words from Google News (Mikolov et al., 2013).", "startOffset": 151, "endOffset": 173}, {"referenceID": 11, "context": "Since the Twitter tweets often contain hashtags such as #womenagainstfeminism and #feminismisawful we use a wordsegment library (Segaran and Hammerbacher, 2009) to capture more words.", "startOffset": 128, "endOffset": 160}, {"referenceID": 3, "context": "WordCNN is a CNN-static version proposed by Kim (2014). The input sentence is first segmented into words and converted into a 300-dimensional embedding word2vec trained on 100 billion words from Google News (Mikolov et al.", "startOffset": 44, "endOffset": 55}, {"referenceID": 15, "context": "We used the two English Twitter Datasets (Waseem and Hovy, 2016; Waseem, 2016) published as unshared tasks for the 1 Workshop on Abusive Language Online(ALW1).", "startOffset": 41, "endOffset": 78}, {"referenceID": 14, "context": "We used the two English Twitter Datasets (Waseem and Hovy, 2016; Waseem, 2016) published as unshared tasks for the 1 Workshop on Abusive Language Online(ALW1).", "startOffset": 41, "endOffset": 78}, {"referenceID": 14, "context": "We used the two English Twitter Datasets (Waseem and Hovy, 2016; Waseem, 2016) published as unshared tasks for the 1 Workshop on Abusive Language Online(ALW1). It contains tweets with sexist and racist comments. Waseem and Hovy (2016) created a list of criteria based on a critical race theory and let an expert annotate the corpus.", "startOffset": 42, "endOffset": 235}, {"referenceID": 4, "context": "We trained the models using mini-batch stochastic gradient descent with AdamOptimizer (Kingma and Ba, 2014).", "startOffset": 86, "endOffset": 107}, {"referenceID": 2, "context": "As baseline, we used the character n-gram logistic regression classifier (indicated as LR on Table 2-4) from Waseem and Hovy (2016), Support Vector Machines (SVM) classifier, and FastText (Joulin et al., 2016) that uses average bag-of-words representations to classify sentences.", "startOffset": 188, "endOffset": 209}, {"referenceID": 0, "context": "It was the second best single model on the same dataset after CNN (Badjatiya et al., 2017).", "startOffset": 66, "endOffset": 90}, {"referenceID": 2, "context": "We trained the models using mini-batch stochastic gradient descent with AdamOptimizer (Kingma and Ba, 2014). For more efficient training in an unbalanced dataset, the mini-batch with a size of 32 had been sampled with equal distribution for all labels. The training continued until the evaluation set loss did not decrease any longer. All the results are average results of 10-fold cross validation. As evaluation metric, we used F1 scores with precision and recall score and weighted averaged the scores to consider the imbalance of the labels. For this reason, total average F1 might not between average precision and recall. As baseline, we used the character n-gram logistic regression classifier (indicated as LR on Table 2-4) from Waseem and Hovy (2016), Support Vector Machines (SVM) classifier, and FastText (Joulin et al.", "startOffset": 87, "endOffset": 760}], "year": 2017, "abstractText": "Automatic abusive language detection is a difficult but important task for online social media. Our research explores a twostep approach of performing classification on abusive language and then classifying into specific types and compares it with one-step approach of doing one multi-class classification for detecting sexist and racist languages. With a public English Twitter corpus of 20 thousand tweets in the type of sexism and racism, our approach shows a promising performance of 0.827 Fmeasure by using HybridCNN in one-step and 0.824 F-measure by using logistic regression in two-steps.", "creator": "Word"}}}