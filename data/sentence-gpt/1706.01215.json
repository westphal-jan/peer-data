{"id": "1706.01215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework", "abstract": "Recent advances in deep learning motivate the use of deep neutral networks in sensing applications, but their excessive resource needs on constrained embedded devices remain an important impediment. A recently explored solution space lies in compressing (approximating or simplifying) deep neural networks in some manner before use on the device. We propose a new compression solution, called DeepIoT, that makes two key contributions in that space. First, unlike current solutions geared for compressing specific types of neural networks, DeepIoT presents a unified approach that compresses all commonly used deep learning structures for sensing applications, including fully-connected, convolutional, and recurrent neural networks, as well as their combinations. Second, unlike solutions that either sparsify weight matrices or assume linear structure within weight matrices, DeepIoT compresses neural network structures into smaller dense matrices by finding the minimum number of non-redundant hidden elements, such as filters and dimensions required by each layer, while keeping the performance of sensing applications the same. Importantly, it does so using an approach that obtains a global view of parameter redundancies, which is shown to produce superior compression. We conduct experiments with five different sensing-related tasks on Intel Edison devices. DeepIoT outperforms all compared baseline algorithms with respect to execution time and energy consumption by a significant margin. It reduces the size of deep neural networks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to 94.5%, and decrease energy consumption by 72.2% to 95.7%. These improvements are achieved without loss of accuracy. The results underscore the potential of DeepIoT for advancing the exploitation of deep neural networks on resource-constrained embedded devices. We conclude that DeepIoT offers a very powerful, efficient, yet difficult, way to capture and analyze information at a very low cost and to enable fast, high-capacity processing.\n\n\n\nThe DeepIoT project was created in the early 1990s to provide a framework for designing and building deep neural networks in discrete networks that can be distributed over multiple network layers. This framework allows applications to store more data in a variety of open source networks and to allow applications to communicate between multiple networks of distributed networks. The current version of DeepIoT comes with the following new features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 5 Jun 2017 06:55:44 GMT  (3236kb,D)", "http://arxiv.org/abs/1706.01215v1", null], ["v2", "Thu, 17 Aug 2017 14:48:44 GMT  (3169kb,D)", "http://arxiv.org/abs/1706.01215v2", "To appear in SenSys2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE cs.NI", "authors": ["shuochao yao", "yiran zhao", "aston zhang", "lu su", "tarek abdelzaher"], "accepted": false, "id": "1706.01215"}, "pdf": {"name": "1706.01215.pdf", "metadata": {"source": "META", "title": "Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework", "authors": ["Shuochao Yao", "Yiran Zhao", "Aston Zhang", "Lu Su", "Tarek Abdelzaher"], "emails": ["permissions@acm.org."], "sections": [{"heading": null, "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, Washington, DC, USA \u00a9 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . .$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn\nACM Reference format: Shuochao Yao, Yiran Zhao, Aston Zhang, Lu Su, and Tarek Abdelzaher. 2016. Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework. In Proceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference\u201917), 14 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "is paper is motivated by the prospect of enabling a \u201csmarter\u201d and more user-friendly category of every-day physical objects capable of performing complex sensing and recognition tasks, such as those needed for understanding human context and enabling more natural interactions with users in emerging Internet of ings (IoT) applications.\nPresent-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53]. e recent commercial interest in IoT technologies promises a proliferation of smart objects in human spaces at a much broader scale. Such objects will ideally have independent means of interacting with their surroundings to perform complex detection and recognition tasks, such as recognizing users, interpre ing voice commands, and understanding human context. e paper explores the feasibility of implementing such functions using deep neural networks on computationally-constrained devices, such as Intel\u2019s suggested IoT platform: the Edison board1.\ne use of deep neural networks in sensing applications has recently gained popularity. Speci c neural network models have been designed to fuse multiple sensory modalities and extract temporal relationships for sensing applications. ese models have shown signi cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi cation [31, 55].\nTraining the neural network can occur on a computationally capable node and, as such, is not of concern in this paper. e key impediment to deploying deep-learning-based sensing applications lies in the high memory consumption, execution time, and energy demand associated with storing and using the trained network on the target device. is leads to increased interest in compressing neural networks (without loss of accuracy) to enable exploitation of deep learning on low-end embedded devices.\nWe propose DeepIoT that compresses commonly used deep neural network structures for sensing applications without the loss of\n1h ps://so ware.intel.com/en-us/iot/hardware/edison\nar X\niv :1\n70 6.\n01 21\n5v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n7\naccuracy through deciding the minimum number of elements in each layer. Previous illuminating studies on neural network compression sparsify large dense parameter matrices into large sparse matrices [21, 23, 29]. In contrast, DeepIoT minimizes the number of elements in each layer, which results in converting parameters into a set of small dense matrices. A small dense matrix does not require additional storage for element indices and is e ciently optimized for processing [18]. DeepIoT greatly reduces the e ort of designing e cient neural structures for sensing applications by deciding the number of elements in each layer in a manner informed by the topology of the neural network, which further improves the achieved compression without sacri cing accuracy.\nDeepIoT borrows the idea of dropping hidden elements from a widely-used deep learning regularization method called dropout [45]. e dropout operation gives each hidden element a dropout probability. During the dropout process, hidden elements can be pruned according to their dropout probabilities. en a \u201cthinned\u201d network structure can be generated. However, these dropout probabilities are usually set to a pre-de ned value, such as 0.5. Such pre-de ned values are not the optimal probabilities, thereby resulting in a less e cient exploration of the solution space. If we can obtain the optimal dropout probability for each hidden element, it becomes possible for us to generate the optimal slim network structure that preserves the accuracy of sensing applications while maximally reducing the resource consumption of sensing systems. An important purpose of DeepIoT is thus to nd the optimal dropout probability for each hidden element in the neural network.\nNotice that, dropout can be easily applied to all commonly used neural network structures. In fully-connected neural networks, neurons are dropped in each layer [45]; in convolutional neural networks, lters are dropped in each layer [13]; and in recurrent neural networks, dimensions are reduced in each layer [14]. is means that DeepIoT can be applied to all commonly-used neural network structures and their combinations.\nTo obtain the optimal dropout probabilities for the neural network, DeepIoT exploits the network parameters themselves. From the perspective of model compression, a hidden element that is connected to redundant model parameters should have a higher probability to be dropped. A contribution of DeepIoT lies in exploiting a novel compressor neural network to solve this problem. It takes model parameters of each layer as input, learns parameter redundancies, and generates the dropout probabilities accordingly. Since there are interconnections of parameters among di erent layers, we design the compressor neural network to be a recurrent neural network that can globally share the redundancy information and generate dropout probabilities layer by layer.\ne compressor neural network is optimized jointly with the original neural network to be compressed through a compressor-critic framework that tries to minimize the loss function of the original sensing application. e compressor-critic framework emulates the idea of the well-known actor-critic algorithm from reinforcement learning [27], optimizing two networks in an iterative manner.\nWe evaluate the DeepIoT framework on the Intel Edison computing platform [1], which Intel markets as an enabler platform for the computing elements of embedded \u201cthings\u201d in IoT systems. We conduct two sets of experiments. e rst set consists of three tasks that enable embedded systems to interact with humans with\nbasic modalities, including handwri en text, vision, and speech, demonstrating superior accuracy of our produced neural networks, compared to others of similar size. e second set provides two examples of applying compressed neural networks to solving humancentric context sensing tasks; namely, human activity recognition and user identi cation, in a resource-constrained scenario.\nWe compare DeepIoT with other state-of-the-art magnitudebased [21] and sparse-coding-based [29] neural network compression methods. e resource consumption of resulting models on the Intel Edison module and the nal performance of sensing applications are estimated for all compressed models. In all experiments, DeepIoT is shown to outperform the other algorithms by a large margin in terms of compression ratio, memory consumption, execution time, and energy consumption. In these experiments, DeepIoT is able to reduce the model size by 90% to 98.9%, shorten the running time by 71.4% to 94.5%, and decrease the energy consumption by 72.2% to 95.7%. Importantly, these improvements are achieved without loss of accuracy. Experiments demonstrate the promise of DeepIoT in enabling resource-constrained embedded devices to bene t from advances in deep learning.\ne rest of this paper is organized as follows. Section 2 introduces related work on optimizating sensing applications for resourceconstrained devices. We describe the technical details of DeepIoT in Section 3. We describe system implementation in Section 4. e evaluation is presented in Section 5. Finally, we discuss the results in Section 6 and conclude in Section 7."}, {"heading": "2 RELATEDWORK", "text": "A key direction in embedded sensing literaure is to enable running progressively more interesting applications under the more pronounced resource constraints of embedded and mobile devices. Brouwers et al. reduced the energy consumption of Wi-Fi based localization with an incremental scanning strategy [4]. Hester et al. proposed an ultra-low-power hardware architecture and a companion so ware framework for energy-e cient sensing system [24]. Ferrari et al. and Schuss et al. focused on low-power wireless communication protocols [12, 42]. Wang et al. enabled energy e cient reliable broadcast by considering poorly correlated links [50]. Saifullah et al. designed a scalable and energy-e cient wireless sensor network (WSN) over white spaces [41]. Alsheikh et al. discussed about data compression in WSN with machine learning techniques [3].\nRecent studies focused on compressing deep neural networks for embedded and mobile devices. Han et al. proposed a magnitudebased compression method with ne-tuning, which illustrated promising compression results [23]. is method removes weight connections with low magnitude iteratively; however, it requires additional implementation of sparse matrix with more resource consumption. In addition, the aggressive pruning method increases the potential risk of irretrievable network damage. Guo et al. proposed a compression algorithm with connection splicing, which provided the chance of rehabilitation with a certain threshold [21]. However, the algorithm still focuses on weight level instead of structure level. Other than the magnitude-based method, the other series of works focused on the factorization-based method that reduced the neural network complexity by exploiting low-rank structures in parameters. Denton et al. exploited various matrix factorization\nmethods with ne-tunning to approximate the convolutional operations in order to reduce the neural network execution time [11]. Lane et al. applied sparse coding based and matrix factorization based method to reduce complexity of fully-connected layer and convolutional layer respectively [29]. However, factorization-based methods usually obtaine lower compression ratio compared with magnitude-based methods, and the low-rank assumption may hurt the nal network performance.\nOur paper is partly inspired by deep reinforcement learning. With the aid of deep neural networks, reinforcement leaning has achieved great success on Atari games [34], Go chess [44], and multichannel access [51].\nTo the best of our knowledge, DeepIoT is the rst framework for neural network structure compressing based on dropout operations and reducing parameter redundancies, where dropout operations provide DeepIoT the chance of rehabilitation with a certain probability. DeepIoT generates a more concise network structure for transplanting large-scale neural networks onto resource-constrained embedded devices without loss of performance."}, {"heading": "3 SYSTEM FRAMEWORK", "text": "We introduce DeepIoT, a neural network structure compression framework for sensing applications without performance loss. Without loss of generality, before introducing the technical details, we rst use an example of compressing a 3-layer fullyconnected neural network structure to illustrate the overall pipeline of DeepIoT. e detailed illustration is shown in Figure 1. e basic steps of compressing neural network structures for sensing applications with DeepIoT can be summarized as follows. (1) Insert dropout operations with dropout probabilities p(l ) (red\nboxes in Figure 1) into internal layers of the original neural network. e internal layers exclude input layers and output layers that have the xed dimension for a sensing application. is step will be detailed in Section 3.1. (2) Construct the compressor neural network. It takes the weight matrices W(l ) (green boxes in Figure 1) from the layers to be compressed in the original neural network as inputs, learns and shares the parameter redundancies among di erent layers, and generates optimal dropout probabilities p(l ), which is then fed back to the dropout operations in the original neural network. is step will be detailed in Section 3.2. (3) Iteratively optimize the compressor neural network and the original neural network with the compressor-critic framework. e compressor neural network is optimized to produce be er dropout probabilities that can generate a more e cient network structure for the original neural network. e original neural network is optimized to achieve a be er performance with the more e cient structure for a sensing application. is step will be detailed in Section 3.3.\nFor the rest of this paper, all vectors are denoted by bold lowercase le ers (e.g., x and y), and matrices and tensors are represented by bold upper-case le ers (e.g., X and Y). For a column vector x, the jth element is denoted by x[j]. For a tensor X, the t th matrix along the third axis is denoted by X\u00b7 \u00b7t , and the other slicing denotations are de ned similarly. e superscript l in x(l ) and X(l ) denote the vector and tensor for the l th layer of the neural network. We use\ncalligraphic le ers to denote sets (e.g., X and Y). For any set X, |X| denotes the cardinality of X."}, {"heading": "3.1 Dropout Operations in the Original Neural Network", "text": "Dropout is commonly used as a regularization method that prevents feature co-adapting and model over ing. e term \u201cdropout\u201d refers to dropping out units (hidden and visible) in a neural network. Since DeepIoT is a structure compression framework, we focus mainly on dropping out hidden units. e de nitions of hidden units are distinct in di erent types of neural networks, and we will describe them in detail. e basic idea is that we regard neural networks with dropout operations as bayesian neural networks with Bernoulli variational distributions [13, 14, 45].\nFor the fully-connected neural networks, the fully-connected operation with dropout can be formulated as\nz(l )[j] \u223c Bernoulli(p (l ) [j]), X\u0303(l ) = X(l )diag ( z(l ) ) ,\nY(l ) = X\u0303(l )W(l ) + b(l ), X(l+1) = f ( Y(l ) ) .\n(1)\nRefer to (1). e notation l = 1, \u00b7 \u00b7 \u00b7 ,L is the layer number in the fully-connected neural network. For any layer l , the weight matrix is denoted as W(l ) \u2208 Rd (l\u22121)\u00d7d (l ) ; the bias vector is denoted as b(l ) \u2208 Rd (l ) ; and the input is denoted as X(l ) \u2208 R1\u00d7d (l\u22121) . In addition, f (\u00b7) is a nonlinear activation function. Since DeepIoT does not drop input units, it holds that p(1)[j] = 1.\nAs shown in (1), each hidden unit is controlled by a Bernoulli random variable. In the original dropout method, the success probabilities of p(l )[j] can be set to the same constant p for all hidden units [45], but DeepIoT uses the Bernoulli random variable with individual success probabilities for di erent hidden units in order to compress the neural network structure in a ner granularity.\nFor the convolutional neural networks, the basic fully-connected operation is replaced by the convolution operation [13]. However,\nthe convolution can be reformulated as a linear operation as shown in (1). For any layer l , we denote K(l ) = { K(l )k } for k = 1, \u00b7 \u00b7 \u00b7 , c(l ) as the set of convolutional neural network (CNN)\u2019s kernels, where K(l )k \u2208 R\nh(l )\u00d7w (l )\u00d7c (l\u22121) is the kernel of CNN with height h(l ), width w(l ), and channel c(l\u22121). e input tensor of layer l is denoted as X\u0302(l ) \u2208 Rh\u0302(l\u22121)\u00d7w\u0302 (l\u22121)\u00d7c (l\u22121) with height h\u0302(l\u22121), width w\u0302(l\u22121), and channel c(l\u22121).\nNext, we convert convolving the kernels with the input into performing matrix product. We extract h(l ) \u00d7w(l ) \u00d7 c(l\u22121) dimensional patches from the input X\u0302(l ) with stride s and vectorize them. Collect these vectorized n patches to be the rows of our new input representation X(l ) \u2208 Rn\u00d7(h(l )w (l )c (l\u22121)). e vectorized kernels form the columns of the weight matrix W(l ) \u2208 R(h(l )w (l )c (l\u22121))\u00d7c (l ) .\nWith this transformation, dropout operations can be applied to convolutional neural networks according to (1). e composition of pooling and activation functions can be regarded as the nonlinear function f (\u00b7) in (1). Instead of dropping out hidden elements in each layer, we drop out convolutional kernels in each layer. From the perspective of structure compression, DeepIoT tries to prune the number of kernels used in the convolutional neural networks.\nFor the recurrent neural network, we take a multi-layer Long Short Term Memory network (LSTM) as an example. e LSTM operation with dropout can be formulated as\nz(l )[j] \u223c Bernoulli(p (l ) [j]),\u00a9\u00ab i f o g \u00aa\u00ae\u00ae\u00ae\u00ac = \u00a9\u00ab sigm sigm sigm tanh \u00aa\u00ae\u00ae\u00ae\u00acW (l ) ( h(l\u22121)t z(l\u22121) h(l )t\u22121 z (l ) ) , c(l )t = f c (l ) t\u22121 + i g,\nh(l )t = o tanh ( c(l )t ) .\n(2)\ne notation l = 1, \u00b7 \u00b7 \u00b7 ,L is the layer number and t = 1, \u00b7 \u00b7 \u00b7 ,T is the step number in the recurrent neural network. Element-wise multiplication is denoted by . Operators si\u0434m and tanh denote sigmoid function and hyperbolic tangent respectively. e vector h(l )t \u2208 Rn\n(l ) is the output of step t at layer l . e vector h(0)t = xt is the input for the whole neural network at step t . e matrix W(l ) \u2208 R4n(l )\u00d7(n(l\u22121)+n(l )) is the weight matrix at layer l . We still let p(0)[j] = 1, since DeepIoT only drops hidden elements.\nAs shown in (2), DeepIoT uses the same vector of Bernoulli random variables z(l ) to control the dropping operations among di erent time steps in each layer, while individual Bernoulli random variables are used for di erent steps in the original LSTM dropout [56]. From the perspective of structure compression, DeepIoT tries to prune the number of hidden dimensions used in LSTM blocks. e dropout operation of other recurrent neural network architectures, such as Gated Recurrent Unit (GRU), can be designed similarly."}, {"heading": "3.2 Compressor Neural Network", "text": "Now we introduce the architecture of the compressor neural network. As we described in Section 1, a hidden element in the original neural network that is connected to redundant model parameters should have a higher probability to be dropped. erefore we design\nthe compressor neural network to take the weights of an original neural network {W(l )} as inputs, learn the redundancies among these weights, and generate dropout probabilities {p(l )} for hidden elements that can be eventually used to compress the original neural network structure.\nA straightforward solution is to train an individual fullyconnected neural network for each layer in the original neural network. However, since there are interconnections among weight redundancies in di erent layers, DeepIoT uses a variant LSTM as the structure of compressor to share and use the parameter redundancy information among di erent layers.\nAccording to the description in Section 3.1, the weight in layer l of fully-connected, convolutional, or recurrent neural network can all be represented as a single matrix W(l ) \u2208 Rd (l ) f \u00d7d (l ) drop , where d (l ) drop denotes the dimension that dropout operation is applied and d (l ) f denotes the dimension of features within each dropout element. Here, we need to notice that the weight matrix of LSTM at layer l can be reshaped as W(l ) \u2208 R4\u00b7(n(l\u22121)+n(l ))\u00d7n(l ) , where d(l )drop = n (l ) and d(l )f = 4 \u00b7 (n (l\u22121) + n(l )). Hence, we take weights from the\noriginal network layer by layer,W = { W(l ) } with l = 1, \u00b7 \u00b7 \u00b7 ,L, as the input of the compressor neural network. Instead of using a vanilla LSTM as the structure of compressor, we apply a variant l-step LSTM model shown as\n\u00a9\u00ab v\u1d40i v\u1d40f v\u1d40o v\u1d40\u0434 \u00aa\u00ae\u00ae\u00ae\u00ae\u00ac =W(l )c W (l )W(l )i , \u00a9\u00ab ui uf uo u\u0434 \u00aa\u00ae\u00ae\u00ae\u00ac =Whhl\u22121, \u00a9\u00ab i f o g \u00aa\u00ae\u00ae\u00ae\u00ac = \u00a9\u00ab sigm sigm sigm tanh \u00aa\u00ae\u00ae\u00ae\u00ac \u00a9\u00ab \u00a9\u00ab vi vf vo v\u0434 \u00aa\u00ae\u00ae\u00ae\u00ac + \u00a9\u00ab ui uf uo u\u0434 \u00aa\u00ae\u00ae\u00ae\u00ac \u00aa\u00ae\u00ae\u00ae\u00ac ,\ncl = f cl\u22121 + i g, hl = o tanh ( cl ) ,\np(l ) = pt = sigm ( W(l )o hl ) ,\nz(l )[j] \u223c Bernoulli(p (l ) [j]).\n(3)\nRefer to (3), we denote dc as the dimension of the variant LSTM\nhidden state. en W(l ) \u2208 Rd (l ) f \u00d7d (l ) drop , W(l )c \u2208 R 4\u00d7d (l )f , W(l )i \u2208 R d (l )drop\u00d7dc , Wh \u2208 R4dc\u00d7dc , and W (l ) o \u2208 R\nd (l )drop\u00d7dc . e set of training parameters of the compressor neural network is denoted as\u03d5, where \u03d5 = { W(l )c ,W (l ) i ,Wh ,W (l ) o } . e matrix W(l ) is the input matrix for step l in the compressor neural network, which is also the l th layer\u2019s parameters of the original neural network in (1) or (2).\nCompared with the vanilla LSTM that requires vectorizing the original weight matrix as inputs, the variant LSTM model preserves the structure of original weight matrix and uses less learning parameters to extract the redundancy information among the dropout elements. e binary vector z(l ) is the dropout mask and probability p(l ) is the dropout probabilities for the l th layer in the original neural network used in (1) and (2), which is also the stochastic\ndropout policy learnt through observing the weight redundancies of the original neural network."}, {"heading": "3.3 Compressor-Critic Framework", "text": "In Section 3.1 and Section 3.2, we have introduced customized dropout operations applied on the original neural networks that need to be compressed and the structure of compressor neural network used to learn dropout probabilities based on parameter redundancies. In this subsection, we will discuss the detail of compressorcritic compressing process. It optimizes the original neural network and the compressor neural network in an iterative manner and enables the compressor neural network to gradually compress the original neural network with so deletion.\nWe denote the original neural network as FW (x|z), and we call it critic. It takes x as inputs and generates predictions based on binary dropout masks z and model parametersW that refer to a set of weightsW = {W(l )} . We assume that FW (x|z) is a pre-trained model. We denote the compressor neural network by z \u223c \u00b5\u03d5 (W). It takes the weights of the critic as inputs and generates the probability distribution of the mask vector z based on its own parameters \u03d5. In order to optimize the compressor to drop out hidden elements in the critic, DeepIoT follows the objective function\nL = Ez\u223c\u00b5\u03d5 [ L ( y, FW (x|z) ) ] =\n\u2211 z\u223c{0,1} |z| \u00b5\u03d5 (W) \u00b7 L ( y, FW (x|z) ) , (4)\nwhere L(\u00b7, \u00b7) is the objective function of the critic. e objective function can be interpreted as the expected loss of the original neural network over the dropout probabilities generated by the compressor.\nDeepIoT optimizes the compressor and critic in an iterative manner. It reduces the expected loss as de ned in (4) by applying the gradient descent method on compressor and critic iteratively. However, since there are discrete sampling operations, i.e., dropout operations, within the computational graph, backpropagation is not directly applicable. erefore we apply an unbiased likelihood-ratio estimator to calculate the gradient over \u03d5 [16, 37]:\n\u2207\u03d5L = \u2211 z \u2207\u03d5\u00b5\u03d5 (W) \u00b7 L ( y, FW (x|z) ) =\n\u2211 z \u00b5\u03d5 (W)\u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 L ( y, FW (x|z) ) = Ez\u223c\u00b5\u03d5 [ \u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 L ( y, FW (x|z) ) ] .\n(5)\nerefore an unbiased estimator for (5) can be \u2207\u03d5L = \u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 L (y, FW (x|z)) z \u223c \u00b5\u03d5 . (6) e gradient over W(l ) \u2208 W is\n\u2207W(l )L = \u2211 z \u00b5\u03d5 (W) \u00b7 \u2207W(l )L ( y, FW (x|z) ) = Ez\u223c\u00b5\u03d5 [ \u2207W(l )L ( y, FW (x|z) ) ] .\n(7)\nSimilarly, an unbiased estimator for (7) can be \u2207W(l )L = \u2207W(l )L (y, FW (x|z)) z \u223c \u00b5\u03d5 . (8) Now we provide more details of \u2207\u03d5L in (6). Although the estimator (6) is an unbiased estimator, it tends to have a higher variance.\nA higher variance of estimator can make the convergence slower. erefore, variance reduction techniques are typically required to make the optimization feasible in practice [20, 33].\nOne variance reduction technique is to subtract a constant c from learning signal L ( y, FW (x|z) ) in (5), which still keeps the expectation of the gradient unchanged [33]. erefore, we keep track of the moving average of the learning signal L ( y, FW (x|z)\n) denoted by c , and subtract c from the gradient estimator (6).\ne other variance reduction technique is keeping track of the moving average of the signal variance v , and divides the learning signal by max(1, \u221a v) [20].\nCombing the aforementioned two variance reduction techniques, the nal estimator (6) for gradient over \u03d5 becomes \u2207\u03d5L = \u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 L (y, FW (x|z)) \u2212 cmax(1,\u221av) z \u223c \u00b5\u03d5 , (9) where c and v are the moving average of mean and the moving average of variance of learning signal L ( y, FW (x|z) ) respectively.\nA er introducing the basic optimization process in DeepIoT, now we are ready to deliver the details of the compressing process. Compared with previous compressing algorithms that gradually delete weights without rehabilitation [23], DeepIoT applies \u201cso \u201d deletion by gradually suppressing the dropout probabilities of hidden elements with a decay factor \u03b3 \u2208 (0, 1). During the experiments in Section 5, we set \u03b3 as the default value 0.5. Since it is impossible to make the optimal compression decisions from the beginning, suppressing the dropout probabilities instead of deleting the hidden elements directly can provide the \u201cdeleted\u201d hidden elements changes to recover. is less aggressive compression process reduces the potential risk of irretrievable network damage and learning ine ciency.\nDuring the compressing process, DeepIoT gradually increases the threshold of dropout probability \u03c4 from 0 with step \u2206. e hidden elements with dropout probability, p(l )[j] that is less than the threshold \u03c4 will be given decay on dropout probability, i.e., p\u0302(l )[j] \u2190 \u03b3 \u00b7 p (l ) [j]. erefore, the operations in compressor (3) are updated as \u00a9\u00ab v\u1d40i v\u1d40f v\u1d40o v\u1d40\u0434 \u00aa\u00ae\u00ae\u00ae\u00ae\u00ac =W(l )c W (l )W(l )i , \u00a9\u00ab ui uf uo u\u0434\n\u00aa\u00ae\u00ae\u00ae\u00ac =Whhl\u22121, \u00a9\u00ab i f o g \u00aa\u00ae\u00ae\u00ae\u00ac = \u00a9\u00ab sigm sigm sigm tanh \u00aa\u00ae\u00ae\u00ae\u00ac \u00a9\u00ab \u00a9\u00ab vi vf vo v\u0434 \u00aa\u00ae\u00ae\u00ae\u00ac + \u00a9\u00ab ui uf uo u\u0434 \u00aa\u00ae\u00ae\u00ae\u00ac \u00aa\u00ae\u00ae\u00ae\u00ac ,\ncl = f cl\u22121 + i g, hl = o tanh ( cl ) ,\np(l ) = pt = sigm ( W(l )o hl ) ,\nz(l )[j] \u223c Bernoulli ( p(l )[j] \u00b7 \u03b3 1p(l )[j ]\u2264\u03c4 ) ,\n(10)\nwhere 1 is the indicator function; \u03b3 \u2208 (0, 1) is the decay factor; and \u03c4 \u2208 [0, 1) is the threshold. Since the operation of suppressing dropout probability with the pre-de ned decay factor \u03b3 is di erentiable, we can still optimize the original and the compressor neural\nnetwork through (8) and (9). e compression process will stop when the percentage of le number of parameters in FW (x|z) is smaller than a user-de ned value \u03b1 \u2208 (0, 1). e default value of \u03b1 is 0.7 during the experiments in Section 5.\nA er the compression, DeepIoT ne-tunes the compressed model FW (x|z\u0302), with a xed mask z\u0302, which is decided by the previous threshold \u03c4 . erefore the mask generation step in (10) will be updated as\nz\u0302(l )[j] = 1p (l ) [j] > \u03c4 . (11)\nWe summarize the compressor-critic compressing process of DeepIoT in Algorithm 1.\nAlgorithm 1 Compressor-predictor compressing process 1: Input: pre-trained predictor FW (x |z) 2: Initialize: compressor \u00b5\u03d5 (W) with parameter \u03d5 , moving average c , moving\naverage of variance v 3: while \u00b5\u03d5 (W) is not convergent do 4: z \u223c \u00b5\u03d5 (W) 5: c \u2190 movingAvg ( L ( y, FW (x |z)\n) ) 6: v \u2190 movingVar ( L ( y, FW (x |z)\n) ) 7: \u03d5 \u2190 \u03d5 \u2212 \u03b1 \u00b7 \u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 ( L ( y, FW (x |z) ) \u2212 c ) /max(1, \u221a v) 8: end while 9: \u03c4 = 0\n10: while the percentage of le number of parameters in FW (x |z) is larger than \u03b1 do 11: z \u223c \u00b5\u03d5 (W) 12: c \u2190 movingAvg ( L ( y, FW (x |z)\n) ) 13: v \u2190 movingVar ( L ( y, FW (x |z)\n) ) 14: \u03d5 \u2190 \u03d5 \u2212 \u03b1 \u00b7 \u2207\u03d5 log \u00b5\u03d5 (W) \u00b7 ( L ( y, FW (x |z) ) \u2212 c ) /max(1, \u221a v)\n15: W \u2190W \u2212 \u03b1 \u00b7 \u2207WL ( y, FW (x |z) ) 16: update threshold \u03c4 : \u03c4 \u2190 \u03c4 + \u2206 for every T rounds 17: end while 18: z\u0302(l )[j ] = 1p (l ) [j ] > \u03c4 19: while FW (x |z\u0302) is not convergent do 20: W \u2190W \u2212 \u03b1 \u00b7 \u2207WL ( y, FW (x |z\u0302)\n) 21: end while\ne algorithm consists of three parts. In the rst part (Line 3 to Line 8), DeepIoT freezes the critic FW (x|z) and initializes the compressor \u00b5\u03d5 (W) according to (9). In the second part (Line 9 to Line 17), DeepIoT optimizes the critic and compressor jointly with the gradients calculated by (8) and (9). At the same time, DeepIoT gradually compresses the predictor by suppressing dropout probabilities according to (10). In the nal part (Line 18 to Line 21), DeepIoT ne-tunes the critic with the gradient calculated by (8) and a deterministic dropout mask is generated according to (11). A er these three phases, DeepIoT generates a binary dropout mask z\u0302 and the ne-tuning parameters of the criticW. With these two results, we can easily obtain the compressed model of the original neural network."}, {"heading": "4 IMPLEMENTATION", "text": "In this section, we brie y describe the hardware, so ware, architecture, and performance summary of DeepIoT."}, {"heading": "4.1 Hardware", "text": "Our hardware is based on Intel Edison computing platform [1]. e Intel Edison computing platform is powered by the Intel Atom SoC dual-core CPU at 500 MHz and is equipped with 1GB memory and 4GB ash storage. For fairness, all neural network models are run solely on CPU during experiments."}, {"heading": "4.2 So ware", "text": "All the original neural networks for all sensing applications mentioned in Section 5 are trained on the workstation with NVIDIA GeForce GTX Titan X. For all baseline algorithms mentioned in Section 5, the compressing processes are also conducted on the workstation. e compressed models are exported and loaded into the ash storage on Intel Edison for experiments.\nWe installed the Ubilinux operation system on Intel Edison computing platform [2]. Far fairness, all compressed deep learning models are run through eano [49] with only CPU device on Intel Edison. e matrix multiplication operations and sparse matrix multiplication operations are optimized by BLAS and Sparse BLAS respectively during the implementation. No additional run-time optimization is applied for any compressed model and in all experiments."}, {"heading": "4.3 Architecture", "text": "Given the original neural network structure and parameters as well as the device resource information, DeepIoT can automatically generate a compressed neural network that is ready to be run on embedded devices with sensor inputs. e system rst obtains the memory information from the embedded device and sets the nal compressed size of the neural network to t in a pre-con gured fraction of available memory, from which the needed compression ratio is computed. is ratio, together with the parameters of the original model are then used to automatically generate the corresponding compressor neural network to compress the original neural network. e resulting compressed neural network is transferred to the embedded device. is model can be then called locally with a data input to decide on the output. e semantics of input and output are not known to the model."}, {"heading": "4.4 Performance Summary", "text": "We list the resource consumption numbers of all compressed models generated by DeepIoT and their corresponding original model in Table 1 with the form of (original/compressed/reduction percentage). ese models are explained in more detail in the evaluation, Section 5.\nAlthough the models generated by DeepIoT do not use sparse matrix representations, other baseline algorithms, as will be introduced in Section 5, may use sparse matrices to represent models. When the proportion of non-zero elements in the sparse matrix is larger than 20%, sparse matrix multiplications can even run slower than their non-sparse counterpart. erefore, there is a tradeo between memory consumption and execution time for sparse matrices with a large proportion of non-zero elements. In addition,\nconvolution operations conducted on CPU are also formulated and optimized as matrix multiplications, as mentioned in Section 3.1. erefore, the tradeo still exists. For all baseline algorithms in Section 5, we implement both the sparse matrix version and the non-sparse matrix version. During all the experiments with baseline algorithms, we \u201ccheat\u201d, in their favor, by choosing the version that performs be er according to the current evaluation metrics."}, {"heading": "5 EVALUATION", "text": "In this section, we evaluate DeepIoT through two sets of experiments. e rst set is motivated by the prospect of enabling future smarter embedded \u201cthings\u201d (physical objects) to interact with humans using user-friendly modalities such as visual cues, handwri en text, and speech commands, while the second evaluates human-centric context sensing, such as human activity recognition and user identi cation. In the following subsections, we rst describe the comparison baselines that are current state of the art deep neural network compression techniques. We then present the rst set of experiments that demonstrate accuracy and resource demands observed if IoT-style smart objects interacted with users via natural human-centric modalities thanks to deep neural networks compressed, for the resource-constrained hardware, with the help of our DeepIoT framework. Finally, we present the second set of experiments that demonstrate accuracy and resource demands when applying DeepIoT to compress deep neural networks trained for human-centric context sensing applications. In both cases, we show signi cant advantages in the accuracy/resource trade-o over the compared state-of-the-art compression baselines."}, {"heading": "5.1 Baseline Algorithms", "text": "We compare DeepIoT with other three baseline algorithms:\n(1) DyNS: is is a magnitude-based network pruning algorithm [21]. e algorithm prunes weights in convolutional kernels and fully-connected layer based on the magnitude. It retrains the network connections a er each pruning step and has the ability to recover the pruned weights. (2) SparseSep: is is a sparse-coding and factorization based algorithm [29]. e algorithm simpli es the fully-connected layer by nding the optimal code-book and code based on a sparse coding technique. For the convolutional layer, the algorithm compresses the model with matrix factorization methods. (3) DyNS-Ext: e previous two algorithms mainly focus on compressing convolutional and fully-connected layers. erefore we further enhance and extend the magnitude-based method used in DyNS to recurrent layers and call this algorithm DyNSExt. Just like DeepIoT, DyNS-Ext can be applied to all commonly used deep network modules, including fully-connected layers, convolutional layers, and recurrent layers. If the network structure does not contain recurrent layers, we apply DyNS instead of DyNS-Ext.\nFor magnitude-based pruning algorithms, DyNS and DyNS-Ext, hidden elements with zero input connections or zero output connections will be pruned to further compress the network structure."}, {"heading": "5.2 Supporting Human-Centric Interaction Modalities", "text": "ree basic interaction modalities among humans are text, vision, and speech. In this section, we describe three di erent experiments that test implementations of these basic interaction modalities on low-end devices using trained and compressed neural networks. We train state-of-art neural networks on traditional benchmark datasets as original models. en, we compress the original models using DeepIoT and the three baseline algorithms described above. Finally, we test the accuracy and resource consumption that result from using these compressed models on the embedded device.\n5.2.1 Handwri en digits recognition with LeNet5. e rst human interaction modality is recognizing handwri en text. In this experiment, we consider a meaningful subset of that; namely recognizing handwri en digits from visual inputs. An example application that uses this capability might be a smart wallet equipped with a camera and a tip calculator. We use MNIST2 as our training and testing dataset. e MNIST is a dataset of handwri en digits that is commonly used for training various image processing systems. It has a training set of 60000 examples, and a test set of 10000 examples.\nWe test our algorithms and baselines on the LeNet-5 neural network model. e corresponding network structure is shown in Table 2. Notice that we omit all the polling layers in Table 2 for simplicity, because they do not contain training parameters.\ne rst column of Table 2 represents the network structure of LeNet-5, where \u201cconvX\u201d represents the convolutional layer and \u201cfcY\u201d represents the fully-connected layer. e second column represents the number of hidden units or convolutional kernels we used in each layer. e third column represents the number of parameters used in each layer and in total. e original LeNet-5 is trained and achieves an error rate of 0.85% in the test dataset.\nWe then apply DeepIoT and two other baseline algorithms, DyNS and SparseSep, to compress LeNet-5. Note that, we do not use DyNSExt because the network does not contain a recurrent layer. e network statistics of the compressed model are shown in Table 2. DeepIoT is designed to prune the number of hidden units for a more e cient network structure. erefore, we illustrate both the remaining number of hidden units and the proportion of the remaining number of parameters in Table 2. Both DeepIoT and DyNS can signi cantly compress the network without hurting the nal performance. SparseSep shows an acceptable drop of performance. is is because SparseSep is designed without netuning. It has the bene t of not ne-tuning the model, but it su ers the loss in the nal performance at the same time.\ne detailed tradeo between testing accuracy and memory consumption by the model is illustrated in Fig 2a. In the zoom-in illustration, DeepIoT achieves at least \u00d72 be er tradeo compared with the two baseline methods. is is mainly due to two reasons. One is that the compressor neural network in DeepIoT obtains a global view of parameter redundancies and is therefore be er capable of eliminating them. e other is that DeepIoT prunes the hidden units directly, which enables us to represent the compressed model parameters with a small dense matrix instead of a large\n2h p://yann.lecun.com/exdb/mnist/\nTable 2: LeNet5 on MNIST dataset\nLayer Hidden Units Params DeepIoT (Hidden Units/ Params) DyNS SparseSep conv1 (5 \u00d7 5) 20 0.5K 10 50.0% 24.2% 84% conv2 (5 \u00d7 5) 50 25K 20 20.0% 20.7% 91%\nfc1 500 400K 10 0.8% 1.0% 78.75% fc2 10 5K 10 2.0% 16.34% 70.28%\ntotal 431K 1.98% 2.35% 72.39% Test Error 0.85% 0.85% 0.85% 1.05%\n0 0.2 0.4 0.6 0.8 1 20\n40\n60\n80\n100\nProportion of memory consumption by the model\nA cc ur\nac y (%\n)\nDeepIoT DyNS SparseSep\n0 0.02 0.04 0.06 0.08 0.1 20\n40\n60\n80\n100\n(a) e tradeo between testing accuracy and memory consumption by models.\n10 20 30 40 50 20\n40\n60\n80\n100\nExecution Time (ms)\nA cc\nur ac\ny (%\n)\nDeepIoT DyNS SparseSep\n6 8 10 12 14 20\n40\n60\n80\n100\n(b) e tradeo between testing accuracy and execution time.\n10 20 30 40 50 20\n40\n60\n80\n100\nInference Energy (mJ)\nA cc\nur ac\ny (%\n)\nDeepIoT DyNS SparseSep\n4 6 8 10 12 14 20\n40\n60\n80\n100\n(c) e tradeo between testing accuracy and energy consumption.\nFigure 2: System performance tradeo for LeNet5 on MNIST dataset\nsparse matrix. e sparse matrix consumes more memory for the indices of matrix elements. Algorithms such as DyNS generate models represented by sparse matrices that cause larger memory consumption.\ne evaluation results on execution time of compressed models on Intel Edison, are illustrated in Fig. 2b. We run each compressed model on Intel Edison for 5000 times and use the mean value for generating the tradeo curves.\nDeepIoT still achieves the best tradeo compared with other two baselines by a signi cant margin. DeepIoT reduces execution time by 71.4% compared with the original network without loss of accuracy. However SparseSep takes less execution time compared with DyNS at the cost of acceptable performance degradation (around 0.2% degradation on test error). e main reason for this observation is that, even though fully-connected layers occupy the most model parameters, most execution time is used by the convolution operations. SparseSep uses a matrix factorization method to covert the 2d convolutional kernel into two 1d convolutional kernels on two di erent dimensions [48]. Although this method makes low-rank assumption on convolutional kernel, it can speed up convolution operations if the size of convolutional kernel is large (5 \u00d7 5 in this experiment). It can sometimes speed up the operation even when two 1d kernels have more parameters in total compared with the original 2d kernel. However DyNS applies a magnitude-based method that prunes most of the parameters in fully-connected layers. For convolutional layers, DyNS does not reduce the number of convolutional operations e ectively, and sparse matrix multiplication is less e cient compared with regular matrix with the same number of elements. DeepIoT directly reduces the number of convolutional kernels in each layer, which reduces the\nnumber of operations in convolutional layers without making the low-rank assumption that can hurt the network performance.\ne evaluation of energy consumption on Intel Edison is illustrated in Fig. 2c. For each compressed model, we run it for 5000 times and measure the total energy consumption by a power meter. en, we calculate the expected energy consumption for one-time execution and use the one-time energy consumption to generate the tradeo curves in Fig. 2c.\nNot surprisingly, DeepIoT still achieves the best tradeo in the evaluation on energy consumption by a signi cant margin. It reduces energy consumption by 73.7% compared with the original network without loss of accuracy. Being similar as the evaluation on execution time, energy consumption focuses more on the number of operations than the model size. erefore, SparseSep can take less energy consumption compared with DyNS at the cost of acceptable loss on performance.\n5.2.2 Image recognition with VGGNet. e second human interaction modality is through vision. During this experiment, we use CIFAR103 as our training and testing dataset. e CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. ere are 50000 training images and 10000 test images. It is a standard testing benchmark dataset for the image recognition tasks. While not necessarily representative of seeing objects in the wild, it o ers a more controlled environment for an apples-to-apples comparison.\nDuring this evaluation, we use the VGGNet structure as our original network structure. It is a huge network with millions of parameters. VGGNet is chosen to show that DeepIoT is able to\n3h ps://www.kaggle.com/c/cifar-10\nTable 3: VGGNet on CIFAR-10 dataset\nLayer Hidden Units Params DeepIoT (Hidden Units/ Params) DyNS SparseSep\nconv1 (3 \u00d7 3) 64 1.7K 27 42.2% 53.9% 93.1% conv2 (3 \u00d7 3) 64 36.9K 47 31.0% 40.1% 57.3% conv3 (3 \u00d7 3) 128 73.7K 53 30.4% 52.3% 85.1% conv4 (3 \u00d7 3) 128 147.5K 68 22.0% 67.0% 56.8% conv5 (3 \u00d7 3) 256 294.9K 104 21.6% 71.2% 85.1% conv6 (3 \u00d7 3) 256 589.8K 97 15.4% 65.0% 56.8% conv7 (3 \u00d7 3) 256 589.8K 89 13.2% 61.2% 56.8% conv8 (3 \u00d7 3) 512 1.179M 122 8.3% 36.5% 85.2% conv9 (3 \u00d7 3) 512 2.359M 95 4.4% 10.6% 56.8% conv10 (3 \u00d7 3) 512 2.359M 64 2.3% 3.9% 56.8% conv11 (2 \u00d7 2) 512 1.049M 128 3.1% 3.0% 85.2% conv12 (2 \u00d7 2) 512 1.049M 112 5.5% 1.7% 85.2% conv13 (2 \u00d7 2) 512 1.049M 149 6.4% 2.4% 85.2%\nfc1 4096 2.097M 27 0.19% 2.2% 95.8% fc2 4096 16.777M 371 0.06% 0.39% 135% fc3 10 41K 10 9.1% 18.5% 90.2%\ntotal 29.7M 2.44% 7.05% 112% Test Accuracy 90.6% 90.6% 90.6% 87.1%\n0 0.2 0.4 0.6 0.8 1 1.2 40\n50\n60\n70\n80\n90\n100\nProportion of memory consumption by the model\nA cc\nur ac\ny (%\n)\nDeepIoT DyNS SparseSep\n(a) e tradeo between testing accuracy and memory consumption by models.\n0 500 1000 1500 20\n40\n60\n80\n100\nExecution Time (ms)\nA cc\nur ac\ny (%\n)\nDeepIoT DyNS SparseSep\n(b) e tradeo between testing accuracy and execution time.\n0 500 1000 1500 20\n40\n60\n80\n100\nInference Energy (mJ)\nA cc\nur ac\ny (%\n)\nDeepIoT DyNS SparseSep\n(c) e tradeo between testing accuracy and energy consumption.\nFigure 3: System performance tradeo for VGGNet on CIFAR-10 dataset\ncompress relative deep and large network structure. e detailed structure is shown Table 3.\nIn Table 3, we illustrate the detailed statistics of best compressed model that keeps the original testing accuracy for three algorithms. We clearly see that DeepIoT beats the other two baseline algorithms by a signi cant margin. is shows that the compressor in DeepIoT can handle networks with relatively deep structure. e compressor uses a variant of the LSTM architecture to share the redundancy information among di erent layers. Compared with other baselines considering only local information within each layer, sharing the global information among layers helps us learn about the parameter redundancy and compress the network structure. In addition, we observe performance loss in the compressed network generated by SparseSep. It is mainly due to the fact that SparseSep avoids the ne-tuning step. is experiment shows that ne-tuning (Line 18 to Line 21 in Algorithm 1) is important for model compression.\nFig. 3a shows the tradeo between testing accuracy and memory consumption for di erent models. DeepIoT achieves a be er performance by even a larger margin, because the model generated by DeepIoT can still be represented by a standard matrix, while other methods that use a sparse matrix representation require more memory consumption.\nFig. 3b shows the tradeo between testing accuracy and execution time for di erent models. DeepIoT still achieves the best tradeo . DeepIoT reduces 94.5% execution time without the loss of\naccuracy. Di erent from the experiment with LeNet-5 on MNIST, DyNS uses less execution time compared with SparseSep in this experiment. ere are two reasons for this. One is that VGGNet use smaller convolutional kernel compared with LeNet-5. erefore factorizing 2d kernel into two 1d kernel helps less on reducing computation time. e other point is that SparseSep fails to compress the original network into a small size while keeping the original performance. As we mentioned before, it is because SparseSep avoids the ne-tuning step.\nFig. 3c shows the tradeo between testing accuracy and energy consumption for di erent models. DeepIoT reduces energy consumption by 95.7% compared with the original VGGNet without loss of accuracy. It greatly helps us to develop a long-standing application with deep neural network in energy-constrained embedded devices.\n5.2.3 Speech recognition with deep Bidirectional LSTM. e third human interaction modality is speech. e sensing system can take the voices of users from the microphone and automatically convert what users said into text. e previous two experiments mainly focus on the network structure with convolutional layers and fully-connected layers. We see how DeepIoT and the baseline algorithms work on the recurrent neural network in this section.\n(a) e tradeo between word error rate and memory consumption by models.\n(b) e tradeo betweenword error rate and execution time.\n(c) e tradeo betweenword error rate and energy consumption.\nFigure 4: System performance tradeo for deep bidirectional LSTM on LibriSpeech ASR corpus\nIn this experiment, we use LibriSpeech ASR corpus [36] as our training and testing dataset. e LibriSpeech ASR corpus is a largescale corpus of read English speech. It consists of 460-hour training data and 2-hour testing data.\nWe choose deep bidirectional LSTM as the original model [19] in this experiment. It takes mel frequency cepstral coe cient (MFCC) features of voices as inputs, and uses two 5-layer long short-term memory (LSTM) in both forward and backward direction. e output of two LSTM are jointly used to predict the spoken text. e detailed network structure is shown in the rst column of Table 4, where \u201cLSTMf\u201d denotes the LSTM in forward direction and \u201cLSTMb\u201d denotes the LSTM in backward direction.\nTwo baseline algorithms are not applicable to the recurrent neural network, so we compared DeepIoT only with SyNS-Ext in this experiment. e word error rate (WER), de ned as the edit distance between the true word sequence and the most probable word sequence predicted by the neural network, is used as the evaluation metric for this experiment.\nWe show the detailed statistics of best compressed model that keeps the original WER in Table 4. DeepIoT achieves a signi cantly be er compression rate compared with DyNS-Ext, and the model generated by DeepIoT even has a li le improvement on WER. However, compared with the previous two examples on convolutional neural network, DeepIoT fails to compress the model to less than 5% of the original parameters in the recurrent neural network case (still a 20-fold reduction though). e main reason is that compressing recurrent networks needs to prune both the output dimension and the hidden dimension. It has been shown that dropping hidden dimension can harm the network performance [56]. However DeepIoT is still successful in compressing network to less than 10% of parameters.\nFig. 4a shows the tradeo between word error rate and memory consumption by compressed models. DeepIoT achieves around \u00d77\nbe er tradeo compared with magnitude-based method, DyNS-Ext. is means compressing recurrent neural networks requires more information about parameter redundancies within and among each layer. Compression using only local information, such as magnitude information, will cause degradation in the nal performance.\nFig. 4b shows the tradeo between word error rate and execution time. DeepIoT reduces execution time by 86.4% without degradation on WER compared with the original network. With the evaluation on Intel Edision, the original network requires 71.15 seconds in average to recognize one human speak voice example with the average length of 7.43 seconds. e compressed structure generated by DeepIoT reduces the average execution time to 9.68 seconds without performance loss, which improves responsiveness of human voice recognition.\nFig. 4c shows the tradeo between word error rate and energy consumption. DeepIoT reduces energy by 87% compared with the original network. It performs be er than DyNS-Ext by a large margin."}, {"heading": "5.3 Supporting Human-Centric Context Sensing", "text": "In addition to experiments about supporting basic human-centric interaction modalities, we evaluate DeepIoT on several humancentric context sensing applications. We compress the state-ofthe-art deep learning models [55] for these problems and evaluate the accuracy and other system performance for the compressed networks. is network structure contains all commonly used modules, including convolutional, recurrent, and fully-connected layers, which is also a good example to test the performance of compression algorithms on the combination of di erent types of neural network modules.\nTwo human-centric context sensing tasks we consider are heterogeneous human activity recognition (HHAR) and user identi cation\n(a) e tradeo between testing accuracy and memory consumption by models.\n(b) e tradeo between testing accuracy and execution time.\n(c) e tradeo between testing accuracy and energy consumption.\n(a) e tradeo between testing accuracy and memory consumption by models.\n(b) e tradeo between testing accuracy and execution time.\n(c) e tradeo between testing accuracy and energy consumption.\nFigure 6: System performance tradeo for user identi cation with biometric motion analysis\nwith biometric motion analysis (UserID). e HHAR task recognizes human activities with motion sensors, accelerometer and gyroscope. \u201cHeterogeneous\u201d means that the task is focus on the generalization ability with human who has not appeared in the training set. e UserID task identi es users during a person\u2019s daily activities such as walking, running, siting, and standing.\nIn this evaluation section, we use the dataset collected by Allan et al. [46]. is dataset contains readings from two motion sensors (accelerometer and gyroscope). Readings were recorded when users execute activities scripted in no speci c order, while carrying smartwatches and smartphones. e dataset contains 9 users, 6 activities (biking, si ing, standing, walking, climbStairup, and climbStair-down), and 6 types of mobile devices. For both tasks, accelerometer and gyroscope measurements are model inputs. However, for HHAR tasks, activities are used as labels, and for UserID tasks, users\u2019 unique IDs are used as labels.\ne original network structure is shown in the rst two columns of Table 5 and 6. Both tasks use a uni ed neural network structure as introduced in [55]. e structure contains both convolutional and recurrent layers. Since SparseSep and DyNS are not directly applicable to recurrent layers, we keep the recurrent layers unchanged while using them. In addition, we also compare DeepIoT with DyNS-Ext in this experiment.\nTable 5 and 6 illustrate the statistics of nal pruned network generated by four algorithms that have no or acceptable degradation on testing accuracy. DeepIoT is the best-performing algorithm considering the remaining number of network parameters. is is mainly due to the design of compressor network and compressor-critic framework that jointly reduce the redundancies among parameters while maintaining a global view across di erent layers. DyNS and SparseSep are two algorithms that can be only applied to the fully-connected and convolutional layers in the original structure. erefore there exists a lower bound of the le proportion of parameters, i.e., the number of parameters in recurrent layers. is lower bound is around 66%.\ne detailed tradeo s between testing accuracy and memory consumption by the models are illustrated in Fig. 5a and 6a. DeepIoT still achieves the best tradeo for sensing applications. Other than the compressor neural network providing global parameter redundancies, directly pruning hidden elements in each layer also enables DeepIoT to obtain more concise representations in matrix form, which results in less memory consumption.\ne tradeo s between system execution time and testing accuracy are shown in Fig. 5b and 6b. DeepIoT uses the least execution time when achieving the same testing accuracy compared with three baselines. It reduces execution time by around 80.8% and 71.4% in UserID and HHAR, respectively, without loss of accuracy. DyNS and DyNS-Ext achieve be er performance on time compared with SparseSep, which is di erent from the previous evaluations on LeNet-5. It is the structure of the original neural network that causes this di erence. As shown in Table 5 and 6, the original network uses 1-d lters in its structure. e matrix factorization based kernel compressing method used in SparseSep cannot help to reduce or even increase the parameter redundancies and the number of operations involved. erefore, there are constraints on the network structure when applying matrix factorization based compression algorithm. In addition, SparseSep cannot be applied to the recurrent layers in the network, which consumes a large proportion of operations during running the neural network.\ne tradeo s between energy consumption and testing accuracy are shown in Fig. 5c and 6c. DeepIoT is the best-performing algorithm for energy consumption. It reduces energy by around 83.3% and 72.2% in UserID and HHAR without loss of accuracy. Due to\nthe aforementioned problem of SparseSep on 1-d lter, redundant factorization causes more execution time and energy consumption in the experiment."}, {"heading": "6 DISCUSSION", "text": "is paper tries to apply state-of-the-art neural network models on resource-constrained embedded and mobile devices by simplifying network structure without hurting accuracy. Our solution, DeepIoT, generates a simpli ed network structure by deciding which elements to drop in each layer. is whole process requires ne-tuning (Line 18 to Line 21 in Algorithm 1). However, we argue that the netuning step should not be the obstacle in applying DeepIoT. First, all the compressing and ne-tuning steps are conducted on the workstation instead of embedded and mobile devices. We can easily apply the DeepIoT algorithm to compress and ne-tune the neural network ahead of time and then deploy the compressed model into embedded and mobile devices without any run-time processing. Second, the original training data must be easily accessible. Developers who want to apply neural networks to solve their own sensing problems will typically have access to their own datasets to ne-tune the model. For others, lots of large-scale datasets are available online, such as vision [10] and audio [15] data. Hence, we hope that for many categories of applications, ne-tuning is feasible. Last but not least, ne-tuning with the original dataset helps improve the performance of compressed models. Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that ne-tuning greatly improved the network accuracy, while reducing the network complexity.\nDeepIoT mainly focuses on structure pruning or weight pruning, which is independent from other model compression methods such as weight quantization [8, 9, 17, 22]. Although weight quantization can compress the network complexity by using limited numerical precision or clustering parameters, the compression ratio of quantization is usually less than the structure pruning methods. Weight pruning and quantization are two non-con icting methods. We can apply weight quantization a er the structure pruning step or a er any other compression algorithm. is is out of the scope of this paper. Also we do not exploit heterogeneous local device processors, such as DSPs [30], to speed up the running time during all the experiments, because this paper focuses on structure compression methods for deep neural networks instead of hardware speed-up."}, {"heading": "7 CONCLUSION", "text": "In this paper, we described DeepIoT, a compression algorithm that learns a more succinct network structure without loss of accuracy of sensing applications. DeepIoT integrates the original network with dropout learning and generates stochastic hidden elements in each layer. We also described a novel compressor neural network to learn the parameter redundancies and generate dropout probabilities for original network layers. e compressor neural network is optimized jointly with the original neural network through the compressor-critic framework. DeepIoT outperforms other baseline compression algorithms by a signi cant margin in all experiments. e compressed structure greatly reduces the resource consumption on sensing system without hurting the performance, and makes a lot of the state-of-the-art deep neural networks deployable on resource-constrained embedded and mobile devices."}], "references": [{"title": "Machine learning in wireless sensor networks: Algorithms, strategies, and applications", "author": ["M.A. Alsheikh", "S. Lin", "D. Niyato", "H.-P. Tan"], "venue": "IEEE Communications Surveys & Tutorials,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Incremental wi-\u0080 scanning for energy-e\u0081cient localization", "author": ["N. Brouwers", "M. Zuniga", "K. Langendoen"], "venue": "In Pervasive Computing and Communications (Per- Com),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Carisma: Context-aware re\u0083ective middleware system for mobile applications", "author": ["L. Capra", "W. Emmerich", "C. Mascolo"], "venue": "IEEE Transactions on so\u0087ware engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Inferring mobile trajectories using a network of binary proximity sensors", "author": ["E. Cho", "K. Wong", "O. Gnawali", "M. Wicke", "L. Guibas"], "venue": "In Sensor, Mesh and Ad Hoc Communications and Networks (SECON),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Visual inertial odometry as a sequence to sequence learning problem", "author": ["R. Clark", "S. Wang", "H. Wen", "A. Markham", "N. Trigoni. Vinet"], "venue": "In AAAI Conference on Arti\u0080cial Intelligence (AAAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Training deep neural networks with low precision multiplications", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pa\u0088ern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for e\u0081cient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Low-power wireless bus", "author": ["F. Ferrari", "M. Zimmerling", "L. Mo\u008aola", "L. \u008ciele"], "venue": "In Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv preprint arXiv:1506.02158,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A theoretically grounded application of dropout in recurrent neural networks. 2016", "author": ["Y. Gal", "Z. Ghahramani"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Ri\u008aer. Audio set: An ontology and human-labeled dataset for audio events", "author": ["J.F. Gemmeke", "D.P.W. Ellis", "D. Freedman", "A. Jansen", "W. Lawrence", "R.C. Moore", "M. Plakal"], "venue": "In Proc. IEEE ICASSP 2017,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["P.W. Glynn"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Understanding the performance of sparse matrix-vector multiplication", "author": ["G. Goumas", "K. Kourtis", "N. Anastopoulos", "V. Karakasis", "N. Koziris"], "venue": "In Parallel, Distributed and Network-Based Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Muprop: Unbiased backpropagation for stochastic neural networks", "author": ["S. Gu", "S. Levine", "I. Sutskever", "A. Mnih"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Dynamic network surgery for e\u0081cient dnns", "author": ["Y. Guo", "A. Yao", "Y. Chen"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and hu\u0082man coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Amulet: An energy-e\u0081cient, multi-application wearable platform", "author": ["J. Hester", "T. Peters", "T. Yun", "R. Peterson", "J. Skinner", "B. Golla", "K. Storer", "S. Hearndon", "K. Freeman", "S. Lord"], "venue": "In Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Vocal-diary: A voice command based ground truth collection system for activity recognition", "author": ["E. Hoque", "R.F. Dickerson", "J.A. Stankovic"], "venue": "In Proceedings of the Wireless Health 2014 on National Institutes of Health,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Network trimming: A data-driven neuron pruning approach towards e\u0081cient deep architectures", "author": ["H. Hu", "R. Peng", "Y.-W. Tai", "C.-K. Tang"], "venue": "arXiv preprint arXiv:1607.03250,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Actor-critic algorithms", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "Tracking mobile nodes using rf doppler shi\u0089s", "author": ["B. Kusy", "A. Ledeczi", "X. Koutsoukos"], "venue": "In Proceedings of the 5th international conference on Embedded networked sensor systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Sparsifying deep learning layers for constrained resource inference on wearables", "author": ["N. Lane", "S. Bha\u008aacharya"], "venue": "In Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Deepx: A so\u0089ware accelerator for low-power deep learning inference on mobile devices", "author": ["N.D. Lane", "S. Bha\u008aacharya", "P. Georgiev", "C. Forlivesi", "L. Jiao", "L. Qendro", "F. Kawsar"], "venue": "In Information Processing in Sensor Networks (IPSN),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Deepear: robust smartphone audio sensing in unconstrained acoustic environments using deep learning", "author": ["N.D. Lane", "P. Georgiev", "L. Qendro"], "venue": "In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Optimal brain damage", "author": ["Y. LeCun", "J.S. Denker", "S.A. Solla", "R.E. Howard", "L.D. Jackel"], "venue": "In NIPs,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M.A. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Musicalheart: A hearty way of listening to music", "author": ["S. Nirjon", "R.F. Dickerson", "Q. Li", "P. Asare", "J.A. Stankovic", "D. Hong", "B. Zhang", "X. Jiang", "G. Shen", "F. Zhao"], "venue": "In Proceedings of the 10th ACM Conference on Embedded Network Sensor Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Policy gradient methods for robotics", "author": ["J. Peters", "S. Schaal"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Towards multimodal deep learning for activity recognition on mobile devices", "author": ["V. Radu", "N.D. Lane", "S. Bha\u008aacharya", "C. Mascolo", "M.K. Marina", "F. Kawsar"], "venue": "In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Leveraging user activities and mobile robots for semantic mapping and user localization", "author": ["S. Rosa", "X. Lu", "H. Wen", "N. Trigoni"], "venue": "In Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2017}, {"title": "Contactless sensing of appliance state transitions through variations in electromagnetic \u0080elds", "author": ["A. Rowe", "M. Berges", "R. Rajkumar"], "venue": "In Proceedings of the 2nd ACM workshop on embedded sensing systems for energy-e\u0081ciency in building,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Snow: Sensor network over white spaces", "author": ["A. Saifullah", "M. Rahman", "D. Ismail", "C. Lu", "R. Chandra", "J. Liu"], "venue": "In Proceedings of the International Conference on Embedded Networked Sensor Systems (ACM SenSys),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "A competition to push the dependability of low-power wireless protocols to the edge", "author": ["M. Schuss", "C.A. Boano", "M. Weber", "K. Roemer"], "venue": "In Proceedings of the 14th International Conference on Embedded Wireless Systems and Networks (EWSN). Uppsala,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2017}, {"title": "Face recognition on smartphones via optimised sparse representation classi\u0080cation", "author": ["Y. Shen", "W. Hu", "M. Yang", "B. Wei", "S. Lucey", "C.T. Chou"], "venue": "In Information Processing in Sensor Networks,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schri\u008awieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from over\u0080\u008aing", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1929}, {"title": "Smart devices are di\u0082erent: Assessing and mitigatingmobile sensing heterogeneities for activity recognition", "author": ["A. Stisen", "H. Blunck", "S. Bha\u008aacharya", "T.S. Prentow", "M.B. Kj\u00e6rgaard", "A. Dey", "T. Sonne", "M.M. Jensen"], "venue": "In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Pandaa: physical arrangement detection of networked devices through ambient-sound awareness", "author": ["Z. Sun", "A. Purohit", "K. Chen", "S. Pan", "T. Pering", "P. Zhang"], "venue": "In Proceedings of the 13th international conference on Ubiquitous computing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Convolutional neural networks with low-rank regularization", "author": ["C. Tai", "T. Xiao", "Y. Zhang", "X. Wang"], "venue": "arXiv preprint arXiv:1511.06067,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Corlayer: A transparent link correlation layer for energy e\u0081cient broadcast", "author": ["S. Wang", "S.M. Kim", "Y. Liu", "G. Tan", "T. He"], "venue": "In Proceedings of the 19th annual international conference on Mobile computing & networking,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Deep reinforcement learning for dynamic multichannel access", "author": ["S. Wang", "H. Liu", "P.H. Gomes", "B. Krishnamachari"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2017}, {"title": "Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks", "author": ["H. Wen", "S. Wang", "R. Clark", "N. Trigoni"], "venue": "International Conference on Robotics and Automation,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2017}, {"title": "See-through walls: Motion tracking using variancebased radio tomography networks", "author": ["J. Wilson", "N. Patwari"], "venue": "IEEE Transactions on Mobile Computing,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "Detecting driver phone use leveraging car speakers", "author": ["J. Yang", "S. Sidhom", "G. Chandrasekaran", "T. Vu", "H. Liu", "N. Cecan", "Y. Chen", "M. Gruteser", "R.P. Martin"], "venue": "In Proceedings of the 17th annual international conference on Mobile computing  and networking,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Deepsense: a uni\u0080ed deep learning framework for time-series mobile sensing data processing", "author": ["S. Yao", "S. Hu", "Y. Zhao", "A. Zhang", "T. Abdelzaher"], "venue": "In Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Commi\u008aee,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2017}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Ocrdroid: A framework to digitize text using mobile phones", "author": ["M. Zhang", "A. Joshi", "R. Kadmawala", "K. Dantu", "S. Poduri", "G.S. Sukhatme"], "venue": "In International Conference on Mobile Computing, Applications, and Services,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 91, "endOffset": 99}, {"referenceID": 53, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 91, "endOffset": 99}, {"referenceID": 2, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 117, "endOffset": 136}, {"referenceID": 32, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 117, "endOffset": 136}, {"referenceID": 37, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 117, "endOffset": 136}, {"referenceID": 44, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 117, "endOffset": 136}, {"referenceID": 50, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 117, "endOffset": 136}, {"referenceID": 3, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 168, "endOffset": 183}, {"referenceID": 25, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 168, "endOffset": 183}, {"referenceID": 40, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 168, "endOffset": 183}, {"referenceID": 49, "context": "Present-day sensing applications cover a broad range of areas including human interactions [25, 57], context sensing [5, 35, 40, 47, 54], object detection and tracking [6, 28, 43, 53].", "startOffset": 168, "endOffset": 183}, {"referenceID": 28, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 96, "endOffset": 111}, {"referenceID": 36, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 96, "endOffset": 111}, {"referenceID": 48, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 96, "endOffset": 111}, {"referenceID": 51, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 96, "endOffset": 111}, {"referenceID": 35, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 140, "endOffset": 148}, {"referenceID": 51, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 140, "endOffset": 148}, {"referenceID": 28, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 173, "endOffset": 181}, {"referenceID": 51, "context": "\u008cese models have shown signi\u0080cant improvements on audio sensing [31], tracking and localization [7, 39, 52, 55], human activity recognition [38, 55], and user identi\u0080cation [31, 55].", "startOffset": 173, "endOffset": 181}, {"referenceID": 18, "context": "Previous illuminating studies on neural network compression sparsify large dense parameter matrices into large sparse matrices [21, 23, 29].", "startOffset": 127, "endOffset": 139}, {"referenceID": 20, "context": "Previous illuminating studies on neural network compression sparsify large dense parameter matrices into large sparse matrices [21, 23, 29].", "startOffset": 127, "endOffset": 139}, {"referenceID": 26, "context": "Previous illuminating studies on neural network compression sparsify large dense parameter matrices into large sparse matrices [21, 23, 29].", "startOffset": 127, "endOffset": 139}, {"referenceID": 15, "context": "A small dense matrix does not require additional storage for element indices and is e\u0081ciently optimized for processing [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 42, "context": "dropout [45].", "startOffset": 8, "endOffset": 12}, {"referenceID": 42, "context": "In fully-connected neural networks, neurons are dropped in each layer [45]; in convolutional neural networks, \u0080lters are dropped in each layer [13]; and in recurrent neural networks, dimensions are reduced in each layer [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "In fully-connected neural networks, neurons are dropped in each layer [45]; in convolutional neural networks, \u0080lters are dropped in each layer [13]; and in recurrent neural networks, dimensions are reduced in each layer [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "In fully-connected neural networks, neurons are dropped in each layer [45]; in convolutional neural networks, \u0080lters are dropped in each layer [13]; and in recurrent neural networks, dimensions are reduced in each layer [14].", "startOffset": 220, "endOffset": 224}, {"referenceID": 24, "context": "\u008ce compressor-critic framework emulates the idea of the well-known actor-critic algorithm from reinforcement learning [27], optimizing two networks in an iterative manner.", "startOffset": 118, "endOffset": 122}, {"referenceID": 18, "context": "We compare DeepIoT with other state-of-the-art magnitudebased [21] and sparse-coding-based [29] neural network compression methods.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": "We compare DeepIoT with other state-of-the-art magnitudebased [21] and sparse-coding-based [29] neural network compression methods.", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "reduced the energy consumption of Wi-Fi based localization with an incremental scanning strategy [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "proposed an ultra-low-power hardware architecture and a companion so\u0089ware framework for energy-e\u0081cient sensing system [24].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "focused on low-power wireless communication protocols [12, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 39, "context": "focused on low-power wireless communication protocols [12, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 46, "context": "enabled energy e\u0081cient reliable broadcast by considering poorly correlated links [50].", "startOffset": 81, "endOffset": 85}, {"referenceID": 38, "context": "designed a scalable and energy-e\u0081cient wireless sensor network (WSN) over white spaces [41].", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "discussed about data compression in WSN with machine learning techniques [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 20, "context": "proposed a magnitudebased compression method with \u0080ne-tuning, which illustrated promising compression results [23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "proposed a compression algorithm with connection splicing, which provided the chance of rehabilitation with a certain threshold [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "methods with \u0080ne-tunning to approximate the convolutional operations in order to reduce the neural network execution time [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 26, "context": "applied sparse coding based and matrix factorization based method to reduce complexity of fully-connected layer and convolutional layer respectively [29].", "startOffset": 149, "endOffset": 153}, {"referenceID": 31, "context": "With the aid of deep neural networks, reinforcement leaning has achieved great success on Atari games [34], Go chess [44], and multichannel access [51].", "startOffset": 102, "endOffset": 106}, {"referenceID": 41, "context": "With the aid of deep neural networks, reinforcement leaning has achieved great success on Atari games [34], Go chess [44], and multichannel access [51].", "startOffset": 117, "endOffset": 121}, {"referenceID": 47, "context": "With the aid of deep neural networks, reinforcement leaning has achieved great success on Atari games [34], Go chess [44], and multichannel access [51].", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "\u008ce basic idea is that we regard neural networks with dropout operations as bayesian neural networks with Bernoulli variational distributions [13, 14, 45].", "startOffset": 141, "endOffset": 153}, {"referenceID": 11, "context": "\u008ce basic idea is that we regard neural networks with dropout operations as bayesian neural networks with Bernoulli variational distributions [13, 14, 45].", "startOffset": 141, "endOffset": 153}, {"referenceID": 42, "context": "\u008ce basic idea is that we regard neural networks with dropout operations as bayesian neural networks with Bernoulli variational distributions [13, 14, 45].", "startOffset": 141, "endOffset": 153}, {"referenceID": 42, "context": "In the original dropout method, the success probabilities of p(l ) [j] can be set to the same constant p for all hidden units [45], but DeepIoT uses the Bernoulli random variable with individual success probabilities for di\u0082erent hidden units in order to compress the neural network structure in a \u0080ner granularity.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "For the convolutional neural networks, the basic fully-connected operation is replaced by the convolution operation [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 52, "context": "As shown in (2), DeepIoT uses the same vector of Bernoulli random variables z(l ) to control the dropping operations among di\u0082erent time steps in each layer, while individual Bernoulli random variables are used for di\u0082erent steps in the original LSTM dropout [56].", "startOffset": 259, "endOffset": 263}, {"referenceID": 13, "context": "\u008cerefore we apply an unbiased likelihood-ratio estimator to calculate the gradient over \u03c6 [16, 37]:", "startOffset": 90, "endOffset": 98}, {"referenceID": 34, "context": "\u008cerefore we apply an unbiased likelihood-ratio estimator to calculate the gradient over \u03c6 [16, 37]:", "startOffset": 90, "endOffset": 98}, {"referenceID": 17, "context": "\u008cerefore, variance reduction techniques are typically required to make the optimization feasible in practice [20, 33].", "startOffset": 109, "endOffset": 117}, {"referenceID": 30, "context": "\u008cerefore, variance reduction techniques are typically required to make the optimization feasible in practice [20, 33].", "startOffset": 109, "endOffset": 117}, {"referenceID": 30, "context": "expectation of the gradient unchanged [33].", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "moving average of the signal variance v , and divides the learning signal by max(1, \u221a v) [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Compared with previous compressing algorithms that gradually delete weights without rehabilitation [23], DeepIoT applies \u201cso\u0089\u201d deletion by gradually suppressing the dropout probabilities of hidden elements with a decay factor \u03b3 \u2208 (0, 1).", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "(1) DyNS: \u008cis is a magnitude-based network pruning algorithm [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "(2) SparseSep: \u008cis is a sparse-coding and factorization based algorithm [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 45, "context": "SparseSep uses a matrix factorization method to covert the 2d convolutional kernel into two 1d convolutional kernels on two di\u0082erent dimensions [48].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "In this experiment, we use LibriSpeech ASR corpus [36] as our training and testing dataset.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "We choose deep bidirectional LSTM as the original model [19] in this experiment.", "startOffset": 56, "endOffset": 60}, {"referenceID": 52, "context": "It has been shown that dropping hidden dimension can harm the network performance [56].", "startOffset": 82, "endOffset": 86}, {"referenceID": 51, "context": "We compress the state-ofthe-art deep learning models [55] for these problems and evaluate the accuracy and other system performance for the compressed networks.", "startOffset": 53, "endOffset": 57}, {"referenceID": 43, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Both tasks use a uni\u0080ed neural network structure as introduced in [55].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "For others, lots of large-scale datasets are available online, such as vision [10] and audio [15] data.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "For others, lots of large-scale datasets are available online, such as vision [10] and audio [15] data.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that \u0080ne-tuning greatly improved the network accuracy, while reducing the network complexity.", "startOffset": 53, "endOffset": 69}, {"referenceID": 20, "context": "Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that \u0080ne-tuning greatly improved the network accuracy, while reducing the network complexity.", "startOffset": 53, "endOffset": 69}, {"referenceID": 23, "context": "Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that \u0080ne-tuning greatly improved the network accuracy, while reducing the network complexity.", "startOffset": 53, "endOffset": 69}, {"referenceID": 29, "context": "Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that \u0080ne-tuning greatly improved the network accuracy, while reducing the network complexity.", "startOffset": 53, "endOffset": 69}, {"referenceID": 45, "context": "Previous work on magnitude-based compression methods [21, 23, 26, 32], factorization-based compression methods [48], and experiments in our paper have all shown that \u0080ne-tuning greatly improved the network accuracy, while reducing the network complexity.", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "DeepIoT mainly focuses on structure pruning or weight pruning, which is independent from other model compression methods such as weight quantization [8, 9, 17, 22].", "startOffset": 149, "endOffset": 163}, {"referenceID": 6, "context": "DeepIoT mainly focuses on structure pruning or weight pruning, which is independent from other model compression methods such as weight quantization [8, 9, 17, 22].", "startOffset": 149, "endOffset": 163}, {"referenceID": 14, "context": "DeepIoT mainly focuses on structure pruning or weight pruning, which is independent from other model compression methods such as weight quantization [8, 9, 17, 22].", "startOffset": 149, "endOffset": 163}, {"referenceID": 19, "context": "DeepIoT mainly focuses on structure pruning or weight pruning, which is independent from other model compression methods such as weight quantization [8, 9, 17, 22].", "startOffset": 149, "endOffset": 163}, {"referenceID": 27, "context": "Also we do not exploit heterogeneous local device processors, such as DSPs [30], to speed up the running time during all the experiments, because this paper focuses on structure compression methods for deep neural networks instead of hardware speed-up.", "startOffset": 75, "endOffset": 79}], "year": 2017, "abstractText": "Recent advances in deep learning motivate the use of deep neutral networks in sensing applications, but their excessive resource needs on constrained embedded devices remain an important impediment. A recently explored solution space lies in compressing (approximating or simplifying) deep neural networks in some manner before use on the device. We propose a new compression solution, called DeepIoT, that makes two key contributions in that space. First, unlike current solutions geared for compressing speci\u0080c types of neural networks, DeepIoT presents a uni\u0080ed approach that compresses all commonly used deep learning structures for sensing applications, including fully-connected, convolutional, and recurrent neural networks, as well as their combinations. Second, unlike solutions that either sparsify weight matrices or assume linear structure within weight matrices, DeepIoT compresses neural network structures into smaller dense matrices by \u0080nding the minimum number of non-redundant hidden elements, such as \u0080lters and dimensions required by each layer, while keeping the performance of sensing applications the same. Importantly, it does so using an approach that obtains a global view of parameter redundancies, which is shown to produce superior compression. \u008ce compressed model generated by DeepIoT can directly use existing deep learning libraries that run on embedded and mobile systems without further modi\u0080cations. We conduct experiments with \u0080ve di\u0082erent sensing-related tasks on Intel Edison devices. DeepIoT outperforms all compared baseline algorithms with respect to execution time and energy consumption by a signi\u0080cant margin. It reduces the size of deep neural networks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to 94.5%, and decrease energy consumption by 72.2% to 95.7%. \u008cese improvements are achieved without loss of accuracy. \u008ce results underscore the potential of DeepIoT for advancing the exploitation of deep neural networks on resource-constrained embedded devices. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\u0080t or commercial advantage and that copies bear this notice and the full citation on the \u0080rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi\u008aed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci\u0080c permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, Washington, DC, USA \u00a9 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . .$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn ACM Reference format: Shuochao Yao, Yiran Zhao, Aston Zhang, Lu Su, and Tarek Abdelzaher. 2016. Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework. In Proceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference\u201917), 14 pages. DOI: 10.1145/nnnnnnn.nnnnnnn", "creator": "LaTeX with hyperref package"}}}