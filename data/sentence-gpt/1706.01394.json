{"id": "1706.01394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Multi-Observation Elicitation", "abstract": "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts. We also explore other areas that should be discussed in this article.", "histories": [["v1", "Mon, 5 Jun 2017 16:07:26 GMT  (2057kb,D)", "http://arxiv.org/abs/1706.01394v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sebastian casalaina-martin", "rafael frongillo", "tom morgan", "bo waggoner"], "accepted": false, "id": "1706.01394"}, "pdf": {"name": "1706.01394.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sebastian Casalaina-Martin", "Rafael Frongillo", "Bo Waggoner"], "emails": ["CASA@MATH.COLORADO.EDU", "RAF@COLORADO.EDU", "TDMORGAN@SEAS.HARVARD.EDU", "BWAG@SEAS.UPENN.EDU"], "sections": [{"heading": "1. Introduction", "text": "In machine learning and statistics, empirical risk minimization (ERM) is a dominant inference technique, wherein a model is chosen which minimizes some loss function over a data set. As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.1\nA natural question, which is still open in the vector-valued case, is the following: for which conditional statistics do there exist loss functions which elicit them? Positive examples include the mean, median, other quantiles, moments, and several others. Perhaps surprisingly, however, there are negative examples as well: it is well-known that the variance is not elicitable, meaning there is no loss function for which minimizing the loss will yield the variance of the data or distribution.\n1. There are also contributions from microeconomics, and crowdsourcing in particular, where one wishes to incentivize humans rather than algorithms, but the mathematics is the same.\nc\u00a9 2017 S. Casalaina-Martin, R. Frongillo, T. Morgan & B. Waggoner.\nar X\niv :1\n70 6.\n01 39\n4v 1\n[ cs\n.L G\n] 5\nThe usual approach to dealing with non-elicitable statistics is called indirect elicitation: elicit other conditional statistics from which one can compute the desired statistic. For example, the variance of a distribution can be written as (2nd moment) - (1st moment)2, and as mentioned above, moments are elicitable. The question of how many such auxiliary statistics are required gives rise to the concept of elicitation complexity; since the variance cannot be elicited with one but can with two, we say it is 2-elicitable (Lambert et al., 2008; Frongillo and Kash, 2015c).\nIn this paper, we explore an alternative approach to dealing with non-elicitable statistics, by allowing the loss function to depend on multiple data points simultaneously. In the language of property elicitation, this corresponds to loss functions such as `(r, y1, y2) which judge the \u201ccorrectness\u201d of the report r based on two (or more) observations y1 and y2. Assuming these observations are drawn independently from the same distribution, this intuitively gives the loss function more power, and could potentially render previously non-elicitable statistics elicitable. In fact, the variance is one such example: if y1 and y2 are both drawn i.i.d. from p, it is easy to see that 12(y1\u2212 y2) 2 will be an unbiased estimator for the variance of p, hence `(r, y1, y2) = (r\u2212 12(y1\u2212y2) 2)2 elicits the variance for the usual reason that squared error elicits expected values. Examples of settings where such i.i.d. observations are readily obtained include: active learning, uncertainty quantification & robust engineering design (Beyer and Sendhoff, 2007), and replication of scientific experiments.\nBeyond the variance, are there other non-elicitable statistics which we can elicit with multiple i.i.d. observations? Moreover, what is the tradeoff between the number of observations and the number of reports? One would expect the elicitation complexity, in the usual number-of-reports sense, to drop as observations are added, but how fast is unclear. Indeed, we will see several examples where the complexity drops dramatically, such a the k-norm of the distribution p. In Section 4 we develop new techniques to prove complexity bounds using algebraic geometry, which show for example that the complexity of the k-norm drops from the support size of p (minus 1) with 1 observation, to 1 with k observations. We call the feasible (# reports, # observations) pairs the elicitation frontier, for which the given statistic is elicitable, a concept we explore in Section 5.\nFinally, in Section 6 we apply multi-observation elicitation to regression. Traditional elicitation complexity expresses a conditional statistic \u0393 as a link of other statistics, but as we illustrate, situations can arise where these other statistics have a much more complicated relationship with the covariates than \u0393 does. We give an example where fitting a model to the conditional variance directly (using nearby data points as proxies for i.i.d. observations) is much better than fitting separate models to the conditional first and second moments and combining these to obtain the variance."}, {"heading": "1.1. Related work", "text": "Our work is inspired in part by Frongillo et al. (2015) which proposes a way to elicit the confidence (inverse of variance) of an agent\u2019s estimate of the bias of a coin by simply flipping it twice. In our terminology, this follows from the fact that the variance is (1, 2)-elicitable. Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed."}, {"heading": "2. Preliminaries", "text": "We are interested in a space Y from which observations y are drawn, which will be a finite set unless otherwise specified. We will denote by P \u2286 \u2206Y a set of probability distributions of interest. (Generally in this paper, P is simply the entire simplex.) We refer to the set \u2206Ym of all distributions\non m outcomes as the m-product space. To capture the assumption that we may collect m \u2208 {1, 2, . . .} observations which are each i.i.d. from the same distribution p \u2208 \u2206Y , we will write pm \u2208 \u2206Ym to denote their joint distribution, pm(y1, . . . , ym) = \u220f i p(yi). The set of all such distributions is denoted Pm = {pm : p \u2208 P} \u2286 \u2206Ym , which we will think of as a manifold in the m-product space.\nWith this notation in hand, we can define the central concepts in elicitation complexity in our context. Properties include any typical statistic,2 for instance, the mean when Y \u2286 R is the property \u0393(p) = \u2211 y p(y)y.\nDefinition 1 (Property) A property is a function \u0393 : P \u2192 R, whereR \u2286 Rk for some k \u2265 1.\nIntuitively, properties represent the information desired about the data or underlying distribution. R is sometimes called the report space. The central notion of property elicitation is the relationship between a loss function ` and the minimizer of its expected loss. If this minimizer is a particular property \u0393, we say ` elicits \u0393. We simply extend this usual definition to allow for multiple observations in the expected loss.\nDefinition 2 (Loss function, elicits) An m-observation loss function is a function ` : R\u00d7 Ym \u2192 R, where `(r, y1, . . . , ym) is the loss for prediction r \u2208 R scored against realized observations yi \u2208 Y . We say ` (directly) elicits a property \u0393 : P \u2192 R if for all p \u2208 P we have {\u0393(p)} = argminr\u2208R E(y1,...,ym)\u223cpm [`(r, y1, . . . , ym)].\nIt is useful to consider a property in terms of its level sets, the set of distributions sharing the same particular value of the property. For example, when the property is the mean of a distribution on {1, 2, 3, 4}, both p = ( 1 2 , 0, 0, 1 2 ) and p = ( 0, 12 , 1 2 , 0 ) lie in the level set \u03932.5.\nDefinition 3 (Level set) A level set \u0393r of a property \u0393 : P \u2192 R is, for r \u2208 R, the set of distributions with property r, i.e. \u0393r = {p \u2208 P | \u0393(p) = r}.\nAn important technical condition on a property, and one which we will need for the notion of indirect elicitability, is that it be identifiable, meaning that its level sets can be described by linear equalities.\nDefinition 4 (Identifiable) A property \u0393 : P \u2192 R, with R \u2286 Rk, is identifiable with m observations if there exists some V : R\u00d7 Ym \u2192 Rk such that \u0393(p) = r \u21d0\u21d2 Epm [V (r,y)] = 0 \u2208 Rk, where y = (y1, . . . , ym) is drawn from pm. We also say it is m-identifiable.\nIdentifiability is a geometric restriction on properties that is intuitively similar to continuity of the property (cf. Lambert et al. (2008); Steinwart et al. (2014)). Technically, observe that differentiable loss functions generally elicit an identifiable property, as any local optimum should have\u2211\ni \u2202 \u2202ri ` (r,y) = 0, meaning that the gradient of ` itself gives an identification function. Following Frongillo and Kash (2015a), we will often assume that properties are identifiable. Notice that any property can be \u201cindirectly\u201d elicited by using a proper scoring rule, which elicits the entire distribution, and then computing the property from the distribution. But this requires a report of dimension |Y|\u2212 1, whereas to indirectly elicit the variance of y, for example, requires just\n2. As defined, statistics like the median would not be included unless restrictions were placed on P for them to be single-valued (distributions in general may have multiple medians); we may instead extend our definition to include set-valued statistics, which would not substantially alter our results, and in fact we do lift this restriction in Section 3.1.\ntwo reports, e.g. r1 = Ey and r2 = Ey2, along with a \u201clink function\u201d \u03c8(r) = r2\u2212 r21. The question of elicitation complexity, studied by Lambert et al. (2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest \u0393 via some elicitable \u0393\u0302 : P \u2192 Rd; one hopes that d is much smaller than |Y|. Here we augment this question by another degree of freedom: how many dimensions d, and observations m, are needed to indirectly elicit \u0393?\nDefinition 5 ((d,m)-elicitable) A property \u0393 : P \u2192 R is (d,m)-elicitable if there exists a ddimensional and identifiable property \u0393\u0302 : P \u2192 R\u0302 where R\u0302 \u2286 Rd, an m-observation loss function ` : R\u0302 \u00d7 Ym \u2192 R, and a \u201clink\u201d function \u03c8 : R\u0302 \u2192 R, such that\n1. ` directly elicits \u0393\u0302, and 2. \u0393(p) = \u03c8 ( \u0393\u0302(p) ) .\nThe elicitation frontier of \u0393 is the set of (d,m) such that \u0393 is (d,m)-elicitable, but neither (d\u22121,m)nor (d,m\u2212 1)-elicitable.\nWe may say that a property\u2019s \u201creport complexity\u201d is d if (d, 1) lies on its frontier, and its \u201cobservation complexity\u201d is m if (1,m) does."}, {"heading": "2.1. Illustrative example", "text": "Recall our observation that the variance is not (1, 1)-elicitable, and the \u201ctraditional\u201d fix is to utilize (2, 1)-elicitability: minimize a loss function over two dimensions (say first and second moments), mapping the result to the variance via a link function. We observed instead that it is possible to utilize (1, 2)-elicitability: minimize a loss function that takes two observations over a single scalar, the variance itself. Can this tradeoff be more extreme? In particular, are there cases where additional observations drastically decrease the report complexity? Consider the 2-norm of a distribution: \u0393(p) = \u2016p\u20162 = \u221a\u2211 y p(y)\n2. We show in Section 5.2 that \u2016p\u20162 has report complexity |Y| \u2212 1 (where Y is the outcome set) for 1 observation \u2013 no single-observation loss function can do better than solving for the entire distribution. However, recall that \u2016p\u201622 = \u2211 y p 2 y = Pr[y1 = y2] for two i.i.d. observations y1, y2, or in other words, \u2016p\u201622 = Ep1{y1 = y2}. The two-norm is actually elicitable with two observations and a single dimension using e.g. loss function `(r, y1, y2) = (r \u2212 1{y1 = y2})2, then simply computing \u2016p\u20162 = \u221a r. In other words, the two-norm\u2019s elicitation frontier on Y consists of the points (|Y| \u2212 1, 1) and (1, 2). The goal for this paper is to investigate the (algebraic-)geometric reasons underpinning why a property might have low or high observation complexity, as well as providing general results and examples based on these ideas. We next introduce the geometric foundations for this investigation."}, {"heading": "3. Geometric Fundamentals", "text": "The most basic (yet powerful) lower bound in property elicitation says that elicitable properties\u2019 level sets must be convex sets (Lambert et al., 2008). Indeed, this is used to prove the variance is not (1,1)-elicitable; but the variance is elicitable with two observations. The geometry is not \u201cbroken\u201d here, but merely lives in a higher-dimensional space. When reasoning about eliciting a property \u0393 : P \u2192 R using m observations, it often useful to instead think of eliciting the property using a single random draw from a distribution on m-tuples of outcomes.\nRemark 6 Since P is isomorphic to Pm, a property \u0393 : P \u2192 R is directly elicitable with m observations if and only if the induced property \u0393m : Pm \u2192 R is directly elicitable with 1 observation. In particular, a sufficient condition for (d,m)-elicitability of \u0393 is that there exists some (d, 1)-elicitable \u0393\u2032 : \u2206Ym \u2192 R that coincides with \u0393m on Pm. One can elicit \u0393 using the same loss that elicits \u0393\u2032, treating the m-tuple of observations as a single draw from the larger space.\nThis gives us one initial way to demonstrate that a property is elicitable withm observations. For example, the loss function `(r, a, b) = ( r \u2212 12(a\u2212 b) 2 )2 elicits the variance with two observations a, b, but if we consider distributions on all of Y \u00d7Y , including non-i.i.d. distributions, it actually is still a valid loss function eliciting a property that coincides with the variance when a, b are i.i.d. To see this, just note that it still elicits an expectation: \u2211 a,b p \u2032(a, b)12(a\u2212 b) 2 where p\u2032 is a distribution on R2.\nHowever, considering elicitation on the larger space \u2206Ym does not resolve the problem in either the necessary or sufficient directions. First, Pm is not a convex set for m > 1, so conditions on the convexity of level sets do not naturally extend here. An example of this is shown in Figure 1. Second, coming up with an \u201cextended property\u201d may be difficult or non-obvious. For example, it is not so clear whether the above loss function elicits anything natural on \u2206Y2 (it is not the covariance, for instance, which is zero for i.i.d. distributions). More fundamentally, it is not clear whether such extensions should generally exist. (Proving or constructing a counterexample is an interesting open problem.) In general, we hope to be able to accomplish much more by restricting to Pm because it is only a tiny |Y|-dimensional manifold in a |Y|m-dimensional space.\nA tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some \u201cextension\u201d of\nthat property on the convex hull of that set. So while the higher-dimensional approach is helpful, it does not preclude reasoning about the space Pm as a manifold inside \u2206Ym .\nMost significantly, Pm is not a convex space, which makes lower bounds on elicitation complexity nontrivial as well. However, the result of Frongillo and Kash (2014) shows that it suffices to provide lower bounds for elicitation on the convex hull of Pm, which we will denote conv(Pm). Quite naturally then, we explore what leverage we can gain by reasoning about conv(Pm).\nTheorem 7 The property \u0393 : P \u2192 R is not directly elicitable with m observations if there exists r1, r2 \u2208 \u0393(P), p1,1, . . . , p1,k1 \u2208 \u0393r1 , p2,1, . . . , p2,k2 \u2208 \u0393r2 , \u03bb1,1, . . . , \u03bb1,k1 \u2208 [0, 1] and \u03bb2,1, . . . , \u03bb2,k2 \u2208 [0, 1] such that r1 6= r2, \u2211k1 i=1 \u03bb1,i = 1, \u2211k2 i=1 \u03bb2,i = 1 and\nk1\u2211 i=1 \u03bb1,ip m 1,i = k2\u2211 i=1 \u03bb2,ip m 2,i.\nIn other words, a property is not elicitable if there is a convex combination of one of its level sets in the m-product space that equals a convex combination of another one of its level sets in the m-product space.\nTheorem 7 allows us to prove for example that the fourth central moment is not directly elicitable with two observations. Consider a Bernoulli random variable Y \u223c p, then two of the level sets of the fourth central moment \u0393(p) = EY\u223cp[(Y \u2212EY\u223cp[Y ])4] are given in Figure 2. When we project these level sets into the 2-product space we can easily find a pair of points from each level set whose connecting lines intersects in conv(\u2206Ym). These lines are convex combinations of points in the same level set, so by Theorem 7 the lines\u2019 intersection implies that \u0393 is not directly elicitable with two observations."}, {"heading": "3.1. Finite Properties", "text": "Finite properties are those where R, the range of \u0393, is a finite set. This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009). In this section, we must allow \u0393 : P \u21d2 R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for \u201cboundary\u201d cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = \u2206Y , and \u0393(p) must be nonempty.\nWe are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex \u2206Y . In our setting, a Voronoi diagram is specified by a finite set of points {xr : r \u2208 R} \u2286 RY m , with each cell Tr = {x : \u2016x \u2212 xr\u2016 \u2264 \u2016x \u2212 xr\u2032\u2016\u2200r\u2032 \u2208 R} consisting of those points in RYm closest in Euclidean distance to xr. Using the geometric constructions above, we can simply apply the main result of Lambert (2011) to finite properties in the m-product space; the result is a characterization of elicitable finite properties with m observations.\nCorollary 8 A finite property \u0393 : \u2206Y \u21d2 R is directly elicitable with m samples if and only if there exists a Voronoi diagram in RYm with {xr : r \u2208 R} satisfying \u0393mr = Tr \u2229 Pm. Here \u0393mr = {pm \u2208 Pm : p \u2208 \u0393r}.\nMultiple observations afford considerable flexibility in the level sets of such an elicitable \u0393. In particular, whereas before the cell boundaries between level sets were restricted to hyperplanes,\nwithm observations these boundaries can be defined by nearly arbitrarym-degree polynomials. We illustrate this flexibility and visualize the cell boundaries in Figure 3. In particular, we show that a classic negative example, where an agent is asked to report whether their belief has low or high variance, is easily elicited with two observations."}, {"heading": "4. Lower Bounds via Geometry", "text": "In this section we discuss lower bounds on elicitation complexity. For technical reasons we will here require P to be a C\u221e submanifold of \u2206Y with corners. Our lower bounds will also generally require \u0393 to be a C\u221e function, in which case we call it a C\u221e property.\nWe begin in the first subsection by recalling the structure of the level sets of identifiable properties, and then introduce a technique for obtaining from this some lower bounds on elicitation complexity via differential geometry. In the next subsection we focus on polynomial properties, and explain some results that use algebraic geometry to obtain sharp bounds."}, {"heading": "4.1. Preliminaries on identifiable properties", "text": "We start by recalling a general method, introduced in Frongillo and Kash (2015c), for showing lower bounds on elicitation complexity: Given a property \u0393, if one can show that no level set from any \u0393\u0302, which is m-identifiable and directly elicitable with m observations, can be contained in a particular level set of \u0393, then \u0393 cannot be (d,m)-elicitable. This follows immediately from the definitions: if \u0393 is indirectly elicited via \u0393\u0302 and link \u03c8, so that \u0393 = \u03c8 \u25e6 \u0393\u0302, then we have the following relationship between the level sets of \u0393 and \u0393\u0302:\n\u0393r = \u22c3\nr\u0302:\u03c8(r\u0302)=r\n\u0393\u0302r\u0302. (1)\nIn other words, the level sets of \u0393 are obtained by combining some of the level sets of \u0393\u0302. For instance, if \u03c8 is a bijection, then the level sets of \u0393 and \u0393\u0302 are identical. This method was used successfully in Frongillo and Kash (2015c) to show lower bounds on the report complexity (d) of\na property, with m = 1. In this section, we will use the same method to show lower bounds on observation complexity (m), with d = 1.\nOur main tool for obtaining these lower bounds will be that the level sets of any directly mobservation-elicitable, identifiable \u0393\u0302 have a specific structure, namely, such a level set is the zero set of a polynomial of degree at most m:\nFact 1 If a property \u0393\u0302(p) ism-identifiable, then each level set of \u0393\u0302 is the set of zeros of a polynomial in p of degree at most m.\nProof The condition EpV (r,y) = 0 is \u2211\ny1,...,ym p(y1) \u00b7 \u00b7 \u00b7 p(ym)V (r, y1, . . . , ym) = 0 .\nCombined with the equality (1) above, Fact 1 tells us that the level sets of indirectly elicitable \u0393 are unions of zero sets of polynomials. As we are focusing on the d = 1 case, however, both \u0393 and \u0393\u0302 are real-valued functions, so with enough regularity, their level sets should coincide. Before making a precise statement, we introduce the following definition:\nDefinition 9 (C\u221e (d,m)-elicitable) We say that a C\u221e property \u0393 : P \u2192 Rd\u2032 is C\u221e (d,m)elicitable if in the definition of (d,m)-elicitable, \u0393\u0302 can be taken to be C\u221e and \u03c8 can be taken to be C\u221e in an open neighborhood of the image of \u0393\u0302.\nCorollary 10 Suppose that a C\u221e property \u0393 : P \u2192 R is C\u221e (1,m)-elicitable. Let r \u2208 R, let Z \u2286 \u0393\u22121(r) be a connected component of the level set, and assume that Z admits a point that is not a critical point of \u0393; i.e., there is a point p \u2208 Z such that the differential of \u0393 at p is nonzero. Then Z is a connected component of the set of zeros of a polynomial of degree at most m. Moreover, if \u0393\u22121(r) is connected, then \u0393\u22121(r) is the zero set of a polynomial of degree at most m.\nProof Let \u0393\u0302 and \u03c8 be as in Definition 9. We have a commutative diagram:\nP \u0393\u0302 //\n\u0393\nR\n\u03c8 R\nSince Z is connected, we have that \u0393\u0302(Z) \u2286 R is connected, and is therefore an interval (see e.g., Browder (1996), Theorem 6.76, 6.77, p.148). The claim is that this interval is a point. Indeed, assume the opposite. Then since \u03c8 is by definition constant on the interval \u0393\u0302(Z), we would have that the differential D\u03c8 vanishes at each point of of \u0393\u0302(Z). Then since D\u0393 = D\u03c8 \u25e6D\u0393\u0302 we would have that D\u0393 vanishes at every point of Z. But this would contradict our assumption. Thus \u0393\u0302(Z) is a point.\nIt then follows from Fact 1 that \u0393\u0302\u22121(\u0393\u0302(Z)) is the zero set of a polynomial of degree at most m. We now use the inclusions\nZ \u2286 \u0393\u0302\u22121(\u0393\u0302(Z)) \u2286 \u0393\u22121(r).\nBy virtue of the inclusion on the right, every connected component of \u0393\u0302\u22121(\u0393\u0302(Z)) is contained in a connected component of \u0393\u22121(r). This proves the first assertion of the lemma. The last assertion of the lemma also follows from these inclusions, since in that case one is assuming Z = \u0393\u22121(r).\nRemark 11 For concreteness, we summarize the contrapositive of Corollary 10 in the way in which we will use it in examples: Suppose that \u0393 : P \u2192 R is a C\u221e property, and there exists an r \u2208 R such that the level set \u0393\u22121(r) is connected, and contains a point P \u2208 \u0393\u22121(r) that is not a critical point for \u0393. Then if \u0393\u22121(r) is not the zero locus of a degreem polynomial in p(y1), . . . , p(ym), then \u0393 is not C\u221e (1,m)-elicitable.\nAs a consequence of Corollary 10, we can immediately show the existence of C\u221e properties with infinite observation complexity; i.e., properties that are not C\u221e (1,m) elicitable for any m. The proof gives such an example for |Y| = 3, a surprising result given that all properties have report complexity |Y|\u22121 = 2; i.e., all of the C\u221e properties are C\u221e (2, 1)-elicitable. Note that if |Y| = 2, then all C\u221e properties are C\u221e (1, 1)-elicitable.\nProposition 12 There are C\u221e properties that are not C\u221e (1,m)-elicitable for any finite m.\nProof Take Y = {1, 2, 3}, P = \u2206\u25e6Y = {p \u2208 \u2206Y : p(y) > 0 \u2200y \u2208 Y}, and \u0393(p) = p1 \u2212 (1/2) sin(1/p2). It is immediate that \u0393 has no critical points. Here the level sets \u0393r satisfy r = p1 \u2212 (1/2) sin(1/p2), in other words, satisfy the equation p1 = (1/2) sin(1/p2) + r. For p2 sufficiently small, the level set \u03930 = {p \u2208 \u22063 : p1 = (1/2) sin(1/p2)} is simply the graph of (1/2) sin(1/x), which intersects the line p1 = 0 infinitely many times, and hence by the Fundamental Theorem of Algebra is not the zero set of any polynomial. Corollary 10 now implies that \u0393 is not (1,m)-elicitable for any m."}, {"heading": "4.2. Polynomial properties and lower bounds using algebraic geometry", "text": "We now describe some lower bounds for elicitation complexity of polynomial properties. The motivation for these lower bounds is the intuition that, in general, a polynomial property \u0393 : P \u2192 R of degree k should not be C\u221e (1,m)-identifiable for any m < k, since the zero set of a degree k polynomial should not be the zero set of a degreem polynomial whenm < k. This statement can of course fail in special cases (e.g., Example 1 below). Indeed, there are some subtleties regarding zero sets of polynomials in Euclidean open sets, considered in Appendix C, that must be addressed to draw such a conclusion. Nevertheless, for a general polynomial property this expectation holds (see Remark C.4 for a precise definition of generality), and in the appendix we provide some elementary techniques for confirming this expectation in particular examples (see Corollary C.3). For instance, we show (Example C.1):\nCorollary 13 If |Y| \u2265 3, then for any natural number k, the k-norm of a distribution, \u0393(p) = ( \u2211\ny p(y) k)1/k, is not C\u221e (1, k \u2212 1)-elicitable.\nExample 1 In contrast to the case considered in Corollary 13, we emphasize that there are polynomial properties \u0393 : P \u2192 R of degree k that are C\u221e (1,m)-elicitable for some m < k. For instance, take \u0393\u0302 : P \u2192 R to be any polynomial property of degree m > 0, let \u03c8 : R \u2192 R be any polynomial function of degree m\u2032 > 1, and set \u0393 = \u03c8 \u25e6 \u0393\u0302. Then \u0393 is of degree k = mm\u2032 > m, but \u0393 is C\u221e (1,m)-elicitable, by Lemma 15."}, {"heading": "5. Examples and Elicitation Frontiers", "text": "We now combine our complexity lower bounds with upper bounds to make progress toward determining the elicitation frontiers of some potential properties of interest. See Figure 4 for a depiction of some of the elicitation frontiers described. We begin with some general, straightforward, but versatile upper bounds.\nLemma 14 For all 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m, let fij : Y \u2192 R be an arbitrary function such that Ep[fij(Y )] exists for all p \u2208 P . Then \u0393(p) = \u2211n i=1 \u220fm j=1 Ep[fij(Y )] is (1,m)-elicitable.\nProof Using Y1, . . . , Ym which are i.i.d. from p, then {fi1(Y1), . . . fim(Ym)} will be independent for all i. Using properties of expectations (linearity and independence), we have\nn\u2211 i=1 m\u220f j=1 E[fij(Y )] = n\u2211 i=1 m\u220f j=1 E[fij(Yj)] = n\u2211 i=1 E  m\u220f j=1 fij(Yj)  = E  n\u2211 i=1 m\u220f j=1 fij(Yj)  (2) Now we see that using squared loss (or any loss for the mean) one can leverage these m samples to\nelicit the desired sum of products, e.g. `(r, y1, . . . , ym) = ( r \u2212 \u2211n i=1 \u220fm j=1 fij(yj) )2 .\nThe proof of Lemma 14 simply constructs an unbiased estimator of the property of interest and elicits the mean of the estimator via squared error. By a very natural extension, this technique also applies to ratios of expectations, as they are elicitable (Gneiting, 2011): construct two unbiased estimators, and elicit the ratio of their means. We will give two instances of such ratios in the next subsection.\nThe following result establishes an upper bound that by now may seem natural: Under some conditions, a property that is itself an m-degree polynomial in p is (1,m)-elicitable.\nLemma 15 Suppose that \u0393 : P \u2192 R is a property such that \u0393 = \u03c8 \u25e6 \u0393\u2032 where \u0393\u2032 : P \u2192 R is polynomial of degree m, and \u03c8 : R \u2192 R is a function that is C\u221e on an open neighborhood of the image of \u0393\u2032. Then \u0393\u2032 is directly (1,m)-elicitable, and \u0393 is C\u221e (1,m)-elicitable.\nProof It is enough to show \u0393\u2032 is directly (1,m)-elicitable. This follows immediately from Lemma 14. Indeed, it is clear from the lemma that it is enough to show the result for monic monomials. For this one takes the fij in Lemma 14 to be characteristic functions 1y for y \u2208 Y ."}, {"heading": "5.1. Ratios of expectations: index of dispersion and Sharpe ratio", "text": "The index of dispersion of a random variable Y with positive mean is defined to be Var(Y )/E[Y ] (Cox and Lewis, 1966). The Sharpe ratio of a random variable Y , which is a commonly-used measure of the risk-adjusted return of an investment, is defined similarly as E[Y ]/ \u221a Var(Y ) (Sharpe, 1966). Both the index of dispersion and the square of the Sharpe ratio are (1, 2)-elicitable by the above discussion: Var(Y ) = Ep[12(Y1 \u2212 Y2)\n2], Ep[Y ] = Ep[Y1], and Ep[Y ]2 = Ep[Y1Y2], so any ratios of these terms is (1, 2)-elicitable. (The link function for the Sharpe ratio is thus the square root.) For example, the index of dispersion is elicited by the loss `(r, y1, y2) = r(y1 \u2212 y2)2 \u2212 r2y1.\nTo finish describing the elicitation frontiers for these properties, we note that neither is (1, 1)- elicitable as the level sets are not convex, but both are (2, 1)-elicitable as we now show. For the\nindex of dispersion, we can take r1 = E[Y ] and r2 = E[Y 2], both elicitable as means, and then compute the property by (r2\u2212r21)/r1. Similarly, for the same r1, r2, the Sharpe ratio can be written as r1/ \u221a r2 \u2212 r21."}, {"heading": "5.2. Norms of distributions", "text": "As we have previously discussed, the 2-norm is (1, 2) elicitable. For general k, the k-norm is (1, k) elicitable with the following loss function `(r, y1, . . . , yk) = (r \u2212 1{y1 = . . . = yk})2. (This case also follows from Lemma 15.) This is a tight bound on the observation complexity, as we proved in Corollary 13 that the k-norm is not (1, k\u22121) elicitable. As it turns out, the report complexity of the k-norm is |Y|\u22121, meaning it is as hard to elicit with one observation as the entire distribution. This follows from Theorem 2 of Frongillo and Kash (2015c), specifically Section 4.2, as \u2016p\u2016k is a convex function of p. An interesting open question, and one that will require additional algebraic tools, is the k-norm\u2019s elicitation frontier when we allow multiple dimensions and multiple observations.\nCorollary 16 For |Y| \u2265 3, the elicitation frontier of the k-norm contains (|Y| \u2212 1, 1) and (1, k)."}, {"heading": "5.3. Central Moments", "text": "The nth central moment \u00b5n of a random variable Y is defined as\n\u00b5n = E[(Y \u2212 E[Y ])n] = n\u2211 i=0 (\u22121)i ( n i ) E[Y ]i \u00b7 E[Y n\u2212i] , (3)\nwhich we see is (n, 1)-elicitable by simply elicitingE[Y i] for all i \u2208 {1, . . . , n} and then combining the results. As we will show, \u00b5n is also (1, n)-elicitable, and moreover, we can achieve other dimension-observation tradeoffs in between, such as (b \u221a nc+1, d \u221a ne). The key idea is to partition the binomial sum (3) into k partial sums and factor out the highest power of E[Y ] from each, such that the jth partial sum can be written as\nE[Y ] j\u00b7n k n k \u22121\u2211 i=0 (\u22121)i ( n j\u00b7n k + i ) \u00b7 E[Y ]i \u00b7 E [ Y (j+1)\u00b7n k \u22121\u2212i ] . (4)\nDoing so gives the following result. Theorem 17 The nth central moment is (k + 1, \u2308 n/k \u2309 )- elicitable; 0 < k \u2264 n\nProof Consider the partial sum (4) without the E[Y ]j\u00b7n/k factor; by Lemma 14 each such factored sum is (1, \u2308 n/k \u2309 )-elicitable, as the maximum number of terms in any product is \u2308 n/k \u2309 . Since we have k such factored sums, and need to additionally elicit the mean E[Y ] to compute their factors, the entire sum can be elicited using \u2308 n/k \u2309 observations and k + 1 dimensions.\nWhen k = 0, we can do much better than m = \u221e: by Lemma 14, as the maximum number of terms in any product of (3) is n, the term (E[Y ])n, we have than \u00b5n is (1, n)-elicitable. For lower bounds, little is known beyond \u00b5n not being (1, 1)-elicitable (Frongillo and Kash, 2015b)."}, {"heading": "6. Multi-Observation Regression", "text": "One of the earliest problems in modern statistics was the estimation of biodiversity in a geographic region (Fisher et al., 1943). One scalar measure of diversity of a distribution is the (inverse of the) 2-norm, which we will take here as an example.3 Consider a dataset of species samples: pairs (x, y) where x gives the features of the geographic region and y is a categorical giving the species to which this sample belongs. Suppose we wish to regress the diversity of species against geographic features such as climate. The single-observation approach would require a surrogate loss function `(f\u0302(x), y) and a link f(x) = \u03c8(f\u0302(x)). We claim that any single-observation loss function `(f(x), y) is poorly suited for this task. For the 2-norm, lower bounds on report complexity show that the best possible approach has dimensionality f\u0302 : x \u2192 Rd\u22121 where d is the number of unique species in the dataset (which may have a very long tail). So this approach requires, in essence, fitting f\u0302 to the entire distribution over species as a function of geographic region, a task of immense idiosyncrasy and complexity compared to the end goal of e.g. estimating a scalar measure of diversity as a function of rainfall level.\nOn the other hand, a two-observation loss function `(f(x), y1, y2) can be used to directly learn an f estimating the desired diversity measure, e.g. 2-norm, as a function of geographic features. One can then use empirical risk minimization to directly learn relationships between, e.g. rainfall level and this measure of species diversity.\nMulti-observation regression does introduce an additional challenge, however: risk in this context is naturally defined as Ex,y`(f(x),y) where y = (y1, . . . , ym) is a set of observations drawn i.i.d. conditioned on x, but our data points are of the form (x, y). If e.g. x comes from a continuous space, we may not have any sets of m samples y1, . . . , ym belonging to the same x. One natural setting where this poses no concern is in active learning where we may choose to re-draw the label for a given x. In a more standard regression framework, we propose to leverage the intuition that the distribution of y conditioned on x generally changes gradually as a function of x.4 Pragmatically, with dense enough data points, we can simply group together nearby x values and \u201cmerge\u201d them into a data point of the form (x\u0304, y1, . . . , ym) where x\u0304 is an average and the yi are drawn independently and approximately identically from approximately the distribution of Y conditioned on x\u0304. For this paper, we demonstrate the idea in simulations below and give a basic proof-of-concept theoretical result in Appendix B, leaving a more thorough investigation to future work.\n3. A similar intuition will hold for most if not all elicitable measures of diversity. 4. Phrased differently, at least it seems reasonable to parameterize the rate of change and expect learning bounds to\ndepend on this parameter.\nIn general, the cases where the multi-observation approach can be useful are those where the property of interest is believed to follow a simple functional form, but the conditional statistics given by the indirect elicitation approach are expected to follow unknown or complicated trends as a function of features. For another example, one could imagine learning the noise (e.g. variance) of a medical test, e.g. white blood cell count, as a function of patient features, in order to improve the test. The indirect elicitation approach suggests first fitting a model for estimating the mean of the test\u2019s outcome as a function of patient data, then fitting the expected square of the statistic, and then computing an estimate for the variance by combining them. In general, these prediction problems may be highly complex and nonlinear even when the noise in the test might follow some simple linear relationship with e.g. height or age. The multi-observation approach allows direct regression of the noise versus features. Formally, we show a basic extension of classic risk guarantees in Appendix B, under the assumption that x is distributed uniformly on [0, 1] and a closeness condition on the conditional distribution of Y given X ."}, {"heading": "6.1. Simulation", "text": "Here we describe some simulations run as a proof of concept of multi-observation regression. Our data points are of the form (x, y) \u2208 R\u00d7R where x is drawn uniformly at random from the interval [0, 1]. Given x, y = a sin(4\u03c0x)+Z, where a is a constant and Z \u223c N(0, 1) is drawn independently for each sample, we wish to learn Var(Y |X).\nOur multi-observation loss function here is `(f(x), y1, y2) = (f(x) \u2212 12(y1 \u2212 y2) 2)2. We approximate (x, y1, y2) samples by sorting the (xi, yi) pairs by xi, and making samples of the form (12(xi + xi+1), yi, yi+1). We compare to the single observation approach, in which we estimate E[Y |X] and E[Y 2|X] and then combine them to estimate Var(Y |X).\nThe point of these simulations is to demonstrate that multi-observation regression can greatly outperform single observation regression in the case when the function is in a known concept class, and the statistics needed to indirectly elicit it with a single observation are not in a known concept class. As such, our multi-observation regression fits a linear function to Var(Y |X), and our single observation regression fits linear functions to E[Y |X] and E[Y 2|X]. The true Var(Y |X) = 1 is indeed a linear function, while the true moment functions E[Y |X = x] = a sin(x) and E[Y 2|X = x] = a2 sin2(x) + 1 are very far from linear.\nFigure 5 gives the results for a = 1 and a = 10. Both plots show the mean squared error of the variance functions reported by the two regression methods (averaged over 4000 simulations) as a function of the number of samples. In both cases we see that for sufficiently many samples, the two observation regression significantly outperforms the single observation regression."}, {"heading": "7. Conclusion and Future Work", "text": "An immediate host of directions is the proving of upper and lower bounds on elicitation frontiers for various properties. In particular, our lower bounds here focus on techniques for lower-bounding observation complexity (the (1,m) case), leaving open approaches for lower bounds on (d,m) complexity for d \u2265 2. Another direction is to formalize learning guarantees for multi-observation regression under suitable assumptions on slow-changing conditional distributions."}, {"heading": "Acknowledgments", "text": "We thank Karthik Kannan for contributing the upper bound for central moments. Sebastian CasalainaMartin was partially supported by NSA grant H98230-16-1-0053. Tom Morgan was funded in part by NSF grants CCF-1320231 and CNS-1228598. Bo Waggoner is supported by the Warren Center for Network and Data Sciences at the University of Pennsylvania."}, {"heading": "Appendix A. Overlapping Level Sets: Proof of Theorem 7", "text": "Theorem 7 states that a property is not elicitable if there is a convex combination of one of its level sets in the m-product space that equals a convex combination of another one of its level sets in the m-product space. To reason about these level sets we will need the following theorem.\nTheorem A.1 (Theorem 3.5, Frongillo and Kash (2014)) The property \u0393 : P \u2032 \u2192 R (where P \u2032 \u2286 \u2206Y \u2032) is directly elicitable by the loss function ` if and only if there exists some convex G : conv(P \u2032) \u2192 R\u0304 with G(P \u2032) \u2286 R, some D \u2286 \u03b4G, and some bijection \u03c6 : \u0393(P \u2032) \u2192 D with \u0393(p) = \u03c6\u22121(D \u2229 \u03b4Gp), such that for all r \u2208 R and y \u2208 Y \u2032,\n`(r, y) = \u03c6(r)(pr \u2212 y)\u2212G(pr),\nwhere {pr} \u2286 P \u2032 satisfies r\u0302 = \u0393(pr\u0302) for all r\u0302.\nHere \u03b4Gr is the set of subgradients to G at r. Proof of Theorem 7 5 Let Y \u2032 = Ym and P \u2032 = Pm. Let ` be a loss function that elicits \u0393 of the form given by Theorem A.1, and let G, {pr} and \u03c6 be the corresponding values defined in Theorem A.1. We will let \u0393\u2032 : Pm \u21d2 R be the property that is elicited by ` on conv(Pm).\nNote that \u0393\u2032 is not necessarily single-valued everywhere on conv(Pm). This is because we cannot guarantee that there is a unique value that minimizes the loss function for distributions in the interior of conv(Pm). However, we can show that whenever q \u2208 conv(Pm) can be written as a convex combination of points on Pm that all have property value r then Ey\u223cq`(r, y) is uniquely minimized at r, thus r is the unique property value of \u0393\u2032 at q. This implies the theorem, as if q can be written as a convex combination of two separate level sets of \u0393 then there must not be an ` of the form specified in Theorem A.1 which elicits it.\nIf q = \u2211k\ni=1 \u03bbip m i for p \u2208 \u0393r\u2217 , \u03bb1, . . . , \u03bbk \u2208 [0, 1] and \u2211k i=1 \u03bbi = 1 then\nEy\u223cq`(r, y) = \u03c6(r)(qr \u2212 q)\u2212G(qr)\n= \u03c6(r) ( qr \u2212\nk\u2211 i=1 \u03bbip m i\n) \u2212G(qr)\n= k\u2211 i=1 \u03bbi (\u03c6(r)(qr \u2212 pmi )\u2212G(qr))\n= k\u2211 i=1 \u03bbiEy\u223cpmi L(r, y).\nWe know that each term of the final sum is uniquely minimized by r = r\u2217, thus Ey\u223cq`(r, y) is uniquely minimized by r\u2217.\n5. An alternate proof can also be constructed using results of Osband (1985)."}, {"heading": "Appendix B. Regression", "text": "In this section, we give a proof-of-concept showing that classic risk bounds for ERM can go through with only slight modification with multi-observation loss functions, under a natural assumption.\nRegression can be naturally formulated in the multi-observation setting as follows: Given a hypothesis class F : X \u2192 R and loss function ` : R \u00d7 Ym \u2192 R, given access to an unknown distribution D on X and conditional distributions {Dx \u2208 \u2206Y : x \u2208 X}, approximately minimize\nRisk(f) = Ex\u223cD,y\u223cDx`(f(x), y1, . . . , ym).\nThe central challenge that arises, new to the multi-observation setting, is that the data we are given is of the form (x1, y1), . . . , (xn, yn) where xi \u223c D and yi \u223c Dxi i.i.d. We may only obtain a single y for any given x. In this section, we give an example of how this obstacle can be overcome under natural assumptions.\nFor simplicity, let us suppose thatX \u2286 Rd (in this section, d is not being used for dimensionality of the report space). The key idea is that, if the distribution Dx changes slowly as a function of x, then with enough samples, then a set of m close neighbors x1, . . . , xm can be viewed as approximating a single x with m \u201calmost i.i.d.\u201d conditional draws y1, . . . , ym. We formalize this intuition here using a Lipschitz condition on the total variation distance:\nDTV (Dx,Dx\u2032) \u2264 K\u2016x\u2212 x\u2032\u20162.\nHowever, the exact formalization is less important than the general idea, and we expect that future work will be able to prove similar results with a variety of similar assumptions.\nOur approach will be to cluster the data into groups of size m having nearby xs, then treat each group as a single sample of the form (x\u2217, y1, . . . , ym) with each yi approximately i.i.d. from Dx\u2217 . We then have n\u2032 \u201csamples\u201d of this form, where n\u2032 is the number of clusters. Of course, for this approach, it is necessary that that m be small compared to the total number of samples n \u2248 n\u2032m; we are often interested in the m = 2 case where our theory and simulations already show dramatic differences from the traditional case of m = 1.\nA classic risk bound translated into our setting is the following, whereRn denotes the Rademacher complexity of a hypothesis class.\nTheorem B.1 (Bartlett and Mendelson (2002)) Suppose ` is L-Lipschitz in its first argument and bounded by c, {xi}ni=1 are drawn i.i.d. from a distribution D, and each yi is drawn independently from Dxi . Then with probability at least 1\u2212 \u03b4, for all f \u2208 F ,\nRisk(f) \u2264 Riskemp(f, {xi,yi}ni=1) + 2LRn(F) + c \u221a log 1/\u03b4\n2n .\nHere the probability is over the randomness in {xi,yi}.\nIn other words, if we could actually sample a set yi = (yi,1, . . . , yi,m) from Dxi i.i.d., we would reduce to the standard setting. This theorem is leveraged to prove specific ERM risk bounds depending on F . Here we just show that this bound changes only slightly in the multi-observation case, with an increase in sample complexity.\nOur \u201ccluster-points\u201d algorithm roughly functions as follows: draw n i.i.d. data points x\u22171, . . . , x \u2217 n and n\u2032 = \u2126(n(m+ log(n/\u03b4))/ ) \u201cscatter points\u201d of the form (x, y). Assign to each x\u2217i a set y \u2217 i of sizem where for each y\u2217ij , its corresponding x has \u2016x\u2212x\u2217i \u20162 \u2264 . We first show that this is possible with probability 1\u2212 \u03b4, in two lemmas.\nLemma B.2 Given x \u2208 [0, 1], < 1 and \u2126((m+ log(1/\u03b4\u2032))/ ) i.i.d. from the uniform distribution over [0, 1], with probability at least 1\u2212 \u03b4\u2032, at least m of the samples fall within of x.\nProof The probability that a given sample falls within of x is at least . If we take s samples, then by a standard Chernoff bound we have that the probability of fewer than m samples falling within of x is upper bounded by\ne\u2212(1\u2212 m s) 2 s/2.\nSolving for s when this is \u03b4\u2032 gives us the Lemma.\nLemma B.3 Let D be the uniform distribution on [0, 1]. n\u2032 = O(n(m+ log(n/\u03b4))/ ) samples of the form (x, y) where x \u223c D and y \u223c Dx are sufficient to find, with probability at least 1\u2212 \u03b4, a set of n independent samples of the form (x\u2217, y\u22171, . . . , y \u2217 m) where x\n\u2217 \u223c D and the y\u2217i s are independent and of the form y\u2217i \u223c Dx\u2032 for |x\u2032 \u2212 x\u2217| \u2264 .\nProof First we take m samples and use there x values as our m x\u2217s. For each x\u2217, we take a new set of n\u2032/m = O((m + log(n/\u03b4))/ ) samples (x1, y1), . . . , (xn\u2032/m, yn\u2032/m). Let j1, . . . , jm be m distinct indices such that for all i, |xji \u2212 x\u2217| \u2264 . By Lemma B.2 (setting \u03b4\u2032 = \u03b4/n) such a set will exist with probability at least 1\u2212 \u03b4/n. We then construct the sample\n(x\u2217, y\u22171, . . . , y \u2217 m) = (x \u2217, yj1 , . . . , yjm).\nBy a union bound, this algorithm will succeed with probability at least 1\u2212 \u03b4, and the produced samples trivially fulfill the distributional requirements of the Lemma.\nNow we obtain the desired result. Note that we can choose as small as desired, e.g. = 1/n2, with a blowup of 1/ in the sample complexity. However, a more sophisticated bound would preferably use higher-powered concentration inequalities or a more carefully tailored assumption in order to get a bound holding with higher probability.\nTheorem B.4 Suppose ` is L-Lipschitz in its first argument and bounded by c, D is uniform on [0, 1], and {x\u2217i ,yi}ni=1 are drawn according to our cluster-points algorithm, taking n\u2032 = O((m + log(n/\u03b4))/ ) total samples. Then with probability at least 1\u2212 2\u03b4 \u2212mnK , for all f \u2208 F ,\nRisk(f) \u2264 Riskemp(f, {x\u2217i ,yi}ni=1) + 2LRn(F) + c \u221a log 1/\u03b4\n2n .\nAgain the probability is over the randomness in {x\u2217i ,yi}.\nProof With probability 1 \u2212 \u03b4, our \u201ccluster-points\u201d algorithm succeeds in finding {x\u2217i }ni=1 drawn i.i.d. and {yi}ni=1 drawn from -close points. We wish to consider Riskemp(f, {x\u2217i ,yi}ni=1), where each yi is Km -close in total variation distance to y\u2217i , as each member is K close. So the whole quantity, by the properties of total variation distance, is mnK -close to Riskemp(f, {xi,yi}ni=1), and we apply Theorem B.1."}, {"heading": "Appendix C. Zero sets of Polynomials over the Real Numbers", "text": "Consider a polynomial f(x1, . . . , xn) in the set R[x1, . . . , xn] of polynomials in n variables with real coefficients. The zero set of f(x1, . . . , xn) is by definition the set\nZ(f(x1, . . . , xn)) := {(\u03b11, . . . , \u03b1n) \u2208 Rn : f(\u03b11, . . . , \u03b1n) = 0} \u2286 Rn.\nRecall that a nonconstant polynomial f(x1, . . . , xn) \u2208 R[x1, . . . , xn] is said to be irreducible if it cannot be written as the product of two polynomials in R[x1, . . . , xn] of strictly lower degree. Recall also that a subset U \u2286 Rn is said to be open in the Euclidean topology if for every \u03b1 = (\u03b11, . . . , \u03b1n) \u2208 U , there exists a real number \u03b1 > 0, depending on \u03b1, such that the ball of radius \u03b1 centered at \u03b1, B \u03b1(\u03b11, . . . , \u03b1n), is contained in U :\nB \u03b1(\u03b11, . . . , \u03b1n) := { (\u03b21, . . . , \u03b2n) \u2208 Rn : \u221a (\u03b21 \u2212 \u03b11)2 + \u00b7 \u00b7 \u00b7+ (\u03b2n \u2212 \u03b1n)2 < \u03b1 } \u2286 U.\nWith this terminology, we can state the following theorem:\nTheorem C.1 Suppose that f(x1, . . . , xn) \u2208 R[x1, . . . , xn] is a nonconstant irreducible polynomial, and U \u2286 Rn is an open subset in the Euclidean topology. If there is a point\n(\u03b11, . . . , \u03b1n) \u2208 Z(f(x1, . . . , xn)) \u2229 U \u2286 Rn\nsuch that ( \u2202f\n\u2202x1 (\u03b11, . . . , \u03b1n), . . . ,\n\u2202f\n\u2202xn (\u03b11, . . . , \u03b1n)\n) 6= (0, . . . , 0) \u2208 Rn, (5)\nthen there are no nonzero polynomials of degree less than the degree of f(x1, . . . , xn) that vanish at every point of the zero set Z(f(x1, . . . , xn)) \u2229 U .\nWe expect the theorem is well known; for instance, the case where U = Rn is a special case of (Bochnak et al., 1998, Thm. 4.5.1). The proof of (Bochnak et al., 1998, Thm. 4.5.1) easily generalizes to our situation. For the convenience of the reader, in Theorem D.1 below we include a generalization of (Bochnak et al., 1998, Thm. 4.5.1) that impiles Theorem C.1.\nRemark C.2 (Checking the conditions of Theorem C.1) There are many techniques for checking that a polynomial f(x1, . . . , xn) \u2208 R[x1, . . . , xn] is irreducible and satisfies the condition (5) for all (\u03b11, . . . , \u03b1n) \u2208 Z(f(x1, . . . , xn)) \u2229 U , and therefore satisfies the hypotheses of Theorem C.1. For n \u2265 2, we recall the following elementary condition that suffices. Suppose f(x1, . . . , xn) is a nonconstant polynomial of degree d. The homogenization of f(x1, . . . , xn) is the degree d homogeneous (all monomials of degree d) polynomial F (X0, X1, . . . , Xn) \u2208 R[X0, . . . , Xn] that is obtained from f(x1, . . . , xn) by replacing xi with Xi for i = 1, . . . , n, and then multiplying each monomial by a power of X0 until it is of degree d. For instance, if f(x1, x2) = x21 + 2x2 + 3, then F (X0, X1, X2) = X 2 1 + 2X0X2 + 3X\n2 0 . If the complex zero set{\n(\u03b10, . . . , \u03b1n) \u2208 Cn+1 : \u2202F\n\u2202X0 (\u03b10, . . . , \u03b1n) = \u00b7 \u00b7 \u00b7 =\n\u2202F\n\u2202Xn (\u03b10, . . . , \u03b1n) = (0, . . . , 0)\n} \u2286 Cn+1\n(6) is equal to {(0, . . . , 0)} or \u2205, then f(x1, . . . , xn) is irreducible and satisfies (5) for all (\u03b11, . . . , \u03b1n) \u2208 Z(f(x1, . . . , xn)). This is by no means a necessary condition for f(x1, . . . , xn) to satisfy the conditions of Theorem C.1, but it is easy to implement in examples. There are a number of other techniques that can be used, including using computer algebra systems.\nUsing the technique outlined in the remark, and standard results in algebraic geometry, it is elementary to establish the following corollary:\nCorollary C.3 Let n \u2265 2, let U \u2286 Rn be a nonempty open subset in the Euclidean topology, let f(x1, . . . , xn) \u2208 R[x1, . . . , xn], and for each c \u2208 R define\nfc(x1, . . . , xn) := f(x1, . . . , xn) + c.\nLet Fc(X0, \u00b7 \u00b7 \u00b7 , Xn) be the homogenization of fc(x1, . . . , xn). If for some c0 \u2208 R the complex zero set (6) for Fc0(X0, . . . , Xn) is equal to {(0, . . . , 0)} \u2286 Cn+1 or \u2205, then there is a nonempty open subset B \u2286 R in the Euclidean topology such that for all c \u2208 B, there are no nonzero polynomials of degree less than d that vanish at every point of the zero set Z(fc(x1, . . . , xn)) \u2229 U .\nAs a consequence:\nExample C.1 For a given pair of natural numbers n and d with n \u2265 2, suppose that:\n\u2022 For c \u2208 R, we set fc(x1, . . . , xn) := xd1 + \u00b7 \u00b7 \u00b7+ xdn + (1\u2212 x1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 xn)d + c. \u2022 U := {(\u03b11, . . . , \u03b1n) \u2208 Rn : \u03b11, . . . , \u03b1n > 0, \u2211n i=1 \u03b1i < 1}.\nThere exists a nonempty open subsetB \u2286 R in the Euclidean topology such that for all c \u2208 B, there are no nonzero polynomials of degree less than d that vanish at every point ofZ(fc(x1, . . . , xn))\u2229U .\nWe can confirm this using the approach in Corollary C.3:\nFc = cX d 0 +X d 1 + \u00b7 \u00b7 \u00b7+Xdn + (X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn)d\n\u2202X0Fc = cdX d\u22121 0 + d(X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn) d\u22121, \u2202X1Fc = dX d\u22121 1 \u2212 d(X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn) d\u22121,\n...\n\u2202XnFc = dX d\u22121 n \u2212 d(X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn)d\u22121.\nTo use Corollary C.3, we need to consider the complex zero set (6):{ (\u03b10, . . . , \u03b1n) \u2208 Cn+1 : \u2202X0Fc(\u03b10, . . . , \u03b1n) = \u00b7 \u00b7 \u00b7 = \u2202XnFc(\u03b10, . . . , \u03b1n) = (0, . . . , 0) } \u2286 Cn+1,\nand show that for some c \u2208 R it is either empty or equal to {(0, . . . , 0)}. We consider the case c = 0. Under this assumption, we have\n0 = \u2202X0Fc = cdX d\u22121 0 + d(X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn) d\u22121 \u21d0\u21d2 (X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn) = 0.\nThen, assuming X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn = 0, we have for i = 1, . . . , n that\n0 = \u2202XiFc = dX d\u22121 i \u2212 d(X0 \u2212X1 \u2212 \u00b7 \u00b7 \u00b7 \u2212Xn) d\u22121 \u21d0\u21d2 Xi = 0.\nWith this new information, returning to \u2202X0Fc, we see that we also must have\nX0 = 0.\nIn other words, the complex zero set is {(0, . . . , 0)} \u2286 Cn, so that our example satisfies the conditions of Corollary C.3.\nRemark C.4 Most polynomials f(x1, . . . , xn) \u2208 R[x1, . . . , xn], n \u2265 2, satisfy the hypotheses of Corollary C.3. More precisely, there is a dense open subset (the complement of linear subspace) of an ( n+d d ) -dimensional real vector space that parameterizes degree-d polynomials in n variables. That subset contains a dense open subset \u2126 (the complement of the discriminant locus; see e.g., Fulton (1998)) such that every f(x1, . . . , xn) \u2208 \u2126 satisfies the hypotheses of the corollary; i.e., there is some c0 \u2208 R (for instance c0 = 0) such that the complex zero set (6) for Fc0(X0, . . . , Xn) is equal to {(0, . . . , 0)} \u2286 Cn+1 or \u2205. On the other hand, as described in Example C.2 below, it is easy to find polynomials f(x1, . . . , xn) \u2208 R[x1, . . . , xn] of degree d \u2265 2, and nonempty open subsets U \u2286 Rn, so that for every c \u2208 R there exist nonzero polynomials of degree less than d that vanish at every point of the zero set Z(fc(x1, . . . , xn)) \u2229 U .\nExample C.2 Consider the polynomial f(x1, . . . , xn) = x21, and take U = R>0\u00d7Rn\u22121. Then for every c \u2208 R there is a linear polynomial that vanishes at every point of Z(fc(x1, . . . , xn))\u2229U ; for c > 0, we can take any linear polynomial, and for c \u2264 0, we can take x1 \u2212 \u221a \u2212c.\nWe can construct many more similar examples in the following way. Let h(x1) \u2208 R[x1] be a polynomial of degree at least 2. We have for every c \u2208 R that h(x1)+c factors in R[x1] as a product of linear terms and a product of quadratic terms each having no real root. For simplicity, let us assume that for all c 6= 0, the polynomial h(x1) + c has a root that is not real; e.g., h(x1) = xm1 for some natural number m \u2265 3. Let g(x1, . . . , xn) \u2208 R[x1, . . . , xn] be any nonconstant polynomial. Let \u03bb be a real root of h(x1) (if there is one), and let U be the complement of the zero set of g(x1, . . . , xn) \u2212 \u03bb, or simply Rn if there is no real root. Then f(x1, . . . , xn) := h(g(x1, . . . , xn)) has the property that for every c \u2208 R, there is a nonzero polynomial of degree less than the degree of f(x1, . . . , xn) that vanishes at every point of the zero set Z(fc(x1, . . . , xn)) \u2229 U .\nRemark C.5 Theorem C.1 and Corollary C.3 are not interesting in the case n = 1. For f(x) \u2208 R[x] (irreducible or not) there are no nonzero polynomials of degree less than d that vanish at every point of the zero set Z(f(x)) \u2229 U if and only if all of the roots of f(x) are real, distinct, and lie in U . There are standard techniques to check this condition (e.g., (Bochnak et al., 1998, pp.12\u201314)). In Example C.1 with n = 1, by inspection one finds that for d = 1, 2 the condition holds if and only if \u22121 < c < 1, and for d = 3, 4 the condition does not hold for any c."}, {"heading": "Appendix D. The Real Nullstellenstatz for Principal Ideals and Open Sets", "text": "The main goal of this section is to prove the following theorem generalizing the well known real Nullstellenstatz for principal ideals (e.g., (Bochnak et al., 1998, Thm. 4.5.1)) to allow for Euclidean open sets.\nTheorem D.1 Let K be a real closed field (e.g., K = R). Let f(x1, . . . , xn) \u2208 K[x1, . . . , xn] be a nonconstant polynomial, and let U \u2286 Kn be an open subset in the Euclidean topology. Suppose that\nf(x1, . . . , xn) = f1(x1, . . . , xn) m1 \u00b7 \u00b7 \u00b7 fr(x1, . . . , xn)mr (7)\nis a factorization into powers of distinct nonconstant irreducible polynomials. The following are equivalent:\n1. (f) = I(Z(f) \u2229 U).\n2. m1 = \u00b7 \u00b7 \u00b7 = mr = 1 and for each i = 1, . . . , r there is a point \u03b1(i) \u2208 Z(fi) \u2229 U with\n(\u2202x1fi(\u03b1 (i)), . . . , \u2202xnfi(\u03b1 (i))) 6= 0 \u2208 Kn.\nFor K = R, this is equivalent to having for each i that Z(fi) \u2229 U is a smooth (n \u2212 1)- dimensional submanifold of an open neighborhood of \u03b1(i).\n3. m1 = \u00b7 \u00b7 \u00b7 = mr = 1 and for each i = 1, . . . , r the sign of the polynomial fi changes on an open ball in U (i.e., for i = 1, . . . , n there is an open ball B(i) \u2286 U and points \u03b1(i), \u03b2(i) \u2208 B(i) such that fi(\u03b1(i))fi(\u03b2(i)) < 0).\n4. m1 = \u00b7 \u00b7 \u00b7 = mr = 1 and for each i = 1, . . . , r the semi-algebraic Krull dimension of the topological spaceZ(fi)\u2229U (i.e., the Krull dimension of the ring K[x1, . . . , xn]/I(Z(fi)\u2229U)) satisfies\ndim(Z(fi) \u2229 U) = n\u2212 1.\nWe expect this result is known to the experts (the case where f is irreducible and U = Rn is (Bochnak et al., 1998, Thm. 4.5.1)), but for lack of a reference we provide a proof in \u00a7D.6. See \u00a7D.1 for an explanation of the notation.\nRemark D.2 The case n = 1 is elementary and has the following simple interpretation: we have (f(x)) = I(Z(f(x)) \u2229 U) if and only if all of the roots of f(x) in an algebraic closure K are distinct, and lie in U \u2286 K. There are standard techniques to check this condition (e.g., (Bochnak et al., 1998, pp.12\u201314)). Remark D.3 If f(x1, . . . , xn) is given as in (7), then the radical of the ideal (f) is the ideal\u221a (f) = (f1 \u00b7 \u00b7 \u00b7 fr). Thus Theorem D.1 also gives conditions for when there is an equality \u221a (f) = I(Z(f) \u2229 U).\nD.1. Notation and conventions\nLet K be a field. Given an ideal I \u2286 K[x1, . . . , xn] we will be interested in both the closed subscheme V (I) \u2286 AnK , as well as the zero set\nV (I)(SpecK) ' ZK(I) := {\u03b1 \u2208 Kn : f(\u03b1) = 0, for all f \u2208 I} \u2286 Kn.\nIf the field is clear from the context, we will write Z(I) = ZK(I). For a subset S \u2286 Kn, we denote as usual the ideal of polynomials vanishing on S as\nI(S) := {g(x1, . . . , xn) \u2208 K[x1, . . . , xn] : g(s) = 0 for all s \u2208 S}.\nWe refer the reader to (Bochnak et al., 1998, Def. 1.1.9, Def. 1.2.1) for a review of the definition of a real closed field. In particular, such a field K is of characteristic 0 and is an ordered field; the Euclidean topology on Kn then has a basis given by the open balls\nB (\u03b1) := {\u03b2 \u2208 Kn : n\u2211 i=1 (\u03b2i \u2212 \u03b1i)2 < 2}\nfor all \u03b1 \u2208 Kn and all \u2208 K with > 0.\nD.2. The principal Nullstellensatz\nFor an ideal I \u2286 K[x1, . . . , xn], there is a natural inclusion \u221a I \u2286 I(Z(I)). (8)\nHilbert\u2019s Nullstellensatz asserts that over an algebraically closed field K = K, this inclusion is an equality. Focusing on principal ideals, this reads\u221a\n(f) = I(Z(f)), (K = K); (9)\nin other words (f) = I(Z(f)) whenever f is reduced and K = K is algebraically closed. Over nonalgebraically closed fields (9) clearly fails; i.e., one may have\u221a\n(f) ( I(Z(f)). For instance, trivially, one has in Q[x] that \u221a\n(x2 + 1) = (x2 + 1) ( Q[x] = I(\u2205) = I(Z(x2 + 1)). The following example is a little more interesting:\nExample D.1 Consider f(x, y) = x2+y2\u2212x3 \u2208 R[x, y], and the zero setZ(f) \u2286 R2. It is a cubic plane curve with an isolated point at (0, 0) \u2208 R2. In particular, if we takeU = B (0, 0) to be a small ball around (0, 0) in R2, then \u221a (x2 + y2 \u2212 x3) = (x2+y2\u2212x3) 6= (x, y) = I(Z(x2+y2\u2212x3)\u2229U). On the other hand, it is true that (x2 + y2 \u2212 x3) = I(Z(x2 + y2 \u2212 x3)).\nD.3. The connection with dimension\nProposition D.4 Let f(x1, . . . , xn) \u2208 K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U \u2286 Kn be any subset. The following are equivalent:\n1. (f) = I(Z(f) \u2229 U).\n2. The semi-algebraic Krull dimension of the topological space Z(f)\u2229U (i.e., the Krull dimension of the ring K[x1, . . . , xn]/I(Z(f) \u2229 U)) satisfies\ndim(Z(f) \u2229 U) = n\u2212 1.\nProof (1) =\u21d2 (2). By assumption we have (f) = I(Z(f) \u2229 U). Now the Krull dimension of K[x1, . . . , xn] is n (e.g., (Atiyah and Macdonald, 1969, Exe. 11.7)). Consequently, since f is neither a zero divisor nor a unit, we have that the Krull dimension of K[x1, . . . , xn]/(f) is (n\u2212 1) (e.g., (Atiyah and Macdonald, 1969, Cor. 11.7); using that f is irreducible, this is even easier). Note that this direction does not require that f be irreducible.\n(2) =\u21d2 (1). We have inclusions\n(f) \u2286 I(Z(f) \u2229 U) \u2286 K[x1, . . . , xn]. (10)\nAs above, since f is neither a zero divisor nor a unit, we have that the Krull dimension of the ring K[x1, . . . , xn]/(f) is (n\u2212 1). By assumption, the Krull dimension of K[x1, . . . , xn]/I(Z(f)\u2229U) is also (n\u2212 1). Now since (f) is prime (finally using that f is irreducible), and has the same Krull dimension as the ideal I(Z(f)\u2229U), it follows from the containment (10) and the definition of Krull dimension that the two ideals are equal.\nD.4. The connection with smoothness\nWe say a zero set Z(I) \u2286 Kn is smooth at a point \u03b1 \u2208 Z(I) if the associated scheme V (I) \u2286 AnK is smooth at the point (x1\u2212\u03b11, . . . , xn\u2212\u03b1n) \u2208 V (I). We will also simply say that V (I) is smooth at \u03b1. Recall that if I = (f) is principal, and \u03b1 \u2208 Z(f), then V (f) is smooth at (x1\u2212\u03b11, . . . , xn\u2212\u03b1n) if and only if (\u2202x1f(\u03b1), . . . , \u2202xnf(\u03b1)) 6= 0 \u2208 Kn.\nLemma D.5 Suppose K is perfect. Let f(x1, . . . , xn) \u2208 K[x1, . . . , xn] be a nonconstant polynomial, and let U \u2286 Kn be any subset. Then:\n1. (f) = I(Z(f) \u2229 U),\nimplies\n(2) There is a point \u03b1(0) \u2208 Z(f) \u2229 U with\n(\u2202x1f(\u03b1 (0)), . . . , \u2202xnf(\u03b1 (0))) 6= 0 \u2208 Kn.\nIn other words, there is a point in U at which V (f) is a smooth scheme.\nProof We will show the contrapositive. Suppose that (2) fails. This means that \u2202x1f, . . . , \u2202xnf \u2208 I(Z(f) \u2229 U). But since f is nonconstant and K is perfect, either there is an i such that \u2202xif is nonzero, or char(K) = p > 0 and there exists a polynomial g \u2208 K[x1, . . . , xn] such that f = gp (e.g., (Cox et al., 2015, Ch. 9 Ex. 10, p.524)). In the first case, since \u2202xif is nonzero of degree less than the degree of f , it cannot be a multiple of f , and therefore is not in (f). Thus (f) ( I(Z(f) \u2229 U), and (1) fails. In the second case, where f = gp, we have g \u2208 I(Z(f) \u2229 U), while g /\u2208 (f), again for degree reasons, so that (1) also fails in this case.\nThe following example shows that the converse to Lemma D.5 need not hold.\nExample D.2 Let K = Q and let f(x1, x2) = x31 + x32 \u2212 1. Then Z(f) \u2286 Q2 is a finite set of points, and in particular one can show that (f) ( I(Z(f)). On the other hand, at the point say (1, 0) \u2208 Z(f), one has (\u2202x1f(1, 0), \u2202x2f(1, 0)) = (3, 0) 6= 0 \u2208 Q2.\nNevertheless, a converse to Lemma D.5 does hold over the real and complex numbers. This is essentially because the implicit function theorem asserts that condition (2) implies that the zero set is an (n \u2212 1)-dimensional manifold in a neighborhood of the given point. In fact, one can also establish the converse over real closed fields:\nLemma D.6 Suppose K = K is real closed or equal to C. Let f(x1, . . . , xn) \u2208 K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U \u2286 Kn be an open subset in the Euclidean topology. Then:\n1. (f) = I(Z(f) \u2229 U),\nis implied by\n(2) There is a point \u03b1(0) \u2208 Z(f) \u2229 U with\n(\u2202x1f(\u03b1 (0)), . . . , \u2202xnf(\u03b1 (0))) 6= 0 \u2208 Kn.\nIn other words, there is a point in U at which V (f) is a smooth scheme.\nProof Consider the case K = K is real closed. Let (Z(f) \u2229 U)Zar \u2286 Kn be the closure in the Zariski topology. Now using condition (2), and (iii) =\u21d2 (ii) of (Bochnak et al., 1998, Prop. 3.3.10), we have that dimK[x1, . . . , xn]/I((Z(f) \u2229 U) Zar ) = n \u2212 1. (We are applying (Bochnak et al., 1998, Prop. 3.3.10) with V = (Z(f) \u2229 U)Zar and P1 = f .) Now we observe that I(Z(f)\u2229U) = I((Z(f) \u2229 U)Zar), and conclude that dim(Z(f)\u2229U) = n\u22121. Note that so far we did not use that f was irreducible, as this is not required in (Bochnak et al., 1998, Prop. 3.3.10). To conclude (1), we use Proposition D.4, and the assumption that f is irreducible. The case where K = C is standard, and can be proven in a similar way.\nD.5. The connection with the sign of the polynomial\nLemma D.7 Suppose K = K is real closed. Let f(x1, . . . , xn) \u2208 K[x1, . . . , xn] be a nonconstant irreducible polynomial, and let U \u2286 Kn be an open subset in the Euclidean topology. Then the following are equivalent:\n1. (f) = I(Z(f) \u2229 U).\n2. The sign of the polynomial f changes on an open ball in U (i.e., there is an open ball B \u2286 U such that f(\u03b1)f(\u03b2) < 0 for some \u03b1, \u03b2 \u2208 B ).\nProof (1) =\u21d2 (2). Assuming (1), then from Lemma D.5, there is a point \u03b1(0) \u2208 Z(f) \u2229 U with (\u2202x1f(\u03b1 (0)), . . . , \u2202xnf(\u03b1 (0))) 6= 0 \u2208 Kn. In other words, there is an i such that \u2202xif(\u03b1(0)) 6= 0. Then consider the polynomial in one variable\n\u03c6(xi) := f(\u03b1 (0) 1 , . . . , xi, . . . , \u03b1 (0) n ).\nWe have \u03c6(\u03b1(0)i ) = 0. But since \u03c6 \u2032(\u03b1 (0) i ) = \u2202xif(\u03b1 (0) i ) is non-zero, the function \u03c6(xi) is monotone in a real interval around \u03b1(0)i , and so it changes sign (Bochnak et al., 1998, Cor. 1.2.7). Therefore f changes sign. (Note that we did not use that f was irreducible.)\n(2) =\u21d2 (1). (Bochnak et al., 1998, Lem. 4.5.2) states the following: Let B \u2286 Kn be an open ball (including the case where B = Kn) and let U1 and U2 be two disjoint nonempty semialgebraic open subsets of B . Then we have dim(B \u2212 (U1 \u222a U2)) \u2265 n\u2212 1. Now apply this in our situation, with\nU1 = {\u03b1 \u2208 B : f(\u03b1) > 0} and U2 = {\u03b1 \u2208 B : f(\u03b1) < 0},\nso that B \u2212 (U1 \u222a U2) = Z(f) \u2229B . Then\nn\u2212 1 = dimZ(f) \u2265 dim(Z(f) \u2229 U) \u2265 dim(Z(f) \u2229B ) \u2265 n\u2212 1.\nAs mentioned above, we have the equality dimZ(f) = n \u2212 1 on the left since f is neither a zero divisor nor a unit. Note that so far we did not use that f was irreducible. To conclude (1), we use Proposition D.4, and the assumption that f is irreducible.\nD.6. Proof of Theorem D.1\nProof [Proof of Theorem D.1] We have now proved the theorem under the hypothesis that f is irreducible (Proposition D.4, Lemma D.5, Lemma D.6, Lemma D.7). We now reduce to this case. First, it is clear that (2) \u21d0\u21d2 (3) \u21d0\u21d2 (4), from the irreducible case. Also, it is clear that if (1) holds (i.e., (f) = I(Z(f) \u2229 U)), we must have that m1 = m2 = \u00b7 \u00b7 \u00b7 = mr = 1. Indeed, if say m1 > 1, then f1fm22 \u00b7 \u00b7 \u00b7 fmrr \u2208 I(Z(f)\u2229U), but for degree reasons f1f m2 2 \u00b7 \u00b7 \u00b7 fmrr is not a multiple of f = fm11 \u00b7 \u00b7 \u00b7 fmrr and thus (1) fails. So from here on, we assume m1 = m2 = \u00b7 \u00b7 \u00b7 = mr = 1. (1) =\u21d2 (2). Suppose that (2) fails. Then there is some i, j so that \u2202xjfi \u2208 I(Z(fi) \u2229 U) and is nonzero. Therefore f1 \u00b7 \u00b7 \u00b7 \u2202xjfi \u00b7 \u00b7 \u00b7 fr \u2208 I(Z(f) \u2229 U)) and is nonzero. But for degree reasons, it is not a multiple of f = f1 \u00b7 \u00b7 \u00b7 fi \u00b7 \u00b7 \u00b7 fr and thus (1) fails.\n(2) =\u21d2 (1). This follows from the fact that r\u22c2 i=1 (fi) = (f1 \u00b7 \u00b7 \u00b7 fr) (K[x1, . . . , xn] is a UFD)\n= (f)\n\u2286 I(Z(f) \u2229 U))\n= I ( r\u22c3 i=1 (Z(fi) \u2229 U) )\n= r\u22c2 i=1 I(Z(fi) \u2229 U),\nsince, assuming (2) and the special case of Theorem D.1 for irreducible polynomials, then for all i, we have (fi) = I(Z(fi) \u2229 U), forcing the containment above to be an equality."}], "references": [{"title": "On consistent surrogate risk minimization and property elicitation", "author": ["Arpit Agarwal", "Shivani Agarwal"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Agarwal and Agarwal.,? \\Q2015\\E", "shortCiteRegEx": "Agarwal and Agarwal.", "year": 2015}, {"title": "Introduction to commutative algebra", "author": ["M.F. Atiyah", "I.G. Macdonald"], "venue": null, "citeRegEx": "Atiyah and Macdonald.,? \\Q1969\\E", "shortCiteRegEx": "Atiyah and Macdonald.", "year": 1969}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Robust optimization\u2013a comprehensive survey", "author": ["Hans-Georg Beyer", "Bernhard Sendhoff"], "venue": "Computer methods in applied mechanics and engineering,", "citeRegEx": "Beyer and Sendhoff.,? \\Q2007\\E", "shortCiteRegEx": "Beyer and Sendhoff.", "year": 2007}, {"title": "Real algebraic geometry, volume 36 of Ergebnisse der Mathematik und ihrer Grenzgebiete (3) [Results in Mathematics and Related Areas (3)", "author": ["Jacek Bochnak", "Michel Coste", "Marie-Fran\u00e7oise Roy"], "venue": "ISBN 3-540-64663-9", "citeRegEx": "Bochnak et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bochnak et al\\.", "year": 1998}, {"title": "Mathematical analysis: An introduction", "author": ["Andrew Browder"], "venue": "Undergraduate Texts in Mathematics. Springer-Verlag, New York,", "citeRegEx": "Browder.,? \\Q1996\\E", "shortCiteRegEx": "Browder.", "year": 1996}, {"title": "Ideals, varieties, and algorithms", "author": ["David A. Cox", "John Little", "Donal O\u2019Shea"], "venue": "Undergraduate Texts in Mathematics. Springer, Cham, fourth edition,", "citeRegEx": "Cox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cox et al\\.", "year": 2015}, {"title": "The statistical analysis of series of events", "author": ["David R Cox", "Peter AW Lewis"], "venue": "Monographs on Applied Probability and Statistics,", "citeRegEx": "Cox and Lewis.,? \\Q1966\\E", "shortCiteRegEx": "Cox and Lewis.", "year": 1966}, {"title": "The relation between the number of species and the number of individuals in a random sample of an animal population", "author": ["Ronald A. Fisher", "A. Steven Corbet", "Carrington B. Williams"], "venue": "The Journal of Animal Ecology,", "citeRegEx": "Fisher et al\\.,? \\Q1943\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1943}, {"title": "General truthfulness characterizations via convex analysis", "author": ["Rafael Frongillo", "Ian Kash"], "venue": "In Web and Internet Economics,", "citeRegEx": "Frongillo and Kash.,? \\Q2014\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2014}, {"title": "Vector-Valued Property Elicitation", "author": ["Rafael Frongillo", "Ian Kash"], "venue": "In Proceedings of the 28th Conference on Learning Theory, pages", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "On Elicitation Complexity and Conditional Elicitation", "author": ["Rafael Frongillo", "Ian A. Kash"], "venue": "arXiv preprint arXiv:1506.07212,", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "On Elicitation Complexity", "author": ["Rafael Frongillo", "Ian A. Kash"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frongillo and Kash.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo and Kash.", "year": 2015}, {"title": "Elicitation for Aggregation", "author": ["Rafael M. Frongillo", "Yiling Chen", "Ian A. Kash"], "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Frongillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo et al\\.", "year": 2015}, {"title": "Intersection theory, volume 2 of Ergebnisse der Mathematik und ihrer Grenzgebiete", "author": ["William Fulton"], "venue": null, "citeRegEx": "Fulton.,? \\Q1998\\E", "shortCiteRegEx": "Fulton.", "year": 1998}, {"title": "Making and Evaluating Point Forecasts", "author": ["T. Gneiting"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Gneiting.,? \\Q2011\\E", "shortCiteRegEx": "Gneiting.", "year": 2011}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "Eliciting truthful answers to multiple-choice questions", "author": ["Nicolas S. Lambert", "Yoav Shoham"], "venue": "In Proceedings of the 10th ACM Conference on Electronic Commerce,", "citeRegEx": "Lambert and Shoham.,? \\Q2009\\E", "shortCiteRegEx": "Lambert and Shoham.", "year": 2009}, {"title": "Eliciting properties of probability distributions", "author": ["Nicolas S. Lambert", "David M. Pennock", "Yoav Shoham"], "venue": "In Proceedings of the 9th ACM Conference on Electronic Commerce,", "citeRegEx": "Lambert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2008}, {"title": "Elicitation and Evaluation of Statistical Forecasts", "author": ["N.S. Lambert"], "venue": null, "citeRegEx": "Lambert.,? \\Q2011\\E", "shortCiteRegEx": "Lambert.", "year": 2011}, {"title": "Providing Incentives for Better Cost Forecasting", "author": ["Kent Harold Osband"], "venue": "University of California, Berkeley,", "citeRegEx": "Osband.,? \\Q1985\\E", "shortCiteRegEx": "Osband.", "year": 1985}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Mutual fund performance", "author": ["William F Sharpe"], "venue": "Journal of Business,", "citeRegEx": "Sharpe.,? \\Q1966\\E", "shortCiteRegEx": "Sharpe.", "year": 1966}, {"title": "Elicitation and Identification of Properties", "author": ["Ingo Steinwart", "Chlo\u00e9 Pasin", "Robert Williamson", "Siyu Zhang"], "venue": "In Proceedings of The 27th Conference on Learning Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2014}, {"title": "Learning deep embeddings with histogram loss", "author": ["Evgeniya Ustinova", "Victor Lempitsky"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ustinova and Lempitsky.,? \\Q2016\\E", "shortCiteRegEx": "Ustinova and Lempitsky.", "year": 2016}, {"title": "\u2206Y \u2032) is directly elicitable by the loss function ` if and only if there exists some convex G : conv(P \u2032) \u2192 R\u0304 with G(P \u2032) \u2286 R, some D \u2286 \u03b4G, and some bijection \u03c6 : \u0393(P \u2032) \u2192 D with \u0393(p) = \u03c6\u22121(D \u2229 \u03b4Gp)", "author": ["Frongillo", "Kash"], "venue": null, "citeRegEx": "Frongillo and Kash,? \\Q2014\\E", "shortCiteRegEx": "Frongillo and Kash", "year": 2014}, {"title": "Lem. 4.5.2) states the following: Let B \u2286 Kn be an open ball (including the case where B = Kn) and let U1 and U2 be two disjoint nonempty semialgebraic open subsets of B", "author": ["Bochnak et al", "Cor"], "venue": "(Bochnak et al.,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 18, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 15, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 23, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 0, "context": "As the choice of loss function used in ERM may have a large impact on the model chosen, how should one choose this loss? A growing body of work in property elicitation seeks to answer this question, by viewing a loss function as \u201cincentivizing\u201d the prediction of a particular conditional statistic (Lambert et al., 2008; Gneiting, 2011; Steinwart et al., 2014; Frongillo and Kash, 2015a; Agarwal and Agarwal, 2015); for example, it is well-known that squared loss elicits the mean, and hence least-squares regression finds the best fit to the conditional means of the data.", "startOffset": 298, "endOffset": 414}, {"referenceID": 18, "context": "The question of how many such auxiliary statistics are required gives rise to the concept of elicitation complexity; since the variance cannot be elicited with one but can with two, we say it is 2-elicitable (Lambert et al., 2008; Frongillo and Kash, 2015c).", "startOffset": 208, "endOffset": 257}, {"referenceID": 3, "context": "observations are readily obtained include: active learning, uncertainty quantification & robust engineering design (Beyer and Sendhoff, 2007), and replication of scientific experiments.", "startOffset": 115, "endOffset": 141}, {"referenceID": 16, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 21, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 24, "context": "Multi-observation losses have been previously introduced to learn embeddings (Hadsell et al., 2006; Schroff et al., 2015; Ustinova and Lempitsky, 2016), though an explicit property/statistic is never discussed.", "startOffset": 77, "endOffset": 151}, {"referenceID": 13, "context": "Related work Our work is inspired in part by Frongillo et al. (2015) which proposes a way to elicit the confidence (inverse of variance) of an agent\u2019s estimate of the bias of a coin by simply flipping it twice.", "startOffset": 45, "endOffset": 69}, {"referenceID": 14, "context": "Lambert et al. (2008); Steinwart et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "Lambert et al. (2008); Steinwart et al. (2014)).", "startOffset": 0, "endOffset": 47}, {"referenceID": 9, "context": "Following Frongillo and Kash (2015a), we will often assume that properties are identifiable.", "startOffset": 10, "endOffset": 37}, {"referenceID": 14, "context": "The question of elicitation complexity, studied by Lambert et al. (2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest \u0393 via some elicitable \u0393\u0302 : P \u2192 Rd; one hopes that d is much smaller than |Y|.", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "(2008) and Frongillo and Kash (2015c), is how many dimensions d are needed to indirectly elicit the property of interest \u0393 via some elicitable \u0393\u0302 : P \u2192 Rd; one hopes that d is much smaller than |Y|.", "startOffset": 11, "endOffset": 38}, {"referenceID": 18, "context": "Geometric Fundamentals The most basic (yet powerful) lower bound in property elicitation says that elicitable properties\u2019 level sets must be convex sets (Lambert et al., 2008).", "startOffset": 153, "endOffset": 175}, {"referenceID": 9, "context": "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some \u201cextension\u201d of that property on the convex hull of that set.", "startOffset": 43, "endOffset": 69}, {"referenceID": 9, "context": "A tighter sufficient condition is given by Frongillo and Kash (2014), which states that essentially all loss functions eliciting a property on any set, such as Pm, also elicit some \u201cextension\u201d of that property on the convex hull of that set. So while the higher-dimensional approach is helpful, it does not preclude reasoning about the space Pm as a manifold inside \u2206Ym . Most significantly, Pm is not a convex space, which makes lower bounds on elicitation complexity nontrivial as well. However, the result of Frongillo and Kash (2014) shows that it suffices to provide lower bounds for elicitation on the convex hull of Pm, which we will denote conv(Pm).", "startOffset": 43, "endOffset": 538}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009).", "startOffset": 48, "endOffset": 74}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009). In this section, we must allow \u0393 : P \u21d2 R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for \u201cboundary\u201d cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = \u2206Y , and \u0393(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex \u2206Y .", "startOffset": 49, "endOffset": 669}, {"referenceID": 17, "context": "This corresponds to a \u201cmultiplechoice question\u201d (Lambert and Shoham, 2009). In this section, we must allow \u0393 : P \u21d2 R to be a set-valued function, possibly assigning multiple possible correct reports to a single distribution; this is necessary for \u201cboundary\u201d cases, such as the mode of the uniform distribution on a finite set. (Similarly, we cannot require identifiability.) We have a finite set of outcomes Y , the distributions considered are all P = \u2206Y , and \u0393(p) must be nonempty. We are interested in understanding which finite properties can be elicited with m observations. Previously, this question was studied for the case of one observation by Lambert (2011), who characterized elicitable properties by the shape of their level sets: they are intersections of Voronoi diagrams in R|Y| with the simplex \u2206Y . In our setting, a Voronoi diagram is specified by a finite set of points {xr : r \u2208 R} \u2286 RY m , with each cell Tr = {x : \u2016x \u2212 xr\u2016 \u2264 \u2016x \u2212 xr\u2032\u2016\u2200r \u2208 R} consisting of those points in RYm closest in Euclidean distance to xr. Using the geometric constructions above, we can simply apply the main result of Lambert (2011) to finite properties in the m-product space; the result is a characterization of elicitable finite properties with m observations.", "startOffset": 49, "endOffset": 1131}, {"referenceID": 9, "context": "Preliminaries on identifiable properties We start by recalling a general method, introduced in Frongillo and Kash (2015c), for showing lower bounds on elicitation complexity: Given a property \u0393, if one can show that no level set from any \u0393\u0302, which is m-identifiable and directly elicitable with m observations, can be contained in a particular level set of \u0393, then \u0393 cannot be (d,m)-elicitable.", "startOffset": 95, "endOffset": 122}, {"referenceID": 9, "context": "This method was used successfully in Frongillo and Kash (2015c) to show lower bounds on the report complexity (d) of", "startOffset": 37, "endOffset": 64}, {"referenceID": 5, "context": ", Browder (1996), Theorem 6.", "startOffset": 2, "endOffset": 17}, {"referenceID": 15, "context": "By a very natural extension, this technique also applies to ratios of expectations, as they are elicitable (Gneiting, 2011): construct two unbiased estimators, and elicit the ratio of their means.", "startOffset": 107, "endOffset": 123}, {"referenceID": 7, "context": "Ratios of expectations: index of dispersion and Sharpe ratio The index of dispersion of a random variable Y with positive mean is defined to be Var(Y )/E[Y ] (Cox and Lewis, 1966).", "startOffset": 158, "endOffset": 179}, {"referenceID": 22, "context": "The Sharpe ratio of a random variable Y , which is a commonly-used measure of the risk-adjusted return of an investment, is defined similarly as E[Y ]/ \u221a Var(Y ) (Sharpe, 1966).", "startOffset": 162, "endOffset": 176}, {"referenceID": 9, "context": "This follows from Theorem 2 of Frongillo and Kash (2015c), specifically Section 4.", "startOffset": 31, "endOffset": 58}, {"referenceID": 8, "context": "Multi-Observation Regression One of the earliest problems in modern statistics was the estimation of biodiversity in a geographic region (Fisher et al., 1943).", "startOffset": 137, "endOffset": 158}], "year": 2017, "abstractText": "We study loss functions that measure the accuracy of a prediction based on multiple data points simultaneously. To our knowledge, such loss functions have not been studied before in the area of property elicitation or in machine learning more broadly. As compared to traditional loss functions that take only a single data point, these multi-observation loss functions can in some cases drastically reduce the dimensionality of the hypothesis required. In elicitation, this corresponds to requiring many fewer reports; in empirical risk minimization, it corresponds to algorithms on a hypothesis space of much smaller dimension. We explore some examples of the tradeoff between dimensionality and number of observations, give some geometric characterizations and intuition for relating loss functions and the properties that they elicit, and discuss some implications for both elicitation and machine-learning contexts.", "creator": "LaTeX with hyperref package"}}}