{"id": "1706.01450", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "A Joint Model for Question Answering and Question Generation", "abstract": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks.\n\n\n\n\n\n\n\nThe project aims to establish a comprehensive classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to develop a highly scalable machine learning framework with a very large subset of information sources. We are currently working on a model for the classification of each source, using multiple sources to support the classification and to allow the authors to collaborate on the model.\n\n\n\nOur goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of each source, using multiple sources to support the classification and to allow the authors to collaborate on the model.\n\nOur goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools.\nOur goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to be a well-known model for the classification of all data sources to be assembled within 10 to 15 years by applying a large subset of these tools.\nWe propose a classification of data sources to be assembled within 10 to 15 years by applying a large subset of these tools. Our goal is to", "histories": [["v1", "Mon, 5 Jun 2017 17:58:52 GMT  (189kb,D)", "http://arxiv.org/abs/1706.01450v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["tong wang", "xingdi yuan", "adam trischler"], "accepted": false, "id": "1706.01450"}, "pdf": {"name": "1706.01450.pdf", "metadata": {"source": "META", "title": "A Joint Model for Question Answering and Question Generation", "authors": ["Tong Wang", "Xingdi Yuan", "Adam Trischler"], "emails": ["<tong.wang@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "Question answering (QA) is the task of automatically producing an answer to a question given a corresponding document. It not only provides humans with efficient access to vast amounts of information, but also acts as an important proxy task to assess machine literacy via reading comprehension. Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016). However, previous models do not treat QA as a task of natural language generation (NLG), but of pointing to an answer span within a document.\nAlongside QA, question generation has also gained increased popularity (Du et al., 2017; Yuan et al., 2017). The task is to generate a natural-language question conditioned on an answer and the corresponding document. Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017; Serban et al., 2016; Yang et al., 2017). A recurring theme among previous\n*Equal contribution 1Microsoft Maluuba. Correspondence to: Tong Wang <tong.wang@microsoft.com>.\nstudies is to augment existing labeled data with machinegenerated questions; to our knowledge, the direct (though implicit) effect of asking questions on answering questions has not yet been explored.\nIn this work, we propose a joint model that both asks and answers questions, and investigate how this joint-training setup affects the individual tasks. We hypothesize that question generation can help models achieve better QA performance. This is motivated partly by observations made in psychology that devising questions while reading can increase scores on comprehension tests (Singer & Donlan, 1982). Our joint model also serves as a novel framework for improving QA performance outside of the networkarchitectural engineering that characterizes most previous studies.\nAlthough the question answering and asking tasks appear symmetric, there are some key differences. First, answering the questions in most existing QA datasets is extractive \u2014 it requires selecting some span of text within the document \u2014 while question asking is comparatively abstractive \u2014 it requires generation of text that may not appear in the document. Furthermore, a (document, question) pair typically specifies a unique answer. Conversely, a typical (document, answer) pair may be associated with multiple questions, since a valid question can be formed from any information or relations which uniquely specify the given answer.\nTo tackle the joint task, we construct an attentionbased (Bahdanau et al., 2014) sequence-to-sequence model (Sutskever et al., 2014) that takes a document as input and generates a question (answer) conditioned on an answer (question) as output. To address the mixed extractive/abstractive nature of the generative targets, we use the pointer-softmax mechanism (Gulcehre et al., 2016) that learns to switch between copying words from the document and generating words from a prescribed vocabulary. Joint training is realized by alternating the input data between question-answering and question-generating examples for the same model. We demonstrate empirically that this model\u2019s QA performance on SQuAD, while not state of the art, improves by about 10% with joint training. A key novelty of our joint model is that it can generate (partially) abstractive answers. ar X\niv :1\n70 6.\n01 45\n0v 1\n[ cs\n.C L\n] 5\nJ un\n2 01\n7"}, {"heading": "2. Related Work", "text": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016). In machine translation, for instance, Firat et al. (2016) demonstrated that translation quality clearly improves over models trained with a single language pair when the attention mechanism in a neural translation model is shared and jointly trained on multiple language pairs.\nIn question answering, Wang & Jiang (2016) proposed one of the first neural models for the SQuAD dataset. SQuAD defines an extractive QA task wherein answers consist of word spans in the corresponding document. Wang & Jiang (2016) demonstrated that learning to point to answer boundaries is more effective than learning to point sequentially to the tokens making up an answer span. Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016). However, the boundarypointing mechanism is not suitable for more open-ended tasks, including abstractive QA (Nguyen et al., 2016) and question generation. While \u201cforcing\u201d the extractive boundary model onto abstractive datasets currently yields stateof-the-art results (Wang et al., 2017), this is mainly because current generative models are poor and NLG evaluation is unsolved.\nEarlier work on question generation has resorted to either rule-based reordering methods (Heilman & Smith, 2010; Agarwal & Mannem, 2011; Ali et al., 2010) or slot-filling with question templates (Popowich & Winne, 2013; Chali & Golestanirad, 2016; Labutov et al., 2015). These techniques often involve pipelines of independent components that are difficult to tune for final performance measures. Partly to address this limitation, end-to-end-trainable neural models have recently been proposed for question generation in both vision (Mostafazadeh et al., 2016) and language. For example, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states. Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.\nSeveral neural models with a questioning component have been proposed for the purpose of improving QA models, an objective shared by this study. Yang et al. (2017) devised a semi-supervised training framework that trained a QA model (Dhingra et al., 2016) on both labeled data and artificial data generated by a separate generative component. Buck et al. (2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset (Dunn et al., 2017). The generated questions were then used to further train an existing QA model (Seo et al., 2016). A key distinction of our model is that we harness the process\nof asking questions to benefit question answering, without training the model to answer the generated questions."}, {"heading": "3. Model Description", "text": "Our proposed model adopts a sequence-to-sequence framework (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2014) and a pointer-softmax decoder (Gulcehre et al., 2016). Specifically, the model takes a document (i.e., a word sequence) D = (wd1 , . . . , w d nd ) and a condition sequence C = (wc1, . . . , w c nc) as input, and outputs a target sequence Y {q,a} = (w\u03021, . . . , w\u0302np). The condition corresponds to the question word sequence in answer-generation mode (a-gen), and the answer word sequence in question-generation mode (q-gen). We also attach a binary variable to indicate whether a data-point is intended for a-gen or q-gen. Intuitively, this should help the model learn the two modalities more easily. Empirically, QA performance improves slightly with this addition.\nEncoder\nA word wi in an input sequence is first embedded with an embedding layer into vector ewi . Character-level information is captured with the final states echi of a bidirectional Long Short-Term Memory model (Hochreiter & Schmidhuber, 1997) on the character sequences of wi. The final representation for a word token ei = \u3008ewi , echi \u3009 concatenates the word- and character-level embeddings. These are subsequently encoded with another BiLSTM into annotation vectors hdi and h c j (for the document and the condition sequence, respectively).\nTo better encode the condition, we also extract the encodings of the document words that appear in the condition sequence. This procedure is particularly helpful in q-gen mode, where the condition (answer) sequence is typically extractive. These extracted vectors are then fed into a condition aggregation BiLSTM to produce the extractive condition encoding hek. We specifically take the final states of the condition encodings hcJ and h e K . To account for the different extractive vs. abstractive nature of questions vs. answers, we use hcJ in a-gen mode (for encoding questions) and heK in q-gen mode (for encoding answers).\nDecoder\nThe RNN-based decoder employs the pointer-softmax mechanism (Gulcehre et al., 2016). At each generation step, the decoder decides adaptively whether (a) to generate from a decoder vocabulary or (b) to point to a word in the source sequence (and copy over). Recurrence of the pointing decoder is implemented with two LSTM cells c1\nand c2:\ns (t) 1 = c1(y (t\u22121), s (t\u22121) 2 ) (1) s (t) 2 = c2(v (t), s (t) 1 ), (2)\nwhere s(t)1 and s (t) 2 are the recurrent states, y (t\u22121) is the embedding of decoder output from the previous time step, and v(t) is the context vector (to be defined shortly in Equation (3)).\nThe pointing decoder computes a distribution \u03b1(t) over the document word positions (i.e., a document attention, Bahdanau et al. 2014). Each element is defined as:\n\u03b1 (t) i = f(h d i ,h c,he, s1 (t\u22121)),\nwhere f is a two-layer MLP with tanh and softmax activation, respectively. The context vector v(t) used in Equation (2) is the sum of the document encoding weighted by the document attention:\nv(t) = n\u2211 i=1 \u03b1 (t) i h d i . (3)\nThe generative decoder, on the other hand, defines a distribution over a prescribed decoder vocabulary with a twolayer MLP g:\no(t) = g(y(t\u22121), s (t) 2 ,v (t),hc,he). (4)\nFinally, the switch scalar s(t) at each time step is computed by a three-layer MLP h:\ns(t) = h(s (t) 2 ,v (t),\u03b1(t),o(t)),\nThe first two layers of h use tanh activation and the final layer uses sigmoid activation, and highway connections are present between the first and the second layer. We also attach the entropy of the softmax distributions to the input of the final layer, postulating that the quantities should help guide the switching mechanism by indicating the confidence of pointing vs generating. The addition is empirically observed to improve model performance.\nThe resulting switch is used to interpolate the pointing and the generative probabilities for predicting the next word:\np(w\u0302t) \u223c s(t)\u03b1(t) + (1\u2212 s(t))o(t)."}, {"heading": "4. Training and Inference", "text": "The optimization objective for updating the model parameters \u03b8 is to maximize the negative log likelihood of the generated sequences with respect to the training data D:\nL = \u2212 \u2211 x\u2208D log p(w\u0302t|w<t, x; \u03b8).\nHere, w<t corresponds to the embeddings y(t\u22121) in Equation (1) and (4). During training, gold targets are used to teacher-force the sequence generation for training, i.e., w<t = w {q,a} <t , while during inference, generation is conditioned on the previously generated words, i.e., w<t = w\u0302<t.\nFor words with multiple occurrence, since their exact references in the document cannot be reiabled determined, we aggregate the probability of these words in the encoder and the pointing decoder (similar to Kadlec et al. 2016). At test time, beam search is used to enhance fluency in the question-generation output.1 The decoder also keeps an explicit history of previously generated words to avoid repetition in the output."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Dataset", "text": "We conduct our experiments on the SQuAD corpus (Rajpurkar et al., 2016), a machine comprehension dataset consisting of over 100k crowd-sourced question-answer pairs on 536 Wikipedia articles. Simple preprocessing is performed, including lower-casing all texts in the dataset and using NLTK (Bird, 2006) for word tokenization. The test split of SQuAD is hidden from the public. We therefore take 5,158 question-answer pairs (self-contained in 23 Wikipedia articles) from the training set as validation set, and use the official development data to report test results. Note that answers in this dataset are strictly extractive, and we therefore constrain the pointer-softmax module to point at all decoding steps in answer generation mode."}, {"heading": "5.2. Baseline Models", "text": "We first establish two baselines without multi-task training. Specifically, model A-gen is trained only to generate an answer given a document and a question, i.e., as a conventional QA model. Analogously, model Q-gen is trained only to generate questions from documents and answers. Joint-training (in model JointQA) is realized by feeding answer-generation and question-generation data to the model in an alternating fashion between mini-batches.\nIn addition, we compare answer-generation performance with the sequence model variant of the match-LSTM (mLSTM) model (Wang & Jiang, 2016). As mentioned earlier, in contrast to existing neural QA models that point to the start and end boundaries of extractive answers, this model predicts a sequence of document positions as the answer. This makes it most comparable to our QA setup. Note, however, that our model has the additional capacity\n1The effectiveness of beam search can be undermined by the generally diminished output length. We therefore do not use beam search in a-gen mode, which also saves training time.\nto generate abstractively from the decoder vocabulary."}, {"heading": "5.3. Quantitative Evaluation", "text": "We use F1 and Exact Match (EM, Rajpurkar et al. 2016) against the gold answer sequences to evaluate answer generation, and BLEU2 (Papineni et al., 2002) against the gold question sequences to evaluate question generation. However, existing studies have shown that the task of question generation often exhibits linguistic variance that is semantically admissible; this renders it inappropriate to judge a generated question solely by matching against a gold sequence (Yuan et al., 2017). We therefore opt to assess the quality of generated questions Y q with two pretrained neural models as well: we use a language model to compute the perplexity of Y q , and a QA model to answer Y q . We measure the F1 score of the answer produced by this QA model.\nWe choose mLSTM as the pretrained QA model and train it on SQuAD with the same split as mentioned in Section 5.1. Performance on the test set (i.e., the official validation set of SQuAD) is 73.78 F1 and 62.7 EM. For the pretrained language model, we train a single-layer LSTM language model on the combination of the text8 corpus3, the Quora Question Pairs corpus4, and the gold questions from SQuAD. The latter two corpora were included to tailor to our purpose of assessing question fluency, and for this reason, we ignore the semantic equivalence labels in the Quora dataset. Validation perplexity is 67.2 for the pretrained language model."}, {"heading": "5.4. Analysis and Discussion", "text": "Evaluation results are provided in Table 1. We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. Performance of q-gen worsens after joint training, but the decrease is relatively small. Furthermore, as pointed\n2We use the Microsoft COCO Caption Evaluation scripts (https://github.com/tylin/coco-caption) to calculate BLEU scores.\n3http://mattmahoney.net/dc/textdata 4https://data.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\nout by earlier studies, automatic metrics often do not correlate well with the generation quality assessed by humans (Yuan et al., 2017). We thus consider the overall outcome to be positive.\nMeanwhile, although our model does not perform as well as mLSTM on the QA task, it has the added capability of generating questions. mLSTM uses a more advanced encoder tailored to QA, while our model uses only a bidirectional LSTM for encoding. Our model uses a more advanced decoder based on the pointer-softmax that enables it to generate abstactively and extractively.\nFor a finer grained analysis, we first categorize test set answers based on their entity types, then stratify the QA performance comparison between A-gen and JointQA. The categorization relies on Stanford CoreNLP (Manning et al., 2014) to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al. 2016 for more details). As seen in Figure 1, the joint model significantly outperforms the single model in all categories. Interestingly, the moving average of the performance gap (dashed curve above bars) exhibits an upward trend as the A-gen model performance decreases across answer types, suggesting that the joint model helps most where the single model performance is weakest."}, {"heading": "5.5. Qualitative Examples", "text": "Qualitatively, we have observed interesting \u201cshifts\u201d in attention before and after joint training. For example, in the positive case in Table 2, the gold question asks about the direct object,Nixon, of the verb endorse, but the A-gen model predicts the indirect object, Kennedy, instead. In contrast, the joint model asks about the appositive of vice president during question generation, which presumably\n\u201cprimes\u201d the model attention towards the correct answer Nixon. Analogously in the negative example, QA attention in the joint model appears to be shifted by joint training towards an answer that is incorrect but closer to the generated question.\nNote that the examples from Table 2 come from the validation set, and it is thus not possible for the joint model to memorize the gold answers from question-generation mode \u2014 the priming effect must come from some form of knowledge transfer between q-gen and a-gen via joint training."}, {"heading": "5.6. Implementation Details", "text": "Implementation details of the proposed model are as follows. The encoder vocabulary indexes all words in the dataset. The decoder vocabulary uses the top 100 words sorted by their frequency in the gold questions in the training data. This encourages the model to generate frequent words (e.g. wh-words and function words) from the decoder vocabulary and copy less frequent ones (e.g., topical words and entities) from the document.\nThe word embedding matrix is initialized with the 300- dimensional GloVe vectors (Pennington et al., 2014). The dimensionality of the character representations is 32. The number of hidden units is 384 for both of the encoder/decoder RNN cells. Dropout is applied at a rate of 0.3 to all embedding layers as well as between the hidden states in the encoder/decoder RNNs across time steps.\nWe use adam (Kingma & Ba, 2014) as the step rule for optimization with mini-batch size 32. The initial learning rate is 2e \u2212 4, which is decayed at a rate of 0.5 when the validation loss increases for two consecutive epochs.\nThe model is implemented using Keras (Chollet et al., 2015) with the Theano (Al-Rfou et al., 2016) backend."}, {"heading": "6. Conclusion", "text": "We proposed a neural machine comprehension model that can jointly ask and answer questions given a document. We hypothesized that question answering can benefit from synergistic interaction between the two tasks through parameter sharing and joint training under this multitask setting. Our proposed model adopts an attention-based sequenceto-sequence architecture that learns to dynamically switch between copying words from the document and generating words from a vocabulary. Experiments with the model confirm our hypothesis: the joint model outperforms its QAonly counterpart by a significant margin on the SQuAD dataset.\nAlthough evaluation scores are still lower than the stateof-the-art results achieved by dedicated QA models, the proposed model nonetheless demonstrates the effectiveness of joint training between QA and question generation, and thus offers a novel perspective and a promising direction for advancing the study of QA."}], "references": [{"title": "Automatic gapfill question generation from text books", "author": ["Agarwal", "Manish", "Mannem", "Prashanth"], "venue": "In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Nltk: the natural language toolkit", "author": ["Bird", "Steven"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "Bird and Steven.,? \\Q2006\\E", "shortCiteRegEx": "Bird and Steven.", "year": 2006}, {"title": "Ask the right questions: Active question reformulation with reinforcement learning", "author": ["Buck", "Christian", "Bulian", "Jannis", "Ciaramita", "Massimiliano", "Gesmundo", "Andrea", "Houlsby", "Neil", "Gajewski", "Wojciech", "Wang", "Wei"], "venue": "arXiv preprint arXiv:1705.07830,", "citeRegEx": "Buck et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2017}, {"title": "Ranking automatically generated questions using common human queries", "author": ["Chali", "Yllias", "Golestanirad", "Sina"], "venue": "In The 9th International Natural Language Generation conference,", "citeRegEx": "Chali et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chali et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Gated-attention readers for text comprehension", "author": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Learning to ask: Neural question generation for reading comprehension", "author": ["Du", "Xinya", "Shao", "Junru", "Cardie", "Claire"], "venue": "arXiv preprint arXiv:1705.00106,", "citeRegEx": "Du et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Du et al\\.", "year": 2017}, {"title": "Searchqa: A new q&a dataset augmented with context from a search engine", "author": ["Dunn", "Matthew", "Sagun", "Levent", "Higgins", "Mike", "Guney", "Ugur", "Cirik", "Volkan", "Cho", "Kyunghyun"], "venue": "arXiv preprint arXiv:1704.05179,", "citeRegEx": "Dunn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat", "Orhan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1601.01073,", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Gulcehre", "Caglar", "Ahn", "Sungjin", "Nallapati", "Ramesh", "Zhou", "Bowen", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny Rose", "Bethard", "Steven", "McClosky", "David"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Generating natural questions about an image", "author": ["Mostafazadeh", "Nasrin", "Misra", "Ishan", "Devlin", "Jacob", "Mitchell", "Margaret", "He", "Xiaodong", "Vanderwende", "Lucy"], "venue": "arXiv preprint arXiv:1603.06059,", "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Nguyen", "Tri", "Rosenberg", "Mir", "Song", "Xia", "Gao", "Jianfeng", "Tiwary", "Saurabh", "Majumder", "Rangan", "Deng", "Li"], "venue": "arXiv preprint arXiv:1611.09268,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Generating natural language questions to support learning on-line", "author": ["Popowich", "David Lindberg Fred", "Winne", "John Nesbit Phil"], "venue": "ENLG", "citeRegEx": "Popowich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Popowich et al\\.", "year": 2013}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Rajpurkar", "Pranav", "Zhang", "Jian", "Lopyrev", "Konstantin", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": "arXiv preprint arXiv:1611.01603,", "citeRegEx": "Seo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Shen", "Yelong", "Huang", "Po-Sen", "Gao", "Jianfeng", "Chen", "Weizhu"], "venue": "arXiv preprint arXiv:1609.05284,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Active comprehension: Problem-solving schema with question generation for comprehension of complex short stories", "author": ["Singer", "Harry", "Donlan", "Dan"], "venue": "Reading Research Quarterly,", "citeRegEx": "Singer et al\\.,? \\Q1982\\E", "shortCiteRegEx": "Singer et al\\.", "year": 1982}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Trischler", "Adam", "Wang", "Tong", "Yuan", "Xingdi", "Harris", "Justin", "Sordoni", "Alessandro", "Bachman", "Philip", "Suleman", "Kaheer"], "venue": "arXiv preprint arXiv:1611.09830,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Wang", "Shuohang", "Jiang", "Jing"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wang", "Wenhui", "Yang", "Nan", "Wei", "Furu", "Chang", "Baobao", "Zhou", "Ming"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Semi-supervised qa with generative domain-adaptive nets", "author": ["Yang", "Zhilin", "Hu", "Junjie", "Salakhutdinov", "Ruslan", "Cohen", "William W"], "venue": "arXiv preprint arXiv:1702.02206,", "citeRegEx": "Yang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Machine comprehension by text-to-text neural question generation", "author": ["Yuan", "Xingdi", "Wang", "Tong", "Gulcehre", "Caglar", "Sordoni", "Alessandro", "Bachman", "Philip", "Subramanian", "Sandeep", "Zhang", "Saizheng", "Trischler", "Adam"], "venue": null, "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 20, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 8, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 25, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 17, "context": "Thanks to the recent release of several large-scale machine comprehension/QA datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Dunn et al., 2017; Trischler et al., 2016; Nguyen et al., 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al.", "startOffset": 86, "endOffset": 196}, {"referenceID": 27, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 22, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 21, "context": ", 2016), the field has undergone significant advancement, with an array of neural models rapidly approaching human parity on some of these benchmarks (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 150, "endOffset": 206}, {"referenceID": 7, "context": "Alongside QA, question generation has also gained increased popularity (Du et al., 2017; Yuan et al., 2017).", "startOffset": 71, "endOffset": 107}, {"referenceID": 3, "context": "Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017; Serban et al., 2016; Yang et al., 2017).", "startOffset": 85, "endOffset": 144}, {"referenceID": 28, "context": "Among its many applications, question generation has been used to improve QA systems (Buck et al., 2017; Serban et al., 2016; Yang et al., 2017).", "startOffset": 85, "endOffset": 144}, {"referenceID": 1, "context": "To tackle the joint task, we construct an attentionbased (Bahdanau et al., 2014) sequence-to-sequence model (Sutskever et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 24, "context": ", 2014) sequence-to-sequence model (Sutskever et al., 2014) that takes a document as input and generates a question (answer) conditioned on an answer (question) as output.", "startOffset": 35, "endOffset": 59}, {"referenceID": 10, "context": "To address the mixed extractive/abstractive nature of the generative targets, we use the pointer-softmax mechanism (Gulcehre et al., 2016) that learns to switch between copying words from the document and generating words from a prescribed vocabulary.", "startOffset": 115, "endOffset": 138}, {"referenceID": 5, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016).", "startOffset": 70, "endOffset": 114}, {"referenceID": 9, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016).", "startOffset": 70, "endOffset": 114}, {"referenceID": 5, "context": "Joint-learning on multiple related tasks has been explored previously (Collobert et al., 2011; Firat et al., 2016). In machine translation, for instance, Firat et al. (2016) demonstrated that translation quality clearly improves over models trained with a single language pair when the attention mechanism in a neural translation model is shared and jointly trained on multiple language pairs.", "startOffset": 71, "endOffset": 174}, {"referenceID": 27, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 22, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 21, "context": "Many later studies adopted this boundary model and achieved nearhuman performance on the task (Wang et al., 2017; Shen et al., 2016; Seo et al., 2016).", "startOffset": 94, "endOffset": 150}, {"referenceID": 17, "context": "However, the boundarypointing mechanism is not suitable for more open-ended tasks, including abstractive QA (Nguyen et al., 2016) and question generation.", "startOffset": 108, "endOffset": 129}, {"referenceID": 27, "context": "While \u201cforcing\u201d the extractive boundary model onto abstractive datasets currently yields stateof-the-art results (Wang et al., 2017), this is mainly because current generative models are poor and NLG evaluation is unsolved.", "startOffset": 113, "endOffset": 132}, {"referenceID": 16, "context": "Partly to address this limitation, end-to-end-trainable neural models have recently been proposed for question generation in both vision (Mostafazadeh et al., 2016) and language.", "startOffset": 137, "endOffset": 164}, {"referenceID": 7, "context": "For example, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states.", "startOffset": 13, "endOffset": 30}, {"referenceID": 7, "context": "For example, Du et al. (2017) used a sequence-tosequence model with an attention mechanism derived from the encoder states. Yuan et al. (2017) proposed a similar architecture but in addition improved model performance through policy gradient techniques.", "startOffset": 13, "endOffset": 143}, {"referenceID": 6, "context": "(2017) devised a semi-supervised training framework that trained a QA model (Dhingra et al., 2016) on both labeled data and artificial data generated by a separate generative component.", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "(2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset (Dunn et al., 2017).", "startOffset": 145, "endOffset": 164}, {"referenceID": 21, "context": "The generated questions were then used to further train an existing QA model (Seo et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 24, "context": "Yang et al. (2017) devised a semi-supervised training framework that trained a QA model (Dhingra et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Buck et al. (2017) used policy gradient with a QA reward to train a sequence-to-sequence paraphrase model to reformulate questions in an existing QA dataset (Dunn et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "Our proposed model adopts a sequence-to-sequence framework (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 1, "context": ", 2014) with an attention mechanism (Bahdanau et al., 2014) and a pointer-softmax decoder (Gulcehre et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 10, "context": ", 2014) and a pointer-softmax decoder (Gulcehre et al., 2016).", "startOffset": 38, "endOffset": 61}, {"referenceID": 10, "context": "The RNN-based decoder employs the pointer-softmax mechanism (Gulcehre et al., 2016).", "startOffset": 60, "endOffset": 83}, {"referenceID": 20, "context": "We conduct our experiments on the SQuAD corpus (Rajpurkar et al., 2016), a machine comprehension dataset consisting of over 100k crowd-sourced question-answer pairs on 536 Wikipedia articles.", "startOffset": 47, "endOffset": 71}, {"referenceID": 15, "context": "The categorization relies on Stanford CoreNLP (Manning et al., 2014) to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 18, "context": "The word embedding matrix is initialized with the 300dimensional GloVe vectors (Pennington et al., 2014).", "startOffset": 79, "endOffset": 104}], "year": 2017, "abstractText": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model\u2019s novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.", "creator": "LaTeX with hyperref package"}}}