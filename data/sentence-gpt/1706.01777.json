{"id": "1706.01777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2017", "title": "Deep Factorization for Speech Signal", "abstract": "Speech signals are complex intermingling of various informative factors, and this information blending makes decoding any of the individual factors extremely difficult. A natural idea is to factorize each speech frame into independent factors, though it turns out to be even more difficult than decoding each individual factor. A major encumbrance is that the speaker trait, a major factor in speech signals, has been suspected to be a long-term distributional pattern and so not identifiable at the frame level.\n\n\nIn short, this may be a sign that the neural processes are changing in terms of the signal strength. When a speech is composed of multiple parts of a speaker, one group is expected to be able to identify different parts of the speaker and then perform the same procedure. Thus, decoding each aspect of the speaker will be difficult to interpret from the viewpoint of the speaker, and may not be a viable method to interpret from the viewpoint of the speaker. In general, encoding a speech is difficult, and there are several mechanisms to interpret a speech. First, the decoding factor and the encoding factor can be easily modulated to distinguish between different parts of the speaker and his or her body. As a result, the encoding factor, as a matter of fact, can be highly sensitive for encoding a given speaker. Furthermore, this can affect speech quality, because these factors cannot be interpreted accurately. For example, as a matter of fact, one speaker's body is more sensitive than another's body.\nThe encoded factor may be sensitive to different factors, or may not be used as a method to evaluate the quality of the speaker. For example, if a particular speaker is very sensitive to different factors, the encoding factor may be used to assess his or her body's sensitivity to different factors. However, there is a very specific process of learning about each part of the speaker. For example, if a speaker is very sensitive to different factors, the encoding factor may be used to evaluate the quality of his or her body's sensitivity to different factors. Thus, encoding an encoded speech may be helpful to recognize the parts of the speaker and then evaluate the quality of the speaker. However, as a matter of fact, the encoding factor may be used to assess the quality of the speaker and then evaluate the quality of the speaker.\nTo interpret a speech, the encoding factor may be used to evaluate a speech given to a particular speaker, depending on the context. For example, if a speaker is very sensitive to different factors, the encoding factor may be used to evaluate the quality of", "histories": [["v1", "Mon, 5 Jun 2017 15:02:39 GMT  (2458kb,D)", "http://arxiv.org/abs/1706.01777v1", null], ["v2", "Sun, 25 Jun 2017 10:10:35 GMT  (2458kb,D)", "http://arxiv.org/abs/1706.01777v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["dong wang", "lantian li", "ying shi", "yixiang chen", "zhiyuan tang"], "accepted": false, "id": "1706.01777"}, "pdf": {"name": "1706.01777.pdf", "metadata": {"source": "CRF", "title": "Deep Factorization for Speech Signal", "authors": ["Dong Wang", "Lantian Li", "Ying Shi", "Yixiang Chen", "Zhiyuan Tang"], "emails": ["wangdong99@mails.tsinghua.edu.cn", "lilt13@mails.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Speech signals are mysterious and fascinating: within just one dimensional vibration, very rich information is represented, including linguistic content, speaker trait, emotion, channel and noise. Scientists have worked for several decades to decode speech, with different goals that focus on different informative factors within the signal. This leads to a multitude of speech information processing tasks, where automatic speech recognition (ASR) and speaker recognition (SRE) are among the most important [1]. After decades of research, some tasks have been addressed pretty well, at least with large amounts of data, e.g., ASR and SRE, while others remain difficult, e.g., automatic emotion recognition (AER) [2].\nA major difficulty of speech processing resides in the fact that multiple informative factors are intermingled together, and therefore whenever we decode for a particular factor, all other factors contribute as uncertainties. A natural idea to deal with the information blending is to factorize the signal into independent informative factors, so that each task can take its relevant factors. Unfortunately, this factorization turns out to be very difficult, in fact more difficult than decoding for individual factors. The main reason is that how the factors are intermingled to compose the speech signal and how they impact each other is far from clear to the speech community, which makes designing a simple yet effective factorization formula nearly impossible.\nAs an example, the two most significant factors, linguistic contents and speaker traits, corresponding to what has been spoken and who has spoken, hold a rather complex correlation. Here \u2018significant factors\u2019 refer to those factors that cause significant variations within speech signals. Researchers have put much effort to factorize speech signals based on these two factors, especially in SRE research. In fact, most of the famous SRE techniques are based on factorization models, including the Gaussian mixture model-universal background model (GMM-UBM) [3], the joint factor analysis\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n01 77\n7v 1\n[ cs\n.S D\n] 5\nJ un\n2 01\n7\n(JFA) [4] and the i-vector model [5]. With these models, the variation caused by the linguistic factor is explained away, which makes the speaker factor easier to identity (infer). Although significant success has been achieved, all these models assume a linear Gaussian relation between the linguistic, speaker and other factors, which is certainly over simplified. Essentially, they all perform shallow and linear factorization, and the speaker factors inferred are long-term distributional patterns rather than short-time spectral patterns.\nIt would be very disappointing if the speaker factor is really a distributional pattern in nature, as it would mean that speaker traits are too volatile to be identified from a short-time speech segment. If this is true, then it would be hopeless to factorize speech signals into independent factors at the frame level, and for most speech processing tasks, we have to resort to complex probabilistic models to collect statistics from long speech segments. This notion has in fact been subconsciously embedded into the thought process of many speech researchers, partly due to the brilliant success of probabilistic models on SRE.\nFortunately, our discovery reported in this paper demonstrated that the speaker trait is essentially a short-time spectral pattern, the same as the linguistic content. We designed a deep neural network (DNN) that can learn speaker traits pretty well from raw speech features, and demonstrated that with only a few frames, a very strong speaker factor can be inferred. Considering that the linguistic factor can be inferred from a short segment as well [6], our finding indicates that most of the significant variations of speech signals can be well explained. Based on the explanation, less significant factors are easier to be inferred. This has motivated a cascaded deep factorization (CDF) approach that factorizes speech signals in a sequential way: factors that are most significant are inferred firstly, and other less significant factors are inferred subsequently, conditioned on the factors that have been inferred. By this approach, speech signals can be factorized into independent informative factors, where all the inferences are based on deep neural models.\nIn this paper, we apply the CDF approach to factorize emotional speech signals to linguistic contents, speaker traits and emotion status. Our experiments on an AER task demonstrated that the CDFbased factorization is highly effective. Furthermore, we show that the original speech signal can be reconstructed from these three factors pretty well. This factorization and reconstruction has far-reaching implications and will provide a powerful tool for many speech processing tasks."}, {"heading": "2 Speaker factor learning", "text": "In this section, we present a DNN structure that can learn speaker traits at the frame level, as shown in Figure 6. This structure consists of a convolutional (CN) component and a time-delay (TD) component, connected by a bottleneck layer of 512 units. The convolutional component comprises two CN layers, each followed by a max-pooling layer. The TD component comprises two TD layers, each followed by a P-norm layer. The settings for the two components are shown in Figure 6. A simple calculation shows that with this configuration, the length of the effective context window is 20 frames. The output of the P-norm layer is projected into a feature layer that consists of 40 units. The activations of these units, after length normalization, form a speaker factor that represents the speaker trait involved in the input speech segment. For model training, the feature layer is fully connected to the output layer whose units correspond to the speakers in the training data. The training is performed to optimize the cross-entropy objective that aims to discriminate the training speakers based on the input frames. In our experiment, the natural stochastic gradient descent (NSGD) [7] algorithm was employed for optimization. Once the DNN model has been trained, the 40-dimensional frame-level speaker factor can be read from the feature layer. The speaker factors inferred by the DNN structure, as will be shown in the experiment, are highly speaker-discriminative. This demonstrates that speaker traits are short-time spectral patterns and can be identified at the frame level."}, {"heading": "3 Cascaded deep factorization", "text": "Due to the highly complex intermingling of multiple informative factors, it is nearly impossible to factorize speech signals by conventional linear factorization methods, e.g., JFA [4]. Fortunately, the ASR research has demonstrated that the linguistic factor can be individually inferred by a DNN structure, without knowing other factors. The previous section further provides deep model that\ncan infer the speaker factor. We denote this single factor inference based on deep neural models by individual deep factorization (IDF).\nThe rationality of the linguistic and speaker IDF is two-fold: firstly the linguistic and speaker factors are sufficiently significant in speech signals, and secondly a large amount of training data is available. It is the large-scale supervised learning that picks up the most task-relevant factors from raw speech features, via the DNN architecture. For factors that are less significant or without sufficient training data, IDF is simply not applicable. Fortunately, the successful inference of the linguistic and/or the speaker factors may significantly simplify the inference of other speech factors, as the largest variations within the speech signal have been explained away. This has motivated a cascaded deep factorization (CDF) approach: firstly we infer a particular factor by IDF, and then use this factor as a conditional variable to infer the second factor, and so on. Finally, the speech signal will be factorized into a set of independent factors, each corresponding to a particular task. The order of the inference can be arbitrary, but a good practice is that factors that are more significant and with more training data should be inferred earlier, so that the variation caused by these factors can be reliably eliminated when inferring the subsequent factors.\nIn this study, we apply the CDF approach to factorize emotional speech signals into three factors: linguistic, speaker and emotion. Figure 2 illustrates the architecture. Firstly an ASR system is trained using word-labelled speech data. The frame-level linguistic factor, which is in the form of phone posteriors in our study, is produced from the ASR DNN, and is concatenated with the raw feature to train an SRE system. This SRE system is used to produce the frame-level speaker factor, as discussed in the previous section. The linguistic factor and the speaker factor are finally concatenated with the raw feature to train an AER system, by which the emotion factor is read from the last hidden layer.\nThe CDF approach is fundamentally different from the conventional joint factorization approach, e.g., JFA [4]. Firstly, CDF heavily relies on discriminative learning to discover task-related factors, while conventional approaches are mostly generative models and the factors inferred are less task-related. Secondly, CDF infers factors sequentially and can use different data resources for different factors, while conventional approaches infer factors jointly using a single multi-labelled database. Thirdly,\nCDF being a deep approach, can leverage various advantages associated with deep learning (e.g., invariant feature learning), while most conventional approaches are mostly based on shallow models."}, {"heading": "4 Spectrum reconstruction", "text": "A key difference between CDF and the conventional factor analysis [8] is that in CDF each factor is inferred individually, without any explicit constraint defined among the factors (e.g., the linear Gaussian relation as in JFA). This on one hand is essential for a flexible factorization, but on the other hand, shuns an important question: How these factors are composed together to produce the speech signal?\nTo answer this question, we reconstruct the spectrum using the CDF-inferred factors. Define the linguistic factor q, the speaker factor s, and the emotion factor e. For each speech frame, we try to use these three factors to recover the spectrum x. Assuming they are convolved, the reconstruction is in the form:\nln(x) = ln{f(q)}+ ln{g(s)}+ ln{h(e)}+\nwhere f , g, h are the non-linear recovery function for q, s and e respectively, each implemented as a DNN. represents the residual which is assumed to be Gaussian. This reconstruction is illustrated in Figure 3, where all the spectra are in the log domain."}, {"heading": "5 Related work", "text": "The idea of learning speaker factors was motivated by Ehsan et al [9], who employed a vanilla DNN to learn frame-level representations of speakers. These representations, however, were rather weak and did not perform well on SRE tasks. Since then, various DNN structures were investigated, e.g., RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13]. These diverse investigations demonstrated reasonable performance, however most of them were based on an end-to-end training, seeking for better performance on speaker verification, rather than factor learning.\nThe CDF approach is also related to the phonetic DNN i-vector approach proposed by Lei [14] and Kenny [15], where the linguistic factor (phonetic posteriors) is firstly inferred using an ASR system, which is then used as an auxiliary knowledge to infer the speaker factor (the i-vector). In CDF, the\nsecond stage linear Gaussian inference (i-vector inference) is replaced by a more complex deep speaker factorization.\nFinally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18]. For example, Senior et al. [19] found that involving the speaker factor in the input feature improved ASR system. Qin [20] and Li et al. [21] found that ASR and SRE systems can be trained jointly, by borrowing information from each other. This idea was recently studied more systematically by Tang et al. [22]. All these approaches focus on linguistic and speaker factors that are mostly significant. The CDF, in contrast, treats these significant factors as conditional variables and focuses more on less significant factors."}, {"heading": "6 Experiment", "text": "In this section, we first present the data used in the experiments, then report the results of speaker factor learning. The CDF-based emotional speech factorization and reconstruction will be also presented."}, {"heading": "6.1 Database", "text": "ASR database: The WSJ database was used to train the ASR system. The training set is the official train_si284 dataset, composed of 282 speakers and 37, 318 utterances, with about 50-155 utterances per speaker. The test set contains three datasets (devl92, eval92 and eval93), including 27 speakers and 1, 049 utterances in total.\nSRE database: The Fisher database was used to train the SRE systems. The training set consists of 2, 500 male and 2, 500 female speakers, with 95, 167 utterances randomly selected from the Fisher database, and each speaker has about 120 seconds of speech signals. It was used for training the UBM, T-matrix and LDA/PLDA models of an i-vector baseline system, and the DNN model proposed in Section 2. The test set consists of 500 male and 500 female speakers randomly selected from the Fisher database. There is no overlap between the speakers of the training set and the evaluation set. For each speaker, 10 utterances (about 30 seconds in total) are used for enrollment and the rest for test. There are 72, 989 utterances for evaluation in total.\nAER database: The CHEAVD database [23] was used to train the AER systems. This database was selected from Chinese movies and TV programs and used as the standard database for the multimodal emotion recognition challenge (MEC 2016) [24]. There are 8 emotions in total: Happy, Angry, Surprise, Disgust, Neutral, Worried, Anxious and Sad. The training set contains 2, 224 utterances and the evaluation set contains 628 utterances.\nNote that WSJ and CHEAVD datasets are in 16kHz sampling rate, while the Fisher corpus is in 8kHz format. All the 16kHz speech signals were down-sampled to 8kHz to ensure data consistency."}, {"heading": "6.2 ASR baseline", "text": "We first build a DNN-based ASR system using the WSJ database. This system will be used to produce the linguistic factor in the following CDF experiments. The Kaldi toolkit [25] is used to train the DNN model, following the Kaldi WSJ s5 nnet recipe. The DNN structure consists of 4 hidden layers, each containing 1, 024 units. The input feature is Fbanks, and the output layer discriminates 3, 383 GMM pdfs. With the official 3-gram language model, the word error rate (WER) of this system is 9.16%. The linguistic factor is represented by 42-dimensional phone posteriors, derived from the output of the ASR DNN."}, {"heading": "6.3 Speaker factor learning", "text": "In this section, we experiment with DNN structure proposed in Section 2 to learn speaker factors. Two models are investigated: one follows the architecture shown in Figure 6, where only the raw features (Fbank) comprise the input; the other model uses both the raw features and the linguistic factors produced by the ASR system. Put it in another way, the first model is trained by IDF, while the second model is trained by CDF. The Fisher database is used to train the model. The 40-dimensional frame-level speaker factors are read out from the last hidden layer of the DNN structure.\nVisualization\nThe discriminative capability of the speaker factor can also be examined by projecting the feature vectors to a 2-dimensional space using t-SNE [26]. We select 20 speakers and draw the frame-level speaker factors of an utterance for each speaker. The results are presented in Figure 4 where plot (a) draws the factors generated by the IDF model, and (b) draws the factors generated by the CDF model. It can be seen that the learned speaker factors are very discriminative, and involving the linguistic factor by CDF indeed reduces the within-speaker variation.\nSRE performance\nThe quality of the speaker factors can be also evaluated by various speaker recognition tasks, i.e., speaker identification task or speaker verification task. In both tasks, the utterance-level speaker vector is derived by averaging the frame-level speaker factors. Following the convention of Ehsan et al [9], the utterance-level representations derived from DNN are called d-vectors, and accordingly the SRE system is called a d-vector system.\nFor comparison, an i-vector baseline is also constructed using the same database. The model is a typical linear Gaussian factorization model and it has been demonstrated to produce state-ofthe-art performance in SRE [5]. In our implementation, the UBM is composed of 2, 048 Gaussian components, and the dimensionality of the i-vector space is set to 400. The system is trained following the Kaldi SRE08 recipe.\nWe report the results on the identification task, though similar observations were obtained on the verification task. In the identification task, a matched speaker is identified given a test utterance. With the i-vector (d-vector) system, each enrolled speaker is represented by the i-vector (d-vector) of their enrolled speech, and the i-vector (d-vector) of the test speech is derived as well. The identification is then conducted by finding the speaker whose enrolled i-vector (d-vector) is nearest to that of the test speech. For the i-vector system, the popular PLDA model [27] is used to measure the similarity between i-vectors; for the d-vector system, the simple cosine distance is used.\nThe results in terms of the Top-1 identification rate (IDR) are shown in Table 1. In this table, \u2018C(3020f)\u2019 means the test condition where the enrollment speech is 30 seconds, while the test speech is 20 frames. Note that 20 frames is just the length of the effective context window of the speaker DNN, so only a single speaker factor is used in this condition. From these results, it can be observed that the d-vector system performs much better than the i-vector baseline, particularly with very short speech segments. Comparing the IDF and CDF results, it can be seen that the CDF approach that involves phone knowledge as the conditional variable greatly improves the d-vector system in the short speech segment condition. Most strikingly, with only 20 frames of speech (0.3 seconds), 47.63% speakers can be correctly identified from the 1, 000 candidates by the simple nearest neighbour search. This is a strong evidence that speaker traits are short-time spectral patterns and can be effectively learned at the frame level."}, {"heading": "6.4 Emotion recognition by CDF", "text": "In the previous experiment we have partially demonstrated the CDF approach with the speaker factor learning task. This section provides further evidence with an emotion recognition task. For that purpose, we first build a DNN-based AER baseline. The DNN model consists of 6 time-delay hidden layers, each containing 200 units. After each TD layer, a P-norm layer reduces the dimensionality from 200 to 40. The output layer comprises 8 units, corresponding to the 8 emotions in the database. This DNN model produces frame-level emotion posteriors. The utterance-level posteriors are obtained by averaging the frame-level posteriors, by which the utterance-level emotion decision is achieved.\nThree CDF configurations are investigated, according to which factor is used as the conditional: the linguistic factor (+ ling.), the speaker factor (+ spk.) and both (+ ling. & spk.). The results are evaluated in two metrics: the identification accuracy (ACC) that is the ratio of the correct identification on all emotion categories; the macro average precision (MAP) that is the average of the ACC on each of the emotion category.\nThe results on the training data are shown in Table 2, where the ACC and MAP values on both the frame-level (fr.) and the utterance-level (utt.) are reported. It can be seen that with the conditional factors involved, either the linguistic factor or the speaker factor, the ACC and MAP values are improved very significantly. The speaker factor seems provide more significant contribution, which can be attributed to the fact that the emotion style of different speakers could be largely different. With both the two factors involved, the AER performance is improved even further. This clearly demonstrates that with the conditional factors considered, the speech signal can be explained much better.\nThe results on the test data are shown in Table 3. Again, we observe a clear advantage with the CDF training. Note that involving the two factors does not improve the utterance-level results. This should be attributed to the fact that the DNN models are trained using frame-level data, so may be not fully consistent with the metric of the utterance-level test. Nevertheless, the superiority of the multiple conditional factors can be seen clearly from the frame-level metrics."}, {"heading": "6.5 Spectrum reconstruction", "text": "In the last experiment, we use the linguistic factor, speaker factor and emotion factor to reconstruct the original speech signal. The reconstruction model has been discussed in Section 4 and shown in Figure 3. This model is trained using the CHEAVD database. Figure 5 shows the reconstruction of a test utterance in the CHEAVD database. It can be seen that these three factors can reconstruct the spectrum patterns extremely well. This re-confirms that the speech signal has been well factorized, and the convolutional reconstruction formula is mostly correct. Finally, the three component spectra (linguistic, speaker, and emotion) are highly interesting and all deserve extensive investigation. For example, the speaker spectrum may be a new voiceprint analysis tool and could be very useful for forensic applications."}, {"heading": "7 Conclusions", "text": "This paper has presented a DNN model to learn short-time speaker traits and a cascaded deep factorization (CDF) approach to factorize speech signals into independent informative factors. Two interesting things were found: firstly speaker traits are indeed short-time spectral patterns and can be identified by deep learning from a very short speech segment; secondly speech signals can be well factorized at the frame level by the CDF approach. We also found that the speech spectrum can be largely reconstructed using deep neural models from the factors that have been inferred by CDF, confirming the correctness of the factorization. The successful factorization and reconstruction of speech signals has very important implications and can find broad applications. To mention several: it can be used to design very parsimonious speech codes, to change the speaker traits or emotion in speech synthesis or voice conversion, to remove background noise, to embed audio watermarks. All are highly interesting and are under investigation."}, {"heading": "8 Appendix A: Model details", "text": ""}, {"heading": "8.1 ASR system", "text": "The ASR system was built following the Kaldi WSJ s5 nnet recipe. The input feature was 40- dimensional Fbanks, with a symmetric 5-frame window to splice neighboring frames. It contained 4 hidden layers, and each layer had 1, 024 units. The output layer consisted of 3, 383 units, equal to the total number of pdfs of the GMM system trained following the WSJ s5 gmm recipe. The language model was the WSJ official 3-gram model (\u2018tgpr\u2019) that consists of 19, 982 words."}, {"heading": "8.2 SRE system", "text": "The i-vector SRE baseline used 19-dimensional MFCCs plus the log energy as the primary feature. This primary feature was augmented by its first and second order derivatives, resulting in a 60- dimensional feature vector. The UBM was composed of 2, 048 Gaussian components, and the dimensionality of the i-vector space was 400. The entire system was trained using the Kaldi SRE08 recipe.\nFor the IDF d-vector system, the architecture was based on Figure 6. The input feature was 40- dimensional Fbanks, with a symmetric 4-frame window to splice the neighboring frames, resulting in 9 frames in total. The number of output units was 5, 000, corresponding to the number of speakers in the training set.\nFor the CDF d-vector system, the linguistic factor in the form of 42-dimensional phone posteriors was augmented to the bottleneck layer, as shown in Figure 7."}, {"heading": "8.3 AER system", "text": "The input feature of the DNN model of the AER baseline was 40-dimensional Fbanks, with a symmetric 4-frame window to splice neighboring frames. The time-delay component involving two time-delay layers was used to extend the temporal context, and the length of the effective context window was 20 frames. It contained 6 hidden layers, and each layer had 200 units. With the P-norm activation, the dimensionality of the output of the previous layer was reduced to 40.\nThe definitions of ACC and MAP are given in Eqs. 1 - 3.\nPi = TPi\nTPi + FPi , (1)\nMAP = 1\ns \u00d7 \u2211s i=1 Pi, (2)\nACC = \u2211s i=1 TPi\u2211s\ni=1 (TPi + FPi) , (3)\nwhere s denotes the number of emotion categories. Pi is the precision of the ith emotion class. TPi and FPi denote the number of correct classification and the number of error classification in the ith emotion class, respectively."}, {"heading": "8.4 Spectrum reconstruction", "text": "The spectrum reconstruction is based on the following convolutional assumption:\nln(x) = ln{f(q)}+ ln{g(s)}+ ln{h(e)}+\nwhere f , g, h are the non-linear recovery function for q, s and e respectively, each implemented as a DNN. represents the residual which is assumed to be Gaussian.\nThe DNN structure for the spectrum reconstruction consists of two parts: A factor spectrum generation component and a spectrum convolution component. The former generates component spectrum for each factor (e.g., f(q), g(s), h(e)), and the latter composes the three component spectra together.\nThe dimensionalities of the linguistic, speaker and emotion factors are 42, 40 and 40, respectively. With a symmetric 4-frame window, the input dimensionalities of three spectrum-generation components are 387, 360 and 360, respectively. Each spectrum-generation component involves 5 hidden layers, each consisting of 1, 024 units and followed by the ReLu (Rectified Linear Unit) non-linear activation function. The outputs of these three spectrum-generation components are fed into the spectrum convolutional component, where the reconstruction of the original spectrum is produced.\nThe MSE (Mean Squared Error) between the recovered spectrum and the original spectrum is used as the training criterion. Note that the target spectrum is in the log domain, so the convolution component is a simple addition."}, {"heading": "9 Appendix B: Samples of spectrum reconstruction", "text": "Here we give more examples to demonstrate the spectrum reconstruction."}, {"heading": "9.1 Training set", "text": ""}, {"heading": "9.2 Evaluation set", "text": ""}], "references": [{"title": "Survey on speech emotion recognition: Features, classification schemes, and databases", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker verification using adapted gaussian mixture models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital signal processing, vol. 10, no. 1-3, pp. 19\u201341, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Joint factor analysis versus eigenchannels in speaker recognition", "author": ["P. Kenny", "G. Boulianne", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 4, pp. 1435\u20131447, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous latent variables", "author": ["C.M. Bishop"], "venue": "Pattern recognition and machine learning, 2006, ch. 12, pp. 583\u2013586.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["E. Variani", "X. Lei", "E. McDermott", "I.L. Moreno", "J. Gonzalez-Dominguez"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 4052\u20134056.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 171\u2013178.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "Spoken Language Technology Workshop (SLT), 2016 IEEE. IEEE, 2016, pp. 165\u2013170.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speaker: an end-to-end neural speaker embedding system", "author": ["C. Li", "X. Ma", "B. Jiang", "X. Li", "X. Zhang", "X. Liu", "Y. Cao", "A. Kannan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1705.02304, 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for extracting baum-welch statistics for speaker recognition", "author": ["P. Kenny", "V. Gupta", "T. Stafylakis", "P. Ouellet", "J. Alam"], "venue": "Proc. Odyssey, 2014, pp. 293\u2013298.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Learning to learn. Springer, 1998, pp. 95\u2013133.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2015 Asia- Pacific. IEEE, 2015, pp. 1225\u20131237. 9", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving DNN speaker independence with i-vector inputs", "author": ["A. Senior", "I. Lopez-Moreno"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 225\u2013229.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural network based multi-factor aware joint training for robust speech recognition", "author": ["Y. Qian", "T. Tan", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2231\u20132240, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling speaker variability using long short-term memory networks for speech recognition", "author": ["X. Li", "X. Wu"], "venue": "INTERSPEECH, 2015, pp. 1086\u20131090.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative joint training with multitask recurrent model for speech and speaker recognition", "author": ["Z. Tang", "L. Li", "D. Wang", "R. Vipperla"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 3, pp. 493\u2013504, 2017.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Building a chinese natural emotional audio-visual database", "author": ["W. Bao", "Y. Li", "M. Gu", "M. Yang", "H. Li", "L. Chao", "J. Tao"], "venue": "Signal Processing (ICSP), 2014 12th International Conference on. IEEE, 2014, pp. 583\u2013587.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Mec 2016: the multimodal emotion recognition challenge of ccpr 2016", "author": ["Y. Li", "J. Tao", "B. Schuller", "S. Shan", "D. Jiang", "J. Jia"], "venue": "Chinese Conference on Pattern Recognition. Springer, 2016, pp. 667\u2013678.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ", automatic emotion recognition (AER) [2].", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "In fact, most of the famous SRE techniques are based on factorization models, including the Gaussian mixture model-universal background model (GMM-UBM) [3], the joint factor analysis 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "(JFA) [4] and the i-vector model [5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "(JFA) [4] and the i-vector model [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Considering that the linguistic factor can be inferred from a short segment as well [6], our finding indicates that most of the significant variations of speech signals can be well explained.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": "In our experiment, the natural stochastic gradient descent (NSGD) [7] algorithm was employed for optimization.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": ", JFA [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": ", JFA [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "4 Spectrum reconstruction A key difference between CDF and the conventional factor analysis [8] is that in CDF each factor is inferred individually, without any explicit constraint defined among the factors (e.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "5 Related work The idea of learning speaker factors was motivated by Ehsan et al [9], who employed a vanilla DNN to learn frame-level representations of speakers.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": ", RNN by Heigold [10], CNN by Zhang [11] and NIN by Snyder [12] and Li [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "The CDF approach is also related to the phonetic DNN i-vector approach proposed by Lei [14] and Kenny [15], where the linguistic factor (phonetic posteriors) is firstly inferred using an ASR system, which is then used as an auxiliary knowledge to infer the speaker factor (the i-vector).", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "The CDF approach is also related to the phonetic DNN i-vector approach proposed by Lei [14] and Kenny [15], where the linguistic factor (phonetic posteriors) is firstly inferred using an ASR system, which is then used as an auxiliary knowledge to infer the speaker factor (the i-vector).", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 87, "endOffset": 95}, {"referenceID": 16, "context": "Finally, the CDF approach is related to multi-task learning [16] and transfer learning [17, 18].", "startOffset": 87, "endOffset": 95}, {"referenceID": 17, "context": "[19] found that involving the speaker factor in the input feature improved ASR system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Qin [20] and Li et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "[21] found that ASR and SRE systems can be trained jointly, by borrowing information from each other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "AER database: The CHEAVD database [23] was used to train the AER systems.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "This database was selected from Chinese movies and TV programs and used as the standard database for the multimodal emotion recognition challenge (MEC 2016) [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 23, "context": "The Kaldi toolkit [25] is used to train the DNN model, following the Kaldi WSJ s5 nnet recipe.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "Visualization The discriminative capability of the speaker factor can also be examined by projecting the feature vectors to a 2-dimensional space using t-SNE [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "Following the convention of Ehsan et al [9], the utterance-level representations derived from DNN are called d-vectors, and accordingly the SRE system is called a d-vector system.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "The model is a typical linear Gaussian factorization model and it has been demonstrated to produce state-ofthe-art performance in SRE [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "[2] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] X.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] L.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Speech signals are complex intermingling of various informative factors, and this information blending makes decoding any of the individual factors extremely difficult. A natural idea is to factorize each speech frame into independent factors, though it turns out to be even more difficult than decoding each individual factor. A major encumbrance is that the speaker trait, a major factor in speech signals, has been suspected to be a long-term distributional pattern and so not identifiable at the frame level. In this paper, we demonstrated that the speaker factor is also a short-time spectral pattern and can be largely identified with just a few frames using a simple deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that infers speech factors in a sequential way, and factors previously inferred are used as conditional variables when inferring other factors. Our experiment on an automatic emotion recognition (AER) task demonstrated that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with high accuracy. This factorization and reconstruction approach provides a novel tool for many speech processing tasks.", "creator": "LaTeX with hyperref package"}}}