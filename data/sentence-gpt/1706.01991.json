{"id": "1706.01991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2017", "title": "Unsupervised Neural-Symbolic Integration", "abstract": "Symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data. The integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge. Although previous works focus intensively on supervised feedforward neural networks, little has been done for the unsupervised counterparts of supervised neural networks.\n\n\n\n\nIn this article, we present a group of experiments that showed the integration of neural networks and supervised models on a population of 8.7 million people using two different datasets, in which the participants displayed the most predictive predictions. The results showed that the ability to predict a simple rule is a critical component of learning, learning and reasoning that is essential for understanding human behavior.\nThe researchers used their data to test a model of the predicted predictors for a given population. They then used these findings to compare the predictions by the individuals using supervised neural networks to see if they were correct for predictions over time.\nTo test whether neural networks could perform more effectively in learning, the researchers tested the models using a variety of different approaches, such as an algorithm for estimating the expected number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number of times a random number", "histories": [["v1", "Tue, 6 Jun 2017 21:58:50 GMT  (56kb,D)", "https://arxiv.org/abs/1706.01991v1", null], ["v2", "Thu, 22 Jun 2017 04:11:21 GMT  (56kb,D)", "http://arxiv.org/abs/1706.01991v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["son n tran"], "accepted": false, "id": "1706.01991"}, "pdf": {"name": "1706.01991.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Neural-Symbolic Integration", "authors": ["Son N. Tran"], "emails": ["son.tran@csiro.au"], "sections": [{"heading": "1 Introduction", "text": "An interesting topic in AI is integration of symbolic and neural networks, two different information processing paradigms. While the former is the key of higher level of intelligence the latter is well known for a capability of effective learning from data. In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Franc\u0327a et al., 2014; Tran and Garcez, 2016].\nIn previous work, supervised neural networks have been used intensively for the integration based on the analogy of modus ponens inference with symbolic rules and forward passing in neural networks [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999]. In such networks, due to the discriminative structures only a subset of variables can be inferred, i.e. the variables in the left hand of if-then \u2190 formulas. This may limit their use in general reasoning. Unsupervised network, on the other hand, offers more flexible inference mechanism which seems more suitable for symbolic reasoning. Let us consider an XOR example z \u2194 (x \u2295 y). Here, given the truth values of any two variables one can infer the rest. For supervised networks, a class variable must be discriminated from the others and only it can be inferred. An\nunsupervised network, in contrast, do not require such discrimination.\nEncoding symbolic knowledge in an unsupervised neural network needs a mechanism to convert symbolic formulas to the network without loss of generality. In previous work, Penalty logic shows that any propositional formula can be represented in a symmetric connectionist network (SCN) where inference with rules is equivalent to minimising the network\u2019s energy. However, SCN uses dense connections of hidden and visible units which make the inference very computational. Recent work shows that any propositional formula can be represented in restricted Boltzmann machines [Tran, 2017]. Different from Penalty logic, here the RBM is a simplified version of SCN where there is no visible-visible and hidden-hidden connections. This makes inference in RBMs is easier.\nSeveral attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016]. Despite achieving good practical results they are still heuristic. In this paper, we show how to encode symbolic knowledge in both propositional and firstorder forms into the RBM by extending the theory in [Tran, 2017].\nThe remainder of this paper is organized as follows. Section 2 review the idea of Confidence rule, a knowledge form to represent symbolic formulas in RBMs. In section 3 we show how to encode knowledge into RBMs. Section 4 presents the empirical verification of our encoding approach and Section 5 concludes the work."}, {"heading": "2 Confidence Rules: Revisit", "text": "A confidence rule [Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form:\nc : h\u2194 \u2227 t xt \u2227 \u2227 k \u00acxk (1)\nwhere h is called hypothesis, c is a non-negative real value called confidence value. Inference with a confidence rule is to find the model that makes the hypothesis h holds. If there exist a target variable y the inference of such variable will be similar to modus ponens, as shown in Table 1\nAn interesting feature of Confidence rules is that one can represent them in an RBM where Gibbs sampling can be seen\nar X\niv :1\n70 6.\n01 99\n1v 2\n[ cs\n.A I]\n2 2\nJu n\n20 17\nequivalently as maximising the total (weighted) satisfiability [Tran, 2017]. If a knowledge base is converted into Confidence rules then we can take the advantage of the computation mechanism in such neural networks for efficient inference. The equivalence between confidence rules and an RBM is defined in that the satisfiability of a formula is inversely proportional to the energy of a network:\ns\u03d5(x) = \u2212aE rank(x) + b where s\u03d5 is the truth value of the formula \u03d5 given an assignment x; Erank(x) = minhE (x,h) is the energy function minimised over all hidden variables; a > 0,b are scalars.\nBy using disjunctive normal form (DNF) to present knowledge Confidence rules attract some criticism for practicality since it is more popular to convert a formula to a conjunctive normal form of polynomial size. However we will show that Confidence rules are still very useful in practice. In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016]. For knowledge integration previous work separates the if-and-only-if symbol in Confidence rules into two if-then rules to encode in a hierarchical networks [Tran and Garcez, 2016]. In this work, we show that such separation is not necessary since any propositional if-then formulas can be efficiently converted to Confidence rules. The details are in the next section."}, {"heading": "3 Knowledge Encoding", "text": "In many cases background knowledge presents a set of if-then formulas (or equivalent Horn clauses). This section shows how to convert them into Confidence rules for both propositional and first-order logic forms"}, {"heading": "3.1 Proposition Logic", "text": "A propositional if-then formula has the form\nc : y\u2190 \u2227 t xt \u2227 \u2227 k \u00acxk\nwhich can be transformed to a DNF as: c : (y \u2227 \u2227 t xt \u2227 \u2227 k \u00acxk) \u2228 \u2228 t (\u00acxt) \u2228 \u2228 k (xk)\nand then to the confidence rules: c : hy \u2194 y \u2227 \u2227 t xt \u2227 \u2227 k \u00acxk\nc : ht \u2194 \u00acxt for \u2200t c : hk \u2194 xk for \u2200k\nEncoding these rules into an RBM does not guarantee the equivalence. This is because it violates the condition that the DNF of a formula should have at most one conjunct is true given an assignment [Tran, 2017]. Fortunately this can be solved by grouping \u00acxt, xk with a max-pooling hidden unit which results in an RBM with the energy function as12:\nE =\u2212 c\u00d7 hy(y + \u2211 t xt \u2212 \u2211 k xk \u2212 |T | \u2212 1 + )\n\u2212 c\u00d7 hp max({\u2212xt + , xk \u2212 1 + |t \u2208 T, k \u2208 K}) (2) Here a max pooling hidden unit represents a hypothesis: hp \u2194 \u2228 t ht \u2228 \u2228 k hk which, in this case, can be written as:\nc : hp \u2194 \u2228 t \u00acxt \u2228 \u2228 k xk. The final set rules are:\nc : hy \u2194 y \u2227 \u2227 t xt \u2227 \u2227 k \u00acxk\nc : hp \u2194 \u2228 t \u00acxt \u2228 \u2228 k xk\nExample 1. Let us consider the formula: 5 : y \u2190 x1 \u2227 \u00acx2 which would be converted to DNF as: 5 : (y \u2227 x1 \u2227 \u00acx2) \u2228 (\u00acx1) \u2228 (x2), and then to an RBM with the energy function: E = \u22125h1(y + x1 \u2212 x2 \u2212 1.5)\u2212 5h2 max(\u2212x1 + 0.5, x2 \u2212 0.5). Table 2 shows the equivalence between the RBM and the formula.\nwhere s\u03d5 is (unweighted) truth values of the formula. This indicates the equivalence between the RBM and the formula as:\ns\u03d5(x1, x2, y) = \u2212 1\n2.5 Erank(x, y, z) + 0"}, {"heading": "3.2 First-order Logic", "text": "A first order logic formula can also be converted into a set of Confidence rules. First, let us consider a predicate: P (x, y) which one can present in a propositional DNF as:\u2228\na,b|P (a,b)=true\npx=a \u2227 py=b \u2227 pP\nwhere (a, b) are the models of P (x, y); px=a and py=b are the propositions that are true if x = a and y = b respectively, otherwise they are false; pP is the proposition indicating if\n1The proof is similar as in [Tran, 2017] 20 < < 1\nthe value of P (a, b). Each conjunct in this DNF then can be represented as a Confidence rule.\nNow, let us consider a first-order formula which we are also able to present in a set of Confidence rules. For example, a clause \u03d5 as:\n\u2200x,y,zson(x, z)\u2190 brother(x, y) \u2227 has father(y, z) can be converted into:\u2228\na,b,c|\u03d5=true\n(px=a \u2227 py=b \u2227 pz=c \u2227 pson \u2227 pbrother \u2227 phas father)\n\u2228 (\u00acpx=a \u2228 \u00acpy=b \u2228 \u00acpz=c \u2228 \u00acpbrother \u2228 \u00acphas father) If one want to encode the background knowledge through\nits samples, for example: son(James,Andrew)\u2190 brother(James, Jen)\n\u2227 has father(Jen,Andrew) then we can convert it into confidence rules: c : h1 \u2194 james \u2227 jen \u2227 andrew \u2227 son \u2227 brother \u2227 has father c : hp \u2194 \u00acjames \u2228 \u00acjen \u2228 \u00acandrew \u2228 \u00acbrother \u2228 \u00achas father In practice, in many cases we are only interested in inferring the predicates therefore we can omit \u00acjames, \u00acjen, \u00acandrew from the second rule."}, {"heading": "4 Empirical Evaluation", "text": "In this section we apply the encoding approaches discussed in the previous section to integrate knowledge into unsupervised networks."}, {"heading": "4.1 DNA promoter", "text": "The DNA promoter dataset consist of a background theory with 14 logical if-then rules [Towell and Shavlik, 1994]. The rules includes four symbols contact, minus10, minus35, conformation which are not observed in the data. This is suitable for hierarchical models as shown in previous works [Towell and Shavlik, 1994; Tran and Garcez, 2016]. In this experiment we group the rules using hypothetical syllogism to eliminate the unseen symbols. After that we encode the rules in an RBM following the theory in Section 3.1. The confidence values are selected empirically.\nWe test the normal RBMs and the RBMs with encoded rule using leave-one-out method, both achieve 100% accuracy. In order to evaluate the effectiveness of our approach we partition the data into nine different training-test sets with number of training samples are 10, 20, 30, 40, 50, 60, 70, 80, 90. All experiments are repeated 50 times and the average results are reported in Figure 1. We perform the prediction using both Gibbs sampling and conditional distribution P (y|x). In particular, Figure 1a shows the prediction results using 1-step Gibbs sampling where the input is fixed to infer the hidden states and then to infer the label unit. In Figure 1b the results show the prediction accuracy achieved by inferring the label unit from the conditional distribution. As we can see, in both cases the integrated RBMs perform better than the normal RBMs on small training sets with number of training sample is less than 60. With larger training sets, the rules are no longer effective since the training samples are adequate to generalise the model to achieve 100% accuracy."}, {"heading": "4.2 Kinship", "text": "In this experiment, we use the approach discussed in Section 3.2 for relation discovery and reasoning tasks with Kinship dataset [Hinton, 1986; Sutskever and Hinton, 2008]. Here given a set of examples about relations we perform two type of reasoning: (1) what is relation between two people, i.e ?(x, y); and (2) a person has a relation R with whom, i.e. R(x, ?). Previous approaches are using matrices/tensors to represents the relations making it difficult to explain [Sutskever and Hinton, 2008]. In this work, since only predicates are given, we encode the examples for the predicates in an unsupervised network as shown earlier in Section 3.2. This constructs the left part of the integrated model in Figure 2. In the right part, we model the unknown clauses by using a set of hidden units. The idea here is that by inferring the predicates using the encoded rules in the left part we can capture the relationship information, from which the desired relation is inferred by reconstruction of such relationship in the right part. In this experiment, we use auto-encoder [Bengio, 2009] for the right part for the purpose of efficient learning. The whole process is described in Algorithm 1.\nLet us take an example where one wants to find the relation between two people R(Marco, P ierro) =?. First, we use the other examples to construct an integrated model as in Figure 2. After that, we train the auto-encoder in the right part using unsupervised learning algorithm, then we extract the relation features from Marco and Pierro as shown in Ta-\nble 3. In that table we also show the reconstructed scores for all the relations where son is the correct one.\nAlgorithm 1 Data: Examples: E, Question: R(a,b) Result: R\nEncode all examples in an RBM: N Initialise D = \u2205 for each example R(x, y) in E do f = INFER(N,x,y)\nAdd f to D\nend Train an Auto-Encoder (AE) on D f = INFER(N,a,b) Reconstruct f\u0302 using AE Return R = argmaxR(f\u0302R) . Return the unseen relation where the reconstruction feature have the highest value.\n1: function INFER(N, a, b) 2: Infer direct relation between a,b 3: Infer possible relations of a: R(a,*) 4: Infer possible relations of b: R(*,b) 5: f = concatenation of all relations 6: Return f 7: end function\nWe test the model on answering the question R(x, y) =? using leave-one-out validation which achieve 100% accuracy. We also use the integrated model to reason about whom one has a relation with. This question may have more than one answer, for example son(Athur, ?) can be either Cristopher or Penelope. We randomly select 10 examples for testing and repeat it for 5 times. If the designate answers are in the top relations with highest reconstructed features then we consider this as correct, otherwise we set it as wrong. The average error of this test is 0%. However, when we increase the number of test samples to 20 and 30 the average errors grow to 2.8% and 6.8% respectively. For comparison, the matrices based approach such as [Sutskever and Hinton, 2008] achieves 0.4%,1.2%,2.0% average error rates for 10, 20, 30 test examples respectively. Note that, such approach and many others [Socher et al., 2013] model each relation by a matrix/tensor while in this experiment we share the parameters across all relations. Also, the others use discriminative learning while we use unsupervised learning. The purpose of this is to exemplify the encoding technique we proposed earlier in this paper. Improvement can be achieved if similar methods are employed."}, {"heading": "5 Conclusions", "text": "The paper shows how to integrate symbolic knowledge into unsupervised neural networks. This work bases on the theoretical finding that any propositional formula can be represented in RBMs [Tran, 2017]. We show that converting background knowledge in the form of if-then rules to Confidence\nrules for encoding is efficient. In the experiments, we evaluate our approaches for DNA promoter prediction and relationship reasoning to show the validity of the approach."}], "references": [{"title": "11(1):5977", "author": ["Artur S. Avila Garcez", "Gerson Zaverucha. The connectionist inductive learning", "logic programming system. Applied Intelligence"], "venue": "July", "citeRegEx": "Avila Garcez and Zaverucha. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "2(1):1\u2013127", "author": ["Yoshua Bengio. Learning deep architectures for ai. Found. Trends Mach. Learn."], "venue": "January", "citeRegEx": "Bengio. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "d\u2019AvilaGarcez", "author": ["Manoel V.M. Fran\u00e7a", "Gerson Zaverucha", "Artur S"], "venue": "Fast relational learning using bottom clause propositionalization with artificial neural networks. Machine Learning, 94(1):81\u2013104,", "citeRegEx": "Fran\u00e7a et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "d\u2019Avila Garcez", "author": ["S Artur"], "venue": "Lus C. Lamb, and Dov M. Gabbay. Neural-Symbolic Cognitive Reasoning. Springer Publishing Company, Incorporated,", "citeRegEx": "Garcez et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "pages 1\u201312", "author": ["Geoffrey E. Hinton. Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society"], "venue": "Hillsdale, NJ: Erlbaum,", "citeRegEx": "Hinton. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "d\u2019Avila Garcez", "author": ["Leo de Penning", "Artur S"], "venue": "Lus C. Lamb, and John-Jules Ch Meyer. A neuralsymbolic cognitive agent for online learning and reasoning. In IJCAI, pages 1653\u20131658,", "citeRegEx": "Penning et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Constituent structure and explanation in an integrated connectionist/symbolic cognitive architecture", "author": ["Paul Smolensky"], "venue": "C. Mcdonald, editor, Connectionism: Debates on Psychological Explanation, pages 221\u2013290. Blackwell, Cambridge,", "citeRegEx": "Smolensky. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher et al", "2013] Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 26th International Conference on Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "editors", "author": ["Ilya Sutskever", "Geoffrey E Hinton. Using matrices to model symbolic relationship. In D. Koller", "D. Schuurmans", "Y. Bengio", "L. Bottou"], "venue": "Advances in Neural Information Processing Systems 21, pages 1593\u20131600. Curran Associates, Inc.,", "citeRegEx": "Sutskever and Hinton. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Artificial Intelligence", "author": ["Geoffrey G. Towell", "Jude W. Shavlik. Knowledge-based artificial neural networks"], "venue": "70(1-2):119\u2013165,", "citeRegEx": "Towell and Shavlik. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Tran and Artur d\u2019Avila Garcez", "author": ["N Son"], "venue": "Knowledge extraction from deep belief networks for images. In IJCAI-2013 Workshop on NeuralSymbolic Learning and Reasoning,", "citeRegEx": "Tran and d.Avila Garcez. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep logic networks: Inserting and extracting knowledge from deep belief networks", "author": ["Son Tran", "Artur Garcez"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, PP(99):1\u201313,", "citeRegEx": "Tran and Garcez. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Propositional knowledge representation in restricted boltzmann machines", "author": ["Son N. Tran"], "venue": "https://arxiv.org/abs/1705.10899,", "citeRegEx": "Tran. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Knowledge infusion", "author": ["G. Valiant"], "venue": "[Valiant,", "citeRegEx": "Valiant.,? \\Q2006\\E", "shortCiteRegEx": "Valiant.", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 6, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 0, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 13, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 3, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 5, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 2, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 11, "context": "In the last two decades, researchers have been working on the idea that combination of the two should offer joint benefits [Towell and Shavlik, 1994; Smolensky, 1995; Avila Garcez and Zaverucha, 1999; Valiant, 2006; Garcez et al., 2008; Penning et al., 2011; Fran\u00e7a et al., 2014; Tran and Garcez, 2016].", "startOffset": 123, "endOffset": 302}, {"referenceID": 9, "context": "In previous work, supervised neural networks have been used intensively for the integration based on the analogy of modus ponens inference with symbolic rules and forward passing in neural networks [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999].", "startOffset": 198, "endOffset": 258}, {"referenceID": 0, "context": "In previous work, supervised neural networks have been used intensively for the integration based on the analogy of modus ponens inference with symbolic rules and forward passing in neural networks [Towell and Shavlik, 1994; Avila Garcez and Zaverucha, 1999].", "startOffset": 198, "endOffset": 258}, {"referenceID": 12, "context": "Recent work shows that any propositional formula can be represented in restricted Boltzmann machines [Tran, 2017].", "startOffset": 101, "endOffset": 113}, {"referenceID": 5, "context": "Several attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016].", "startOffset": 87, "endOffset": 132}, {"referenceID": 11, "context": "Several attempts have been made recently to integrate symbolic representation and RBMs [Penning et al., 2011; Tran and Garcez, 2016].", "startOffset": 87, "endOffset": 132}, {"referenceID": 12, "context": "In this paper, we show how to encode symbolic knowledge in both propositional and firstorder forms into the RBM by extending the theory in [Tran, 2017].", "startOffset": 139, "endOffset": 151}, {"referenceID": 10, "context": "A confidence rule [Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form:", "startOffset": 18, "endOffset": 72}, {"referenceID": 11, "context": "A confidence rule [Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016] is a propositional formula in the form:", "startOffset": 18, "endOffset": 72}, {"referenceID": 12, "context": "equivalently as maximising the total (weighted) satisfiability [Tran, 2017].", "startOffset": 63, "endOffset": 75}, {"referenceID": 5, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 10, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 11, "context": "In fact, in such tasks as knowledge extraction, transfer, and integration Confidence rules have been already employed [Penning et al., 2011; Tran and d\u2019Avila Garcez, 2013; Tran and Garcez, 2016].", "startOffset": 118, "endOffset": 194}, {"referenceID": 11, "context": "For knowledge integration previous work separates the if-and-only-if symbol in Confidence rules into two if-then rules to encode in a hierarchical networks [Tran and Garcez, 2016].", "startOffset": 156, "endOffset": 179}, {"referenceID": 12, "context": "This is because it violates the condition that the DNF of a formula should have at most one conjunct is true given an assignment [Tran, 2017].", "startOffset": 129, "endOffset": 141}, {"referenceID": 12, "context": "The proof is similar as in [Tran, 2017] 0 < < 1", "startOffset": 27, "endOffset": 39}, {"referenceID": 9, "context": "1 DNA promoter The DNA promoter dataset consist of a background theory with 14 logical if-then rules [Towell and Shavlik, 1994].", "startOffset": 101, "endOffset": 127}, {"referenceID": 9, "context": "This is suitable for hierarchical models as shown in previous works [Towell and Shavlik, 1994; Tran and Garcez, 2016].", "startOffset": 68, "endOffset": 117}, {"referenceID": 11, "context": "This is suitable for hierarchical models as shown in previous works [Towell and Shavlik, 1994; Tran and Garcez, 2016].", "startOffset": 68, "endOffset": 117}, {"referenceID": 4, "context": "2 for relation discovery and reasoning tasks with Kinship dataset [Hinton, 1986; Sutskever and Hinton, 2008].", "startOffset": 66, "endOffset": 108}, {"referenceID": 8, "context": "2 for relation discovery and reasoning tasks with Kinship dataset [Hinton, 1986; Sutskever and Hinton, 2008].", "startOffset": 66, "endOffset": 108}, {"referenceID": 8, "context": "Previous approaches are using matrices/tensors to represents the relations making it difficult to explain [Sutskever and Hinton, 2008].", "startOffset": 106, "endOffset": 134}, {"referenceID": 1, "context": "In this experiment, we use auto-encoder [Bengio, 2009] for the right part for the purpose of efficient learning.", "startOffset": 40, "endOffset": 54}, {"referenceID": 8, "context": "For comparison, the matrices based approach such as [Sutskever and Hinton, 2008] achieves 0.", "startOffset": 52, "endOffset": 80}], "year": 2017, "abstractText": "Symbolic has been long considered as a language of human intelligence while neural networks have advantages of robust computation and dealing with noisy data. The integration of neural-symbolic can offer better learning and reasoning while providing a means for interpretability through the representation of symbolic knowledge. Although previous works focus intensively on supervised feedforward neural networks, little has been done for the unsupervised counterparts. In this paper we show how to integrate symbolic knowledge into unsupervised neural networks. We exemplify our approach with knowledge in different forms, including propositional logic for DNA promoter prediction and firstorder logic for understanding family relationship.", "creator": "LaTeX with hyperref package"}}}