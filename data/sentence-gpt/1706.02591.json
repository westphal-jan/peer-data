{"id": "1706.02591", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2017", "title": "Dynamic Discovery of Type Classes and Relations in Semantic Web Data", "abstract": "The continuing development of Semantic Web technologies and the increasing user adoption in the recent years have accelerated the progress incorporating explicit semantics with data on the Web. With the rapidly growing RDF (Resource Description Framework) data on the Semantic Web, processing large semantic graph data have become more challenging. Constructing a summary graph structure from the raw RDF can help obtain semantic type relations and reduce the computational complexity for graph processing purposes. In this paper, we addressed the problem of graph summarization in RDF graphs, and we proposed an approach for building summary graph structures automatically from RDF graph data. Moreover, we introduced a measure to help discover optimum class dissimilarity thresholds and an effective method to discover the type classes automatically. In future work, we plan to investigate further improvement options on the scalability of the proposed method. We have published some new RDF graphs and data source code in several papers and can be found here. Finally, we propose a more formal classification of RDF graphs with explicit semantics. For this paper, we will also introduce a more formal classification of RDF graphs with explicit semantics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 31 May 2017 11:58:31 GMT  (389kb,D)", "http://arxiv.org/abs/1706.02591v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.AI", "authors": ["serkan ayvaz", "mehmet aydar"], "accepted": false, "id": "1706.02591"}, "pdf": {"name": "1706.02591.pdf", "metadata": {"source": "CRF", "title": "Dynamic Discovery of Type Classes and Relations in Semantic Web Data", "authors": ["Serkan Ayvaz", "Mehmet Aydar"], "emails": ["serkan.ayvaz@eng.bau.edu.tr", "maydar@kent.edu"], "sections": [{"heading": null, "text": "Keywords Semantic Web \u00b7 RDF \u00b7 Graph Summarization \u00b7 Automatic Weight Generation"}, {"heading": "1 Introduction", "text": "The Web as the global information source is growing exponentially. In recent years, there has been significant developments in publishing data with expressed semantics in the Web. The Linking Open Data[6] and similar community projects have recommended the publication of large amount of globally useful datasets in machine-readable forms. Moreover, the utilization of\nSerkan Ayvaz Department of Software Engineering, Bahcesehir University, Besiktas 34353, Istanbul, Turkey. E-mail: serkan.ayvaz@eng.bau.edu.tr\nMehmet Aydar Department of Computer Science, Kent State University, Kent, Ohio 44240, USA. E-mail: maydar@kent.edu\nar X\niv :1\n70 6.\n02 59\n1v 1\n[ cs\n.D B\n] 3\n1 M\nay 2\n01 7\nResource Description Framework (RDF), along with other forms of semantic data in forms of RDFa [1] and microformats [22] in web pages has expanded.\nAs a standard data model for the Semantic Web, RDF is a graph-structured general purpose language for representing information in a way that the resources are described unambiguously using RDF statements. The RDF statements are in the form of subject-predicate-object triples. Every triple is a relationship between two entities.\nRDF uses rdf:type property for stating class membership of entities. The entity type information is particularly useful for semantic searches in finding related entities and in traversal of the hierarchical structure of the RDF graph. However, the semantic data available on the Web today often don\u2019t have precise entity type information. It is partially due to (1) not containing the entity types owing to the flexibility of RDF model not forcing constraints on the schema, (2) representing data with non-standard vocabularies for typing as some data publishers do not use standard vocabularies such as rdf:type and rdfs:subClassOf, which is making it challenging to locate the type triples, furthermore, (3) defining the type information too generally that loosely coupled entities are represented in the same types.\nConstructing a graph structure containing the entity type classes, class attributes and relations between the type classes can be instrumental for Semantic Search algorithms in terms of query time since the entire input data does not need to be completely processed at the query time. We call this structure as the Summary Graph [5,4]. Semantic search is a common graph processing task and it often requires a summary graph structure for effective and faster graph processing. For instance, the semantic search approach proposed by [41] uses a summary graph structure in the search mechanism. They generate the summary graph using a set of aggregation rules, which calculate the equivalence classes of all nodes belonging to one type class and project all edges to corresponding edges accordingly. In this approach, one needs to know what constitutes a type class in advance. However, this assumption may not be realistic for real-world RDF datasets, i.e., the RDF data may not be tied to a standard ontology or vocabulary. Our work attempts to address this issue by automatically building the summary graph structure from the data itself by utilizing graph node similarity scores.\nThere exists related methods to obtain a summary graph: (1) A summary graph can be obtained from the dataset ontology, if the dataset is already tied to an ontology. (2) Another way to obtain the summary graph is to locate the type triples (rdf:type) in the dataset and to organize the type classes and relations accordingly, if the data set is published using a standard vocabulary [14]. (3) Or the summary graph can be built automatically without relying on an ontology or a standard vocabulary. Our graph summarization approach is based on the latter method. Rdf:type is an optional property and it is often missing in commonly used datasets. Furthermore, it can potentially be inconsistent or erroneous in some cases [32]. Thus, the methods, which rely on rdf:type or existence of an ontology may have implications for general use. Therefore, there is a need for automatic generation of summary graphs from\nRDF Data. In this regard, we describe an entity similarity metric and the methods used for automatically generating a summary graph from RDF Data. To the best of our knowledge, this is the first approach to attempt to generate summary graph of RDF graph automatically based on the entity similarities.\nContributions and Outline\nIn this study, we focus on the problem of efficiently building a summary graph structure automatically from underlying RDF data. We utilize the notion of entity similarity in an RDF dataset so that fundamentally similar entities could be associated with the same class, which we call it type class in this paper.\nIn our approach, the type classes, importance weight of each property and each string word for each of the referenced IRIs (Internationalized Resource Identifiers) are auto-generated. Furthermore, the weights in the pairwise similarity calculation are generated dynamically and applied during the summary graph generation. Our methodology is to utilize graph locality and neighborhood similarity. Our algorithm does not rely on the existence of a common vocabulary. We use the Jaccard measure context for entity similarity such that the properties of the entities are treated as the dimensions of the entities.\nA stability measure, which represents the degree of confidence of a relation between classes in the summary graph, is proposed. From the input RDF data itself, we generate the summary graph along with the classes and class relations with the stability measure for each class relation. The main contributions of present study are\n\u2013 We investigated the graph summary problem in RDF graphs and provided an effective approach for generating summary graphs automatically from RDF data. \u2013 For automatic discovery of the summary graphs, we introduced a measure, which we call Favorability that helps discover optimal class dissimilarity thresholds and provided an effective method that discovers the type classes using the class threshold measure. \u2013 To assess the effectiveness of our approach, we applied our methods to real-world datasets.\nThe rest of the paper is organized as follows. In section 2, we define the graph summarization problem in RDF graph data. Then, in section 3, we discuss our methods and the algorithms in detail. In section 4, the results of the evaluations assessing the efficiency of the proposed methods are presented. Finally, we review the related work in section 6 and follow this with our conclusion and future work in section 7."}, {"heading": "2 Defining Graph Summarization Problem", "text": "RDF data consist of a collection of statements that intrinsically represent a labeled, directed multi-graph with which the resources are expressed unam-\nbiguously. RDF statements describe resources in the form of triples, consisting of subject-predicate-object expressions that describe a resource, the type of a resource (type triple), or a relationship between two resources [12].\nTo describe resources, each RDF node that corresponds to an RDF entity is represented with an IRI. The values such as strings, numbers and dates in RDF data are represented by literal nodes. A predicate in an RDF triple is also called a property of the RDF subject node. A predicate can be one of two types: a DatatypeProperty where the subject of the triple is an IRI and the object of the triple is a literal or an ObjectProperty where both the subject and object of the triple are IRIs. Each object of a subject node is called a neighbor of that subject node. The subject in an RDF triple is either an IRI or a blank node, the predicate is an IRI, and the object is either an IRI, a literal or a blank node. The subjects and objects of triples in the RDF graph form RDF nodes. As an example, figure 1 represents two sample university entities Kent State and Case Western and their properties.\nFormally, RDF graph is a directed labeled graph, which can be represented as G = (V,L,E), where the set of vertices V represents entities (resources), the set of directed edges E of the form l(u, v), with u \u2208 V , v \u2208 V and l \u2208 L,\ndenote predicates (properties) between entities, and the labels L are predicate names or labels. Note that an edge l(u, v) represents the RDF triple (u, l, v).\nA summary graph of a data graph is the directed graph such that each node in the summary graph is a subset of the original graph nodes of the same type. Thus, we define the Graph Summarization Problem as finding the corresponding summary graph G\u2032 = (V \u2032, L\u2032, E\u2032) of G, such that each element of V \u2032 is a subset of V containing all elements of the same type. For v \u2208 V , we let [v] denote the subset of V containing all elements in V with the same type class as v. The vertices V \u2032 in the summary graph G\u2032 are equivalence classes over the original graph G, and the vertices in V \u2032 are disjoint subsets of V . E\u2032 and L\u2032 are, respectively, the sets of edges and labels in the graph G\u2032. Hence, L\u2032 \u2282 L, and the elements of E\u2032 are defined by the elements in the equivalence classes in V \u2032 and the edges in E. Let u, v \u2208 V ; then [u], [v] \u2208 V \u2032. There is an edge l([u], [v]) \u2208 E\u2032 if and only if there exist s \u2208 [u] \u2286 V and t \u2208 [v] \u2286 V such that l(s, t) \u2208 E."}, {"heading": "3 Methods", "text": "The Graph Summarization Problem can be considered as a problem of identifying entity type classes. The set of entity type classes can be inferred from RDF data such that each type class in the set of entity types contains the same or very similar entities only. In our method, the entity type classes are derived from the entity similarities. The discovery of the type classes, i.e., the elements v in V \u2032 can be also seen as clustering problem. Using the calculated similarity measurements of the entity pairs, the entities that are the same or very similar, satisfying a similarity threshold, are combined in the same type class. Our entity similarity measurement approach is based on the intuition that the graph nodes that have similar relations to similar neighbors tend themselves to be similar nodes.\nPreviously, we investigated methods for computing entity similarities effectively. We then developed a framework for building a summary graph structure in RDF data [5,4]. This current study extends our summary graph generation approach [5] and enhances it by incorporating a measure to help discover optimum class dissimilarity thresholds and an effective method to discover the type classes automatically."}, {"heading": "Similarity of IRI Nodes", "text": "Intuitively, the characteristics of an RDF graph node are defined by its properties and the neighboring entities which are connected and related by similar properties. Based on the intuition that similar IRI nodes tend to have similar properties and interact with similar neighbor nodes, the similarities of entities in our method are calculated using the predicates of the IRI nodes, in addition to the neighbor nodes that they interact with common predicates. By neighbor we mean that a neighbor of a graph node is another node which is \u201cconnected\u201d\nby a predicate. More formally, a node u is connected to node v, i.e., u is a neighbor of v, if there is a label l \u2208 L such that l(u, v) \u2208 E. Therefore, a node and its neighbors are connected by a property. Thus, the similarity calculation may yield more accurate results with the addition of neighborhood similarity."}, {"heading": "Similarity of Literal Neighbor Nodes", "text": "The similarity of literal nodes indirectly impact the similarity of IRI nodes in the calculation of neighborhood similarity. The neighbors of IRI nodes can be either other IRI nodes or literals. Incorporating neighboring literals in the computation of the similarity of pairs can be beneficial, especially, in datasets where the entities are commonly described using literals. Therefore, the similarity of literal neighbor nodes are taken into account in our approach.\nA literal node can consist of two or three elements: a lexical form, a datatype IRI and a language tag. The language tag in a literal node is included if and only if the datatype IRI of the literal node corresponds to rdf:langString [7]. It is important to note that the literals should be in the same language while incorporating literals in the computation of the similarity of IRI node pairs. As the same literals may have totally different meanings in different languages, we are assuming that all the literals are in the same language. If present, the rdf:langString component of the literal nodes is expected to have only one value. When calculating the similarity, the lexical form and the data type URI components a pair of literal nodes are considered. As comparing different data types is meaningless, the similarity of literal nodes is considered only when the two data types are equal.\nInferring the semantics of literal nodes is challenging. To calculate the similarity of pairs of IRI nodes, an effective literal node similarity metric is needed. We make use of string similarities for the lexical form components of the literal nodes that measures common words within the two lexical forms along with their auto-generated importance weights. While calculating the weight of word importance in literal nodes consisting of a set of words, we consider the following factors: the source subject node, the frequency of the word within the triple collection for each subject node, and the frequency of the word within the entire dataset.\nLiteralSim(x, y) =\n\u2211 i\u2208(x\u2229y)\n|ti| \u00d7 wi\u2211 j\u2208(x\u222ay) |tj | \u00d7 wj (1)\nwhere t is the term that appears in the neighbor literal nodes, such that, u, v are IRI nodes in V , x, y are literal nodes in V , and l(u, x), l(v, y) \u2208 E, and\u2211 j\u2208(x\u222ay) wj = 1. |ti| is the number of times the term appears and wi is the importancy weight of the term for the literal nodes (u, v). The importancy weight of the term for the literal nodes is calculated based on the concept of the term frequency-inverse document frequency (tf \u2212 idf)\n[27,37], which is a widely known technique in information retrieval. tf \u2212 idf indicates that some terms may be important in some documents but not as important in other documents. Said differently, the importance of a word in a document increases by its frequency in the document but its importance decreases by its frequency in the corpus [34].\n3.1 Computation of Pairwise Entity Similarities\nTo identify the type classes in the summary graph, a metric is required to calculate the similarities of entities. For entity similarity metric, we employ an efficient graph node pair similarity metric, which utilizes the graph localities and neighborhood similarity within RoleSim similarity [21] in conjunction with the Jaccard measure context [19]."}, {"heading": "Jaccard Similarity Measure", "text": "The Jaccard similarity coefficient also known as the Jaccard index is a wellknown statistical measure. It is commonly used for comparing similarity and diversity of sample sets. Jaccard similarity is simply defined as the size of the intersection divided by the size of the union of the sample sets [19].\nFor given two sets S1 and S2 in a dataset, the Jaccard similarity, J(S1, S2), between S1 and S2 is formulated as:\nJ(S1, S2) = |S1 \u2229 S2| |S1 \u222a S2|\n(2)\nWhen calculating the Jaccard similarity in RDF data, the subject nodes are considered to be the names or labels for the sets. Thus, the subject of the triples determine the sets. Similarly, the properties of the triples whose subject is the name or label of the set are the elements of each set. The objects of the triples whose subject is the name or label of the set become the neighbors of each subject set. The object nodes, or in other words the neighboring nodes, may themselves be names or labels of sets. For given two subject nodes u and v in an RDF graph, we calculate the Jaccard similarity by noting that |u \u2229 v| is the number of predicates that the subject nodes u and v have in common while |u \u222a v| is the number of predicates in the union of the subject nodes u and v."}, {"heading": "RoleSim Similarity Measure", "text": "The Jaccard similarity has a limitation when the Jaccard index applied to an RDF graph. Because the Jaccard index determines the set similarity based on the number of common set elements only, by treating the subject nodes as sets and the predicates of the subject nodes as the set elements. However, it does not consider the relations between set elements. Thus, it does not take into account the neighboring node similarities.\nFor this reason, we utilize the RoleSim similarity metric which is based on the maximal matching of neighborhood pairs and a simple iterative computational method. The intuition in RoleSim similarity measure is that two nodes or entities tend to have the same role when they interact with equivalent sets of neighbors.\nGiven a regular unlabeled graph G = (V,E), RoleSim measures the similarity of each node pair in V based on their neighborhood similarities [21]:\nRoleSim(u, v)= (1\u2212 \u03b2) (3)\n\u00d7maxM\u2208Mm(u,v)\n\u2211 (x,y)\u2208M RoleSim(x, y)\nNu +Nv \u2212 |M | +\u03b2\nRoleSim(u, v) denotes the similarity of the nodes u, v \u2208 V . The definition of RoleSim is recursive; i.e., RoleSim(x, y) is calculated the same way as RoleSim(u, v). N(u) and N(v) denote their respective sets of neighborhoods and Nu and Nv denote their respective degrees, i.e., Nu = |N(u)| and Nv = |N(v)|.\nM is defined as a set of ordered pairs (x, y) where x \u2208 N(u) and y \u2208 N(v) such that there does not exist (x\u2032, y\u2032) \u2208M , s.t. x = x\u2032 or y = y\u2032, and moreover, M is maximal in that no more ordered pairs may be added to M and keep the constraint above. Mm(u, v) is the set of all such M \u2019s. Mm(u, v) is a set of sets.\nM is a maximal subset of N(u) \u00d7 N(v) such that no element of N(u) appears more than once as a first coordinate and no element of N(v) appears more than once as a second coordinate of an ordered pair in M . Thus, |M | = min(Nu, Nv). The maximal matching ensures that the total value of selected cells has the maximum possible value. The maximal matching value, M(u, v), is calculated as\nM(u, v) = maxM\u2208Mm(u,v)\n\u2211 (x,y)\u2208M RoleSim(x, y)\nMax(Nu, Nv) (4)\nThe parameter \u03b2 is a decay factor, 0 < \u03b2 < 1. The parameter \u03b2 is for decreasing the influence of neighbors with further distance which dampens the recursive effect."}, {"heading": "Combined Pairwise Entity Similarity Measure", "text": "To utilize neighborhood similarity in RDF graphs, we improve the initial Jaccard similarity by augmenting it with the RoleSim similarity measure of the neighboring nodes. When computing neighborhood similarity, comparing all\nneighbors to all neighbors is not an efficient method. Thus, we compare only the neighboring nodes which are related by the same predicate. For instance, given two nodes u and v, let\u2019s assume that s1 and s2 are neighbors of u, and t is a neighbor of v. We calculate similarity of the neighborhood pairs (s1, t) and (s2, t) only if there is a predicate which connects u to s1, u to s2 and also connects v to t, and we use the maximum similarity between the neighborhood pairs (s1, t) and (s2, t) as implied in the maximal matching concept of RoleSim similarity measure. The impact of the similarity of the neighbor nodes are weighted by each common predicate.\nWe note, however, that the generic version of the RoleSim measure is introduced for the unlabeled graph. In this work the input data is in RDF model, which is a directed and labeled graph. Therefore, we utilize the RoleSim(u, v) measure when there may be multiple neighbors reached from the node pairs u and v by a common predicate, where u and v are the nodes in the input graph.\nIn the lists below, for 1 \u2264 i \u2264 j, li is a label for an edge, i.e., li \u2208 L. When 1 \u2264 h \u2264 j and if i and h are not equal, then li and lh are different labels, i.e., li and lh are different properties. [xi] and [yi] are the sets of nodes which are related to u and v, respectively, by predicate li.\nl1(u, [x1]), l2(u, [x2]), ...lj(u, [xj ]) \u2208 E l1(v, [y1]), l2(v, [y2]), ...lj(v, [yj ]) \u2208 E. Thus, we are assuming that there are j different predicates which are predicates in triples with subject u and are also predicates in triples with subject v.\nThen, by using the Jaccard index in conjunction with the RoleSim measure, their similarity can be calculated as:\nPairSim(u, v)k= (1\u2212 \u03b2) (5)\n\u00d7 1 |u \u222a v| \u00d7( \u2211\nj\u2208(u\u2229v)\nmaxM\u2208Mmj(u,v)(\n\u2211 (x,y)\u2208M Sim(x, y)k\u22121\nN ju +N j v \u2212 |M |\n)\u00d7 wj)\n+\u03b2\nwhere k is the iteration number 1 \u2264 k < MaxIter, MaxIter is the maximum number of iterations, such that, if k = 3 then PairSim(u, v)k denotes to the similarity of the node pair (u, v) at the third iteration and PairSim(u, v)k\u22121 denotes to the similarity of the node pair (u, v) by the end of the second iteration. Also, N j(u) and N j (v) denote their respective neighborhoods that are reached by jth common edge. x \u2208 N j(u) and y \u2208 N j (v), and N j u and N jv denote their respective degree connected by jth common edge. Said differently, N j(u) is the cardinality of [xj ], and N j (v) is the cardinality of [yj ].\nwj is the weight of the property connecting the graph nodes (u, v) and their respective neighbors (x, y).\nSim(x, y)k\u22121 =  PairSim(x, y)k\u22121, if x,y are IRI nodes LiteralSim(x, y), if x,y are Literal nodes\n0, otherwise\n(6)\nWe define M to be a set of ordered pairs (x, y) where x \u2208 N j(u) and y \u2208 N j( v) such that there does not exist (x\n\u2032, y\u2032) \u2208 M , s.t. x = x\u2032 or y = y\u2032, and furthermore, M is maximal in that no more ordered pairs may be added to M and keep the constraint above. Mmj(u, v) is the set of all such M \u2019s. Mmj(u, v) is a set of sets.\nBy a \u201cmaximal nonrepeating matching\u201d, we mean that we form as many pairs as we can from the elements in N j(u) and N j (v) with the restriction that no element in either N j(u) and N j (v) may be used in more than one ordered pair. The parameter \u03b2 is a decay factor 0 < \u03b2 < 1, which helps reduce the influence of neighbors with further distance due to the recursive effect. l1(u, x) and l2(v, y) represent directed edge labels s.t. l1, l2 \u2208 L, and l1 = l2, x \u2208 N(u) and y \u2208 N(v).\n3.2 The Summary Graph Generator Algorithm\nWhile calculating the neighborhood similarity, our proposed node similarity metric makes calls to the immediate neighbors\u2019 similarities. Since neighbors\u2019 similarities depend on their own neighbors\u2019 similarities, the immediate neighbors\u2019 similarities are not known ahead of time. A solution involving recursive calls is not an efficient option in this case as it may lead to inefficient resource utilization and excessive recursion. For instance, an object node n1 of a subject node n2 in an RDF triple may be a subject node n1 of the object node n2 in another RDF triple. Therefore, our algorithm runs in multiple iterations until the rate of change in calculated similarities drops under a given threshold. It is a similar approach to the PageRank algorithm [30]. The initial similarity of a node pair is set to 1 if they share a common predicate and 0, otherwise. Such that: \u2200(u, v \u2208 V ) : (S(u, v) = 1)\u2192 (|u \u2229 v| > 0) and, (S(u, v) = 0)\u2192 (|u \u2229 v| = 0)\nOur approach is two folds. Firstly, the pairwise similarity algorithm calculates the similarity values for each pair which constructs a similarity matrix. Once the pairwise similarities converge, the type class generation algorithm begins and generates the type classes i,e, it assigns the node u and v to the\nsame type class if their dissimilarity value is less than an auto calculated threshold which is the class dissimilarity threshold.\nAs the algorithm generates common pairs if two candidate nodes that share at least one common predicate, the overall complexity for the algorithm is n2 in the worst case. It occurs when all subject nodes in the RDF graph have a common predicate with every other subject node. When the noise predicates excluded, i.e. the predicates that is referenced by most if not all the subject nodes, the algorithm performs better than n2. Thus, the complexity of the algorithm depends on the characteristics of the dataset. On a dense graph, the complexity approaches to n2 while it gets near to n(logn)k time in sparse graphs, where k is a constant number of iteration.\nThe basic steps of the algorithm include sorting the triples according to their predicate label, extraction of the subject node pairs for each of the predicates, running the similarity computation algorithm in iterations until convergence and generating the type classes based on the calculated similarity measures.\nThe type class generation algorithm creates distinct classes, such that subject node pairs that have similarity greater than a given threshold get put to the same type class. The input parameter \u03b2 is a decay factor 0 < \u03b2 < 1. l1(u, x) and l2(v, y) represent directed edge labels s.t. l1, l2 \u2208 L, and l1 = l2, x \u2208 N(u) and y \u2208 N(v).\n3.3 Dynamic Assignment of Weights of IRI Node Descriptors\nAn IRI node is described through its predicates and the collection of literal neighboring nodes in the lexical form. For simplicity, we call the predicates and literal neighboring nodes as descriptors of the IRI nodes. As stated above, the similarity of a pair of IRI nodes depend upon their descriptor similarities and the similarities of their neighbors.\nThe weight of each descriptor may vary significantly as each descriptor may have different impact on an IRI node. Hence, identifying appropriate metrics for generating IRI descriptor weights is a vital task in computation of accurate similarity values.\nUpon investigations on the factors that can impact the weight of a descriptor, we propose an approach in this study for generating the importance weights of the IRI node descriptors automatically. Based on the investigations, we think that the weight of a descriptor may differ for each IRI for which it is a descriptor and the weight increases proportionally by the number of times a descriptor appears in the reference IRI, but it is offset by the frequency of the descriptor in the entire RDF dataset. This tendency is similar to the concept of the term frequency-inverse document frequency (tf\u2212idf). While computing the weight of properties dynamically, we apply the tfidf to the properties and nodes in RDF graphs. tfidf is calculated as follows:\ntfidf(p, u,G) = tf(p, u)\u00d7 idf(p,G). (7)\nAlgorithm 1: SimMeasure\ninput : Graph G(V, L,E) output : Similarity-Matrix S, Pairs H parameter: MaximumIteration MaxIter, Iteration-Convergence-Threshold Ict \u2200pair(u, v) \u2208 V (S(u, v)\u2190 0); H \u2190 \u2205; T \u2190 Sort(E) by l, u, v s.t. l(u, v) \u2208 E; for each distinct pair(u, v) from T do\nif \u2203(l1(u, x) and l2(v, y)) s.t. l1 = l2 then S(u, v)\u2190 1 P (u, v)\u2190 (u, v, Lj , Nj(u), Nj(v)) where u, v \u2208 V , Lj is the list of common\nlabels between u and v, and Lj \u2208 L H \u2190 H \u222a {P (u, v)}\nend\nend Sprevious \u2190 \u2205 converged\u2190 false count\u2190 0 while converged = false and count < MaxIter do\nfor each((u, v, Lj , Nj(u), Nj(v)) \u2208 H) do\nPairSim(u, v)k = (1\u2212 \u03b2)\u00d7 1|u\u222av| \u00d7 ( \u2211\nj\u2208(u\u2229v) maxM\u2208Mmj(u,v)(\n\u2211 (x,y)\u2208M Sim(x,y)k\u22121\nN j u+N j v\u2212|M|\n)\u00d7 wj) + \u03b2\nS(u, v)\u2190 PairSim(u, v)k end converged = |S \u2212 Sprevious| \u2264 Ict Sprevious \u2190 S count\u2190 count+ 1\nend return S,H\nwhere the term frequency (tf) [27] represents the frequency of a proposition p with respect to a graph subject node u. More exactly, when u \u2208 V and p \u2208 L, then\nf(p, u) = |{v \u2208 V : p(u, v) \u2208 E}|. (8)\nEquivalently, f(p, u) is the number of RDF triples with subject u and property p.\nTo define tf(p, u), it is helpful to have a notation for the set of all properties with subject u. Thus, for u \u2208 V , L(u) = {q \u2208 L : \u2203v \u2208 V with q(u, v) \u2208 E}. Then\ntf(p, u) = f(p, u)\u2211\nq\u2208L(u) f(q, u) . (9)\nAlgorithm 2: CreateClasses input : Similarity-Matrix S, Pairs H output : Auto-Generated-Type-Classes-Map C parameter: Class-Dissimilarity-Threshold\nfor each((u, v, Lj , Nj(u), Nj(v)) \u2208 H) do if C(u) exists then\nci \u2190 C(u) else\nci \u2190 {u} end\nend if 1\u2212 S(u, v) < then\nif C(v) exists then ci \u2190 ci \u222a C(v) else\nci \u2190 ci \u222a {v} end\nend ci \u2190 C(v) C(u)\u2190 ci C(v)\u2190 ci\nend\nend return C\nThe inverse document frequency (idf) [37] represents the frequency of a property usage across all graph nodes, and it is defined as\nidf(p,G) = ln |V |\n|{u \u2208 V : p \u2208 L(u)}| . (10)\nThe property importance weights are based on the degree of distinctiveness of a property describing an entity. With property distinctiveness, we mean the uniqueness of a property in describing the key characteristics of an entity type. For instance, if a property is specific to an entity type, it is a distinguishing character of the type from other types. When a property exists in all entity types, its quality of being distinctive is low. The noise labels tend to be common for a majority of entities if not for all entities. By increasing importance weights of properties with a higher degree of distinctiveness, we reduce the importance of noise labels automatically. As a result, the noise labels have significantly less impact on the overall similarity measures.\n3.4 Class Predicate Stability\nIn this work, the summary graph is built automatically from an RDF dataset. However, automatically generated summary graphs can be error prone. It is essential to have an effective metric to measure the degree of confidence of a relation between classes in the summary graph. We define this metric as Class\nPredicate Stability (CPS), which is a similar notion to the concept of stability that introduced by Paige and Tarjan [31].\nFor u and v being IRIs in the dataset, G = (V,E, L), and u \u2208 c1 and v \u2208 c2 with both c1 and c2 being type classes in the summary graph, G\n\u2032 = (V \u2032, E\u2032, L\u2032), a class relation between the class c1 and the class c2 is generated as a predicate and represented as l(c1, c2) if there is at least one relation l(u, v). Consequently, l \u2208 L\u2032 and l(c1, c2) \u2208 E\u2032.\nThe CPS metric is calculated as the number of the IRI nodes u in class c1 having a triple of the form (u, p, v) with u \u2208 c1 and v \u2208 c2 divided by the total number of the IRI nodes in c1 in the summary graph such that the triple (c1, p, c2) is in the summary graph G\n\u2032 and c1 and c2 are type class IRI nodes with p being a predicate between them. CPS(c1, p, c2) is formulated as\nCPS(c1, p, c2) = |(u, p, v) : u \u2208 c1, v \u2208 c2}|\n|c1| (11)\nwhere |c1| is the number of IRI nodes in the class c1. Note that |c1| > 0. The CPS value for a triple (c1, p, c2) indicates the degree of partitioning coarseness of the type classes c1 and c2 with the predicate p in the summary graph. Hence, the mean of all the CPS values in the summary graph is an indicator of accuracy for the generated summary graph. CPS(G\u2032) is formulated as\nCPS(G\u2032) =\n|E\u2032|\u2211 i=1 CPS(ci1, p i, ci2)\n|E\u2032| (12)\nwhere G\u2032 = (V \u2032, E\u2032, L\u2032) is the summary graph and pi(ci1, c i 2) \u2208 E\u2032, and thus |E\u2032| > 0. For two classes c1 and c2 in the summary graph, when either all the IRI nodes from c1 are connected with a predicate p to at least one IRI node in c2 or none of the IRI nodes in c1 are connected with the predicate p to an IRI node in c2, we call that the classes c1 and c2 have full CPS.\n3.5 Automatic Calculation of the Class Dissimilarity Thresholds\nOur approach automatically builds the summary graph from RDF data. A drawback in the automatic summary graph generation approach is the need for estimating the optimum parameters that help determine the type classes. As expected, higher class dissimilarity threshold generates more coarse classes, whereas the classes become more granular when the threshold is chosen smaller. The optimum values for the class dissimilarity threshold depend on the characteristics of the datasets. In real-world datasets, users may not have a good grasp on the underlying data to determine optimal class dissimilarity threshold values.\nTo determine how closely the entities fit the type class, an effective metric is needed to measure the degree of fit within each type class in the summary graph. For this purpose, we utilize the root-mean-square deviation (RMSD), which is a commonly used measure of the differences between the values in comparison [24].\nThe root-mean-square deviation (RMSD) in RDF summary graphs\nThe RMSD represents the amount of the deviations of IRI node property values from the class center and provides a single measure of predictive power. In RDF summary graph, we calculate the overall RMSD by aggregating the sum of RMSD values for each type class in the summary graph.\nTo calculate the RMSD of summary graph, we first determine the centroids for each type class and then compute the RMSD between the class centroids and all IRI nodes within the type class using Manhattan distance. In RMSD calculation, the IRI node properties represent the dimensions of the IRI nodes within the type class. RMSD(G\u2032) of summary graph G\u2032 is formulated as follows\nRMSD(G\u2032) = \u2211 ci\u2208G\u2032 \u221a\u221a\u221a\u221a\u221a n\u2211(i=1)\u2208L\u2032(xi \u2212 x\u0304) n\n(13)\nwhere ci, L \u2032 are, respectively, the list of classes and the property labels in the summary graph G\u2032. xi represents the IRI nodes in type class and x\u0304 denotes the centroid for members of a particular type class in the summary graph G\u2032.\nHigher RMSD values in a summary graph indicate that entities within type classes sparsely located. When the entities in type classes are very similar to each other, the center of the cluster will be dense. Thus, the sum of distances to the centroids and the cumulative RMSD value will be lower accordingly."}, {"heading": "Discovery of Class Dissimilarity Threshold", "text": "To discover the type classes in summary graph automatically, we propose a measure, called Favorability, to calculate the class dissimilarity threshold automatically as follows.\nFavorability(G\u2032) = max\n{ Stability(G\u2032) \u2217 TypificationRate(G\u2032)\n(RMSD(G\u2032) + 0.1)\n} (14)\nThe idea behind the formula is that we think that the quality of summary graph type classes is associated and directly proportional to the summary graph stability, a measure of relationships between type classes, and the ratio of entities belonging to a type class, and inversely proportional to the RMSD, the degree of inner class deviation.\nAlgorithm 3: FindOptimumEpsilon input : Similarity-Matrix S, Pairs H, Minimum-Threshold min ,\nMaximum-Threshold max , Number-of-try n, Previous-Favorability prev favor, Previous-Optimum-Threshold prev optimum\noutput : Optimum-Threshold optimum parameter: Epsilon-Convergence-Threshold Ect current \u2190 min inc\u2190 (max \u2212min )/n optimumfavor \u2190 prev favor optimum \u2190 prev optimum while current \u2264 max do\n(G\u2032, C)\u2190 CreateClasses(S,H, current ) Favorability(G\u2032) = Stability(G \u2032)\u2217TypificationRate(G\u2032)\n(RMSD(G\u2032)+0.1)\nif Favorability(G\u2032) > optimumfavor then optimumfavor \u2190 Favorability(G\u2032) optimum \u2190 current end current \u2190 current + inc\nend if |(optimumfavor \u2212 prev favor)| > Ect then\noptimum \u2190 FindOptimumEpsilon(S,H, optimum \u2212 inc, optimum + inc, n/2, optimumfavor, optimum )\nend return optimum\nIn the formula, Favorability(G\u2032) is the class dissimilarity threshold for the summary graph G\u2032 and TypificationRate(G\u2032) represents the rate of entities that belong to a type class based on the class dissimilarity threshold. The TypificationRate(G\u2032) is low when the class dissimilarity threshold is too high since the number of entities satisfying high similarity threshold for class membership will be small.\nTo obtain the high quality results while reducing the computation cost, we gradually change the threshold values in constant number of times and set the class threshold value that provides the maximum value of the proposed measure. The algorithm 3 demonstrates how the optimum class dissimilarity threshold is discovered utilizing the favorability measure. In the algorithm, optimum refers to the optimum class dissimilarity threshold for the summary graph.\nThe proposed automatic threshold discovery measure is not assumed to be perfect. Finding optimum summary graph type classes is a formidable problem as the quality of summary graph is dependent on the type of datasets. Despite this, the proposed measure integrates different aspects of the graph summaries and provides intuitively accurate graph summaries based on our evaluations."}, {"heading": "4 Evaluations", "text": "In the evaluations, we conducted preliminary experiments on three datasets: a subset of DBpedia [3]; a subset of SemanticDB [13], and a subset of Lehigh University Benchmark (LUBM) [17]. Our experimental datasets are in different domains and they represent different aspects of real world semantic data.\nSemanticDB is a Semantic Web content repository for Clinical Research and Quality Reporting in cardiovascular surgery domain. The structured entity type information exist in SemanticDB which we utilized as the ground truth for the verification of the algorithm. Lehigh University Benchmark (LUBM) is a well-known benchmark for OWL knowledge base systems, which also has entity type information available. But unlike SemanticDB, LUBM data has hierarchical types. Lastly, DBPedia a central source in the Linked Open Data Cloud [6] and is a commonly used general purpose dataset. However, using the entity type information for the verification of the algorithm is more problematic in DBPedia, as the type information may not present, or an entity may have several types including the hierarchical types. Therefore, we manually verified the results of the algorithm. Table 1 demonstrates a sample of RDF triples from each dataset in the evaluations.\n4.1 Assessing Algorithm Parameters\nWe tested several parameters of the algorithm, including the maximum iteration, beta factor, class dissimilarity threshold, iteration convergence threshold (Ict), and the size of dataset in type generation. The results of our evaluations are demonstrated in Table 2. For verification, we extracted the ground truth, entity types present in the datasets, against the entity type classes generated by the algorithm. For the assessment of our evaluations, we used the measure of precision. Precision is defined as the ratio of correct results over all results.\nThe similarity computation algorithm stops the iterations, once the rate of change in the similarity measures drops below the threshold or once it reaches the maximum number of iterations. In our evaluations, we observed that the similarity measures typically converge after a few iterations with the values of the maximum number of iterations and the iteration convergence threshold being as 10 and 0.001, respectively.\n4.2 Performance of dynamic assignment of descriptor weights\nWe also evaluated the performance of dynamic assignment of descriptor weights. Table 3 shows a sample of dynamically assigned descriptor weights from each dataset. As expected, the algorithm assigned higher weights to the properties with a higher degree of distinctiveness describing the resource type. For instance in LUBM dataset, takesCourse property is more descriptive of the Student type than the name property, which is a common property for all\ntype classes in the dataset. Thus, takesCourse was assigned a weight of 44.1% as compared to the weight of 7.5% for name.\n4.3 Effectiveness of the Automatic Computation of Class Thresholds\nIn a set of evaluations, we further assessed the effectiveness of automatic calculation of the class threshold approach using a subset of the same set of datasets. As demonstrated in Table 4, the stability, RMSD and optimum class threshold may vary depending on the characteristics of the datasets. In LUBM\ndataset, the RMSD result was higher compared to the other datasets. Among them, the highest optimum class dissimilarity threshold, 0.56, was achieved in DBPedia dataset. This means that the entities within the type classes of the summary graph generated by the dataset contained similar properties and were very similar to the entities that belonged to the same type class. In our evaluations, we observed that the epsilon convergence threshold around 0.9 performed well.\n4.4 Generated Summary Graph\nFigure 2 illustrates a small sample set of entities in the RDF graph from SemanticDB and their corresponding type classes in the summary graph. As demonstrated in Figure 2, the classes C-E1 and C-E2 represent the entities that are patient event types. They are classified in two different classes because when compared with the original dataset we observed that the entities in C-E1 are more specifically patient surgery-related event types while the entities in C-E2 are patient-encounter related event types. Also, the classes E-SP1 and E-SP2 are surgical procedure types. More specifically, the entities in E-SP1 are coronary artery and vascular procedure-related procedures while the entities in E-SP2 are cardiac valve related-procedures. The classes C-VP and C-CAG represent the entities that are related to vascular procedures and coronary artery grafts, respectively. We implemented a basic algorithm to name the classes based on the class member IRIs. The classes C-E1, C-E2, C-SP1, C-SP2, C-VP and C-CAG are named as C-Event-1, C-Event-2, C-SurgicalProcedure1, C-SurgicalProcedure-2, C-VascularProcedure and C-CoronaryArteryGraft, respectively.\nThe summary graph is generated along with the classes and the class relations with a stability measure for each relation. Figure 3 shows an excerpt from the summary graph representing the class relations from SemanticDB\ndataset. The percentage values beside the predicates are the stability (CPS) measure.\nOverall, we observed that the class dissimilarity threshold ranging between 0.25 to 0.6 with the beta factor of 0.15 appeared to work well in our evaluations. The automatically calculated class dissimilarity threshold values during the evaluations were in close proximity of the threshold values for the datasets that were kept as the ground truth in the assessment."}, {"heading": "5 Limitations of the method", "text": "The algorithm used for the similarity calculation runs in the n2 in the worst case and in the n(logn)k time in average, where k is a constant number of iterations. For Web-scale usage, the scalability of the algorithm needs to be further improved as the size of the input RDF data can be very large. For instance, as of today, the Linking Open Data[6] project already contains more than 30 billions triples. In future work, we plan to address the performance issues for big datasets in the worst-case scenario and perform Web-scale evaluations.\nFurthermore, the literal node similarity calculation currently does not perform well in cases where the literal nodes belong to different languages with disparate linguistic properties as we do not perform any linguistic analysis.\nAlso, in the current study, the classes in the summary graph are automatically named exploiting the frequent entity names and literal values that belong\nto the related class. The naming method may not always generate the best names for human readers."}, {"heading": "6 Related Work", "text": "The problem of Graph Summarization has been studied by various communities from different perspectives including Graph compression, graph partitioning, social network analysis, data visualization.\nFrom the Graph Compression perspective, numerous approaches have explored the graph summarization problem with the aim of reducing the storage space of the large graph datasets [33,15,18,11]. Different from these approaches, we deal with labeled directed graphs as in the case of RDF. Also, a summary graph structure based on the original graph is generated in our method.\nSeveral studies such as [28,10] have broadly investigated statistical methods to help understand the properties of large networks. These approaches provide useful information but they do not generate a summary graph from the graph data as it is the focus of our approach.\nIn the area of graph partitioning area, many methods have been introduced [29,42,39,43] to partition graph data into specific components. While these methods are helpful in discovering neighborhoods in large graph networks, they don\u2019t consider the similarities of the node properties. Tian et al. [39, 43] proposed an aggregation-based graph summarization utilizing graph node attributes. However, the approach only deals with categorical node attributes and users need to group numerical attributes into categories manually, which is not feasible for large real-world datasets.\nIn the Semantic Web community, there has also been some related studies [8,23,40,5,16]. The studies in [8] and [16] are query driven graph summarization methods. They primarily focus on the problem of SPARQL query formulation over RDF data. The approaches using bisimulation [23,40] have a limitation to be applied in real-world datasets due to the exponential complexity of bisimulation.\nNeighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36]. Especially, SimRank is a widely-known measure, which utilizes the mean of the edge similarities between nodes. However, this may reduce the similarity score of similar graph nodes in a counterintuitive manner when the nodes have multiple edges that differ in weights. On contrary, our method considers the maximal matching for calculating the similarity in a structural context.\nEntity properties might have different impact on entity similarity scores. The weights of the entity properties can be determined using a similarity measure. There are some studies that try to calculate the property weights and apply them in similarity calculations such as [35,9]. But, they primarily focus on instance matching. In instance matching, the property weights yield\nprecedence to properties making the instances more unique. Contrary to instance matching, the properties that would help describe the entity types more distinctively are weighted higher in our approach. In [9], they determine the property weights using the distinct value based weight generation and assign higher weight to a property that references more distinct values. However, a training set of instances may not always be available."}, {"heading": "7 Conclusion", "text": "In this paper, we have investigated the main aspects for graph summary problem in RDF graphs. We described our pairwise graph node similarity calculation with the addition of the property and string word importance weights, along with the Class Predicate Stability metric, which allows evaluation of the degree of confidence of each class predicate in the summary graph. Furthermore, we studied obtaining the optimum value of the class dissimilarity threshold automatically in RDF summary graphs. Based on our investigations, a measure to determine optimum class dissimilarity thresholds and an effective method to discover the type classes automatically were introduced. Using a set of real-world datasets, we assessed the effectiveness of our automatic summary graph generation approach. For future work, we plan to focus on the scalability of the proposed method in very large datasets.\nAcknowledgements The authors would like to thank Prof. Austin Melton for his invaluable help and his guidance during the study, Dr. Ruoming Jin and Dr. Viktor Lee for sharing RoleSim similarity measure."}], "references": [{"title": "RDFa in XHTML: Syntax and processing", "author": ["B. Adida", "M. Birbeck", "S. McCarron", "S. Pemberton"], "venue": "Recommendation, W3C", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Simrank++: Query rewriting through link analysis of the click graph", "author": ["I. Antonellis", "H.G. Molina", "C.C. Chang"], "venue": "Proceedings of the VLDB Endowment 1(1), 408\u2013421", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic weight generation and class predicate stability in rdf summary graphs", "author": ["M. Aydar", "S. Ayvaz", "A.C. Melton"], "venue": "Workshop on Intelligent Exploration of Semantic Data (IESD2015), co-located with ISWC2015, vol. 1472", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Building summary graphs of rdf data in semantic web", "author": ["S. Ayvaz", "M. Aydar", "A. Melton"], "venue": "Computer Software and Applications Conference (COMPSAC), 2015 IEEE 39th Annual, vol. 2, pp. 686\u2013691", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Linked data-the story so far", "author": ["C. Bizer", "T. Heath", "T. Berners-Lee"], "venue": "International journal on semantic web and information systems 5(3), 1\u201322", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Introducing rdf graph summary with application to assisted sparql formulation", "author": ["S. Campinas", "T.E. Perry", "D. Ceccarelli", "R. Delbru", "G. Tummarello"], "venue": "2012 23rd International Workshop on Database and Expert Systems Applications, pp. 261\u2013266. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Instance Matching for Ontology Population", "author": ["S. Castano", "A. Ferrara", "S. Montanelli", "D. Lorusso"], "venue": "SEBD, pp. 121\u2013132", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph mining: Laws, generators, and algorithms", "author": ["D. Chakrabarti", "C. Faloutsos"], "venue": "ACM computing surveys (CSUR) 38(1), 2", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "On compressing social networks", "author": ["F. Chierichetti", "R. Kumar", "S. Lattanzi", "M. Mitzenmacher", "A. Panconesi", "P. Raghavan"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 219\u2013228. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "RDF 1.1 Concepts and Abstract Syntax", "author": ["R. Cyganiak", "D. Wood", "M. Lanthaler"], "venue": "W3c Recommendation", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Semanticdb: A semantic web infrastructure for clinical research and quality reporting", "author": ["C. D Pierce", "D. Booth", "C. Ogbuji", "C. Deaton", "E. Blackstone", "D. Lenat"], "venue": "Current Bioinformatics 7(3), 267\u2013277", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Apples and oranges: a comparison of rdf benchmarks and real rdf datasets", "author": ["S. Duan", "A. Kementsietsidis", "K. Srinivas", "O. Udrea"], "venue": "Proceedings of the 2011 ACM SIGMOD International Conference on Management of data, pp. 145\u2013156. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Query preserving graph compression", "author": ["W. Fan", "J. Li", "X. Wang", "Y. Wu"], "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, pp. 157\u2013 168. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Query-oriented summarization of rdf graphs", "author": ["F. Goasdou\u00e9", "I. Manolescu"], "venue": "Proceedings of the VLDB Endowment 8(12)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Lubm: A benchmark for owl knowledge base systems", "author": ["Y. Guo", "Z. Pan", "J. Heflin"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web 3(2), 158\u2013182", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast general methodology for information-theoretically optimal encodings of graphs", "author": ["X. He", "M.Y. Kao", "H.I. Lu"], "venue": "SIAM Journal on Computing 30(3), 838\u2013846", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Algorithms for clustering data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": "Prentice-Hall, Inc.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "SimRank: a measure of structural-context similarity", "author": ["G. Jeh", "J. Widom"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 538\u2013543. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Axiomatic ranking of network role similarity", "author": ["R. Jin", "V.E. Lee", "H. Hong"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 922\u2013930. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Microformats: a pragmatic path to the semantic web", "author": ["R. Khare", "T. elik"], "venue": "Proceedings of the 15th international conference on World Wide Web, pp. 865\u2013866. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Explod: summary-based exploration of interlinking and rdf usage in the linked open data cloud", "author": ["S. Khatchadourian", "M.P. Consens"], "venue": "Extended Semantic Web Conference, vol. 272-287, pp. 272\u2013287. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "The wiener (root mean square) error criterion in filter design and prediction", "author": ["N. Levinson"], "venue": "Journal of Mathematics and Physics 25(1), 261\u2013278", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1946}, {"title": "Pagesim: a novel link-based measure of web page aimilarity", "author": ["Z. Lin", "M.R. Lyu", "I. King"], "venue": "Proceedings of the 15th international conference on World Wide Web, pp. 1019\u20131020. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Matchsim: a novel neighbor-based similarity measure with maximum neighborhood matching", "author": ["Z. Lin", "M.R. Lyu", "I. King"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, pp. 1613\u20131616. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "A statistical approach to mechanized encoding and searching of literary information", "author": ["H.P. Luhn"], "venue": "IBM Journal of research and development 1(4), 309\u2013317", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1957}, {"title": "The structure and function of complex networks", "author": ["M.E. Newman"], "venue": "SIAM review 45(2), 167\u2013256", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding and evaluating community structure in networks", "author": ["M.E. Newman", "M. Girvan"], "venue": "Physical review E 69(2), 026,113", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "The PageRank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Stanford InfoLab", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "Three partition refinement algorithms", "author": ["R. Paige", "R.E. Tarjan"], "venue": "SIAM Journal on Computing 16(6), 973\u2013989", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1987}, {"title": "Discovering concept coverings in ontologies of linked data sources", "author": ["R. Parundekar", "C.A. Knoblock", "J.L. Ambite"], "venue": "International Semantic Web Conference, pp. 427\u2013443. Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Representing web graphs", "author": ["S. Raghavan", "H. Garcia-Molina"], "venue": "Data Engineering, 2003. Proceedings. 19th International Conference on, pp. 405\u2013416. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J.D. Ullman"], "venue": "Cambridge University Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "An Efficient Metric of Automatic Weight Generation for Properties in Instance Matching Technique", "author": ["M.H. Seddiqui", "R.P.D. Nath", "M. Aono"], "venue": "International Journal of Web & Semantic Technology 6(1), 1", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Co-citation in the scientific literature: A new measure of the relationship between two documents", "author": ["H. Small"], "venue": "Journal of the American Society for information Science 24(4), 265\u2013269", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1973}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K. Sparck Jones"], "venue": "Journal of documentation 28(1), 11\u201321", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1972}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "VLDB11", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient aggregation for graph summarization", "author": ["Y. Tian", "R.A. Hankins", "J.M. Patel"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 567\u2013580. ACM", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Structure index for rdf data", "author": ["T. Tran", "G. Ladwig"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Top-k exploration of query candidates for efficient keyword search on graph-shaped (rdf) data", "author": ["T. Tran", "H. Wang", "S. Rudolph", "P. Cimiano"], "venue": "Data Engineering, 2009. ICDE\u201909. IEEE 25th International Conference on, pp. 405\u2013416. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Scan: a structural clustering algorithm for networks", "author": ["X. Xu", "N. Yuruk", "Z. Feng", "T.A. Schweiger"], "venue": "Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 824\u2013833. ACM", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "Discovery-driven graph summarization", "author": ["N. Zhang", "Y. Tian", "J.M. Patel"], "venue": "2010 IEEE 26th International Conference on Data Engineering (ICDE 2010), pp. 880\u2013891. IEEE", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "The Linking Open Data[6] and similar community projects have recommended the publication of large amount of globally useful datasets in machine-readable forms.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "Resource Description Framework (RDF), along with other forms of semantic data in forms of RDFa [1] and microformats [22] in web pages has expanded.", "startOffset": 95, "endOffset": 98}, {"referenceID": 20, "context": "Resource Description Framework (RDF), along with other forms of semantic data in forms of RDFa [1] and microformats [22] in web pages has expanded.", "startOffset": 116, "endOffset": 120}, {"referenceID": 4, "context": "We call this structure as the Summary Graph [5,4].", "startOffset": 44, "endOffset": 49}, {"referenceID": 3, "context": "We call this structure as the Summary Graph [5,4].", "startOffset": 44, "endOffset": 49}, {"referenceID": 39, "context": "For instance, the semantic search approach proposed by [41] uses a summary graph structure in the search mechanism.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "(2) Another way to obtain the summary graph is to locate the type triples (rdf:type) in the dataset and to organize the type classes and relations accordingly, if the data set is published using a standard vocabulary [14].", "startOffset": 217, "endOffset": 221}, {"referenceID": 30, "context": "Furthermore, it can potentially be inconsistent or erroneous in some cases [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "RDF statements describe resources in the form of triples, consisting of subject-predicate-object expressions that describe a resource, the type of a resource (type triple), or a relationship between two resources [12].", "startOffset": 213, "endOffset": 217}, {"referenceID": 4, "context": "We then developed a framework for building a summary graph structure in RDF data [5,4].", "startOffset": 81, "endOffset": 86}, {"referenceID": 3, "context": "We then developed a framework for building a summary graph structure in RDF data [5,4].", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "This current study extends our summary graph generation approach [5] and enhances it by incorporating a measure to help discover optimum class dissimilarity thresholds and an effective method to discover the type classes automatically.", "startOffset": 65, "endOffset": 68}, {"referenceID": 25, "context": "[27,37], which is a widely known technique in information retrieval.", "startOffset": 0, "endOffset": 7}, {"referenceID": 35, "context": "[27,37], which is a widely known technique in information retrieval.", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "Said differently, the importance of a word in a document increases by its frequency in the document but its importance decreases by its frequency in the corpus [34].", "startOffset": 160, "endOffset": 164}, {"referenceID": 19, "context": "For entity similarity metric, we employ an efficient graph node pair similarity metric, which utilizes the graph localities and neighborhood similarity within RoleSim similarity [21] in conjunction with the Jaccard measure context [19].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "For entity similarity metric, we employ an efficient graph node pair similarity metric, which utilizes the graph localities and neighborhood similarity within RoleSim similarity [21] in conjunction with the Jaccard measure context [19].", "startOffset": 231, "endOffset": 235}, {"referenceID": 17, "context": "Jaccard similarity is simply defined as the size of the intersection divided by the size of the union of the sample sets [19].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "Given a regular unlabeled graph G = (V,E), RoleSim measures the similarity of each node pair in V based on their neighborhood similarities [21]:", "startOffset": 139, "endOffset": 143}, {"referenceID": 28, "context": "It is a similar approach to the PageRank algorithm [30].", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "where the term frequency (tf) [27] represents the frequency of a proposition p with respect to a graph subject node u.", "startOffset": 30, "endOffset": 34}, {"referenceID": 35, "context": "The inverse document frequency (idf) [37] represents the frequency of a property usage across all graph nodes, and it is defined as", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "Predicate Stability (CPS), which is a similar notion to the concept of stability that introduced by Paige and Tarjan [31].", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "For this purpose, we utilize the root-mean-square deviation (RMSD), which is a commonly used measure of the differences between the values in comparison [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 2, "context": "In the evaluations, we conducted preliminary experiments on three datasets: a subset of DBpedia [3]; a subset of SemanticDB [13], and a subset of Lehigh University Benchmark (LUBM) [17].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "In the evaluations, we conducted preliminary experiments on three datasets: a subset of DBpedia [3]; a subset of SemanticDB [13], and a subset of Lehigh University Benchmark (LUBM) [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "In the evaluations, we conducted preliminary experiments on three datasets: a subset of DBpedia [3]; a subset of SemanticDB [13], and a subset of Lehigh University Benchmark (LUBM) [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "Lastly, DBPedia a central source in the Linked Open Data Cloud [6] and is a commonly used general purpose dataset.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "For instance, as of today, the Linking Open Data[6] project already contains more than 30 billions triples.", "startOffset": 48, "endOffset": 51}, {"referenceID": 31, "context": "From the Graph Compression perspective, numerous approaches have explored the graph summarization problem with the aim of reducing the storage space of the large graph datasets [33,15,18,11].", "startOffset": 177, "endOffset": 190}, {"referenceID": 13, "context": "From the Graph Compression perspective, numerous approaches have explored the graph summarization problem with the aim of reducing the storage space of the large graph datasets [33,15,18,11].", "startOffset": 177, "endOffset": 190}, {"referenceID": 16, "context": "From the Graph Compression perspective, numerous approaches have explored the graph summarization problem with the aim of reducing the storage space of the large graph datasets [33,15,18,11].", "startOffset": 177, "endOffset": 190}, {"referenceID": 9, "context": "From the Graph Compression perspective, numerous approaches have explored the graph summarization problem with the aim of reducing the storage space of the large graph datasets [33,15,18,11].", "startOffset": 177, "endOffset": 190}, {"referenceID": 26, "context": "Several studies such as [28,10] have broadly investigated statistical methods to help understand the properties of large networks.", "startOffset": 24, "endOffset": 31}, {"referenceID": 8, "context": "Several studies such as [28,10] have broadly investigated statistical methods to help understand the properties of large networks.", "startOffset": 24, "endOffset": 31}, {"referenceID": 27, "context": "In the area of graph partitioning area, many methods have been introduced [29,42,39,43] to partition graph data into specific components.", "startOffset": 74, "endOffset": 87}, {"referenceID": 40, "context": "In the area of graph partitioning area, many methods have been introduced [29,42,39,43] to partition graph data into specific components.", "startOffset": 74, "endOffset": 87}, {"referenceID": 37, "context": "In the area of graph partitioning area, many methods have been introduced [29,42,39,43] to partition graph data into specific components.", "startOffset": 74, "endOffset": 87}, {"referenceID": 41, "context": "In the area of graph partitioning area, many methods have been introduced [29,42,39,43] to partition graph data into specific components.", "startOffset": 74, "endOffset": 87}, {"referenceID": 37, "context": "[39, 43] proposed an aggregation-based graph summarization utilizing graph node attributes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[39, 43] proposed an aggregation-based graph summarization utilizing graph node attributes.", "startOffset": 0, "endOffset": 8}, {"referenceID": 6, "context": "In the Semantic Web community, there has also been some related studies [8,23,40,5,16].", "startOffset": 72, "endOffset": 86}, {"referenceID": 21, "context": "In the Semantic Web community, there has also been some related studies [8,23,40,5,16].", "startOffset": 72, "endOffset": 86}, {"referenceID": 38, "context": "In the Semantic Web community, there has also been some related studies [8,23,40,5,16].", "startOffset": 72, "endOffset": 86}, {"referenceID": 4, "context": "In the Semantic Web community, there has also been some related studies [8,23,40,5,16].", "startOffset": 72, "endOffset": 86}, {"referenceID": 14, "context": "In the Semantic Web community, there has also been some related studies [8,23,40,5,16].", "startOffset": 72, "endOffset": 86}, {"referenceID": 6, "context": "The studies in [8] and [16] are query driven graph summarization methods.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "The studies in [8] and [16] are query driven graph summarization methods.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "The approaches using bisimulation [23,40] have a limitation to be applied in real-world datasets due to the exponential complexity of bisimulation.", "startOffset": 34, "endOffset": 41}, {"referenceID": 38, "context": "The approaches using bisimulation [23,40] have a limitation to be applied in real-world datasets due to the exponential complexity of bisimulation.", "startOffset": 34, "endOffset": 41}, {"referenceID": 18, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 99, "endOffset": 103}, {"referenceID": 1, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 115, "endOffset": 118}, {"referenceID": 23, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 143, "endOffset": 147}, {"referenceID": 36, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 157, "endOffset": 161}, {"referenceID": 34, "context": "Neighborhood-based similarity measures have been investigated by several studies including SimRank [20], SimRank++ [2], PageSim [25], MatchSim [26], PathSim [38], and Co-Citation [36].", "startOffset": 179, "endOffset": 183}, {"referenceID": 33, "context": "There are some studies that try to calculate the property weights and apply them in similarity calculations such as [35,9].", "startOffset": 116, "endOffset": 122}, {"referenceID": 7, "context": "There are some studies that try to calculate the property weights and apply them in similarity calculations such as [35,9].", "startOffset": 116, "endOffset": 122}, {"referenceID": 7, "context": "In [9], they determine the property weights using the distinct value based weight generation and assign higher weight to a property that references more distinct values.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "The continuing development of Semantic Web technologies and the increasing user adoption in the recent years have accelerated the progress incorporating explicit semantics with data on the Web. With the rapidly growing RDF (Resource Description Framework) data on the Semantic Web, processing large semantic graph data have become more challenging. Constructing a summary graph structure from the raw RDF can help obtain semantic type relations and reduce the computational complexity for graph processing purposes. In this paper, we addressed the problem of graph summarization in RDF graphs, and we proposed an approach for building summary graph structures automatically from RDF graph data. Moreover, we introduced a measure to help discover optimum class dissimilarity thresholds and an effective method to discover the type classes automatically. In future work, we plan to investigate further improvement options on the scalability of the proposed method.", "creator": "LaTeX with hyperref package"}}}