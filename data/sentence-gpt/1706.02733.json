{"id": "1706.02733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Climbing a shaky ladder: Better adaptive risk estimation", "abstract": "We revisit the \\emph{leaderboard problem} introduced by Blum and Hardt (2015) in an effort to reduce overfitting in machine learning benchmarks. We show that a randomized version of their Ladder algorithm achieves leaderboard error O(1/n^{0.4}) compared with the previous best rate of O(1/n^{1/3}) for the time series, but the L-level approach does not have an o(1/n^{1/3}) for the time series. This change in O(1/n^{1/3}) compares with the L-level solution in the L-level model (see Table S5), but this change in O(1/n^{1/3}) is not statistically significant.\n\n\n\nThe following table summarizes the results of a regression with a logarithmic correlation that is computed after all the variance between the L-level solution and the final algorithm. We present the L-level algorithm as the primary choice in our test. The linear regression model performs an L-level regression for each of our two tests by generating a logarithmic linear regression, and then generating the O(1/n^{1/3}) linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two test trials by generating a logarithmic linear regression for each of our two", "histories": [["v1", "Thu, 8 Jun 2017 18:48:38 GMT  (57kb,D)", "http://arxiv.org/abs/1706.02733v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["moritz hardt"], "accepted": false, "id": "1706.02733"}, "pdf": {"name": "1706.02733.pdf", "metadata": {"source": "CRF", "title": "Climbing a shaky ladder: Better adaptive risk estimation", "authors": ["Moritz Hardt"], "emails": [], "sections": [{"heading": null, "text": "Short of proving that our algorithm is optimal, we point out a major obstacle toward further progress. Specifically, any improvement to our upper bound would lead to asymptotic improvements in the general adaptive estimation setting as have remained elusive in recent years. This connection also directly leads to lower bounds for specific classes of algorithms. In particular, we exhibit a new attack on the leaderboard algorithm that both theoretically and empirically distinguishes between our algorithm and previous leaderboard algorithms."}, {"heading": "1 Introduction", "text": "Machine learning benchmarks across industry and science are largely based on the simple mechanism of a holdout set. Participants repeatedly evaluate their models on the holdout set and use the feedback to improve their models. This feedback loop has become the de facto experimental paradigm in machine learning. What is concerning is that the analyst uses the holdout in a sequential and adaptive manner, thus creating dependencies between the model to be evaluated and the holdout data. The lack of independence between model and holdout data is what invalidates classical confidence bounds for the holdout setting. This insight was articulated in sequence of papers on what is now called adaptive data analysis [DFH+1, HU, DFH+2]. In a general formulation, adaptive data analysis can be thought of as an interaction between an algorithm that holds the sample, and an analyst that repeatedly asks queries about the data, such as \u201cWhat is the loss of this model on the underlying population?\u201d\nIn its general formulation, adaptive data analysis runs into strong computational lower bounds. Under computational hardness assumptions, no computationally efficient algorithm working with n samples can preserve even mild statistical validity on more than n2 queries [HU, SU]. This stands in sharp contrast to the non-adaptive setting where the error bounds deteriorate logarithmically with the number of queries k.\nCircumventing these lower bounds, Blum and Hardt [BH] introduced a simpler setting that allowed for much better guarantees. The key idea is that oftentimes it\u2019s sufficient to find the best model out of a sequence of adaptively chosen models, or to keep a ranking of some of the models. This is the relevant task in machine learning benchmarks, competitions, and hyperparameter tuning. Even adaptive early stopping can be posed as an instance of this problem. Within this\nar X\niv :1\n70 6.\n02 73\n3v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\nframework, there\u2019s a particularly simple and efficient algorithm called the Ladder algorithm. The algorithm maintains an internal threshold. Whenever a given model exceeds the previous quality threshold by a significant amount, the algorithm updates the threshold and provides the analyst with feedback about the quality of the model. If the model did not exceed the threshold, the analyst receives no feedback at all.\nThe Ladder algorithm maintains the risk of the best model (with respect to a bounded loss function) on a sequence of k adaptively chosen models up to an additive error ofO(log(kn)1/3/n1/3). This type of guarantee is called leaderboard error, since it does not require an accurate estimate for all models, but only the best performing one at any point in time. While this bound features a logarithmic dependence on k, the rate in terms of n falls short of the non-adaptive bound O( \u221a log(k)/n)."}, {"heading": "1.1 Our contributions", "text": "We narrow the gap between existing upper and lower bounds. Our first result is a randomized variant of the Ladder algorithm, called Shaky Ladder that achieves leaderboard error O(1/n0.4).\nTheorem 1.1 (Informal version of Theorem 2.7). On n samples and k adaptively chosen models, the Shaky Ladder achieves with high probability leaderboard error\nO\n( log(k)2/5 log(kn)1/5\nn2/5\n) .\nThe algorithm is based on analyzing noise addition via differential privacy, in particular, the so-called sparse vector technique as described in [DR]. We combine this analysis with powerful adaptive generalization bounds for differential privacy, where it is important to use the recently improved bound of Bassily et al. [BNS+]. The earlier bound due to Dwork et al. [DFH+1] would not suffice to give any improvement over the Ladder algorithm that achieved leaderboard error O(log(kn)1/3/n1/3).\nOur upper bound falls short of the information-theoretic lower bound of \u2126( \u221a\nlog(k)/n) that holds even in the non-adaptive estimation setting. Intuition from online learning and the literature on bandit algorithms suggest that either the exponent 1/3 or the exponent 1/2 could be a natural answer. Surprisingly, our result shows that a natural algorithm achieves the unusual rate of 1/n0.4. Moreover, we show that going beyond this rate will likely require powerful new techniques.\nIn order to make this point, we develop a new connection between leaderboard and the general adaptive estimation setting. Specifically, we show that any accurate leaderboard algorithm for sufficiently many queries readily implies a general adaptive estimator (formally introduced in Section 3) for a smaller number of queries.\nTheorem 1.2 (Informal version of Theorem 3.3). Suppose there exists a leaderboard algorithm A that is (\u03b1/2)-accurate on n samples and 1/\u03b12 models. Then, there exists a general adaptive estimator B that is \u03b1-accurate on k = 1/3\u03b1 queries.\nIn the regime where k 6 n, the best current upper bound is \u03b1 = O\u0303(k1/4/ \u221a n). For k = n0.4, this bound simplifies to O\u0303(1/n0.4) and thus coincides with what would follow from our theorem. This is no coincidence since the bounds are proved using the same techniques. What is new, however, is that any further improvement in leaderboard accuracy over our result would directly improve on the best known bounds in the general adaptive estimation setting. In particular, a\nleaderboard upper bound of O( \u221a\nlog(k)/n), as is currently not ruled out, would lead to a general adaptive estimator for nearly \u221a n queries and accuracy O\u0303(1/ \u221a n). Going to the natural statistical rate of O(1/ \u221a n) has remained elusive in the general adaptive estimation setting for any k > nc with c > 0. What our result shows is that this task is no easier in the leaderboard setting. It\u2019s worth noting that there are lower bounds in special cases, e.g., [RZ, WLF].\nWe use Theorem 3.3 to prove a lower bound against a natural class of leaderboard algorithms that we call faithful. Intuitively, speaking when faithful algorithms return feedback, the feedback is close to the empirical risk of the submitted model with high probability. This class of algorithms includes both the Ladder algorithm and it\u2019s heuristic counterpart the parameter-free Ladder. While those algorithms are deterministic, faithful algorithms may also be randomized.\nTheorem 1.3 (Informal version of Corollary 3.12). No faithful algorithm can achieve leaderboard error o(n\u22121/3).\nIn particular, this theorem separates our algorithm from earlier work. In Section 4, we illustrate this separation with a practical attack that causes a major bias in the Ladder algorithm, while being ineffective against our algorithm.\nBeyond the related work already discussed, Neto et al. [NHB+] proposed a number of heuristic leaderboard algorithms based on the idea of replacing the holdout estimate by Bootstrap estimates. In practice, this results in noise addition that can be helpful. However, these algorithms do not come with theoretical bound on the leaderboard error better than the Ladder."}, {"heading": "1.2 Preliminaries", "text": "Let X be a data domain and Y be a finite set of class labels, e.g., X = Rd and Y = {0,1}. A loss function is a mapping ` : Y \u00d7 Y \u2192 [0,1] and a model is a mapping f : X \u2192 Y . A standard loss function is the 0/1-loss defined as `01(y,y\u2032) = 1 if y , y\u2032 and 0 otherwise. Throughout this paper we assume that ` is a loss function with bounded range. We assume that we are given a sample S = {(x1, y1), . . . , (xn, yn)} drawn i.i.d. from an unknown distribution D over X \u00d7 Y . The risk of a model f is defined as its expected loss on the unknown distribution RD(f ) def= E(x,y)\u223cD [`(f (x), y))] . The empirical risk is the standard way of estimating risk from a sample. RS(f ) def= 1n \u2211n i=1 `(f (xi), yi) .\nAdaptive risk estimation. Given a sequence of models f1, . . . , fk and a finite sample S of size n, a fundamental estimation problem is to compute estimates R1, . . . ,Rk of the risk of each model. Classically, this is done via the empirical risk. Applying Hoeffding\u2019s bound to each empirical risk estimate, and taking a union bound over all functions, reveals that the largest deviation of any such estimate is bounded by O( \u221a log(k)/n). This is the estimation error we expect to see in the standard non-adaptive setting. In the adaptive estimation setting, we assume that the model ft may be chosen by an analyst as a function of previously observed estimates and previously chosen models. Formally, there exists a mappingA such that for all t \u2208 [k], the mappingA returns a function ft =A(f1,R1, . . . , ft\u22121,Rt\u22121) from all previously observed information. We will assume for simplicity that the analyst A is a deterministic algorithm. The tuple (f1,R1, . . . , ft\u22121,Rt\u22121) is nevertheless a random variable due to the random sample used to compute the estimates, as well possibly additional randomness introduced in the estimates. A natural notion of estimation error in the adaptive setting is the maximum error of any of the estimates, i.e., max16t6k |RD(fi)\u2212Rt | . Unfortunately, lower\nbounds [HU, SU] show that no computationally efficient estimator can achieve maximum error o(1) on more than n2+o(1) adaptively chosen functions (under a standard hardness assumption).\nLeaderboard error. Blum and Hardt [BH] introduced a weaker notion of estimation error called leaderboard error. Informally speaking, leaderboard error asks us to maintain a good estimate of the best (lowest risk) model seen so far, but does not require an accurate estimate for all models that we encounter.\nDefinition 1.4 (Leaderboard error). Given an adaptively chosen sequence of models f1, . . . , fk , we define the leaderboard error of estimates R1, . . . ,Rk as\nlberr(R1, . . . ,Rk) def= max16t6k \u2223\u2223\u2223min16i6tRD(fi)\u2212Rt\u2223\u2223\u2223 (1)"}, {"heading": "2 The Shaky Ladder algorithm", "text": "We introduce an algorithm called Shaky Ladder that achieves small leaderboard accuracy. The algorithm is very simple. For each given function, it compares the empirical risk of the function to the previously smallest empirical risk plus some noise variables. If the estimate is below the previous best by some margin, it releases the estimate plus noise and updates the best estimate. Importantly, if the estimate is not smaller by a margin, the algorithm releases the previous best risk (rather than the new estimate). A formal description follows in Figure 1. For simplicity we assume we know an upper bound k on the total number of rounds.\nParameter settings. We introduce a new parameter \u03b2 > 0 for the failure probability of our algorithm. For the purpose of our analysis we fix the parameters as follows:\n\u03b4 = \u03b2\nkn \u03b5 =  log(k/\u03b2)\u221alog(1/\u03b4)n 3/5 \u03bb = 4log(4k/\u03b2)\u03c3 (2)\nWith these settings all parameters are frozen with the one expection of \u03b2. The settings are optimized to prove the theorem, and do not necessarily reflect a good choice for practical settings. We will revisit this question in a later section.\nFrom here on we let B denote the number of update rounds of the algorithm:\nB = |{t > 1: Rt < Rt\u22121}| . (3)\nWe can quantify the privacy guarantee of the algorithm in terms of this parameter. Lemma 2.1. Algorithm 1 is (\u03b5 \u221a B,O(\u03b4))-differentially private.\nProof. For the purpose of its privacy analysis, the algorithm is equivalent to the algorithm \u201cNumericSparse\u201d in [DR] whose guarantees follow from the sparse vector technique. The only difference in our algorithm is that the threshold at each step varies. This difference is irrelevant for the privacy analysis, since only the parameter B matters.\nSince \u03b5 and \u03b4 are related multiplicatively through \u03c3, we can absorb all constant factors appearing in the analysis of \u201cNumericSparse\u201d in the O(\u03b4)-term.\nOur goal is to invoke a \u201ctransfer theorem\u201d that translates the privacy guarantee of the algorithm into a generalization bound for the adaptive setting. The following theorem due to Bassily et al. [BNS+] intuitively shows that an (\u03b5,\u03b4)-differentially private algorithm is unable to find a function that generalizes poorly.\nTheorem 2.2 (Theorem 7.2 in [BNS+]). Let \u03b5 \u2208 (0,1/3),\u03b4 \u2208 (0, \u03b5/4), and n > 1\u03b52 log(4\u03b5/\u03b4). LetM be an (\u03b5,\u03b4)-differentially private algorithm that, on input of a sample S of size n drawn i.i.d. from the population D, returns a function f : X\u2192 [0,1]. Then,\nPr S,M {|RS(f )\u2212R(f )| > 18\u03b5} < \u03b4 \u03b5 .\nThe original theorem is stated slightly differently. This version follows from the fact that the empirical risk with respect to a bounded loss function has \u201csensitivity\u201d 1/n in the terminology of [BNS+].\nRelevant to us is the following corollary.\nCorollary 2.3. Let f1, . . . , fk be the functions encountered by the Shaky Ladder algorithm (Figure 1). Then, taking probability over both the sample S and the randomness of the algorithm, we have\nPr {\nmax 16t6k\n|RS(ft)\u2212R(ft)| > 18\u03b5 \u221a B } < O ( k\u03b4 \u03b5 ) .\nProof. Let \u03b5\u2032 = 18\u03b5 \u221a B and \u03b4\u2032 = O(k\u03b4/\u03b5). To apply Theorem 2.2 we need to observe that the composition of the Shaky Ladder algorithm with an arbitrary analyst (who does not otherwise have access to the sample S) satifies (\u03b5\u2032 ,\u03b4\u2032)-differential privacy at every step of the algorithm. Hence, every function ft is generated by an (\u03b5\u2032 ,\u03b4\u2032)-differentially private algorithm so that the theorem applies. The corollary now follows from a union bound over all k functions.\nLemma 2.4. Let L1, . . . ,L3k+1 be all the Laplacian variables generated by our algorithm and consider the maximum absolute value L = max16i6k\u2032 |Li |. Then,\nPr { lberr(R1, . . . ,Rk) > 18\u03b5 \u221a B+\u03bb+ 2L } 6O ( k\u03b4 \u03b5 )\nProof. In the comparison step of the algorithm at step t, note that\nRS(ft) + \u03bet +\u03bb+ \u03be = R(ft) + e,\nwhere |e| 6 18\u03b5 \u221a B+\u03bb+ 2L. Here we used Corollary 2.3, as well as our bound on the Laplacian random variables. Similarly, if we update Rt at step t, we have that\n|Rt \u2212R(ft)| 6 18\u03b5 \u221a B+L.\nHence, we can think of our algorithm as observing the population risk of each classifier up to the specified error bound. This implies, by induction, that the estimates achieve the specified leaderboard error.\nWe have the following tail bound for the quantity L that appeared in Lemma 2.4.\nLemma 2.5. For every \u03b2 > 0, Pr {L > log(4k/\u03b2)\u03c3 } 6 \u03b2 . Proof. Note that L is the maximum of at most 4k centered Laplacian random variables with standard deviation \u03c3. For a single such random variable, we have\nPr{|Lap(\u03c3 )| > t\u03c3 } = 2 \u222b \u221e t\u03c3 1 2\u03c3 exp(\u2212r/\u03c3 )dr = \u222b \u221e t exp(\u2212u)du = exp(\u2212t) .\nThe claim now follows by applying this bound with t = log(4k/\u03b2) and taking a union bound over all 3k + 1 6 4k Laplacian variables which L is the maximum of.\nWe also need to bound the number of update steps B. This is easy to do assuming we have a bound on L.\nLemma 2.6. Pr {B 6 4/\u03bb | L 6 \u03bb/4} = 1. Proof. Assume that L 6 \u03bb/4. This implies that whenever t satisfies\nRS(ft) + \u03bet < Rt\u22121 \u2212\u03bb+ \u03be, (4) we must also have RS(ft) < Rt\u22121 \u2212 \u03bb/2. Since Rt = RS(ft) + \u03be \u2032t , we also have Rt < RS(ft) + \u03bb/4. Therefore, Rt < Rt\u22121\u2212\u03bb/4. In particular, we can have at most 4/\u03bb rounds t for which the event (4) occurs.\nTheorem 2.7. There is a constant C > 0 such that with suitably chosen parameter settings the Shaky Ladder algorithm (Figure 1) satisfies for any sequence of adaptively chosen classifiers f1, . . . , fk ,\nPr { lberr(R1, . . . ,Rk) > C \u00b7 log(k/\u03b2)2/5 log(kn/\u03b2)1/5\nn2/5\n} 6 \u03b2 .\nProof. Consider the event G that simultaneously L 6 log(4k/\u03b2)\u03c3, and lberr(R1, . . . ,Rk) 6 18\u03b5 \u221a B+ \u03bb+ 2L. Invoking our tail bounds from Lemma 2.5 and Lemma 2.4, we have that"}, {"heading": "Pr {G} > 1\u2212O(k\u03b4/\u03b5)\u2212 \u03b2 > 1\u2212O(\u03b2) .", "text": "Here we used the definition of \u03b4 and the fact that \u03b5 > 1/n.\nProceeding under the condition that G occurs, we can plug in our parameter settings from Equation 2 to verify that\nlberr(R1, . . . ,Rk) 6 18\u03b5 \u221a B+\u03bb+ 2L 6O\n( log(k/\u03b2)2/5 log(kn/\u03b2)1/5\nn2/5\n) .\nRescaling \u03b2 to eliminate the constant in front of the error probability bound establishes the bound claimed in the theorem."}, {"heading": "3 Connection to general adaptive estimation", "text": "In the general adaptive estimation setting, the adaptive analyst choose a sequence of bounded functions g1, . . . , gk : X\u2192 [0,1] usually called queries. The algorithm must return estimates a1, . . . , ak in an online fashion such that each estimate ak is close to the population expectation ED gk . We will refer to algorithms in this setting as general adaptive estimators to distinguish them from leaderboard algorithms that we studied earlier. The following definition of accuracy is common in the literature.\nDefinition 3.1. We say that a general adaptive estimator B is (\u03b1,\u03b2)-accurate on n samples and k queries if for every distribution over X, given n samples from the distribution and adaptively chosen queries g1, . . . , gk : X\u2192 [0,1], the algorithm B returns estimates a1, . . . , ak such that Pr {max16t6k |ED gt \u2212 at | 6 \u03b1} > 1\u2212 \u03b2 .\nTo bear out the connection with the leaderboard setting, we introduce an analogous definition for leaderboard error.\nDefinition 3.2. We say that a leaderboard algorithm A is (\u03b1,\u03b2)-accurate on n samples and k classifiers if for every distribution over X \u00d7 Y and every bounded loss function, given n samples and adaptively chosen sequence of classifiers f1, . . . , fk : X\u2192 Y , the algorithm A returns estimates R1, . . . ,Rk such that Pr {lberr(R1, . . . ,Rk) 6 \u03b1} > 1\u2212 \u03b2 .\nGiven these definition, we can show a reduction from designing general adaptive estimators to designing leaderboard algorithms in the regime where the number of queries k is small.\nTheorem 3.3. Suppose there exists a leaderboard algorithm A that is (\u03b1/2,\u03b2)-accurate on n samples and 1/\u03b12 classifiers. Then, there exists a general adaptive estimator B that is (\u03b1,\u03b2)-accurate on k = 1/3\u03b1 queries. Moreover if A is computationally efficient, then so is B.\nProof. Assume the existence ofA and construct B as follows. LetD be the distribution over X for which B needs to be accurate. Take the range Y = [0,1] and let the loss function be `(y,y\u2032) = y. With this loss function, we can think of a query g : X \u2192 [0,1] as a classifier that satisfies RD(g) = ED g.\nAt each step 1 6 t 6 k, the algorithm B receives a query gt from an adaptive analyst and has to use the algorithm A to answer the query. The algorithm B is described in Figure 2. Note that all functions constructed in this procedure range in [0,1].\nOur first claim shows that if A has small leaderboard error, then the answers extracted from the above procedure are accurate.\nClaim 3.4. If A has leaderboard error \u03b1/2, then |at \u2212ED gt | 6 \u03b1.\nProof. First note that by construction\nR(ft,i) = c \u2212 i\u03b1 2 + 1 2 E D gt .\nBy the definition of leaderboard error and our assumption, if R(ft,i) < c \u2212 \u03b1/2, the algorithm A must output a value rt,i that is lower than c and moreover satisfies |rt,i \u2212 R(ft,i)| 6 \u03b1/3. By definition, rt,i = at/2 + c \u2212 i\u03b1/2 and therefore,\nrt,i \u2212R(ft,i) = at 2 \u2212 ED gt 2 .\nHence, \u2223\u2223\u2223\u2223\u2223at \u2212ED gt \u2223\u2223\u2223\u2223\u2223 6 \u03b1.\nOur second claim ensures that we don\u2019t lower the threshold c too quickly, thus allowing B to answer sufficiently many queries.\nClaim 3.5. If A has leaderboard error \u03b1/2, then the procedure we run for each function gt lowers the threshold c by at most 3\u03b1/2.\nProof. Observe that R(ft,i+1) > R(ft,i) \u2212 \u03b1/2. In other words, the difference in risk of any two consecutive classifiers is bounded by \u03b1/2. Hence, rt,i+1 > rt,i \u2212 3/\u03b1/2. Therefore, the threshold c can decrease by at most 3\u03b1/2.\nAssuming A has leaderboard error \u03b1/2, the previous claim implies that the algorithm B can use the algorithm A for up to k\u2032 = 1/3\u03b1 queries before the threshold c reaches 0. The total number of classifiers that B gives to A is bounded by 1/\u03b12.\nIt is natural to ask if the converse of the theorem is also true. Ideally, we would like to have a result showing that a general adaptive estimator for few queries implies a leaderboard algorithm for many queries. However, at this level of generality it is not clear why there should be such an argument to amplify the number of queries. Of course, by definition, we can say that a general adaptive estimator for k queries implies a leaderboard algorithm for k queries with the same accuracy."}, {"heading": "3.1 Lower bounds for faithful algorithms", "text": "In this section, we prove a lower bound on a natural class of leaderboard algorithms that we call faithful. It includes both of the algorithms proposed by Blum and Hardt, the Ladder and the parameter-free Ladder algorithm. Both of these algorithms are deterministic, but the class of faithful algorithms also includes many natural randomization schemes.\nDefinition 3.6. A leaderboard algorithm is faithful if given a sample S of size n for every adaptively chosen sequence of models f1, . . . , fk its estimates (R1, . . . ,Rk) satisfy with probability 2/3 for all 1 < t 6 k such that Rt < Rt\u22121, we also have |Rt \u2212RS(ft)| 6 12\u221an\nIn words, given that the algorithm updated its estimate, i.e., Rt < Rt\u22121, the new estimate is likely close to the empirical risk of the t-th model. The constants in the definition are somewhat arbitrary. Other choices are possible. What matters is that the algorithm returns something close to the empirical risk with reasonably high probability whenever it gives feedback at all.\nTo prove a lower bound against faithful algorithms, we will invoke our connection with the general estimation setting.\nDefinition 3.7. A general adaptive estimator is faithful if given a sample S of size n for every sequence of adaptively chosen function g1, . . . , gk its estimates (a1, . . . , ak) satisfy with probability 2/3, \u2200t : \u2223\u2223\u2223at \u2212 1n\u2211x\u2208S gt(x)\u2223\u2223\u2223 6 12\u221an . The reduction we saw earlier preserves faithfulness.\nLemma 3.8. IfA is a faithful leaderboard algorithm, then the algorithm B resulting from the reduction in Figure 2 is a faithful general adaptive estimator.\nWe can therefore obtain a lower bound on faithful leaderboard algorithms by proving one against faithful general adaptive estimators. Theorem 3.9. No faithful general adaptive estimator is (o( \u221a k/n),1/4)-accurate on n samples and k 6 n queries.\nProof. Set up the distribution D over X \u00d7 Y with the label set Y = {0,1} such that the label y is uniformly random conditional on any instance x \u2208 X. Fix a general adaptive estimator B that gets a sample S of size n drawn from D. We need to show that the estimator B cannot be (o( \u221a k/n),1/4)-accurate. To show this claim we will analyze the following procedure (majority attack):\n\u2013 Pick k 6 n random functions f1, . . . , fk : X\u2192 {0,1}.\n\u2013 Let ai = RS(fi) be the empirical risk of fi with respect to the 0/1-loss. Further, let a\u0302i be the answer from the general adaptive estimator on the query gi(x,y) = I {fi(x) , y} .\n\u2013 Consider the index set I = { i : a\u0302i < 1/2\u2212 1/ \u221a n } .\n\u2013 Let f = maji\u2208Ifi be the pointwise majority function of all functions in I . That is f (x) is the majority value among fi(x) with i \u2208 I.\n\u2013 Ask B to estimate the 0/1-loss of f , i.e., submit the query g\u2217(x,y) = I {f (x) , y} .\nNote that Ex,y\u223cD g\u2217(x,y) = R(f ) and hence it remains to analyze the difference between the risk and empirical risk of f .\nClaim 3.10. R(f ) = 1/2.\nProof. This is true for any function f : X \u2192 Y given the way we chose the distribution over X \u00d7Y .\nWe claim that the empirical risk is bounded away from 1/2 by \u2126( \u221a k/n) with constant\nprobability. A similar claim appeared in [BH] without proof.\nClaim 3.11. Assume k 6 n. Then, with probability 1/3, RS(f ) 6 1/2\u2212\u2126 (\u221a k/n ) \u2212O ( 1/ \u221a n ) .\nProof. Following Definition 3.7, condition on the event that for all t \u2208 [k], we have |at \u2212 a\u0302t | 6 1/2 \u221a n. By the definition, this even occurs with probability 2/3. Under this condition all i \u2208 I satisfy ai < 1/2\u22121/2 \u221a n. Furthermore, we claim that |I | >\u2126(k) with probability 2/3. This follows\nbecause Pr{ai < 1/2\u22121/ \u221a n} = \u2126(1). In particular both events occur with probability at least 1/3. Let \u03b5i = Pr\n(x,y)\u2208S {fi(x) = y} \u2212 1/2\nbe the advantage over random of gi in correctly labeling an element of S. By definition of \u03b5i , we must have that \u03b5i > 1/2 \u221a n for all i \u2208 I. We will argue that this advantage over random is amplified by the majority vote. Let Zi be the indicator of the event that fi(x) = y for random (x,y) \u2208 S. For ease of notation rearrange indices such that I = {1,2, . . . ,m}, where m = \u2126(k) as argued earlier. We know that Zi is Bernoulli with parameter 1/2 + \u03b5i where by construction \u03b5i > 1/2 \u221a n. Let Z be the indicator of\nthe event that f (x) = y. Let \u03b5 = 1/2 \u221a n and observe that \u03b5 6 1/ \u221a m since k 6 n. Therefore,\nPr{Z = 1} > 1 2 Pr  m\u2211 i=1 Zi > m/2  > Pr {Binomial(m,1/2 + \u03b5) > m/2}\n> 1 2\n+\u2126 (\u221a m\u03b5 ) \u2212O ( 1/ \u221a m )\n(Claim A.1, using \u03b5 < 1/ \u221a m)\n= 1 2\n+\u2126 (\u221a k/n ) \u2212O ( 1/ \u221a n ) .\nThe claim now follows, since RS(f ) = 1\u2212Pr{Z = 1}.\nTaking Claim 3.10 and Claim 3.11 together, we have that R(f )\u2212RS(f ) >\u2126(k/n)\u2212O(1/ \u221a n), with probability 1/3. In particular, when k = \u03c9(1), this shows that the estimator B is not (o( \u221a k/n),1/4)-accurate. For k =O(1), the same claim follows from a standard variance calculation.\nThe previous theorem implies that faithful leaderboard algorithms cannot have leaderboard error better than n1/3.\nCorollary 3.12. No faithful leaderboard algorithm is (\u03b1,\u03b2)-accurate on n samples and k queries for any \u03b1 = ko(1)/n1/3\u2212c, \u03b2 = 1\u2212 o(1) and constant c > 0.\nProof. Combine our lower bound from Theorem 3.9 with the reduction in Theorem 3.3. By Lemma 3.8, faithfulness is preserved and hence we get the stated lower bound."}, {"heading": "4 Experiments with a shifted majority attack", "text": "The attack implicit in Corollary 3.12 corresponds to what we will call the shifted majority attack. To understand the idea, we briefly review the Boosting attack from [BH]. In this procedure, the analyst first asks k random queries (thought of as vectors in {0,1}n, one binary label for each point in the holdout set), and then selects the ones that have error (0/1-loss) less than 1/2. Note that the expected loss is 1/2. Among these selected queries, the analyst computes a coordinatewise majority vote, resulting in a final output vector y\u0302 \u2208 {0,1}n. Blum and Hardt observed that this output vector has expected error 1/2 \u2212\u2126( \u221a k/n), with respect to the true holdout labels y \u2208 {0,1}n. Despite the fact that the vector setup is a slight simplification of the actual formal framework we have, this idea carries over to our setting by replacing random vectors with random functions. We will refer to this procedure as majority attack.\nThe majority attack has the property that when run against the Ladder algorithm, the analyst quickly stops receiving new feedback. Newly chosen random functions are increasingly unlikely to improve upon the error of previous functions. Our procedure in Figure 2, however, shows how to offset the queries in such a way that the analyst continues to receive as much feedback as possible from the algorithm. In theory, this requires knowledge about the underlying distribution (which is fine for the purpose of proving the theorem). In reality, we can imagine that there may be a subset of the domain on which the classification problem is easy so that the analyst knows a fraction of the labels with near certainty. The analyst can then use this \u201ceasy set\u201d to offset the functions as required by the attack. This leads to what we call the shifted majority attack.\nSetup. Rather than running the shifted majority attack, we will run the majority attack for a varying number of queries k. The reason for this setup is that there is no canonical parameter choice for the implementation of the Ladder algorithm, or the Shaky Ladder. In particular, the number of queries that can be answered using the shifting idea is closely related to the inverse of the step size parameter. It is therefore more transparent to leave the number of queries as a parameter that can be varied. Section B contains a reference implementation of the majority attack that we experiment with.\nThe primary purpose of our experiments is to understand in simulation the effect of adding noise to the feedback of the leaderboard algorithm.\nObservations. Figure 3 shows that even a small amount of Gaussian noise (e.g., standard deviation \u03c3 = 3/ \u221a n) mostly neutralizes the majority attack that is otherwise very effective against the standard Ladder algorithm. We note in passing that the parameter-free Ladder algorithm [BH] only reveals more feedback than the Ladder algorithm. As such it fares even more poorly than the Ladder algorithm under the shifted majority attack.\nFigure 4 consolidates the observation by showing the effect of varying noise levels. There appears to be a sweet spot at 3 standard deviations, where much of the harm of the shifted majority attack is neutralized, while the amount of noise added is still small as a function of n. In particular, in simulation it appears that less noise is necessary than our theorem suggests."}, {"heading": "5 Conclusion and open problems", "text": "We saw a new algorithm with leaderboard error O(n\u22120.4). This upper bound lies strictly between the two more natural bounds of O(n\u22121/3) and O(n\u22121/2). If experience from online and Bandit learning is any guide, the new upper bound might suggest that there is hope of attaining the tightO(n\u22121/2) error rate. This possibility is further supported by the fact that the majority attack we saw in Section 4 is quite sensitive to noise on the order ofO(n\u22121/2). This leads us to conjecture that O(n\u22121/2) might in fact be the right answer. However, in light of our connection between the general adaptive estimation setting and leaderboard error, such a conjecture can now be refuted by stronger lower bounds for the general adaptive estimation setting. It is unclear if more sophisticated lower bounding techniques based on Fingerprinting codes as used in [HU, SU] could be used to obtain stronger lower bounds in the small number of query regime (k n)."}, {"heading": "Acknowledgments", "text": "Many thanks to Avrim Blum, Yair Carmon, Roy Frostig, and Tomer Koren for insightful observations and suggestions at various stages of this work."}, {"heading": "A Anti-concentration inequality for the Binomial distribution", "text": "Claim A.1. Let 0 < \u03b5 6 1/ \u221a m. Then,\nPr {Binomial(m,1/2 + \u03b5) > m/2} > 1 2\n+\u2126 (\u221a m\u03b5 ) \u2212O ( 1/ \u221a m ) .\nProof. Put p = 1/2 + \u03b5 and q = 1 \u2212 p. On the one hand, for the given upper bound on \u03b5, the Berry-Esseen theorem implies the normal approximation\nPr {Binomial(m,1/2 + \u03b5) > m/2} > Pr {N(mp,mpq) > mp \u2212 \u03b5m} \u2212O ( 1/ \u221a m ) .\nOn the other hand,\nPr {N(mp,mpq) > mp \u2212 \u03b5m} = Pr { N(0,pq) > \u2212\u03b5 \u221a m } >\n1 2\n+\u2126 ( \u03b5 \u221a m ) .\nIn the last step, we used the our upper bound on \u03b5, which ensures that \u03b5 \u221a m 6 1. Noting that pq >\u2126(1), the last step now follows from the fact that the density of N(0,pq) is lower bounded by a constant in the interval [\u2212\u03b5 \u221a m,0].\nPutting the two observations together we get\nPr {Binomial(m,1/2 + \u03b5) > m/2} > 1 2\n+\u2126 ( \u03b5 \u221a m ) \u2212O ( 1/ \u221a m ) ."}, {"heading": "B Reference implementation for majority attack", "text": "For definedness, we include a reference implementation of the majority attack used in our experiments.\n1 import numpy as np 2 3 def majority_attack(n, k, sigma=None): 4 \"\"\"Run majority attack and report resulting bias.\"\"\" 5 hidden_vector = 2.0 * np.random.randint(0, 2, n) - 1.0 6 queries = 2.0 * np.random.randint(0, 2, (k, n)) - 1.0 7 answers = queries.dot(hidden_vector)/n 8 if sigma: 9 answers += np.random.normal(0, sigma, k)\n10 positives = queries[answers > 0., :] 11 negatives = queries[answers <= 0., :] 12 weighted = np.vstack([positives, -1.0*negatives]) 13 weights = weighted.T.dot(np.ones(k)) 14 final = np.ones(n) 15 final[weights < 0.] = -1.0 16 return np.mean(final != hidden_vector)"}], "references": [{"title": "The Ladder: A reliable leaderboard for machine learning competitions", "author": ["BH] Avrim Blum", "Moritz Hardt"], "venue": "In Proc. 32nd ICML,", "citeRegEx": "Blum and Hardt.,? \\Q2015\\E", "shortCiteRegEx": "Blum and Hardt.", "year": 2015}, {"title": "Algorithmic stability for adaptive data analysis", "author": ["BNS+] Raef Bassily", "Kobbi Nissim", "Adam D. Smith", "Thomas Steinke", "Uri Stemmer", "Jonathan Ullman"], "venue": "In Proc. 48th STOC,", "citeRegEx": "Bassily et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2016}, {"title": "Preserving validity in adaptive data analysis", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": "In Proc. 47th STOC,", "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "The reusable holdout: Preserving validity in adaptive data analysis", "author": ["Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth"], "venue": null, "citeRegEx": "Dwork et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2015}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "Dwork and Roth.,? \\Q2014\\E", "shortCiteRegEx": "Dwork and Roth.", "year": 2014}, {"title": "Preventing false discovery in interactive data analysis is hard", "author": ["HU] Moritz Hardt", "Jonathan Ullman"], "venue": "In Proc. 55th FOCS,", "citeRegEx": "Hardt and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Ullman.", "year": 2014}, {"title": "Reducing overfitting in challenge-based competitions", "author": ["Elias Chaibub Neto", "Bruce R Hoff", "Chris Bare", "Brian M Bot", "Thomas Yu", "Lara Magravite", "Andrew D Trister", "Thea Norman", "Pablo Meyer", "Julio Saez-Rodrigues", "James C Costello", "Justin Guinney", "Gustavo Stolovitzky"], "venue": null, "citeRegEx": "Neto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neto et al\\.", "year": 2016}, {"title": "Interactive fingerprinting codes and the hardness of preventing false discovery", "author": ["SU] Thomas Steinke", "Jonathan Ullman"], "venue": "CoRR, abs/1410.1228,", "citeRegEx": "Steinke and Ullman.,? \\Q2014\\E", "shortCiteRegEx": "Steinke and Ullman.", "year": 2014}, {"title": "A minimax theory for adaptive data analysis", "author": ["WLF] Yu-Xiang Wang", "Jing Lei", "Stephen E. Fienberg"], "venue": "CoRR, abs/1602.04287,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Abstract We revisit the leaderboard problem introduced by Blum and Hardt (2015) in an effort to reduce overfitting in machine learning benchmarks.", "startOffset": 58, "endOffset": 80}], "year": 2017, "abstractText": "We revisit the leaderboard problem introduced by Blum and Hardt (2015) in an effort to reduce overfitting in machine learning benchmarks. We show that a randomized version of their Ladder algorithm achieves leaderboard error O(1/n0.4) compared with the previous best rate of O(1/n1/3). Short of proving that our algorithm is optimal, we point out a major obstacle toward further progress. Specifically, any improvement to our upper bound would lead to asymptotic improvements in the general adaptive estimation setting as have remained elusive in recent years. This connection also directly leads to lower bounds for specific classes of algorithms. In particular, we exhibit a new attack on the leaderboard algorithm that both theoretically and empirically distinguishes between our algorithm and previous leaderboard algorithms.", "creator": "LaTeX with hyperref package"}}}