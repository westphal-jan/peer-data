{"id": "1706.03146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2017", "title": "Rethinking Skip-thought: A Neighborhood based Approach", "abstract": "We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks.\n\n\n\n\nWe have described three other aspects of skipping-thought neighbor models. The first is the use of a simple method that takes the form of a simple form of classification of items in a specific order, such as the items that are placed in the list (e.g. the \"item\" with \"top\" in the second item). The second is a form of classification that considers each item as related to the group in that list (e.g. a list that includes the item on the list with \"bottom\" in the first item), and we consider a single item (e.g. \"item\" with \"top\" in the second item), and we consider a single item (e.g. a list that includes the item on the list with \"bottom\" in the first item), and we consider a single item (e.g. a list that includes the item on the list with \"bottom\" in the second item).\n\n\n\nTo understand the types of items that are included in our study, we need to consider how we classify those items. For example, in our initial analysis we examined the word list, which is represented by the same words as each other (slightly different than the words listed in the text), to see whether we actually considered the word list. In contrast, in our earlier analysis we considered the word list, which is represented by the same words as each other (slightly different than the words listed in the text), to see if we actually considered the word list. The two main problems we found here are that one of the main problems with our first analysis is that many people use \"spicy\" words to describe items in their lives. It would be difficult to tell if the word list really describes them, but we found that there are more than 3,000 examples of people using \"spicy\" words in their lives than does the other 3,000.\nIn the first analysis we did not use words like \"spicy\" words, but instead of making use of \"spicy\" words, we chose to refer to the same words as each other. However, our third problem", "histories": [["v1", "Fri, 9 Jun 2017 22:39:31 GMT  (201kb,D)", "http://arxiv.org/abs/1706.03146v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["shuai tang", "hailin jin", "chen fang", "zhaowen wang", "virginia r de sa"], "accepted": false, "id": "1706.03146"}, "pdf": {"name": "1706.03146.pdf", "metadata": {"source": "CRF", "title": "Rethinking Skip-thought: A Neighborhood based Approach", "authors": ["Shuai Tang", "Hailin Jin", "Chen Fang", "Zhaowen Wang", "Virginia R. de Sa"], "emails": ["shuaitang93@ucsd.edu,", "desa@ucsd.edu,", "hljin@adobe.com", "cfang@adobe.com", "zhawang@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "We are interested in learning distributed sentence representation in an unsupervised fashion. Previously, the skip-thought model was introduced by Kiros et al. (2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder. The skip-thought model encodes the current sentence and then decodes the previous sentence and the next one, instead of itself. Two independent decoders were applied, since intuitively, the previous sentence and the next sentence should be drawn from 2 different conditional distributions, respectively.\nBy posing a hypothesis that the adjacent sentences provide the same neighborhood informa-\ntion for learning sentence representation, we first drop one of the 2 decoders, and use only one decoder to reconstruct the surrounding 2 sentences at the same time. The empirical results show that our skip-thought neighbor model performs as well as the skip-thought model on 7 evaluation tasks. Then, inspired by Hill et al. (2016), as they tested the effect of incorporating an autoencoder branch in their proposed FastSent model, we also conduct experiments to explore reconstructing the input sentence itself as well in both our skip-thought neighbor model and the skip-thought model. From the results, we can tell that our model didn\u2019t benefit from the autoencoder path, while reconstructing the input sentence hurts the performance of the skip-thought model. Furthermore, we conduct an interesting experiment on only decoding the next sentence without the previous sentence, and it gave us the best results among all the models. Model details will be discussed in Section 3."}, {"heading": "2 Related Work", "text": "Distributional sentence representation learning involves learning word representations and the compositionality of the words within the given sentence. Previously, Mikolov et al. (2013b) proposed a method for distributed representation learning for words by predicting surrounding words, and empirically showed that the additive composition of the learned word representations successfully captures contextual information of phrases and sentences. Similarly, Le and Mikolov (2014) proposed a method that learns a fixed-dimension vector for each sentence, by predicting the words within the given sentence. However, after training, the representation for a new sentence is hard to derive, since it requires optimizing the sentence representation towards an objective.\nUsing an RNN-based autoencoder for language\nar X\niv :1\n70 6.\n03 14\n6v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\n7\nrepresentation learning was proposed by Dai and Le (2015). The model combines an LSTM encoder, and an LSTM decoder to learn language representation in an unsupervised fashion on the supervised evaluation datasets, and then finetunes the LSTM encoder for supervised tasks on the same datasets. They successfully show that learning the word representation and the compositionality of words could be done at the same time in an end-to-end machine learning system.\nSince the RNN-based encoder processes an input sentence in the word order, it is obvious that the dependency of the representation on the starting words will decrease as the encoder processes more and more words. Tai et al. (2015) modified the plain LSTM network to a tree-structured LSTM network, which helps the model to address the long-term dependency problem. Other than modifying the network structure, additional supervision could also help. Bowman et al. (2016) proposed a model that learns to parse the sentence at the same time as the RNN is processing the input sentence. In the proposed model, the supervision comes from the objective function for the supervised tasks, and the parsed sentences, which means all training sentences need to be parsed prior to training. These two methods require additional preprocessing on the training data, which could be slow if we need to deal with a large corpus.\nInstead of learning to compose a sentence representation from the word representations, the skip-thought model Kiros et al. (2015) utilizes the structure and relationship of the adjacent sentences in the large unlabelled corpus. Inspired by the skip-gram model (Mikolov et al., 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently. The skip-thought model provides an alternative way for unsupervised sentence representation learning, and has shown great success. The learned sentence representation encoder outperforms previous unsupervised pretrained models on 8 evaluation tasks with no finetuning, and the results are comparable to supervised trained models. In Triantafillou et al. (2016), they finetuned the skip-thought models on the Stanford Natural Language Inference (SNLI)\ncorpus (Bowman et al., 2015), which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.\nIn Hill et al. (2016), the proposed FastSent model takes summation of the word representations to compose a sentence representation, and predicts the words in both the previous sentence and the next sentence. The results on this semantic related task is comparable with the RNN-based skip-thought model, while the skip-thought model still outperforms the FastSent model on the other six classification tasks. Later, Siamese CBOW (Kenter et al., 2016) aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent.\nFollowing the skip-thought model, we designed our skip-thought neighbor model by a simple modification. Section 3 presents the details."}, {"heading": "3 Approach", "text": "In this section, we present the skip-thought neighbor model. We first briefly introduce the skipthought model (Kiros et al., 2015), and then discuss how to explicitly modify the decoders in the skip-thought model to get the skip-thought neighbor model."}, {"heading": "3.1 Skip-thought Model", "text": "In the skip-thought model, given a sentence tuple (si\u22121, si, si+1), the encoder computes a fixed-dimension vector as the representation zi for the sentence si, which learns a distribution p(zi|si; \u03b8e), where \u03b8e stands for the set of parameters in the encoder. Then, conditioned on the representation zi, two separate decoders are applied to reconstruct the previous sentence si\u22121, and the next sentence si+1, respectively. We call them previous decoder p(si\u22121|zi; \u03b8p) and next decoder p(si+1|zi; \u03b8n), where \u03b8\u00b7 denotes the set of parameters in each decoder.\nSince the two conditional distributions learned from the decoders are parameterized independently, they implicitly utilize the sentence order information within the sentence tuple. Intuitively, given the current sentence si, inferring the previous sentence si\u22121 is considered to be different from inferring the next sentence si+1."}, {"heading": "3.2 Encoder: GRU", "text": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al. (2015). Since the comparison among different recurrent units is not our main focus, we decided to use GRU, which is a fast and stable recurrent unit. In addition, Chung et al. (2014) shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).\nSuppose sentence si contains N words, which are w1i , w 2 i , ..., w N i . At an arbitrary time step t, the encoder produces a hidden state hti, and we regard it as the representation for the previous subsequence through time t. At time N , the hidden state hNi represents the given sentence si, which is zi. The computation flow of the GRU in our experiments is shown below (omitting the subscript i) :\n[ mt\nrt\n] = \u03c3 ( Whh t\u22121 +Wxx t )\n(1)\nh\u0302t = tanh ( Wxt +U ( rt ht\u22121 )) (2)\nht = (1\u2212mt) ht\u22121 +mt h\u0302t (3)\nwhere xt is the embedding for the word wti , W\u00b7 and U are the parameter matrices, and is the element-wise product."}, {"heading": "3.3 Decoder: Conditional GRU", "text": "The decoder needs to reconstruct the previous sentence si\u22121 and the next sentence si+1 given the representation zi. Specifically, the decoder is a recurrent neural network with conditional GRU, and it takes the representation zi as an additional input at each time step."}, {"heading": "3.4 Skip-thought Neighbor Model", "text": "Our hypothesis is that, even without the order information within a given sentence tuple, the skipthought model should behave similarly in terms of the reconstruction error, and perform similarly on the evaluation tasks. To modify the skipthought model, given si, we assume that inferring si\u22121 is the same as inferring si+1. If we define {si\u22121, si+1} as the two neighbors of si, then the inferring process can be denoted as sj \u223c p(s|zi; \u03b8d), for any j in the neighborhood of si. The conditional distribution learned from the decoder is parameterized by \u03b8d.\nIn experiments, we directly drop one of the two decoders, and use only one decoder to reconstruct the previous sentence si\u22121 and next sentence si+1 at the same time, given the representation zi of si. Our skip-thought neighbor model can be considered as sharing the parameters between the previous decoder and the next decoder in the original skip-thought model. An illustration is shown in Figure 1.\nThe objective at each time step is defined as the log-likelihood of the predicted word given the previous words, which is\n`ti,j(\u03b8e, \u03b8d) = log p(w t j |w<tj , zi; \u03b8e, \u03b8d) (4)\nmax \u03b8e,\u03b8d \u2211 i \u2211 j\u2208{i\u22121,i+1} \u2211 t `ti,j(\u03b8e, \u03b8d) (5)\nwhere \u03b8e is the set of parameters in the encoder, and \u03b8d is the set of parameters in the decoder. The loss function is summed across the whole training corpus."}, {"heading": "3.5 Skip-thought Neighbor with Autoencoder", "text": "Previously, we defined {si\u22121, si+1} as the two neighbors of si. In addition, we assume that si could also be a neighbor of itself. Therefore, the neighborhood of si becomes {si\u22121, si, si+1}. In-\nferring sj \u223c p(s|zi; \u03b8d) for any j in the neighborhood of si then involves adding an autoencoder path into to our skip-thought neighbor model. In experiments, the decoder in the model is required to reconstruct all three sentences {si\u22121, si, si+1} in the neighborhood of si at the same time. The objective function becomes\nmax \u03b8e,\u03b8d \u2211 i \u2211 j\u2208{i\u22121,i,i+1} \u2211 t lti,j(\u03b8e, \u03b8d) (6)\nPreviously Hill et al. (2016) tested adding an autoencoder path into their FastSent model. Their results show that, with the additional autoencoder path, the performance on the classification tasks slightly improved, while there was no significant performance gain or loss on the semantic relatedness task.\nWe tested both our skip-thought neighbor model and the original skip-thought model with the autoencoder path, respectively. The results are presented in Sections 4, 5 and 6."}, {"heading": "3.6 Skip-thought Neighbor with One Target", "text": "In our skip-thought neighbor model, for a given sentence si, the decoder needs to reconstruct the sentences in its neighborhood {si\u22121, si+1}, which are two targets. We denote the inference process as si \u2192 {si\u22121, si+1}. For the next sentence si+1,\nthe inference process is si+1 \u2192 {si, si+2}. In other words, for a given sentence pair {si, si+1}, the inference process includes si \u2192 si+1 and si+1 \u2192 si.\nIn our hypothesis, the model doesn\u2019t distinguish between the sentences in a neighborhood. In this case, an inference process that includes si \u2192 si+1 and si+1 \u2192 si is equivalent to an inference process with only one of them. Thus, we define a skip-thought neighbor model with only one target, and the target is always the next sentence. The objective becomes\nmax \u03b8e,\u03b8d \u2211 i \u2211 t `ti,i+1(\u03b8e, \u03b8d) (7)"}, {"heading": "4 Experiment Settings", "text": "The large corpus that we used for unsupervised training is the BookCorpus dataset (Zhu et al., 2015), which contains 74 million sentences from 7000 books in total.\nAll of our experiments were conducted in Torch7 (Collobert et al., 2011). To make the comparison fair, we reimplemented the skip-thought model under the same settings, according to Kiros et al. (2015), and the publicly available theano code1. We adopted the multi-GPU train-\n1https://github.com/ryankiros/skip-thoughts\ning scheme from the Facebook implementation of ResNet2.\nWe use the ADAM (Kingma and Ba, 2014) algorithm for optimization. Instead of applying the gradient clipping according to the norm of the gradient, which was used in Kiros et al. (2015), we directly cut off the gradient to make it within [\u22121, 1] for stable training.\nThe dimension of the word embedding and the sentence representation are 620 and 1200. respectively. For the purpose of fast training, all the sentences were zero-padded or clipped to have the same length."}, {"heading": "5 Quantitative Evaluation", "text": "We compared our proposed skip-thought neighbor model with the skip-thought model on 7 evaluation tasks, which include semantic relatedness, paraphrase detection, question-type classification and 4 benchmark sentiment and subjective datasets. After unsupervised training on the BookCorpus dataset, we fix the parameters in the encoder, and apply it as a sentence representation extractor on the 7 tasks.\nFor semantic relatedness, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al. (2015). For a given sentence pair, the encoder computes a pair of representations, denoted as u and v, and the concatenation of the component-wise product u \u00b7v and the absolute difference |u \u2212 v| is regarded as the feature for the given sentence pair. Then we train a logistic regression on top of the feature to predict the semantic relatedness score. The evaluation metrics are Pearsons r, Spearmans \u03c1, and mean squared error MSE.\nThe dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus (Dolan et al., 2004). We follow the same feature engineering idea from Tai et al. (2015) to compute a single feature for each sentence pair. Then we train a logistic regression, and 10-fold cross validation is applied to find the optimal hyperparameter settings.\nThe 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification\n2https://github.com/facebook/fb.resnet.torch\n(SUBJ) (Pang and Lee, 2004), and opinion polarity (MPQA) (Wiebe et al., 2005).\nIn order to deal with more words besides the words used for training, the same word expansion method, which was introduced by Kiros et al. (2015), is applied after training on the BookCorpus dataset.\nThe results are shown in Table 1, where the model name is given by encoder type - model type - model size. We tried with three different types of the encoder, denoted as uni-, bi-, and combinein Table 1. The first one is a uni-directional GRU, which computes a 1200-dimension vector as the sentence representation. The second one is a bi-directional GRU, which computes a 600- dimension vector for each direction, and then the two vectors are concatenated to serve as the sentence representation. Third, after training the unidirectional model and the bi-directional model, the representation from both models are concatenated together to represent the sentence, denoted as combine-.\nIn Table 1, -N- refers to our skip-thought neighbor model, -N-next- refers to our skip-thought neighbor with only predicting the next sentence, and -skip- refers to the original skip-thought model."}, {"heading": "5.1 Skip-thought Neighbor vs. Skip-thought", "text": "From the results we show in Table 1, we can tell that our skip-thought neighbor models perform as well as skip-thought models but with fewer parameters, which means that the neighborhood information is effective in terms of helping the model capture sentential contextual information."}, {"heading": "5.2 Skip-thought Neighbor+AE vs. Skip-thought+AE", "text": "For our skip-thought neighbor model, incorporating an antoencoder (+AE) means that, besides reconstructing the two neighbors si\u22121 and si+1, the decoder also needs to reconstruct si. For the skipthought model, since the implicit hypothesis in the model is that different decoders learn different conditional distributions, we add another decoder in the skip-thought model to reconstruct the sentence si. The results are also shown in Table 1.\nAs we can see, our skip-thought neighbor+AE models outperform skip-thought+AE models significantly. Specifically, in skip-thought model, adding an autoencoder branch hurts the performance on SICK, MR, CR, SUBJ and MPQA\ndataset. We find that the reconstruction error on the autoencoder branch decreases drastically during training, while the sum of the reconstruction errors on the previous decoder and next decoder fluctuates widely, and is larger than that in the model without the autoencoder branch. It seems that, the autoencoder branch hurt the skip-thought model to capture sentential contextual information from the surrounding sentences. One could vary the weights on the three independent branches to get better results, but it is not our main focus in this paper.\nIn our skip-thought neighbor model, the inclusion of the autoencoder constraint did not have the same problem. With the autoencoder branch, the model gets lower errors on reconstructing all three sentences. However, it doesn\u2019t help the model to perform better on the evaluation tasks."}, {"heading": "5.3 Increasing the Number of Neighbors", "text": "We also explored adding more neighbors into our skip-thought neighbor model. Besides using one decoder to predict the previous 1 sentence, and the next 1 sentence, we expand the neighborhood to contain 4 sentences, which are the previous 2 sentences, and the next 2 sentences. In this case, the decoder is required to reconstruct 4 sentences at the same time. We ran experiments with our model, and we evaluated the trained encoder on 7 tasks.\nThere is no significant performance gain or loss on our model trained with 4 neighbors; it seems that, increasing the number of neighbors doesn\u2019t improve the performance, but it also doesn\u2019t hurt the performance. Our hypothesis is that, reconstructing four different sentences in a neighborhood with only one set of parameters is a hard task, which might distract the model from capturing the sentential contextual information."}, {"heading": "5.4 Skip-thought Neighbor with One Target", "text": "Compared to the skip-thought model, our skipthought neighbor model with one target contains fewer parameters, and runs faster during training, since for a given sentence, our model only needs to reconstruct its next sentence while the skipthought model needs to reconstruct its surrounding two sentences. The third section in Table 1 presents the results of our model with only one target. Surprisingly, it overall performs as well as the skip-thought models as all previous models."}, {"heading": "5.5 A Note on Normalizing the Representation", "text": "An interesting observation was found when we were investigating the publicly available code for Kiros et al. (2015), which is, during training, the representation produced from the encoder will be directly sent to the two decoders, however, after training, the output from the encoder will be normalized to keep the l2-norm as 1, so the sentence representation is a normalized vector.\nWe conducted experiments on the effect of normalization during the evaluation, and we evaluated both on our skip-thought neighbor model, and our implemented skip-thought model. Generally, the normalization step slightly hurts the performance on the semantic relatedness SICK task, while it improves the performance across all the other classification tasks. The Table 1 presents the results with the normalization step, and Table 2 presents the results without normalization on SICK dataset."}, {"heading": "6 Qualitative Investigation", "text": "We conducted investigation on the decoder in our trained skip-thought neighbor model."}, {"heading": "6.1 Sentence Retrieval", "text": "We first pick up 1000 sentences as the query set, and then randomly pick up 1 million sentences as the database. In the previous section, we\nmentioned that normalization improves the performance of the model, so the distance measure we applied in the sentence retrieval experiment is the cosine distance. Most of retrieved sentences look semantically related and can be viewed as the sentential contextual extension to the query sentences. Several samples can be found in Table 3."}, {"heading": "6.2 Conditional Sentence Generation", "text": "Since the models are trained to minimizing the reconstruction error across the whole training corpus, it is reasonable to analyze the behavior of the decoder on the conditional sentence generation. We first randomly pick up sentences from the training corpus, and compute a representation for each of them. Then, we greedily decode the representations to sentences. Table 4 presents the generated sentences. Several interesting observations worth mentioning here.\nThe decoder in our skip-thought neighbor model aims to minimize the distance of the gen-\nerated sentence to two targets, which lead us to doubt if the decoder is able to generate at least grammatically-correct English sentences. But, the results shows that the generated sentences are both grammatically-correct and generally meaningful.\nWe also observe that, the generated sentences tend to have similar starting words, and usually have negative expression, such as i was n\u2019t, i \u2019m not, i do n\u2019t, etc. After investigating the training corpus, we noticed that this observation is caused by the dataset bias. A majority of training sentences start with i and i \u2019m and i was , and there is a high chance that the negation comes after was and \u2019m. In addition, the generated sentences rarely are the sentential contextual extension of their associated input sentences, which is same for the skipthought models. More investigations are needed for the conditional sentence generation."}, {"heading": "7 Conclusion", "text": "We proposed a hypothesis that the neighborhood information is effective in learning sentence representation, and empirically tested our hypothesis. Our skip-thought neighbor models were trained in an unsupervised fashion, and evaluated on 7 tasks. The results showed that our models perform as well as the skip-thought models. Furthermore, our model with only one target performs better than the skip-thought model. Future work could explore more on the our skip-thought neighbor model with only one target, and see if the pro-\nposed model is able to generalize to even larger corpora, or another corpus that is not derived from books."}, {"heading": "Acknowledgments", "text": "We gratefully thank Jeffrey L. Elman, Benjamin K. Bergen, Seana Coulson, and Marta Kutas for insightful discussion, and thank Thomas Andy Keller, Thomas Donoghue, Larry Muhlstein, and Reina Mizrahi for suggestive chatting. We also thank Adobe Research Lab for GPUs support, and thank NVIDIA for DGX-1 trial as well as support from NSF IIS 1528214 and NSF SMA 1041755."}], "references": [{"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts."], "venue": "ACL.", "citeRegEx": "Bowman et al\\.,? 2016", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["William B. Dolan", "Chris Quirk", "Chris Brockett."], "venue": "COLING.", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word 10(2-3):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "HLT-NAACL.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Juergen Schmidhuber."], "venue": "Neural Computation 9:1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "KDD.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Siamese cbow: Optimizing word embeddings for sentence representations", "author": ["Tom Kenter", "Alexey Borisov", "Maarten de Rijke."], "venue": "ACL.", "citeRegEx": "Kenter et al\\.,? 2016", "shortCiteRegEx": "Kenter et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Jamie Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "COLING.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "LREC.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "ACL.", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee."], "venue": "ACL.", "citeRegEx": "Pang and Lee.,? 2005", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Towards generalizable sentence embeddings", "author": ["Eleni Triantafillou", "Jamie Ryan Kiros", "Raquel Urtasun", "Richard Zemel."], "venue": "RepL4NLP, ACL Workshop.", "citeRegEx": "Triantafillou et al\\.,? 2016", "shortCiteRegEx": "Triantafillou et al\\.", "year": 2016}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."], "venue": "Language Resources and Evaluation 39:165\u2013210.", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "ICCV pages 19\u201327.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "We study the skip-thought model proposed by Kiros et al. (2015) with neighborhood information as weak supervision.", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": "(2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder.", "startOffset": 82, "endOffset": 96}, {"referenceID": 11, "context": "Previously, the skip-thought model was introduced by Kiros et al. (2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder.", "startOffset": 53, "endOffset": 73}, {"referenceID": 7, "context": "(2015), which learns to explore the semantic continuity within adjacent sentences (Harris, 1954) as supervision for learning a generic sentence encoder. The skip-thought model encodes the current sentence and then decodes the previous sentence and the next one, instead of itself. Two independent decoders were applied, since intuitively, the previous sentence and the next sentence should be drawn from 2 different conditional distributions, respectively. By posing a hypothesis that the adjacent sentences provide the same neighborhood information for learning sentence representation, we first drop one of the 2 decoders, and use only one decoder to reconstruct the surrounding 2 sentences at the same time. The empirical results show that our skip-thought neighbor model performs as well as the skip-thought model on 7 evaluation tasks. Then, inspired by Hill et al. (2016), as they tested the effect of incorporating an autoencoder branch in their proposed FastSent model, we also conduct experiments to explore reconstructing the input sentence itself as well in both our skip-thought neighbor model and the skip-thought model.", "startOffset": 83, "endOffset": 878}, {"referenceID": 16, "context": "Previously, Mikolov et al. (2013b) proposed a method for distributed representation learning for words by predicting surrounding words, and empirically showed that the additive composition of the learned word representations successfully captures contextual information of phrases and sentences.", "startOffset": 12, "endOffset": 35}, {"referenceID": 14, "context": "Similarly, Le and Mikolov (2014) proposed a method that learns a fixed-dimension vector for each sentence, by predicting the words within the given sentence.", "startOffset": 11, "endOffset": 33}, {"referenceID": 5, "context": "representation learning was proposed by Dai and Le (2015). The model combines an LSTM encoder, and an LSTM decoder to learn language representation in an unsupervised fashion on the supervised evaluation datasets, and then finetunes the LSTM encoder for supervised tasks on the same datasets.", "startOffset": 40, "endOffset": 58}, {"referenceID": 19, "context": "Tai et al. (2015) modified the plain LSTM network to a tree-structured LSTM network, which helps the model to address the long-term dependency problem.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Bowman et al. (2016) proposed a model that learns to parse the sentence at the same time as the RNN is processing the input sentence.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Inspired by the skip-gram model (Mikolov et al., 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently.", "startOffset": 32, "endOffset": 55}, {"referenceID": 7, "context": ", 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently.", "startOffset": 59, "endOffset": 73}, {"referenceID": 0, "context": "(2016), they finetuned the skip-thought models on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which shows that the skip-thought pretraining scheme is generalizable to other specific NLP tasks.", "startOffset": 104, "endOffset": 125}, {"referenceID": 10, "context": "Instead of learning to compose a sentence representation from the word representations, the skip-thought model Kiros et al. (2015) utilizes the structure and relationship of the adjacent sentences in the large unlabelled corpus.", "startOffset": 111, "endOffset": 131}, {"referenceID": 5, "context": ", 2013a), and the sentence-level distributional hypothesis (Harris, 1954), the skip-thought model encodes the current sentence as a fixed-dimension vector, and instead of predicting the input sentence itself, the decoders predict the previous sentence and the next sentence independently. The skip-thought model provides an alternative way for unsupervised sentence representation learning, and has shown great success. The learned sentence representation encoder outperforms previous unsupervised pretrained models on 8 evaluation tasks with no finetuning, and the results are comparable to supervised trained models. In Triantafillou et al. (2016), they finetuned the skip-thought models on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al.", "startOffset": 60, "endOffset": 650}, {"referenceID": 11, "context": "Later, Siamese CBOW (Kenter et al., 2016) aimed to learn the word representations to make the cosine similarity of adjacent sentences in the representation space larger than that of sentences which are not adjacent.", "startOffset": 20, "endOffset": 41}, {"referenceID": 8, "context": "In Hill et al. (2016), the proposed FastSent model takes summation of the word representations to compose a sentence representation, and predicts the words in both the previous sentence and the next sentence.", "startOffset": 3, "endOffset": 22}, {"referenceID": 13, "context": "We first briefly introduce the skipthought model (Kiros et al., 2015), and then discuss how to explicitly modify the decoders in the skip-thought model to get the skip-thought neighbor model.", "startOffset": 49, "endOffset": 69}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al.", "startOffset": 118, "endOffset": 136}, {"referenceID": 9, "context": "(2014) shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 105, "endOffset": 139}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al. (2015). Since the comparison among different recurrent units is not our main focus, we decided to use GRU, which is a fast and stable recurrent unit.", "startOffset": 119, "endOffset": 199}, {"referenceID": 2, "context": "In order to make the comparison fair, we choose to use a recurrent neural network with the gated recurrent unit (GRU) (Cho et al., 2014), which is the same recurrent unit used in Kiros et al. (2015). Since the comparison among different recurrent units is not our main focus, we decided to use GRU, which is a fast and stable recurrent unit. In addition, Chung et al. (2014) shows that, on language modeling tasks, GRU performs as well as the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 119, "endOffset": 375}, {"referenceID": 8, "context": "Previously Hill et al. (2016) tested adding an autoencoder path into their FastSent model.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "The large corpus that we used for unsupervised training is the BookCorpus dataset (Zhu et al., 2015), which contains 74 million sentences from 7000 books in total.", "startOffset": 82, "endOffset": 100}, {"referenceID": 4, "context": "All of our experiments were conducted in Torch7 (Collobert et al., 2011).", "startOffset": 48, "endOffset": 72}, {"referenceID": 4, "context": "All of our experiments were conducted in Torch7 (Collobert et al., 2011). To make the comparison fair, we reimplemented the skip-thought model under the same settings, according to Kiros et al. (2015), and the publicly available theano code1.", "startOffset": 49, "endOffset": 201}, {"referenceID": 12, "context": "We use the ADAM (Kingma and Ba, 2014) algorithm for optimization.", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": "We use the ADAM (Kingma and Ba, 2014) algorithm for optimization. Instead of applying the gradient clipping according to the norm of the gradient, which was used in Kiros et al. (2015), we directly cut off the gradient to make it within [\u22121, 1] for stable training.", "startOffset": 17, "endOffset": 185}, {"referenceID": 16, "context": "For semantic relatedness, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 16, "context": "For semantic relatedness, we use the SICK dataset (Marelli et al., 2014), and we adopt the feature engineering idea proposed by Tai et al. (2015). For a given sentence pair, the encoder computes a pair of representations, denoted as u and v, and the concatenation of the component-wise product u \u00b7v and the absolute difference |u \u2212 v| is regarded as the feature for the given sentence pair.", "startOffset": 51, "endOffset": 146}, {"referenceID": 6, "context": "The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus (Dolan et al., 2004).", "startOffset": 93, "endOffset": 113}, {"referenceID": 6, "context": "The dataset we use for the paraphrase detection is the Microsoft Paraphrase Detection Corpus (Dolan et al., 2004). We follow the same feature engineering idea from Tai et al. (2015) to compute a single feature for each sentence pair.", "startOffset": 94, "endOffset": 182}, {"referenceID": 15, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 67, "endOffset": 86}, {"referenceID": 20, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 116, "endOffset": 136}, {"referenceID": 10, "context": "The 5 classification tasks are question-type classification (TREC) (Li and Roth, 2002), movie review sentiment (MR) (Pang and Lee, 2005), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification", "startOffset": 168, "endOffset": 186}, {"referenceID": 19, "context": "torch (SUBJ) (Pang and Lee, 2004), and opinion polarity (MPQA) (Wiebe et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 23, "context": "torch (SUBJ) (Pang and Lee, 2004), and opinion polarity (MPQA) (Wiebe et al., 2005).", "startOffset": 63, "endOffset": 83}, {"referenceID": 13, "context": "In order to deal with more words besides the words used for training, the same word expansion method, which was introduced by Kiros et al. (2015), is applied after training on the BookCorpus dataset.", "startOffset": 126, "endOffset": 146}, {"referenceID": 13, "context": "An interesting observation was found when we were investigating the publicly available code for Kiros et al. (2015), which is, during training, the representation produced from the encoder will be directly sent to the two decoders, however, after training, the output from the encoder will be normalized to keep the l2-norm as 1, so the sentence representation is a normalized vector.", "startOffset": 96, "endOffset": 116}], "year": 2017, "abstractText": "We study the skip-thought model proposed by Kiros et al. (2015) with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn\u2019t aid our model to perform better, while it hurts the performance of the skip-thought model.", "creator": "LaTeX with hyperref package"}}}