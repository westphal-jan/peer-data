{"id": "1706.03175", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Recovery Guarantees for One-hidden-layer Neural Networks", "abstract": "In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to $\\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective. Most popular nonlinear activation functions satisfy the distilled properties, including rectified linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids, which form the basis for the linear-loss objective. For example, a given equation generates a Gaussian distribution (which is a function of two input and a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-lying function with a fixed-", "histories": [["v1", "Sat, 10 Jun 2017 02:56:39 GMT  (125kb,D)", "http://arxiv.org/abs/1706.03175v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["kai zhong", "zhao song", "prateek jain 0002", "peter l bartlett", "inderjit s dhillon"], "accepted": true, "id": "1706.03175"}, "pdf": {"name": "1706.03175.pdf", "metadata": {"source": "CRF", "title": "Recovery Guarantees for One-hidden-layer Neural Networks\u2217", "authors": ["Kai Zhong", "Zhao Song", "Peter L. Bartlett", "Inderjit S. Dhillon"], "emails": ["zhongkai@ices.utexas.edu", "zhaos@utexas.edu", "prajain@microsoft.com", "bartlett@cs.berkeley.edu", "inderjit@cs.utexas.edu"], "sections": [{"heading": null, "text": "\u2217A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on Machine Learning (ICML 2017). \u2020Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and part of the work was done while interning in Microsoft research, India. \u2021Supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). \u00a7Supported in part by Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS), and NSF grants IIS-1619362. \u00b6Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.\nar X\niv :1\n70 6.\n03 17\n5v 1\n[ cs\n.L G\n] 1\n0 Ju"}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Related Work 4", "text": "2.1 Expressive Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Achievability of Global Optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Generalization Bound / Recovery Guarantees . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "3 Problem Formulation 6", "text": ""}, {"heading": "4 Positive Definiteness of Hessian 7", "text": ""}, {"heading": "5 Tensor Methods for Initialization 9", "text": "5.1 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.3 Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "6 Global Convergence 11", "text": ""}, {"heading": "7 Numerical Experiments 12", "text": ""}, {"heading": "8 Conclusion 13", "text": "References 14"}, {"heading": "A Notation 19", "text": ""}, {"heading": "B Preliminaries 19", "text": "B.1 Useful Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Matrix Bernstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"}, {"heading": "C Properties of Activation Functions 25", "text": ""}, {"heading": "D Local Positive Definiteness of Hessian 26", "text": "D.1 Main Results for Positive Definiteness of Hessian . . . . . . . . . . . . . . . . . . . . 26\nD.1.1 Bounding the Spectrum of the Hessian near the Ground Truth . . . . . . . . 26 D.1.2 Local Linear Convergence of Gradient Descent . . . . . . . . . . . . . . . . . 27\nD.2 Positive Definiteness of Population Hessian at the Ground Truth . . . . . . . . . . . 28 D.2.1 Lower Bound on the Eigenvalues of Hessian for the Orthogonal Case . . . . . 29 D.2.2 Lower Bound on the Eigenvalues of Hessian for Non-orthogonal Case . . . . . 33 D.2.3 Upper Bound on the Eigenvalues of Hessian for Non-orthogonal Case . . . . . 37 D.3 Error Bound of Hessians near the Ground Truth for Smooth Activations . . . . . . . 38 D.3.1 Second-order Smoothness near the Ground Truth for Smooth Activations . . 38 D.3.2 Empirical and Population Difference for Smooth Activations . . . . . . . . . . 42 D.4 Error Bound of Hessians near the Ground Truth for Non-smooth Activations . . . . . 48 D.5 Positive Definiteness for a Small Region . . . . . . . . . . . . . . . . . . . . . . . . . 51"}, {"heading": "E Tensor Methods 55", "text": "E.1 Tensor Initialization Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 E.2 Main Result for Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 E.3 Error Bound for the Subspace Spanned by the Weight Matrix . . . . . . . . . . . . . 59\nE.3.1 Error Bound for the Second-order Moment in Different Cases . . . . . . . . . 59 E.3.2 Error Bound for the Second-order Moment . . . . . . . . . . . . . . . . . . . . 63 E.3.3 Subspace Estimation Using Power Method . . . . . . . . . . . . . . . . . . . . 63\nE.4 Error Bound for the Reduced Third-order Moment . . . . . . . . . . . . . . . . . . . 66 E.4.1 Error Bound for the Reduced Third-order Moment in Different Cases . . . . . 66 E.4.2 Final Error Bound for the Reduced Third-order Moment . . . . . . . . . . . . 70 E.5 Error Bound for the Magnitude and Sign of the Weight Vectors . . . . . . . . . . . . 71 E.5.1 Robustness for Solving Linear Systems . . . . . . . . . . . . . . . . . . . . . . 71 E.5.2 Error Bound for the First-order Moment . . . . . . . . . . . . . . . . . . . . . 71 E.5.3 Linear System for the First-order Moment . . . . . . . . . . . . . . . . . . . . 73 E.5.4 Linear System for the Second-order Moment . . . . . . . . . . . . . . . . . . . 74\nF Acknowledgments 75"}, {"heading": "1 Introduction", "text": "Neural Networks (NNs) have achieved great practical success recently. Many theoretical contributions have been made very recently to understand the extraordinary performance of NNs. The remarkable results of NNs on complex tasks in computer vision and natural language processing inspired works on the expressive power of NNs [CSS16, CS16, RPK+16, DFS16, PLR+16, MPCB14, Tel16]. Indeed, several works found NNs are very powerful and the deeper the more powerful. However, due to the high non-convexity of NNs, knowing the expressivity of NNs doesn\u2019t guarantee that the targeted functions will be learned. Therefore, several other works focused on the achievability of global optima. Many of them considered the over-parameterized setting, where the global optima or local minima close to the global optima will be achieved when the number of parameters is large enough, including [FB16, HV15, LSSS14, DPG+14, SS16, HM17]. This, however, leads to overfitting easily and can\u2019t provide any generalization guarantees, which are actually the essential goal in most tasks.\nA few works have considered generalization performance. For example, [XLS17] provide generalization bound under the Rademacher generalization analysis framework. Recently [ZBH+17] describe some experiments showing that NNs are complex enough that they actually memorize the training data but still generalize well. As they claim, this cannot be explained by applying generalization analysis techniques, like VC dimension and Rademacher complexity, to classification loss (although it does not rule out a margins analysis\u2014see, for example, [Bar98]; their experiments involve the unbounded cross-entropy loss).\nIn this paper, we don\u2019t develop a new generalization analysis. Instead we focus on parameter recovery setting, where we assume there are underlying ground-truth parameters and we provide recovery guarantees for the ground-truth parameters up to equivalent permutations. Since the parameters are exactly recovered, the generalization performance will also be guaranteed.\nSeveral other techniques are also provided to recover the parameters or to guarantee generalization performance, such as tensor methods [JSA15] and kernel methods [AGMR17]. These methods require sample complexity O(d3) or computational complexity O\u0303(n2), which can be intractable in practice. We propose an algorithm that has recovery guarantees for 1NN with sample complexity O\u0303(d) and computational time O\u0303(dn) under some mild assumptions.\nRecently [Sha16] show that neither specific assumptions on the niceness of the input distribution or niceness of the target function alone is sufficient to guarantee learnability using gradient-based methods. In this paper, we assume data points are sampled from Gaussian distribution and the parameters of hidden neurons are linearly independent.\nOur main contributions are as follows,\n1. We distill some properties for activation functions, which are satisfied by a wide range of activations, including ReLU, squared ReLU, sigmoid and tanh. With these properties we show positive definiteness (PD) of the Hessian in the neighborhood of the ground-truth parameters given enough samples (Theorem 4.2). Further, for activations that are also smooth, we show local linear convergence is guaranteed using gradient descent.\n2. We propose a tensor method to initialize the parameters such that the initialized parameters fall into the local positive definiteness area. Our contribution is that we reduce the sample/computational complexity from cubic dependency on dimension to linear dependency (Theorem 5.6).\n3. Combining the above two results, we provide a globally converging algorithm (Algorithm 2) for smooth homogeneous activations satisfying the distilled properties. The whole procedure\nrequires sample/computational complexity linear in dimension and logarithmic in precision (Theorem 6.1)."}, {"heading": "2 Related Work", "text": "The recent empirical success of NNs has boosted their theoretical analyses [FZK+16, Bal16, BMBY16, SBL16, APVZ14, AGMR17, GKKT17]. In this paper, we classify them into three main directions."}, {"heading": "2.1 Expressive Power", "text": "Expressive power is studied to understand the remarkable performance of neural networks on complex tasks. Although one-hidden-layer neural networks with sufficiently many hidden nodes can approximate any continuous function [Hor91], shallow networks can\u2019t achieve the same performance in practice as deep networks. Theoretically, several recent works show the depth of NNs plays an essential role in the expressive power of neural networks [DFS16]. As shown in [CSS16, CS16, Tel16], functions that can be implemented by a deep network of polynomial size require exponential size in order to be implemented by a shallow network. [RPK+16, PLR+16, MPCB14, AGMR17] design some measures of expressivity that display an exponential dependence on the depth of the network. However, the increasing of the expressivity of NNs or its depth also increases the difficulty of the learning process to achieve a good enough model. In this paper, we focus on 1NNs and provide recovery guarantees using a finite number of samples."}, {"heading": "2.2 Achievability of Global Optima", "text": "The global convergence is in general not guaranteed for NNs due to their non-convexity. It is widely believed that training deep models using gradient-based methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minima. [SCP16] present examples showing that for this to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. Indeed the achievability of global optima has been shown under many different types of assumptions.\nIn particular, [CHM+15] analyze the loss surface of a special random neural network through spin-glass theory and show that it has exponentially many local optima, whose loss is small and close to that of a global optimum. Later on, [Kaw16] eliminate some assumptions made by [CHM+15] but still require the independence of activations as [CHM+15], which is unrealistic. [SS16] study the geometric structure of the neural network objective function. They have shown that with high probability random initialization will fall into a basin with a small objective value when the network is over-parameterized. [LSSS14] consider polynomial networks where the activations are square functions, which are typically not used in practice. [HV15] show that when a local minimum has zero parameters related to a hidden node, a global optimum is achieved. [FB16] study the landscape of 1NN in terms of topology and geometry, and show that the level set becomes connected as the network is increasingly over-parameterized. [HM17] show that products of matrices don\u2019t have spurious local minima and that deep residual networks can represent any function on a sample, as long as the number of parameters is larger than the sample size. [SC16] consider over-specified NNs, where the number of samples is smaller than the number of weights. [DPG+14] propose a new approach to second-order optimization that identifies and attacks the saddle point problem in high-dimensional non-convex optimization. They apply the approach to recurrent neural networks and show practical performance. [AGMR17] use results from tropical geometry to show global optimality of an algorithm, but it requires (2n)k poly(n) computational complexity.\nAlmost all of these results require the number of parameters is larger than the number of points, which probably overfits the model and no generalization performance will be guaranteed. In this paper, we propose an efficient and provable algorithm for 1NNs that can achieve the underlying ground-truth parameters."}, {"heading": "2.3 Generalization Bound / Recovery Guarantees", "text": "The achievability of global optima of the objective from the training data doesn\u2019t guarantee the learned model to be able to generalize well on unseen testing data. In the literature, we find three main approaches to generalization guarantees.\n1) Use generalization analysis frameworks, including VC dimension/Rademacher complexity, to bound the generalization error. A few works have studied the generalization performance for NNs. [XLS17] follow [SC16] but additionally provide generalization bounds using Rademacher complexity. They assume the obtained parameters are in a regularization set so that the generalization performance is guaranteed, but this assumption can\u2019t be justified theoretically. [HRS16] apply stability analysis to the generalization analysis of SGD for convex and non-convex problems, arguing early stopping is important for generalization performance.\n2) Assume an underlying model and try to recover this model. This direction is popular for many non-convex problems including matrix completion/sensing [JNS13, Har14, SL15, BLWZ17], mixed linear regression [ZJD16], subspace recovery [EV09] and other latent models [AGH+14].\nWithout making any assumptions, those non-convex problems are intractable [AGKM12, GV15, SWZ17a, GG11, RSW16, SR11, HM13, AGM12, YCS14]. Recovery guarantees for NNs also need assumptions. Several different approaches under different assumptions are provided to have recovery guarantees on different NN settings.\nTensor methods [AGH+14, WTSA15, WA16, SWZ16] are a general tool for recovering models with latent factors by assuming the data distribution is known. Some existing recovery guarantees for NNs are provided by tensor methods [SA15, JSA15]. However, [SA15] only provide guarantees to recover the subspace spanned by the weight matrix and no sample complexity is given, while [JSA15] require O(d3/ 2) sample complexity. In this paper, we use tensor methods as an initialization step so that we don\u2019t need very accurate estimation of the moments, which enables us to reduce the total sample complexity from 1/ 2 to log(1/ ).\n[ABGM14] provide polynomial sample complexity and computational complexity bounds for learning deep representations in unsupervised setting, and they need to assume the weights are sparse and randomly distributed in [\u22121, 1].\n[Tia17] analyze 1NN by assuming Gaussian inputs in a supervised setting, in particular, regression and classification with a teacher. This paper also considers this setting. However, there are some key differences. a) [Tia17] require the second-layer parameters are all ones, while we can learn these parameters. b) In [Tia17], the ground-truth first-layer weight vectors are required to be orthogonal, while we only require linear independence. c) [Tia17] require a good initialization but doesn\u2019t provide initialization methods, while we show the parameters can be efficiently initialized by tensor methods. d) In [Tia17], only the population case (infinite sample size) is considered, so there is no sample complexity analysis, while we show finite sample complexity.\nRecovery guarantees for convolution neural network with Gaussian inputs are provided in [BG17], where they show a globally converging guarantee of gradient descent on a one-hidden-layer nooverlap convolution neural network. However, they consider population case, so no sample complexity is provided. Also their analysis depends on ReLU activations and the no-overlap case is very unlikely to be used in practice. In this paper, we consider a large range of activation functions, but for one-hidden-layer fully-connected NNs.\n3) Improper Learning. In the improper learning setting for NNs, the learning algorithm is not restricted to output a NN, but only should output a prediction function whose error is not much larger than the error of the best NN among all the NNs considered. [ZLJ16, ZLW16] propose kernel methods to learn the prediction function which is guaranteed to have generalization performance close to that of the NN. However, the sample complexity and computational complexity are exponential. [AZS14] transform NNs to convex semi-definite programming. The works by [Bac14] and [BRV+05] are also in this direction. However, these methods are actually not learning the original NNs. Another work by [ZLWJ17] uses random initializations to achieve arbitrary small excess risk. However, their algorithm has exponential running time in 1/ .\nRoadmap. The paper is organized as follows. In Section 3, we present our problem setting and show three key properties of activations required for our guarantees. In Section 4, we introduce the formal theorem of local strong convexity and show local linear convergence for smooth activations. Section 5 presents a tensor method to initialize the parameters so that they fall into the basin of the local strong convexity region."}, {"heading": "3 Problem Formulation", "text": "We consider the following regression problem. Given a set of n samples\nS = {(x1, y1), (x2, y2), \u00b7 \u00b7 \u00b7 (xn, yn)} \u2282 Rd \u00d7 R,\nlet D denote a underlying distribution over Rd \u00d7 R with parameters\n{w\u22171, w\u22172, \u00b7 \u00b7 \u00b7w\u2217k} \u2282 Rd, and {v\u22171, v\u22172, \u00b7 \u00b7 \u00b7 , v\u2217k} \u2282 R\nsuch that each sample (x, y) \u2208 S is sampled i.i.d. from this distribution, with\nD : x \u223c N (0, I), y = k\u2211 i=1 v\u2217i \u00b7 \u03c6(w\u2217>i x), (1)\nwhere \u03c6(z) is the activation function, k is the number of nodes in the hidden layer. The main question we want to answer is: How many samples are sufficient to recover the underlying parameters?\nIt is well-known that, training one hidden layer neural network is NP-complete [BR88]. Thus, without making any assumptions, learning deep neural network is intractable. Throughout the paper, we assume x follows a standard normal distribution; the data is noiseless; the dimension of input data is at least the number of hidden nodes; and activation function \u03c6(z) satisfies some reasonable properties.\nActually our results can be easily extended to multivariate Gaussian distribution with positive definite covariance and zero mean since we can estimate the covariance first and then transform the input to a standard normal distribution but with some loss of accuracy. Although this paper focuses on the regression problem, we can transform classification problems to regression problems if a good teacher is provided as described in [Tia17]. Our analysis requires k to be no greater than d, since the first-layer parameters will be linearly dependent otherwise.\nFor activation function \u03c6(z), we assume it is continuous and if it is non-smooth let its first derivative be left derivative. Furthermore, we assume it satisfies Property 3.1, 3.2, and 3.3. These properties are critical for the later analyses. We also observe that most activation functions actually satisfy these three properties.\nProperty 3.1. The first derivative \u03c6\u2032(z) is nonnegative and homogeneously bounded, i.e., 0 \u2264 \u03c6\u2032(z) \u2264 L1|z|p for some constants L1 > 0 and p \u2265 0.\nProperty 3.2. Let \u03b1q(\u03c3) = Ez\u223cN (0,1)[\u03c6\u2032(\u03c3 \u00b7 z)zq],\u2200q \u2208 {0, 1, 2}, and \u03b2q(\u03c3) = Ez\u223cN (0,1)[\u03c6\u20322(\u03c3 \u00b7 z)zq],\u2200q \u2208 {0, 2}. Let \u03c1(\u03c3) denote min{\u03b20(\u03c3) \u2212 \u03b120(\u03c3) \u2212 \u03b121(\u03c3), \u03b22(\u03c3) \u2212 \u03b121(\u03c3) \u2212 \u03b122(\u03c3), \u03b10(\u03c3) \u00b7 \u03b12(\u03c3)\u2212 \u03b121(\u03c3)} The first derivative \u03c6\u2032(z) satisfies that, for all \u03c3 > 0, we have \u03c1(\u03c3) > 0.\nProperty 3.3. The second derivative \u03c6\u2032\u2032(z) is either (a) globally bounded |\u03c6\u2032\u2032(z)| \u2264 L2 for some constant L2, i.e., \u03c6(z) is L2-smooth, or (b) \u03c6\u2032\u2032(z) = 0 except for e (e is a finite constant) points.\nRemark 3.4. The first two properties are related to the first derivative \u03c6\u2032(z) and the last one is about the second derivative \u03c6\u2032\u2032(z). At high level, Property 3.1 requires \u03c6 to be non-decreasing with homogeneously bounded derivative; Property 3.2 requires \u03c6 to be highly non-linear; Property 3.3 requires \u03c6 to be either smooth or piece-wise linear.\nTheorem 3.5. ReLU \u03c6(z) = max{z, 0}, leaky ReLU \u03c6(z) = max{z, 0.01z}, squared ReLU \u03c6(z) = max{z, 0}2 and any non-linear non-decreasing smooth functions with bounded symmetric \u03c6\u2032(z), like the sigmoid function \u03c6(z) = 1/(1 + e\u2212z), the tanh function and the erf function \u03c6(z) = \u222b z 0 e \u2212t2dt, satisfy Property 3.1,3.2,3.3. The linear function, \u03c6(z) = z, doesn\u2019t satisfy Property 3.2 and the quadratic function, \u03c6(z) = z2, doesn\u2019t satisfy Property 3.1 and 3.2.\nThe proof can be found in Appendix C."}, {"heading": "4 Positive Definiteness of Hessian", "text": "In this section, we study the Hessian of empirical risk near the ground truth. We consider the case when v\u2217 is already known. Note that for homogeneous activations, we can assume v\u2217i \u2208 {\u22121, 1} since v\u03c6(z) = v|v|\u03c6(|v|\n1/pz), where p is the degree of homogeneity. As v\u2217i only takes discrete values for homogeneous activations, in the next section, we show we can exactly recover v\u2217 using tensor methods with finite samples.\nFor a set of samples S, we define the Empirical Risk,\nf\u0302S(W ) = 1 2|S| \u2211\n(x,y)\u2208S ( k\u2211 i=1 v\u2217i \u03c6(w > i x)\u2212 y )2 . (2)\nFor a distribution D, we define the Expected Risk,\nfD(W ) = 1\n2 E (x,y)\u223cD ( k\u2211 i=1 v\u2217i \u03c6(w > i x)\u2212 y )2 . (3) Let\u2019s calculate the gradient and the Hessian of f\u0302S(W ) and fD(W ). For each j \u2208 [k], the partial gradient of fD(W ) with respect to wj can be represented as\n\u2202fD(W )\n\u2202wj = E (x,y)\u223cD [( k\u2211 i=1 v\u2217i \u03c6(w > i x)\u2212 y ) v\u2217j\u03c6 \u2032(w>j x)x ] .\nFor each j, l \u2208 [k] and j 6= l, the second partial derivative of fD(W ) for the (j, l)-th off-diagonal block is,\n\u22022fD(W )\n\u2202wj\u2202wl = E (x,y)\u223cD\n[ v\u2217j v \u2217 l \u03c6 \u2032(w>j x)\u03c6 \u2032(w>l x)xx > ] ,\nand for each j \u2208 [k], the second partial derivative of fD(W ) for the j-th diagonal block is\n\u22022fD(W )\n\u2202w2j = E (x,y)\u223cD [( k\u2211 i=1 v\u2217i \u03c6(w > i x)\u2212 y ) v\u2217j\u03c6 \u2032\u2032(w>j x)xx > + (v\u2217j\u03c6 \u2032(w>j x)) 2xx> ] .\nIf \u03c6(z) is non-smooth, we use the Dirac function and its derivatives to represent \u03c6\u2032\u2032(z). Replacing the expectation E(x,y)\u223cD by the average over the samples |S|\u22121 \u2211 (x,y)\u2208S , we obtain the Hessian of the empirical risk. Considering the case when W = W \u2217 \u2208 Rd\u00d7k, for all j, l \u2208 [k], we have,\n\u22022fD(W \u2217)\n\u2202wj\u2202wl = E (x,y)\u223cD\n[ v\u2217j v \u2217 l \u03c6 \u2032(w\u2217>j x)\u03c6 \u2032(w\u2217>l x)xx > ] .\nIf Property 3.3(b) is satisfied, \u03c6\u2032\u2032(z) = 0 almost surely. So in this case the diagonal blocks of the empirical Hessian can be written as,\n\u22022f\u0302S(W )\n\u2202w2j =\n1 |S| \u2211\n(x,y)\u2208S\n(v\u2217j\u03c6 \u2032(w>j x)) 2xx>.\nNow we show the Hessian of the objective near the global optimum is positive definite.\nDefinition 4.1. Given the ground truth matrix W \u2217 \u2208 Rd\u00d7k, let \u03c3i(W \u2217) denote the i-th singular value of W \u2217, often abbreviated as \u03c3i. Let \u03ba = \u03c31/\u03c3k, \u03bb = ( \u220fk i=1 \u03c3i)/\u03c3 k k . Let vmax denote maxi\u2208[k] |v\u2217i | and vmin denote mini\u2208[k] |v\u2217i | . Let \u03bd = vmax/vmin. Let \u03c1 denote \u03c1(\u03c3k). Let \u03c4 = (3\u03c31/2) 4p/min\u03c3\u2208[\u03c3k/2,3\u03c31/2]{\u03c1 2(\u03c3)}.\nTheorem 4.2 (Informal version of Theorem D.1). For any W \u2208 Rd\u00d7k with \u2016W \u2212W \u2217\u2016 \u2264 poly(1/k, 1/\u03bb, 1/\u03bd, \u03c1/\u03c32p1 ) \u00b7 \u2016W \u2217\u2016, let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function satisfy Property 3.1,3.2,3.3. Then for any t \u2265 1, if |S| \u2265 d \u00b7 poly(log d, t, k, \u03bd, \u03c4, \u03bb, \u03c32p1 /\u03c1), we have with probability at least 1\u2212 d\u2212\u2126(t),\n\u2126(v2min\u03c1(\u03c3k)/(\u03ba 2\u03bb))I \u22072f\u0302S(W ) O(kv2max\u03c3 2p 1 )I.\nRemark 4.3. As we can see from Theorem 4.2, \u03c1(\u03c3k) from Property 3.2 plays an important role for positive definite (PD) property. Interestingly, many popular activations, like ReLU, sigmoid and tanh, have \u03c1(\u03c3k) > 0, while some simple functions like linear (\u03c6(z) = z) and square (\u03c6(z) = z2) functions have \u03c1(\u03c3k) = 0 and their Hessians are rank-deficient. Another important numbers are \u03ba and \u03bb, two different condition numbers of the weight matrix, which directly influences the positive definiteness. If W \u2217 is rank deficient, \u03bb \u2192 \u221e, \u03ba \u2192 \u221e and we don\u2019t have PD property. In the best case when W \u2217 is orthogonal, \u03bb = \u03ba = 1. In the worse case, \u03bb can be exponential in k. Also W should be close enough to W \u2217. In the next section, we provide tensor methods to initialize w\u2217i and v\u2217i such that they satisfy the conditions in Theorem 4.2.\nFor the PD property to hold, we need the samples to be independent of the current parameters. Therefore, we need to do resampling at each iteration to guarantee the convergence in iterative algorithms like gradient descent. The following theorem provides the linear convergence guarantee of gradient descent for smooth activations.\nTheorem 4.4 (Linear convergence of gradient descent, informal version of Theorem D.2). Let W be the current iterate satisfying \u2016W \u2212W \u2217\u2016 \u2264 poly(1/\u03bd, 1/k, 1/\u03bb, \u03c1/\u03c32p1 )\u2016W \u2217\u2016. Let S denote a set\nof i.i.d. samples from distribution D (defined in (1)) with |S| \u2265 d \u00b7poly(log d, t, k, \u03bd, \u03c4, \u03bb, \u03c32p1 /\u03c1) and let the activation function satisfy Property 3.1,3.2 and 3.3(a). Define m0 := \u0398(v2min\u03c1(\u03c3k)/(\u03ba 2\u03bb)) and M0 := \u0398(kv2max\u03c3 2p 1 ). If we perform gradient descent with step size 1/M0 on f\u0302S(W ) and obtain the next iterate,\nW\u0303 = W \u2212 1 M0 \u2207f\u0302S(W ),\nthen with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016W\u0303 \u2212W \u2217\u20162F \u2264 (1\u2212 m0 M0 )\u2016W \u2212W \u2217\u20162F .\nWe provide the proofs in the Appendix D.1"}, {"heading": "5 Tensor Methods for Initialization", "text": "In this section, we show that Tensor methods can recover the parameters W \u2217 to some precision and exactly recover v\u2217 for homogeneous activations.\nIt is known that most tensor problems are NP-hard [H\u00e5s90, HL13] or even hard to approximate [SWZ17b]. However, by making some assumptions, tensor decomposition method becomes efficient [AGH+14, WTSA15, WA16, SWZ16]. Here we utilize the noiseless assumption and Gaussian inputs assumption to show a provable and efficient tensor methods."}, {"heading": "5.1 Preliminary", "text": "Let\u2019s define a special outer product \u2297\u0303 for simplification of the notation. If v \u2208 Rd is a vector and I is the identity matrix, then v\u2297\u0303I = \u2211d j=1[v\u2297 ej \u2297 ej + ej \u2297 v\u2297 ej + ej \u2297 ej \u2297 v]. If M is a symmetric\nrank-r matrix factorized as M = \u2211r\ni=1 siviv > i and I is the identity matrix, then M\u2297\u0303I = r\u2211 i=1 si d\u2211 j=1 6\u2211 l=1 Al,i,j ,\nwhere A1,i,j = vi\u2297vi\u2297ej\u2297ej , A2,i,j = vi\u2297ej\u2297vi\u2297ej , A3,i,j = ej\u2297vi\u2297vi\u2297ej , A4,i,j = vi\u2297ej\u2297ej\u2297vi, A5,i,j = ej \u2297 vi \u2297 ej \u2297 vi and A6,i,j = ej \u2297 ej \u2297 vi \u2297 vi.\nDenote w = w/\u2016w\u2016. Now let\u2019s calculate some moments.\nDefinition 5.1. We define M1,M2,M3,M4 and m1,i,m2,i,m3,i,m4,i as follows : M1 = E(x,y)\u223cD[y \u00b7 x]. M2 = E(x,y)\u223cD[y \u00b7 (x\u2297 x\u2212 I)]. M3 = E(x,y)\u223cD[y \u00b7 (x\u22973 \u2212 x\u2297\u0303I)]. M4 = E(x,y)\u223cD[y \u00b7 (x\u22974 \u2212 (x\u2297 x)\u2297\u0303I + I\u2297\u0303I)]. \u03b3j(\u03c3) = Ez\u223cN (0,1)[\u03c6(\u03c3 \u00b7 z)zj ], \u2200j = 0, 1, 2, 3, 4. m1,i = \u03b31(\u2016w\u2217i \u2016). m2,i = \u03b32(\u2016w\u2217i \u2016)\u2212 \u03b30(\u2016w\u2217i \u2016). m3,i = \u03b33(\u2016w\u2217i \u2016)\u2212 3\u03b31(\u2016w\u2217i \u2016). m4,i = \u03b34(\u2016w\u2217i \u2016) + 3\u03b30(\u2016w\u2217i \u2016)\u2212 6\u03b32(\u2016w\u2217i \u2016).\nAccording to Definition 5.1, we have the following results, Claim 5.2. For each j \u2208 [4], Mj = \u2211k i=1 v \u2217 imj,iw \u2217\u2297j i .\nNote that some mj,i\u2019s will be zero for specific activations. For example, for activations with symmetric first derivatives, i.e., \u03c6\u2032(z) = \u03c6\u2032(\u2212z), like sigmoid and erf, we have \u03c6(z) + \u03c6(\u2212z) being a constant and M2 = 0 since \u03b30(\u03c3) = \u03b32(\u03c3). Another example is ReLU. ReLU functions have vanishing M3, i.e., M3 = 0, as \u03b33(\u03c3) = 3\u03b31(\u03c3). To make tensor methods work, we make the following assumption.\nAssumption 5.3. Assume the activation function \u03c6(z) satisfies the following conditions: 1. If Mj 6= 0, then mj,i 6= 0 for all i \u2208 [k]. 2. At least one of M3 and M4 is non-zero. 3. If M1 = M3 = 0, then \u03c6(z) is an even function, i.e., \u03c6(z) = \u03c6(\u2212z). 4. If M2 = M4 = 0, then \u03c6(z) is an odd function, i.e., \u03c6(z) = \u2212\u03c6(\u2212z).\nIf \u03c6(z) is an odd function then \u03c6(z) = \u2212\u03c6(\u2212z) and v\u03c6(w>x) = \u2212v\u03c6(\u2212w>x). Hence we can always assume v > 0. If \u03c6(z) is an even function, then v\u03c6(w>x) = v\u03c6(\u2212w>x). So if w recovers w\u2217 then \u2212w also recovers w\u2217. Note that ReLU, leaky ReLU and squared ReLU satisfy Assumption 5.3. We further define the following non-zero moments.\nDefinition 5.4. Let \u03b1 \u2208 Rd denote a randomly picked vector. We define P2 and P3 as follows: P2 = Mj2(I, I, \u03b1, \u00b7 \u00b7 \u00b7 , \u03b1) , where j2 = min{j \u2265 2|Mj 6= 0} and P3 = Mj3(I, I, I, \u03b1, \u00b7 \u00b7 \u00b7 , \u03b1), where j3 = min{j \u2265 3|Mj 6= 0}.\nAccording to Definition 5.1 and 5.4, we have, Claim 5.5. P2 = \u2211k i=1 v \u2217 imj2,i(\u03b1 >w\u2217i ) j2\u22122w\u2217\u22972i and P3 = \u2211k i=1 v \u2217 imj3,i(\u03b1 >w\u2217i ) j3\u22123w\u2217\u22973i .\nIn other words for the above definition, P2 is equal to the first non-zero matrix in the ordered sequence {M2,M3(I, I, \u03b1),M4(I, I, \u03b1, \u03b1)}. P3 is equal to the first non-zero tensor in the ordered sequence {M3,M4(I, I, I, \u03b1)}. Since \u03b1 is randomly picked up, w\u2217>i \u03b1 6= 0 and we view this number as a constant throughout this paper. So by construction and Assumption 5.3, both P2 and P3 are rank-k. Also, let P\u03022 \u2208 Rd\u00d7d and P\u03023 \u2208 Rd\u00d7d\u00d7d denote the corresponding empirical moments of P2 \u2208 Rd\u00d7d and P3 \u2208 Rd\u00d7d\u00d7d respectively."}, {"heading": "5.2 Algorithm", "text": "Now we briefly introduce how to use a set of samples with size linear in dimension to recover the ground truth parameters to some precision. As shown in the previous section, we have a rank-k 3rd-order moment P3 that has tensor decomposition formed by {w\u22171, w\u22172, \u00b7 \u00b7 \u00b7 , w\u2217k}. Therefore, we can use the non-orthogonal decomposition method [KCL15] to decompose the corresponding estimated tensor P\u03023 and obtain an approximation of the parameters. The precision of the obtained parameters depends on the estimation error of P3, which requires \u2126(d3/ 2) samples to achieve error. Also, the time complexity for tensor decomposition on a d\u00d7 d\u00d7 d tensor is \u2126(d3).\nIn this paper, we reduce the cubic dependency of sample/computational complexity in dimension [JSA15] to linear dependency. Our idea follows the techniques used in [ZJD16], where they first used a 2nd-order moment P2 to approximate the subspace spanned by {w\u22171, w\u22172, \u00b7 \u00b7 \u00b7 , w\u2217k}, denoted as V , then use V to reduce a higher-dimensional third-order tensor P3 \u2208 Rd\u00d7d\u00d7d to a lower-dimensional tensor R3 := P3(V, V, V ) \u2208 Rk\u00d7k\u00d7k. Since the tensor decomposition and the tensor estimation are conducted on a lower-dimensional Rk\u00d7k\u00d7k space, the sample complexity and computational complexity are reduced.\nThe detailed algorithm is shown in Algorithm 1. First, we randomly partition the dataset into three subsets each with size O\u0303(d). Then apply the power method on P\u03022, which is the estimation\nAlgorithm 1 Initialization via Tensor Method 1: procedure Initialization(S) . Theorem 5.6 2: S2, S3, S4 \u2190 Partition(S, 3) 3: P\u03022 \u2190 ES2 [P2] 4: V \u2190 PowerMethod(P\u03022, k) 5: R\u03023 \u2190 ES3 [P3(V, V, V )] 6: {u\u0302i}i\u2208[k] \u2190 KCL(R\u03023) 7: {w(0)i , v (0) i }i\u2208[k] \u2190 RecMagSign(V, {u\u0302i}i\u2208[k], S4)\n8: Return {w(0)i , v (0) i }i\u2208[k] 9: end procedure\nAlgorithm 2 Globally Converging Algorithm 1: procedure Learning1NN(S, d, k, ) . Theorem 6.1 2: T \u2190 log(1/ ) \u00b7 poly(k, \u03bd, \u03bb, \u03c32p1 /\u03c1). 3: \u03b7 \u2190 1/(kv2max\u03c3 2p 1 ).\n4: S0, S1, \u00b7 \u00b7 \u00b7 , Sq \u2190 Partition(S, q + 1). 5: W (0), v(0) \u2190 Initialization(S0). 6: Set v\u2217i \u2190 v (0) i in Eq. (2) for all f\u0302Sq(W ), q \u2208 [T ] 7: for q = 0, 1, 2, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 8: W (q+1) = W (q) \u2212 \u03b7\u2207f\u0302Sq+1(W (q)) 9: end for\n10: Return {w(T )i , v (0) i }i\u2208[k] 11: end procedure\nof P2 from S2, to estimate V . After that, the non-orthogonal tensor decomposition (KCL)[KCL15] on R\u03023 outputs u\u0302i which estimates siV >w\u2217i for i \u2208 [k] with unknown sign si \u2208 {\u22121, 1}. Hence w\u2217i can be estimated by siV u\u0302i. Finally we estimate the magnitude of w\u2217i and the signs si, v \u2217 i in the RecMagSign function for homogeneous activations. We discuss the details of each procedure and provide PowerMethod and RecMagSign algorithms in Appendix E."}, {"heading": "5.3 Theoretical Analysis", "text": "We formally present our theorem for Algorithm 1, and provide the proof in the Appendix E.2.\nTheorem 5.6. Let the activation function be homogeneous satisfying Assumption 5.3. For any 0 < < 1 and t \u2265 1, if |S| \u2265 \u22122 \u00b7d \u00b7poly(t, k, \u03ba, log d), then there exists an algorithm (Algorithm 1) that takes |S|k \u00b7 O\u0303(d) time and outputs a matrix W (0) \u2208 Rd\u00d7k and a vector v(0) \u2208 Rk such that, with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016W (0) \u2212W \u2217\u2016F \u2264 \u00b7 poly(k, \u03ba)\u2016W \u2217\u2016F , and v(0)i = v \u2217 i ."}, {"heading": "6 Global Convergence", "text": "Combining the positive definiteness of the Hessian near the global optimal in Section 4 and the tensor initialization methods in Section 5, we come up with the overall globally converging algorithm Algorithm 2 and its guarantee Theorem 6.1.\nTheorem 6.1 (Global convergence guarantees). Let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function be homogeneous satisfying Property 3.1, 3.2, 3.3(a) and Assumption 5.3. Then for any t \u2265 1 and any > 0, if |S| \u2265 d log(1/ )\u00b7poly(log d, t, k, \u03bb), T \u2265 log(1/ ) \u00b7 poly(k, \u03bd, \u03bb, \u03c32p1 /\u03c1) and 0 < \u03b7 \u2264 1/(kv2max\u03c3 2p 1 ), then there is an Algorithm (procedure Learning1NN in Algorithm 2) taking |S| \u00b7 d \u00b7 poly(log d, k, \u03bb) time and outputting a matrix W (T ) \u2208 Rd\u00d7k and a vector v(0) \u2208 Rk satisfying\n\u2016W (T ) \u2212W \u2217\u2016F \u2264 \u2016W \u2217\u2016F , and v(0)i = v \u2217 i .\nwith probability at least 1\u2212 d\u2212\u2126(t).\nThis follows by combining Theorem 4.4 and Theorem 5.6."}, {"heading": "7 Numerical Experiments", "text": "In this section we use synthetic data to verify our theoretical results. We generate data points {xi, yi}i=1,2,\u00b7\u00b7\u00b7 ,n from Distribution D(defined in Eq. (1)). We set W \u2217 = U\u03a3V >, where U \u2208 Rd\u00d7k and V \u2208 Rk\u00d7k are orthogonal matrices generated from QR decomposition of Gaussian matrices, \u03a3 is a diagonal matrix whose diagonal elements are 1, 1+\u03ba\u22121k\u22121 , 1+ 2(\u03ba\u22121) k\u22121 , \u00b7 \u00b7 \u00b7 , \u03ba. In this experiment, we set \u03ba = 2 and k = 5. We set v\u2217i to be randomly picked from {\u22121, 1} with equal chance. We use squared ReLU \u03c6(z) = max{z, 0}2, which is a smooth homogeneous function. For non-orthogonal tensor methods, we directly use the code provided by [KCL15] with the number of random projections fixed as L = 100. We pick the stepsize \u03b7 = 0.02 for gradient descent. In the experiments, we don\u2019t do the resampling since the algorithm still works well without resampling.\nFirst we show the number of samples required to recover the parameters for different dimensions. We fix k = 5, change d for d = 10, 20, \u00b7 \u00b7 \u00b7 , 100 and n for n = 1000, 2000, \u00b7 \u00b7 \u00b7 , 10000. For each pair of d and n, we run 10 trials. We say a trial successfully recovers the parameters if there exists a permutation \u03c0 : [k]\u2192 [k], such that the returned parameters W and v satisfy\nmax j\u2208[k] {\u2016w\u2217j \u2212 w\u03c0(j)\u2016/\u2016w\u2217j\u2016} \u2264 0.01 and v\u03c0(j) = v\u2217j .\nWe record the recovery rates and represent them as grey scale in Fig. 1(a). As we can see from Fig. 1(a), the least number of samples required to have 100% recovery rate is about proportional to the dimension.\nNext we test the tensor initialization. We show the error between the output of the tensor method and the ground truth parameters against the number of samples under different dimensions\nin Fig 1(b). The pure dark blocks indicate, in at least one of the 10 trials, \u2211k\ni=1 v (0) i 6= \u2211k i=1 v \u2217 i ,\nwhich means v(0)i is not correctly initialized. Let \u03a0(k) denote the set of all possible permutations \u03c0 : [k]\u2192 [k]. The grey scale represents the averaged error,\nmin \u03c0\u2208\u03a0(k) max j\u2208[k] {\u2016w\u2217j \u2212 w (0) \u03c0(j)\u2016/\u2016w \u2217 j\u2016},\nover 10 trials. As we can see, with a fixed dimension, the more samples we have the better initialization we obtain. We can also see that to achieve the same initialization error, the sample complexity required is about proportional to the dimension.\nWe also compare different initialization methods for gradient descent in Fig. 1(c). We fix d = 10, k = 5, n = 10000 and compare three different initialization approaches, (I) Let both v and W be initialized from tensor methods, and then do gradient descent for W while v is fixed; (II) Let both v and W be initialized from random Gaussian, and then do gradient descent for both W and v; (III) Let v = v\u2217 and W be initialized from random Gaussian, and then do gradient descent for W while v is fixed. As we can see from Fig 1(c), Approach (I) is the fastest and Approach (II) doesn\u2019t converge even if more iterations are allowed. Both Approach (I) and (III) have linear convergence rate when the objective value is small enough, which verifies our local linear convergence claim."}, {"heading": "8 Conclusion", "text": "As shown in Theorem 6.1, the tensor initialization followed by gradient descent will provide a globally converging algorithm with linear time/sample complexity in dimension, logarithmic in precision and polynomial in other factors for smooth homogeneous activation functions. Our distilled properties for activation functions include a wide range of non-linear functions and hopefully provide an intuition to understand the role of non-linear activations played in optimization. Deeper neural networks and convergence for SGD will be considered in the future."}, {"heading": "A Notation", "text": "For any positive integer n, we use [n] to denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. For random variable X, let E[X] denote the expectation of X (if this quantity exists). For any vector x \u2208 Rn, we use \u2016x\u2016 to denote its `2 norm.\nWe provide several definitions related to matrix A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A\u2020 denote the Moore-Penrose pseudoinverse of A. Let A\u22121 denote the inverse of a full rank square matrix. Let \u2016A\u2016F denote the Frobenius norm of matrix A. Let \u2016A\u2016 denote the spectral norm of matrix A. Let \u03c3i(A) to denote the i-th largest singular value of A. We often use capital letter to denote the stack of corresponding small letter vectors, e.g.,W = [w1 w2 \u00b7 \u00b7 \u00b7 wk]. For two same-size matrices, A,B \u2208 Rd1\u00d7d2 , we use A\u25e6B \u2208 Rd1\u00d7d2 to denote element-wise multiplication of these two matrices.\nWe use \u2297 to denote outer product and \u00b7 to denote dot product. Given two column vectors u, v \u2208 Rn, then u\u2297v \u2208 Rn\u00d7n and (u\u2297v)i,j = ui \u00b7vj , and u>v = \u2211n i=1 uivi \u2208 R. Given three column vectors u, v, w \u2208 Rn, then u\u2297 v\u2297w \u2208 Rn\u00d7n\u00d7n and (u\u2297 v\u2297w)i,j,k = ui \u00b7 vj \u00b7wk. We use u\u2297r \u2208 Rn r to denote the vector u outer product with itself r \u2212 1 times. Tensor T \u2208 Rn\u00d7n\u00d7n is symmetric if and only if for any i, j, k, Ti,j,k = Ti,k,j = Tj,i,k = Tj,k,i = Tk,i,j = Tk,j,i. Given a third order tensor T \u2208 Rn1\u00d7n2\u00d7n3 and three matrices A \u2208 Rn1\u00d7d1 , B \u2208 Rn2\u00d7d2 , C \u2208 Rn3\u00d7d3 , we use T (A,B,C) to denote a d1 \u00d7 d2 \u00d7 d3 tensor where the (i, j, k)-th entry is,\nn1\u2211 i\u2032=1 n2\u2211 j\u2032=1 n3\u2211 k\u2032=1 Ti\u2032,j\u2032,k\u2032Ai\u2032,iBj\u2032,jCk\u2032,k.\nWe use \u2016T\u2016 to denote the operator norm of the tensor T , i.e.,\n\u2016T\u2016 = max \u2016a\u2016=1 |T (a, a, a)|.\nFor tensor T \u2208 Rn1\u00d7n2\u00d7n3 , we use matrix T (1) \u2208 Rn1\u00d7n2n3 to denote the flattening of tensor T along the first dimension, i.e., [T (1)]i,(j\u22121)n3+k = Ti,j,k, \u2200i \u2208 [n1], j \u2208 [n2], k \u2208 [n3]. Similarly for matrices T (2) \u2208 Rn2\u00d7n3n1 and T (3) \u2208 Rn3\u00d7n1n2 .\nWe use 1f to denote the indicator function, which is 1 if f holds and 0 otherwise. Let Id \u2208 Rd\u00d7d denote the identity matrix. We use \u03c6(z) to denote an activation function. We define (z)+ := max{0, z}. We use D to denote a Gaussian distribution N (0, Id) or to denote a joint distribution of (X,Y ) \u2208 Rd \u00d7 R, where the marginal distribution of X is N (0, Id).\nFor any function f , we define O\u0303(f) to be f \u00b7 logO(1)(f). In addition to O(\u00b7) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f \u2264 Cg (resp. \u2265) for an absolute constant C. We use f h g to mean cf \u2264 g \u2264 Cf for constants c, C."}, {"heading": "B Preliminaries", "text": "In this section, we introduce some lemmata and corollaries that will be used in the proofs."}, {"heading": "B.1 Useful Facts", "text": "We provide some facts that will be used in the later proofs.\nFact B.1. Let z denote a fixed d-dimensional vector, then for any C \u2265 1 and n \u2265 1, we have\nPr x\u223cN (0,Id)\n[|\u3008x, z\u3009|2 \u2264 5C\u2016z\u20162 log n] \u2265 1\u2212 1/(ndC).\nProof. This follows by Proposition 1.1 in [HKZ12].\nFact B.2. For any C \u2265 1 and n \u2265 1, we have\nPr x\u223cN (0,Id)\n[\u2016x\u20162 \u2264 5Cd log n] \u2265 1\u2212 1/(ndC).\nProof. This follows by Proposition 1.1 in [HKZ12].\nFact B.3. Given a full column-rank matrix W = [w1, w2, \u00b7 \u00b7 \u00b7 , wk] \u2208 Rd\u00d7k, let W = [ w1\u2016w1\u2016 , w2 \u2016w2\u2016 , \u00b7 \u00b7 \u00b7 , wk\u2016wk\u2016 ]. Then, we have: (I) for any i \u2208 [k], \u03c3k(W ) \u2264 \u2016wi\u2016 \u2264 \u03c31(W ); (II) 1/\u03ba(W ) \u2264 \u03c3k(W ) \u2264 \u03c31(W ) \u2264 \u221a k.\nProof. Part (I). We have, \u03c3k(W ) \u2264 \u2016Wei\u2016 = \u2016wi\u2016 \u2264 \u03c31(W )\nPart (II). We first show how to lower bound \u03c3k(W ),\n\u03c3k(W ) = min \u2016s\u2016=1\n\u2016Ws\u2016\n= min \u2016s\u2016=1 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 si \u2016wi\u2016 wi \u2225\u2225\u2225\u2225\u2225 by definition of W \u2265 min \u2016s\u2016=1 \u03c3k(W ) ( k\u2211 i=1 ( si \u2016wi\u2016 )2 ) 1 2 by \u2016wi\u2016 \u2265 \u03c3k(W )\n\u2265 min \u2016s\u20162=1 \u03c3k(W ) ( k\u2211 i=1 ( si maxj\u2208[k] \u2016wj\u2016 )2 ) 1 2\nby max j\u2208[k] \u2016wj\u2016 \u2265 \u2016wi\u2016\n= \u03c3k(W )/max j\u2208[k] \u2016wj\u2016 by \u2016s\u2016 = 1\n\u2265 \u03c3k(W )/\u03c31(W ). by max j\u2208[k] \u2016wj\u2016 \u2264 \u03c31(W )\n= 1/\u03ba(W ).\nIt remains to upper bound \u03c31(W ),\n\u03c31(W ) \u2264 ( k\u2211 i=1 \u03c32i (W ) ) 1 2 = \u2016W\u2016F \u2264 \u221a k.\nFact B.4. Let U \u2208 Rd\u00d7k and V \u2208 Rd\u00d7k (k \u2264 d) denote two orthogonal matrices. Then \u2016UU> \u2212 V V >\u2016 = \u2016(I \u2212 UU>)V \u2016 = \u2016(I \u2212 V V >)U\u2016 = \u221a 1\u2212 \u03c32k(U>V ).\nProof. Let U\u22a5 \u2208 Rd\u00d7(d\u2212k) and V\u22a5 \u2208 Rd\u00d7(d\u2212k) be the orthogonal complementary matrices of U, V \u2208 Rd\u00d7k respectively.\n\u2016UU> \u2212 V V >\u2016 = \u2016(I \u2212 V V >)UU> \u2212 V V >(I \u2212 UU>)\u2016 = \u2016V\u22a5V >\u22a5 UU> \u2212 V V >U\u22a5U>\u22a5 \u2016\n= \u2225\u2225\u2225\u2225[V\u22a5 V ] [V >\u22a5 U 00 \u2212V >U\u22a5 ] [ U> U>\u22a5 ]\u2225\u2225\u2225\u2225 = max(\u2016V >\u22a5 U\u2016, \u2016V >U\u22a5\u2016).\nWe show how to simplify \u2016V >\u22a5 U\u2016, \u2016V >\u22a5 U\u2016 = \u2016(I \u2212 V V >)U\u2016 = \u221a \u2016U>(I \u2212 V V >)U\u2016 = max\n\u2016a\u2016=1\n\u221a 1\u2212 \u2016V >Ua\u20162 = \u221a 1\u2212 \u03c32k(V >U).\nSimilarly we can simplify \u2016U>\u22a5V \u2016, \u2016U>\u22a5V \u2016 = \u221a 1\u2212 \u03c32k(U>V ) = \u221a 1\u2212 \u03c32k(V >U).\nFact B.5. Let C \u2208 Rd1\u00d7d2 , B \u2208 Rd2\u00d7d3 be two matrices. Then \u2016CB\u2016 \u2264 \u2016C\u2016\u2016B\u2016F and \u2016CB\u2016 \u2265 \u03c3min(C)\u2016B\u2016F . Proof. For each i \u2208 [d3], let bi denote the i-th column of B. We can upper bound \u2016CB\u2016,\n\u2016CB\u2016F = ( d2\u2211 i=1 \u2016Cbi\u20162 )1/2 \u2264 ( d2\u2211 i=1 \u2016C\u20162\u2016bi\u20162 )1/2 = \u2016C\u2016\u2016B\u2016F .\nWe show how to lower bound \u2016CB\u2016,\n\u2016CB\u2016 = ( d2\u2211 i=1 \u2016Cbi\u20162 )1/2 \u2265 ( d2\u2211 i=1 \u03c32k(C)\u2016bi\u20162 )1/2 = \u03c3min(C)\u2016B\u2016F .\nFact B.6. Let a, b, c \u2265 0 denote three constants, let u, v, w \u2208 Rd denote three vectors, let Dd denote Gaussian distribution N (0, Id) then\nE x\u223cDd\n[ |u>x|a|v>x|b|w>x|c ] h \u2016u\u2016a\u2016v\u2016b\u2016w\u2016c.\nProof.\nE x\u223cDd\n[ |u>x|a|v>x|b|w>x|c ] \u2264 (\nE x\u223cDd\n[|u>x|2a] )1/2 \u00b7 (\nE x\u223cDd\n[|u>x|4b] )1/4 \u00b7 (\nE x\u223cDd\n[|u>x|4c] )1/4\n. \u2016u\u2016a\u2016v\u2016b\u2016w\u2016c,\nwhere the first step follows by H\u00f6lder\u2019s inequality, i.e., E[|XY Z|] \u2264 (E[|X|2])1/2 \u00b7 (E[|Y |4])1/4 \u00b7 (E[|Z|4])1/4, the third step follows by calculating the expectation and a, b, c are constants.\nSince all the three components |u>x|, |v>x|, |w>x| are positive and related to a common random vector x, we can show a lower bound,\nE x\u223cDd\n[ |u>x|a|v>x|b|w>x|c ] & \u2016u\u2016a\u2016v\u2016b\u2016w\u2016c."}, {"heading": "B.2 Matrix Bernstein", "text": "In many proofs we need to bound the difference between some population matrices/tensors and their empirical versions. Typically, the classic matrix Bernstein inequality requires the norm of the random matrix be bounded almost surely (e.g., Theorem 6.1 in [Tro12]) or the random matrix satisfies subexponential property (Theorem 6.2 in [Tro12]) . However, in our cases, most of the random matrices don\u2019t satisfy these conditions. So we derive the following lemmata that can deal with random matrices that are not bounded almost surely or follow subexponential distribution, but are bounded with high probability.\nLemma B.7 (Matrix Bernstein for unbounded case (A modified version of bounded case, Theorem 6.1 in [Tro12])). Let B denote a distribution over Rd1\u00d7d2. Let d = d1 + d2. Let B1, B2, \u00b7 \u00b7 \u00b7Bn be i.i.d. random matrices sampled from B. Let B = EB\u223cB[B] and B\u0302 = 1n \u2211n i=1Bi. For parameters m \u2265 0, \u03b3 \u2208 (0, 1), \u03bd > 0, L > 0, if the distribution B satisfies the following four properties,\n(I) Pr B\u223cB\n[\u2016B\u2016 \u2264 m] \u2265 1\u2212 \u03b3;\n(II)\n\u2225\u2225\u2225\u2225 EB\u223cB[B] \u2225\u2225\u2225\u2225 > 0;\n(III) max (\u2225\u2225\u2225\u2225 EB\u223cB[BB>] \u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225 EB\u223cB[B>B] \u2225\u2225\u2225\u2225) \u2264 \u03bd; (IV) max\n\u2016a\u2016=\u2016b\u2016=1\n( E\nB\u223cB\n[( a>Bb )2])1/2 \u2264 L.\nThen we have for any 0 < < 1 and t \u2265 1, if\nn \u2265 (18t log d) \u00b7 (\u03bd + \u2016B\u20162 +m\u2016B\u2016 )/( 2\u2016B\u20162) and \u03b3 \u2264 ( \u2016B\u2016/(2L))2\nwith probability at least 1\u2212 1/d2t \u2212 n\u03b3,\n\u2016B\u0302 \u2212B\u2016 \u2264 \u2016B\u2016.\nProof. Define the event \u03bei = {\u2016Bi\u2016 \u2264 m},\u2200i \u2208 [n].\nDefine Mi = 1\u2016Bi\u2016\u2264mBi. Let M = EB\u223cB[1\u2016B\u2016\u2264mB] and M\u0302 = 1 n \u2211n i=1Mi. By triangle inequality, we have\n\u2016B\u0302 \u2212B\u2016 \u2264 \u2016B\u0302 \u2212 M\u0302\u2016+ \u2016M\u0302 \u2212M\u2016+ \u2016M \u2212B\u2016. (4)\nIn the next a few paragraphs, we will upper bound the above three terms. The first term in Eq. (4). Denote \u03bec as the complementary set of \u03be, thus Pr[\u03beci ] \u2264 \u03b3. By a union bound over i \u2208 [n], with probability 1\u2212 n\u03b3, \u2016Bi\u2016 \u2264 m for all i \u2208 [n]. Thus M\u0302 = B\u0302. The second term in Eq. (4). For a matrix B sampled from B, we use \u03be to denote the event\nthat \u03be = {\u2016B\u2016 \u2264 m}. Then, we can upper bound \u2016M \u2212B\u2016 in the following way,\n\u2016M \u2212B\u2016\n= \u2225\u2225\u2225\u2225 EB\u223cB[1\u2016B\u2016\u2264m \u00b7B]\u2212 EB\u223cB[B] \u2225\u2225\u2225\u2225\n= \u2225\u2225\u2225\u2225 EB\u223cB[B \u00b7 1\u03bec ] \u2225\u2225\u2225\u2225\n= max \u2016a\u2016=\u2016b\u2016=1 E B\u223cB\n[a>Bb1\u03bec ]\n\u2264 max \u2016a\u2016=\u2016b\u2016=1 E B\u223cB [(a>Bb)2]1/2 \u00b7 E B\u223cB [1\u03bec ] 1/2 by H\u00f6lder\u2019s inequality\n\u2264 L E B\u223cB [1\u03bec ] 1/2 by Property (IV) \u2264 L\u03b31/2, by Pr[\u03bec] \u2264 \u03b3\n\u2264 1 2 \u2016B\u2016, by \u03b3 \u2264 ( \u2016B\u2016/(2L))2\nwhich implies\n\u2016M \u2212B\u2016 \u2264 2 \u2016B\u2016.\nSince < 1, we also have \u2016M \u2212B\u2016 \u2264 12\u2016B\u2016 and 3 2\u2016B\u2016 \u2265 \u2016M\u2016 \u2265 1 2\u2016B\u2016.\nThe third term in Eq. (4). We can bound \u2016M\u0302\u2212M\u2016 by Matrix Bernstein\u2019s inequality [Tro12]. We define Zi = Mi \u2212M . Thus we have E\nBi\u223cB [Zi] = 0, \u2016Zi\u2016 \u2264 2m, and\u2225\u2225\u2225\u2225 EBi\u223cB[ZiZ>i ] \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 EBi\u223cB[MiM>i ]\u2212M \u00b7M> \u2225\u2225\u2225\u2225 \u2264 \u03bd + \u2016M\u20162 \u2264 \u03bd + 3\u2016B\u20162.\nSimilarly, we have \u2225\u2225\u2225\u2225 EBi\u223cB[Z>i Zi] \u2225\u2225\u2225\u2225 \u2264 \u03bd + 3\u2016B\u20162. Using matrix Bernstein\u2019s inequality, for any > 0, Pr\nB1,\u00b7\u00b7\u00b7 ,Bn\u223cB\n[ 1\nn \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 Zi \u2225\u2225\u2225\u2225\u2225 \u2265 \u2016B\u2016 ] \u2264 d exp ( \u2212 2\u2016B\u20162n/2 \u03bd + 3\u2016B\u20162 + 2m\u2016B\u2016 /3 ) .\nBy choosing\nn \u2265 (3t log d) \u00b7 \u03bd + 3\u2016B\u2016 2 + 2m\u2016B\u2016 /3\n2\u2016B\u20162/2 ,\nfor t \u2265 1, we have with probability at least 1\u2212 1/d2t,\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Mi \u2212M \u2225\u2225\u2225\u2225\u2225 \u2264 2\u2016B\u2016 Putting it all together, we have for 0 < < 1, if\nn \u2265 (18t log d) \u00b7 (\u03bd + \u2016B\u20162 +m\u2016B\u2016 )/( 2\u2016B\u20162) and \u03b3 \u2264 ( \u2016B\u2016/(2L))2\nwith probability at least 1\u2212 1/d2t \u2212 n\u03b3,\u2225\u2225\u2225\u2225\u2225 1n n\u2211 i=1 Bi \u2212 E B\u223cB [B] \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225 EB\u223cB[B] \u2225\u2225\u2225\u2225 .\nCorollary B.8 (Error bound for symmetric rank-one random matrices). Let x1, x2, \u00b7 \u00b7 \u00b7xn denote n i.i.d. samples drawn from Gaussian distribution N (0, Id). Let h(x) : Rd \u2192 R be a function satisfying the following properties (I), (II) and (III).\n(I) Pr x\u223cN (0,Id)\n[|h(x)| \u2264 m] \u2265 1\u2212 \u03b3\n(II) \u2225\u2225\u2225\u2225 Ex\u223cN (0,Id)[h(x)xx>] \u2225\u2225\u2225\u2225 > 0;\n(III) ( E\nx\u223cN (0,Id) [h4(x)]\n)1/4 \u2264 L.\nDefine function B(x) = h(x)xx> \u2208 Rd\u00d7d, \u2200i \u2208 [n]. Let B = E x\u223cN (0,Id) [h(x)xx>]. For any\n0 < < 1 and t \u2265 1, if\nn & (t log d) \u00b7 (L2d+ \u2016B\u20162 + (mtd log n)\u2016B\u2016 )/( 2\u2016B\u20162), and \u03b3 + 1/(nd2t) . ( \u2016B\u2016/L)2\nthen\nPr x1,\u00b7\u00b7\u00b7 ,xn\u223cN (0,Id) [\u2225\u2225\u2225\u2225\u2225B \u2212 1n n\u2211 i=1 B(xi) \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016B\u2016 ] \u2265 1\u2212 2/(d2t)\u2212 n\u03b3.\nProof. We show that the four Properties in Lemma B.7 are satisfied. Define function B(x) = h(x)xx>.\n(I) \u2016B(x)\u2016 = \u2016h(x)xx>\u2016 = |h(x)|\u2016x\u20162. By using Fact B.2, we have\nPr x\u223cN (0,Id)\n[\u2016x\u20162 \u2264 10td log n] \u2265 1\u2212 1/(nd2t)\nTherefore,\nPr x\u223cN (0,Id)\n[\u2016B(x)\u2016 \u2264 m \u00b7 10td log(n)] \u2265 1\u2212 \u03b3 \u2212 1/(nd2t).\n(II) \u2225\u2225\u2225\u2225 EB\u223cB[B] \u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 Ex\u223cN (0,Id)[h(x)xx>] \u2225\u2225\u2225\u2225 > 0. (III)\nmax (\u2225\u2225\u2225\u2225 EB\u223cB[BB>] \u2225\u2225\u2225\u2225 , \u2225\u2225\u2225\u2225 EB\u223cB[B>B] \u2225\u2225\u2225\u2225) = max \u2016a\u2016=1 E x\u223cN (0,Id) [(h(x))2\u2016x\u20162(a>x)2]\n\u2264 (\nE x\u223cN (0,Id) [(h(x))4]\n)1/2 \u00b7 (\nE x\u223cN (0,Id)\n[\u2016x\u20168] )1/4\n\u00b7 max \u2016a\u2016=1\n( E\nx\u223cN (0,Id) [(a>x)8] )1/4 . L2d.\n(IV)\nmax \u2016a\u2016=\u2016b\u2016=1\n( E\nB\u223cB [(a>Bb)2] )1/2 = max \u2016a\u2016=1 ( E x\u223cN (0,Id) [h2(x)(a>x)4]\n)1/2 \u2264 (\nE x\u223cN (0,Id) [h4(x)] )1/4 \u00b7 max \u2016a\u2016=1 ( E x\u223cN (0,Id) [(a>x)8] )1/4 . L.\nApplying Lemma B.7, we obtain, for any 0 < < 1 and t \u2265 1, if\nn & (t log d) \u00b7 (L2d+ \u2016B\u20162 + (mtd log n)\u2016B\u2016 )/( 2\u2016B\u20162), and \u03b3 + 1/(nd2t) . ( \u2016B\u2016/L)2\nthen\nPr x1,\u00b7\u00b7\u00b7 ,xn\u223cN (0,Id) [\u2225\u2225\u2225\u2225\u2225B \u2212 1n n\u2211 i=1 B(xi) \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016B\u2016 ] \u2265 1\u2212 2/(d2t)\u2212 n\u03b3."}, {"heading": "C Properties of Activation Functions", "text": "Theorem 3.5. ReLU \u03c6(z) = max{z, 0}, leaky ReLU \u03c6(z) = max{z, 0.01z}, squared ReLU \u03c6(z) = max{z, 0}2 and any non-linear non-decreasing smooth functions with bounded symmetric \u03c6\u2032(z), like the sigmoid function \u03c6(z) = 1/(1 + e\u2212z), the tanh function and the erf function \u03c6(z) = \u222b z 0 e \u2212t2dt, satisfy Property 3.1,3.2,3.3. The linear function, \u03c6(z) = z, doesn\u2019t satisfy Property 3.2 and the quadratic function, \u03c6(z) = z2, doesn\u2019t satisfy Property 3.1 and 3.2.\nProof. We can easily verify that ReLU , leaky ReLU and squared ReLU satisfy Property 3.2 by calculating \u03c1(\u03c3) in Property 3.2, which is shown in Table 1. Property 3.1 for ReLU , leaky ReLU and squared ReLU can be verified since they are non-decreasing with bounded first derivative. ReLU and leaky ReLU are piece-wise linear, so they satisfy Property 3.3(b). Squared ReLU is smooth so it satisfies Property 3.3(a).\nSmooth non-decreasing activations with bounded first derivatives automatically satisfy Property 3.1 and 3.3. For Property 3.2, since their first derivatives are symmetric, we have E[\u03c6\u2032(\u03c3 \u00b7z)z] = 0. Then by H\u00f6lder\u2019s inequality and \u03c6\u2032(z) \u2265 0, we have\nE z\u223cD1\n[\u03c6\u20322(\u03c3 \u00b7 z)] \u2265 (\nE z\u223cD1\n[\u03c6\u2032(\u03c3 \u00b7 z)] )2 ,\nE z\u223cD1 [\u03c6\u20322(\u03c3 \u00b7 z)z2] \u00b7 E z\u223cD1\n[z2] \u2265 (\nE z\u223cD1\n[\u03c6\u2032(\u03c3 \u00b7 z)z2] )2 ,\nE z\u223cD1 [\u03c6\u2032(\u03c3 \u00b7 z)z2] \u00b7 E z\u223cD1 [\u03c6\u2032(\u03c3 \u00b7 z)] = E z\u223cD1\n[( \u221a \u03c6\u2032(\u03c3 \u00b7 z)z)2] \u00b7 E z\u223cD1 [( \u221a \u03c6\u2032(\u03c3 \u00b7 z))2] \u2265 ( E z\u223cD1 [\u03c6\u2032(\u03c3 \u00b7 z)z] )2 .\nThe equality in the first inequality happens when \u03c6\u2032(\u03c3 \u00b7 z) is a constant a.e.. The equality in the second inequality happens when |\u03c6\u2032(\u03c3 \u00b7z)| is a constant a.e., which is invalidated by the non-linearity and smoothness condition. The equality in the third inequality holds only when \u03c6\u2032(z) = 0 a.e., which leads to a constant function under non-decreasing condition. Therefore, \u03c1(\u03c3) > 0 for any smooth non-decreasing non-linear activations with bounded symmetric first derivatives. The statements about linear activations and quadratic activation follow direct calculations."}, {"heading": "D Local Positive Definiteness of Hessian", "text": ""}, {"heading": "D.1 Main Results for Positive Definiteness of Hessian", "text": ""}, {"heading": "D.1.1 Bounding the Spectrum of the Hessian near the Ground Truth", "text": "Theorem D.1 (Bounding the spectrum of the Hessian near the ground truth). For any W \u2208 Rd\u00d7k with \u2016W \u2212W \u2217\u2016 . v4min\u03c12(\u03c3k)/(k2\u03ba5\u03bb2v4max\u03c3 4p 1 ) \u00b7 \u2016W \u2217\u2016, let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function satisfy Property 3.1,3.2,3.3. Then for any t \u2265 1, if |S| \u2265 d \u00b7 poly(log d, t) \u00b7 k2v4max\u03c4\u03ba8\u03bb2\u03c3 4p 1 /(v 4 min\u03c1\n2(\u03c3k)), we have with probability at least 1\u2212 d\u2212\u2126(t),\n\u2126(v2min\u03c1(\u03c3k)/(\u03ba 2\u03bb))I \u22072f\u0302S(W ) O(kv2max\u03c3 2p 1 )I.\nProof. The main idea of the proof follows the following inequalities,\n\u22072fD(W \u2217)\u2212 \u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016I \u22072f\u0302S(W ) \u22072fD(W \u2217) + \u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016I\nThe proof sketch is first to bound the range of the eigenvalues of \u22072fD(W \u2217) (Lemma D.3) and then bound the spectral norm of the remaining error, \u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016. \u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016 can be further decomposed into two parts, \u2016\u22072f\u0302S(W ) \u2212 H\u2016 and \u2016H \u2212 \u22072fD(W \u2217)\u2016, where H is \u22072fD(W ) if \u03c6 is smooth, otherwise H is a specially designed matrix . We can upper bound them whenW is close enough toW \u2217 and there are enough samples. In particular, if the activation satisfies Property 3.3(a), see Lemma D.10 for bounding \u2016H \u2212 \u22072fD(W \u2217)\u2016 and Lemma D.11 for bounding \u2016H \u2212\u22072f\u0302S(W )\u2016. If the activation satisfies Property 3.3(b), see Lemma D.15.\nFinally we can complete the proof by setting \u03b4 = O(v2min\u03c1(\u03c31)/(kv 2 max\u03ba 2\u03bb\u03c32p1 )) in Lemma D.11 and Lemma D.15, setting \u2016W \u2212W \u2217\u2016 . v2min\u03c1(\u03c3k)/(k\u03ba2\u03bbv2max\u03c3 p 1) in Lemma D.10 and setting \u2016W \u2212 W \u2217\u2016 \u2264 v4min\u03c12(\u03c3k)\u03c3k/(k2\u03ba4\u03bb2v4max\u03c3 4p 1 ) in Lemma D.15."}, {"heading": "D.1.2 Local Linear Convergence of Gradient Descent", "text": "Although Theorem D.1 gives upper and lower bounds for the spectrum of the Hessian w.h.p., it only holds when the current set of parameters W are independent of samples. When we use iterative methods, like gradient descent, to optimize this objective, the next iterate calculated from the current set of samples will depend on this set of samples. Therefore, we need to do resampling at each iteration. Here we show that for activations that satisfies Properties 3.1, 3.2 and 3.3(a), linear convergence of gradient descent is guaranteed. To the best of our knowledge, there is no linear convergence guarantees for general non-smooth objective. So the following proposition also applies to smooth objectives only, which excludes ReLU.\nTheorem D.2 (Linear convergence of gradient descent, formal version of Theorem 4.4). Let W c \u2208 Rd\u00d7k be the current iterate satisfying\n\u2016W c \u2212W \u2217\u2016 . v4min\u03c12(\u03c3k)/(k2\u03ba5\u03bb2v4max\u03c3 4p 1 )\u2016W \u2217\u2016.\nLet S denote a set of i.i.d. samples from distribution D (defined in (1)) Let the activation function satisfy Property 3.1,3.2 and 3.3(a). Define\nm0 = \u0398(v 2 min\u03c1(\u03c3k)/(\u03ba 2\u03bb)) and M0 = \u0398(kv2max\u03c3 2p 1 ).\nFor any t \u2265 1, if we choose\n|S| \u2265 d \u00b7 poly(log d, t) \u00b7 k2v4max\u03c4\u03ba8\u03bb2\u03c3 4p 1 /(v 4 min\u03c1 2(\u03c3k)) (5)\nand perform gradient descent with step size 1/M0 on f\u0302S(W c) and obtain the next iterate,\nW\u0303 = W c \u2212 1 M0 \u2207f\u0302S(W c),\nthen with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016W\u0303 \u2212W \u2217\u20162F \u2264 (1\u2212 m0 M0 )\u2016W c \u2212W \u2217\u20162F .\nProof. To prove Theorem D.2, we need to show the positive definite properties on the entire line between the current iterate and the optimum by constructing a set of anchor points, which are independent of the samples. Then we apply traditional analysis for the linear convergence of gradient descent.\nIn particular, given a current iterate W c, we set d(p+1)/2 anchor points {W a}a=1,2,\u00b7\u00b7\u00b7 ,d(p+1)/2 equally along the line \u03beW \u2217 + (1\u2212 \u03be)W c for \u03be \u2208 [0, 1].\nAccording to Theorem D.1, by setting t \u2190 t + (p + 1)/2, we have with probability at least 1\u2212 d\u2212(t+(p+1)/2) for each anchor point {W a},\nm0I \u22072f\u0302S(W a) M0I.\nThen given an anchor point W a, according to Lemma D.16, we have with probability 1 \u2212 2d\u2212(t+(p+1)/2), for any points W between (W a\u22121 +W a)/2 and (W a +W a+1)/2,\nm0I \u22072f\u0302S(W ) M0I. (6)\nFinally by applying union bound over these d(p+1)/2 small intervals, we have with probability at least 1\u2212 d\u2212t for any points W on the line between W c and W \u2217,\nm0I \u22072f\u0302S(W ) M0I.\nNow we can apply traditional analysis for linear convergence of gradient descent. Let \u03b7 denote the stepsize.\n\u2016W\u0303 \u2212W \u2217\u20162F = \u2016W c \u2212 \u03b7\u2207f\u0302S(W c)\u2212W \u2217\u20162F = \u2016W c \u2212W \u2217\u20162F \u2212 2\u03b7\u3008\u2207f\u0302S(W c), (W c \u2212W \u2217)\u3009+ \u03b72\u2016\u2207f\u0302S(W c)\u20162F\nWe can rewrite f\u0302S(W c),\n\u2207f\u0302S(W c) = (\u222b 1\n0 \u22072f\u0302S(W \u2217 + \u03b3(W c \u2212W \u2217))d\u03b3\n) vec(W c \u2212W \u2217).\nWe define function H\u0302S : Rd\u00d7k \u2192 Rdk\u00d7dk such that\nH\u0302S(W c \u2212W \u2217) = (\u222b 1 0 \u22072f\u0302S(W \u2217 + \u03b3(W c \u2212W \u2217))d\u03b3 ) .\nAccording to Eq. (6), m0I H\u0302 M0I. (7)\nWe can upper bound \u2016\u2207f\u0302S(W c)\u20162F ,\n\u2016\u2207f\u0302S(W c)\u20162F = \u3008H\u0302S(W c \u2212W \u2217), H\u0302S(W c \u2212W \u2217)\u3009 \u2264M0\u3008W c \u2212W \u2217, H\u0302S(W c \u2212W \u2217)\u3009.\nTherefore,\n\u2016W\u0303 \u2212W \u2217\u20162F \u2264 \u2016W c \u2212W \u2217\u20162F \u2212 (\u2212\u03b72M0 + 2\u03b7)\u3008W c \u2212W \u2217, H\u0302(W c \u2212W \u2217)\u3009 \u2264 \u2016W c \u2212W \u2217\u20162F \u2212 (\u2212\u03b72M0 + 2\u03b7)m0\u2016W c \u2212W \u2217\u20162F = \u2016W c \u2212W \u2217\u20162F \u2212\nm0 M0 \u2016W c \u2212W \u2217\u20162F\n\u2264 (1\u2212 m0 M0 )\u2016W c \u2212W \u2217\u20162F\nwhere the third equality holds by setting \u03b7 = 1M0 ."}, {"heading": "D.2 Positive Definiteness of Population Hessian at the Ground Truth", "text": "The goal of this Section is to prove Lemma D.3.\nLemma D.3 (Positive definiteness of population Hessian at the ground truth). If \u03c6(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W \u2217,\n\u2126(v2min\u03c1(\u03c3k)/(\u03ba 2\u03bb))I \u22072fD(W \u2217) O(kv2max\u03c3 2p 1 )I.\nProof. The proof directly follows Lemma D.6 (Section D.2.2) and Lemma D.7(Section D.2.3)."}, {"heading": "D.2.1 Lower Bound on the Eigenvalues of Hessian for the Orthogonal Case", "text": "Lemma D.4. Let D1 denote Gaussian distribution N (0, 1). Let \u03b10 = Ez\u223cD1 [\u03c6\u2032(z)], \u03b11 = Ez\u223cD1 [\u03c6\u2032(z)z], \u03b12 = Ez\u223cD1 [\u03c6\u2032(z)z2], \u03b20 = Ez\u223cD1 [\u03c6\u20322(z)] ,\u03b22 = Ez\u223cD1 [\u03c6\u20322(z)z2]. Let \u03c1 denote min{(\u03b20 \u2212 \u03b120 \u2212 \u03b121), (\u03b22 \u2212 \u03b121 \u2212 \u03b122)}. Let P = [ p1 p2 \u00b7 \u00b7 \u00b7 pk ] \u2208 Rk\u00d7k. Then we have,\nE u\u223cDk ( k\u2211 i=1 p>i u \u00b7 \u03c6\u2032(ui) )2 \u2265 \u03c1\u2016P\u20162F (8) Proof. The main idea is to explicitly calculate the LHS of Eq (8), then reformulate the equation and find a lower bound represented by \u03b10, \u03b11, \u03b12, \u03b20, \u03b22.\nE u\u223cDk ( k\u2211 i=1 p>i u \u00b7 \u03c6\u2032(ui) )2 =\nk\u2211 i=1 k\u2211 l=1 E u\u223cDk [p>i (\u03c6 \u2032(ul)\u03c6 \u2032(ui) \u00b7 uu>)pl]\n= k\u2211 i=1 E u\u223cDk [p>i (\u03c6 \u2032(ui)\n2 \u00b7 uu>)pi]\ufe38 \ufe37\ufe37 \ufe38 A\n+ \u2211 i 6=l E u\u223cDk [p>i (\u03c6 \u2032(ul)\u03c6\n\u2032(ui) \u00b7 uu>)pl]\ufe38 \ufe37\ufe37 \ufe38 B\nFurther, we can rewrite the diagonal term in the following way,\nA = k\u2211 i=1 E u\u223cDk [p>i (\u03c6 \u2032(ui) 2 \u00b7 uu>)pi]\n= k\u2211 i=1 E u\u223cDk\np>i \u03c6\u2032(ui)2 \u00b7 u2i eie>i +\u2211 j 6=i uiuj(eie > j + eje > i ) + \u2211 j 6=i \u2211 l 6=i ujuleje > l  pi \n= k\u2211 i=1 E u\u223cDk\np>i \u03c6\u2032(ui)2 \u00b7 u2i eie>i +\u2211 j 6=i u2jeje > j  pi \n= k\u2211 i=1 p>i  E u\u223cDk [\u03c6\u2032(ui) 2u2i ]eie > i + \u2211 j 6=i E u\u223cDk [\u03c6\u2032(ui) 2u2j ]eje > j  pi  = k\u2211 i=1 p>i \u03b22eie>i +\u2211 j 6=i \u03b20eje > j  pi \n= k\u2211 i=1 p>i ((\u03b22 \u2212 \u03b20)eie>i + \u03b20Ik)pi\n= (\u03b22 \u2212 \u03b20) k\u2211 i=1 p>i eie > i pi + \u03b20 k\u2211 i=1 p>i pi = (\u03b22 \u2212 \u03b20)\u2016diag(P )\u20162 + \u03b20\u2016P\u20162F ,\nwhere the second step follows by rewriting uu> = k\u2211 i=1 k\u2211 j=1 uiujeie > j , the third step follows by\nE u\u223cDk [\u03c6\u2032(ui) 2uiuj ] = 0, \u2200j 6= i and E u\u223cDk [\u03c6\u2032(ui) 2ujul] = 0, \u2200j 6= l, the fourth step follows by pushing expectation, the fifth step follows by E u\u223cDk [\u03c6\u2032(ui) 2u2i ] = \u03b22 and E u\u223cDk [\u03c6\u2032(ui) 2u2j ] = E u\u223cDk [\u03c6\u2032(ui) 2] = \u03b20,\nand the last step follows by k\u2211 i=1 p2i,i = \u2016diag(P )\u20162 and k\u2211 i=1 p>i pi = k\u2211 i=1 \u2016pi\u20162 = \u2016P\u20162F .\nWe can rewrite the off-diagonal term in the following way,\nB = \u2211 i 6=l E u\u223cDk [p>i (\u03c6 \u2032(ul)\u03c6 \u2032(ui) \u00b7 uu>)pl]\n= \u2211 i 6=l E u\u223cDk\np>i \u03c6\u2032(ul)\u03c6\u2032(ui) \u00b7 u2i eie>i + u2l ele>l + uiul(eie>l + ele>i ) +\u2211 j 6=l uiujeie > j\n+ \u2211 j 6=i ujuleje > l + \u2211 j 6=i,l \u2211 j\u2032 6=i,l ujuj\u2032eje > j\u2032\n pl \n= \u2211 i 6=l E u\u223cDk\np>i \u03c6\u2032(ul)\u03c6\u2032(ui) \u00b7 u2i eie>i + u2l ele>l + uiul(eie>l + ele>i ) + \u2211 j 6=i,l u2jeje > j  pl \n= \u2211 i 6=l [ p>i ( E u\u223cDk [\u03c6\u2032(ul)\u03c6 \u2032(ui)u 2 i ]eie > i + E u\u223cDk [\u03c6\u2032(ul)\u03c6 \u2032(ui)u 2 l ]ele > l\n+ E u\u223cDk\n[\u03c6\u2032(ul)\u03c6 \u2032(ui)uiul](eie > l + ele > i ) + \u2211 j 6=i,l E u\u223cDk [\u03c6\u2032(ul)\u03c6 \u2032(ui)u 2 j ]eje > j\n pl \n= \u2211 i 6=l\np>i \u03b10\u03b12(eie>i + ele>l ) + \u03b121(eie>l + ele>i ) + \u2211\nj 6=i,l \u03b120eje > j\n pl \n= \u2211 i 6=l [ p>i ( (\u03b10\u03b12 \u2212 \u03b120)(eie>i + ele>l ) + \u03b121(eie>l + ele>i ) + \u03b120Ik ) pl ] = (\u03b10\u03b12 \u2212 \u03b120)\n\u2211 i 6=l p>i (eie > i + ele\n> l )pl\ufe38 \ufe37\ufe37 \ufe38\nB1\n+\u03b121 \u2211 i 6=l p>i (eie > l + ele\n> i )pl\ufe38 \ufe37\ufe37 \ufe38\nB2\n+\u03b120 \u2211 i 6=l\np>i pl\ufe38 \ufe37\ufe37 \ufe38 B3 ,\nwhere the third step follows by E u\u223cDk [\u03c6\u2032(ul)\u03c6 \u2032(ui)uiuj ] = 0 and E u\u223cDk [\u03c6\u2032(ul)\u03c6 \u2032(ui)uj\u2032uj ] = 0 for\nj\u2032 6= j.\nFor the term B1, we have B1 = (\u03b10\u03b12 \u2212 \u03b120) \u2211 i 6=l p>i (eie > i + ele > l )pl\n= 2(\u03b10\u03b12 \u2212 \u03b120) \u2211 i 6=l p>i eie > i pl = 2(\u03b10\u03b12 \u2212 \u03b120) k\u2211 i=1 p>i eie > i ( k\u2211 l=1 pl \u2212 pi )\n= 2(\u03b10\u03b12 \u2212 \u03b120) ( k\u2211 i=1 p>i eie > i k\u2211 l=1 pl \u2212 k\u2211 i=1 p>i eie > i pi ) = 2(\u03b10\u03b12 \u2212 \u03b120)(diag(P )> \u00b7 P \u00b7 1\u2212 \u2016diag(P )\u20162)\nFor the term B2, we have\nB2 = \u03b1 2 1 \u2211 i 6=l p>i (eie > l + ele > i )pl\n= \u03b121 \u2211 i 6=l p>i eie > l pl + \u2211 i 6=l p>i ele > i pl  = \u03b121  k\u2211 i=1 k\u2211 l=1 p>i eie > l pl \u2212 k\u2211 j=1 p>j eje > j pj + k\u2211 i=1 k\u2211 l=1 p>i ele > i pl \u2212 k\u2211 j=1 p>j eje > j pj\n = \u03b121((diag(P ) >1)2 \u2212 \u2016diag(P )\u20162 + \u3008P, P>\u3009 \u2212 \u2016diag(P )\u20162)\nFor the term B3, we have\nB3 = \u03b1 2 0 \u2211 i 6=l p>i pl\n= \u03b120 ( k\u2211 i=1 p>i k\u2211 l=1 pl \u2212 k\u2211 i=1 p>i pi )\n= \u03b120 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 pi \u2225\u2225\u2225\u2225\u2225 2 \u2212 k\u2211 i=1 \u2016pi\u20162 \n= \u03b120(\u2016P \u00b7 1\u20162 \u2212 \u2016P\u20162F )\nLet diag(P ) denote a length k column vector where the i-th entry is the (i, i)-th entry of\nP \u2208 Rk\u00d7k. Furthermore, we can show A+B is,\nA+B\n= A+B1 +B2 +B3 = (\u03b22 \u2212 \u03b20)\u2016diag(P )\u20162 + \u03b20\u2016P\u20162F\ufe38 \ufe37\ufe37 \ufe38 A + 2(\u03b10\u03b12 \u2212 \u03b120)(diag(P )> \u00b7 P \u00b7 1\u2212 \u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38 B1 + \u03b121((diag(P ) > \u00b7 1)2 \u2212 \u2016diag(P )\u20162 + \u3008P, P>\u3009 \u2212 \u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38\nB2\n+\u03b120(\u2016P \u00b7 1\u20162 \u2212 \u2016P\u20162F )\ufe38 \ufe37\ufe37 \ufe38 B3\n= \u2016\u03b10P \u00b7 1 + (\u03b12 \u2212 \u03b10)diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C1 +\u03b121(diag(P ) > \u00b7 1)2\ufe38 \ufe37\ufe37 \ufe38 C2 + \u03b121 2 \u2016P + P> \u2212 2diag(diag(P ))\u20162F\ufe38 \ufe37\ufe37 \ufe38\nC3\n+ (\u03b20 \u2212 \u03b120 \u2212 \u03b121)\u2016P \u2212 diag(diag(P ))\u20162F\ufe38 \ufe37\ufe37 \ufe38 C4 + (\u03b22 \u2212 \u03b121 \u2212 \u03b122)\u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C5 \u2265 (\u03b20 \u2212 \u03b120 \u2212 \u03b121)\u2016P \u2212 diag(diag(P ))\u20162F + (\u03b22 \u2212 \u03b121 \u2212 \u03b122)\u2016diag(P )\u20162 \u2265 min{(\u03b20 \u2212 \u03b120 \u2212 \u03b121), (\u03b22 \u2212 \u03b121 \u2212 \u03b122)} \u00b7 (\u2016P \u2212 diag(diag(P ))\u20162F + \u2016diag(P )\u20162) = min{(\u03b20 \u2212 \u03b120 \u2212 \u03b121), (\u03b22 \u2212 \u03b121 \u2212 \u03b122)} \u00b7 (\u2016P \u2212 diag(diag(P ))\u20162F + \u2016diag(diag(P ))\u20162) \u2265 min{(\u03b20 \u2212 \u03b120 \u2212 \u03b121), (\u03b22 \u2212 \u03b121 \u2212 \u03b122)} \u00b7 \u2016P\u20162F = \u03c1\u2016P\u20162F ,\nwhere the first step follows by B = B1 + B2 + B3, and the second step follows by the definition of A,B1, B2, B3 the third step follows by A + B1 + B2 + B3 = C1 + C2 + C3 + C4 + C5, the fourth step follows by C1, C2, C3 \u2265 0, the fifth step follows a \u2265 min(a, b), the sixth step follows by \u2016diag(P )\u20162 = \u2016diag(diag(P ))\u20162, the seventh step follows by triangle inequality, and the last step follows the definition of \u03c1.\nClaim D.5. A+B1 +B2 +B3 = C1 + C2 + C3 + C4 + C5.\nProof. The key properties we need are, for two vectors a, b, \u2016a+ b\u20162 = \u2016a\u20162 + 2\u3008a, b\u3009+ \u2016b\u20162; for two\nmatrices A,B, \u2016A+B\u20162F = \u2016A\u20162F + 2\u3008A,B\u3009+ \u2016B\u20162F . Then, we have\nC1 + C2 + C3 + C4 + C5\n= (\u2016\u03b10P \u00b7 1\u2016)2 + 2(\u03b10\u03b12 \u2212 \u03b120)\u3008P \u00b7 1,diag(P )\u3009+ (\u03b12 \u2212 \u03b10)2\u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C1 +\u03b121(diag(P ) > \u00b7 1)2\ufe38 \ufe37\ufe37 \ufe38 C2 + \u03b121 2\n(2\u2016P\u20162F + 4\u2016diag(diag(P ))\u20162F + 2\u3008P, P>\u3009 \u2212 4\u3008P,diag(diag(P ))\u3009 \u2212 4\u3008P>, diag(diag(P ))\u3009)\ufe38 \ufe37\ufe37 \ufe38 C3\n+ (\u03b20 \u2212 \u03b120 \u2212 \u03b121)(\u2016P\u20162F \u2212 2\u3008P,diag(diag(P ))\u3009+ \u2016diag(diag(P ))\u20162F )\ufe38 \ufe37\ufe37 \ufe38 C4 + (\u03b22 \u2212 \u03b121 \u2212 \u03b122)\u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C5 = \u03b120\u2016P \u00b7 1\u20162 + 2(\u03b10\u03b12 \u2212 \u03b120)\u3008P \u00b7 1, diag(P )\u3009+ (\u03b12 \u2212 \u03b10)2\u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C1 +\u03b121(diag(P ) > \u00b7 1)2\ufe38 \ufe37\ufe37 \ufe38 C2 + \u03b121 2\n(2\u2016P\u20162F + 4\u2016diag(P )\u20162 + 2\u3008P, P>\u3009 \u2212 8\u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38 C3\n+ (\u03b20 \u2212 \u03b120 \u2212 \u03b121)(\u2016P\u20162F \u2212 2\u2016diag(P )\u20162 + \u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38 C4 + (\u03b22 \u2212 \u03b121 \u2212 \u03b122)\u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 C5 = \u03b120\u2016P \u00b7 1\u20162 + 2(\u03b10\u03b12 \u2212 \u03b120)diag(P )> \u00b7 P \u00b7 1 + \u03b121(diag(P )> \u00b7 1)2 + \u03b121\u3008P, P>\u3009 + (\u03b20 \u2212 \u03b120)\u2016P\u20162F + ((\u03b12 \u2212 \u03b10)2 \u2212 2\u03b121 \u2212 \u03b20 + \u03b120 + \u03b121 + \u03b22 \u2212 \u03b121 \u2212 \u03b122)\ufe38 \ufe37\ufe37 \ufe38\n\u03b22\u2212\u03b20\u22122(\u03b12\u03b10\u2212\u03b120+\u03b121)\n\u2016diag(P )\u20162\n= 0\ufe38\ufe37\ufe37\ufe38 part of A + 2(\u03b12\u03b10 \u2212 \u03b120) \u00b7 diag(P )>P \u00b7 1\ufe38 \ufe37\ufe37 \ufe38 part of B1 +\u03b121 \u00b7 ((diag(P )>1)2 + \u3008P, P>\u3009)\ufe38 \ufe37\ufe37 \ufe38 part of B2 +\u03b120 \u00b7 \u2016P \u00b7 1\u20162\ufe38 \ufe37\ufe37 \ufe38 part of B3 + (\u03b20 \u2212 \u03b120) \u00b7 \u2016P\u20162F\ufe38 \ufe37\ufe37 \ufe38 proportional to \u2016P\u20162F + (\u03b22 \u2212 \u03b20 \u2212 2(\u03b12\u03b10 \u2212 \u03b120 + \u03b121)) \u00b7 \u2016diag(P )\u20162\ufe38 \ufe37\ufe37 \ufe38 proportional to \u2016diag(P )\u20162 = (\u03b22 \u2212 \u03b20)\u2016diag(P )\u20162 + \u03b20\u2016P\u20162F\ufe38 \ufe37\ufe37 \ufe38 A + 2(\u03b10\u03b12 \u2212 \u03b120)(diag(P )> \u00b7 P \u00b7 1\u2212 \u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38 B1 + \u03b121((diag(P ) > \u00b7 1)2 \u2212 \u2016diag(P )\u20162 + \u3008P, P>\u3009 \u2212 \u2016diag(P )\u20162)\ufe38 \ufe37\ufe37 \ufe38\nB2\n+\u03b120(\u2016P \u00b7 1\u20162 \u2212 \u2016P\u20162F )\ufe38 \ufe37\ufe37 \ufe38 B3\n=A+B1 +B2 +B3\nwhere the second step follows by \u3008P,diag(diag(P ))\u3009 = \u2016diag(P )\u20162 and \u2016diag(diag(P ))\u20162F = \u2016diag(P )\u20162."}, {"heading": "D.2.2 Lower Bound on the Eigenvalues of Hessian for Non-orthogonal Case", "text": "First we show the lower bound of the eigenvalues. The main idea is to reduce the problem to a k-by-k problem and then lower bound the eigenvalues using orthogonal weight matrices.\nLemma D.6 (Lower bound). If \u03c6(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W \u2217,\n\u2126(v2min\u03c1(\u03c3k)/(\u03ba 2\u03bb))I \u22072fD(W \u2217).\nProof. Let a \u2208 Rdk denote vector [ a>1 a > 2 \u00b7 \u00b7 \u00b7 a>k ]>, let b \u2208 Rdk denote vector [b>1 b>2 \u00b7 \u00b7 \u00b7 b>k ]> and let c \u2208 Rdk denote vector [ c>1 c > 2 \u00b7 \u00b7 \u00b7 c>k\n]>. The smallest eigenvalue of the Hessian can be calculated by\n\u22072f(W \u2217) min \u2016a\u2016=1 a>\u22072f(W \u2217)a Idk = min \u2016a\u2016=1 E x\u223cDd ( k\u2211 i=1 v\u2217i a > i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 Idk (9) Note that\nmin \u2016a\u2016=1 E x\u223cDd ( k\u2211 i=1 (v\u2217i ai) >x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = min \u2016a\u20166=0 E x\u223cDd ( k\u2211 i=1 (v\u2217i ai) >x \u00b7 \u03c6\u2032(w\u2217>i x)\n)2 /\u2016a\u20162 = min\u2211\ni \u2016bi/v\u2217i \u20162 6=0 E x\u223cDd ( k\u2211 i=1 b>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 /( k\u2211 i=1 \u2016bi/v\u2217i \u20162 )\nby ai = bi/v\u2217i\n= min\u2211 i \u2016bi\u20162 6=0 E x\u223cDd ( k\u2211 i=1 b>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 /( k\u2211 i=1 \u2016bi/v\u2217i \u20162 )\n\u2265 v2min min\u2211 i \u2016bi\u20162 6=0 E x\u223cDd ( k\u2211 i=1 b>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 /( k\u2211 i=1 \u2016bi\u20162 )\nby vmin = min i\u2208[k] |v\u2217i |\n= v2min min\u2016a\u2016=1 E x\u223cDd ( k\u2211 i=1 a>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 (10) Let U \u2208 Rd\u00d7k be the orthonormal basis of W \u2217 and let V = [ v1 v2 \u00b7 \u00b7 \u00b7 vk ] = U>W \u2217 \u2208 Rk\u00d7k. Also note that V and W \u2217 have same singular values and W \u2217 = UV . We use U\u22a5 \u2208 Rd\u00d7(d\u2212k) to denote the complement of U . For any vector aj \u2208 Rd, there exist two vectors bj \u2208 Rk and cj \u2208 Rd\u2212k such that\naj = Ubj + U\u22a5cj .\nWe useDd to denote Gaussian distributionN (0, Id), Dd\u2212k to denote Gaussian distributionN (0, Id\u2212k), and Dk to denote Gaussian distribution N (0, Ik). Then we can rewrite formulation (10) (removing v2min) as\nE x\u223cDd ( k\u2211 i=1 a>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = E x\u223cDd ( k\u2211 i=1 (b>i U > + c>i U > \u22a5 )x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = A+B + C\nwhere\nA = E x\u223cDd ( k\u2211 i=1 b>i U >x \u00b7 \u03c6\u2032(w\u2217>i x) )2 , B = E\nx\u223cDd ( k\u2211 i=1 c>i U > \u22a5x \u00b7 \u03c6\u2032(w\u2217>i x) )2 , C = E\nx\u223cDd\n[ 2 ( k\u2211 i=1 b>i U >x \u00b7 \u03c6\u2032(w\u2217>i x) ) \u00b7 ( k\u2211 i=1 c>i U > \u22a5x \u00b7 \u03c6\u2032(w\u2217>i x) )] .\nWe calculate A,B,C separately. First, we can show\nA = E x\u223cDd ( k\u2211 i=1 b>i U >x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = E z\u223cDk ( k\u2211 i=1 b>i z \u00b7 \u03c6\u2032(v>i z) )2 . Second, we can show\nB = E x\u223cDd ( k\u2211 i=1 c>i U > \u22a5x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = E\ns\u223cDd\u2212k,z\u223cDk ( k\u2211 i=1 c>i s \u00b7 \u03c6\u2032(v\u2217>i z) )2 = E\ns\u223cDd\u2212k,z\u223cDk [(y>s)2] by defining y = k\u2211 i=1 \u03c6\u2032(v\u2217>i z)ci \u2208 Rd\u2212k\n= E z\u223cDk\n[ E\ns\u223cDd\u2212k [(y>s)2]\n]\n= E z\u223cDk  E s\u223cDd\u2212k d\u2212k\u2211 j=1 s2jy 2 j  by E[sjsj\u2032 ] = 0 = E\nz\u223cDk d\u2212k\u2211 j=1 y2j  by sj \u223c N (0, 1) = E\nz\u223cDk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(v\u2217>i z)ci \u2225\u2225\u2225\u2225\u2225 2  by definition of y\nThird, we have C = 0 since U>\u22a5x is independent of w \u2217> i x and U >x. Thus, putting them all together,\nE x\u223cDd ( k\u2211 i=1 a>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = E z\u223cDk ( k\u2211 i=1 b>i z \u00b7 \u03c6\u2032(v>i z) )2+ E z\u223cDk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(v>i z)ci \u2225\u2225\u2225\u2225\u2225 2 \nLet us lower bound A,\nA = E z\u223cDk ( k\u2211 i=1 b>i z \u00b7 \u03c6\u2032(v>i z) )2 = \u222b (2\u03c0)\u2212k/2\n( k\u2211 i=1 b>i z \u00b7 \u03c6\u2032(v>i z) )2 e\u2212\u2016z\u2016 2/2dz\n= \u03be1\n\u222b (2\u03c0)\u2212k/2 ( k\u2211 i=1 b>i V \u2020>s \u00b7 \u03c6\u2032(si) )2 e\u2212\u2016V \u2020>s\u20162/2 \u00b7 | det(V \u2020)|ds\n\u2265 \u03be2\n\u222b (2\u03c0)\u2212k/2 ( k\u2211 i=1 b>i V \u2020>s \u00b7 \u03c6\u2032(si) )2 e\u2212\u03c3 2 1(V \u2020)\u2016s\u20162/2 \u00b7 | det(V \u2020)|ds\n= \u03be3\n\u222b (2\u03c0)\u2212k/2 ( k\u2211 i=1 b>i V \u2020>u/\u03c31(V \u2020) \u00b7 \u03c6\u2032(ui/\u03c31(V \u2020)) )2 e\u2212\u2016u\u2016 2/2| det(V \u2020)|/\u03c3k1 (V \u2020)du\n= \u222b (2\u03c0)\u2212k/2 ( k\u2211 i=1 p>i u \u00b7 \u03c6\u2032(\u03c3k \u00b7 ui) )2 e\u2212\u2016u\u2016 2/2 1 \u03bb du\n= 1\n\u03bb E u\u223cDk ( k\u2211 i=1 p>i u \u00b7 \u03c6\u2032(\u03c3k \u00b7 ui) )2 where V \u2020 \u2208 Rk\u00d7k is the inverse of V \u2208 Rk\u00d7k, i.e., V \u2020V = I, p>i = b>i V \u2020>/\u03c31(V \u2020) and \u03c3k = \u03c3k(W \u2217). \u03be1 replaces z by z = V \u2020>s, so v\u2217>i z = si. \u03be2 uses the fact \u2016V \u2020>s\u2016 \u2264 \u03c31(V \u2020)\u2016s\u2016. \u03be3 replaces s by s = u/\u03c31(V\n\u2020). Note that \u03c6\u2032(\u03c3k \u00b7ui)\u2019s are independent of each other, so we can simplify the analysis. In particular, Lemma D.4 gives a lower bound in this case in terms of pi. Note that \u2016pi\u2016 \u2265 \u2016bi\u2016/\u03ba. Therefore,\nE z\u223cDk ( k\u2211 i=1 b>i z \u00b7 \u03c6\u2032(v>i z) )2 \u2265 \u03c1(\u03c3k) 1 \u03ba2\u03bb \u2016b\u20162.\nFor B, similar to the proof of Lemma D.4, we have,\nB = E z\u223cDk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(v>i z)ci \u2225\u2225\u2225\u2225\u2225 2 \n= \u222b (2\u03c0)\u2212k/2 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(v>i z)ci \u2225\u2225\u2225\u2225\u2225 2 e\u2212\u2016z\u2016 2/2dz\n= \u222b (2\u03c0)\u2212k/2 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(\u03c3k \u00b7 ui)ci \u2225\u2225\u2225\u2225\u2225 2 e\u2212\u2016V \u2020>u/\u03c31(V \u2020)\u20162/2 \u00b7 det(V \u2020/\u03c31(V \u2020))du\n= \u222b (2\u03c0)\u2212k/2 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(\u03c3k \u00b7 ui)ci \u2225\u2225\u2225\u2225\u2225 2 e\u2212\u2016V \u2020>u/\u03c31(V \u2020)\u20162/2 \u00b7 1 \u03bb du\n\u2265 \u222b (2\u03c0)\u2212k/2 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(\u03c3k \u00b7 ui)ci \u2225\u2225\u2225\u2225\u2225 2 e\u2212\u2016u\u2016 2/2 \u00b7 1 \u03bb du\n= 1\n\u03bb E u\u223cDk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03c6\u2032(\u03c3k \u00b7 ui)ci \u2225\u2225\u2225\u2225\u2225 2 \n= 1\n\u03bb  k\u2211 i=1 E u\u223cDk [\u03c6\u2032(\u03c3k \u00b7 ui)\u03c6\u2032(\u03c3k \u00b7 ui)c>i ci] + \u2211 i 6=l E u\u223cDk [\u03c6\u2032(\u03c3k \u00b7 ui)\u03c6\u2032(\u03c3k \u00b7 ul)c>i cl]  = 1\n\u03bb  E z\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 ui)2] k\u2211 i=1 \u2016ci\u20162 + ( E z\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)] )2\u2211 i 6=l c>i cl  = 1\n\u03bb ( E z\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)] )2 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 ci \u2225\u2225\u2225\u2225\u2225 2\n2\n+ ( E\nz\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)2]\u2212\n( E\nz\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)]\n)2) \u2016c\u20162  \u2265 1 \u03bb ( E z\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)2]\u2212 ( E z\u223cD1 [\u03c6\u2032(\u03c3k \u00b7 z)] )2) \u2016c\u20162\n\u2265 \u03c1(\u03c3k) 1\n\u03bb \u2016c\u20162,\nwhere the first step follows by definition of Gaussian distribution, the second step follows by replacing z by z = V \u2020>u/\u03c31(V \u2020), and then v>i z = ui/\u03c31(V \u2020) = ui\u03c3k(W \u2217), the third step follows by \u2016u\u20162 \u2265 \u2016 1 \u03c31(V \u2020) V \u2020 > u\u20162 , the fourth step follows by det(V \u2020/\u03c31(V \u2020)) = det(V \u2020)/\u03c3k1 (V \u2020) = 1/\u03bb, the fifth step follows by definition of Gaussian distribution, the ninth step follows by x2 \u2265 0 for any x \u2208 R, and the last step follows by Property 3.2.\nNote that 1 = \u2016a\u20162 = \u2016b\u20162 + \u2016c\u20162. Thus, we finish the proof for the lower bound."}, {"heading": "D.2.3 Upper Bound on the Eigenvalues of Hessian for Non-orthogonal Case", "text": "Lemma D.7 (Upper bound). If \u03c6(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W \u2217,\n\u22072fD(W \u2217) O(kv2max\u03c3 2p 1 )I.\nProof. Similarly, we can calculate the upper bound of the eigenvalues by\n\u2016\u22072f(W \u2217)\u2016 = max \u2016a\u2016=1 a>\u22072f(W \u2217)a\n= v2max max\u2016a\u2016=1 E x\u223cDk ( k\u2211 i=1 a>i x \u00b7 \u03c6\u2032(w\u2217>i x) )2 = v2max max\u2016a\u2016=1 k\u2211 i=1 k\u2211 l=1 E x\u223cDk [a>i x \u00b7 \u03c6\u2032(w\u2217>i x) \u00b7 a>l x \u00b7 \u03c6\u2032(w\u2217>l x)]\n\u2264 v2max max\u2016a\u2016=1 k\u2211 i=1 k\u2211 l=1 ( E x\u223cDk [(a>i x) 4] \u00b7 E x\u223cDk [(\u03c6\u2032(w\u2217>i x)) 4] \u00b7 E x\u223cDk [(a>l x) 4] \u00b7 E x\u223cDk [(\u03c6\u2032(w\u2217>l x)) 4] )1/4\n. v2max max\u2016a\u2016=1 k\u2211 i=1 k\u2211 l=1 \u2016ai\u2016 \u00b7 \u2016al\u2016 \u00b7 \u2016w\u2217i \u2016p \u00b7 \u2016w\u2217l \u2016p\n\u2264 v2max max\u2016a\u2016=1 k\u2211 i=1 k\u2211 l=1 \u2016ai\u2016 \u00b7 \u2016al\u2016 \u00b7 \u03c32p1 \u2264 kv2max\u03c3 2p 1 ,\nwhere the first inequality follows H\u00f6lder\u2019s inequality, the second inequality follows by Property 3.1, the third inequality follows by \u2016w\u2217i \u2016 \u2264 \u03c31(W \u2217), and the last inequality by Cauchy-Schwarz inequality."}, {"heading": "D.3 Error Bound of Hessians near the Ground Truth for Smooth Activations", "text": "The goal of this Section is to prove Lemma D.8\nLemma D.8 (Error bound of Hessians near the ground truth for smooth activations). Let \u03c6(z) satisfy Property 3.1, Property 3.2 and Property 3.3(a). Let W satisfy \u2016W \u2212 W \u2217\u2016 \u2264 \u03c3k/2. Let S denote a set of i.i.d. samples from the distribution defined in (1). Then for any t \u2265 1 and 0 < < 1/2, if\n|S| \u2265 \u22122d\u03ba2\u03c4 \u00b7 poly(log d, t)\nthen we have, with probability at least 1\u2212 1/d\u2126(t),\n\u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016 . v2maxk\u03c3 p 1( \u03c3 p 1 + \u2016W \u2212W \u2217\u2016).\nProof. This follows by combining Lemma D.10 and Lemma D.11 directly."}, {"heading": "D.3.1 Second-order Smoothness near the Ground Truth for Smooth Activations", "text": "The goal of this Section is to prove Lemma D.10.\nFact D.9. Let wi denote the i-th column ofW \u2208 Rd\u00d7k, and w\u2217i denote the i-th column ofW \u2217 \u2208 Rd\u00d7k. If \u2016W \u2212W \u2217\u2016 \u2264 \u03c3k(W \u2217)/2, then for all i \u2208 [k],\n1 2 \u2016w\u2217i \u2016 \u2264 \u2016wi\u2016 \u2264 3 2 \u2016w\u2217i \u2016.\nProof. Note that if \u2016W \u2212W \u2217\u2016 \u2264 \u03c3k(W \u2217)/2, we have \u03c3k(W \u2217)/2 \u2264 \u03c3i(W ) \u2264 32\u03c31(W \u2217) for all i \u2208 [k] by Weyl\u2019s inequality. By definition of singular value, we have \u03c3k(W \u2217) \u2264 \u2016w\u2217i \u2016 \u2264 \u03c31(W \u2217). By definition of spectral norm, we have \u2016wi \u2212 w\u2217i \u2016 \u2264 \u2016W \u2212W \u2217\u2016. Thus, we can lower bound \u2016wi\u2016,\n\u2016wi\u2016 \u2264 \u2016w\u2217i \u2016+ \u2016wi \u2212 w\u2217i \u2016 \u2264 \u2016w\u2217i \u2016+ \u2016W \u2212W \u2217\u2016 \u2264 \u2016w\u2217i \u2016+ \u03c3k/2 \u2264 3 2 \u2016w\u2217i \u2016.\nSimilarly, we have \u2016wi\u2016 \u2265 12\u2016w \u2217 i \u2016.\nLemma D.10 (Second-order smoothness near the ground truth for smooth activations). If \u03c6(z) satisfies Property 3.1, Property 3.2 and Property 3.3(a), then for any W with \u2016W \u2212W \u2217\u2016 \u2264 \u03c3k/2, we have\n\u2016\u22072fD(W )\u2212\u22072fD(W \u2217)\u2016 . k2v2max\u03c3 p 1\u2016W \u2212W \u2217\u2016.\nProof. Let \u2206 = \u22072fD(W )\u2212\u22072fD(W \u2217). For each (i, l) \u2208 [k]\u00d7 [k], let \u2206i,l \u2208 Rd\u00d7d denote the (i, l)-th block of \u2206. Then, for i 6= l, we have\n\u2206i,l = E x\u223cDd\n[ v\u2217i v \u2217 l ( \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x) ) xx> ] ,\nand for i = l, we have\n\u2206i,i\n= E x\u223cDd [( k\u2211 r=1 v\u2217r\u03c6(w > r x)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx > + v\u22172i ( \u03c6\u20322(w>i x)\u2212 \u03c6\u20322(w\u2217>i x) ) xx> ]\n= E x\u223cDd [( k\u2211 r=1 v\u2217r\u03c6(w > r x)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx > ] + E x\u223cDd [ v\u22172i ( \u03c6\u20322(w>i x)\u2212 \u03c6\u20322(w\u2217>i x) ) xx> ] . (11)\nIn the next a few paragraphs, we first show how to bound the off-diagonal term, and then show how to bound the diagonal term.\nFirst, we consider off-diagonal terms.\n\u2016\u2206i,l\u2016\n= \u2225\u2225\u2225\u2225 Ex\u223cDd [ v\u2217i v \u2217 l ( \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x) ) xx> ]\u2225\u2225\u2225\u2225 \u2264 v2max max\u2016a\u2016=1 E x\u223cDd\n[\u2223\u2223\u2223\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x)\u2223\u2223\u2223 \u00b7 (x>a)2] \u2264 v2max max\u2016a\u2016=1 E x\u223cDd [( |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|+ |\u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>l x)| ) (x>a)2\n] = v2max max\u2016a\u2016=1 ( E x\u223cDd [ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>a)2\n] + E\nx\u223cDd\n[ |\u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>l x)|(x>a)2 ]) \u2264 v2max max\u2016a\u2016=1 ( E x\u223cDd [ L2|(wi \u2212 w\u2217i )>x||\u03c6\u2032(w>l x)|(x>a)2 ] + E x\u223cDd [ |\u03c6\u2032(w\u2217>i x)|L2|(wl \u2212 w\u2217l )>x|(x>a)2\n]) \u2264 v2max max\u2016a\u2016=1 ( E x\u223cDd [ L2|(wi \u2212 w\u2217i )>x|L1|w>l x|p(x>a)2 ] + E x\u223cDd [ L1|w\u2217>i x|pL2|(wl \u2212 w\u2217l )>x|(x>a)2\n]) \u2264 v2maxL1L2 max\u2016a\u2016=1 ( E x\u223cDd [ |(wi \u2212 w\u2217i )>x||w>l x|p(x>a)2 ] + E x\u223cDd [ |(wl \u2212 w\u2217l )>x||w\u2217>i x|p(x>a)2\n]) . v2maxL1L2 max\u2016a\u2016=1 (\u2016wi \u2212 w\u2217i \u2016\u2016wl\u2016p\u2016a\u20162 + \u2016wl \u2212 w\u2217l \u2016\u2016w\u2217i \u2016p\u2016a\u20162)\n. v2maxL1L2\u03c3 p 1(W \u2217)\u2016W \u2212W \u2217\u2016 (12)\nwhere the first step follows by definition of \u2206i,l, the second step follows by definition of spectral norm and v\u2217i v \u2217 l \u2264 v2max, the third step follows by triangle inequality, the fourth step follows by linearity of expectation, the fifth step follows by Property 3.3(a), i.e., |\u03c6\u2032(w>i x)\u2212\u03c6\u2032(w\u2217>i x)| \u2264 L2|(wi\u2212w\u2217i )>x|, the sixth step follows by Property 3.1, i.e., \u03c6\u2032(z) \u2264 L1|z|p, the seventh step follows by Fact B.6, and the last step follows by \u2016a\u20162 = 1, \u2016wi \u2212 w\u2217i \u2016 \u2264 \u2016W \u2212W \u2217\u2016, \u2016wi\u2016 \u2264 32\u2016w \u2217 i \u2016, and \u2016w\u2217i \u2016 \u2264 \u03c31(W \u2217).\nNote that the proof for the off-diagonal terms also applies to bounding the second-term in the diagonal block Eq. (11). Thus we only need to show how to bound the first term in the diagonal\nblock Eq. (11). \u2225\u2225\u2225\u2225\u2225 Ex\u223cDd [( k\u2211 r=1 v\u2217r\u03c6(w > r x)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx > ]\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 Ex\u223cDd [( k\u2211 r=1 v\u2217r (\u03c6(w > r x)\u2212 \u03c6(w\u2217>r x)) ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx >\n]\u2225\u2225\u2225\u2225\u2225 \u2264 v2max\nk\u2211 r=1 max \u2016a\u2016=1 E x\u223cDd [|\u03c6(w>r x)\u2212 \u03c6(w\u2217>r x)||\u03c6\u2032\u2032(w>i x)|(x>a)2]\n\u2264 v2max k\u2211 r=1 max \u2016a\u2016=1 E x\u223cDd [|\u03c6(w>r x)\u2212 \u03c6(w\u2217>r x)|L2(x>a)2]\n\u2264 v2maxL2 k\u2211 r=1 max \u2016a\u2016=1 E x\u223cDd [ max z\u2208[w>r x,w\u2217>r x] |\u03c6\u2032(z)| \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 (x>a)2 ]\n\u2264 v2maxL2 k\u2211 r=1 max \u2016a\u2016=1 E x\u223cDd [ max z\u2208[w>r x,w\u2217>r x] L1|z|p \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 (x>a)2 ]\n\u2264 v2maxL1L2 k\u2211 r=1 max \u2016a\u2016=1 E x\u223cDd [(|w>r x|p + |w\u2217>r x|p) \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 (x>a)2]\n. v2maxL1L2 k\u2211 r=1 [(\u2016wr\u2016p + \u2016w\u2217r\u2016p)\u2016wr \u2212 w\u2217r\u2016] . kv2maxL1L2\u03c3 p 1(W \u2217)\u2016W \u2212W \u2217\u2016, (13)\nwhere the first step follows by y = \u2211k\nr=1 v \u2217 r\u03c6(w \u2217> r x), the second step follows by definition of spectral\nnorm and v\u2217rv\u2217i \u2264 |vmax|2, the third step follows by Property 3.3(a), i.e., |\u03c6\u2032\u2032(w>i x)| \u2264 L2, the fourth step follows by |\u03c6(w>r x) \u2212 \u03c6(w\u2217>r x) \u2264 maxz\u2208[w>r x,w\u2217>r x] |\u03c6\n\u2032(z)||(wr \u2212 w\u2217r)>x|, the fifth step follows Property 3.1, i.e., |\u03c6\u2032(z)| \u2264 L1|z|p, the sixth step follows by maxz\u2208[w>r x,w\u2217>r x] |z|\np \u2264 (|w>r x|p + |w\u2217>r x|p), the seventh step follows by Fact B.6.\nPutting it all together, we can bound the error by\n\u2016\u22072fD(W )\u2212\u22072fD(W \u2217)\u2016 = max \u2016a\u2016=1 a>(\u22072fD(W )\u2212\u22072fD(W \u2217))a\n= max \u2016a\u2016=1 k\u2211 i=1 k\u2211 l=1 a>i \u2206i,lal\n= max \u2016a\u2016=1  k\u2211 i=1 a>i \u2206i,iai + \u2211 i 6=l a>i \u2206i,lal  \u2264 max \u2016a\u2016=1  k\u2211 i=1 \u2016\u2206i,i\u2016\u2016ai\u20162 + \u2211 i 6=l \u2016\u2206i,l\u2016\u2016ai\u2016\u2016al\u2016\n \u2264 max \u2016a\u2016=1  k\u2211 i=1 C1\u2016ai\u20162 + \u2211 i 6=l C2\u2016ai\u2016\u2016al\u2016\n = max \u2016a\u2016=1 C1 k\u2211 i=1 \u2016ai\u20162 + C2 ( k\u2211 i=1 \u2016ai\u2016 )2 \u2212 k\u2211 i=1 \u2016ai\u20162 \n\u2264 max \u2016a\u2016=1\n( C1\nk\u2211 i=1 \u2016ai\u20162 + C2\n( k\nk\u2211 i=1 \u2016ai\u20162 \u2212 k\u2211 i=1\n\u2016ai\u20162 ))\n= max \u2016a\u2016=1\n(C1 + C2(k \u2212 1))\n. k2v2maxL1L2\u03c3 p 1(W \u2217)\u2016W \u2212W \u2217\u2016.\nwhere the first step follows by definition of spectral norm and a denotes a vector \u2208 Rdk, the first inequality follows by \u2016A\u2016 = max\u2016x\u20166=0,\u2016y\u20166=0 x >Ay \u2016x\u2016\u2016y\u2016 , the second inequality follows by \u2016\u2206i,i\u2016 \u2264 C1 and \u2016\u2206i,l\u2016 \u2264 C2, the third inequality follows by Cauchy-Scharwz inequality, the eighth step follows by\u2211 i=1 \u2016ai\u20162 = 1, and the last step follows by Eq (12) and (13)."}, {"heading": "D.3.2 Empirical and Population Difference for Smooth Activations", "text": "The goal of this Section is to prove Lemma D.11. For each i \u2208 [k], let \u03c3i denote the i-th largest singular value of W \u2217 \u2208 Rd\u00d7k.\nNote that Bernstein inequality requires the spectral norm of each random matrix to be bounded almost surely. However, since we assume Gaussian distribution for x, \u2016x\u20162 is not bounded almost surely. The main idea is to do truncation and then use Matrix Bernstein inequality. Details can be found in Lemma B.7 and Corollary B.8.\nLemma D.11 (Empirical and population difference for smooth activations). Let \u03c6(z) satisfy Property 3.1,3.2 and 3.3(a). Let W satisfy \u2016W \u2212W \u2217\u2016 \u2264 \u03c3k/2. Let S denote a set of i.i.d. samples from distribution D (defined in (1)). Then for any t \u2265 1 and 0 < < 1/2, if\n|S| \u2265 \u22122d\u03ba2\u03c4 \u00b7 poly(log d, t)\nthen we have, with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016\u22072f\u0302S(W )\u2212\u22072fD(W )\u2016 .v2maxk\u03c3 p 1( \u03c3 p 1 + \u2016W \u2212W \u2217\u2016).\nProof. Define \u2206 = \u22072fD(W )\u2212\u22072f\u0302S(W ). Let\u2019s first consider the diagonal blocks. Define\n\u2206i,i = E x\u223cDd [( k\u2211 r=1 v\u2217r\u03c6(w > r x)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx > + v\u22172i \u03c6 \u20322(w>i x)xx > ]\n\u2212  1 n n\u2211 j=1 ( k\u2211 r=1 v\u2217r\u03c6(w > r xj)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i xj)xjx > j + v \u22172 i \u03c6 \u20322(w>i xj)xjx > j  . Let\u2019s further decompose \u2206i,i into \u2206i,i = \u2206 (1) i,i + \u2206 (2) i,i , where\n\u2206 (1) i,i\n= E x\u223cDd [( k\u2211 r=1 v\u2217r\u03c6(w > r x)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i x)xx > ] \u2212 1 n n\u2211 j=1 ( k\u2211 r=1 v\u2217r\u03c6(w > r xj)\u2212 y ) v\u2217i \u03c6 \u2032\u2032(w>i xj)xjx > j\n= v\u2217i k\u2211 r=1 ( v\u2217r E x\u223cDd [ (\u03c6(w>r x)\u2212 \u03c6(w\u2217>r x))\u03c6\u2032\u2032(w>i x)xx> ] \u2212 1 n n\u2211 j=1 (\u03c6(w>r xj)\u2212 \u03c6(w\u2217>r xj))\u03c6\u2032\u2032(w>i xj)xjx>j ) ,\nand\n\u2206 (2) i,i = E x\u223cDd [v\u22172i \u03c6 \u20322(w>i x)xx >]\u2212 1 n n\u2211 j=1 [v\u22172i \u03c6 \u20322(w>i xj)xjx > j ]. (14)\nThe off-diagonal block is\n\u2206i,l = v \u2217 i v \u2217 l  E x\u223cDd [\u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)xx >]\u2212 1 n n\u2211 j=1 \u03c6\u2032(w>i xj)\u03c6 \u2032(w>l xj)xjx > j  Combining Claims. D.12, D.13 and D.14, and taking a union bound over k2 different \u2206i,j , we\nobtain if n \u2265 \u22122\u03ba2(W \u2217)\u03c4d \u00b7 poly(t, log d) for any \u2208 (0, 1/2), with probability at least 1\u2212 1/dt,\n\u2016\u22072f\u0302S(W )\u2212\u22072f(W )\u2016 . v2maxk\u03c3 p 1(W \u2217)( \u03c3p1(W \u2217) + \u2016W \u2212W \u2217\u2016)\nClaim D.12. For each i \u2208 [k], if n \u2265 dpoly(log d, t)\n\u2016\u2206(1)i,i \u2016 . kv 2 max\u03c3 p 1(W \u2217)\u2016W \u2212W \u2217\u2016\nholds with probability 1\u2212 1/d4t.\nProof. For each r \u2208 [k], we define function B\u0302r : Rd \u2192 Rd\u00d7d,\nB\u0302r(x) = L1L2 \u00b7 (|w>r x|p + |w\u2217>r x|p) \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 xx>.\nAccording to Properties 3.1,3.2 and 3.3(a), we have for each x \u2208 S,\n\u2212B\u0302r(x) (\u03c6(w>r x)\u2212 \u03c6(w\u2217>r x))\u03c6\u2032\u2032(w>i x)xx> B\u0302r(x)\nTherefore,\n\u2206 (1) i,i v 2 max k\u2211 r=1\n( E\nx\u223cDd [B\u0302r(x)] +\n1 |S| \u2211 x\u2208S B\u0302r(x)\n) .\nLet hr(x) = L1L2|w>r x|p \u00b7 |(wr \u2212 w\u2217r)>x|. Let Br = Ex\u223cDd [hr(x)xx>]. Define function Br(x) = hr(x)xx\n>. (I) Bounding |hr(x)|. According to Fact B.1, we have for any constant t \u2265 1, with probability 1\u2212 1/(nd4t),\n|hr(x)| = L1L2|w>r x|p|(wr \u2212 w\u2217r)>x| . L1L2\u2016wr\u2016p\u2016wr \u2212 w\u2217r\u2016(t log n)(p+1)/2.\n(II) Bounding \u2016Br\u2016.\n\u2016Br\u2016 \u2265 E x\u223cDd\n[ L1L2|w>r x|p|(wr \u2212 w\u2217r)>x| ( (wr \u2212 w\u2217r)>x \u2016wr \u2212 w\u2217r\u2016 )2] & L1L2\u2016wr\u2016p\u2016wr \u2212 w\u2217r\u2016,\nwhere the first step follows by definition of spectral norm, and last step follows by Fact B.6. Using Fact B.6, we can also prove an upper bound \u2016Br\u2016, \u2016Br\u2016 . L1L2\u2016wr\u2016p\u2016wr \u2212 w\u2217r\u2016.\n(III) Bounding (Ex\u223cDd [h4(x)])1/4 Using Fact B.6, we have(\nE x\u223cDd [h4(x)]\n)1/4 = L1L2 ( E\nx\u223cDd\n[( |w>r x|p|(wr \u2212 w\u2217r)>x| )4])1/4 . L1L2\u2016wr\u2016p\u2016wr \u2212 w\u2217r\u2016.\nBy applying Corollary B.8, if n \u2265 \u22122dpoly(log d, t), then with probability 1\u2212 1/d4t,\n\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd [ |w>r x|p \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 xx> ] \u2212 1 |S| (\u2211 x\u2208S |w>r xj |p \u00b7 |(wr \u2212 w\u2217r)>x| \u00b7 xx> )\u2225\u2225\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225Br \u2212 1|S|\u2211 x\u2208S Br(x) \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016Br\u2016 . \u2016wr\u2016p\u2016wr \u2212 w\u2217r\u2016. (15)\nIf \u2264 1/2, we have\n\u2016\u2206(1)i,i \u2016 . k\u2211 i=1 v2max\u2016Br\u2016 . kv2max\u03c3 p 1(W \u2217)\u2016W \u2212W \u2217\u2016\nClaim D.13. For each i \u2208 [k], if n \u2265 \u22122d\u03c4 poly(log d, t) , then\n\u2016\u2206(2)i,i \u2016 . v 2 max\u03c3 2p 1\nholds with probability 1\u2212 1/d4t.\nProof. Recall the definition of \u2206(2)i,i .\n\u2206 (2) i,i = E x\u223cDd [v\u22172i \u03c6 \u20322(w>i x)xx >]\u2212 1 |S| \u2211 x\u2208S [v\u22172i \u03c6 \u20322(w>i x)xx >]\nLet hi(x) = \u03c6\u20322(w>i x). Let Bi = Ex\u223cDd [hi(x)xx>] Define function Bi(x) = hi(x)xx>. (I) Bounding |hi(x)|. For any constant t \u2265 1, (\u03c6\u2032(w>i x))2 \u2264 L21|w>i x|2p . L21\u2016wi\u20162ptp log\np(n) with probability 1 \u2212 1/(nd4t) according to Fact B.1.\n(II) Bounding \u2016Bi\u2016.\u2225\u2225\u2225\u2225 Ex\u223cDd[\u03c6\u20322(w>i x)xx>] \u2225\u2225\u2225\u2225 = max\u2016a\u2016=1 Ex\u223cDd [ \u03c6\u20322(w>i x)(x >a)2 ]\n= max \u2016a\u2016=1 E x\u223cDd\n[ \u03c6\u20322(w>i x) ( \u03b1w>i x+ \u03b2x >v )2]\n= max \u03b12+\u03b22=1,\u2016v\u2016=1 E x\u223cDd\n[ \u03c6\u20322(w>i x) ( (\u03b1w>i x) 2 + (\u03b2x>v)2 )]\n= max \u03b12+\u03b22=1\n( \u03b12 E\nz\u223cD1 [\u03c6\u20322(\u2016wi\u2016z)z2] + \u03b22 E z\u223cD1 [\u03c6\u20322(\u2016wi\u2016z)] ) = max ( E\nx\u223cD1 [\u03c6\u20322(\u2016wi\u2016z)z2], E x\u223cD1 [\u03c6\u20322(\u2016wi\u2016z)] ) where wi = wi/\u2016wi\u2016 and v is a unit vector orthogonal to wi such that a = \u03b1wi + \u03b2v. Now from Property 3.2, we have\n\u03c1(\u2016wi\u2016) \u2264 \u2225\u2225\u2225\u2225 Ex\u223cDd[\u03c6\u20322(w>i x)xx>] \u2225\u2225\u2225\u2225 . L21\u2016wi\u20162p. (III) Bounding (Ex\u223cDd [h4i (x)])1/4.(\nE x\u223cDd [h4i (x)]\n)1/4 = ( E\nx\u223cDd [\u03c6\u20328(w>i x)]\n)1/4 . L21\u2016wi\u20162p.\nBy applying Corollary B.8, we have, for any 0 < < 1, if n \u2265 \u22122d \u2016wi\u2016 4p\n\u03c12(\u2016wi\u2016) poly(log d, t) the following bound holds \u2225\u2225\u2225\u2225\u2225Bi \u2212 1|S|\u2211\nx\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 \u2016Bi\u2016, with probability at least 1\u2212 1/d4t.\nTherefore, if n \u2265 \u22122d\u03c4 poly(log d, t), where \u03c4 = (3\u03c31/2) 4p\nmin\u03c3\u2208[\u03c3k/2,3\u03c31/2] \u03c1 2(\u03c3)\n, we have with probability\n1\u2212 1/d4t\n\u2016\u2206(2)i,i \u2016 . v 2 max\u03c3 2p 1\nClaim D.14. For each i \u2208 [k], j \u2208 [k], i 6= j, if n \u2265 \u22122\u03ba2\u03c4d poly(log d, t), then\n\u2016\u2206i,j\u2016 \u2264 v2max\u03c3 2p 1 (W \u2217)\nholds with probability 1\u2212 1/d4t.\nProof. Recall the definition of off-diagonal blocks \u2206i,l,\n\u2206i,l = v \u2217 i v \u2217 l\n( E\nx\u223cDd [\u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)xx >]\u2212 1 |S| \u2211 x\u2208S \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)xx >\n) (16)\nLet hi,l(x) = \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x). Define functionBi,l(x) = hi,l(x)xx >. LetBi,l = Ex\u223cDd [hi,l(x)xx>]. (I) Bounding |hi,l(x)|. For any constant t \u2265 1, we have with probability 1\u2212 1/(nd4t)\n|hi,l(x)| = |\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)| \u2264 L21\u2016w>i x\u2016p\u2016w>l x\u2016p\n\u2264 L21\u2016wi\u2016p\u2016wl\u2016p(t log n)p . L21\u03c3 2p 1 (t log n) p\nwhere the third step follows by Fact B.1. (II) Bounding \u2016Bi,l\u2016. Let U \u2208 Rd\u00d72 be the orthogonal basis of span{wi, wl} and U\u22a5 \u2208 Rd\u00d7(d\u22122) be the complementary matrix of U . Let matrix V \u2208 R2\u00d72 denote U>[wi wl], then UV = [wi wl] \u2208 Rd\u00d72. Given any vector a \u2208 Rd, there exist vectors b \u2208 R2 and c \u2208 Rd\u22122 such that a = Ub + U\u22a5c. We can simplify \u2016Bi,l\u2016 in the following way,\n\u2016Bi,l\u2016 = \u2225\u2225\u2225\u2225 Ex\u223cDd[\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)xx>] \u2225\u2225\u2225\u2225 = max \u2016a\u2016=1 E x\u223cDd [\u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)(x >a)2]\n= max \u2016b\u20162+\u2016c\u20162=1 E x\u223cDd\n[\u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)(b >U>x+ c>U>\u22a5x) 2]\n= max \u2016b\u20162+\u2016c\u20162=1 E x\u223cDd\n[\u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)((b >U>x)2 + (c>U>\u22a5x) 2)]\n= max \u2016b\u20162+\u2016c\u20162=1  Ez\u223cD2[\u03c6\u2032(v>1 z)\u03c6\u2032(v>2 z)(b>z)2]\ufe38 \ufe37\ufe37 \ufe38 A1 + E z\u223cD2,s\u223cDd\u22122 [\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(c >s)2]\ufe38 \ufe37\ufe37 \ufe38 A2 \nWe can lower bound the term A1,\nA1 = E z\u223cD2\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(b >z)2]\n= \u222b (2\u03c0)\u22121\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(b >z)2e\u2212\u2016z\u2016 2/2dz\n= \u222b (2\u03c0)\u22121\u03c6\u2032(s1)\u03c6 \u2032(s2)(b >V \u2020>s)2e\u2212\u2016V \u2020>s\u20162/2 \u00b7 | det(V \u2020)|ds\n\u2265 \u222b (2\u03c0)\u22121(\u03c6\u2032(s1)\u03c6 \u2032(s2))(b >V \u2020>s)2e\u2212\u03c3 2 1(V \u2020)\u2016s\u20162/2 \u00b7 | det(V \u2020)|ds\n= \u222b (2\u03c0)\u22121(\u03c6\u2032(u1/\u03c31(V \u2020))\u03c6\u2032(u2/\u03c31(V \u2020))) \u00b7 (b>V \u2020>u/\u03c31(V \u2020))2e\u2212\u2016u\u2016 2/2|det(V \u2020)|/\u03c321(V \u2020)du\n= \u03c32(V )\n\u03c31(V ) E u\u223cD2\n[ (p>u)2\u03c6\u2032(\u03c32(V ) \u00b7 u1)\u03c6\u2032(\u03c32(V ) \u00b7 u2) ] = \u03c32(V )\n\u03c31(V ) E u\u223cD2\n[( (p1u1) 2 + (p2u2) 2 + 2p1p2u1u2 ) \u03c6\u2032(\u03c32(V ) \u00b7 u1)\u03c6\u2032(\u03c32(V ) \u00b7 u2) ] = \u03c32(V )\n\u03c31(V )\n( \u2016p\u20162 E\nz\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)z2] \u00b7 E z\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)]\n+ ((p>1)2 \u2212 \u2016p\u20162) E z\u223cD1\n[\u03c6\u2032(\u03c32(V ) \u00b7 z)z]2 )\nwhere p = V \u2020b \u00b7 \u03c32(V ) \u2208 R2. Since we are maximizing over b \u2208 R2, we can choose b such that \u2016p\u2016 = \u2016b\u2016. Then\nA1 = E z\u223cD2\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(b >z)2]\n\u2265 \u03c32(V ) \u03c31(V )\n\u2016b\u20162 (\nE z\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)z2] \u00b7 E z\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)]\u2212 E z\u223cD1\n[\u03c6\u2032(\u03c32(V ) \u00b7 z)z]2 )\nFor the term A2, similarly we have\nA2 = E z\u223cD2,s\u223cDd\u22122\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(c >s)2]\n= E z\u223cD2\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)] E s\u223cDd\u22122 [(c>s)2]\n= \u2016c\u20162 E z\u223cD2 [\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)]\n\u2265 \u2016c\u20162\u03c32(V ) \u03c31(V )\n( E\nz\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)] )2 For simplicity, we just set \u2016b\u2016 = 1 and \u2016c\u2016 = 0 to lower bound,\u2225\u2225\u2225\u2225 Ex\u223cDd [ \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)xx > ]\u2225\u2225\u2225\u2225\n\u2265 \u03c32(V ) \u03c31(V )\n( E\nz\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)z2] \u00b7 E z\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)]\u2212\n( E\nz\u223cD1 [\u03c6\u2032(\u03c32(V ) \u00b7 z)z]\n)2)\n\u2265 \u03c32(V ) \u03c31(V ) \u03c1(\u03c32(V )) \u2265 1 \u03ba(W \u2217) \u03c1(\u03c32(V ))\nwhere the second step is from Property 3.2 and the fact that \u03c3k/2 \u2264 \u03c32(V ) \u2264 \u03c31(V ) \u2264 3\u03c31/2. For the upper bound of \u2016Ex\u223cDd [\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)xx>]\u2016, we have\nE z\u223cD2\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(b >z)2] \u2264 L21 E z\u223cD2 [|v>1 z|p \u00b7 |v>2 z|p \u00b7 |b>z|2]\n. L21\u2016v1\u2016p\u2016v2\u2016p\u2016b\u20162 . L21\u03c3 2p 1\nwhere the first step follows by Property 3.1, the second step follows by Fact B.6, and the last step follows by \u2016v1\u2016 \u2264 \u03c31, \u2016v2\u2016 \u2264 \u03c31 and \u2016b\u2016 \u2264 1. Similarly, we can upper bound,\nE z\u223cD2,s\u223cDd\u22122\n[\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)(c >s)2] = \u2016c\u20162 E z\u223cD2 [\u03c6\u2032(v>1 z)\u03c6 \u2032(v>2 z)] . L 2 1\u03c3 2p 1\nThus, we have \u2225\u2225\u2225\u2225 Ex\u223cDd[\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)xx>] \u2225\u2225\u2225\u2225 . L21\u03c32p1 . \u03c32p1 .\n(III) Bounding (Ex\u223cDd [h4i,l(x)])1/4.\n( E\nx\u223cDd [h4i,l(x)]\n)1/4 = ( E\nx\u223cDd [\u03c6\u20324(w>i x) \u00b7 \u03c6\u20324(w>l x)]\n)1/4 . L21\u2016wi\u2016p\u2016wl\u2016p . L21\u03c3 2p 1 .\nTherefore, applying Corollary B.8, we have, if n \u2265 \u22122\u03ba2(W \u2217)\u03c4d poly(log d, t), then\n\u2016\u2206i,j\u2016 \u2264 v2max\u03c3 2p 1 (W \u2217).\nholds with probability at least 1\u2212 1/d4t.\nD.4 Error Bound of Hessians near the Ground Truth for Non-smooth Activations\nThe goal of this Section is to prove Lemma D.15,\nLemma D.15 (Error bound of Hessians near the ground truth for non-smooth activations). Let \u03c6(z) satisfy Property 3.1,3.2 and 3.3(b). Let W satisfy \u2016W \u2212W \u2217\u2016 \u2264 \u03c3k/2. Let S denote a set of i.i.d. samples from the distribution defined in (1). Then for any t \u2265 1 and 0 < < 1/2, if\n|S| \u2265 \u22122\u03ba2\u03c4d poly(log d, t)\nwith probability at least 1\u2212 d\u2212\u2126(t),\n\u2016\u22072f\u0302S(W )\u2212\u22072fD(W \u2217)\u2016 . v2maxk\u03c3 2p 1 ( + (\u03c3 \u22121 k \u00b7 \u2016W \u2212W \u2217\u2016)1/2).\nProof. As we noted previously, when Property 3.3(b) holds, the diagonal blocks of the empirical Hessian can be written as, with probability 1,\n\u22022f\u0302S(W )\n\u2202w2i =\n1 |S| \u2211\n(x,y)\u2208S\n(v\u2217i \u03c6 \u2032(w>i x)) 2xx>\nWe construct a matrix H \u2208 Rdk\u00d7dk with i, l-th block as\nHi,l = v \u2217 i v \u2217 l E x\u223cDd\n[ \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)xx > ] \u2208 Rd\u00d7d, \u2200i \u2208 [k], l \u2208 [k].\nNote that H 6= \u22072fD(W ). However we can still bound \u2016H \u2212\u22072f\u0302S(W )\u2016 and \u2016H \u2212\u22072fD(W \u2217)\u2016 when we have enough samples and \u2016W \u2212W \u2217\u2016 is small enough. The proof for \u2016H \u2212 \u22072f\u0302S(W )\u2016 basically follows the proof of Lemma D.11 as \u2206(2)ii in Eq. (14) and \u2206il in Eq. (16) forms the blocks of H \u2212\u22072f\u0302S(W ) and we can bound them without smoothness of \u03c6(\u00b7).\nNow we focus on H \u2212\u22072fD(W \u2217). We again consider each block.\n\u2206i,l = E x\u223cDd\n[ v\u2217i v \u2217 l (\u03c6 \u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x))xx> ] .\nWe used the boundedness of \u03c6\u2032\u2032(z) to prove Lemma D.10. Here we can\u2019t use this condition. Without smoothness, we will stop at the following position.\u2225\u2225\u2225\u2225 Ex\u223cDd[v\u2217i v\u2217l (\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x))xx>]\n\u2225\u2225\u2225\u2225 \u2264 |v\u2217i v\u2217l | max\u2016a\u2016=1 E x\u223cDd [ |\u03c6\u2032(w>i x)\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>i x)\u03c6\u2032(w\u2217>l x)|(x>a)2\n] \u2264 |v\u2217i v\u2217l | max\u2016a\u2016=1 E x\u223cDd [ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|\n+ |\u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>l x)|(x>a)2 ]\n= |v\u2217i v\u2217l | max\u2016a\u2016=1\n( E\nx\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>a)2 ] + E\nx\u223cDd\n[ |\u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)\u2212 \u03c6\u2032(w\u2217>l x)|(x>a)2 ]) . (17)\nwhere the first step follows by definition of spectral norm, the second step follows by triangle inequality, and the last step follows by linearity of expectation.\nWithout loss of generality, we just bound the first term in the above formulation. Let U be the orthogonal basis of span(wi, w\u2217i , wl). If wi, w \u2217 i , wl are independent, U is d-by-3. Otherwise it can be d-by-rank(span(wi, w\u2217i , wl)). Without loss of generality, we assume U = span(wi, w \u2217 i , wl) is d-by-3. Let [vi v\u2217i vl] = U >[wi w \u2217 i wl] \u2208 R3\u00d73, and [ui u\u2217i ul] = U>\u22a5 [wi w\u2217i wl] \u2208 R(d\u22123)\u00d73. Let a = Ub+U\u22a5c, where U\u22a5 \u2208 Rd\u00d7(d\u22123) is the complementary matrix of U .\nE x\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>a)2 ] = E\nx\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>(Ub+ U\u22a5c))2 ] . E\nx\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)| ( (x>Ub)2 + (x>U\u22a5c) 2 )]\n= E x\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>Ub)2 ] + E\nx\u223cDd\n[ |\u03c6\u2032(w>i x)\u2212 \u03c6\u2032(w\u2217>i x)||\u03c6\u2032(w>l x)|(x>U\u22a5c)2 ] = E\nz\u223cD3\n[ |\u03c6\u2032(v>i z)\u2212 \u03c6\u2032(v\u2217>i z)||\u03c6\u2032(v>l z)|(z>b)2 ] + E\ny\u223cDd\u22123\n[ |\u03c6\u2032(u>i y)\u2212 \u03c6\u2032(u\u2217>i y)||\u03c6\u2032(u>l y)|(y>c)2 ] (18)\nwhere the first step follows by a = Ub+ U\u22a5c, the last step follows by (a+ b)2 \u2264 2a2 + 2b2. By Property 3.3(b), we have e exceptional points which have \u03c6\u2032\u2032(z) 6= 0. Let these e points be p1, p2, \u00b7 \u00b7 \u00b7 , pe. Note that if v>i z and v\u2217>i z are not separated by any of these exceptional points, i.e., there exists no j \u2208 [e] such that v>i z \u2264 pj \u2264 v\u2217>i z or v\u2217>i z \u2264 pj \u2264 v>i z, then we have \u03c6\u2032(v>i z) = \u03c6 \u2032(v\u2217>i z) since \u03c6 \u2032\u2032(s) are zeros except for {pj}j=1,2,\u00b7\u00b7\u00b7 ,e. So we consider the probability that v>i z, v \u2217> i z are separated by any exception point. We use \u03bej to denote the event that v > i z, v \u2217> i z\nare separated by an exceptional point pj . By union bound, 1\u2212 \u2211e\nj=1 Pr[\u03bej ] is the probability that v>i z, v \u2217> i z are not separated by any exceptional point. The first term of Equation (18) can be bounded as,\nE z\u223cD3\n[ |\u03c6\u2032(v>i z)\u2212 \u03c6\u2032(v\u2217>i z)||\u03c6\u2032(v>l z)|(z>b)2 ] = E\nz\u223cD3\n[ 1\u222aej=1\u03bej |\u03c6 \u2032(v>i z) + \u03c6 \u2032(v\u2217>i z)||\u03c6\u2032(v>l z)|(z>b)2 ] \u2264 (\nE z\u223cD3\n[ 1\u222aej=1\u03bej ])1/2( E\nz\u223cD3\n[ (\u03c6\u2032(v>i z) + \u03c6 \u2032(v\u2217>i z)) 2\u03c6\u2032(v>l z) 2(z>b)4 ])1/2\n\u2264  e\u2211 j=1 Pr z\u223cD3 [\u03bej ] 1/2( E z\u223cD3 [ (\u03c6\u2032(v>i z) + \u03c6 \u2032(v\u2217>i z)) 2\u03c6\u2032(v>l z) 2(z>b)4 ])1/2\n.  e\u2211 j=1 Pr z\u223cD3 [\u03bej ] 1/2 (\u2016vi\u2016p + \u2016v\u2217i \u2016p)\u2016vl\u2016p\u2016b\u20162 where the first step follows by if v>i z, v \u2217> i z are not separated by any exceptional point then \u03c6\n\u2032(v>i z) = \u03c6\u2032(v\u2217>i z) and the last step follows by H\u00f6lder\u2019s inequality and Property 3.1.\nIt remains to upper bound Prz\u223cD3 [\u03bej ]. First note that if v>i z, v \u2217> i z are separated by an excep-\ntional point, pj , then |v\u2217>i z \u2212 pj | \u2264 |v>i z \u2212 v\u2217>i z| \u2264 \u2016vi \u2212 v\u2217i \u2016\u2016z\u2016. Therefore,\nPr z\u223cD3 [\u03bej ] \u2264 Pr z\u223cD3 [ |v>i z \u2212 pj | \u2016z\u2016 \u2264 \u2016vi \u2212 v\u2217i \u2016 ] .\nNote that ( v \u2217> i z\n\u2016z\u2016\u2016v\u2217i \u2016 + 1)/2 follows Beta(1,1) distribution which is uniform distribution on [0, 1].\nPr z\u223cD3 [ |v\u2217>i z \u2212 pj | \u2016z\u2016\u2016v\u2217i \u2016 \u2264 \u2016vi \u2212 v \u2217 i \u2016 \u2016v\u2217i \u2016 ] \u2264 Pr z\u223cD3 [ |v\u2217>i z| \u2016z\u2016\u2016v\u2217i \u2016 \u2264 \u2016vi \u2212 v \u2217 i \u2016 \u2016v\u2217i \u2016 ] . \u2016vi \u2212 v\u2217i \u2016 \u2016v\u2217i \u2016 . \u2016W \u2212W \u2217\u2016 \u03c3k(W \u2217) ,\nwhere the first step is because we can view v \u2217> i z \u2016z\u2016 and pj \u2016z\u2016 as two independent random variables: the former is about the direction of z and the later is related to the magnitude of z. Thus, we have\nE z\u2208D3\n[|\u03c6\u2032(v>i z)\u2212 \u03c6\u2032(v\u2217>i z)||\u03c6\u2032(v>l z)|(z>b)2] . (e\u2016W \u2212W \u2217\u2016/\u03c3k(W \u2217))1/2\u03c3 2p 1 (W \u2217)\u2016b\u20162. (19)\nSimilarly we have\nE y\u2208Dd\u22123\n[|\u03c6\u2032(u>i y)\u2212 \u03c6\u2032(u\u2217>i y)||\u03c6\u2032(u>l y)|(y>c)2] . (e\u2016W \u2212W \u2217\u2016/\u03c3k(W \u2217))1/2\u03c3 2p 1 (W \u2217)\u2016c\u20162. (20)\nFinally combining Eq. (17), Eq. (19) and Eq. (20), we have\n\u2016H \u2212\u22072fD(W \u2217)\u2016 . kv2max(e\u2016W \u2212W \u2217\u2016/\u03c3k(W \u2217))1/2\u03c3 2p 1 (W \u2217),\nwhich completes the proof."}, {"heading": "D.5 Positive Definiteness for a Small Region", "text": "Here we introduce a lemma which shows that the Hessian of any W , which may be dependent on the samples but is very close to an anchor point, is close to the Hessian of this anchor point.\nLemma D.16. Let S denote a set of samples from Distribution D defined in Eq. (1). Let W a \u2208 Rd\u00d7k be a point (respect to function f\u0302S(W )), which is independent of the samples S, satisfying \u2016W a \u2212W \u2217\u2016 \u2264 \u03c3k/2. Assume \u03c6 satisfies Property 3.1, 3.2 and 3.3(a). Then for any t \u2265 1, if\n|S| \u2265 dpoly(log d, t),\nwith probability at least 1\u2212 d\u2212t, for any W (which is not necessarily to be independent of samples) satisfying \u2016W a \u2212W\u2016 \u2264 \u03c3k/4, we have\n\u2016\u22072f\u0302S(W )\u2212\u22072f\u0302S(W a)\u2016 \u2264 kv2max\u03c3 p 1(\u2016W a \u2212W \u2217\u2016+ \u2016W \u2212W a\u2016d(p+1)/2).\nProof. Let \u2206 = \u22072f\u0302S(W )\u2212\u22072f\u0302S(W a) \u2208 Rdk\u00d7dk, then \u2206 can be thought of as k2 blocks, and each block has size d\u00d7 d. The off-diagonal blocks are,\n\u2206i,l = v \u2217 i v \u2217 l\n1 |S| \u2211 x\u2208S ( \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(wa>i x)\u03c6\u2032(wa>l x) ) xx>\nFor diagonal blocks,\n\u2206i,i = 1 |S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w > q x)\u2212 y  v\u2217i \u03c6\u2032\u2032(w>i x)xx> + v\u22172i \u03c6\u20322(w>i x)xx> \n\u2212 1 |S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w a> q x)\u2212 y  v\u2217i \u03c6\u2032\u2032(wa>i x)xx> + v\u22172i \u03c6\u20322(wa>i x)xx> \nWe further decompose \u2206i,i into \u2206i,i = \u2206 (1) i,i + \u2206 (2) i,i , where\n\u2206 (1) i,i = v \u2217 i\n1 |S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w > q x)\u2212 y \u03c6\u2032\u2032(w>i x)\u2212  k\u2211 q=1 v\u2217q\u03c6(w a> q x)\u2212 y \u03c6\u2032\u2032(wa>i x) xx>,\nand\n\u2206 (2) i,i =v \u22172 i\n1 |S| \u2211\n(x,y)\u2208S\n\u03c6\u20322(w>i x)xx > \u2212 \u03c6\u20322(wa>i x)xx>. (21)\nWe can further decompose \u2206(1)i,i into \u2206 (1,1) i,i and \u2206 (1,2) i,i ,\n\u2206 (1) i,i = v \u2217 i\n1 |S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w > q x)\u2212 y \u03c6\u2032\u2032(w>i x)\u2212  k\u2211 q=1 v\u2217q\u03c6(w a> q x)\u2212 y \u03c6\u2032\u2032(wa>i x)xx> = v\u2217i 1\n|S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w > q x)\u2212 k\u2211 q=1 v\u2217q\u03c6(w a> q x) \u03c6\u2032\u2032(w>i x)xx> + v\u2217i 1\n|S| \u2211 x\u2208S  k\u2211 q=1 v\u2217q\u03c6(w a> q x)\u2212 y  (\u03c6\u2032\u2032(w>i x)\u2212 \u03c6\u2032\u2032(wa>i x))xx> = v\u2217i 1\n|S| \u2211 x\u2208S k\u2211 q=1 v\u2217q (\u03c6(w > q x)\u2212 \u03c6(wa>q x))\u03c6\u2032\u2032(w>i x)xx>\n+ v\u2217i 1 |S| \u2211 x\u2208S k\u2211 q=1 v\u2217q (\u03c6(w a> q x)\u2212 \u03c6(w\u2217>q x))(\u03c6\u2032\u2032(w>i x)\u2212 \u03c6\u2032\u2032(wa>i x))xx>\n= \u2206 (1,1) i,i + \u2206 (1,2) i,i .\nCombining Claim D.17 and Claim D.18 , we have if\nn \u2265 dpoly(log d, t)\nwith probability at least 1\u2212 1/d4t,\n\u2016\u2206(1)i,i \u2016 . kv 2 max\u03c3 p 1(\u2016W a \u2212W \u2217\u2016+ \u2016W a \u2212W\u2016d(p+1)/2). (22)\nTherefore, combining Eq. (22), Claim D.19 and Claim D.20, we complete the proof.\nClaim D.17. For each i \u2208 [k], if n \u2265 dpoly(log d, t), then\n\u2016\u2206(1,1)i,i \u2016 . kv 2 max\u03c3 p 1\u2016W a \u2212W\u2016d(p+1)/2\nProof. Define function h1(x) = \u2016x\u2016p+1 and h2(x) = |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|. Note that h1 and h2 don\u2019t contain W which maybe depend on the samples. Therefore, we can use the modified matrix Bernstein inequality (Corollary B.8) to bound \u2206(1)i,i .\n(I) Bounding |h1(x)|. By Fact B.2, we have h1(x) . (td log n)(p+1)/2 with probability at least 1\u2212 1/(nd4t). (II) Bounding \u2016Ex\u223cDd [\u2016x\u2016p+1xx>]\u2016.\nLet g(x) = (2\u03c0)\u2212d/2e\u2212\u2016x\u20162/2. Note that xg(x)dx = \u2212dg(x).\nE x\u223cDd\n[ \u2016x\u2016p+1xx> ] = \u222b \u2016x\u2016p+1g(x)xx>dx\n= \u2212 \u222b \u2016x\u2016p+1d(g(x))x>\n= \u2212 \u222b \u2016x\u2016p+1d(g(x)x>) + \u222b \u2016x\u2016p+1g(x)Iddx\n= \u222b d(\u2016x\u2016p+1)g(x)x> + \u222b \u2016x\u2016p+1g(x)Iddx\n= \u222b (p+ 1)\u2016x\u2016p\u22121g(x)xx>dx+ \u222b \u2016x\u2016p+1g(x)Iddx\n\u222b \u2016x\u2016p+1g(x)Iddx\n= E x\u223cDd [\u2016x\u2016p+1]Id.\nSince \u2016x\u20162 follows \u03c72 distribution with degree d, Ex\u223cDd [\u2016x\u2016q] = 2q/2 \u0393((d+q)/2) \u0393(d/2) for any q \u2265 0. So, dq/2 . Ex\u223cDd [\u2016x\u2016q] . dq/2. Hence, \u2016Ex\u223cDd [h1(x)xx>]\u2016 & d(p+1)/2. Also\u2225\u2225\u2225\u2225 Ex\u223cDd [ h1(x)xx > ]\u2225\u2225\u2225\u2225 \u2264 max\u2016a\u2016=1 Ex\u223cDd [ h1(x)(x >a)2 ]\n\u2264 max \u2016a\u2016=1\n( E\nx\u223cDd\n[ h21(x) ])1/2( E\nx\u223cDd\n[ (x>a)4 ])1/2 . d(p+1)/2.\n(III) Bounding (Ex\u223cDd [h41(x)])1/4.\n( E\nx\u223cDd [h41(x)]\n)1/4 . d(p+1)/2.\nDefine function B(x) = h(x)xx> \u2208 Rd\u00d7d, \u2200i \u2208 [n]. Let B = Ex\u223cDd [h(x)xx>]. Therefore by applying Corollary B.8, we obtain for any 0 < < 1, if\nn \u2265 \u22122dpoly(log d, t)\nwith probability at least 1\u2212 1/dt,\u2225\u2225\u2225\u2225\u2225 1|S|\u2211 x\u2208S \u2016x\u2016p+1xx> \u2212 E x\u223cDd [ \u2016x\u2016p+1xx> ]\u2225\u2225\u2225\u2225\u2225 . \u03b4d(p+1)/2. Therefore we have with probability at least 1\u2212 1/dt,\u2225\u2225\u2225\u2225\u2225 1|S|\u2211\nx\u2208S \u2016x\u2016p+1xx>\n\u2225\u2225\u2225\u2225\u2225 . d(p+1)/2. (23)\nClaim D.18. For each i \u2208 [k], if n \u2265 dpoly(log d, t), then\n\u2016\u2206(1,2)i,i \u2016 . k\u03c3 p 1\u2016W a \u2212W \u2217\u2016,\nholds with probability at least 1\u2212 1/d4t.\nProof. Recall the definition of \u2206(1,2)i,i ,\n\u2206 (1,2) i,i = v \u2217 i\n1 |S| \u2211 x\u2208S k\u2211 q=1 v\u2217q (\u03c6(w a> q x)\u2212 \u03c6(w\u2217>q x))(\u03c6\u2032\u2032(w>i x)\u2212 \u03c6\u2032\u2032(wa>i x))xx>\nIn order to upper bound the \u2016\u2206(1,2)i,i \u2016, it suffices to upper bound the spectral norm of this quantity, 1 |S| \u2211 x\u223cS (\u03c6(wa>q x)\u2212 \u03c6(w\u2217>q x))(\u03c6\u2032\u2032(w>i x)\u2212 \u03c6\u2032\u2032(wa>i x))xx>.\nBy Property 3.1, we have\n|\u03c6(wa>q x)\u2212 \u03c6(w\u2217>q x)| . L1(|wa>q x|p + |w\u2217>q x|)|(w\u2217q \u2212 waq )>x|.\nBy Property 3.3, we have |\u03c6\u2032\u2032(w>i x)\u2212 \u03c6\u2032\u2032(wa>i x)| \u2264 2L2. For the second part |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|xx>, according to Eq. (15), we have with probability 1\u2212 d\u2212t, if n \u2265 d poly(log d, t),\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd [ |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|xx> ] \u2212 1 |S| \u2211 x\u2208S |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|xx>\n\u2225\u2225\u2225\u2225\u2225 . \u03b4\u2016w\u2217q\u2016p\u2016w\u2217q \u2212 waq\u2016. Also, note that \u2225\u2225\u2225\u2225 Ex\u223cDd [ |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|xx>\n]\u2225\u2225\u2225\u2225 . \u2016w\u2217q\u2016p\u2016w\u2217q \u2212 waq\u2016. Thus, we obtain \u2225\u2225\u2225\u2225\u2225 1|S|\u2211\nx\u2208S |w\u2217>q x|p|(w\u2217q \u2212 waq )>x|xx> \u2225\u2225\u2225\u2225\u2225 . \u2016W a \u2212W \u2217\u2016\u03c3p1 . (24) Claim D.19. For each i \u2208 [k], if n \u2265 dpoly(log d, t), then\n\u2016\u2206(2)i,i \u2016 . kv 2 max\u03c3 p 1\u2016W \u2212W a\u2016d(p+1)/2\nholds with probability 1\u2212 1/d4t. Proof. We have\n\u2016\u2206(2)i,i \u2016 \u2264 v \u22172 i \u2225\u2225\u2225\u2225\u2225 1|S|\u2211 x\u2208S ( (\u03c6\u2032(w>i xj)\u2212 \u03c6\u2032(wa>i x)) \u00b7 (\u03c6\u2032(w>i x) + \u03c6\u2032(wa>i x)) ) xx> \u2225\u2225\u2225\u2225\u2225 \u2264 v\u22172i\n\u2225\u2225\u2225\u2225\u2225 1|S|\u2211 x\u2208S ( L2|(wi \u2212 wai )>x| \u00b7 L1(|w>i x|p + |wa>i x|p) ) xx> \u2225\u2225\u2225\u2225\u2225 \u2264 v\u22172i \u2016W \u2212W a\u2016\n\u2225\u2225\u2225\u2225\u2225 1|S|\u2211 x\u2208S ( L2\u2016x\u2016 \u00b7 L1(\u2016wi\u2016p\u2016x\u2016p + |wa>i x|p) ) xx> \u2225\u2225\u2225\u2225\u2225 . Applying Corollary B.8 finishes the proof.\nClaim D.20. For each i \u2208 [k], j \u2208 [k], i 6= j, if n \u2265 dpoly(log d, t), then\n\u2016\u2206i,l\u2016 . v2max\u03c3 p 1\u2016W a \u2212W\u2016\nholds with probability 1\u2212 d4t.\nProof. Recall the definition of \u2206i,l,\n\u2206i,l = v \u2217 i v \u2217 l\n1 |S| \u2211 x\u2208S ( \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(w>i x)\u03c6\u2032(wa>l x)\n+\u03c6\u2032(w>i x)\u03c6 \u2032(wa>l x)\u2212 \u03c6\u2032(wa>i x)\u03c6\u2032(wa>l x)\n) xx>\n= v\u2217i v \u2217 l\n1 |S| \u2211 x\u2208S ( \u03c6\u2032(w>i x)\u03c6 \u2032(w>l x)\u2212 \u03c6\u2032(w>i x)\u03c6\u2032(wa>l x) )\n+ v\u2217i v \u2217 l\n1 |S| \u2211 x\u2208S ( \u03c6\u2032(w>i x)\u03c6 \u2032(wa>l x)\u2212 \u03c6\u2032(wa>i x)\u03c6\u2032(wa>l x) ) xx>\n|v\u2217i v\u2217l | 1 |S| \u2211 x\u2208S ( L1\u2016wi\u2016pL2\u2016wl \u2212 wal \u2016\u2016x\u2016p+1 + L2\u2016wi \u2212 wai \u2016\u2016x\u2016L1\u2016wa>l x\u2016p ) xx>\nApplying Corollary B.8 completes the proof."}, {"heading": "E Tensor Methods", "text": ""}, {"heading": "E.1 Tensor Initialization Algorithm", "text": "We describe the details of each procedure in Algorithm 1 in this section. a) Compute the subspace estimation from P\u03022 (Algorithm 3). Note that the eigenvalues of P2 and P\u03022 are not necessarily nonnegative. However, only k of the eigenvalues will have large magnitude. So we can first compute the top-k eigenvectors/eigenvalues of both C \u00b7 I + P\u03022 and C \u00b7 I \u2212 P\u03022, where C is large enough such that C \u2265 2\u2016P2\u2016. Then from the 2k eigen-pairs, we pick the top-k eigenvectors with the largest eigenvalues in magnitude, which is executed in TopK in Algorithm 3. For the outputs of TopK, k1, k2 are the numbers of picked largest eigenvalues from C \u00b7 I + P\u03022 and C \u00b7 I \u2212 P\u03022 respectively and \u03c01(i) returns the original index of i-th largest eigenvalue from C \u00b7 I + P\u03022 and similarly \u03c02 is for C \u00b7 I \u2212 P\u03022. Finally orthogonalizing the picked eigenvectors leads to an estimation of the subspace spanned by {w\u22171 w\u22172 \u00b7 \u00b7 \u00b7 w\u2217k}. Also note that forming P\u03022 takes O(n \u00b7 d2) time and each step of the power method doing a multiplication between a d \u00d7 d matrix and a d \u00d7 k matrix takes k \u00b7 d2 time by a naive implementation. Here we reduce this complexity from O((k + n)d2) to O(knd). The idea is to compute each step of the power method without explicitly forming P\u03022. We take P2 = M2 as an example; other cases are similar. In Algorithm 3, let the step P\u03022V be calculated as P\u03022V = 1|S| \u2211 (x,y)\u2208S y(x(x\n>V ) \u2212 V ). Now each iteration only needs O(knd) time. Furthermore, the number of iterations required will be a small number, since the power method has a linear convergence rate and as an initialization method we don\u2019t need a very accurate solution. The detailed algorithm is shown in Algorithm 3. The approximation error bound of P\u03022 to P2 is provided in Lemma E.5. Lemma E.6 provides the theoretical bound for Algorithm 3.\nb) Form and decompose the 3rd-moment R\u03023 (Algorithm 1 in [KCL15]). We apply the non-orthogonal tensor factorization algorithm, Algorithm 1 in [KCL15], to decompose R\u03023. According to Theorem 3 in [KCL15], when R\u03023 is close enough to R3, the output of the algorithm, u\u0302i will\nclose enough to siV >w\u2217i , where si is an unknown sign. Lemma E.10 provides the error bound for \u2016R\u03023 \u2212R3\u2016.\nc) Recover the magnitude of w\u2217i and the signs si, v \u2217 i (Algorithm 4). For Algorithm 4, we only consider homogeneous functions. Hence we can assume v\u2217i \u2208 {\u22121, 1} and there exist some universal constants cj such that mj,i = cj\u2016w\u2217i \u2016p+1 for j = 1, 2, 3, 4, where p + 1 is the degree of homogeneity. Note that different activations have different situations even under Assumption 5.3. In particular, ifM4 = M2 = 0, \u03c6(\u00b7) is an odd function and we only need to know siv\u2217i . IfM3 = M1 = 0, \u03c6(\u00b7) is an even function, so we don\u2019t care about what si is.\nLet\u2019s describe the details for Algorithm 4. First define two quantities Q1 and Q2,\nQ1 = Ml1(I, \u03b1, \u00b7 \u00b7 \u00b7 , \u03b1\ufe38 \ufe37\ufe37 \ufe38 (l1\u22121) \u03b1\u2019s ) = k\u2211 i=1 v\u2217i cl1\u2016w\u2217i \u2016p+1(\u03b1>w\u2217i )l1\u22121w\u2217i , (25)\nQ2 = Ml2(V, V, \u03b1, \u00b7 \u00b7 \u00b7 , \u03b1\ufe38 \ufe37\ufe37 \ufe38 (l2\u22122) \u03b1\u2019s ) = k\u2211 i=1 v\u2217i cl2\u2016w\u2217i \u2016p+1(\u03b1>w\u2217i )l2\u22122(V >w\u2217i )(V >w\u2217i )>, (26)\nwhere l1 \u2265 1 such that Ml1 6= 0 and l2 \u2265 2 such that Ml2 6= 0. There are possibly multiple choices for l1 and l2. We will discuss later on how to choose them. Now we solve two linear systems.\nz\u2217 = argmin z\u2208Rk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 zisiw \u2217 i \u2212Q1 \u2225\u2225\u2225\u2225\u2225 , and r\u2217 = argminr\u2208Rk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 riV >w\u2217i (V >w\u2217i ) > \u2212Q2 \u2225\u2225\u2225\u2225\u2225 F . (27)\nThe solutions of the above linear systems are\nz\u2217i = v \u2217 i s l1 i cl1\u2016w \u2217 i \u2016p+1(\u03b1>siw\u2217i )l1\u22121, and r\u2217i = v\u2217i s l2 i cl2\u2016w \u2217 i \u2016p+1(\u03b1>siw\u2217i )l2\u22122.\nWe can approximate siw\u2217i by V u\u0302i and approximate Q1 and Q2 by their empirical versions Q\u03021 and Q\u03022 respectively. Hence, in practice, we solve\nz\u0302 = argmin z\u2208Rk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 ziV u\u0302i \u2212 Q\u03021 \u2225\u2225\u2225\u2225\u2225 , and r\u0302 = argminr\u2208Rk \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 riu\u0302iu\u0302 > i \u2212 Q\u03022 \u2225\u2225\u2225\u2225\u2225 F\n(28)\nSo we have the following approximations,\nz\u0302i \u2248 v\u2217i s l1 i cl1\u2016w \u2217 i \u2016p+1(\u03b1>V u\u0302i)l1\u22121, and r\u0302i \u2248 v\u2217i s l2 i cl2\u2016w \u2217 i \u2016p+1(\u03b1>V u\u0302i)l2\u22122, \u2200i \u2208 [k].\nIn Lemma E.13 and Lemma E.14, we provide robustness of the above two linear systems, i.e., the solution errors, \u2016z\u0302 \u2212 z\u2217\u2016 and \u2016r\u0302 \u2212 r\u2217\u2016, are bounded under small perturbations of w\u2217i , Q1 and Q2. Recall that the final goal is to approximate \u2016w\u2217i \u2016 and the signs v\u2217i , si. Now we can approximate \u2016w\u2217i \u2016 by (|z\u0302i/(cl1(\u03b1>V u\u0302i)l1\u22121)|)1/(p+1). To recover v\u2217i , si, we need to note that if l1 and l2 are both odd or both even, we can\u2019t recover both v\u2217i and si. So we consider the following situations,\n1. If M1 = M3 = 0, we choose l1 = l2 = min{j \u2208 {2, 4}|Mj 6= 0}. Return v(0)i = sign(r\u0302icl2) and s\n(0) i being \u22121 or 1.\n2. If M2 = M4 = 0, we choose l1 = min{j \u2208 {1, 3}|Mj 6= 0}, l2 = 3. Return v(0)i being \u22121 or 1 and s(0)i = sign(v (0) i z\u0302icl1).\nAlgorithm 3 Power Method via Implicit Matrix-Vector Multiplication\n1: procedure PowerMethod(P\u03022, k) 2: C \u2190 3\u2016P\u03022\u2016, T \u2190 a large enough constant. 3: Initial guess V\u0302 (0)1 \u2208 R d\u00d7k, V\u0302 (0) 2 \u2208 R d\u00d7k 4: for t = 1\u2192 T do 5: V\u0302\n(t) 1 \u2190 QR(CV\u0302 (t\u22121) 1 + P\u03022V\u0302 (t\u22121) 1 ) . P\u03022V\u0302 (t\u22121) 1 is not calculated directly, see Sec. E.1(a)\n6: V\u0302 (t) 2 \u2190 QR(CV\u0302 (t\u22121) 2 \u2212 P\u03022V\u0302 (t\u22121)\n2 ) 7: end for 8: for j = 1, 2 do 9: V\u0302\n(T ) j \u2190 [ v\u0302j,1 v\u0302j,2 \u00b7 \u00b7 \u00b7 v\u0302j,k ] 10: for i = 1\u2192 k do 11: \u03bbj,i \u2190 |v\u0302>j,iP\u03022v\u0302j,i| . Calculate the absolute of eigenvalues 12: end for 13: end for 14: \u03c01, \u03c02, k1, k2 \u2190 TopK(\u03bb, k) . \u03c0j : [kj ]\u2192 [k] and k1 + k2 = k, see Sec. E.1(a) 15: for j = 1, 2 do 16: Vj \u2190 [ v\u0302j,\u03c0j(1) v\u03021,\u03c0j(2) \u00b7 \u00b7 \u00b7 v\u0302j,\u03c0j(kj)\n] 17: end for 18: V\u03032 \u2190 QR((I \u2212 V1V >1 )V2) 19: V \u2190 [V1, V\u03032] 20: return V 21: end procedure\n3. Otherwise, we choose l1 = min{j \u2208 {1, 3}|Mj 6= 0}, l2 = min{j \u2208 {2, 4}|Mj 6= 0}. Return v\n(0) i = sign(r\u0302icl2) and s (0) i = sign(v (0) i z\u0302icl1).\nThe 1st situation corresponds to part 3 of Assumption 5.3,where si doesn\u2019t matter, and the 2nd situation corresponds to part 4 of Assumption 5.3, where only siv\u2217i matters. So we recover \u2016w\u2217i \u2016 to some precision and v\u2217i , si exactly provided enough samples. The recovery of w \u2217 i and v \u2217 i then follows.\nSample complexity: We use matrix Bernstein inequality to bound the error between P\u03022 and P2, which requires \u2126\u0303(d) samples (Lemma E.5). To bound the estimation error between R3 and R\u03023, we flatten the tensor to a matrix and then use matrix Bernstein inequality to bound the error, which requires \u2126\u0303(k3) samples (Lemma E.10). In Algorithm 4, we also need to approximate a Rd vector and a Rk\u00d7k matrix, which also requires \u2126\u0303(d). Thus, taking O\u0303(d) + O\u0303(k3) samples is sufficient.\nTime complexity: In Part a), by using a specially designed power method, we only need O(knd) time to compute the subspace estimation V . Part b) needs O(knd) to form R\u03023 and the tensor factorization needs O(k3) time. Part c) requires calculation of d\u00d7k and k2\u00d7k linear systems in Eq. (28), which takes at most O(knd) running time. Hence, the total time complexity is O(knd)."}, {"heading": "E.2 Main Result for Tensor Methods", "text": "The goal of this Section is to prove Theorem 5.6.\nTheorem 5.6. Let the activation function be homogeneous satisfying Assumption 5.3. For any 0 < < 1 and t \u2265 1, if |S| \u2265 \u22122 \u00b7d \u00b7poly(t, k, \u03ba, log d), then there exists an algorithm (Algorithm 1) that takes |S|k \u00b7 O\u0303(d) time and outputs a matrix W (0) \u2208 Rd\u00d7k and a vector v(0) \u2208 Rk such that, with\nAlgorithm 4 Recovery of the Ground Truth Parameters of the Neural Network, i.e., w\u2217i and v \u2217 i\n1: procedure RecMagSign(V, {u\u0302i}i\u2208[k], S) 2: if M1 = M3 = 0 then 3: l1 \u2190 l2 \u2190 min{j \u2208 {2, 4}|Mj 6= 0} 4: else if M2 = M4 = 0 then 5: l1 \u2190 min{j \u2208 {1, 3}|Mj 6= 0}, l2 \u2190 3 6: else 7: l1 \u2190 min{j \u2208 {1, 3}|Mj 6= 0}, l2 \u2190 min{j \u2208 {2, 4}|Mj 6= 0}. 8: end if 9: S1, S2 \u2190 Partition(S, 2) . |S1|, |S2| = \u2126\u0303(d) 10: Choose \u03b1 to be a random unit vector 11: Q\u03021 \u2190 ES1 [Q1] . Q\u03021 is the empirical version of Q1(defined in Eq.(25)) 12: Q\u03022 \u2190 ES2 [Q2] . Q\u03022 is the empirical version of Q2(defined in Eq.(26)) 13: z\u0302 \u2190 argminz\n\u2225\u2225\u2225\u2211ki=1 ziV u\u0302i \u2212 Q\u03021\u2225\u2225\u2225 14: r\u0302 \u2190 argminr \u2225\u2225\u2225\u2211ki=1 riu\u0302iu\u0302>i \u2212 Q\u03022\u2225\u2225\u2225 F 15: v (0) i \u2190 sign(r\u0302icl2) 16: s (0) i \u2190 sign(v (0) i z\u0302icl1) 17: w (0) i \u2190 s (0) i (|z\u0302i/(cl1(\u03b1>V u\u0302i)l1\u22121)|)1/(p+1)V u\u0302i 18: return v(0)i ,w (0) i 19: end procedure\nprobability at least 1\u2212 d\u2212\u2126(t),\n\u2016W (0) \u2212W \u2217\u2016F \u2264 \u00b7 poly(k, \u03ba)\u2016W \u2217\u2016F , and v(0)i = v \u2217 i .\nProof. The success of Algorithm 1 depends on two approximations. The first is the estimation of the normalized w\u2217i up to some unknown sign flip, i.e., the error \u2016w\u2217i \u2212 siV u\u0302i\u2016 for some si \u2208 {\u22121, 1}. The second is the estimation of the magnitude of w\u2217i and the signs v \u2217 i , si which is conducted in Algorithm 4. For the first one,\n\u2016w\u2217i \u2212 siV u\u0302i\u2016 \u2264 \u2016V V >w\u2217i \u2212 w\u2217i \u2016+ \u2016V V >w\u2217i \u2212 V siu\u0302i\u2016 = \u2016V V >w\u2217i \u2212 w\u2217i \u2016+ \u2016V >w\u2217i \u2212 siu\u0302i\u2016, (29)\nwhere the first step follows by triangle inequality, the second step follows by V >V = I. We can upper bound \u2016V V >w\u2217i \u2212 w\u2217i \u2016,\n\u2016V V >w\u2217i \u2212 w\u2217i \u2016 \u2264 (\u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + )\n\u2264 (poly(k, \u03ba)\u2016P\u03022 \u2212 P2\u2016+ ) \u2264 poly(k, \u03ba) , (30)\nwhere the first step follows by Lemma E.6, the second step follows by \u03c3k(P2) \u2265 1/poly(k, \u03ba), and the last step follows by \u2016P\u03022 \u2212 P2\u2016 \u2264 poly(k, \u03ba) if the number of samples is proportional to O\u0303(d/ 2) as shown in Lemma E.5.\nWe can upper bound \u2016V >w\u2217i \u2212 siu\u0302i\u2016,\n\u2016V >w\u2217i \u2212 siu\u0302i\u2016 \u2264 poly(k, \u03ba)\u2016R\u03023 \u2212R3\u2016 \u2264 poly(k, \u03ba), (31)\nwhere the first step follows by Theorem 3 in [KCL15], and the last step follows by \u2016R\u03023 \u2212 R3\u2016 \u2264 poly(k, \u03ba) if the number of samples is proportional to O\u0303(k2/ 2) as shown in Lemma E.10.\nCombining Eq. (29), (30) and (31) together,\n\u2016w\u2217i \u2212 siV u\u0302i\u2016 \u2264 poly(k, \u03ba).\nFor the second one, we can bound the error of the estimation of moments, Q1 and Q2, using number of samples proportional to O\u0303(d) by Lemma E.12 and Lemma E.5 respectively. The error of the solutions of the linear systems Eq.(28) can be bounded by \u2016Q1\u2212 Q\u03021\u2016, \u2016Q2\u2212 Q\u03022\u2016, \u2016u\u0302i\u2212 V >w\u2217i \u2016 and \u2016(I \u2212 V V >)w\u2217i \u2016 according to Lemma E.13 and Lemma E.14. Then we can bound the error of the output of Algorithm 4. Furthermore, since v\u2217i \u2019s are discrete values, they can be exactly recovered. All the sample complexities mentioned in the above lemmata are linear in dimension and polynomial in other factors to achieve a constant error. So accumulating all these errors we complete our proof.\nRemark E.1. The proofs of these lemmata for Theorem 1 can be found in the following sections. Note that these lemmata also hold for any activations satisfying Property 3.1 and Assumption 5.3. However, since we are unclear how to implement the last step of Algorithm 1 (Algorithm 4) for general non-homogeneous activations, we restrict our theorem to homogeneous activations only."}, {"heading": "E.3 Error Bound for the Subspace Spanned by the Weight Matrix", "text": ""}, {"heading": "E.3.1 Error Bound for the Second-order Moment in Different Cases", "text": "Lemma E.2. Let M2 be defined as in Definition 5.1. Let M\u03022 be the empirical version of M2, i.e.,\nM\u03022 = 1 |S| \u2211\n(x,y)\u2208S\ny \u00b7 (x\u2297 x\u2212 Id),\nwhere S denote a set of samples from Distribution D defined in Eq. (1). Assume M2 6= 0, i.e., m\n(2) i 6= 0 for any i. Then for any 0 < < 1, t \u2265 1, if\n|S| \u2265 max i\u2208[k] (\u2016w\u2217i \u2016p+1/|m2,i|+ 1) \u00b7 \u22122dpoly(log d, t)\nwith probability at least 1\u2212 d\u2212t,\n\u2016M2 \u2212 M\u03022\u2016 \u2264 k\u2211 i=1 |v\u2217im2,i|.\nProof. Recall that, for each sample (x, y), y = \u2211k\ni=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component\ni \u2208 [k]. Define function Bi(x) : Rd \u2192 Rd\u00d7d such that\nBi(x) = \u03c6(w \u2217> i x) \u00b7 (x\u2297 x\u2212 Id). Define g(z) = \u03c6(z)\u2212\u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1/(p+1)|z|p+1, which follows Property 3.1.\n(I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 . ( L1 p+ 1 |w\u2217>i x|p+1 + |\u03c6(0)|)(\u2016xj\u20162 + 1)\n. ( L1 p+ 1 \u2016w\u2217i \u2016p+1 + |\u03c6(0)|)dpoly(log d, t)\nwhere the last step follows by Fact B.1 and Fact B.2. (II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m2,iw\u2217iw\u2217>i . Therefore, \u2016Ex\u223cDd [Bi(x)]\u2016 = |m2,i|. (III) Bounding max(Ex\u223cDd \u2016Bi(x)Bi(x)>\u2016,Ex\u223cDd \u2016Bi(x)>Bi(x)\u2016). Note that Bi(x) is a symmetric matrix, thus it suffices to only bound one of them.\u2225\u2225\u2225\u2225 Ex\u223cDd[B2i (x)] \u2225\u2225\u2225\u2225 . ( Ex\u223cDd[\u03c6(w\u2217>i x)4] )1/2( E x\u223cDd [\u2016x\u20164] )1/2 . ( L1 p+ 1 \u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2d.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [(a>Bi(x)b)2]). Note that Bi(x) is a symmetric matrix, thus it suffices to consider the case where a = b.\nmax \u2016a\u2016=1\n( E\nx\u223cDd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx\u223cDd [\u03c64(w\u2217>i x)]\n)1/4 .\nL1 p+ 1 \u2016w\u2217i \u2016p+1 + |\u03c6(0)|.\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\nn & L2d+ |m2,i|2 + L|m2,i|d \u00b7 poly(log d, t)\n2|m2,i|2 t log d\nwith probability at least 1\u2212 1/dt,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S|\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m2,i|.\nLemma E.3. Let M3 be defined as in Definition 5.1. Let M\u03023 be the empirical version of M3, i.e.,\nM\u03023 = 1 |S| \u2211\n(x,y)\u2208S\ny \u00b7 (x\u22973 \u2212 x\u2297\u0303I),\nwhere S denote a set of samples (each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M3 6= 0, i.e., m3,i 6= 0 for any i. Let \u03b1 be a fixed unit vector. Then for any 0 < < 1, t \u2265 1, if\n|S| \u2265 max i\u2208[k] (\u2016w\u2217i \u2016p+1/|m3,i(w\u2217>i \u03b1)|2 + 1) \u00b7 \u22122dpoly(log d, t)\nwith probability at least 1\u2212 1/dt,\n\u2016M3(I, I, \u03b1)\u2212 M\u03023(I, I, \u03b1)\u2016 \u2264 k\u2211 i=1 |v\u2217im3,i(w\u2217>i \u03b1)|.\nProof. Since y = \u2211k\ni=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component i \u2208 [k].\nDefine function Bi(x) : Rd \u2192 Rd\u00d7d such that\nBi(x) = [\u03c6(w \u2217> i x) \u00b7 (x\u22973 \u2212 x\u2297\u0303I)](I, I, \u03b1) = \u03c6(w\u2217>i x) \u00b7 ((x>\u03b1)x\u22972 \u2212 \u03b1>xI \u2212 \u03b1x> \u2212 x\u03b1>).\nDefine g(z) = \u03c6(z) \u2212 \u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1p+1 |z| p+1 . |z|p+1, which follows Property 3.1. In order to apply Lemma B.7, we need to bound the following four quantities,\n(I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 = \u2016\u03c6(w\u2217>i x) \u00b7 ((x>\u03b1)x\u22972 \u2212 \u03b1>xId \u2212 \u03b1x> \u2212 x\u03b1>)\u2016 \u2264 |\u03c6(w\u2217>i x)| \u00b7 \u2016(x>\u03b1)x\u22972 \u2212 \u03b1>xI \u2212 \u03b1x> \u2212 x\u03b1>\u2016 . (|w\u2217>i x|p+1 + |\u03c6(0)|)\u2016(x>\u03b1)x\u22972 \u2212 \u03b1>xI \u2212 \u03b1x> \u2212 x\u03b1>\u2016 . (|w\u2217>i x|p+1 + |\u03c6(0)|)(|x>\u03b1|\u2016x\u20162 + 3|\u03b1>x|),\nwhere the third step follows by definition of g(z), and last step follows by definition of spectral norm and triangle inequality.\nUsing Fact B.1 and Fact B.2, we have for any constant t \u2265 1, with probability 1\u2212 1/(nd4t),\n\u2016Bi(x)\u2016 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)dpoly(log d, t).\n(II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m3,i(w\u2217>i \u03b1)w\u2217iw\u2217>i . Therefore, \u2016Ex\u223cDd [Bi(x)]\u2016 = |m3,i(w\u2217>i \u03b1)|. (III) Bounding max(\u2016Ex\u223cDd [Bi(x)Bi(x)>]\u2016, \u2016Ex\u223cDd [Bi(x)>Bi(x)]\u2016). Because matrix Bi(x) is symmetric, thus it suffices to bound one of them,\u2225\u2225\u2225\u2225 Ex\u223cDd[B2i (x)] \u2225\u2225\u2225\u2225 . ( Ex\u223cDd [ \u03c6(w\u2217>i x) 4 ])1/2( E x\u223cDd [ (x>\u03b1)4 ])1/2( E x\u223cDd [\u2016x\u20164]\n)1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2d.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [(a>Bi(x)b)2])1/2.\nmax \u2016a\u2016=1\n( E\nx\u223cDd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx\u223cDd\n[ \u03c64(w\u2217>i x) ])1/4 . \u2016w\u2217i \u2016p+1 + |\u03c6(0)|.\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\n|S| & L 2d+ |m3,i(w\u2217>i \u03b1)|2 + L|m3,i(w\u2217>i \u03b1)|d \u00b7 poly(log d, t)\n2|m3,i(w\u2217>i \u03b1)|2 \u00b7 t log d\nwith probability at least 1\u2212 d\u2212t,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S|\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m3,i(w\u2217>i \u03b1)|.\nLemma E.4. Let M4 be defined as in Definition 5.1. Let M\u03024 be the empirical version of M4, i.e.,\nM\u03024 = 1 |S| \u2211\n(x,y)\u2208S\ny \u00b7 (x\u22974 \u2212 (x\u2297 x)\u2297\u0303I + I\u2297\u0303I),\nwhere S denote a set of samples (where each sample is i.i.d. sampled are sampled from Distribution D defined in Eq. (1)). Assume M4 6= 0, i.e., m4,i 6= 0 for any i. Let \u03b1 be a fixed unit vector. Then for any 0 < < 1, t \u2265 1, if\n|S| \u2265 max i\u2208[k] (\u2016w\u2217i \u2016p+1/|m4,i|(w\u2217>i \u03b1)2 + 1)2 \u00b7 \u22122 \u00b7 dpoly(log d, t)\nwith probability at least 1\u2212 1/dt,\n\u2016M4(I, I, \u03b1, \u03b1)\u2212 M\u03024(I, I, \u03b1, \u03b1)\u2016 \u2264 k\u2211 i=1 |v\u2217im4,i|(w\u2217>i \u03b1)2.\nProof. Since y = \u2211k\ni=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component i \u2208 [k].\nDefine function Bi(x) : Rd \u2192 Rd\u00d7d such that\nBi(x) = [\u03c6(w \u2217> i x) \u00b7 (x\u22974 \u2212 (x\u2297 x)\u2297\u0303I + I\u2297\u0303I)](I, I, \u03b1, \u03b1)\n= \u03c6(w\u2217>i x) \u00b7 ((x>\u03b1)2x\u22972 \u2212 (\u03b1>x)2I \u2212 2(\u03b1>x)(x\u03b1> + \u03b1x>)\u2212 xx> + 2\u03b1\u03b1> + I).\nDefine g(z) = \u03c6(z) \u2212 \u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1/(p + 1)|z|p+1 . |z|p+1, which follows\nProperty 3.1. (I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 .|\u03c6(w\u2217>i x)| \u00b7 ((x>\u03b1)2\u2016x\u20162 + 1 + \u2016x\u20162 + (\u03b1>x)2) .(|w\u2217>i x|p+1 + |\u03c6(0)|) \u00b7 ((x>\u03b1)2\u2016x\u20162 + 1 + \u2016x\u20162 + (\u03b1>x)2)\nUsing Fact B.1 and Fact B.2, we have for any constant t \u2265 1, with probability 1\u2212 1/(nd4t),\n\u2016Bi(x)\u2016 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)d poly(log d, t).\n(II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m4,i(w\u2217>i \u03b1)2w\u2217iw\u2217>i . Therefore, \u2016Ex\u223cDd [Bi(x)]\u2016 = |m4,i|(w\u2217>i \u03b1)2. (III) Bounding max(Ex\u223cDd \u2016Bi(x)Bi(x)>\u2016,Ex\u223cDd \u2016Bi(x)>Bi(x)\u2016).\n\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)2] \u2225\u2225\u2225\u2225 . ( Ex\u223cDd[\u03c6(w\u2217>i x)4] )1/2( E x\u223cDd [(x>\u03b1)8] )1/2( E x\u223cDd [\u2016x\u20164] )1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2d.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [(a>Bi(x)b)2])1/2.\nmax \u2016a\u2016=1\n( E\nx\u223cDd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx\u223cDd\n[ \u03c64(w\u2217>i x) ])1/4 . \u2016w\u2217i \u2016p+1 + |\u03c6(0)|.\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\nn & L2d+ |m4,i|2(w\u2217>i \u03b1)4 + L|m4,i|(w\u2217>i \u03b1)2dpoly(log d, t)\n2|m4,i|2(w\u2217>i \u03b1)4 \u00b7 t log d\nwith probability at least 1\u2212 d\u2212t,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S|\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m4,i|(w\u2217>i \u03b1)2."}, {"heading": "E.3.2 Error Bound for the Second-order Moment", "text": "The goal of this section is to prove Lemma E.5, which shows we can approximate the second order moments up to some precision by using linear sample complexity in d.\nLemma E.5 (Estimation of the second order moment). Let P2 and j2 be defined in Definition 5.4. Let S denote a set of i.i.d. samples generated from distribution D(defined in (1)). Let P\u03022 be the empirical version of P2 using dataset S, i.e., P\u03022 = ES [P2]. Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t \u2265 1, and m0 = mini\u2208[k]{|mj2,i|2(w\u2217>i \u03b1)2(j2\u22122)}, if\n|S| & \u03c32p+21 \u00b7 d \u00b7 poly(t, log d)/( 2m0),\nthen with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016P2 \u2212 P\u03022\u2016 \u2264 k\u2211 i=1 |v\u2217imj2,i(w\u2217>i \u03b1)j2\u22122|.\nProof. We have shown the bound for j2 = 2, 3, 4 in Lemma E.2, Lemma E.3 and Lemma E.4 respectively. To summarize, for any 0 < < 1 we have if\n|S| \u2265 max i\u2208[k]\n{ (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|+ |mj2,i(w\u2217>i \u03b1)(j2\u22122)|)2\n|mj2,i|2(w\u2217>i \u03b1)2(j2\u22122)\n} \u00b7 \u22122dpoly(log d, t)\nwith probability at least 1\u2212 d\u2212t,\n\u2016P2 \u2212 P\u03022\u2016 \u2264 k\u2211 i=1 |v\u2217imj2,i(w\u2217>i \u03b1)j2\u22122|."}, {"heading": "E.3.3 Subspace Estimation Using Power Method", "text": "Lemma E.6 shows a small number of power iterations can estimate the subspace of {w\u2217i }i\u2208[k] to some precision, which provides guarantees for Algorithm 3.\nLemma E.6 (Bound on subspace estimation). Let P2 be defined as in Definition. 5.4 and P\u03022 be its empirical version. Let U \u2208 Rd\u00d7k be the orthogonal column span of W \u2217 \u2208 Rd\u00d7k. Assume \u2016P\u03022 \u2212 P2\u2016 \u2264 \u03c3k(P2)/10. Let C be a large enough positive number such that C > 2\u2016P2\u2016. Then after T = O(log(1/ )) iterations, the output of Algorithm 3, V \u2208 Rd\u00d7k, will satisfy\n\u2016UU> \u2212 V V >\u2016 . \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + ,\nwhich implies\n\u2016(I \u2212 V V >)w\u2217i \u2016 . (\u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + )\u2016w\u2217i \u2016.\nProof. According to Weyl\u2019s inequality, we are able to pick up the correct numbers of positive eigenvalues and negative eigenvalues in Algorithm 3 as long as P\u03022 and P2 are close enough.\nLet U = [U1 U2] \u2208 Rd\u00d7k be the eigenspace of span{w\u22171 w\u22172 \u00b7 \u00b7 \u00b7 w\u2217k}, where U1 \u2208 R d\u00d7k1 corre-\nsponds to positive eigenvalues of P2 and U2 \u2208 Rd\u00d7k2 is for negatives.\nLet V 1 be the top-k1 eigenvectors of CI + P\u03022. Let V 2 be the top-k2 eigenvectors of CI \u2212 P\u03022. Let V = [V 1 V 2] \u2208 Rd\u00d7k.\nAccording to Lemma 9 in [HK13], we have \u2016(I\u2212U1U>1 )V 1\u2016 . \u2016P\u03022\u2212P2\u2016/\u03c3k(P2),\u2016(I\u2212U2U>2 )V 2\u2016 . \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) . Using Fact B.4, we have \u2016(I \u2212 UU>)V \u2016 = \u2016UU> \u2212 V V\n>\u2016. Let be the precision we want to achieve using power method. Let V1 be the top-k1 eigenvectors returned after O(log(1/ )) iterations of power methods on CI + P\u03022 and V2 \u2208 Rd\u00d7k2 for CI \u2212 P\u03022 similarly.\nAccording to Theorem 7.2 in [AKZ12], we have \u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 and \u2016V 2V > 2 \u2212 V2V >2 \u2016 \u2264 . Let U\u22a5 be the complementary matrix of U . Then we have,\n\u2016(I \u2212 U1U>1 )V 1\u2016 = max\u2016a\u2016=1 \u2016(I \u2212 U1U>1 )V 1a\u2016\n= max \u2016a\u2016=1\n\u2016(U\u22a5U>\u22a5 + U2U>2 )V 1a\u2016\n= max \u2016a\u2016=1\n\u221a \u2016U\u22a5U>\u22a5V 1a\u20162 + \u2016U2U>2 V 1a\u20162\n\u2265 max \u2016a\u2016=1 \u2016U2U>2 V 1a\u2016 = \u2016U2U>2 V 1\u2016, (32)\nwhere the first step follows by definition of spectral norm, the second step follows by I = U1U>1 + U2U > 2 + U > \u22a5U > \u22a5 , the third step follows by U > 2 U\u22a5 = 0, and last step follows by definition of spectral norm. We can upper bound \u2016(I \u2212 UU>)V \u2016,\n\u2016(I \u2212 UU>)V \u2016 \u2264 (\u2016(I \u2212 U1U>1 )V 1\u2016+ \u2016(I \u2212 U2U>2 )V 2\u2016+ \u2016U2U>2 V 1\u2016+ \u2016U1U>1 V 2\u2016) \u2264 2(\u2016(I \u2212 U1U>1 )V 1\u2016+ \u2016(I \u2212 U2U>2 )V 2\u2016)\n. \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2), (33)\nwhere the first step follows by triangle inequality, the second step follows by Eq. (32), and the last step follows by Lemma 9 in [HK13].\nWe define matrix R such that V 2R = (I \u2212 V1V >1 )V2 is the QR decomposition of (I \u2212 V1V >1 )V2, then we have\n\u2016(I \u2212 V 2V > 2 )V 2\u2016\n= \u2016(I \u2212 V 2V > 2 )(I \u2212 V1V >1 )V2R\u22121\u2016 = \u2016(I \u2212 V 2V > 2 )(I \u2212 V 1V > 1 + V 1V > 1 \u2212 V1V >1 )V2R\u22121\u2016 \u2264 \u2016(I \u2212 V 2V > 2 )(I \u2212 V 1V > 1 )V2R\n\u22121\u2016\ufe38 \ufe37\ufe37 \ufe38 \u03b1 + \u2016(I \u2212 V 2V > 2 )\u2016\u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016\ufe38 \ufe37\ufe37 \ufe38\n\u03b2\n,\nwhere the first step follows by V 2 = (I\u2212V1V >1 )V2R\u22121, and the last step follows by triangle inequality.\nFurthermore, we have,\n\u03b1+ \u03b2\n= \u2016(I \u2212 V 2V > 2 \u2212 V 1V > 1 )V2R \u22121\u2016+ \u2016(I \u2212 V 2V > 2 )\u2016\u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 \u2016(I \u2212 V 2V > 2 )V2R \u22121\u2016+ \u2016V 1V > 1 V2R \u22121\u2016+ \u2016(I \u2212 V 2V > 2 )\u2016\u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 \u2016V 2V > 2 \u2212 V2V >2 \u2016\u2016R\u22121\u2016+ \u2016V 1V > 1 V2\u2016\u2016R\u22121\u2016+ \u2016(I \u2212 V 2V > 2 )\u2016\u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016 = \u2016V 2V > 2 \u2212 V2V >2 \u2016\u2016R\u22121\u2016+ \u2016V 1V > 1 V2\u2016\u2016R\u22121\u2016+ \u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 \u2016V 2V > 2 \u2212 V2V >2 \u2016\u2016R\u22121\u2016+ \u2016(I \u2212 V 2V > 2 )V2\u2016\u2016R\u22121\u2016+ \u2016R\u22121\u2016\u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 (2\u2016V 2V > 2 \u2212 V2V >2 \u2016+ \u2016V 1V > 1 \u2212 V1V >1 \u2016)\u2016R\u22121\u2016 \u2264 3 \u2016R\u22121\u2016 \u2264 6 ,\nwhere the first step follows by definition of \u03b1, \u03b2, the second step follows by triangle inequality, the third step follows by \u2016AB\u2016 \u2264 \u2016A\u2016\u2016B\u2016, the fourth step follows by \u2016(I \u2212 V 2V > 2 )\u2016 = 1, the fifth step follows by Eq. (32), the sixth step follows by Fact B.4, the seventh step follows by \u2016V 1V > 1 \u2212V1V >1 \u2016 \u2264 and \u2016V 2V > 2 \u2212V2V >2 \u2016 \u2264 , and the last step follows by \u2016R\u22121\u2016 \u2264 2 (Claim E.7).\nFinally,\n\u2016UU> \u2212 V V >\u2016 \u2264 \u2016UU> \u2212 V V >\u2016+ \u2016V V > \u2212 V V >\u2016\n= \u2016(I \u2212 UU>)V \u2016+ \u2016V V > \u2212 V V >\u2016\n\u2264 \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + \u2016V V > \u2212 V V >\u2016 \u2264 \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + \u2016V 1V > 1 \u2212 V1V >1 \u2016+ \u2016V 2V > 2 \u2212 V2V >2 \u2016\n\u2264 \u2016P\u03022 \u2212 P2\u2016/\u03c3k(P2) + 2 ,\nwhere the first step follows by triangle inequality, the second step follows by Fact B.4, the third step follows by Eq. (33), the fourth step follows by triangle inequality, and the last step follows by \u2016V 1V > 1 \u2212 V1V >1 \u2016 \u2264 and \u2016V 2V > 2 \u2212 V2V >2 \u2016 \u2264 .\nTherefore we finish the proof.\nIt remains to prove Claim E.7.\nClaim E.7. \u03c3k(R) \u2265 1/2.\nProof. First, we can rewrite R>R in the follow way,\nR>R = V >2 (I \u2212 V1V >1 )V2 = I \u2212 V >2 V1V >1 V2\nSecond, we can upper bound \u2016V >2 V1\u2016 by 1/4,\n\u2016V >2 V1\u2016 = \u2016V2V >2 V1\u2016\n\u2264 \u2016(V2V >2 \u2212 V 2V > 2 )V1\u2016+ \u2016V 2V > 2 V1\u2016 \u2264 \u2016(V2V >2 \u2212 V 2V > 2 )V1\u2016+ \u2016V > 2 (V1V > 1 \u2212 V 1V > 1 )\u2016+ \u2016V > 2 V 1V > 1 \u2016 = \u2016(V2V >2 \u2212 V 2V > 2 )V1\u2016+ \u2016V > 2 (V1V > 1 \u2212 V 1V > 1 )\u2016 \u2264 \u2016V2V >2 \u2212 V 2V > 2 \u2016 \u00b7 \u2016V1\u2016+ \u2016V > 2 \u2016 \u00b7 \u2016V1V >1 \u2212 V 1V > 1 \u2016 \u2264 + \u2264 1/4,\nwhere the first step follows by V >2 V2 = I, the second step follows by triangle inequality, the third step follows by triangle inequality, the fourth step follows by \u2016V >2 V 1V > 1 \u2016 = 0, the fifth step follows by \u2016AB\u2016 \u2264 \u2016A\u2016\u00b7\u2016B\u2016, and the last step follows by \u2016V 1V > 1 \u2212V1V >1 \u2016 \u2264 , \u2016V1\u2016 = 1, \u2016V 2V > 2 \u2212V2V >2 \u2016 \u2264 and \u2016V >2 \u2016 = 1, and the last step follows by < 1/8. Thus, we can lower bound \u03c32k(R),\n\u03c32k(R) = \u03bbmin(R >R)\n= min \u2016a\u2016=1\na>R>Ra\n= min \u2016a\u2016=1\na>Ia\u2212 \u2016V >2 V1a\u20162\n= 1\u2212 max \u2016a\u2016=1 \u2016V >2 V1a\u20162 = 1\u2212 \u2016V >2 V1\u20162 \u2265 3/4\nwhich implies \u03c3k(R) \u2265 1/2."}, {"heading": "E.4 Error Bound for the Reduced Third-order Moment", "text": ""}, {"heading": "E.4.1 Error Bound for the Reduced Third-order Moment in Different Cases", "text": "Lemma E.8. Let M3 be defined as in Definition 5.1. Let M\u03023 be the empirical version of M3, i.e.,\nM\u03023 = 1 |S| \u2211\n(x,y)\u2208S\ny \u00b7 (x\u22973 \u2212 x\u2297\u0303I),\nwhere S denote a set of samples (where each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M3 6= 0, i.e., m3,i 6= 0 for any i. Let V \u2208 Rd\u00d7k be an orthogonal matrix satisfying \u2016UU> \u2212 V V >\u2016 \u2264 1/4, where U is the orthogonal basis of span{w\u22171, w\u22172, \u00b7 \u00b7 \u00b7 , w\u2217k}. Then for any 0 < < 1, t \u2265 1, if\n|S| \u2265 max i\u2208[k] (\u2016w\u2217i \u2016p+1/|m3,i|2 + 1) \u00b7 \u22122 \u00b7 k2 poly(log d, t)\nwith probability at least 1\u2212 1/dt,\n\u2016M3(V, V, V )\u2212 M\u03023(V, V, V )\u2016 \u2264 k\u2211 i=1 |v\u2217im3,i|.\nProof. Since y = \u2211k\ni=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component i \u2208 [k]. We define function\nTi(x) : Rd \u2192 Rk\u00d7k\u00d7k such that,\nTi(x) = \u03c6(w \u2217> i x) \u00b7 ((V >x)\u22973 \u2212 (V >x)\u2297\u0303I).\nWe flatten tensor Ti(x) along the first dimension into matrix Bi(x) \u2208 Rk\u00d7k 2 . Define g(z) = \u03c6(z)\u2212 \u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1/(p+ 1)|z|p+1, which follows Property 3.1.\n(I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 \u2264 |\u03c6(w\u2217>i x)| \u00b7 (\u2016V >x\u20163 + 3k\u2016V >x\u2016) . (|w\u2217>i x|p+1 + |\u03c6(0)|) \u00b7 (\u2016V >x\u20163 + 3k\u2016V >xj\u2016)\nNote that V >x \u223c N (0, Ik). According to Fact B.1 and Fact B.2, we have for any constant t \u2265 1, with probability 1\u2212 1/(ndt),\n\u2016Bi(x)\u2016 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)k3/2 poly(log d, t)\n(II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m3,i(V >w\u2217i )vec((V >w\u2217i )(V >w\u2217i )>)>. Therefore, \u2016Ex\u223cDd [Bi(x)]\u2016 = |m3,i|\u2016V >w\u2217i \u20163. Since \u2016V V > \u2212 UU>\u2016 \u2264 1/4, \u2016V V >w\u2217i \u2212 w\u2217i \u2016 \u2264 1/4 and 3/4 \u2264 \u2016V >w\u2217i \u2016 \u2264 5/4. So 1 4 |m3,i| \u2264 \u2016B\u2016 \u2264 2|m3,i|.\n(III) Bounding max(Ex\u223cDd \u2016Bi(x)Bi(x)>\u2016,Ex\u223cDd \u2016Bi(x)>Bi(x)\u2016).\n\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)Bi(x)>] \u2225\u2225\u2225\u2225 . ( Ex\u223cDd[\u03c6(w\u2217>i x)4] )1/2( E x\u223cDd [\u2016V >x\u20166] )1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2k3/2.\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)>Bi(x)]\n\u2225\u2225\u2225\u2225 . ( E\nx\u223cDd [\u03c6(w\u2217>i x) 4]\n)1/2( E\nx\u223cDd [\u2016V >x\u20164] )1/2 max \u2016A\u2016F=1 ( E x\u223cDd [\u3008A, (V >x)(V >x)>\u30094] )1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2k2.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [(a>Bi(x)b)2]).\nmax \u2016a\u2016=\u2016b\u2016=1\n( E\nx\u223cDd [(a>Bi(x)b) 2] )1/2 . ( E\nx\u223cDd [(\u03c6(w\u2217>i x)) 4] )1/4 max \u2016a\u2016=1 ( E x\u223cDd [(a>V >x)4] )1/2 max \u2016A\u2016F=1 ( E x\u223cDd [\u3008A, (V >x)(V >x)>\u30094] )1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)k\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\n|S| & L 2k2 + |m3,i|2 + k3/2 poly(log d, t)|m3,i|\n2|m3,i|2 t log(k)\nwith probability at least 1\u2212 k\u2212t,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S|\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m3,i|. We can set t = T log(d)/ log(k), then if\n|S| \u2265 \u22122(1 + 1/|m3,i|2) poly(T, log d)\nwith probability at least 1\u2212 d\u2212T ,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S|\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m3,i|. Also note that for any symmetric 3rd-order tensor R, the operator norm of R,\n\u2016R\u2016 = max \u2016a\u2016=1 |R(a, a, a)| \u2264 max \u2016a\u2016=1 \u2016R(a, I, I)\u2016F = \u2016R(1)\u2016.\nLemma E.9. Let M4 be defined as in Definition 5.1. Let M\u03024 be the empirical version of M4, i.e.,\nM\u03024 = 1 |S| \u2211\n(x,y)\u2208S\ny \u00b7 ( x\u22974 \u2212 (x\u2297 x)\u2297\u0303I + I\u2297\u0303I ) ,\nwhere S is a set of samples (where each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M4 6= 0, i.e., m4,i 6= 0 for any i. Let \u03b1 be a fixed unit vector. Let V \u2208 Rd\u00d7k be an orthogonal matrix satisfying \u2016UU> \u2212 V V >\u2016 \u2264 1/4, where U is the orthogonal basis of span{w\u22171, w\u22172, \u00b7 \u00b7 \u00b7 , w\u2217k}. Then for any 0 < < 1, t \u2265 1, if\n|S| \u2265 max i\u2208[k] (1 + \u2016w\u2217i \u2016p+1/|m4,i(\u03b1>w\u2217i )|2) \u00b7 \u22122 \u00b7 k2 poly(log d, t)\nwith probability at least 1\u2212 d\u2212t,\n\u2016M4(V, V, V, \u03b1)\u2212 M\u03024(V, V, V, \u03b1)\u2016 \u2264 k\u2211 i=1 |v\u2217im4,i(\u03b1>w\u2217i )|.\nProof. Recall that for each (x, y) \u2208 S, we have y = \u2211k\ni=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component\ni \u2208 [k]. We define function r(x) : Rd \u2192 Rk such that\nr(x) = V >x.\nDefine function Ti(x) : Rd \u2192 Rk\u00d7k\u00d7k such that\nTi(x) = \u03c6(w \u2217> i x)\n( x>\u03b1 \u00b7 r(x)\u2297 r(x)\u2297 r(x)\u2212 (V >\u03b1)\u2297\u0303(r(x)\u2297 r(x))\u2212 \u03b1>x \u00b7 r(x)\u2297\u0303I + (V >\u03b1)\u2297\u0303I ) .\nWe flatten Ti(x) : Rd \u2192 Rk\u00d7k\u00d7k along the first dimension to obtain functionBi(x) : Rd \u2192 Rk\u00d7k 2 . Define g(z) = \u03c6(z)\u2212\u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1/(p+1)|z|p+1, which follows Property 3.1.\n(I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 .(|w\u2217>i x|p+1 + |\u03c6(0)|) \u00b7 (|(x>\u03b1)|\u2016V >x\u20163 + 3\u2016V >\u03b1\u2016\u2016V >x\u20162\n+ 3|(x>\u03b1)|\u2016V >xj\u2016 \u221a k + 3\u2016V >\u03b1\u2016 \u221a k)\nNote that V >x \u223c N (0, Ik). According to Fact B.1 and Fact B.2, we have for any constant t \u2265 1, with probability 1\u2212 1/(ndt),\n\u2016Bi(x)\u2016 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)k3/2 poly(log d, t)\n(II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m4,i(\u03b1>w\u2217i )(V >w\u2217i )vec((V >w\u2217i )(V >w\u2217i )>)>. Therefore,\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\n\u2225\u2225\u2225\u2225 = |m4,i(\u03b1>w\u2217i )|\u2016V >w\u2217i \u20163. Since \u2016V V >\u2212UU>\u2016 \u2264 1/4, \u2016V V >w\u2217i \u2212w\u2217i \u2016 \u2264 1/4 and 3/4 \u2264 \u2016V >w\u2217i \u2016 \u2264 5/4. So 14 |m4,i(\u03b1\n>w\u2217i )| \u2264 \u2016Ex\u223cDd [Bi(x)]\u2016 \u2264 2|m4,i(\u03b1>w\u2217i )|.\n(III) Bounding max(Ex\u223cDd [Bi(x)Bi(x)>],Ex\u223cDd [Bi(x)>Bi(x)]).\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)Bi(x)>] \u2225\u2225\u2225\u2225 . ( Ex\u223cDd [ \u03c6(w\u2217>i x) 4 ])1/2( E x\u223cDd [ (\u03b1>x)4 ])1/2( E x\u223cDd [ \u2016V >x\u20166 ])1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2k3/2.\n\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)>Bi(x)] \u2225\u2225\u2225\u2225\n. ( E\nx\u223cDd [\u03c6(w\u2217>i x) 4]\n)1/2( E\nx\u223cDd [(\u03b1>x)4]\n)1/2( E\nx\u223cDd [\u2016V >x\u20164] )1/2 \u00b7 (\nmax \u2016A\u2016F=1 E x\u223cDd\n[ \u3008A, (V >x)(V >x)>\u30094 ])1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2k2.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [ (a>Bi(x)b) 2 ] )1/2.\nmax \u2016a\u2016=\u2016b\u2016=1\n( E\nx\u223cDd\n[ (a>Bi(x)b) 2 ])1/2\n. ( E\nx\u223cDd [\u03c64(w\u2217>i x)]\n)1/4( E\nx\u223cDd\n[ (\u03b1>x)4 ])1/4 max \u2016a\u2016=1 ( E x\u223cDd [ (a>V >x)4 ])1/2 \u00b7 max \u2016A\u2016F=1 ( E x\u223cDd [ \u3008A, (V >x)(V >x)>\u30094\n])1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)k.\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\n|S| \u2265 L 2k2 + |m4,i(\u03b1>w\u2217i )|2 + k3/2 poly(t, log d)|m4,i(\u03b1>w\u2217i )|\n2(m4,i(\u03b1>w\u2217i )) 2\n\u00b7 t log k\nwith probability at least 1\u2212 k\u2212t,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S| n\u2211 x\u2208S Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m4,i(\u03b1>w\u2217i )|. (34) We can set t = T log(d)/ log(k), then if\n|S| \u2265 (L+ |m4,i(\u03b1 >w\u2217i )|)2k2 poly(T, log d)\n2|m4,i(\u03b1>w\u2217i )|2 \u00b7 T log2 d\nwith probability at least 1\u2212d\u2212T , Eq. (34) holds. Also note that for any symmetric 3rd-order tensor R, the operator norm of R,\n\u2016R\u2016 = max \u2016a\u2016=1 |R(a, a, a)| \u2264 max \u2016a\u2016=1 \u2016R(a, I, I)\u2016F = \u2016R(1)\u2016."}, {"heading": "E.4.2 Final Error Bound for the Reduced Third-order Moment", "text": "Lemma E.10 shows R\u03023 can approximate R3 to some small precision with poly(k) samples.\nLemma E.10 (Estimation of the reduced third order moment). Let U \u2208 Rd\u00d7k denote the orthogonal column span of W \u2217. Let \u03b1 be a fixed unit vector and V \u2208 Rd\u00d7k denote an orthogonal matrix satisfying \u2016V V >\u2212UU>\u2016 \u2264 1/4. Define R3 := P3(V, V, V ), where P3 is defined as in Definition 5.4 using \u03b1. Let R\u03023 be the empirical version of R3 using dataset S, where each sample of S is i.i.d. sampled from distribution D(defined in (1)). Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t \u2265 1, define j3 = min{j \u2265 3|Mj 6= 0} and m0 = mini\u2208[k]{(m (j3) i (\u03b1 >w\u2217i ) j3\u22123)2}, if\n|S| \u2265 \u03c32p+21 \u00b7 k 2 \u00b7 poly(log d, t)/( 2m0)\nthen we have ,\n\u2016R3 \u2212 R\u03023\u2016 \u2264 k\u2211 i=1 |v\u2217imj3,i(w\u2217>i \u03b1)j3\u22123|,\nholds with probability at least 1\u2212 d\u2212\u2126(t).\nProof. The main idea is to use matrix Bernstein bound after matricizing the third-order tensor. Similar to the proof of Lemma E.5, we consider each node component individually and then sum up the errors and apply union bound.\nWe have shown the bound for j3 = 3, 4 in Lemma E.8 and Lemma E.9 respectively. To summarize, for any 0 < < 1 we have if\n|S| \u2265 max i\u2208[k]\n( 1 + \u2016w\u2217i \u2016p+1/|mj3,i(w\u2217>i \u03b1)(j3\u22123)|2 ) \u00b7 \u22122 \u00b7 k2 poly(log d, t)\nwith probability at least 1\u2212 d\u2212t,\n\u2016R3 \u2212 R\u03023\u2016 \u2264 k\u2211 i=1 |v\u2217imj3,i(w\u2217>i \u03b1)j3\u22123|."}, {"heading": "E.5 Error Bound for the Magnitude and Sign of the Weight Vectors", "text": "The lemmata in this section together with Lemma E.5 provide guarantees for Algorithm 4. In particular, Lemma E.12 shows with linear sample complexity in d, we can approximate the 1storder moment to some precision. And Lemma E.13 and Lemma E.14 provide the error bounds of linear systems Eq. (28) under some perturbations."}, {"heading": "E.5.1 Robustness for Solving Linear Systems", "text": "Lemma E.11 (Robustness of linear system). Given two matrices A, A\u0303 \u2208 Rd\u00d7k, and two vectors b, b\u0303 \u2208 Rd. Let z\u2217 = argminz\u2208Rk \u2016Az\u2212b\u2016 and z\u0302 = argminz\u2208Rk \u2016(A+A\u0303)z\u2212(b+ b\u0303)\u2016. If \u2016A\u0303\u2016 \u2264 1 4\u03ba\u03c3k(A) and \u2016b\u0303\u2016 \u2264 14\u2016b\u2016, then, we have\n\u2016z\u2217 \u2212 z\u0302\u2016 .(\u03c3\u22124k (A)\u03c3 2 1(A) + \u03c3 \u22122 k (A))\u2016b\u2016\u2016A\u0303\u2016+ \u03c3 \u22122 k (A)\u03c31(A)\u2016b\u0303\u2016.\nProof. By definition of z and z\u0302, we can rewrite z and z\u0302,\nz = A\u2020b = (A>A)\u22121A>b\nz\u0302 = (A+ A\u0303)\u2020(b+ b\u0303) = ((A+ A\u0303)>(A+ A\u0303))\u22121(A+ A\u0303)>(b+ b\u0303).\nAs \u2016A\u0303\u2016 \u2264 14\u03ba\u03c3k(A), we have \u2016A\u0303 >A + A>A\u0303\u2016\u2016(A>A)\u22121\u2016 \u2264 1/4. Together with \u2016b\u0303\u2016 \u2264 14\u2016b\u2016, we can ignore the high-order errors. So we have\n\u2016z\u0302 \u2212 z\u2217\u2016\n. \u2016(A>A)\u22121(A\u0303>b+A>b\u0303) + (A>A)\u22121(A>A\u0303+ A\u0303>A)(A>A)\u22121A>b\u2016\n. \u2016(A>A)\u22121\u2016(\u2016A\u0303\u2016\u2016b\u2016+ \u2016A\u2016\u2016b\u0303\u2016) + \u2016(A>A)\u22122\u2016 \u00b7 \u2016A\u2016\u2016A\u0303\u2016\u2016A\u2016\u2016b\u2016\n. \u03c3\u22122k (A)(\u2016A\u0303\u2016\u2016b\u2016+ \u03c31(A)\u2016b\u0303\u2016) + \u03c3 \u22124 k (A) \u00b7 \u03c3 2 1(A)\u2016A\u0303\u2016\u2016b\u2016."}, {"heading": "E.5.2 Error Bound for the First-order Moment", "text": "Lemma E.12 (Error bound for the first-order moment). Let Q1 be defined as in Eq. (25) and Q\u03021 be the empirical version of Q1 using dataset S, where each sample of S is i.i.d. sampled from distribution D(defined in (1)). Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t \u2265 1, define j1 = min{j \u2265 1|Mj 6= 0} and m0 = mini\u2208[k](mj1,i(w \u2217> i \u03b1) j1\u22121)2 if\n|S| \u2265 \u03c32p+21 d poly(t, log d)/( 2m0)\nwe have with probability at least 1\u2212 d\u2212\u2126(t),\n\u2016Q1 \u2212 Q\u03021\u2016 \u2264 k\u2211 i=1 |v\u2217imj1,i(w\u2217>i \u03b1)j1\u22121|.\nProof. We consider the case when l1 = 3, i.e.,\nQ1 = M3(I, \u03b1, \u03b1) = k\u2211 i=1 v\u2217im3,i(\u03b1 >w\u2217i ) 3w\u2217i .\nAnd other cases are similar. Since y = \u2211k i=1 v \u2217 i \u03c6(w \u2217> i x). We consider each component i \u2208 [k].\nDefine function Bi(x) : Rd \u2192 Rd such that\nBi(x) = [\u03c6(w \u2217> i x) \u00b7 (x\u22973 \u2212 x\u2297\u0303I)](I, \u03b1, \u03b1) = \u03c6(w\u2217>i x) \u00b7 ((x>\u03b1)2x\u2212 2(x>\u03b1)\u03b1\u2212 x).\nDefine g(z) = \u03c6(z) \u2212 \u03c6(0), then |g(z)| = | \u222b z\n0 \u03c6 \u2032(s)ds| \u2264 L1/(p + 1)|z|p+1, which follows Prop-\nerty 3.1. (I) Bounding \u2016Bi(x)\u2016.\n\u2016Bi(x)\u2016 \u2264 |\u03c6(w\u2217>i x)| \u00b7 \u2016((x>\u03b1)2x\u2212 2\u03b1>x\u03b1\u2212 x)\u2016 \u2264 (|w\u2217>i x|p+1 + |\u03c6(0)|)(((x>\u03b1)2 + 1)\u2016x\u2016+ 2|\u03b1>x|)\nAccording to Fact B.1 and Fact B.2, we have for any constant t \u2265 1, with probability 1\u2212 1/(ndt),\n\u2016Bi(x)\u2016 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|) \u221a dpoly(log d, t)\n(II) Bounding \u2016Ex\u223cDd [Bi(x)]\u2016. Note that Ex\u223cDd [Bi(x)] = m3,i(w\u2217>i \u03b1)2w\u2217i . Therefore, \u2016Ex\u223cDd [Bi(x)]\u2016 = |m3,i(w\u2217>i \u03b1)2|. (III) Bounding max(Ex\u223cDd \u2016Bi(x)Bi(x)>\u2016,Ex\u223cDd \u2016Bi(x)>Bi(x)\u2016).\n\u2225\u2225\u2225\u2225 Ex\u223cDd [ Bi(x) >Bi(x) ]\u2225\u2225\u2225\u2225 . ( Ex\u223cDd [ \u03c6(w\u2217>i x) 4 ])1/2( E x\u223cDd [ (x>\u03b1)8 ])1/2( E x\u223cDd [ \u2016x\u20164 ])1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2d.\n\u2225\u2225\u2225\u2225 Ex\u223cDd [ Bi(x)Bi(x) > ]\u2225\u2225\u2225\u2225 . ( Ex\u223cDd [ \u03c6(w\u2217>i x) 4 ])1/2( E x\u223cDd [ (x>\u03b1)8 ])1/2( max \u2016a\u2016=1 E x\u223cDd [ (x>a)4 ])1/2 . (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|)2.\n(IV) Bounding max\u2016a\u2016=\u2016b\u2016=1(Ex\u223cDd [(a>Bi(x)b)2]).\nmax \u2016a\u2016=1\n( E\nx\u223cDd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx\u223cDd\n[ \u03c64(w\u2217>i x) ])1/4 . \u2016w\u2217i \u2016p+1 + |\u03c6(0)|.\nDefine L = \u2016w\u2217i \u2016p+1 + |\u03c6(0)|. Then we have for any 0 < < 1, if\n|S| & L 2d+ |m3,i(w\u2217>i \u03b1)2|2 + L|m3,i(w\u2217>i \u03b1)2|\n\u221a dpoly(log d, t)\n2|m3,i(w\u2217>i \u03b1)2|2 \u00b7 t log d\nwith probability at least 1\u2212 1/dt,\u2225\u2225\u2225\u2225\u2225 Ex\u223cDd[Bi(x)]\u2212 1|S| n\u2211\nx\u223cS Bi(x) \u2225\u2225\u2225\u2225\u2225 \u2264 |m3,i(w\u2217>i \u03b1)2|. Summing up all k components, we obtain if\n|S| \u2265 max i\u2208[k]\n{ (\u2016w\u2217i \u2016p+1 + |\u03c6(0)|+ |m3,i(w\u2217>i \u03b1)2|)2\n|m3,i(w\u2217>i \u03b1)2|2\n} \u00b7 \u22122dpoly(log d, t)\nwith probability at least 1\u2212 1/dt,\n\u2016M3(I, \u03b1, \u03b1)\u2212 M\u03023(I, \u03b1, \u03b1)\u2016 \u2264 k\u2211 i=1 |v\u2217im3,i(w\u2217>i \u03b1)2|.\nOther cases (j1 = 1, 2, 4) are similar, so we complete the proof."}, {"heading": "E.5.3 Linear System for the First-order Moment", "text": "The following lemma provides estimation error bound for the first linear system in Eq. (28).\nLemma E.13 (Solution of linear system for the first order moment). Let U \u2208 Rd\u00d7k be the orthogonal column span of W \u2217. Let V \u2208 Rd\u00d7k denote an orthogonal matrix satisfying that \u2016V V > \u2212 UU>\u2016 \u2264 \u03b4\u03022 . 1/(\u03ba2 \u221a k). For each i \u2208 [k], let u\u0302i denote the vector satisfying \u2016u\u0302i \u2212 V >w\u2217i \u2016 \u2264 \u03b4\u03023 . 1/(\u03ba2 \u221a k). Let Q1 be defined as in Eq. (25) and Q\u03021 be the empirical version of Q1 such that \u2016Q1 \u2212 Q\u03021\u2016 \u2264 \u03b4\u03024\u2016Q1\u2016 \u2264 14\u2016Q1\u2016. Let z \u2217 \u2208 Rk and z\u0302 \u2208 Rk be defined as in Eq. (27) and Eq. (28). Then\n|z\u0302i \u2212 z\u2217i | \u2264 (\u03ba4k3/2(\u03b4\u03022 + \u03b4\u03023) + \u03ba2k1/2\u03b4\u03024)\u2016z\u2217\u20161.\nProof. Let A \u2208 Rk\u00d7k denote the matrix where the i-th column is siw\u2217i . Let A\u0303 \u2208 Rk\u00d7k denote the matrix where the i-th column is V u\u0302i. Let b \u2208 Rk denote the vector Q1, let b\u0303 denote the vector Q\u03021 \u2212Q1. Then we have\n\u2016A\u2016 \u2264 \u221a k.\nUsing Fact B.3, we can lower bound \u03c3k(A),\n\u03c3k(A) \u2265 1/\u03ba.\nWe can upper bound \u2016A\u0303\u2016 in the following way,\n\u2016A\u0303\u2016 \u2264 \u221a kmax i\u2208[k] {\u2016V u\u0302i \u2212 siw\u2217i \u2016}\n\u2264 \u221a kmax i\u2208[k] {\u2016V u\u0302i \u2212 siV V >w\u2217i + siV V >w\u2217i \u2212 siUU>w\u2217i \u2016} \u2264 \u221a k(\u03b4\u03023 + \u03b4\u03022).\nWe can upper bound \u2016b\u2016 and \u2016b\u0303\u2016,\n\u2016b\u2016 = \u2016Q1\u2016 \u2264 k k\u2211 i=1 |z\u2217i |, and \u2016b\u0303\u2016 \u2264 \u03b4\u03024\u2016Q1\u2016.\nTo apply Lemma E.11, we need \u03b4\u03024 \u2264 1/4 and \u03b4\u03022 . 1/( \u221a k\u03ba2), \u03b4\u03023 . 1/( \u221a k\u03ba2). So we have,\n\u2016z\u0302i \u2212 z\u2217i \u2016 \u2264 (\u03ba4k3/2(\u03b4\u03022 + \u03b4\u03023) + \u03ba2k1/2\u03b4\u03024)\u2016Q1\u2016\n\u2264 (\u03ba4k3/2(\u03b4\u03022 + \u03b4\u03023) + \u03ba2k1/2\u03b4\u03024) k\u2211 i=1 |z\u2217i |."}, {"heading": "E.5.4 Linear System for the Second-order Moment", "text": "The following lemma provides estimation error bound for the second linear system in Eq. (28).\nLemma E.14 (Solution of linear system for the second order moment). Let U \u2208 Rd\u00d7k be the orthogonal column span of W \u2217 denote an orthogonal matrix satisfying that \u2016V V > \u2212 UU>\u2016 \u2264 \u03b4\u03022 . 1/(\u03ba \u221a k). For each i \u2208 [k], let u\u0302i denote the vector satisfying \u2016u\u0302i\u2212V >w\u2217i \u2016 \u2264 \u03b4\u03023 . 1/( \u221a k\u03ba3). Let Q2 be defined as in Eq. (26) and Q\u03022 be the estimation of Q2 such that \u2016Q2\u2212Q\u03022\u2016F \u2264 \u03b4\u03024\u2016Q2\u2016F \u2264 14\u2016Q2\u2016F . Let r\u2217 \u2208 Rk and r\u0302 \u2208 Rk be defined as in Eq. (27) and Eq. (28). Then\n|r\u0302i \u2212 r\u2217i | \u2264 (k3\u03ba8\u03b4\u03023 + \u03ba2k2\u03b4\u03024)\u2016r\u2217\u2016.\nProof. For each i \u2208 [k], let ui = V >w\u2217i . Let A \u2208 Rk 2\u00d7k denote the matrix where the i-th column is vec(uiu>i ). Let A\u0303 \u2208 Rk 2\u00d7k denote the matrix where the i-th column is vec(uiu>i \u2212 u\u0302iu\u0302>i ). Let b \u2208 Rk2 denote the vector vex(Q2), let b\u0303 \u2208 Rk 2 denote the vector vec(Q2 \u2212 Q\u03022).\nLet \u25e6 be the element-wise matrix product (a.k.a. Hadamard product), W = [w\u22171 w\u22172 \u00b7 \u00b7 \u00b7 w\u2217k] and U = [u1 u2 \u00b7 \u00b7 \u00b7 uk] = V >W . We can upper bound \u2016A\u2016 and \u2016A\u0303\u2016 as follows,\n\u2016A\u2016 = max \u2016x\u2016=1 \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 xivec(uiu>i ) \u2225\u2225\u2225\u2225\u2225 = max \u2016x\u2016=1 \u2016Udiag(x)U>\u2016F\n\u2264 \u2016U\u20162\n\u2264 \u03c321(V >W ),\nand\n\u2016A\u0303\u2016 = \u221a kmax i\u2208[k] \u2016A\u0303i\u2016\n\u2264 \u221a kmax i\u2208[k] \u2016uiu>i \u2212 u\u0302iu\u0302>i \u2016F \u2264 \u221a kmax i\u2208[k] 2\u2016ui \u2212 u\u0302i\u20162 \u2264 2 \u221a k\u03b4\u03023.\nWe can lower bound \u03c3k(A),\n\u03c3k(A) = \u221a \u03c3k(A>A)\n= \u221a \u03c3k((U>U) \u25e6 (U>U))\n= min \u2016x\u2016=1\n\u221a x>((U>U) \u25e6 (U>U))x\n= min \u2016x\u2016=1\n\u2016(U>U)1/2diag(x)(U>U)1/2\u2016F\n\u2265 \u03c32k(V >W )\nwhere fourth step follows Schur product theorem, the last step follows by the fact that \u2016CB\u2016F \u2265 \u03c3min(C)\u2016B\u2016F and \u25e6 is the element-wise multiplication of two matrices.\nWe can upper bound \u2016b\u2016 and \u2016b\u0303\u2016,\n\u2016b\u2016 \u2264\u2016Q2\u2016F \u2264 \u2016r\u2217\u2016,\n\u2016b\u0303\u2016 =\u2016Q2 \u2212 Q\u03022\u2016F \u2264 \u03b4\u03024\u2016r\u2217\u2016.\nSince \u2016V V >W \u2212W\u2016 \u2264 \u221a k\u03b4\u03022, we have for any x \u2208 Rk,\n\u2016V V >Wx\u2016 \u2265 \u2016Wx\u2016 \u2212 \u2016(V V >W \u2212W )x\u2016\n\u2265 \u03c3k(W )\u2016x\u2016 \u2212 \u03b4\u03022 \u221a k\u2016x\u2016\nNote that according to Fact B.3, \u03c3k(W ) \u2265 1/\u03ba. Therefore, if \u03b4\u03022 \u2264 1/(2\u03ba \u221a k), we will have\n\u03c3k(V >W ) \u2265 1/(2\u03ba). Similarly, we have \u03c31(V >W ) \u2264 \u2016V \u2016\u2016W\u2016 \u2264 \u221a k. Then applying Lemma E.11 and setting \u03b4\u03022 . 1\u221ak\u03ba3 , we complete the proof."}, {"heading": "F Acknowledgments", "text": "The authors would like to thank Surbhi Goel, Adam Klivans, Qi Lei, Eric Price, David P. Woodruff, Peilin Zhong, Hongyang Zhang and Jiong Zhang for useful discussions."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs).<lb>We distill some properties of activation functions that lead to local strong convexity in the<lb>neighborhood of the ground-truth parameters for the 1NN squared-loss objective. Most popular<lb>nonlinear activation functions satisfy the distilled properties, including rectified linear units<lb>(ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also<lb>smooth, we show local linear convergence guarantees of gradient descent under a resampling rule.<lb>For homogeneous activations, we show tensor methods are able to initialize the parameters to<lb>fall into the local strong convexity region. As a result, tensor initialization followed by gradient<lb>descent is guaranteed to recover the ground truth with sample complexity d \u00b7 log(1/ ) \u00b7poly(k, \u03bb)<lb>and computational complexity n \u00b7 d \u00b7 poly(k, \u03bb) for smooth homogeneous activations with high<lb>probability, where d is the dimension of the input, k (k \u2264 d) is the number of hidden nodes, \u03bb<lb>is a conditioning property of the ground-truth parameter matrix between the input layer and<lb>the hidden layer, is the targeted precision and n is the number of samples. To the best of our<lb>knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample<lb>complexity and computational complexity linear in the input dimension and logarithmic in the<lb>precision. \u2217A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on<lb>Machine Learning (ICML 2017).<lb>\u2020Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and part of the work was done<lb>while interning in Microsoft research, India.<lb>\u2021Supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security).<lb>Supported in part by Australian Research Council through an Australian Laureate Fellowship (FL110100281) and<lb>through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS),<lb>and NSF grants IIS-1619362.<lb>\u00b6Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.<lb>ar<lb>X<lb>iv<lb>:1<lb>70<lb>6.<lb>03<lb>17<lb>5v<lb>1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>1<lb>0<lb>Ju<lb>n<lb>20<lb>17", "creator": "LaTeX with hyperref package"}}}