{"id": "1706.03199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "abstract": "This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework of the neural networks to maximize the efficiency.\n\n\n\nThe paper has been used in the field of neural nets as an aid to the search for novel and potentially useful artificial intelligence platforms for artificial intelligence, particularly in the general population.\nThe paper is based on an algorithm-based method to investigate the potential of neural nets to identify neural networks based on individual data and to demonstrate the optimal algorithms for the neural networks.\nReferences\nEmanuel J, M., and Nell E, S. (2010). The potential for deep learning from human knowledge and machine learning. Nature, 48(1):637-60.\nJohn D, D., and Z. F. (2009). Deep neural network learning from human memory using human vision systems. Nature, 38(1):939-40.", "histories": [["v1", "Sat, 10 Jun 2017 07:55:38 GMT  (974kb,D)", "https://arxiv.org/abs/1706.03199v1", null], ["v2", "Tue, 20 Jun 2017 11:38:25 GMT  (974kb,D)", "http://arxiv.org/abs/1706.03199v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier bousquet", "sylvain gelly", "karol kurach", "marc schoenauer", "michele sebag", "olivier teytaud", "damien vincent"], "accepted": false, "id": "1706.03199"}, "pdf": {"name": "1706.03199.pdf", "metadata": {"source": "CRF", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "authors": ["Olivier Bousquet", "Sylvain Gelly", "Karol Kurach", "Marc Schoenauer", "Mich\u00e8le Sebag", "Olivier Teytaud", "Damien Vincent"], "emails": ["oteytaud@google.com"], "sections": [{"heading": null, "text": "parallel setting is considered to address the algorithm calibration problem \u2212 selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art [7] with no extra hyper-parameter."}, {"heading": "1 Introduction", "text": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20]. Several challenges have been organized to further investigate both algorithm selection and calibration issues in the last few years [10, 1].\nThe algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23]. This renders the algorithm calibration an even more critical issue: on the one hand, DL notoriously requires high computational resources; on the other hand, it involves a structured hyper-parameter space, hindering the approximation of the performance model. Automatic algorithm calibration thus is challenged by\nar X\niv :1\n70 6.\n03 19\n9v 2\n[ cs\n.L G\nmanual algorithm calibration, as noted by [7]. As the experienced practitioner can easily detect and stop the unpromising runs based on their learning curves in the first epochs, she can afford to consider many more hyper-parameter settings.\nHow to discard as early as possible runs/solutions that will eventually yield under-optimal results has long and thoroughly been investigated (section 2). The early discard decision problem raises two interdependent questions: uncertainty modelling, as the eventual quality of a run result is unknown until the run is achieved; risk control, as one needs guarantees that the run which would have yielded the best result has not been stopped.\nThis paper addresses the early discard problem in the context of parallel one-shot deep neural training. Formally, the considered framework, referred to as parallel one-shot run race (PaRR), allocates all available computational resources at the beginning of the period to train deep neural nets; each core runs with its specific hyper-parameter setting, or configuration, with no communication among the cores. The goal is to make DL robust w.r.t. random hazards (e.g. initializations) and bad decisions (e.g. configuration set), eventually delivering the optimal configuration/learned model, with a minimal computational budget. The challenge lies in making the stopping decision with little and censored evidence: as all runs are simultaneous, only prior information about the learning curves behavior is available, as in [7]. Formally, the PaRR problem is a constrained optimization problem: i) the constraint regards the guarantees about eventually delivering the optimal result, i.e. ensuring that the best run lives until the end of the period; ii) the optimization consists of minimizing the computational budget subject to the optimality guarantee, by stopping any run (with no possible resuming of the run, as opposed to [26]) as early as possible.\nThe present paper, building upon the current best approach [7], makes theoretical and empirical contributions. On the theoretical side and with no additional hyper-parameter, a principled approach is used to set the pruning thresholds; furthermore, guarantees are obtained through a principled treatment of the multiple hypothesis testing issue. On the empirical side, experimentations on the Cifar [13], PTB [17], and MiniWiki [11] benchmarks show a consistent improvement compared to [7], for each and every hyper-parameter setting.\nThis paper is organized as follows. Section 2 briefly discusses related work. Section 3 formulates the PaRR problem together with different types of statistical risk and associated criteria. The proposed PaRR decision maker, with the different variants associated to the risks, are empirically assessed and compared to the state of the art in section 4."}, {"heading": "2 Related work", "text": "Performance modelling. In the domain of algorithm selection and calibration, a usual approach is to build a performance model [22], predicting the eventual performance of the algorithm based on its hyper-parameter configuration and on the description of the problem instance at hand. In the Machine Learning domain however, in contrast with the SAT and CSP domains, to our\nbest knowledge there does not exist yet an affordable feature set, able to accurately describe a problem instance and to support the prediction of an algorithm performance on this instance. For this reason, algorithm selection and calibration in Machine Learning (see e.g. [3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]). In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration. In [12], coordinate-based optimization reports good results, particularly so in high-dimensional hyper-parameter space.\nBy construction, the above approaches are intrinsically sequential, making it difficult to use the above performance models to stop unpromising runs. A first extension overcoming the sequential issue is proposed by [26]. The instance-dependent Gaussian Process model built from the available learning curves1 is used to decide whether to freeze a run or start another run. Overall, [26] maintains a basket of runs, typically involving 10 alive (non-frozen) runs and 3 new ones, where the decision is based on the maximum \"asymptotic\" performance reached on this learning curve according to Expected Improvement. In each round, the GP model is updated and the basket of runs is recomposed. Another approach is that of [7], with two differences compared to [26]. Firstly, the domain knowledge is leveraged to select 11 models best reflecting the usual learning curves (ranging from vapor-pressure to Weibull law; see [7] for more detail), and referred to as basic models in the following. The ensemble of these basic models constitute a parametric ensemble modelling space, including the parameters of each model and the weight of each model in the ensemble. Each learning curve is exploited using Bayesian inference to derive a posterior distribution on the ensemble modelling space, best accounting for this learning curve. The exploitation of this posterior distribution via MCMC supports an estimation of the performance that might be reached later on this learning curve, and the confidence thereof. Finally, based on a (user-supplied) confidence level \u03b4, a learning curve is halted whenever the probability that its eventual performance improves on the best-so-far performance is less than \u03b4.\nMulti-Armed Bandit. Another approach to parallel online optimization and pruning is based on the Multi-Armed Bandit framework, offering rigorous guarantees about the optimal allocation of trials. In [16], the problem of hyperparameter optimization is formulated as a pure exploration adaptive resource allocation problem. The approach builds upon the Successive-Halving process proposed by [9], which most simply prunes 50% of the runs with lowest current performance, until a single configuration remains; each run corresponds to a (uniformly sampled) configuration. Naturally, the overall performance of Successive-Halving critically depends on the initial allocated computational resources. The Hyperband approach [16] addresses this limitation using a infinitely-many arm bandit approach on the space of number n of configurations\n1In the following, learning curve denotes the available evidence about a configuration, reporting the performance w.r.t. the number of epochs so far.\nto be considered in parallel, times computational time r allocated between two pruning steps, where the instant reward associated to an (n, r) pair is the best learning performance achieved by Successive-Halving(n, r).\nDiscussion. Compared to Hyperband, performance modelling offers two significant advantages. Firstly, it makes it possible to prune an arbitrary number of runs whenever an excellent one is found; in contrast, Hyperband does not allow learning across runs; each trial consider a new iid configuration sample. Secondly, Hyperband involves a fixed discarding rate, determining the fraction of pruned runs in each Successive-Halving step. In the Deep Learning context however, validation curves are very noisy at the beginning, and some hyper-parameters (e.g. when the learning rate decay starts) have a delayed impact, making all learning curves very similar in the early steps. In such contexts, early pruning is mostly random. On the other side, performance modelling does not allow for an efficient pruning in the parallel setting. Typically, whenever several runs are very similar and close from the best-so-far one, the parallel approach proposed by [7] is bound to keep them all.\nOur goal, as said, is to achieve an optimal pruning under the optimality constraint (preserving the optimal performance out of the initial set of configurations). To this aim, the contributions described in next section will focus on how to use the performance model in order to adjust the selection threshold, and how to address the multiple hypothesis testing issue in a consistent way."}, {"heading": "3 Overview of PaRR", "text": "The presented approach relies on performance modelling and closely follows the approach of [7].The same 11 basic models are used2. All attempts to reduce the number of models resulted in lesser performances. Each learning curve (validation-error(t), for t = 1 . . .current epoch) derives a posterior distribution on the ensemble modelling space, using Bayesian inference from the same uninformative prior. This posterior distribution is likewise used by MCMC to derive an estimate of the validation-error for t\u2032 > t, together with the confidence thereof. The overall computational budget is finite, with t < T . For simplicity and by abuse of language, we will refer to asymptotic properties to designate properties that are true at epoch T .\nCriteria for halting a run. Six criteria are presented below, to make the decision of halting a learning curve (halting the run with no later resuming). These criteria are parameterized by a confidence threshold \u03b4 \u2208 [0, 1], as in [7]. Another quantity involved in these criteria is the current best performance noted y\u2217(t) and the predicted asymptotic result of the current best learning curve, noted y\u0302\u2217(T ).\n2As the goal is a minimization one, the model f\u03b8(x) becomes \u03b8\u2032 \u2212f\u03b8(x) with \u03b8\u2032 an additional parameter.\n(a) Default halt operator: a run is halted if the probability that it performs asymptotically better than the current best is less than \u03b4. This is the criterion used by [7]). If we trust the confidence intervals and if the validation error is noise-free, this criterion has a probability at least 1\u2212 \u03b4 not to halt an optimal curve. At each given time step, the probability of a mishalt (i.e., halt of the optimal run) is therefore bounded by \u03b4.\n(b) a run is halted if the probability that it performs asymptotically better than the predicted asymptotic result of the current best is less than \u03b4. Compared to (a), the bound now is the predicted expected performance of the current best, instead of the current result of the current best. But this criterion is risky, as it does not use the confidence interval of the current best; it will therefore not be mentioned any more here, subsumed by next criterion (c).\n(c) Prediction-halt operator: a run is halted if the probability that it performs asymptotically better than a conservative estimate of the predicted asymptotic result of the current best is less than \u03b4. This criterion uses an upper bound on the asymptotic performance of the current best run. The decision hence depends on two curve models. There is thus, there is a probability of at least 1\u2212 2\u03b4 not to halt an optimal run: the probability f mishalt is therefore bounded by 2\u03b4. A drawback of both (b) and (c) is how they handle currently poor runs that present a steep improvement, whereas the current best is stagnating. In such case, the criterion might be too conservative. This leads to proposing the following criteria (d) and (e), counterparts of criteria (b) and (c) but using the overall best conservative prediction instead of the conservative prediction of the current best.\n(d) a run is halted if the probability that it performs asymptotically better than the overall best of all predicted asymptotic results is less than \u03b4.\n(e) Best-prediction halt operator: a run is halted if the probability that it performs asymptotically better than the overall best conservative estimate of all predicted asymptotic results is less than \u03b4. There is however a subtle side-effect with this operator: if the probability of failure is 1\u2212 \u03b4 for each run, and there are n runs, then the cumulated risk can be 1!. This is why we propose the following criterion:\n(f) Clever-halt operator: a run is halted if the probability that it performs asymptotically better than the kth best predicted asymptotic results is less than \u03b4. We will now discuss the choice of k.\nLet us assume that there are n competing runs, and that the probability that one given curve modelling fails in providing an upper or lower bound is at most\n\u03b4 (we trust our models with confidence \u03b4). Then, the probability that at least one curve is poorly modeled is less than 1\u2212 (1\u2212 \u03b4)n \u2013 and one poor modeling can lead to the failure of methods (d) or (e).\nFurthermore, this implies that the probability either the best asymptotic run or the current best run is poorly modeled is less than 2\u03b4: this justifies method (c), but not (b).\nFinally, the probability that at least k curves are poorly modeled can be made less than \u03b4 by choosing k sufficiently large. Here, using the Gaussian approximation of this probability, we want that k satisfies P (N (0, 1) \u2265 k\u2212n\u03b4\u221a\nn\u03b4(1\u2212\u03b4) ) \u2264 \u03b4.\nWe select k numerically as the smallest integer satisfying this inequality. As k depends on n, it will vary from one context to the next."}, {"heading": "4 Experiments", "text": "This section presents the experimental setting and the experimental methodology followed to empirically compared the proposed PaRR pruning criteria. The first experimental results (section 4.3) suggest some simple heuristic improvements (section 4.4)."}, {"heading": "4.1 Experimental setting", "text": "Two families of large-size problems are considered, within the domain of language modelling and classification. In the former case, the loss is expressed in terms of perplexity (bits-per-unit, when predicting the next word). In the latter one, the loss is the cross-entropy one unless otherwise stated. Due to the large size of the datasets, only the validation error is considered; the overfitting issue is beyond the scope of the paper.\nFive language modelling tasks are considered. PTB [17] aims at language modeling at the bit, byte and word levels. MiniWiki is a subset of the Hutter dataset [11] (also referred to as enwik8.zip); the size of the training set is 6% of the overall size; the modelling task is at the bit and byte level. The considered neural architecture involves 3 stacked LSTM with 500 units, batch size 50, 30 unrolling steps, with a budget of 30 epochs. The hyper-parameters (uniformly and independently drawn) are the weight init scale (in [0.02, 1]), the learning rate (in [5, 100]), the dropout keep probability (in [0.2, 1]), the clipping gradient norm (in [0.05, 1]).\nFour classification settings are considered, all based on the Cifar10 dataset [13], and involving four learning rate adaptation methods, namely Adagrad, Adam, Gradient, and Momentum. The NN architecture is made of 3 convolutional layers, with filter size 7x7, max-pooling, stride 1, 512 chanels; followed by a convolutional layer with filter size 5x5 and 64 chanels; followed by two fully connected layers with 384 and 192 units respectively. In all cases, the batch size is 64 with a budget of 200 epochs. The hyper-parameters (uniformly and independently drawn) include the weight init scale (in [0.001, 100]), the weight init scale for convolutional layers (in [0.001, 0.1]), the learning rate (in [0.00001, 10]),\nthe clipping gradient norm (in [0.01, 10]), the number of epochs before learning rate decay (in [12, 198]), the learning rate decay (in [0.9, 0.999]) and the dropout keep probability (in [0.8, 1]) for non-convolutional layers.\nSpecific experiments are considered for investigation: Mini: considers the PTB word prediction task, with a small net with 2 layers of 20 units, 30 unrolling steps, learning rate 35, dropout 0.5, gradient clipping norm 0.143, 30 epochs, cell clipping [5] between 1 and 10000.\nMaxi: only differs from Mini as it considers more runs in parallel. Coupled: considers the PTB bytes or word prediction task, with a larger net involving 2 LSTM with 650 units, optimized by stochastic gradient descent, optionally with coupled input and forget gates, other values as in Mini.\nMetaCifar: operates on four experiments, each one considering 22 runs with a different learning rate adaptation method (Adagrad, Adam, Gradient, and Momentum).\nMetanormalization: considers 32 runs with different variants of normalization for language modeling with three different toy sequences: (i) rote learning of sequences \u201c.M\u201d made of repetition of identical words; (ii) sequences \u2018AN\u201d repeating identical repetitions of same length words made of a same letter (but with possibly different lengths for different sequences); (iii) sequences of the form \u201canbn\u201d (aaabbb aaaaaabbbbb. . . ) with varying n."}, {"heading": "4.2 Experimental methodology", "text": "In the remainder of the paper, a failure (FAIL) stands for halting the best run. The main components for the PaRR decisions are: i) the confidence threshold \u03b4 needed to prune a run; ii) the comparison threshold: the pruning decision is taken if the predictive performance of a run is below the comparison threshold with confidence at least 1 - \u03b4, and iii) the overall computational budget. The sensitivity w.r.t. the confidence value is discussed in section 4.3. The impact of the overall computational budget (here, the number of simultaneous runs) is dramatic, as illustrated on Fig. 1 in the case of the PTB modelling task at the bit level. 50 runs are launched in parallel, with an allowed number of epochs set to 30, 15, and 7. Empirically, circa 30 independent parallel runs are required to eventually deliver a \"reasonably optimal\" performance. Note that this experimental setting is a typical one, with a very significant computational cost; hence the need for the present work. Additionally, Fig. 1 empirically demonstrates that the early ranks of the runs can be very misleading. The experimental observations on a given problem are illustrated on the Mini case (Fig. 2) for \u03b4 = .5. In the following, the performance of a pruning criterion on a given problem is assessed depending on whether it failed and halted the best run (FAIL); otherwise, it reports the computational savings."}, {"heading": "4.3 Pruning with confidence \u03b4 = .5", "text": "Table 1 reports the results of criteria (a) (the baseline [7]); (c), (e) and (f) on all experimental cases, for \u03b4 = .5.\nUnexpectedly, although the confidence is very low (which might imply at first sight that the probability of FAIL is ca 50%), these results are very good. A tentative interpretation for this fact is that, although the confidence is low, the comparison threshold is set to the best validation error so far, which is a quite conservative threshold. With a low confidence \u03b4, more aggressive comparison thresholds entail failures. Criterion (c), considering the predicted error threshold, fails. Criterion (e), considering the best predicted validation error, fails even more often. Overall, the baseline method (a) and method (e) do not fail. The computational savings are such that method (e) wins over (a) in 5 experimental cases, and (a) wins over (e) in 1 experimental case."}, {"heading": "4.4 Conservative pruning rules", "text": "A natural question raised from the empirical results (Table 1) is whether undesirable failures could be prevented using simple heuristic conservative rules, such as: i) never prune the current best; ii) discard all predictions of a negative loss; iii) discard predictions with correlation data/observation less than 0.5. Accordingly, the experiments are repeated by enriching all pruning criteria with the conservative rules (Table 2). Although these simple rules do save some failures for methods (c) and (e), the overall conclusions remain the same as from Table 1: methods (a) and (f) are the only safe ones, with (e) slightly outperforming (a) in terms of computational savings. As expected, the overall gain is eroded as the aggressive (c) and (e) methods do no longer fail on 4 out of the 15 problems."}, {"heading": "4.5 Pruning with low confidence \u03b4 and conservative rules", "text": "Assuming that the conservative heuristic rules will prevent aggressive pruning criteria from most failures, a question is whether better savings can be obtained by lowering the confidence threshold. As shown in Table 3 (supplementary material), a lower \u03b4 = .01 does significantly improve the results for criterion (c) (though dominated by the results of criterion (f) for \u03b4 = .5). For criterion (e) however, failures are still observed; the interpretation for this fact (in agreement with the multiple hypothesis testing framework indeed) is that increasing the number of tests is a strong factor of failure. Interestingly, the overall results are globally worse than for \u03b4 = 0.5.\nOther confidence levels (\u03b4 = 0.1, \u03b4 = 0.3, \u03b4 = 0.05, \u03b4 = 0.01) have been considered (results in supplementary material). Fig. 3 graphically displays the performance of the pruning criterion (f) compared to the baseline (a) [7]."}, {"heading": "5 Conclusions", "text": "A first contribution of the presented work is to confirm the relevance of the pruning method proposed by [7], with computational savings often above 85% (particularly so for image applications), and applicable at both levels of randomized hyper-parameter optimization, and model selection (the model selection problem itself embedding a hyper-parameter selection problem). Cumulative gains from both levels can decrease the computational cost by more than an order of magnitude as shown on the Cifar and Metacifar experiments. Interestingly, the sensitivity of the approach w.r.t. the confidence threshold \u03b4 reveals itself to be low. Set to 0.05 in [7], we show that it can be increased up to 0.5; this stability is explained from the conservative modelling of the performance in the considered settings. Our second contribution is a new and more principled pruning method, slightly but significantly outperforming the former method for all confidence threshold \u03b4, with a excellent stability with respect to \u03b4 (see Fig.3 and supplementary material). The novelty: of the proposed approach is to refine the halting threshold using predictions, and use a quantile of the predictions (as opposed to, the best prediction), in an adaptive manner. The robustness of this approach relies on its consistent grounding on the multiple hypothesis testing framework. A first research perspective is to introduce diversity-based pruning for ensemble methods [15], taking inspiration from the clearing methods\nin multi-modeal optimization [21]. A second perspective will investigate whether the quantile-based proposed approach can make the inference simpler (as opposed to the Metropolis-Hasting method used in [7]). Finally, combining the proposed approach with mainstream Bayesian Optimization, exploiting both the validation curves and the structure of the hyperparameter domain, would allow to learn accross runs."}, {"heading": "A Experiments with other values of \u03b4", "text": "A.1 Comparison between criteria with \u03b4 = 0.1\nTestbed Budget (numberof runs) Computational cost saved up by method (a) (equal to Domhan et al) C.c. saved up by method (c): prediction-halt operator. C.c. saved up by method (e): best-predictionhalt operator. C.c. saved up by method \u201cCleverhalt\u201d Cifar-adagrad 22 -50.7% -50.7% -57.7% -50.7% Cifar-adam 22 -87.0% -92.4% FAIL by0.797 \u2192 0.923 -95.2% FAIL by 0.797 \u2192 0.933 -87.0% Cifar-gradient 22 -79.0% -79.0% -79.0% -79.0% Cifarmomentum 22 -81.3% -85.2% -85.2% -81.3% Miniwiki bits 250 -30.0% -30.0% -35.2% -30.0% Miniwiki bytes 250 -12.0% -12.0% -12.0% -12.0% PTB bits 250 -23.6% -23.6% -26.0% -23.6% PTB bytes 250 -19.73% -19.73% -19.73% -19.73% PTB words 250 -52.09% -60.16 -63.33 -52.12 Miniwiki bits 50 -36.3% -36.3% -36.3% -36.3% Miniwiki bytes 50 -33.5% -33.5% -62.3% -33.5% PTB bits 50 -28.7% -28.7% -28.7% -28.7% PTB bytes 50 -19.4% -19.4% -19.4% -19.4% PTB words 50 -51.5% -59.4% -67% FAIL by1.201 \u2192 1.238 -52.3%\nNo fail, best mean performance.\nA.2 Comparison between criteria with \u03b4 = 0.3 We checked the stability of the method by also checking what is going on at \u03b4 = 0.3. We still get the best results with our method.\nTestbed Budget (numberof runs) Computational cost saved up by method (a) (equal to Domhan et al) C.c. saved up by method (c): prediction-halt operator. C.c. saved up by method (e): best-predictionhalt operator. C.c. saved up by method \u201cCleverhalt\u201d Cifar-adagrad 22 -50.2% -50.2% -57.2% -50.2% Cifar-adam 22 -85.8 % -85.8% -94.75% FAILby 0.80 \u2192 0.94 -85.9% Cifar-gradient 22 -70.75% -70.75% -70.75% -70.75% Cifarmomentum 22 -63.3% -81.1% -81.1% -63.3% Miniwiki bits 250 -19% -19% -20.5% -19% Miniwiki bytes 250 -9.5% -9.5% -9.5% -9.1% PTB bits 250 -17.1% -17.1% -19.2% -17.1% PTB bytes 250 -17.7% -17.7% -17.7% -17.7% PTB words 250 -40.5% -51.0% -56.4% -44.4% Miniwiki bits 50 -32.7% -32.7% -32.7% -32.7% Miniwiki bytes 50 -30.5% -30.5% -57.8% -32.7% PTB bits 50 -27.1% -27.1% -27.1% -27.1% PTB bytes 50 -18% -18% -18% -18% PTB words 50 -49.6% -58.1% -65.7% FAIL by 1.2014 \u2192 1.23329 -50.3%\nNo fail, best mean performance."}], "references": [{"title": "Collaborative hyperparameter tuning", "author": ["R. Bardenet", "M. Brendel", "B. K\u00e9gl", "M. Sebag"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML), volume 28, pages 199\u2013207,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for hyperparameter optimization", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "P. B. F. P. K. W. J. Shawe-Taylor, R.S. Zemel, editor, Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS), volume 24 of Advances in Neural Information Processing Systems, Granada, Spain,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparison of ranking methods for classification algorithm selection", "author": ["P. Brazdil", "C. Soares"], "venue": "R. L. de M\u00e1ntaras and E. Plaza, editors, the 11th European Conference on Machine Learning (ECML), volume 1810 of LNCS, pages 63\u201374. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep recurrent neural networks for acoustic modelling", "author": ["W. Chan", "I. Lane"], "venue": "CoRR, abs/1504.01482,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "New types of deep neural network learning for speech recognition and related applications: an overview", "author": ["L. Deng", "G.E. Hinton", "B. Kingsbury"], "venue": "ICASSP, pages 8599\u20138603. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["T. Domhan", "J.T. Springenberg", "F. Hutter"], "venue": "IJCAI, pages 3460\u20133468. AAAI Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages 647\u2013655. JMLR.org,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-stochastic Best Arm Identification and Hyperparameter Optimization, volume 51 of JMLR Workshop and Conference Proceedings", "author": ["A. Gretton", "C.C. Robert", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Hyperparameter optimization of deep neural networks using non-probabilistic RBF surrogate model", "author": ["I. Ilievski", "T. Akhtar", "J. Feng", "C.A. Shoemaker"], "venue": "CoRR, abs/1607.08316,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, NIPS 12, pages 1106\u20131114,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Bayesian hyperparameter optimization for ensemble learning", "author": ["J. Levesque", "C. Gagn\u00e9", "R. Sabourin"], "venue": "CoRR, abs/1605.06394,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient hyperparameter optimization and infinitely many armed bandits", "author": ["L. Li", "K.G. Jamieson", "G. DeSalvo", "A. Rostamizadeh", "A. Talwalkar"], "venue": "CoRR, abs/1603.06560,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Comput. Linguist., 19(2):313\u2013 330, June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "al"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Bayesian heuristic approach to global optimization and examples", "author": ["J. Mockus"], "venue": "J. Global Optimization, 22(1-4):191\u2013203,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Using meta-mining to support data mining workflow planning and optimization", "author": ["P. Nguyen", "M. Hilario", "A. Kalousis"], "venue": "J. Artif. Intell. Res. (JAIR), 51:605\u2013644,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A clearing procedure as a niching method for genetic algorithms", "author": ["A. P\u00e9trowski"], "venue": "International Conference on Evolutionary Computation, pages 798\u2013803,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "The algorithm selection problem", "author": ["J. Rice"], "venue": "Advances in computers, 15:65\u2013118,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1976}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "al"], "venue": "search. Nature,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "P. L. Bartlett, F. C. N. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, NIPS12, pages 2960\u20132968,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise meta-rules for better meta-learningbased algorithm ranking", "author": ["Q. Sun", "B. Pfahringer"], "venue": "Machine Learning, 93(1):141\u2013161,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Freeze-thaw bayesian optimization", "author": ["K. Swersky", "J. Snoek", "R.P. Adams"], "venue": "CoRR, abs/1406.3896,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 847\u2013855. ACM,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-weka: combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "I. S. Dhillon, Y. Koren, R. Ghani, T. E. Senator, P. Bradley, R. Parekh, J. He, R. L. Grossman, and R. Uthurusamy, editors, The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, (KDD), pages 847\u2013855,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art [7] with no extra hyper-parameter.", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 1, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 22, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 0, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 20, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 15, "context": "The algorithm selection problem \u2212 aimed at selecting a priori the learning algorithm best suited to a given dataset, and the algorithm calibration problem \u2212 aimed at identifying the best hyper-parameter setting of an algorithm for the dataset at hand, have been acknowledged to be key issues since the late 80s [4, 3, 27, 2, 25, 20].", "startOffset": 311, "endOffset": 332}, {"referenceID": 9, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 6, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 4, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 13, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 18, "context": "The algorithm selection issue appears settled as of now, at least in the case where sufficient training data is available: deep learning (DL) consistently delivers dominant performances in many application domains, and is currently considered to the best learning algorithm in the large data regime [14, 8, 6, 18, 23].", "startOffset": 299, "endOffset": 317}, {"referenceID": 5, "context": "manual algorithm calibration, as noted by [7].", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "The challenge lies in making the stopping decision with little and censored evidence: as all runs are simultaneous, only prior information about the learning curves behavior is available, as in [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 21, "context": "ensuring that the best run lives until the end of the period; ii) the optimization consists of minimizing the computational budget subject to the optimality guarantee, by stopping any run (with no possible resuming of the run, as opposed to [26]) as early as possible.", "startOffset": 241, "endOffset": 245}, {"referenceID": 5, "context": "The present paper, building upon the current best approach [7], makes theoretical and empirical contributions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "On the empirical side, experimentations on the Cifar [13], PTB [17], and MiniWiki [11] benchmarks show a consistent improvement compared to [7], for each and every hyper-parameter setting.", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": "On the empirical side, experimentations on the Cifar [13], PTB [17], and MiniWiki [11] benchmarks show a consistent improvement compared to [7], for each and every hyper-parameter setting.", "startOffset": 140, "endOffset": 143}, {"referenceID": 17, "context": "In the domain of algorithm selection and calibration, a usual approach is to build a performance model [22], predicting the eventual performance of the algorithm based on its hyper-parameter configuration and on the description of the problem instance at hand.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 23, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 1, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 102, "endOffset": 109}, {"referenceID": 19, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 102, "endOffset": 109}, {"referenceID": 23, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "[3, 24, 28]), builds online an instance-dependent performance model, learned using Gaussian Processes [3, 24], Random Forests [28] or radius-based functions [12]).", "startOffset": 157, "endOffset": 161}, {"referenceID": 1, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 19, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 23, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 14, "endOffset": 25}, {"referenceID": 14, "context": "In most cases [3, 24, 28] the performance model is used along Bayesian Optimization principles [19] to determine the most promising algorithm configuration.", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "In [12], coordinate-based optimization reports good results, particularly so in high-dimensional hyper-parameter space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "A first extension overcoming the sequential issue is proposed by [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Overall, [26] maintains a basket of runs, typically involving 10 alive (non-frozen) runs and 3 new ones, where the decision is based on the maximum \"asymptotic\" performance reached on this learning curve according to Expected Improvement.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Another approach is that of [7], with two differences compared to [26].", "startOffset": 28, "endOffset": 31}, {"referenceID": 21, "context": "Another approach is that of [7], with two differences compared to [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "Firstly, the domain knowledge is leveraged to select 11 models best reflecting the usual learning curves (ranging from vapor-pressure to Weibull law; see [7] for more detail), and referred to as basic models in the following.", "startOffset": 154, "endOffset": 157}, {"referenceID": 11, "context": "In [16], the problem of hyperparameter optimization is formulated as a pure exploration adaptive resource allocation problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "The approach builds upon the Successive-Halving process proposed by [9], which most simply prunes 50% of the runs with lowest current performance, until a single configuration remains; each run corresponds to a (uniformly sampled) configuration.", "startOffset": 68, "endOffset": 71}, {"referenceID": 11, "context": "The Hyperband approach [16] addresses this limitation using a infinitely-many arm bandit approach on the space of number n of configurations", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Typically, whenever several runs are very similar and close from the best-so-far one, the parallel approach proposed by [7] is bound to keep them all.", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "The presented approach relies on performance modelling and closely follows the approach of [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "These criteria are parameterized by a confidence threshold \u03b4 \u2208 [0, 1], as in [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "This is the criterion used by [7]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 12, "context": "PTB [17] aims at language modeling at the bit, byte and word levels.", "startOffset": 4, "endOffset": 8}, {"referenceID": 3, "context": "02, 1]), the learning rate (in [5, 100]), the dropout keep probability (in [0.", "startOffset": 31, "endOffset": 39}, {"referenceID": 8, "context": "01, 10]), the number of epochs before learning rate decay (in [12, 198]), the learning rate decay (in [0.", "startOffset": 62, "endOffset": 71}, {"referenceID": 3, "context": "143, 30 epochs, cell clipping [5] between 1 and 10000.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Table 1 reports the results of criteria (a) (the baseline [7]); (c), (e) and (f) on all experimental cases, for \u03b4 = .", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "3 graphically displays the performance of the pruning criterion (f) compared to the baseline (a) [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "A first contribution of the presented work is to confirm the relevance of the pruning method proposed by [7], with computational savings often above 85% (particularly so for image applications), and applicable at both levels of randomized hyper-parameter optimization, and model selection (the model selection problem itself embedding a hyper-parameter selection problem).", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "05 in [7], we show that it can be increased up to 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "A first research perspective is to introduce diversity-based pruning for ensemble methods [15], taking inspiration from the clearing methods", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "in multi-modeal optimization [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "A second perspective will investigate whether the quantile-based proposed approach can make the inference simpler (as opposed to the Metropolis-Hasting method used in [7]).", "startOffset": 167, "endOffset": 170}], "year": 2017, "abstractText": "This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem \u2212 selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art [7] with no extra hyper-parameter.", "creator": "LaTeX with hyperref package"}}}