{"id": "1706.03304", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2017", "title": "Deep Optimization for Spectrum Repacking", "abstract": "Over 13 months in 2016-17 the FCC conducted an \"incentive auction\" to repurpose radio spectrum from broadcast television to wireless internet. In the end, the auction yielded $19.8 billion, $10.1 billion and $8.7 billion.\n\n\n\n\nBut in July 2016 the FCC issued an open letter to the FCC, asking that the company immediately cease to sell radio spectrum that was not broadcast on broadcast networks. The company's president, Chris C. Cox, said the FCC had received only 10 requests and not received any public notice. But the FCC's general counsel said that the FCC's decision on the matter was \"a critical development,\" saying that, given its actions, the company was in \"serious need of action.\"\nBut according to the FCC's report, the company did not respond to a request for comment. A spokesperson for the FCC confirmed that it was working with C. Cox. But Cox's spokesperson did not immediately respond to an email seeking comment.\nFCC Chairman Tom Wheeler announced earlier this month that he would replace the FCC's Communications Commissioner Thomas Wheeler. But the FCC's public comment period began on Sept. 16.\nThe FCC on Tuesday released a draft rules update on its net neutrality rules and its net neutrality rules for new regulations. They include a set of recommendations and a timeline to finalize them. The rules are scheduled to be posted to the FCC's Web site later this month.", "histories": [["v1", "Sun, 11 Jun 2017 03:15:20 GMT  (6892kb,D)", "http://arxiv.org/abs/1706.03304v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["neil newman", "alexandre fr\\'echette", "kevin leyton-brown"], "accepted": false, "id": "1706.03304"}, "pdf": {"name": "1706.03304.pdf", "metadata": {"source": "CRF", "title": "Deep Optimization for Spectrum Repacking", "authors": ["Neil Newman", "Alexandre Fr\u00e9chette", "Kevin Leyton-Brown"], "emails": ["newmanne@cs.ubc.ca", "afrechet@cs.ubc.ca", "kevinlb@cs.ubc.ca"], "sections": [{"heading": "1. INTRODUCTION", "text": "Many devices, including broadcast television receivers and\nThis paper builds in part on a conference publication by the same authors, \u201cSolving the Station Repacking Problem\u201d, which was published at AAAI 2016.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2017 ACM 0001-0782/08/0X00 ...$5.00.\ncell phones, rely on the transmission of electromagnetic signals. These signals can interfere with each other, so transmission is regulated: e.g., in the US, by the Federal Communications Commission (FCC). Since electromagnetic spectrum suitable for wireless transmission is a scarce resource and since it is difficult for a central authority to assess the relative merits of competing claims on it, since 1994 the FCC has used spectrum auctions to allocate broadcast rights (see, e.g., [27]). Many regulators around the world have followed suit. At this point, in the US (as in many other countries), most useful radio spectrum has been allocated. Interest has thus grown in the reallocation of radio spectrum from less to more valuable uses. Spectrum currently allocated to broadcast television has received particular attention, for two reasons. First, over-the-air television has been losing popularity with the rise of cable, satellite, and streaming services. Second, the upper UHF frequencies used by TV broadcasters are particularly well suited to wireless data transmission on mobile devices\u2014for which demand is growing rapidly\u2014as they can penetrate walls and travel long distances [23].\nIt thus made sense for at least some broadcasters to sell their licenses to wireless internet providers willing to pay for them. Ideally, these trades would have occurred bilaterally and without government involvement, as occurs in many other markets. However, two key obstacles made such trade unlikely to produce useful, large-scale spectrum reallocation, both stemming from the fact that wireless internet services require large, contiguous blocks of spectrum to work efficiently. First, a buyer\u2019s decision about which block of spectrum to buy would limit the buyer to trading only with broadcasters holding licenses to parts of that block; it could be hard or impossible to find such a block in which all broadcasters were willing to trade. Second, each of these broadcasters would have \u201choldout power\u201d, meaning the broadcaster could demand an exorbitant payment in exchange for allowing the deal to proceed. The likely result would have been very little trade, even if broadcasters valued the spectrum much less than potential buyers.\nA 2012 Act of Congress implemented a clever solution to this problem. It guaranteed each broadcaster interferencefree coverage in its broadcast area on some channel, but not necessarily on its currently used channel. This meant that if a broadcaster was unwilling to sell its license it could instead be moved to another channel, solving the holdout problem. To free up the channel that would permit this move to take place, broadcast rights could be bought from another station in the appropriate geographical area, even if this second station did not hold a license for spectrum due for\nar X\niv :1\n70 6.\n03 30\n4v 1\n[ cs\n.A I]\n1 1\nJu n\n20 17\nreallocation. In what follows, we call such an interference-free reassignment of channels to stations a feasible repacking.\nThese trades and channel reassignments were coordinated via a novel spectrum auction run by the FCC between March 2016 and April 2017, dubbed the Incentive Auction. It consisted of two interrelated parts. The first was a forward auction that sold large blocks of upper UHF spectrum to interested buyers in a manner similar to previous auctions of unallocated spectrum. The key innovation was the second part: a reverse auction that was specially designed to perform well in the Incentive Auction [28, 25]. It identified both a set of broadcasters who would voluntarily give up their broadcast rights and prices at which they would be compensated, simultaneously ensuring that all remaining broadcasters could feasibly be repacked in the unsold spectrum. The choice of how much spectrum to reallocate, called the clearing target, linked these two parts: the incentive auction alternated between reverse and forward auction stages with progressively shrinking clearing targets until revenue generated by the forward auction covered the cost of purchasing and reassigning stations in the reverse auction.\nWe now describe the reverse auction\u2019s rules in more detail. First, all participating stations are given initial price quotes and respond either that they agree to sell their broadcast rights at the quoted price or that they \u201cexit the auction\u201d (decline to participate), meaning that they will be guaranteed some interference-free channel. The auction then repeatedly iterates over the active bidders. Every time a bidder i is considered, the software first checks whether i can be feasibly repacked along with all exited stations. If such a feasible repacking exists, i is given a (geometrically) lower price quote and again has the options of accepting or exiting. Otherwise, i is frozen: its price stops descending and it is no longer active. The auction ends when all bidders are either frozen, exited, or receive price quotes of zero.\nThe problem of checking the feasibility of repackings is central to the reverse auction, likely to arise tens of thousands of times in a single auction. Unfortunately, this problem is NP-complete, generalizing graph coloring. The silver lining is that interference constraints were known in advance\u2014they were derived based on the locations and broadcast powers of existing television antennas\u2014and so it was reasonable to hope for a heuristic algorithm that achieved good performance on the sorts of problems that would arise in a real auction. However, identifying an algorithm that would be fast and reliable enough to use in practice remained challenging. Since each feasibility check depends on the results of those that came before\u2014if a station is found to be frozen, it cannot exit\u2014these problems must be solved sequentially. Time constraints for the auction as a whole required that the auction iterate through the stations at least twice a day, which worked out to a time cutoff on the order of minutes. It was thus inevitable that some problems would remain unsolved. Luckily, the auction design is robust to such failures, treating them as proofs of infeasibility at the expense of raising the cost required to clear spectrum.\nThis paper describes our experience building SATFC 2.3.1, the feasibility checker used in the reverse auction. We leveraged automatic algorithm configuration approaches to derive a portfolio of complementary algorithms that differ in their underlying (local and complete) search strategies, SAT encodings, constraint graph decompositions, domain-specific heuristics, and use of a novel caching scheme. We use the\nterm \u201cdeep optimization\u201d to refer to this approach,1 with the goal of emphasizing its conceptual similarity to deep learning. Classical machine learning relied on features crafted based on expert insight, model families selected manually, and model hyperparameters tuned essentially by hand. Deep learning has shown that it is often possible to achieve substantially better performance by relying less on expert knowledge and more on enormous amounts of computation and huge training sets. Specifically, deep learning considers parametric models of very high dimension, using expert knowledge only to identify appropriate invariances and model biases, such as convolutional structure. (In some cases it is critical that these models be \u201cdeep\u201d in the sense of having long chains of dependencies between parameters, but in other cases great flexibility can be achieved even with models only a couple of levels deep; e.g., [34].) We argue that a similar dynamic applies in the case of heuristic algorithms for discrete optimization, which aim to achieve good performance on some given dataset rather than in the worst case. Traditionally, experts have designed such heuristic algorithms by hand, iteratively conducting small experiments to refine their designs. We advocate an approach in which a computationally intensive procedure is used to search a high-dimensional space of parameterized algorithm designs to optimize performance over a large set of training data. We aim to minimize the role played by expert knowledge, restricting it to the identification of parameters that could potentially lead to fruitful algorithm designs. We also encourage deep dependencies via chains of parameters each of whose meaning depends on the value taken by one or more parents.\nOverall, this paper demonstrates the value of the deep optimization approach via the enormous performance gains it yielded on the challenging and socially important problem of spectrum repacking. After formally stating the station repacking problem, we define our large algorithm design space and the search techniques we used. We assess the results on problems that arose in runs of our new open-source reverse auction simulator, investigating both our solver\u2019s runtime and its impact on economic outcomes."}, {"heading": "2. THE STATION REPACKING PROBLEM", "text": "We now describe the station repacking problem in more detail.2 Each television station in the US and Canada s \u2208 S is 1There exists a large body of prior work that investigates the use of algorithm configuration to design novel algorithms from large, parameterized spaces (some of which, indeed, we have coauthored); we believe, however, that the work described in this paper is the most consequential application of such techniques to date. Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology. Most work in the latter vein either addresses the much broader problem of algorithm synthesis (e.g., [29, 5, 31]) or defines the overall approach only implicitly (e.g., [22]). The most prominent exception is \u201cprogramming by optimization\u201d [14]; however, it emphasizes connections to software engineering and does not limit itself to parametric design spaces. 2 Similar problems have been studied in other contexts, falling under the umbrella of frequency assignment problems. See e.g., [1] for a survey and a discussion of applications to mobile telephony, radio and TV broadcasting, satellite communication, wireless LANs, and military operations. We are not aware of other published work that aims to optimize feasibility checking in the Incentive Auction setting.\ncurrently assigned to a channel cs \u2208 C \u2286 N that ensures that it will not excessively interfere with other, nearby stations. (Although Canadian stations did not participate in the auction, they were eligible to be reassigned new channels.) The FCC determined pairs of channel assignments that would cause harmful interference based on a complex, grid-based physical simulation (\u201cOET-69\u201d [7]); this pairwise constraint data is publicly available [9]. Let I \u2286 (S \u00d7 C)2 denote a set of forbidden station\u2013channel pairs {(s, c), (s\u2032, c\u2032)}, each representing the proposition that stations s and s\u2032 may not concurrently be assigned to channels c and c\u2032, respectively. The effect of the auction was to remove some broadcasters from the airwaves and to reassign channels to the remaining stations from a reduced set. This reduced set was defined by a clearing target, fixed for each stage of the reverse auction, corresponding to some channel c \u2208 C such that all stations are only eligible to be assigned channels from C = {c \u2208 C | c < c}. Each station can only be assigned a channel on a subset of C, given by a domain function D : S \u2192 2C that maps from stations to these reduced sets. The station repacking problem is then the task of finding a repacking \u03b3 : S \u2192 C that assigns each station a channel from its domain that satisfies the interference constraints: i.e., for which \u03b3(s) \u2208 D(s) for all s \u2208 S, and \u03b3(s) = c\u21d2 \u03b3(s\u2032) 6= c\u2032 for all {(s, c), (s\u2032, c\u2032)} \u2208 I. A problem instance thus corresponds to a set of stations S \u2286 S and channels C \u2286 C into which they must be packed, with domains D and constraints I implicitly being restricted to S and C; we call the resulting restrictions D and I.\nWhy should we hope that this (NP-complete) problem can be solved effectively in practice? First, we only need to be concerned with problems involving subsets of a fixed set of stations and a fixed set of interference constraints: those describing the television stations currently broadcasting in the United States and Canada. Let us define the interference graph as an undirected graph in which there is one vertex per station and an edge exists between two vertices s and s\u2032 if the corresponding stations participate together in any interference constraint: i.e., if there exist c, c\u2032 \u2208 C such that {(s, c), (s\u2032, c\u2032)} \u2208 I. Figure 1 shows the Incentive Auction interference graph. As it turns out, interference constraints come in two kinds. Co-channel constraints specify that two stations may not be assigned to the same channel; adjacent-channel constraints specify that two stations may not be assigned to two nearby channels. Hence, any forbidden station\u2013channel pairs are of the form {(s, c), (s\u2032, c+ i)} for\nsome stations s, s\u2032 \u2208 S, channel c \u2208 C, and i \u2208 {0, 1, 2}. Furthermore, channels can be partitioned into three equivalence classes: LVHF (channels 1\u20136), HVHF (channels 7\u201313), and UHF (channels 14\u201351) with the property that no interference constraint involves channels in more than one band.\nSecond, note that we are not interested in optimizing worstcase performance even given our fixed interference graph, but rather in achieving good performance on the sort of instances generated by actual reverse auctions. These instances depend on the order in which stations exit the auction, which depends on stations\u2019 valuations, which depend in turn (among many other factors) on the size and character of the population reached by their broadcasts. The distribution over repacking problems is hence far from uniform.\nThird, descending clock auctions repeatedly generate station repacking problems by adding a single station s+ to a set S\u2212 of provably repackable stations. This means that every station repacking problem (S\u2212 \u222a {s+}, C) comes with a partial assignment \u03b3\u2212 : S\u2212 \u2192 C that we know is feasible on restricted station set S\u2212; we will see in what follows that this fact is extremely useful.\nFinally, many repacking problems are trivial: in our experience, problems involving only VHF channels can all be solved quickly; furthermore, the vast majority of UHF problems can be solved greedily simply by checking whether s+ can be augmented directly with \u03b3\u2212. However, solving the remaining problems is crucial to the economic outcomes achieved by the auction (as we show in Section 6). In what follows, we restrict ourselves to \u201cnon-trivial\u201d UHF problems that cannot be solved by greedy feasibility checking."}, {"heading": "3. A DEEP OPTIMIZATION APPROACH", "text": "As we show in our experiments (see Section 5), off-theshelf solvers could not solve a large enough fraction of station repacking problems to be effective in practice. To do better, we needed a customized algorithm optimized to perform well on our particular distribution of station repacking problems. We built our algorithm via the deep optimization approach, meaning that we aimed to use our own insight only to identify design ideas that showed promise, relegating the work of combining these ideas and evaluating the performance of the resulting algorithm on realistic data (see Section 4) to an automatic search procedure."}, {"heading": "3.1 The Design Space", "text": "Our first task was thus to identify a space of algorithm designs to consider. This was not just a pen-and-paper exercise, since each point in the space needed to correspond to runnable code. We focused on encoding station repacking as a propositional satisfiability (SAT) problem. The SAT formalism is well suited to station repacking, which is a pure feasibility problem with only combinatorial constraints. (It may also be possible to achieve good performance with MIP or other encodings; we did not investigate such alternatives in depth.) The SAT reduction is straightforward: given a station repacking problem (S,C) with domains D and interference constraints I, we create a Boolean variable xs,c \u2208 {>,\u22a5} for every station\u2013channel pair (s, c) \u2208 S \u00d7 C, representing the proposition that station s is assigned to channel c. We then create three kinds of clauses: (1) \u2228 d\u2208D(s) xs,d \u2200s \u2208 S (each station is assigned at least one channel); (2) \u00acxs,c \u2228 \u00acxs,c\u2032 \u2200s \u2208 S, \u2200c, c\u2032 6= c \u2208 D(s) (each station is assigned at most one channel); (3)\n\u00acxs,c \u2228 \u00acxs\u2032,c\u2032 \u2200{(s, c), (s\u2032, c\u2032)} \u2208 I (interference constraints are respected). Note that (2) is optional: if a station is assigned more than one channel, we can simply pick one channel to assign it from among these channels arbitrarily. We thus created a parameter indicating whether to include these constraints. In the end, a SAT encoding of a problem involving all stations at a clearing target of 36 involved 73 187 variables and 2 917 866 clauses.\n3.1.1 Selecting Solvers Perhaps the most important top-level parameter deter-\nmines which SAT solver to run. (Of course, each such solver will have its own (deep) parameter space; other parameters will describe design dimensions orthogonal to the choice of solver, as we will discuss in what follows.) The SAT community has developed a very wide variety of solvers and made them publicly available (see e.g., [19]). In principle, we would have made it possible to choose every solver that offered even reasonable performance. However, doing so would have been too costly from the perspective of software integration and (especially) reliability testing. We thus conducted initial algorithm configuration experiments (see Section 3.2) on 20 state-of-the-art SAT solvers, drawn mainly from SAT solver competition entries collected in AClib [18]. We illustrate the performance of their default configurations later in Figure 2; most improved at least somewhat from their default configurations as a result of algorithm configuration. We identified two solvers that ended up with the strongest postconfiguration performance\u2014one complete and one based on local search\u2014both of which have been shown in the literature to adapt well to a wide range of SAT domains via large and flexible parameter spaces. Our first solver was clasp [12], an open-source solver based on conflict-driven nogood learning (98 parameters). Our second was the open-source SATenstein framework [22], which allows arbitrary composition of design elements taken from a wide range of high-performance stochastic local search solvers (90 parameters).\n3.1.2 Using the Previous Solution While adapting clasp and SATenstein to station repacking\ndata yielded substantial performance improvements, neither reached a point sufficient for deployment in the real auction. To do better, it was necessary to leverage specific properties of the incentive auction problem. Rather than committing to specific speedups, we exposed a wide variety of possibilities via further parameters. We began by considering two methods for taking advantage of the existence of a partial assignment \u03b3\u2212. The first method checks whether a simple transformation of \u03b3\u2212 is enough to yield a satisfiable repacking. Specifically, we construct a small SAT problem in which the stations to be repacked are s+ and all stations \u0393(s+) \u2286 S neighboring s+ in the interference graph, fixing all other stations S \\ \u0393(s+) to their assignments in \u03b3\u2212. Any solution to this reduced problem must be a feasible repacking; however, if the reduced problem is infeasible we cannot conclude anything. However (depending on the value of a parameter), we can keep searching: unfixing all stations that neighbor a station in \u0393(s+), and so on.\nOur second method uses \u03b3\u2212 to initialize local search solvers. Such solvers search a space of complete variable assignments, typically following gradients to minimize an objective function such as the number of violated constraints, with occasional random steps. They are thus sensitive to their starting\npoints. Optionally, we can start at the assignment given by \u03b3\u2212 (randomly initializing variables pertaining to s+). We can also optionally redo this initialization on some fraction of random restarts.\n3.1.3 Problem Simplification Next, we considered three preprocessing techniques that\ncan simplify station repacking problems. First, we added the option to run the arc consistency algorithm, repeatedly pruning values from each station\u2019s domain that are incompatible with every channel on a neighboring station\u2019s domain.\nSecond, we enabled elimination of unconstrained stations. A station s is unconstrained if, given any feasible assignment of all of the other stations in S \\ s, there always exists some way of feasibly repacking s. Unconstrained stations can be removed without changing a problem\u2019s satisfiability status. Various algorithms exist for identifying unconstrained stations; we determine this choice via a parameter. (All such stations can be found via a reduction to the polytime problem of eliminating variables in a binary CSP [3]; various sound but incomplete heuristics run more quickly but identify progressively fewer unconstrained stations.)\nThird, the interference graph induced by a problem may consist of multiple connected components; we can optionally run a linear-time procedure to separate them into distinct SAT problems. We only need to solve the component to which s+ belongs: \u03b3\u2212 supplies feasible assignments for all others. Arc consistency and unconstrained station removal can simplify the interference graph by removing edges and nodes respectively. This can shrink the size of the component containing s+ and make this technique even more effective.\n3.1.4 Containment Caching Finally, we know that every repacking problem will be\nderived from a restriction of the interference graph to some subset of S. We know this graph in advance of the auction; this suggests the possibility of doing offline work to precompute solutions. However, our graph has 2 990 nodes, and the number of restricted graphs is thus 22990 \u2248 10900. Thus, it is not possible to consider all of them offline.\nNot every restricted problem is equally likely to arise in practice. To target likely problems, we could simply run a large number of simulations and cache the solution to every repacking problem encountered. Unfortunately, we found that it was extremely rare for problems to repeat across sufficiently different simulator inputs, even after running hundreds of simulations (generating millions of instances and costing years of CPU time). However, we can do better than simply looking for previous solutions to a given repacking problem. If we know that S is repackable then we know the same is true for every S\u2032 \u2286 S (and indeed, we know the packing itself\u2014the packing for S restricted to the stations in S\u2032). Similarly, if we know that S was not packable then we know the same for every S\u2032 \u2287 S. This observation dramatically magnifies the usefulness of each cached entry S, because each S can be used to answer queries about an exponential number of subsets or supersets. This is especially useful because sometimes it can be harder to find a repacking for subsets of S than it can be to find a repacking for S.\nWe call a cache meant to be used in this way a containment cache, because it is queried to determine whether one set contains another (i.e., whether the query contains the cache item or vice versa). To the best of our knowledge, contain-\nment caching is a novel idea. A likely reason why this scheme is not already common is that querying a containment cache is nontrivial: one cannot simply index entries with a hash function; instead, an exponential number of keys can match a given query. We were nevertheless able to construct an algorithm that solved this problem quickly in our setting. We observe that containment caching is applicable to any family of feasibility testing problems generated as subsets of a master set of constraints, not just to spectrum repacking.\nIn more detail, we maintain two caches, a feasible cache and an infeasible cache, and store each problem we solve in the appropriate cache. We leverage the methods from Section 3.1.3 to enhance the efficiency of our cache, storing full instances for SAT problems and the smallest simplified component for UNSAT problems. When asked whether it is possible to repack station set S, we first check whether a subset of S belongs to the infeasible cache (in which case the original problem is infeasible); if we find no matches, we decompose the problem into its smallest simplified component and check if the feasible cache contains a superset of those stations, in which case the original problem is feasible."}, {"heading": "3.2 Searching the Design Space", "text": "Overall, our design space had 191 parameters, nested as much as 4 levels deep. We now describe how we searched this space to building a customized solver. Identifying a set of parameters that optimize a given algorithm\u2019s performance on a given dataset is called algorithm configuration. There exist a wide variety of algorithm configuration tools [16, 26, 15, 2]. We used Sequential Model-based Algorithm Configuration (SMAC) [15], the publicly available method that arguably achieves the best performance (see e.g., [17]). SMAC uses the \u201cBayesian optimization\u201d approach of interleaving random sampling and the exploration of algorithm designs that appear promising based on a learned model.\nUnfortunately, even after performing algorithm configuration, it is rare to find a single algorithm that outperforms all others on instances of an NP-complete problem such as SAT. This inherent variability across solvers can be exploited by algorithm portfolios [13, 30, 33]. Most straightforwardly, one selects a small set of algorithms with complementary performance on problems of interest and, when asked to solve a new instance, executes them in parallel. Of course, we wanted to construct such algorithm portfolios automatically as part of our deep optimization approach. We did this by using a method called Hydra [32] which runs iteratively, at each step directing the algorithm configurator to optimize marginal gains over the given portfolio. This allows Hydra to find algorithms that may perform poorly overall but that complement the existing portfolio. Overall, we ran Hydra for eight steps, thereby producing a portfolio of novel solvers (dubbed SATFC) that could run on a standard eight-core workstation. The Incentive Auction used SATFC 2.3.1, which is available online at https://github.com/FCC/SATFC."}, {"heading": "4. DATA FROM AUCTION SIMULATIONS", "text": "During the development of SATFC [11] the FCC shared with us a wide range of anonymized problem instances that arose in auction simulations they performed in order to validate the auction design. These formed the \u201ctraining set\u201d we used in the deep optimization process when constructing SATFC 2.3.1. These simulations explored a very narrow set of answers to the questions of which stations would participate\nand how bidders would interact with the auction mechanism; they do not represent a statement either by us or by the FCC about how these questions were resolved in the real auction (indeed, by law the answers will not be revealed to us or to the public for two years). It is of course impossible to guarantee that variations in the assumptions would not have yielded computationally different problems.\nWhile SATFC 2.3.1 is itself open-source software, it is unfortunately impossible for us to share the data that was used to build it. In this paper, we have opted for what we hope is the next best thing: evaluating SATFC 2.3.1 and various alternatives using a publicly available test set. We thus wrote our own reverse auction simulator and released it as open source software (see http://cs.ubc.ca/labs/beta/ Projects/SATFC). We used this simulator to simulate 20 auctions, in each case randomly sampling bidder valuations from a publicly available model [6] using parameters obtained directly from its authors. This model specifies stations\u2019 values for broadcasting in UHF, vs,UHF. Of course, a station has no value for going off air: vs,OFF = 0. In some cases the reverse auction can reassign a UHF station to a channel in one of two less valuable VHF bands (LVHF, HVHF) in exchange for lesser compensation. We assume that vs,HVHF =\n2 3 vs,UHF\nand vs,LVHF = 1 3 vs,UHF. We excluded from our simulator all stations for which the authors of the model were unable to supply us with parameters: stations outside the mainland US and Hawaii, all US VHF stations, and an additional 25 US UHF stations. This left us with 1 638 eligible US stations. We further included all Canadian stations in our simulations: because the auction rules forbade them from being paid to leave their home bands, we did not need to model their valuations. Specifically, from Canada we included 113 LVHF stations, 332 HVHF stations, and 348 UHF stations.\nWe set the auction\u2019s opening prices to the values announced by the FCC in November 2015 [10]. We assumed that stations chose to participate in the reverse auction if their opening price offer for going off air was greater than their valuation for remaining on air in their current band. We assumed that stations always selected the option that myopically maximized their utility. We used the interference constraints and station domains announced by the FCC in November 2015 [9]. For each simulation, we used the largest clearing target for which we could find a feasible assignment for the non-participating stations; in all cases this led to a clearing target of 84 Mhz, corresponding to a maximum allowable channel of 36. We note that this is the amount of spectrum actually cleared by the Incentive Auction. Just like the real auction, an auction simulator needs a feasibility checker to determine which price movements are possible. We used SATFC 2.3.1 with a cutoff of 60 seconds. We sampled 10 000 \u201cnontrivial\u201d UHF problems uniformly at random from all of the problems across all simulations to use as our dataset, where we defined nontrivial problems as those that could not be solved by greedily augmenting the previous solution. Fewer than 3% of UHF problems in our simulations were nontrivial. This test set consisted of 9 482 feasible problems, 121 infeasible problems, and 397 problems that timed out at our one minute cutoff and therefore have unknown feasibility."}, {"heading": "5. RUNTIME PERFORMANCE", "text": "We now evaluate SATFC\u2019s performance by contrasting it with various off-the-shelf alternatives. The FCC\u2019s initial investigations included modeling the station repack-\ning problem as a mixed-integer program (MIP) and using off-the-shelf solvers paired with problem-specific speedups [8]. Unfortunately, the problem-specific elements of this solution were not publicly released, so we cannot evaluate them in this article. Instead, to assess the feasibility of a MIP approach, we ran what are arguably the two best-performing MIP solvers\u2014CPLEX and Gurobi\u2014on our test set of 10 000 non-trivial instances. To encode the station repacking problem as a MIP, we created a variable xs,c \u2208 {0, 1} for every station\u2013channel pair, representing the proposition that station s is assigned to channel c. We imposed the constraints \u2211 c\u2208D(s) xs,c = 1 \u2200 s \u2208 S and xs,c + xs\u2032,c\u2032 \u2264 1 \u2200 {(s, c), (s\u2032, c\u2032)} \u2208 I, ensuring that each station is assigned to exactly one channel and that interference constraints are not violated. Both MIP solvers solved under half of the instances within our cutoff time of one minute; the results are shown in Figure 2. Such performance would likely have been insufficient for deployment in practice, since it implies unnecessarily high payments to many stations.\nAs already discussed, there exist a wide variety of SAT solvers that are available for use off the shelf. Figure 2 illustrates the performance of the 20 state-of-the-art solvers we considered in our initial configuration experiments in their default configurations. With few exceptions, the SAT solvers outperformed the MIP solvers, as can be seen by comparing the solid and dashed lines in Figure 2. However, runtimes and percentages of instances solved by the cutoff time were still not good enough for us to recommend deployment of any of these solvers in the actual auction. The best solver in its default configuration, Gnovelty+PCL, was able to solve\nthe largest number of problems\u201479.96%\u2014within the cutoff. (As mentioned earlier, the SATenstein design space includes Gnovelty+PCL alongside many other solvers.) The parallel portfolio of all 20 solvers from Figure 2 was little better, being able to solve only 81.58% of problems.\nWe now turn to SATFC 2.3.1. This 8-solver parallel portfolio stochastically dominated every individual solver that we considered and achieved very substantial gains after a few tenths of a second. It solved 87.73% of the problems in under a second and 96.03% within the one-minute cutoff time. The histogram at the bottom of the figure indicates satisfiability status of instances solved by SATFC grouped by runtime; our instances were overwhelmingly satisfiable."}, {"heading": "6. IMPACT ON ECONOMIC OUTCOMES", "text": "We now ask whether SATFC\u2019s improved performance is likely to have translated into a better economic outcome in the Incentive Auction, assessing both cost and efficiency. The cost of an auction is the sum of payments to the winning stations. To assess efficiency we measured the total value lost by the auction, comparing the sums of values of stations for their allocated bands both before and after the auction.3 If we can find an efficient repacking \u03b3\u2217, then we can compute the additional fraction of value lost by some other repacking\n\u03b3. We call this the value loss ratio: \u2211 s\u2208S vs,pre(s)\u2212vs,post(\u03b3,s)\u2211 s\u2208S vs,pre(s)\u2212vs,post(\u03b3\u2217,s) , where pre(s) returns the band to which s was assigned before the auction and post(\u03b3, s) returns either the band to which s is assigned under channel assignment \u03b3 or OFF if s is not assigned to a band under \u03b3. When it is intractable to compute \u03b3\u2217, we resort to comparing the absolute value loss between different assignments.\nGiven our interest in the efficiency of the reverse auction, it is natural to compare it to the VCG mechanism, which always chooses the optimal packing \u03b3\u2217. VCG pays losing stations nothing and pays each winning station s the difference between the sum of values of stations other than s for \u03b3\u2217 and the sum of the same stations\u2019 values for a packing that is optimal subject to the constraint that s does not win. We identified these optimal packings using the MIP encoding from Section 5 with two changes. First, we added the objective of maximizing the aggregate values of the participating stations: maximize \u2211 s\u2208Sbidding \u2211 c\u2208D(s) xs,c \u00b7 vs,band(c), where band (c) is a function that returns the corresponding band for a given channel. Second, we allowed the option of not assigning a channel to a bidding station."}, {"heading": "6.1 Greater New York City Simulations", "text": "Unfortunately, it was impossible to solve these optimization problems at a national scale, even given several days of computing time. We therefore constructed tractable problems by restricting ourselves to stations in the vicinity of New York City, which we chose because it corresponds to one of the most densely connected regions in the interference graph. More specifically, we dropped all Canadian stations\n3 The more standard measure of efficiency\u2014the sum of stations\u2019 values for their allocated bands\u2014has the same optimum. Value loss has the advantage that it is influenced only by stations that a feasibility checker is unable to repack in their home bands; it is thus more appropriate for comparing feasibility checkers. The more standard measure is sensitive to changes in the values of easy-to-repack stations, even those that do not participate in any interference constraints.\nand restricted ourselves to the UHF band using the smallest possible clearing target (maximum allowable channel of 29). Using the interference graph induced by these restrictions, we then dropped every station whose shortest path length to a station in New York City exceeded two. The result was a setting with 218 stations and 78 499 channel-specific interference constraints, yielding a MIP encoding with 2 465 variables and 78 717 constraints.\nWe randomly generated 20 different valuation profiles, using the methodology described in Section 4 but restricting ourselves to the stations in the restricted interference graph. For each valuation profile, we conducted four simulations. The first was of a VCG auction; we computed allocations and payments using CPLEX, solving all MIPs optimally to within 10\u22126 absolute MIP gap tolerance. We also ran three reverse auction simulations for each valuation profile, varying the feasibility checker to consider two alternatives to SATFC. The first is the greedy feasibility checker, which represents the simplest reasonable feasibility checker and thus serves as a baseline. The second is the default configuration of picoSAT. To our knowledge, alongside MIP approaches this is the only other solver that has been used in publications on the Incentive Auction [21, 4], probably because we showed it to be the best among a set of alternatives in an early talk on the subject [24].\nIn total, our 80 simulations consumed over 5 years of CPU time (dominated by the VCG simulations). Figure 3 (left) illustrates the results. Each point shows the value loss (x axis) and cost (y axis) of a single simulation; in both cases, these quantities are normalized by the corresponding quantity achieved by VCG for the same valuation profile. The SATFC simulations had a mean value loss ratio of 1.048 and a mean cost ratio of 0.760, indicating that the reverse auction achieved nearly optimal efficiency at much lower cost than VCG. The picoSAT results were nearly identical, differing in only two value profiles, and then only slightly. Both the SATFC and picoSAT runs dominated the greedy runs according to both metrics; on average, reverse auctions based on greedy cost 1.742 times more and lost 1.366 times as much broadcaster value than those based on SATFC. Despite these differences, all of the solvers were able to solve a very large fraction of the feasibility checking problems encountered in their respective simulations (which took different trajectories\nonce two solvers differed in their ability to solve a given problem): 99.978%, 99.945%, and 99.118% for SATFC, picoSAT, and greedy respectively (including trivial problems)."}, {"heading": "6.2 National Simulations", "text": "We were more interested in economic outcomes at the national scale, even though we could not simulate VCG in such a large setting. We generated 20 valuation profiles for our full set of stations and ran reverse auction simulations using our three feasibility checkers. In total, these experiments consumed over 5 days of CPU time.\nAll solvers were again able to solve a large fraction of the problems they encountered: 99.902%, 99.765%, and 98.031% for SATFC, picoSAT, and greedy respectively (including trivial problems). The economic impact of these differences is illustrated in Figure 3 (right). In this graph, x- and y-axis values correspond to unnormalized value loss and cost respectively. Each SATFC and picoSAT simulation again dominated its greedy counterpart in both efficiency and cost. Averaging over all of our observations, reverse auctions based on greedy feasibility checking cost 3.550 times ($5.114 billion) more and lost 2.850 times as much ($2.030 billion) broadcaster value than those based on SATFC. At the larger scale we also found that SATFC dominated picoSAT in every simulation: on average, picoSAT auctions cost 1.495 times ($987 million) more and lost 1.427 times as much ($469 million) value than SATFC auctions."}, {"heading": "7. CONCLUSIONS", "text": "Station repacking in the Incentive Auction is a difficult but important problem, with progress translating into significant gains in both government expenditures and social welfare. We designed a customized solution to this problem using an approach we dub deep optimization. Specifically, we drew on a large parameterized design space to construct a strong portfolio of heuristic algorithms: SATFC 2.3.1, an open-source solver that was used in the real auction. To evaluate it for this paper, we conducted experiments with a new reverse auction simulator. We found that replacing SATFC with an off-the-shelf feasibility checker resulted in both efficiency losses and increased costs. It thus appears likely that our efforts led to significant economic benefits to broadcasters, the US government, and the American public."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "We gratefully acknowledge support from Auctionomics and the FCC; valuable conversations with Paul Milgrom, Ilya Segal, and James Wright; help from Peter West in conducting some of the experiments reported in Section 6; contributions (mostly in the form of code) from past research assistants Nick Arnosti, Guillaume Saulnier-Comte, Ricky Chen, Alim Virani, Chris Cameron, Emily Chen, Paul Cernek; experimental infrastructure assistance from Steve Ramage; and help gathering data from Ulrich Gall, Rory Molinari, Karla Hoffman, Brett Tarnutzer, Sasha Javid, and others at the FCC. This work was funded by Auctionomics and by NSERC via a Discovery Grant and an E.W.R. Steacie Fellowship; it was conducted in part at the Simons Institute for Theoretical Computer Science at UC Berkeley."}, {"heading": "9. REFERENCES", "text": "[1] K. I. Aardal, S. P. Van Hoesel, A. M. Koster, C. Mannino,\nand A. Sassano. Models and solution techniques for frequency assignment problems. Annals of Operations Research, 153(1):79\u2013129, 2007.\n[2] C. Anso\u0301tegui, M. Sellmann, and K. Tierney. A gender-based genetic algorithm for the automatic configuration of algorithms. In Conference on Principles and Practice of Constraint Programming (CP), pages 142\u2013157, 2009. [3] D. A. Cohen, M. C. Cooper, G. Escamocher, and S. Z\u030civny\u0300. Variable and value elimination in binary constraint satisfaction via forbidden patterns. Journal of Computer and System Sciences, 81(7):1127\u20131143, 2015. [4] P. Cramton, H. Lopez, D. Malec, and P. Sujarittanonta. Design of the reverse auction in the broadcast incentive auction. Working Paper, http://www.cramton.umd.edu/papers2015-2019/ cramton-reverse-auction-design-fcc-comment-pn.pdf, 2015. [5] L. Di Gaspero and A. Schaerf. Easysyn++: A tool for automatic synthesis of stochastic local search algorithms. In Conference on Engineering Stochastic Local Search Algorithms (SLS), pages 177\u2013181, 2007. [6] U. Doraszelski, K. Seim, M. Sinkinson, and P. Wang. Ownership concentration and strategic supply reduction. Working Paper, http://www.nber.org/papers/w23034 23034, National Bureau of Economic Research, January 2017. [7] FCC. Office of engineering and technology releases and seeks comment on updated OET-69 software. FCC Public Notice, DA 13-138, February 2013. [8] FCC. Information related to incentive auction repacking feasibility checker. FCC Public Notice, DA 14-3, 1 2014. [9] FCC. Repacking constraint files. http://data.fcc.gov/ download/incentive-auctions/Constraint Files/, 2015. Accessed 2015-11-20.\n[10] FCC. Reverse auction opening prices. http://wireless.fcc.gov/auctions/incentive-auctions/ Reverse Auction Opening Prices 111215.xlsx, 2015. Accessed 2015-11-15. [11] A. Fre\u0301chette, N. Newman, and K. Leyton-Brown. Solving the station repacking problem. In AAAI Conference on Artificial Intelligence, pages 702\u2013709, 2016. [12] M. Gebser, B. Kaufmann, A. Neumann, and T. Schaub. clasp: A conflict-driven answer set solver. In Logic Programming and Nonmonotonic Reasoning, pages 260\u2013265. 2007. [13] C. P. Gomes and B. Selman. Algorithm portfolios. Artificial Intelligence, 126(1):43\u201362, 2001. [14] H. H. Hoos. Programming by optimization. Communications of the ACM, 55(2):70\u201380, Feb. 2012. [15] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization Conference (LION), pages 507\u2013523, 2011. [16] F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. Stu\u0308tzle.\nParamILS: an automatic algorithm configuration framework. Journal of Artificial Intelligence Research, 36(1):267\u2013306, 2009.\n[17] F. Hutter, M. Lindauer, S. Bayless, H. Hoos, and K. Leyton-Brown. Results of the configurable SAT solver challenge 2014. Technical report, University of Freiburg, Department of Computer Science, 2014. [18] F. Hutter, M. Lo\u0301pez-Iba\u0301n\u0303ez, C. Fawcett, M. Lindauer, H. H. Hoos, K. Leyton-Brown, and T. Stu\u0308tzle. AClib: A benchmark library for algorithm configuration. In Conference on Learning and Intelligent Optimization (LION), pages 36\u201340. Springer, 2014. [19] M. Ja\u0308rvisalo, D. Le Berre, O. Roussel, and L. Simon. The international SAT solver competitions. Artificial Intelligence Magazine, 33(1):89\u201392, 2012. [20] S. Kadioglu, Y. Malitsky, M. Sellmann, and K. Tierney. ISAC\u2013instance-specific algorithm configuration. In European Conference on Artificial Intelligence (ECAI), pages 751\u2013756, 2010. [21] M. Kearns and L. Dworkin. A computational study of feasible repackings in the FCC incentive auctions. CoRR, abs/1406.4837, 2014. [22] A. R. KhudaBukhsh, L. Xu, H. H. Hoos, and K. Leyton-Brown. SATenstein: Automatically building local search SAT solvers from components. Artificial Intelligence, 232:20\u201342, 2016. [23] R. Knutson and T. Gryta. Verizon, AT&T May Face Bidding Limits in Spectrum Auction. Wall Street Journal, Apr. 2014. http://www.wsj.com/articles/ SB10001424052702304626304579510154106120342, Accessed 2016-12-12. [24] K. Leyton-Brown. The viability of exact feasibility checking. Talk at the Stanford Institute for Economic Policy Research Conference on the design of the U.S. Incentive Auction for reallocating spectrum between wireless telecommunications and television broadcasting, February 2013. [25] S. Li. Obviously strategy-proof mechanisms. Available at SSRN 2560028, 2015. [26] M. Lo\u0301pez-Iba\u0301n\u0303ez, J. Dubois-Lacoste, L. Pe\u0301rez Ca\u0301ceres, T. Stu\u0308tzle, and M. Birattari. The irace package: Iterated racing for automatic algorithm configuration. Operations Research Perspectives, 3:43\u201358, 2016. [27] P. Milgrom. Putting Auction Theory to Work. Churchill Lectures in Economics. Cambridge University Press, 2004. [28] P. Milgrom and I. Segal. Deferred-acceptance auctions and radio spectrum reallocation. In ACM Conference on Economics and Computation (EC), pages 185\u2013186, 2014. [29] J.-N. Monette, Y. Deville, and P. Van Hentenryck. Aeon: Synthesizing Scheduling Algorithms from High-Level Models, pages 43\u201359. 2009. [30] E. Nudelman, K. Leyton-Brown, G. Andrew, C. Gomes, J. McFadden, B. Selman, and Y. Shoham. Satzilla 0.9. Solver description, International SAT Competition, 2003. [31] S. J. Westfold and D. R. Smith. Synthesis of efficient constraint-satisfaction programs. The Knowledge Engineering Review, 16(1):69\u201384, March 2001. [32] L. Xu, H. H. Hoos, and K. Leyton-Brown. Hydra: Automatically configuring algorithms for portfolio-based selection. In AAAI Conference on Artificial Intelligence, pages 210\u2013216, 2010. [33] L. Xu, F. Hutter, H. H. Hoos, and K. Leyton-Brown. SATzilla: portfolio-based algorithm selection for SAT. Journal of Artificial Intelligence Research, 32:565\u2013606, June 2008. [34] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016."}], "references": [{"title": "Models and solution techniques for frequency assignment problems", "author": ["K.I. Aardal", "S.P. Van Hoesel", "A.M. Koster", "C. Mannino", "A. Sassano"], "venue": "Annals of Operations Research, 153(1):79\u2013129", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A gender-based genetic algorithm for the automatic configuration of algorithms", "author": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "Conference on Principles and Practice of Constraint Programming (CP), pages 142\u2013157", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Variable and value elimination in binary constraint satisfaction via forbidden patterns", "author": ["D.A. Cohen", "M.C. Cooper", "G. Escamocher", "S. \u017divn\u1ef3"], "venue": "Journal of Computer and System Sciences, 81(7):1127\u20131143", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Design of the reverse auction in the broadcast incentive auction", "author": ["P. Cramton", "H. Lopez", "D. Malec", "P. Sujarittanonta"], "venue": "Working Paper, http://www.cramton.umd.edu/papers2015-2019/ cramton-reverse-auction-design-fcc-comment-pn.pdf", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Easysyn++: A tool for automatic synthesis of stochastic local search algorithms", "author": ["L. Di Gaspero", "A. Schaerf"], "venue": "Conference on Engineering Stochastic Local Search Algorithms (SLS), pages 177\u2013181", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Ownership concentration and strategic supply reduction", "author": ["U. Doraszelski", "K. Seim", "M. Sinkinson", "P. Wang"], "venue": "Working Paper,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Solving the station repacking problem", "author": ["A. Fr\u00e9chette", "N. Newman", "K. Leyton-Brown"], "venue": "AAAI Conference on Artificial Intelligence, pages 702\u2013709", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "clasp: A conflict-driven answer set solver", "author": ["M. Gebser", "B. Kaufmann", "A. Neumann", "T. Schaub"], "venue": "In Logic Programming and Nonmonotonic Reasoning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Algorithm portfolios", "author": ["C.P. Gomes", "B. Selman"], "venue": "Artificial Intelligence, 126(1):43\u201362", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Programming by optimization", "author": ["H.H. Hoos"], "venue": "Communications of the ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Learning and Intelligent Optimization Conference (LION), pages 507\u2013523", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research, 36(1):267\u2013306", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Results of the configurable SAT solver challenge 2014", "author": ["F. Hutter", "M. Lindauer", "S. Bayless", "H. Hoos", "K. Leyton-Brown"], "venue": "Technical report, University of Freiburg, Department of Computer Science", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "AClib: A benchmark library for algorithm configuration", "author": ["F. Hutter", "M. L\u00f3pez-Ib\u00e1\u00f1ez", "C. Fawcett", "M. Lindauer", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Conference on Learning and Intelligent Optimization (LION), pages 36\u201340. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The international SAT solver competitions", "author": ["M. J\u00e4rvisalo", "D. Le Berre", "O. Roussel", "L. Simon"], "venue": "Artificial Intelligence Magazine, 33(1):89\u201392", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "ISAC\u2013instance-specific algorithm configuration", "author": ["S. Kadioglu", "Y. Malitsky", "M. Sellmann", "K. Tierney"], "venue": "European Conference on Artificial Intelligence (ECAI), pages 751\u2013756", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A computational study of feasible repackings in the FCC incentive auctions", "author": ["M. Kearns", "L. Dworkin"], "venue": "CoRR, abs/1406.4837", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "SATenstein: Automatically building local search SAT solvers from components", "author": ["A.R. KhudaBukhsh", "L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Artificial Intelligence, 232:20\u201342", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Verizon, AT&T May Face Bidding Limits in Spectrum Auction", "author": ["R. Knutson", "T. Gryta"], "venue": "Wall Street Journal,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The viability of exact feasibility checking. Talk at the Stanford Institute for Economic Policy Research Conference on the design of the U.S. Incentive Auction for reallocating spectrum between wireless telecommunications and television", "author": ["K. Leyton-Brown"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Obviously strategy-proof mechanisms", "author": ["S. Li"], "venue": "Available at SSRN 2560028", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The irace package: Iterated racing for automatic algorithm configuration", "author": ["M. L\u00f3pez-Ib\u00e1\u00f1ez", "J. Dubois-Lacoste", "L. P\u00e9rez C\u00e1ceres", "T. St\u00fctzle", "M. Birattari"], "venue": "Operations Research Perspectives, 3:43\u201358", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Putting Auction Theory to Work", "author": ["P. Milgrom"], "venue": "Churchill Lectures in Economics. Cambridge University Press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Deferred-acceptance auctions and radio spectrum reallocation", "author": ["P. Milgrom", "I. Segal"], "venue": "ACM Conference on Economics and Computation (EC), pages 185\u2013186", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Aeon: Synthesizing Scheduling Algorithms from High-Level Models, pages 43\u201359", "author": ["J.-N. Monette", "Y. Deville", "P. Van Hentenryck"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "and Y", "author": ["E. Nudelman", "K. Leyton-Brown", "G. Andrew", "C. Gomes", "J. McFadden", "B. Selman"], "venue": "Shoham. Satzilla 0.9. Solver description, International SAT Competition", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Synthesis of efficient constraint-satisfaction programs", "author": ["S.J. Westfold", "D.R. Smith"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}, {"title": "Hydra: Automatically configuring algorithms for portfolio-based selection", "author": ["L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "AAAI Conference on Artificial Intelligence, pages 210\u2013216", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "SATzilla: portfolio-based algorithm selection for SAT", "author": ["L. Xu", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "CoRR, abs/1611.03530", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": ", [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "Second, the upper UHF frequencies used by TV broadcasters are particularly well suited to wireless data transmission on mobile devices\u2014for which demand is growing rapidly\u2014as they can penetrate walls and travel long distances [23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 23, "context": "The key innovation was the second part: a reverse auction that was specially designed to perform well in the Incentive Auction [28, 25].", "startOffset": 127, "endOffset": 135}, {"referenceID": 20, "context": "The key innovation was the second part: a reverse auction that was specially designed to perform well in the Incentive Auction [28, 25].", "startOffset": 127, "endOffset": 135}, {"referenceID": 29, "context": ", [34].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 21, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 10, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 1, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 27, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 15, "context": "Much of the literature just mentioned focuses on algorithm configuration tools [16, 26, 15, 2, 32, 20] (which we take as given in this paper) rather than algorithm design methodology.", "startOffset": 79, "endOffset": 102}, {"referenceID": 24, "context": ", [29, 5, 31]) or defines the overall approach only implicitly (e.", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": ", [29, 5, 31]) or defines the overall approach only implicitly (e.", "startOffset": 2, "endOffset": 13}, {"referenceID": 26, "context": ", [29, 5, 31]) or defines the overall approach only implicitly (e.", "startOffset": 2, "endOffset": 13}, {"referenceID": 17, "context": ", [22]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "The most prominent exception is \u201cprogramming by optimization\u201d [14]; however, it emphasizes connections to software engineering and does not limit itself to parametric design spaces.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": ", [1] for a survey and a discussion of applications to mobile telephony, radio and TV broadcasting, satellite communication, wireless LANs, and military operations.", "startOffset": 2, "endOffset": 5}, {"referenceID": 14, "context": ", [19]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "2) on 20 state-of-the-art SAT solvers, drawn mainly from SAT solver competition entries collected in AClib [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "Our first solver was clasp [12], an open-source solver based on conflict-driven nogood learning (98 parameters).", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "Our second was the open-source SATenstein framework [22], which allows arbitrary composition of design elements taken from a wide range of high-performance stochastic local search solvers (90 parameters).", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "(All such stations can be found via a reduction to the polytime problem of eliminating variables in a binary CSP [3]; various sound but incomplete heuristics run more quickly but identify", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "There exist a wide variety of algorithm configuration tools [16, 26, 15, 2].", "startOffset": 60, "endOffset": 75}, {"referenceID": 21, "context": "There exist a wide variety of algorithm configuration tools [16, 26, 15, 2].", "startOffset": 60, "endOffset": 75}, {"referenceID": 10, "context": "There exist a wide variety of algorithm configuration tools [16, 26, 15, 2].", "startOffset": 60, "endOffset": 75}, {"referenceID": 1, "context": "There exist a wide variety of algorithm configuration tools [16, 26, 15, 2].", "startOffset": 60, "endOffset": 75}, {"referenceID": 10, "context": "We used Sequential Model-based Algorithm Configuration (SMAC) [15], the publicly available method that arguably achieves the best performance (see e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": ", [17]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "This inherent variability across solvers can be exploited by algorithm portfolios [13, 30, 33].", "startOffset": 82, "endOffset": 94}, {"referenceID": 25, "context": "This inherent variability across solvers can be exploited by algorithm portfolios [13, 30, 33].", "startOffset": 82, "endOffset": 94}, {"referenceID": 28, "context": "This inherent variability across solvers can be exploited by algorithm portfolios [13, 30, 33].", "startOffset": 82, "endOffset": 94}, {"referenceID": 27, "context": "We did this by using a method called Hydra [32] which runs iteratively, at each step directing the algorithm configurator to optimize marginal gains over the given portfolio.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "During the development of SATFC [11] the FCC shared with us a wide range of anonymized problem instances that arose in auction simulations they performed in order to validate the auction design.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "We used this simulator to simulate 20 auctions, in each case randomly sampling bidder valuations from a publicly available model [6] using parameters obtained directly from its authors.", "startOffset": 129, "endOffset": 132}, {"referenceID": 16, "context": "To our knowledge, alongside MIP approaches this is the only other solver that has been used in publications on the Incentive Auction [21, 4], probably because we showed it to be the best among a set of alternatives in an early talk on the subject [24].", "startOffset": 133, "endOffset": 140}, {"referenceID": 3, "context": "To our knowledge, alongside MIP approaches this is the only other solver that has been used in publications on the Incentive Auction [21, 4], probably because we showed it to be the best among a set of alternatives in an early talk on the subject [24].", "startOffset": 133, "endOffset": 140}, {"referenceID": 19, "context": "To our knowledge, alongside MIP approaches this is the only other solver that has been used in publications on the Incentive Auction [21, 4], probably because we showed it to be the best among a set of alternatives in an early talk on the subject [24].", "startOffset": 247, "endOffset": 251}], "year": 2017, "abstractText": "Over 13 months in 2016\u201317 the US Federal Communications Commission conducted an \u201cincentive auction\u201d to repurpose radio spectrum from broadcast television to wireless internet. In the end, the auction yielded $19.8 billion, $10.05 billion of which was paid to 175 broadcasters for voluntarily relinquishing their licenses across 14 UHF channels. Stations that continued broadcasting were assigned potentially new channels to fit as densely as possible into the channels that remained. The government netted more than $7 billion (used to pay down the national debt) after covering costs. A crucial element of the auction design was the construction of a solver, dubbed SATFC, that determined whether sets of stations could be \u201crepacked\u201d in this way; it needed to run every time a station was given a price quote. This paper describes the process by which we built SATFC. We adopted an approach we dub \u201cdeep optimization\u201d, taking a data-driven, highly parametric, and computationally intensive approach to solver design. More specifically, to build SATFC we designed software that could pair both complete and local-search SAT-encoded feasibility checking with a wide range of domain-specific techniques, such as constraint graph decomposition and novel caching mechanisms that allow for reuse of partial solutions from related, solved problems. We then used automatic algorithm configuration techniques to construct a portfolio of eight complementary algorithms to be run in parallel, aiming to achieve good performance on instances that arose in proprietary auction simulations. To evaluate the impact of our solver in this paper, we built an open-source reverse auction simulator. We found that within the short time budget required in practice, SATFC solved more than 95% of the problems it encountered. Furthermore, the incentive auction paired with SATFC produced nearly optimal allocations in a restricted setting and substantially outperformed other alternatives at national scale.", "creator": "LaTeX with hyperref package"}}}