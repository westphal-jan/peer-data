{"id": "1706.03993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary input/output networks", "abstract": "Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., the 'composite' domains for each query), the data on each of these domains can be stored in a single file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 13 Jun 2017 10:50:25 GMT  (65kb,D)", "http://arxiv.org/abs/1706.03993v1", "Accepted for publication at ACM RecSys 2017; previous version submitted to ICLR 2016"]], "COMMENTS": "Accepted for publication at ACM RecSys 2017; previous version submitted to ICLR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR cs.NE", "authors": ["joan serr\\`a", "alexandros karatzoglou"], "accepted": false, "id": "1706.03993"}, "pdf": {"name": "1706.03993.pdf", "metadata": {"source": "META", "title": "Getting Deep Recommenders Fit: Bloom Embeddings for Sparse Binary Input/Output Networks", "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "emails": ["rstname.lastname@telefonica.com"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Computing methodologies\u2192 Neural networks; Learning latent representations; \u2022Information systems\u2192 Document representation;\nKEYWORDS Deep recommenders, sparse input/output, Bloom lters, neural network, embeddings."}, {"heading": "1 INTRODUCTION", "text": "e size of neural network models that deal with sparse inputs and outputs is o en dominated by the dimensionality of such inputs and outputs. Deep networks used for recommender systems and collaborative ltering are a paradigmatic case, as they have to deal with high-dimensional sparse vectors, typically in the order from tens of thousands to hundreds of millions, both at the input and the output of the network (e.g., [12, 26, 42, 49]). is results in large models that present a number of di culties, both at training and prediction stages. Apart from training and prediction times, an\n, . . DOI:\nobvious bo leneck of such models is space: their size (and even performance) is hampered by the physical memory of graphical processing units (GPUs), and they are di cult to deploy on mobile devices with limited hardware (cf. [24]).\nOne option to reduce the size of sparse inputs and outputs is to embed them into a lower-dimensional space. Embedding sparse high-dimensional inputs is commonplace (e.g., [5, 37, 44]). However, embedding sparse high-dimensional outputs, or even inputs and outputs at the same time, is much less common (cf. [1, 4, 48]). Importantly, typical embeddings still require the storage and processing of large matrices with the same dimensionality as the input/output (like the original neural network model would do). us, the gains in terms of space are limited. As mentioned, the size of such models is dominated by the input/output dimensionality, with input and output layers representing around 99.9% of the total amount of models\u2019 parameters. An example can be found in the deep recommender of Hidasi et al. [26], which uses a gated recurrent unit to perform session-based recommendations with input/output layers of dimensionality 330,000 and internal layers of dimensionality 100.\nIn general, an ideal embedding procedure for sparse highdimensional inputs/outputs should produce compact embeddings, of much lower dimensionality than the original input/output. In addition, it should consume li le space, both in terms of storage and memory space. Smaller sizes imply less parameters, thus training the model on embedded vectors would also be faster than with the original instances. e embedding of the output should also lead to a formulation for which the appropriate loss should be clear. Embeddings should not compromise the accuracy of the model nor the required number of training epochs to obtain that accuracy. In addition, no changes to the original core architecture of the model should be required to achieve good performance (obviously, input/output dimensions must change). e embedding should also be fast; if not to be done directly \u2018on-the- y\u2019, at least fast enough so that speed improvements made during training are not lost in the embedding operation. Last, but not least, output embeddings should be easily reversible, so that the output of the model could be mapped to the original items at prediction time.\nIn this work, we propose an unsupervised embedding technique that ful lls all the previous requirements. It can be applied to both input and output layers of neural network models that deal with binary (one-hot encoded) inputs and/or outputs. In doing so, it produces lower-dimensionality binary embeddings that can be easily mapped to the original instances. Provided that the embedding dimension is not too low, the accuracy is not compromised. Furthermore, in some cases, we show that training with embedded vectors can even increase prediction accuracy (we nd that is the\nar X\niv :1\n70 6.\n03 99\n3v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17\ncase for three of the considered recommendation tasks). e embedding requires no changes to the core network structure nor to the model con guration, and works with a so max output, the most common output activation for binary-coded instances. e embedding moreover preserves the ranking order of items which is crucial in recommender systems. As it is unsupervised, the embedding does not require any preliminary training. Moreover, it is a constant-time operation that can be either performed on-the- y, requiring no disk or memory space, or can be cached in memory, occupying orders of magnitude less space than a typical embedding matrix. Lower dimensionality of input/output vectors result in faster training, and the mapping from the embedded space to the original one does not add an overwhelming amount of time to the prediction stage. e proposed embedding is based on the idea of Bloom lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38]."}, {"heading": "2 RELATEDWORK", "text": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46]. However, the hashing trick approach does not deal with outputs, as it o ers no explicit way to map back from the (dense) embedding space to the original space. A more elementary version of the hashing trick [20] can be used at the outputs by considering it as a special case of the Bloom-based methodology proposed here. A framework providing both encoding and decoding strategies is the error-correcting output codes (ECOC) framework [17]. Originally designed for single-class outputs, it can be also applied to class sets [3]. Another example of a framework o ering recovery capabilities is kernel dependency estimation [48]. e compressed sensing approach of Hsu et al. [30] builds on top of ECOC to reduce multi-label regression to binary regression problems. Similarly, Cisse\u0301 et al. [15] use Bloom lters to reduce multi-label classi cation to binary classi cation problems and improve the robustness of individual binary classi ers\u2019 errors.\nData-dependent embeddings that require some form of learning also exist. A typical approach is to rely on variants of latent semantic analysis or singular value decomposition (SVD), exploiting similarities or correlations that may be present in the data. Again, the issue of mapping from the embedding space to the original space is le unresolved. Nonetheless, recently, Chollet [14] has successfully applied a K-nearest neighbors (KNN) algorithm to perform such a mapping and to derive a ranking of the items in the original space. An SVD decomposition of the pairwise mutual information matrix (PMI) is used to perform the embedding, and cosine similarity is used as loss function and to retrieve neighbors. Using the KNN trick o ers the possibility to exploit di erent types of factorization of similarity-based matrices. Canonical correlation analysis is an example that considers both inputs and outputs at the same time [28]. Other examples considering output embeddings are nuclear norm regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47]. In the presence of side information, like text descriptions, item or class taxonomies, or manually-collected data, a range of approaches are applicable. Akata et al. [1] provide a comprehensive list. In our study, we assume no side information is available and focus on input/output-based embeddings.\nFrom a more general perspective, reducing the space of (or compressing) neural network models is an active research topic, driven by the need to deploy such models in systems with limited hardware resources. A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers [16, 24, 31]. A less frequent approach is to reduce the model size before training [11]. ese methods typically do not focus on input layers and, to the best of our knowledge, none of them deals with high-dimensional outputs. It is also worth noting that a number of techniques have been proposed to e ciently deal with high-dimensional outputs, specially in the natural language processing domain. e hierarchical so max approach [39] or the more recent adaptive so max [22] are two examples of those. Yet, as mentioned, the focus of these works is on speed, not on space. e work of Vincent et al. [45] focuses on both aspects of very large sparse outputs but, to the best of our knowledge, cannot be directly applied to common so max outputs."}, {"heading": "3 BLOOM EMBEDDINGS", "text": ""}, {"heading": "3.1 Bloom lters", "text": "Bloom lters [7] are a compact probabilistic data structure that is used to represent sets of items, and to e ciently check whether an item is a member of a set [38]. Since the instances we deal with represent sets of one-hot encoded items, Bloom lters are an interesting option to embed those in a compact space with good recovery (or checking) guarantees.\nIn essence, Bloom lters project every item of a set to k di erent positions of a binary array u of sizem. Projections are done using a set of k independent hash functions H = {Hi }ki=1, each of which with a range from 1 tom, ideally distributing the projected items uniformly at random [38]. Proper independent hash functions can be derived using enhanced double hashing or triple hashing [18]. e number of hash functions k is usually a constant, k m, proportional to the expected number of items to be projected.\nTo check if an item is in u, one feeds it to the k hash functions H to get k array positions. If any of the bits at these positions is 0, then the item is de nitely not in the set. us, item checks return no false negatives, meaning that the structure gives an answer with 100% recall [38]. However, if all k bits at the projected positions are 1, then either the item is in the set, or the bits have by chance been set to 1 during the insertion of other set items. is implies that false positives are possible, due to collisions between projections of di erent items [8]. e values of m and k can be adjusted to control the probability of such collisions. However, in practice,m is usually constrained by space requirements, and k \u2264 10 is employed, independently of the number of items to be projected, and giving less than 1% false positive probability [9]."}, {"heading": "3.2 Embedding and recovery", "text": "In the following, we describe the use of Bloom lter techniques in embedding binary high-dimensional instances, and the recovery or mapping to such instances from these embeddings. We denote our approach as Bloom embedding (BE). e idea we pursue is to embed both inputs and outputs and to perform training in the embedding space. For that, only a probability-based output activation\nis required, together with a loss function that is appropriate for such activations.\nLet x be an input or output instance with dimensionality d , such that x = [x1, . . . xd ], xi \u2208 {0, 1}. Instances x are assumed to be sparse, that is, \u2211d i=1 xi d . Because of that, we can more conveniently (and compactly) represent x as set p = {pi }ci=1, pi \u2208 N\u2264d , where c is the number of non-zero elements and pi is the position of such elements in x. For every set p, we generate an embedded instance u of dimensionality m < d , such that u = [u1, . . .um ], ui \u2208 {0, 1}. To do so, we rst set allm components of u to 0. en, iteratively, for every position pi , i = 1, . . . c , and every projection Hj , j = 1, . . .k , we assign\nuHj (pi ) = 1. (1)\nNotice that, since Hj has a range between 1 and m, k \u2265 1, and m < d , a number of original positions pi may map to the same index in u. Bloom lters mitigate this by properly choosing k independent hash functions H (Sec. 3.1). Notice furthermore that the process has no space requirements, as H is computed on-the- y. Finally, notice that the embedding of a set p is constant time: the process isO(ck), with c bounded by the maximum number of active items in x, c d , and k being a constant that is set beforehand, k m < d . In practice, this constant time is dominated by the time spent on H to generate a hash. If we want to be faster than that, and at the same time ensure an optimal (uniform) distribution of the outputs of H , we can pre-compute a hash matrix storing the projections or hash indices for all the potential items in p. We can do it by generating vectors h = [h1, . . .hk ] for each pi , where hj is a uniformly randomly chosen integer between 1 and m (without replacement). is way, by pre-generating all projections for all d items, we end up with a d \u00d7 k matrix H of integers between 1 and m, which we can easily store in random-access memory (RAM), not in the GPU memory.\nWe now explain how to recover a probability-based ranking of the d items at the output of the model. Assuming a so max activation is used, we have a probability vector v\u0302 = [v\u03021, . . . v\u0302m ] that, at training time, is compared to the binary embedding v of the ground truth y, with its active positions q (analogously to x and p). To unravel the embedding v\u0302 and map to the d original items of y, we can understand v\u0302 as a k-way factorization of every item y\u0302i of our prediction y\u0302. en, following the idea of Bloom lters, if yi maps to vi and v\u0302i = 0, we can con rm that that item is de nitely not in the output of the model (Sec. 3.1). Otherwise, if v\u0302i is relatively large, we want the likelihood of that item to re ect that. Speci cally, given an active position qi from q representing y, we can compute the likelihood\nL(qi ) = k\u220f j=1 v\u0302Hj (qi ), (2)\nand assign outputs y\u0302i = L(qi ). Alternatively, if a more numericallystable output is desired, we can compute the negative log-likelihood\nL(qi ) = \u2212 k\u2211 j=1 log ( v\u0302Hj (qi ) ) . (3)\nBoth operations, when iterated for i = 1, . . .d , de ne a ranking over the items in y and from there, if needed, a probability distribution can be recovered by re-normalization. We here do not perform such\nre-normalization, as all the problems we consider can be mapped to a typical ranking recommendation se ing."}, {"heading": "3.3 Suitability", "text": "Note that BE, by construction, already o ers a number of the aforementioned desired qualities for sparse binary high-dimensional embeddings (Sec. 1). Speci cally, BE is designed for both inputs and outputs, o ering a rank-based mapping between the original instances and the embedded vectors. BE yields a more compact representation of the original instance and requires no disk or memory space (at most some marginal RAM space, not GPU memory). In addition, BE can be performed on-the- y, without training, and in constant time. In the following, we demonstrate the remaining desirable qualities using a comprehensive experimental setup: we show that the accuracy of the model remains stable or even increases given a reasonable embedding dimension, that no changes in the model architecture nor con guration are required, that training times are faster thanks to the reduction of the number of parameters of the model, that evaluation times do not carry much overhead, and that performance is generally be er than a number of alternative approaches."}, {"heading": "4 EXPERIMENTAL SETUP", "text": ""}, {"heading": "4.1 General considerations", "text": "We demonstrate that BE works under several se ings and that it can be applied to multiple tasks. In particular, we focus on recommendation and collaborative ltering but also demonstrate the validity of the approach on a natural language processing task. We consider a number of data sets, network architectures, con gurations, and evaluation measures. In total, we de ne 7 di erent setups, which we describe in Sec. 4.2. We also demonstrate that BE is competitive with respect to the available alternatives. To this end, we consider 4 di erent state-of-the-art approaches, which we overview in Sec. 4.3.\nData sets are formed by inputs withn instances, corresponding to either individual instances (or one-hot encoded user pro les) or to sequences of instances (or pro le lists). Outputs, also of n instances, correspond to individual instances or to class labels. Instances have an original dimensionality d , corresponding to the cardinality of all possible pro le items. Given the nature of the considered problems, instances are very sparse, with all but c items being di erent from 0, c d , typically with c/d in the order of 10\u22125 (Table 1).\nFor each data set, and based on the literature, we select an appropriate baseline neural network architecture. We experiment with both feed-forward (autoencoder-like) and recurrent networks, carefully selecting their parameters and con guration to match (or even improve) the state-of-the-art results. For the sake of comparison, we also choose appropriate and well-known evaluation measures [34]. Depending on the data set (Table 2), we work with mean average precision (MAP), reciprocal ranks (RR), or % of accuracy (Acc).\nEach combination of data set, network architecture, con guration, and evaluation measure de nes a task. For every task, we compute a baseline score S0, corresponding to running the plain neural network model without any embedding. We then report the performance of the i-th execution with a particular embedding with respect to the baseline score using Si/S0. is way, we can compare\nthe performance across di erent tasks using di erent evaluation measures, reporting relative improvement/loss with respect to the baseline. Similarly, to compare across di erent dimensionalities, we report the ratio of embedding dimensionality with respect to the original dimensionality,m/d , and to compare across di erent training and evaluation times, we report time ratios with respect to the baseline, Ti/T0."}, {"heading": "4.2 Tasks", "text": "We now give a brief summary of the 7 considered tasks (Tables 1 and 2). All data sets are publicly-available and, for all tasks, we make sure that the network architecture and the experimental setup is su cient to achieve a state-of-the-art result. We use so max outputs and categorical cross-entropy losses in all experiments.\n(1) Movielens (ML): movie recommendation with the Movielens 20M data set1 [25]. Ratings were discretized with a threshold of 3.5 and movies with less than 5 ratings and users with less than two movies were discarded. Inputs and outputs were built by spli ing user pro les at a certain timestamp uniformly at random, ensuring a minimum of one movie in both input and output. To perform recommendations, we build on top of Wu et al. [49] and employ a 3-layer feed-forward neural network model with 150 rectied linear units [21] in the hidden layers. We optimize the weights of the network using cross-entropy and Adam [32],\n1h p://grouplens.org/datasets/movielens/20m/\nwith a learning rate of 0.001 and parameters \u03b21 = 0.9 and \u03b22 = 0.999. We evaluate the accuracy of the model using MAP. (2) Million song data set (MSD): song recommendation with the Million song data set2 [6]. We assume that a user likes a song when he/she has listened to it a minimum of 3 times. We then remove the songs that appear less than 20 times and build user pro les with a minimum of 5 songs. Inputs and outputs are split at a uniformly random timestamp. To recommend future listens to the user we use a 3-layer feed-forward neural network and 300 recti ed linear units in the hidden layers. As for the rest, we proceed as with the ML task. We evaluate the accuracy of the model with MAP. (3) Amazon book reviews (AMZ): book recommendation with the Amazon book reviews data set3 [35]. We proceed as with the ML data set, but this time se ing the minimum number of ratings per book to 100. We employ a 4-layer feed-forward neural network with 300 recti ed linear units in the hidden layers, and optimize its parameters with Adam. We evaluate the accuracy of the model with MAP. (4) Book crossing (BC): book recommendation with the book crossing data set4 [50]. To perform recommendations we use the same architecture and con guration as with the MSD task, but this time we use 250 units in the hidden layers. (5) YooChoose (YC): session-based recommendation with the YooChoose RecSys15 challenge data set5. We work with the training set of the challenge and keep only the click events. We take the rst 2 million sessions of the data set which have a minimum of 2 clicks. To predict the next click, we proceed as in Hidasi et al. [26] and consider a GRU model [13]. We set the inner dimensionality to 100 and train the network with Adagrad [19], using a learning rate of 0.01. We evaluate the accuracy of the model with RR. (6) Penn treebank (PTB): next word prediction with the Penn treebank data set [36]. e vocabulary is limited to 10,000 words, with all other words mapped to an \u2018unknown\u2019 token (Table 1). We consider the end of the sentence as an additional token and form input sequences of length 10. Inspired by Graves [23], we perform next word prediction with an LSTM network [27]. We set the inner dimensionality to 250 and train the network with SGD. We use a learning rate of 0.25, a momentum of 0.99, and clip gradients to have a maximum norm of 1 [23]. We evaluate the accuracy of the model with the RR of the correct prediction. (7) CADE web directory (CADE): text categorization with the CADE web directory of Brazilian web pages6 [10]. e data set contains around 40,000 documents assigned to one of 12 categories such as services, education, health, or culture. To perform classi cation we use a 4-layer feed-forward\n2h p://labrosa.ee.columbia.edu/millionsong/tastepro le 3h p://jmcauley.ucsd.edu/data/amazon/ 4h p://www2.informatik.uni-freiburg.de/\u223ccziegler/BX/ 5h p://recsys.yoochoose.net 6h p://ana.cachopo.org/datasets-for-single-label-text-categorization\nneural network with a so max output. e number of units is, from input to output, 400, 200, 100, and 12, and we use recti ed linear units as activations for the hidden layers. We train the network with RMSprop [43], a learning rate of 0.0002, and exponential decay of 0.9. Notice that this is the only considered task where output embeddings are not required (classi cation into 12 text categories). We use Acc as evaluation measure."}, {"heading": "4.3 Alternative approaches", "text": "To compare the performance of BE with the state-of-the-art, we consider 4 di erent embedding alternatives. We base our evaluation on performance, measured at a given input/output compression ratio. It is important to note that, in general, besides performance, alternative approaches do not present some of the other desired qualities that BE o ers, such as on-the- y operation, constanttime, no supervision, or no network/con guration changes (Secs. 1 and 3.3). Note also that methods for embedding both inputs and outputs, allowing to map embedded instances to the original ones, are scarce (Sec. 2). Because of that, in some of the considered alternatives we had to perform some adaptations.\n(1) Hashing trick (HT). We rst consider the popular hashing trick for classi er and recommender inputs [33, 46]. In general, these methodologies only focus on inputs and are not designed to deal with any type of output. Nonetheless, in the case of binary outputs, variants like the one used by Ganchev and Dredze [20] can be adapted to map to the original items using Eqs. 2 or 3. In fact, considering this adaptation for recovery, the approach can be seen as a special case of BE with k = 1 (Sec. 3.2). (2) Error-correcting output codes (ECOC). Originally designed for single-class targets [17], ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies [3]. Yet, in the case of training neural networks, it is not clear which loss function should be used. e obvious choice would be to use the Hamming distance. However, in pre-analysis, a Hamming loss turned out to be signi cantly inferior than cross-entropy. erefore, we use the la er in our experiments. We construct the ECOC matrix with the randomized hill-climbing method of [17]. (3) Pairwise mutual information (PMI). Recently, Chollet [14] has proposed a PMI approach for embedding sets of image labels into a dense space of real-valued vectors. e approach is based on the SVD of a PMI matrix computed from counting pairwise co-occurrences. It uses cosine similarity as the loss function and, at prediction time, it performs KNN (again using cosine similarity) with the projection of individual labels to obtain a ranking. (4) Canonical correlation analysis (CCA). CCA is a common way to learn a joint dense, real-valued embedding for both inputs and outputs at the same time [28]. CCA can be computed using SVD on a correlation matrix [29] and, similarly to PMI, we can use the KNN trick to rank items or labels at prediction time. Correlation is now the metric\nof choice, both for the loss function and for determining the neighbors."}, {"heading": "5 RESULTS", "text": ""}, {"heading": "5.1 Compression and performance", "text": "We start by reporting on the performance of BE as a function of the embedding dimension. As mentioned, to facilitate comparisons, we report in relative terms, using score ratios Si/S0 and dimensionality ratios m/d . When plo ing the former as a function of the la er, we see several things that are worth noting (Fig. 1). Firstly, we observe that, for most of the tasks, score ratios approach 1 as m approaches d . is indicates that the introduction of BE does not degrade the original score of the Baseline when the embedding dimensionm is comparable to the original dimension d . Secondly, we observe that the lower the dimensionality ratio, the lower the score ratio. is is to be expected, as one cannot embed sets of items with their intrinsic dimensionality to an in nitesimally small m. Importantly, the reduction of Si/S0 should not be linear withm/d , but should maximize Si for low m (thus ge ing curves close to the top le corner of Fig. 1). We see that BE ful lls this requirement. In general, we can reduce the input/output size up to 5 times (m/d = 0.2) and still maintain more than 92% of the value of the original score. e ML task is the only exception, which we think is due to the abnormally high density of the data (Table 1), inhibiting the embedding to low dimensions7. CADE is the task for which BE achieves the highest Si for low m. Presumably, the CADE task is the easiest one we consider, as only input embeddings are required.\nAn additional observation is worth noting (Fig. 1). Interestingly, we nd that BE can improve the scores over the Baseline for a number of tasks. at is the case for 3 out of the 7 considered tasks (all of them recommendation tasks): MSD with m/d \u2265 0.3, AMZ with m/d \u2265 0.2, and BC with 0.3 \u2264 m/d \u2264 0.6. e fact that an embedding performs be er than the original Baseline has been also observed in some other methods for speci c data sets [14, 33, 48]. For instance, Chollet [14] has reported increases up to 7% using\n7Note that the ML data is essentially collected through a survey-type method [25].\nthe PMI approach on the so-called JFT data set. Here, depending on the task and the embedding dimension, relative increases go from 1 to 12%. Given that the data sets where we observe these increases are some of the less dense ones (Table 1), we hypothesize that, in the case of BE, such increases come from having k times more active positions in the ground truth output (recall that one output item is projectedk times usingk independent hash functions, Sec. 3.2). With k more times elements set to 1 in the output, a be er estimation of the gradient may be computed (larger errors that propagate back to the rest of the network).\nWe now focus on performance as a function of the number of projections k (Fig. 2), reporting score ratios Si/S0 as above. From repeating the plots for di erent values ofm/d , we observe that Si/S0 is always low for k = 1 (Fig. 2, le ), except whenm approaches d , where we have an almost at behavior (Fig. 2, right). In general, Si/S0 jumps up for k \u2265 2 and remains stable until k \u2248 10, where the decrease of Si/S0 becomes more apparent (Fig. 2, le ). e best operating range typically corresponds to 2 \u2264 k \u2264 4. e ML task is again an exception, with a best operating range around 7 \u2264 k \u2264 10."}, {"heading": "5.2 Training and retrieval time", "text": "Besides performance scores, it is interesting to assess whether the reduction of input and output dimensions has an e ect to training and evaluation times. To this end, we plot the time ratios Ti/T0 as a function of the dimensionality ratio m/d (Fig. 3). Regarding training times, we basically observe a linear decrease with m/d (Fig. 3, le ). ML is an exception to the trend, and CADE and AMZ experiment almost no decrease for very low dimensionality ratios m/d < 0.2. In general, we con rm faster training times thanks to the reduction of the number of parameters of the model, dominated by input/output matrices (output dimension also a ecting the time to compute the loss function). We obtain a 2 times speedup for a 2 times input/output compression and, roughly, a li le bit over 3 times speedup for a 5 times input/output compression. Regarding evaluation times, we also observe a linear trend (Fig. 3, right). However, this time,Ti/T0 is not as low, with values slightly above 1 but always below 1.5 (with the exception of CADE form/d > 0.6). Overall, this indicates that, compared to the Baseline evaluation time, the mapping used by BE when reconstructing the output does not introduce an overwhelming amount of extra computation time. With the exception of ML, extra computation time is below 20% for m/d < 0.5."}, {"heading": "5.3 Comparison with alternatives", "text": "Finally, we compare the performance of BE to the one of the considered alternative methods. We do so by establishing a dimensionality ratiom/d and computing the corresponding score ratio Si/S0 for a given task (Table 3). We see that BE is be er than the alternative methods in 5 out of the 7 tasks (10 out of the 14 considered test points). PMI is be er in one of the tasks (CADE) and CCA is be er also in one of the tasks (AMZ). It is relevant to note that, when BE wins, it always does so by a relatively large margin (see, for instance, the ML or YC tasks). Otherwise, when an alternative approach wins, generally it does so by a smaller margin (see, for instance, the AMZ task).\nese results become more relevant if we realize that PMI and CCA are both SVD-based approaches, introducing a separate degree of supervised learning to the task, exploiting pairwise item co-occurrences and correlations, respectively (Sec. 4.3). In contrast, BE does not require any learning. We formulate a co-occurrencebased version of BE below, which achieves moderate performance increments over BE and more closely approaches the performance of PMI and CCA on the two tasks where BE was not already performing best. A further interesting thing to note is that we con rm the small variation in the score ratios obtained for 2 \u2264 k \u2264 10 (Fig. 2). Here, score ratios for 3 \u2264 k \u2264 5 are o en comparable in a statistical signi cance sense (Table 3)."}, {"heading": "6 GOING ONE STEP FURTHERWITH CO-OCCURRENCE-BASED COLLISIONS", "text": ""}, {"heading": "6.1 Co-occurrence-based embedding", "text": "In Bloom lters and BE, collisions are unavoidable due to the lower embedding dimensionality and the use of multiple projections (Sec. 3). In addition, we have seen that some alternative approaches produce embeddings by exploiting co-occurrence information (Secs. 2 and 4.3). Here, we study a variant of BE that takes advantage of co-occurrence information to adjust the collisions that will inevitably take place when performing the embedding. We denote this approach by co-occurrence-based Bloom embedding (CBE).\nAlgorithm 1 Pseudocode for CBE.\nInput: Input and/or output instancesX (n\u00d7d sparse binary matrix), embedding dimensionalitym, number of projections k , and precomputed hashing matrix H (d \u00d7 k integers matrix). Output: Co-occurrence-based hashing matrix H\u2032.\n1: C\u2190 XTX 2: C\u2190 C sgn(C\u2212avgfreq(X)) 3: cval, crow, ccol \u2190 coord(lowtri(C)) 4: for i in argsort(cval) 5: a,b \u2190 crowi , c col i 6: r \u2190 urnd(1,m, ha \u222a hb ) 7: ja \u2190 urnd(1,k, \u2205) 8: jb \u2190 urnd(1,k, \u2205) 9: ha, ja ,hb, jb \u2190 r\n10: return H\nWhat we propose is a quite straightforward approach to CBE, which does not add much extra pre-computation time. Training and testing times remain the same, as CBE uses a pre-computed hashing matrix H (Sec. 3.2). e general idea of the proposed approach is to \u2018re-direct\u2019 the collisions of the co-occurring items to the same bits or positions of u. Our implementation of this idea is detailed in Algorithm 1, and brie y explained below.\nFirst, we count pairwise co-occurrences and store them in a sparse matrix C (line 1). Next, we threshold C by the average item frequency in X using the Hadamard product and a componentwise sign function (line 2). We then get the lower triangular part of C and return it in coordinates format, that is, using a tuple of values, row indices, and column indices (line 3). We will use the order in cval to update the hash matrix H. To do so, we rst loop over the indices of the sorted values of cval in increasing order (line 4). A er selecting the corresponding items a and b (line 5), we then draw integers from urnd (lines 6\u20138). e function urnd(x ,y, z) is a uniform random integer generator between x and y (both included)\nsuch that the output integer is not included in the set z, that is, urnd(x ,y, z) < z. Rows a and b of H are transformed to sets ha and hb and its union is computed (line 6). Finally, we use the integers generated by urnd to pick projections ja and jb from H, and assign them the same bit r (line 9). By updating the projections in H in increasing order of co-occurrence (line 4), we give priority to the pairs with largest co-occurrence, se ing them to collide to the same bit r (line 9)."}, {"heading": "6.2 CBE results", "text": "Overall, the performance of CBE only provides moderate increments over the original BE approach (Fig. 4). With the exception of the BC task, the performance of CBE is always higher than the one of BE. However, with the exception of the AMZ task, we do not observe dramatic increases of CBE over BE. On average, such increases are between 0.4% and 8.4% (Table 4, right). One possible explanation for these moderate performance increases is the low cooccurrence in the considered data (Table 4, le ). As it can be seen, typically less than 3% of all possible pairs show a co-occurrence. Moreover, the average co-occurrence count of such co-occurring pairs is very low, with ratios \u03c1 to the total number of instances n in the order of 10\u22125 or 10\u22126.\nDespite being moderate on average, we observed that the increments provided by CBE were more prominent for low dimensionality ratios m/d . By relating CBE with the best approaches resulting from the previous comparison of Table 3, we see that CBE is generally be er than BE, sometimes with a statistically significant di erence (Table 5). Furthermore, we see that CBE, being based on co-occurrences, more closely approaches PMI and CCA in the tasks where those were performing best, and even outperforms them in one test point (AMZ,m/d = 0.2). Being closer to those cooccurrence-based approaches is an indication that CBE leverages co-occurrence information to some extent."}, {"heading": "7 CONCLUSION", "text": "We have proposed the use of Bloom embeddings to represent sparse high-dimensional binary-coded inputs and outputs. We have shown that a compact representation can be obtained without compromising the performance of the original neural network model or, in some cases, even increasing it by a substantial factor. Due to the compact representation, the loss function and the input and output layers deal with less parameters, which results in faster training\ntimes. e approach compares favorably with respect to the considered alternatives, and o ers a number of further advantages such as on-the- y operation or zero space requirements, all this without introducing changes to the core network architecture, task con guration, or loss function.\nIn the future, besides continuing to exploit co-occurrences, one could enhance the proposed approach by considering further extensions of Bloom lters such as counting Bloom lters [9]. In theory, those extensions could provide a more compact representation by breaking the binary nature of the embedding. However, they could require the modi cation of the loss function or the mapping process (Eqs. 2 and 3). A faster mapping process using the sorted probabilities of v could also be studied. A detailed, comparative analysis of false positives and false negatives is also pending. Finally, it would be interesting to assess the utility of BE in combination with classical collaborative ltering approaches or factorization machines [40]."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the curators of the data sets used in this study for making them publicly-available and Santi Pascual for his comments on a previous version of the paper."}], "references": [{"title": "Label-embedding for image classi\u0080cation", "author": ["Z. Akata", "F. Perronnin", "Z. Harchaoui", "C. Schmid"], "venue": "IEEE Trans. on Pa\u0088ern Analysis and Machine Intelligence 38,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Uncovering shared structures in multiclass classi\u0080cation", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Error-correcting output codes for multi-label text categorization", "author": ["G. Armano", "C. Chira", "N. Hatami"], "venue": "In Proc. of the Italian Information Retrieval Conf. (IIR)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "\u008ce million song dataset", "author": ["T. Bertin-Mahieux", "D.P.W. Ellis", "B. Whitman", "P. Lamere"], "venue": "In Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Space/time trade-o\u0082s in hash coding with allowable errors", "author": ["B.H. Bloom"], "venue": "Commun. ACM 13,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}, {"title": "Bloom \u0080lters \u2013 a tutorial, analysis, and survey", "author": ["J. Blustein", "A. El-Maazawi"], "venue": "Technical Report", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "An improved construction for counting Bloom \u0080lters", "author": ["F. Bonomi", "M. Mitzenmacher", "R. Panigrahy", "S. Singh", "G. Varghese"], "venue": "In European Symposium on Algorithms (ESA), Y. Azar and T. Erlebach (Eds.). Lecture Notes in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Improving methods for single-label text categorization", "author": ["A. Cardoso-Cachopo"], "venue": "Ph.D. Dissertation. Instituto Superior Tecnico, Universidade Tecnica de Lisboa", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J. Wilson", "S. Tyree", "K. Weinberger", "Y. Chen"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Wide & deep learning for recommender systems", "author": ["H.-T. Cheng", "L. Koc", "J. Harmsen", "T. Shaked", "T. Chandra", "H. Aradhye", "G. Anderson", "G. Corrado", "W. Chai", "M. Ispir", "R. Anil", "Z. Haque", "L. Hong", "V. Jain", "X. Liu", "H. Shah"], "venue": "In Proc. of the Workshop on Deep Learning for Recommender Systems (DLRS)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "On the properties of neural machine translation: encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "In Proc. of the Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Information-theoretic label embeddings for large-scale image classi\u0080cation", "author": ["F. Chollet"], "venue": "ArXiv: 1607.05691", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Robust Bloom \u0080lters for large multilabel classi\u0080cation tasks", "author": ["M. Ciss\u00e9", "N. Usunier", "T. Arti\u00e8res", "P. Gallinari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "BinaryConnect: training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Die\u008aerich", "G. Bakiri"], "venue": "Journal of Arti\u0080cial Intelligence Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Bloom \u0080lters in probabilistic veri\u0080cation", "author": ["P.C. Dillinger", "P. Manolios"], "venue": "In Proc. of the Int. Conf. on Formal Methods in Computer-Aided Design (FMCAD)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Small statistical models by random feature mixing", "author": ["K. Ganchev", "M. Dredze"], "venue": "In ACL Workshop on Mobile Language Processing (MLP)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Deep sparse recti\u0080er neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of the Int. Conf. on Arti\u0080cial Intelligence and Statistics (AISTATS)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "E\u0081cient so\u0089max approximation for GPUs", "author": ["E. Grave", "A. Joulin", "M. Ciss\u00e9", "D. Grangier", "H. J\u00e9gou"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Generating sequences with recurrent neural networks. ArXiv: 1308.0850", "author": ["A. Graves"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "\u008ce MovieLens datasets: history and context", "author": ["F.M. Harper", "J.K. Konstan"], "venue": "ACM Trans. on Interactive Intelligent Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Long short-term memory networks", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika 28,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1936}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "J. Comput. System Sci. 78,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Multi-label prediction via compressed sensing", "author": ["D.J. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "In Proc. of the Int. Conf. on Learning Representations (ICLR)", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Vowpal wabbit online learning project", "author": ["J. Langford", "L. Li", "A. Strehl"], "venue": "Technical Report", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "In Proc. of the ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Ph.D. Dissertation. Brno University of Technology", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "E\u0081cient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Probability and computing: randomized algorithms and probabilistic analysis", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Proc. of the Int. Workshop on Arti\u0080cial Intelligence and Statistics (AISTATS)", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Pe\u008aerson", "G. Dror", "J. Langford", "A. Smola", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Hybrid recommender system based on autoencoders", "author": ["F. Strub", "R. Gaudel", "J. Mary"], "venue": "In Proc. of theWorkshop on Deep Learning for Recommender Systems (DLRS)", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Lecture 6.5-RMSprop: divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine", "author": ["T. Tieleman", "G. Hinton"], "venue": "Learning 4,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proc. of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "E\u0081cient exact gradient update for training deep networks with very large sparse targets", "author": ["P. Vincent", "A. Br\u00e9bisson", "X. Bouthilier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. A\u008aenberg", "J. Langford", "A. Smola"], "venue": "In Proc. of the Int. Conf. on Machine Learning (ICML)", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine Learning 81,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "Kernel dependency estimation", "author": ["J. Weston", "O. Chapelle", "A. Elissee", "B. Sch\u00f6lkopf", "V. Vapnik"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2002}, {"title": "Collaborative denoising auto-Encoders for top-N recommender systems", "author": ["Y. Wu", "C. DuBois", "A.X. Zheng", "M. Ester"], "venue": "In Proc. of the ACM Int. Conf. on Web Search and Data Mining (WSDM)", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Improving recommendation lists through topic diversi\u0080cation", "author": ["C.-N. Ziegler", "S.M. McNee", "J.A. Konstan", "G. Lausen"], "venue": "In Proc. of the Int. World Wide Web Conf. (WWW)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 37, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 44, "context": ", [12, 26, 42, 49]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 4, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 33, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 39, "context": ", [5, 37, 44]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 0, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 43, "context": "[1, 4, 48]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 6, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 8, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 17, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 34, "context": "\u008ce proposed embedding is based on the idea of Bloom \u0080lters [7], and therefore it inherits part of the theory developed around that idea [8, 9, 18, 38].", "startOffset": 136, "endOffset": 150}, {"referenceID": 29, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 36, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 41, "context": "A common approach to embed high-dimensional inputs is the hashing trick [33, 41, 46].", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "A more elementary version of the hashing trick [20] can be used at the outputs by considering it as a special case of the Bloom-based methodology proposed here.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "A framework providing both encoding and decoding strategies is the error-correcting output codes (ECOC) framework [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "Originally designed for single-class outputs, it can be also applied to class sets [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 43, "context": "Another example of a framework o\u0082ering recovery capabilities is kernel dependency estimation [48].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "[30] builds on top of ECOC to reduce multi-label regression to binary regression problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] use Bloom \u0080lters to reduce multi-label classi\u0080cation to binary classi\u0080cation problems and improve the robustness of individual binary classi\u0080ers\u2019 errors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Nonetheless, recently, Chollet [14] has successfully applied a K-nearest neighbors (KNN) algorithm to perform such a mapping and to derive a ranking of the items in the original", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "Canonical correlation analysis is an example that considers both inputs and outputs at the same time [28].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 48, "endOffset": 51}, {"referenceID": 42, "context": "regularized learning [2], label embedding trees [4], or the WSABIE algorithm [47].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "[1] provide a comprehensive list.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "A common approach is to reduce the size of already trained models by some quantization and/or pruning of the connections in dense layers [16, 24, 31].", "startOffset": 137, "endOffset": 149}, {"referenceID": 10, "context": "A less frequent approach is to reduce the model size before training [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "\u008ce hierarchical so\u0089max approach [39] or the more recent adaptive so\u0089max [22] are two examples of those.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "\u008ce hierarchical so\u0089max approach [39] or the more recent adaptive so\u0089max [22] are two examples of those.", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "[45] focuses on both aspects of very large sparse outputs but, to the best of our knowledge, cannot be directly applied to common so\u0089max outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Bloom \u0080lters [7] are a compact probabilistic data structure that is used to represent sets of items, and to e\u0081ciently check whether an item is a member of a set [38].", "startOffset": 13, "endOffset": 16}, {"referenceID": 34, "context": "Bloom \u0080lters [7] are a compact probabilistic data structure that is used to represent sets of items, and to e\u0081ciently check whether an item is a member of a set [38].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "a set of k independent hash functions H = {Hi }i=1, each of which with a range from 1 tom, ideally distributing the projected items uniformly at random [38].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "Proper independent hash functions can be derived using enhanced double hashing or triple hashing [18].", "startOffset": 97, "endOffset": 101}, {"referenceID": 34, "context": "\u008cus, item checks return no false negatives, meaning that the structure gives an answer with 100% recall [38].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "\u008cis implies that false positives are possible, due to collisions between projections of di\u0082erent items [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "less than 1% false positive probability [9].", "startOffset": 40, "endOffset": 43}, {"referenceID": 30, "context": "For the sake of comparison, we also choose appropriate and well-known evaluation measures [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "(1) Movielens (ML): movie recommendation with the Movielens 20M data set1 [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 44, "context": "[49] and employ a 3-layer feed-forward neural network model with 150 recti-", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "\u0080ed linear units [21] in the hidden layers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 28, "context": "We optimize the weights of the network using cross-entropy and Adam [32],", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "(2) Million song data set (MSD): song recommendation with the Million song data set2 [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 31, "context": "(3) Amazon book reviews (AMZ): book recommendation with the Amazon book reviews data set3 [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 45, "context": "(4) Book crossing (BC): book recommendation with the book crossing data set4 [50].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "[26] and consider a GRU model [13].", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "We set the inner dimensionality to 100 and train the network with Adagrad [19], using a learning rate of 0.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "(6) Penn treebank (PTB): next word prediction with the Penn treebank data set [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Inspired by Graves [23], we perform next word prediction with an LSTM network [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "Inspired by Graves [23], we perform next word prediction with an LSTM network [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "99, and clip gradients to have a maximum norm of 1 [23].", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "(7) CADE web directory (CADE): text categorization with the CADE web directory of Brazilian web pages6 [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 38, "context": "We train the network with RMSprop [43], a learning rate of 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "We \u0080rst consider the popular hashing trick for classi\u0080er and recommender inputs [33, 46].", "startOffset": 80, "endOffset": 88}, {"referenceID": 41, "context": "We \u0080rst consider the popular hashing trick for classi\u0080er and recommender inputs [33, 46].", "startOffset": 80, "endOffset": 88}, {"referenceID": 19, "context": "Nonetheless, in the case of binary outputs, variants like the one used by Ganchev and Dredze [20] can be adapted to map to the original items using Eqs.", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "Originally designed for single-class targets [17], ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies [3].", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "Originally designed for single-class targets [17], ECOC can be applied to class sets (inputs and outputs), with its corresponding encoding and decoding strategies [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 16, "context": "ECOC matrix with the randomized hill-climbing method of [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Recently, Chollet [14]", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "CCA is a common way to learn a joint dense, real-valued embedding for both inputs and outputs at the same time [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "computed using SVD on a correlation matrix [29] and, similarly to PMI, we can use the KNN trick to rank items or labels at prediction time.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 29, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 43, "context": "\u008ce fact that an embedding performs be\u008aer than the original Baseline has been also observed in some other methods for speci\u0080c data sets [14, 33, 48].", "startOffset": 135, "endOffset": 147}, {"referenceID": 13, "context": "For instance, Chollet [14] has reported increases up to 7% using", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "7Note that the ML data is essentially collected through a survey-type method [25].", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In the future, besides continuing to exploit co-occurrences, one could enhance the proposed approach by considering further extensions of Bloom \u0080lters such as counting Bloom \u0080lters [9].", "startOffset": 181, "endOffset": 184}], "year": 2017, "abstractText": "Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., one-hotencoded vectors of item preferences), these algorithms tend to have large input and output dimensionalities that dominate their overall size. \u008cis makes them di\u0081cult to train, due to the limited memory of graphical processing units, and di\u0081cult to deploy on mobile devices with limited hardware. To address these di\u0081culties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally e\u0081cient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as \u2018on-the-\u0083y\u2019 constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training con\u0080guration.", "creator": "LaTeX with hyperref package"}}}