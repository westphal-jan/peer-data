{"id": "1706.04223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures", "abstract": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training.\n\n\n\n\n\n\nThe following steps are aimed at training a program of our choosing that is a continuous programming language (i.e., the language used for the study).\n\n1. Define the first part of the program (i.e., the original source code).\n2. Define the second part.\n3. Create an appropriate base for the training (i.e., the last part of the training) for the training (i.e., the last part of the training).\n4. Provide the second part with the necessary training (i.e., the last part of the training).\n5. Add a training model to the training (i.e., the last part of the training).\nThe basic basic training model is an initial training model for the whole training (i.e., the last part of the training).\nIf the training is a discrete set of discrete patterns, the data are assigned as the base of the model. We specify how the model is to be computed for the training and then it can be used as an initial training model.\nFor the base of the training (i.e., the last part of the training). For the entire training (i.e., the last part of the training).\nTo get the data set, we need to specify an initialization condition, which is an initial training model, that contains any kind of data. This condition is that we assume the first part of the training is the data of the first part of the training.\nThe initialization condition is that we have the initial training condition. For the training (i.e., the last part of the training). For the training (i.e., the last part of the training). For the training (i.e., the last part of the training).\nTo make the training (i.e., the last part of the training). For the training (i.e., the last part of the training). For the training (", "histories": [["v1", "Tue, 13 Jun 2017 19:00:53 GMT  (146kb,D)", "http://arxiv.org/abs/1706.04223v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["junbo zhao", "yoon kim", "kelly zhang", "alexander m rush", "yann lecun"], "accepted": false, "id": "1706.04223"}, "pdf": {"name": "1706.04223.pdf", "metadata": {"source": "CRF", "title": "Adversarially Regularized Autoencoders for Generating Discrete Structures", "authors": ["Junbo (Jake"], "emails": ["jakezhao@cs.nyu.edu", "yoonkim@seas.harvard.edu", "kz918@nyu.edu", "srush@seas.harvard.edu", "yann@cs.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37]. These latent representations facilitate the ability to apply smooth transformations and interpolations in latent space in order to produce complex modifications of generated outputs, while still remaining on the data manifold.\nUnfortunately, learning similar latent representations of discrete structures, such as text sequences or discretized images, remains a challenging problem. Applying GANs directly to this task produces discrete output from the generator, which then requires clever approaches for backpropagation. Furthermore this issue is compounded in cases where the generative model is recurrent, e.g. in sequence modeling. Researchers have circumvented some of these issues by using policy gradient\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n04 22\n3v 1\n[ cs\nmethods [5, 12, 36] or with the Gumbel-Softmax distribution [17]. However, neither approach can yet produce robust latent representations directly from samples, particularly for discrete structures.\nAn alternative approach is to instead encode discrete structures in a continuous code space to circumvent this problem altogether. As this space is continuous, traditional GAN training can be directly applied to learn a latent representation of the code space itself. Samples from the GAN can then be decoded to generate discrete outputs. While in theory this technique can be applied directly, in practice, learning general-purpose autoencoders is in itself a difficult problem.\nIn this work, we propose a simple extension of this technique by jointly training a code-space GAN and a discrete structure autoencoder, which we call an adversarially regularized autoencoder (ARAE). The approach allows us to use a general-purpose GAN architecture that generates continuous code representations, while at the same time deploying task-specific autoencoder architectures, like a recurrent neural network for text, to produce and decode from these latent representations.\nThe ARAE approach can be used both as a generative model and as a way to obtain an encoding of the input. First, it learns a GAN with a Gaussian latent space that can be sampled to produce discrete structures. This model can be compared directly with existing generative models. Second, it learns an adversarially regularized encoder that can produce useful code space representations from discrete structures, without requiring an explicit code-space prior. We can compare this method to other specialized autoencoders such as denoising and variational autoencoders.\nOur experiments test ARAE on two different discrete domains: discretized images and text sequences. We show that this approach successfully learns latent representations for both tasks, as the model is able to generate coherent samples, ignore or fix corrupted inputs, and produce predictable changes in the outputs when performing manipulations in the latent space. We find that we are able perform consistent sentence manipulations by moving around in the latent space via offset vectors. A similar property was observed in image GANs [25] and word representations [23]. Finally, experiments on a semi-supervised learning task for natural language inference provide quantitative evidence that this approach improves upon continuous representations learned by autoencoders. Code is available at https://github.com/jakezhaojb/ARAE."}, {"heading": "2 Related Work", "text": "GANs for Discrete Structures The success of GANs on images have led many researchers to consider applying GANs to discrete data such as text. Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space [8, 34]. When trained on text data however, such methods often require pre-training/co-training with a maximum likelihood (i.e. language modeling) objective [5, 36, 18]. This precludes there being a latent encoding of the sentence, and is also a potential disadvantage of existing language models (which can otherwise generate locally-coherent samples).\nAnother direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem. There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24]. For example, Shen et al. [30] train with adversarial loss for unaligned style transfer between text by having the discriminator act on the RNN hidden states and using the soft outputs at each step as input to an RNN generator.\nVariational Autoencoders Ideally, autoencoders would learn useful coded feature representations of their inputs. However in practice simple autoencoders often learn a degenerate identity mapping where the latent code space is free of any structure. One way\u2014among others\u2014to regularize the code space is through having an explicit prior on the code space and using a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) [16, 27]. Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).\nA possible reason for the difficulty in training VAEs is due to the strictness of the prior (usually a spherical Gaussian) and/or the parameterization of the posterior. There has been some work on making\nthe prior/posterior more flexible through explicit parameterization [26, 15, 6]. One notable technique is adversarial autoencoders (AAE) [21] which attempt to imbue the model with a more flexible prior implicitly through adversarial training. In AAEs, the discriminator is trained to distinguish between samples from a fixed prior distribution and the input encoding, thereby pushing the code distribution to match the prior. Our approach has similar motivation, but notably we do not sample from a fixed prior distribution\u2014our \u2018prior\u2019 is instead parameterized through a generator. Nonetheless, this view (which has been observed by various researchers [32, 22, 20]) provides an interesting connection between VAEs and GANs."}, {"heading": "3 Background", "text": ""}, {"heading": "3.1 Generative Adversarial Networks", "text": "Generative Adversarial Networks (GANs) are a class of parameterized implicit generative models [9]. The method approximates drawing samples from a true distribution c \u223c Pr by instead employing a latent variable z and a parameterized deterministic generator function c\u0303 = g\u03b8(z) to produce generated samples c\u0303 \u223c Pg . The aim is to characterize the complex data manifold described by the unknown Pr within the latent space z.\nGAN training utilizes two separate models: a generator g(z) maps a latent vector from some easy-tosample source distribution to a value and a critic/discriminator f(c) aims to distinguish real data and fake samples, generated by g. The generator is trained to fool the critic, and the critic to separate out real from generated.\nIn this work, we utilize the recently-proposed Wasserstein GAN (WGAN) [1]. WGAN replaces the Jensen-Shannon divergence in standard GANs with Earth-Mover (Wasserstein-1) distance. WGAN training uses the following min-max optimization over generator parameters \u03b8 and critic parameters w,\nmin \u03b8 max w\u2208W\nEc\u223cPr [fw(c)]\u2212 Ec\u0303\u223cPg [fw(c\u0303)], (1)\nwhere fw : C 7\u2192 R denotes the critic function, c\u0303 is obtained from the generator, c\u0303 = g\u03b8(z), and Pr and Pg are real (true samples) and fake (generated samples) distribution respectively. Notably the critic parameters w are restricted to an 1-Lipschitz function setW , which can be shown to make this term correspond to Wasserstein-1 Distance. We follow [1] and use a naive implementation to approximately enforce this property by weight-clipping, i.e. w = [\u2212 , ]d. Throughout this work we only use g\u03b8 and fw as fully-connected networks, namely MLPs."}, {"heading": "3.2 Discrete Structure Autoencoders", "text": "An autoencoder is any model trained to map an input to a code space and then back to the original form. Ideally the code represents important abstracted features of the original input (although this is difficult to formalize) instead of learning to simply copy any given input.\nWe are interested in probabilistic autoencoders for discrete structures. Define X = Vn to be a discrete set of structures where V is a vocabulary of symbols. For instance, for binarized images V = {0, 1} and n is the number of pixels , or for sentences V = {1, . . . , #words} and n is the sentence length. A discrete structure autoencoder consists of two parameterized functions: a deterministic encoder function enc\u03c6 : X 7\u2192 C with parameters \u03c6 and a decoder distribution p\u03c8(x | c) with parameters \u03c8 that gives a distribution over structures X . The model is trained on cross-entropy reconstruction loss where we learn parameters to minimize the negative log-likelihood of reconstruction:\nLAE(\u03c6, \u03c8) = \u2212 log p\u03c8(x | enc\u03c6(x)) (2)\nComputing this for arbitrary sets is intractable, so the choice of p\u03c8 is important and problem specific. Finally it is often useful to use the decoder to produce a point estimate from X . We call this x\u0303 = argmaxx p\u03c8(x | enc\u03c6(x)) When x = x\u0303 the autoencoder is said to copy the input, or perfectly reconstruct x."}, {"heading": "4 Model", "text": "An adversarially regularized autoencoder (ARAE) combines a discrete autoencoder with a code-space GAN. Our model employs a discrete autoencoder to learn continuous codes based on discrete inputs\nand a WGAN to learn an implicit probabilistic model over these codes. The aim is to exploit the GAN\u2019s ability to learn the latent structure of code data, while using an autoencoder to abstract away the encoding and generation of discrete structure to support GAN training.\nThe main difference with WGANs as described above, is that we no longer have access to observed data samples for the GAN. Instead we have access to discrete structure x \u223c Px where Px is the distribution of interest. (Working with this space directly would require backpropagating through non-differentiable operations and is the basis for policy gradient methods for GAN training.) We handle this issue by integrating an encoder into the procedure which first maps x to a continuous code c = enc\u03c6(x), i.e. using the code vector for each observed structure defined by enc\u03c6 .\nThe full model has a three part objective. We minimize reconstruction error in the AE while employing adversarial training on its code space.\nmin (\u03c6,\u03c8)\nLAE(\u03c6, \u03c8) (3)\nmin w\u2208W max \u03c6 LWGAN-Cri(w, \u03c6) = min w\u2208W max \u03c6 Ex\u223cPx [fw(enc\u03c6(x))]\u2212 Ec\u0303\u223cPg [fw(c\u0303)] (4)\nmin \u03b8 LWGAN-Gen(\u03b8) = min \u03b8 Ec\u0303\u223cPg [fw(c\u0303)] (5)\nwhere Px is the real distribution in the input space. We minimize the three objectives all jointly in this work. Our model is visually depicted in Figure 1. The algorithm used for training is shown in Algorithm 1. We use block coordinate descent to optimize the AE, critic and generator in turn.\nNotably with this change we now receive gradients through the encoder from the adversarial loss.\nThis gradient will allow the encoder to help the generator to produce samples in the support of the true data learned by the WGAN critic. Theoretically, the effect of such term should decrease (and eventually diminish) as the GAN converges to a Nash-Equalibrium."}, {"heading": "5 Architectures", "text": "We consider two different instantiations of ARAEs, one for discrete images and the other for text sequences. For both models we use the same WGAN architecture but substitute in different autoencoder architectures. The generator architecture uses a low dimensional z with a Gaussian prior p(z) = N (0, I), and maps it to c. Both the critic fw and the generator g\u03b8 are parameterized as feed-forward MLPs. The structure of the deterministic encoder enc\u03c6 and probabilistic decoder p\u03c8 is specialized for the domain.\nImage Model Our first model uses a fully-connected neural network to encode binarized images. Here X = {0, 1}n where n is the image size. The encoder used is a feed-forward MLP network mapping from {0, 1}n 7\u2192 Rm, enc\u03c6(x) = MLP(x;\u03c6) = c. The decoder predicts each pixel in x as a parameterized logistic regression, p\u03c8(x | c) = \u220fn j=1 \u03c3(h) xj (1\u2212 \u03c3(h))1\u2212xj where h = MLP(c;\u03c8).\nAlgorithm 1 ARAE Training Procedure for number of training iterations do\nTrain the autoencoder Sample {x(i)}mi=1 \u223c Px a batch from the training data Compute the latent representations c(i) = enc\u03c6(x(i)) Compute the autoencoder loss, LAE = \u2212 1m \u2211m i=1 log p\u03c8(x\n(i) |c(i)), backpropagate gradients, update the decoder (\u03c8) and the encoder (\u03c6)\nTrain the critic for k steps do\nPositive sample phase Compute the adversarial loss on the real samples 1\nm \u2211m i=1 Ex\u223cPx [fw(c\n(i))] and , backpropagate gradients, update the critic (w) and the encoder (\u03c6)\nNegative sample phase Sample a batch of random noise {z(i)}mi=1 \u223c N (0, I) Generate code representation c\u0303(i) = g\u03b8(z(i)) by passing z(i) through the generator Compute the adversarial loss \u2212 1\nm \u2211m i=1 Ec\u0303\u223cPg [fw(c\u0303\n(i))], backpropagate gradients, update the critic (w) , clip the weights of the critic w to [\u2212 , ]d\nTrain the generator Sample a batch of random noise {z(i)}mi=1 \u223c N (0, I) Generate code representation c\u0303(i) = g\u03b8(z(i)) by passing z(i) through the generator Compute the generator loss 1\nm \u2211m i=1 Ec\u0303\u223cPg (fw(c\u0303\n(i))), backpropagate gradients through the critic into the generator to update the generator (\u03b8)\nText Model Our second model is devised for text. Here X = Vn where n is the sentence length and V is the vocabulary of the underlying language (typically |V| \u2208 [10k, 100k]). Following usual practice we use a recurrent neural network (RNN) as both the text encoder and decoder. Define an RNN as a parameterized recurrent function hj = RNN(xj ,hj\u22121;\u03c6) for j = 1 . . . n (with h0 = 0) that maps a discrete input structure x to hidden vectors h1 . . .hn. For the encoder, we define enc\u03c6(x) = hn = c, the last hidden state in this recurrence. The decoder is defined in a similar way, with parameters \u03c8. For prediction we combine c with hj to produce a distribution over V at each time step, p\u03c8(x | c) = \u220fn j=1 softmax(W[hj ; c] + b)xj where W and b are parameters (part of \u03c8). Finding the most likely sequence x\u0303 under this distribution is intractable, but we can approximate it using greedy search or beam search. In our experiments we use an LSTM architecture [13] for both the encoder/decoder, and train with teacher-forcing.\nSemi-Supervised Model Our model is trained in an unsupervised manner as a combination of an autoencoder and a GAN. As an extension we also consider the use case where the code vector is additionally used as input to a supervised classification task. As in the standard semi-supervised setup, we assume that our data consist of a small set of labeled data {xi, yi}i and large set of unlabeled data {xj}j . We can set up a standard supervised classification loss function using the code vectors from the encoder and a new set of parameters \u03b3:\nLNLL(\u03b3, \u03c6) = \u2211 i log p\u03b3(yi|enc\u03c6(xi)) (6)\nWe then extend our multi-task loss function to include this objective."}, {"heading": "6 Methods and Data", "text": "We consider two different settings for testing the ARAE: (1) images, utilizing the binarized version of MNIST, and (2) text, using the Stanford Natural Language Inference corpus [3]. This corpus provides a useful testbed as it comprises of sentences with relatively simple structure. The corpus is additionally annotated for pairwise sentence classification, which allows us to experiment with semi-supervised learning in a controlled setting. For this task the model is presented with two sentences\u2014premise and hypothesis\u2014and has to predict their relationship: entailment, contradiction, or neutral. For training, we used a subset of the corpus consisting of sentences of less than 15 words, although preliminary results suggest this approach works up to 30 words.\nWe consider several different empirical tasks to test the performance of the model as both an autoencoder (ARAE), by use the encoder aspect, and as latent-variable model (ARAE-GAN), by sampling z\u2019s (the two are trained identically). Experiments include: (1) code space structure; does the model preserve natural inputs x \u223c Px while not preserving noised inputs x\u2032; (2) semi-supervised learning; does the performance of a supervised model improve when it is additionally trained as an autoencoder; (3) sample generation; how well does a simple model do when trained on generated samples; (4) interpolation and arithmetic; how easily can we manipulate vectors in Z to smoothly control the generated text samples x\u0303.\nFor these experiments we compare to a standard AE, trained without the code-space GAN component, as well as a standard language model. We also attempted to train VAEs on the text dataset but found that it was unable to learn meaningful latent representations despite tuning the latent dimension size, KL annealing, and word dropout. Refer to the appendix for a detailed description of the hyperparameters, model architecture, and training regime."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Code Space Structure", "text": "As the code space we use by definition does not have the capacity to represent the entire discrete input space, ideally the autoencoder would learn to maintain valid representations for only real inputs which roughly exist along a low-dimensional manifold determined by the space of natural images or natural language sentences. This property is difficult to maintain in standard autoencoders, which often learn a partial identity mapping, but ideally should be improved by code space regularization. We test this property by passing two sets of samples through ARAE, one of true held-out samples and the other of explicitly-noised examples existing off this manifold.\nFigure 2 (left) shows these examples and their reconstruction from the discretized MNIST where the noised examples come from adding noise to the original image. For images we observe that a regular AE simply copies inputs, regardless of whether the input is on the data manifold. ARAE, on the other hand, will learn not reproduce the noised samples. Table 1 (right) shows similar experiments for text where we add noise by permuting k words in each sentence. Again we observe that the ARAE is able to map a noised sample back onto coherent sentences. Table 1 (left) shows empirical results for these experiments. We obtain the reconstruction error (i.e. negative log likelihood) of the original\n(non-noised) sentence under the decoder, utilizing the noised code. We find that when k = 0 (i.e. no swaps), the regular AE better reconstructs the input (as expected). However, as we increase the number of swaps and push the input further away from the data manifold, the ARAE is more likely to produce the original sentence.\nWe note that unlike denoising autoencoders which require a domain-specific noising function [11, 33], the ARAE is not explicitly trained to denoise an input, but learns to do so as a byproduct of adversarial regularization."}, {"heading": "7.2 Semi-Supervised Training", "text": "Next we utilize ARAE for semi-supervised training on a natural language inference task, shown in Table 2 (right). We experiment with using 22.2%, 10.8% and 5.25% of the original labeled training data, and use the rest of the training set for unlabeled AE training. The labeled set is randomly picked. The full SNLI training set contains 543k sentence pairs, and we use supervised sets of 120k, 59k and 28k sentence pairs respectively for the three settings. As a baseline we use an AE trained on the additional data, similar to the setting explored in [7]. For ARAE we use the subset of unsupervised data of length < 15, which roughly includes 655k single sentences (due to the length restriction, this is a subset of 715k sentences that were used for AE training). As observed by Dai and Le [7], training on unlabeled data with an AE objective improves upon a model just trained on labeled data. Training with adversarial regularization provides further gains.\nARAE-GAN Samples A woman preparing three fish . A woman is seeing a man in the river . There passes a woman near birds in the air . Some ten people is sitting through their office . The man got stolen with young dinner bag . Monks are running in court . The Two boys in glasses are all girl . The man is small sitting in two men that tell a children . The two children are eating the balloon animal . A woman is trying on a microscope . The dogs are sleeping in bed ."}, {"heading": "AE Samples", "text": "Two Three woman in a cart tearing over of a tree . A man is hugging and art . The fancy skier is starting under the drag cup in . A dog are <unk> a A man is not standing . The Boys in their swimming . A surfer and a couple waiting for a show . A couple is a kids at a barbecue . The motorcycles is in the ocean loading I \u2019s bike is on empty The actor was walking in a a small dog area . no dog is young their mother\nLM Samples a man walking outside on a dirt road , sitting on the dock . A large group of people is taking a photo for Christmas and at night . Someone is avoiding a soccer game . The man and woman are dressed for a movie . Person in an empty stadium pointing at a mountain . Two children and a little boy are <unk> a man in a blue shirt . A boy rides a bicycle . A girl is running another in the forest . the man is an indian women .\nFigure 3: Text samples generated from ARAE-GAN, a simple AE, and from a baseline LM trained on the same data. To generate from an AE we fit a multivariate Gaussian to the learned code space and generate code vectors from this Gaussian.\nA man is on the corner in a sport area . A man is on corner in a road all . A lady is on outside a racetrack . A lady is outside on a racetrack . A lot of people is outdoors in an urban setting . A lot of people is outdoors in an urban setting . A lot of people is outdoors in an urban setting .\nA man is on a ship path with the woman . A man is on a ship path with the woman . A man is passing on a bridge with the girl . A man is passing on a bridge with the girl . A man is passing on a bridge with the girl . A man is passing on a bridge with the dogs . A man is passing on a bridge with the dogs .\nA man in a cave is used an escalator . A man in a cave is used an escalator . A man in a cave is used chairs . A man in a number is used many equipment . A man in a number is posing so on a big rock . People are posing in a rural area . People are posing in a rural area.\nFigure 4: Sample interpolations from the ARAE-GAN. Constructed by linearly interpolating in the latent space and decoding to the output space. Word changes are highlighted in black."}, {"heading": "7.3 Sample Generation", "text": "A common test for a GANs ability to generate realistic samples that cover the original data space is to train a simple model on the samples from the GAN itself. Acknowledging the pitfalls of such quantitative evaluations [31], for text GANs we can do this by producing a large set of sampled sentences, and training a simple language model over the generations. For these experiments we generate 100k samples from (i) ARAE-GAN, (ii) an AE, (iii) a RNN LM trained on the same data, and (iv) the real training set. To \u201csample\u201d from an AE we fit a multivariate Gaussian to the code space (of the training data) after training the AE and generate code vectors from this Gaussian and decode back into sentence space. All models are of the same size to allow for fair comparison. Samples from the models are shown in Figure 3.\nWe subsequently train a standard RNN language model on the generated data and evaluate perplexity on held-out real data. The language model is of the same size as the decoder of the ARAE. As can be seen from Table 2 training on real data (understandably) outperforms training on generated data by a large margin. Surprisingly however, we find that a language model trained on ARAE-GAN data performs slightly better than one trained on LM-generated/AE-generated data."}, {"heading": "7.4 Interpolation and Vector Arithmetic", "text": "A widely observed property of GANs (and VAEs) is that the Gaussian prior p(z) induces the ability to smoothly interpolate between outputs by exploiting the structure of the latent space. While language models may provide a better estimate of the underlying probability space, constructing this style of interpolation would require combinatorial search, which makes this a useful feature of text GANs. We experiment with this property by sampling two points z0 and z1 from p(z) and constructing intermediary points z\u03bb = \u03bbz1+(1\u2212\u03bb)z0. For each we generate the argmax output x\u0303\u03bb. The samples are shown in Figure 4 for text and in Figure 2 (right-bottom) for MNIST. While it is difficult to assess the \u201caccuracy\u201d of these interpolations, we generally qualitatively observe smooth changes in the output sentences/images as we move from one latent space to another.\nAnother intriguing property of image GANs is the ability to move in the latent space via offset vectors (similar to the case with word vectors [23]). For example, Radford et al. [25] observe that when the mean latent vector for \u201cmen with glasses\u201d is subtracted from the mean latent vector for \u201cmen without glasses\u201d and applied to an image of a \u201cwoman without glasses\u201d, the resulting image is that of a \u201cwoman with glasses\u201d. We experiment to see if a similar property holds for sentences.\nWe generate 1 million sentences from the ARAE-GAN and parse the sentences to obtain the main verb, subject, and modifier. Then for a given sentence, to change the main verb we subtract the mean latent vector (t) for all other sentences with the same main verb (in the first example in Figure 5 this would correspond to all sentences that had \u201csleeping\u201d as the main verb) and add the mean latent vector for all sentences that have the desired transformation (with the running example this would be all sentences whose main verb was \u201cwalking\u201d). We do the same to transform the subject and the modifier. We decode back into sentence space with the transformed latent vector via sampling from p\u03c8(g(z+ t)). Some examples of successful transformations are shown in Figure 5 (right).\nQuantitative evaluation of the success of the vector transformations is given in Figure 5 (left). For each original vector z we sample 100 sentences from p\u03c8(g(z+ t)) over the transformed new latent vector and consider it a match if any of the sentences demonstrate the desired transformation. Match % is proportion of original vectors that yield a match post transformation. As we ideally want the generated samples to only differ in the specified transformation, we also calculate the average word precision against the original sentence (Prec) for any match."}, {"heading": "8 Conclusion", "text": "We present adversarially regularized autoencoders, as a simple approach for training a discrete structure autoencoder jointly with a code-space generative adversarial network. The model learns an improved autoencoder as demonstrated by semi-supervised experiments and analysis of the manifold structure for text and images. It also learns a useful generative model for text that exhibits a robust latent space, as demonstrated by natural interpolations and vector arithmetic. We however note that (as has been frequently observed when training GANs) our model seemed to be quite sensitive to hyperparameters. Finally, while many useful models for text generation already exist, text GANs provide a qualitatively different approach influenced by the underlying latent variable structure. We envision that such a framework could be extended to a conditional setting, combined with other existing decoding schemes, or used to provide a more interpretable model of language."}, {"heading": "Acknowledgment", "text": "We thank Sam Wiseman, Kyunghyun Cho, Sam Bowman, Joan Bruna, Yacine Jernite, Mart\u00edn Arjovsky, Mikael Henaff and Michael Mathieu for fruitful discussions. Yoon Kim is sponsored by a SYSTRAN research award. We also thank the NVIDIA Corporation for the donation of a Titan X Pascal GPU that was used for this research."}, {"heading": "Appendix: Experiments details", "text": "MNIST experiments \u2022 The encoder is a three-layer MLP, 784-800-400-100. The output of the encoder is nor-\nmalized onto a unit ball (having l2 norm 1), denoted as c \u2208 R100 before being forwarded further. Except for the output layer, batch normalization and ReLU are used following the linear layers. \u2022 We also add some additive Gaussian noise into c which is then fed into the decoder. The standard deviation of that noise is initialized to be 0.4, and then exponentially decayed to 0. \u2022 The decoder is a four-layer MLP, 100-400-800-1000-784 Except for the output layer, batch normalization and LeakyReLU (scale = 0.2) are used following the linear layers. \u2022 The autoencoder is optimized by Adam, with learning rate 5e-04. \u2022 The GAN employs a MLP generator, with structure 32-64-100-150-100. The noise vector\nis z \u2208 R32. \u2022 The GAN employs a MLP critic, with structure 100-100-60-20-1. The clipping factor = 0.05. The critic is trained with 10 iterations in each GAN loop. \u2022 The GAN is optimized by Adam, with learning rate 5e-04 on the generator, and 5e-05 on the critic. \u2022 When updating the encoder, we multiply the critic gradient by 0.2 before backpropping to the encoder.\nText experiments\nThe architecture we used for the text generation task is described blow:\n\u2022 The encoder is an one-layer LSTM with 300 hidden units. The output of the encoder is normalized onto a unit ball (having l2 norm 1), denoted as c \u2208 R300, before being forwarded further. \u2022 We add Gaussian noise into c before feeding it into the decoder. The standard deviation of that noise is initialized to be 0.2, and then exponentially decayed every 100 iterations by a factor of 0.995. \u2022 The decoder is a one-layer LSTM with 300 hidden units. \u2022 The decoding process at each time step takes the top layer LSTM hidden state and con-\ncatenates it with the hidden codes c, before feeding them into the output (i.e. vocabulary projection) and the softmax layer. \u2022 The word embedding is of size 300. \u2022 We adopt a grad clipping on the encoder/decoder, with max grad norm = 1. \u2022 The encoder/decoder is optimized by vanilla SGD with learning rate 1. \u2022 The GAN employs a MLP generator, with structure 100-300-300, batch normalization,\nand ReLU nonlineary. The noise vector z \u2208 R100. \u2022 The GAN employs a MLP critic, with structure 300-300-1, batch normalization, and\nLeakyReLU (with scale 0.2) nonlinearity. The clipping factor = 0.01. The critic is trained with 5 iterations in each GAN loop. \u2022 The GAN is optimized by Adam, with learning rate 5e-05 on the generator, and 1e-05 on\nthe critic. \u2022 When we update the encoder, we normalize both sources of gradients, i.e. from the critic\nand decoder, based on their norms. After that, a weight factor 0.01 is imposed on the critic backproped gradient. \u2022 We increment the number of GAN training loop1 by 1 (it initially is set to 1) , respectively at the beginning of epoch #2, epoch #4 and epoch #6. \u2022 We train for a total of 6 epochs.\n1The GAN training loop refers to how many times we train GAN in each entire training loop (one training loop contains training autoencoder for one loop, and training GAN for one or several).\nSemi-supervised experiments\nThe architecture we used for semi-supervised learning task is described blow:\n\u2022 The encoder is a three-layer LSTM, with the hidden state size being 300. The output of the encoder is normalized onto a unit ball (having l2 norm 1), denoted as c \u2208 R300, before being forwarded further. \u2022 The decoder is a one-layer LSTM, with the hidden state size being 300. \u2022 The decoding process at each time step takes the top layer LSTM hidden state and concate-\nnates it with the hidden codes c, before forward them to a word transition matrix. \u2022 The initial decoder hidden state is initialized with c, with a linear transformation. \u2022 The word embedding is of size 300. \u2022 We adopt a grad clipping on both LSTMs, with a maximum allowed gradient norm being 1. \u2022 The encoder/decoder is optimized by vanilla SGD, learning rate 1. \u2022 The GAN employs a MLP generator, with structure 100-150-300-500, batch normaliza-\ntion, and ReLU nonlinearity. The noise vector z \u2208 R100. \u2022 The GAN employs a MLP critic, with structure 500-500-150-80-20-1, batch normaliza-\ntion, and LeakyReLU (scale = 0.2) nonlinearity. The clipping factor = 0.02. The critic is trained with 10 iterations in each GAN loop. \u2022 The GAN is optimized by Adam, with learning rate 5e-05 on the generator, and 1e-05 on the critic. \u2022 When we update the encoder, we normalize both sources of gradients, i.e. from the critic and decoder, based on their norms. After that, we multiply the critic gradients by 0.01 before backpropping to the encoder..\nNote we use the same architecture for all three experiment settings (i.e. label set portion: Medium (22.2%), Small (10.8%), Tiny (5.5%)). The baseline models under comparison (Supervised Encoder, Semi-Supervised AE) use the same setting as described above."}], "references": [{"title": "Began: Boundary equilibrium generative adversarial networks", "author": ["David Berthelot", "Tom Schumm", "Luke Metz"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "author": ["Samuel R. Bowman", "Luke Vilnis", "Andrew M. Dai Oriol Vinyal", "Rafal Jozefowicz", "Samy Bengio"], "venue": "In Proceedings of CoNLL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Maximum-Likelihood Augment Discrete Generative Adversarial Networks", "author": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R Devon Hjelm", "Wenjie Li", "Yangqui Song", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Variational Lossy Autoencoder", "author": ["Xi Chen", "Diederik P. Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In Proceedings of ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Likelihood Ratio Gradient Estimation: An Overview", "author": ["Peter Glynn"], "venue": "In Proceedings of Winter Simulation Conference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Proceedings of NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improved Training of Wasserstein GANs", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Aaron Courville Vincent Dumoulin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "venue": "In Proceedings of NAACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Athul Paul Jacob", "author": ["R Devon Hjelm"], "venue": "Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundary-Seeking Generative Adversarial Networks. arXiv:1702.08431", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "venue": "In Proceedings of ICLR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Improving Variational Inference with Autoregressive Flow", "author": ["Diederik P. Kingma", "Tim Salimans", "Max Welling"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Auto-Encoding Variational Bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In Proceedings of ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "GANs for Sequences of Discrete Elements with the Gumbel-Softmax Distribution", "author": ["Matt Kusner", "Jose Miguel Hernandez-Lobato"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Adversarial Learning for Neural Dialogue Generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "S\u00e9bastien Jean", "Alan Ritter", "Dan Jurafsky"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "author": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Adversarial Variational Bayes", "author": ["Lars Mescheder", "Sebastian Nowozin", "Andreas Geiger"], "venue": "Unifying Variational Autoencoders and Generative Adversarial Networks", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Scott Wen tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Language Generation with Recurrent Generative Adversarial Networks without Pre-training", "author": ["Ofir Press", "Amir Bar", "Ben Bogin", "Jonathan Berant", "Lior Wolf"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In Proceedings of ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Variational Inference with Normalizing Flows", "author": ["Danilo J. Rezende", "Shakir Mohamed"], "venue": "In Proceedings of ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In Proceedings of ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A Hybrid Convolutional Variational Autoencoder for Text Generation", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "author": ["Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "In Proceedings of ICLR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Simple Statistical Gradient-following Algorithms for Connectionist Reinforcement Learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1992}, {"title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions", "author": ["Zichao Yang", "Zhiting Hu", "Ruslan Salakhutdinov", "Taylor Berg-Kirkpatrick"], "venue": "In Proceedings of ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": "In Proceedings of AAAI,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 237, "endOffset": 251}, {"referenceID": 21, "context": "Recent work on generative adversarial networks (GANs) [9] and other deep latent variable models has shown significant progress in learning smooth latent variable representations of complex, highdimensional continuous data such as images [1, 2, 25, 37].", "startOffset": 237, "endOffset": 251}, {"referenceID": 3, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 10, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 30, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 15, "context": "methods [5, 12, 36] or with the Gumbel-Softmax distribution [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "A similar property was observed in image GANs [25] and word representations [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "A similar property was observed in image GANs [25] and word representations [23].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space [8, 34].", "startOffset": 149, "endOffset": 156}, {"referenceID": 28, "context": "Policy gradient methods are a natural way to deal with the resulting non-differentiable generator objective when training directly in discrete space [8, 34].", "startOffset": 149, "endOffset": 156}, {"referenceID": 3, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 30, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 16, "context": "language modeling) objective [5, 36, 18].", "startOffset": 29, "endOffset": 40}, {"referenceID": 12, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 119, "endOffset": 127}, {"referenceID": 15, "context": "Another direction of work has been through reparameterizing the categorical distribution with the Gumbel-Softmax trick [14, 19]\u2014while initial experiments were encouraging on a synthetic task [17], scaling them to work on natural language is a challenging open problem.", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 25, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 20, "context": "There has also been a flurry of recent, related approaches that work directly with the soft outputs from a generator [10, 28, 30, 24].", "startOffset": 117, "endOffset": 133}, {"referenceID": 25, "context": "[30] train with adversarial loss for unaligned style transfer between text by having the discriminator act on the RNN hidden states and using the soft outputs at each step as input to an RNN generator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "One way\u2014among others\u2014to regularize the code space is through having an explicit prior on the code space and using a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) [16, 27].", "startOffset": 228, "endOffset": 236}, {"referenceID": 23, "context": "One way\u2014among others\u2014to regularize the code space is through having an explicit prior on the code space and using a variational approximation to the posterior, leading to a family of models called variational autoencoders (VAE) [16, 27].", "startOffset": 228, "endOffset": 236}, {"referenceID": 2, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 169, "endOffset": 172}, {"referenceID": 24, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 319, "endOffset": 327}, {"referenceID": 29, "context": "Unfortunately VAEs for text can be challenging to train\u2014for example, if the training procedure is not carefully tuned with techniques like word dropout and KL annealing [4], the decoder simply becomes a language model and ignores the latent code (although there has been some recent successes with convolutional models [29, 35]).", "startOffset": 319, "endOffset": 327}, {"referenceID": 22, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 13, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 4, "context": "the prior/posterior more flexible through explicit parameterization [26, 15, 6].", "startOffset": 68, "endOffset": 79}, {"referenceID": 18, "context": "Nonetheless, this view (which has been observed by various researchers [32, 22, 20]) provides an interesting connection between VAEs and GANs.", "startOffset": 71, "endOffset": 83}, {"referenceID": 7, "context": "Generative Adversarial Networks (GANs) are a class of parameterized implicit generative models [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "In our experiments we use an LSTM architecture [13] for both the encoder/decoder, and train with teacher-forcing.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "We consider two different settings for testing the ARAE: (1) images, utilizing the binarized version of MNIST, and (2) text, using the Stanford Natural Language Inference corpus [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "We note that unlike denoising autoencoders which require a domain-specific noising function [11, 33], the ARAE is not explicitly trained to denoise an input, but learns to do so as a byproduct of adversarial regularization.", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "We note that unlike denoising autoencoders which require a domain-specific noising function [11, 33], the ARAE is not explicitly trained to denoise an input, but learns to do so as a byproduct of adversarial regularization.", "startOffset": 92, "endOffset": 100}, {"referenceID": 5, "context": "As a baseline we use an AE trained on the additional data, similar to the setting explored in [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "As observed by Dai and Le [7], training on unlabeled data with an AE objective improves upon a model just trained on labeled data.", "startOffset": 26, "endOffset": 29}, {"referenceID": 26, "context": "Acknowledging the pitfalls of such quantitative evaluations [31], for text GANs we can do this by producing a large set of sampled sentences, and training a simple language model over the generations.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Another intriguing property of image GANs is the ability to move in the latent space via offset vectors (similar to the case with word vectors [23]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "[25] observe that when the mean latent vector for \u201cmen with glasses\u201d is subtracted from the mean latent vector for \u201cmen without glasses\u201d and applied to an image of a \u201cwoman without glasses\u201d, the resulting image is that of a \u201cwoman with glasses\u201d.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Generative adversarial networks are an effective approach for learning rich latent representations of continuous data, but have proven difficult to apply directly to discrete structured data, such as text sequences or discretized images. Ideally we could encode discrete structures in a continuous code space to avoid this problem, but it is difficult to learn an appropriate general-purpose encoder. In this work, we consider a simple approach for handling these two challenges jointly, employing a discrete structure autoencoder with a code space regularized by generative adversarial training. The model learns a smooth regularized code space while still being able to model the underlying data, and can be used as a discrete GAN with the ability to generate coherent discrete outputs from continuous samples. We demonstrate empirically how key properties of the data are captured in the model\u2019s latent space, and evaluate the model itself on the tasks of discrete image generation, text generation, and semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}