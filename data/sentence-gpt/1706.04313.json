{"id": "1706.04313", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Teaching Compositionality to CNNs", "abstract": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application-specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks. For example, we use the same algorithm to train CNNs on the task of forming a list of objects using a different set of training options, in order to identify the possible target features.\n\n\n\n\n\n\n\nThe neural network can provide a comprehensive range of neural networks which are the most generalized and are used as model networks to better predict behavior when trained.\nThe model can be derived from a neural network of approximately 7 million neurons, where each neuron is represented as a discrete network representing a single network: a typical network for all tasks, and a network representing a single network for a single task.\nFor each of the 3.6 million neurons, we construct an additional 4 million neurons for each of these neurons. Each neuron has a unique network, with a special unique layer, where the input layer can be used for all tasks. For each of the 3.6 million neurons, we construct an additional 4 million neurons for each of these neurons. Each neuron has a unique network, with a special layer, where the input layer can be used for all tasks. For each of the 3.6 million neurons, we construct an additional 4 million neurons for each of these neurons. Each neuron has a unique network, with a special layer, where the input layer can be used for all tasks. For each of the 3.6 million neurons, we construct an additional 4 million neurons for each of these neurons. Each neuron has a unique network, with a special layer, where the input layer can be used for all tasks. For each of the 3.6 million neurons, we construct an additional 4 million neurons for each of these neurons. Each neuron has a unique network, with a special layer, where the input layer can be used for all tasks. For each of the 3.6 million", "histories": [["v1", "Wed, 14 Jun 2017 04:34:59 GMT  (9418kb,D)", "http://arxiv.org/abs/1706.04313v1", "Preprint appearing in CVPR 2017"]], "COMMENTS": "Preprint appearing in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["austin stone", "huayan wang", "michael stark", "yi liu", "d scott phoenix", "dileep george"], "accepted": false, "id": "1706.04313"}, "pdf": {"name": "1706.04313.pdf", "metadata": {"source": "CRF", "title": "Teaching Compositionality to CNNs\u2217", "authors": ["Austin Stone", "Yi Liu", "Huayan Wang", "D. Scott Phoenix", "Michael Stark", "Dileep George"], "emails": ["dileep}@vicarious.com"], "sections": [{"heading": "1. Introduction", "text": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6]. Their success is typically attributed to two factors; they have large enough capacity to make effective use of the ever-increasing amount of image training data available today, while at the same time managing the number of free parameters through the use of inductive biases from neuroscience. Specifically, the interleaving of locally connected filter and pooling layers [15] bears similarity to the visual cortex\u2019s interleaving of simple cells, which have localized receptive fields, and complex cells, which have wider receptive fields and greater local invariance.\nRecently, researchers have investigated more inductive biases from neuroscience to improve CNN architectures. Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-\n\u2217Preprint appearing in CVPR 2017.\nformation [14], and using physical interaction with the environment [31] to bias representations.\nIn this paper, we follow a similar philosophy, but focus our attention on the inductive bias of compositionality: the notion that the representation of the whole should be composed of the representation of its parts (we give a precise formal definition of this notion in Sect. 3). Intuitively, encouraging this property during training results in representations that are more robust to re-combination (e.g., when seeing a familiar object in a novel context) and less prone to focusing on discriminative but irrelevant background features. It is also in line with findings from neuroscience that suggest separate processing of figure and ground regions in the visual cortex [16, 32]. Note that a typical CNN does not exhibit this property (Fig. 1 visualizes the difference in activations between a CNN trained without (VGG [37]) and with our compositionality objective1).\n1Fig. 1 shows the activation difference in the airplane region between the current frame and a frame where the airplane is shown in isolation. Activations are taken from intermediate conv. layers with spatial resolution 28\u00d7 28. We marginalize over feature channels to create visualization.\n1\nar X\niv :1\n70 6.\n04 31\n3v 1\n[ cs\n.C V\n] 1\n4 Ju\nn 20\nIn contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner. While our current implementation requires object masks for training, it allows apples to apples comparison of networks trained with or without the compositionality objective. As our experiments show (Sect. 4), the objective consistently improves performance over non-compositional baselines.\nThis paper makes the following specific contributions: First, we introduce a novel notion of compositionality as an inductive bias for training arbitrary convolutional neural networks (CNNs). It captures the intuition that the representation of a partial image should be equal to the partial representation of that image. Second, we implement that notion in the form of a modified CNN training objective, which we show to be straightforward to optimize yet effective in learning compositional representations. Third, we give an extensive experimental evaluation on both synthetic and real-world images that highlights the efficacy of our approach for object recognition tasks and demonstrates the contributions of different components of our objective."}, {"heading": "2. Related work", "text": "Our work is related mostly to three major lines of research: compositional models, inductive biases, and the role of context in visual recognition.\nCompositional models. Compositional models have existed since the early days of computer vision [24] and have appeared mainly in two different varieties. The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45]. The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].\nThe basis for our work is a notion of compositionality (Sect. 3.1) that is distinct from all these approaches in that it does not have to be baked into the design of a model but can be applied as a soft constraint to a CNN. Recent work [28] constrains CNN activations to lie within object masks in the context of weakly-supervised localization. Our compositional objective (Sect. 3.3) goes beyond this formulation: it consists of multiple components that not only suppress background activations, but also explicitly encourage object activations to be invariant to both background clutter and adjacent objects. Our experiments verify that each component is important for performance (Sect. 4.3).\nInductive biases. A recent line of work on neural network architectures takes inspiration from human learning in its design of training regimen. It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31]. The benefit arises from providing helpful inductive biases to the learner that regularize the learned representations. The inductive bias of compositionality presented in this work (Sect. 3.1) follows a similar motivation but is largely complementary to the biases explored by these prior approaches.\nThe role of context in visual recognition. It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5]. Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46]. At the extreme, object classes can be successfully recognized even in the absence of local information by relying exclusively on scene context [33].\nWhile CNN-based representations typically support the use of context implicitly (by including pixels indiscriminately in a receptive field), they lack the ability to explicitly address context and non-context information. The notion of compositionality proposed in this work (Sect. 3.1) is a step towards making CNN-based representations more amenable to explicit context modeling through an external mechanism (by cleanly separating the representation of objects from their context). The experiments in this paper (Sect. 4) do not further elaborate on this aspect, but indicate that the compositional objective (i) elicits a performance improvement, (ii) the improvement is similar for objects appearing in and out-of-context, and (iii) the improvement is least pronounced for very small object instances."}, {"heading": "3. Teaching compositionality to CNNs", "text": "This section describes our approach to encouraging CNNs to learn compositional representations. To that end, we proceed from introducing our notion of compositionality (Sect. 3.1) to describing network architecture (Sect. 3.2) and training procedure (Sect. 3.3) to giving technical details of our implementation (Sect. 3.4)."}, {"heading": "3.1. Compositionality notion", "text": "The goal of our notion of compositionality is to encourage the representation of a part of an image to be similar to the corresponding part of the representation of that image. More formally, let X be an image, m a binary mask that\nidentifies part of X (i.e, m is a tensor of the same shape as X with 1s indicating part affiliation), \u03c6 a mapping from an image onto an arbitrary feature layer of a CNN, and p the projection operator onto the feature map represented by \u03c6. We define \u03c6 to be compositional iff the following holds:\n\u03c6(m \u00b7X) = p(m) \u00b7 \u03c6(X). (1)\nHere, the \u00b7 operator represents element-wise multiplication. The projection operator, p, down-samples the object mask to the size of the output of \u03c6. E.g., if \u03c6(X) is the activations of a convolutional layer with size (h,w, c) (the first two dimensions are spatial and c is the number of feature channels), p will down-sample the object mask to size (h,w) and then stack c copies of the down-sized object mask on top of each other to produce a mask of size (h,w, c).\nNote that in practice we do not require Eq. (1) to hold for all possible masks m, as this would constrain \u03c6 to be the identity map. Instead, we apply the inductive bias selectively to image parts that we would like to be treated as a unit \u2013 obvious choices for these selected parts include objects or object parts. In the following, we use object masks (as provided by standard data sets such as MS-COCO [23]) as the basis for compositionality."}, {"heading": "3.2. Enhanced network architecture", "text": "To encourage a network to satisfy the compositionality property of Eq. (1) (Sect. 3.1), we devise an enhanced architecture and corresponding objective function. Note that this enhancement is non-destructive in nature and leaves the\noriginal network completely intact; it merely makes virtual copies of the original network, Fig. 2.\nWhen there is only one object in the input image, teaching compositionality takes the form of ensuring that the activations within the region of that object remain invariant regardless of what background the object appears on. With multiple objects, we also explicitly ensure that the activations of each object remain the same as if that object were shown in isolation (i.e., activations should be invariant to the other objects within the respective object mask).\nTo implement this notion, we create K + 1 weightsharing CNNs where K is the number of objects shown in the scene. K of these CNNs take as input a different object instance, each shown against a blank background (we apply the mask for the kth object instance to the input image before giving the input image to the kth CNN). We refer to these K CNNs as \u201cmasked CNNs,\u201d and we denote the mapping onto layer n of the kth masked CNN as \u03c6mk,n.\nEach of these K masked CNNs have their respective object mask reapplied to their activations at multiple layers in the hierarchy (see Sect. 3.4), zeroing out activations outside of the object region. These masked activations are then passed on to higher layers (which might also re-apply the mask again in the same way). This constrains the masked CNNs to only use activations within the object mask region when classifying the input image. The final (K+1th) CNN receives as input the original image with no masks applied, and we refer to it subsequently as the \u201cunmasked CNN\u201d. We denote the mapping onto layer n of this CNN as \u03c6u,n. We denote the total number of layers as N ."}, {"heading": "3.3. Training procedure", "text": "We train the architecture of Sect. 3.2 for compositionality by introducing an objective function that combines an application-specific discriminative CNN loss with additional terms that establish dependencies between the different masked and unmasked CNNs.\nDiscriminative loss. To encourage correct discrimination, we add separate discriminative loss terms for both the K masked and the one unmasked CNN, denoted Lmk and Lu, respectively. Their relative contributions are controlled by the hyperparameter \u03b3 \u2208 [0, 1], to yield\nLd = 1\nK\n(\u2211\nk\n\u03b3Lmk ) + (1\u2212 \u03b3)Lu. (2)\nCompositional loss. To encourage compositionality, we add K \u00d7 N terms that establish dependencies between the responses of corresponding layers of the masked and unmasked CNNs, respectively. Specifically, on all layers at which an object mask is applied, we take the l2 difference\nbetween the activations of the masked CNN and the activations of the unmasked CNN. We then multiply this difference by a layer specific penalty hyper-parameter (denoted as \u03bbn) and add this to our compositional loss:\nLc = 1\nK\n\u2211\nk\n\u2211\nn\n\u03bbn||\u03c6mk,n \u2212 \u03c6u,nm\u2032k||22. (3)\nThe final objective can then be stated simply as L = Ld + Lc. Because the unmasked CNN sees all objects and will naturally have different activations from the kth masked CNN due to the presence of the objects other than the kth object, we apply a mask to the unmasked CNN\u2019s activations before computing the penalty term. We denote this mask as m\u2032k. However, we do not pass these masked activations (\u03c6u,nm\u2032k) up to higher layers as was done for the masked CNNs; we only use them to compute the compositional penalty term on layer n.\nDesign choices. The above objective leaves degrees of freedom w.r.t. choosing the precise nature of the masksm\u2032k, and the corresponding choices do have an impact on performance (Sect. 4.3). First, to penalize background activations outside of the regions of objects of interest, we can make m\u2032k be a tensor of 1s but with the locations of all objects other than the kth object filled with 0s. Second, we can penalize any shifts in activations within the region of the kth object without discouraging background activations by making m\u2032k equal to mk."}, {"heading": "3.4. Implementation details", "text": "Our experiments (Sect. 4) use the following network architectures: MS-COCO-sub (Sect. 4.4): conv1-conv3 (224\u00d7224\u00d764), pool1, conv4-conv6 (128\u00d7 128\u00d7 128), pool2, conv7-conv9 (64\u00d7 64\u00d7 256), pool3, conv10-conv12 (32\u00d7 32\u00d7 512), pool4, fc1 (131072\u00d7 20). 3D-Single (Sect. 4.3): conv1-conv3 (128\u00d7128\u00d764), pool1, conv4-conv6 (64 \u00d7 64 \u00d7 128), pool2, conv7-conv9 (32 \u00d7 32 \u00d7 256), pool3, conv10conv12 (16 \u00d7 16 \u00d7 512), pool4, fc1 (32768 \u00d7 14). MNIST (Sect. 4.3): conv1-conv3 (120\u00d7120\u00d732), pool1, conv3-conv4 (60\u00d760\u00d764), pool2, conv5-conv6 (30\u00d7 30\u00d7 128), pool3, fc1 (28800\u00d7 10).\nThe discriminative loss functionsLmk andLu are instantiated as softmax-cross entropy or element-wise sigmoidcross entropy for joint or independent class prediction, respectively. Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).\nEmpirically, we find that best performance is achieved when applying Lc to the topmost convolutional and pooling layers of the network (i.e., \u03bbn is zero on most early layers). We believe this to be an artifact of the CNN needing a certain minimum number of layers and corresponding representational power to successfully discriminate between relevant and irrelevant (background) pixels.\nIn practice, we create only two weight-sharing CNNs (independent of the number of object training instances): one which sees 1 randomly selected out of K objects in the input image, and one which sees the entire scene. Empirically, this model is only about 50% slower to train than a standard CNN. The parameter space is just that of a single CNN due to weight sharing. \u03b3 is fixed to .5."}, {"heading": "4. Experiments", "text": "In this section, we give a detailed experimental evaluation of our approach for teaching compositionality to CNNs, highlighting its ability to improve performance over standard CNN training on both synthetic (Sect. 4.3) and real images (MS-COCO [23], Sect. 4.4). Our emphasis lies on providing an in-depth analysis of the contributions of different components of our compositional objective as well as quantifying the impact of object context on performance."}, {"heading": "4.1. Datasets and metrics", "text": "Rendered 3D objects. We perform diagnostic experiments on two novel datasets of rendered 3D objects. We use rendered datasets so we have maximum control over the statistics of our image data in terms of depicted objects and context (notably, segmentation masks come for free in this setting). Specifically, the datasets are based on 12 3D object classes (e.g., car, bus, boat, or airplane), with \u2248 20 object instances per category, each rendered from \u2248 50 different viewpoints (uniform sampling of the upper viewing halfsphere) in front of 20 different real-image backgrounds.\nThe first dataset, termed 3D-Single, has 1, 600 images depicting single objects in front of random backgrounds. The second dataset, termed 3D-Multi, has 800 images of multiple objects with varying degrees of occlusion (see Fig. 1). For both datasets, we distinguish between a category-level recognition setting and easier variants (3DSingle-Inst, 3D-Multi-Inst) that allow the set of 3D object instances seen during training and test to be non-empty (whereas the set of views of the same instance has to be empty). In both cases, we ensure that the backgrounds seen in training (80% of the images) and test (20%) are distinct.\nMNIST. We create two variants of the popular MNIST dataset [22], in analogy to the two aforementioned 3D object datasets. The first variant, MNIST-Single, depicts individual MNIST characters in front of randomized, cluttered backgrounds (we use the standard train/test split). The second variant, MNIST-Multi, depicts multiple characters with varying degrees of overlap, against these backgrounds.\nMS-COCO-sub. MS-COCO [23] constitutes a move away from \u201ciconic\u201d views of objects towards a dataset in which objects frequently occur in their respective natural\ncontexts. For most experiments, we focus on subsets of MSCOCO training and validation (for testing) images that contain at least one of 20 diverse object classes2 (see Fig. 4a) and further restrict the set of images to ones with sufficiently large object instances of at least 7, 000 pixels. This results in 22, 476 training and 12, 245 test images.\nIn addition, we quantify the impact of context on classification performance by defining two further test sets. The first test set is the full validation set of MS-COCO. Here, we measure classification performance on object instances of different sizes (small, medium, large) as defined for the MS-COCO detection challenge3. In order to make the performance comparable, we stratify the number of positive and negative examples by randomly sampling 20 negatives for each positive example.\nThe second test set examines object instances in and out of context (see Fig. 5 (b)). We start with all test images from MS-COCO-sub. For each object instance o of a category c occurring in that set, we create two positive examples, one by cropping o and placing it in front of a new random test image that does not have c in it (this will be the out-ofcontext set), and one by leaving o in its original context (the in-context set). For both, we add as negative examples all images where c does not occur.\nMetrics. All experiments consider image-level classification tasks, not object class detection or localization. For diagnostic experiments (Sect. 4.3), we evaluate performance as the average fraction of correctly predicted object classes among the top-k scoring network predictions, where k is the number of objects in a given image. For MS-COCOsub (Sect. 4.4), we treat object classes separately and report (mean) average precision (AP) over independent binary classification problems. In all cases, we monitor performance on a held-out test set over different epochs as training progresses, and report both the resulting curves and best achieved values per method (Fig. 3, Fig 4)."}, {"heading": "4.2. Methods", "text": "In this section, we evaluate the following baselines and variations on our compositional training technique (see Sect. 3.3). For the sake of clean comparison, we always train all networks from scratch (i.e., we do not use pretraining of any form). COMP-FULL. Our main architecture, where m\u2032k is chosen to be equal to a block of all 1s but with the locations of objects other than the kth object set to 0s. COMP-OBJ-ONLY. Like COMP-FULL, but with m\u2032k equal to mk (this penalizes any shifts in activations within the object region but does not discourage background activations). COMP-NO-MASK. Like COMP-FULL, except that the\n2The first 20 classes in the original MS-COCO ordering w/o person. 3http://mscoco.org/dataset/#detections-eval\nmasked CNNs do not apply mk to any of their activations. BASELINE. Architecture with the same layer sizes as COMP-FULL but without compositional objective terms \u2013 a \u201cstandard\u201d CNN. BASELINE-AUG. Like BASELINE, except for each batch we make half of the images be a single object shown in isolation against a black background and the other half be the raw images of the same objects in the same locations against cluttered background. This method has access to the same information as COMP-FULL (it knows about the object mask), but without any compositional objective. BASELINE-REG. Like BASELINE, but with dropout [40] and l2-regularization. BASELINE-AUG-REG. Like BASELINE-AUG, but with dropout and l2-regularization."}, {"heading": "4.3. Diagnostic experiments on synthetic data", "text": "We commence by comparing the performance of different variants of our compositional objective and the corresponding baselines (Sect. 4.2) in a diagnostic setting on synthetic data. In order to assess both best case performance and convergence behavior, we plot test performance vs. training epochs in Fig. 3a through 3f. The respective best performance per curve is given in parentheses in plot legends. Fig. 5 and 6 give qualitative results.\nRendered 3D objects. In Fig. 3, we observe that all variants of compositional CNNs (blue curves) perform consistently better than the baselines (red curves), both per epoch and in terms of best case performance.\nOur full model, COMP-FULL, performs overall best (blue-solid). It outperforms the best baseline by between 17.1% (3D-Multi, Fig. 3d) and 35.2% (3D-Single-Inst, Fig. 3a). Performance drops for COMP-OBJ-ONLY (bluedashed) by 14.7% (3D-Single-Inst, Fig. 3a), 7.3% (3DSingle, Fig. 3b), 4.4% (3D-Multi-Inst, Fig. 3c), and 2.9% (3D-Multi, Fig. 3d), respectively. COMP-NO-MASK (bluedotted) performs worst among our models, but still outperforms the best baseline by 0.3% (3D-Single-Inst, Fig. 3a), 6.8% (3D-Single, Fig. 3b), 26.6% (3D-Multi-Inst, Fig. 3c), and 17.0% (3D-Multi, Fig. 3d), respectively.\nAs expected, the baseline benefits from observing additional masked training data mostly for images with multiple objects: BASELINE (red-dashed) and BASELINE-AUG (red-solid) perform comparably on 3D-Single-Inst and 3DSingle, but BASELINE-AUG improves over BASELINE on 3D-Multi-Inst (by 6.2%) and 3D-Multi (by 7.8%). In terms of convergence, the compositional CNNs (blue curves) tend to stabilize later than the baselines (red curves).\nMNIST. In Fig. 3e and 3f, the absolute performance differences between our compositional CNNs and the corresponding baselines are less clear cut, but still highlight\nthe importance of the compositional objective when object masks are used (COMP-FULL outperforms BASELINE-AUG by 2.0% on MNIST-Single and by 2.6% on MNIST-Multi). Without reapplying the masks to the activations, performance decreases, but the trend remains (COMP-NO-MASK is better than BASELINE by 20.4% and 4.1%, respectively)."}, {"heading": "4.4. Experiments on real-world data (MS-COCO)", "text": "We proceed to evaluating our best performing method COMP-FULL on the real-world images of MS-COCO (Sect. 4.1). We compare to the same baselines as before, plus two baselines with dropout [40] and l2-regularization (see Sect. 4.2). Specifically, we report performance for COMP-FULL at convergence (last epoch, see Fig. 3g for convergence behavior); for all baselines we consider the best performing model over all epochs. Fig. 4 gives details w.r.t. individual object categories (4a), amount of training data (4b), size of object instances (4c), and context (4d).\nMS-COCO-sub. In Fig. 3g, COMP-FULL (blue-solid) outperforms the best baseline BASELINE-AUG-REG (orange-solid) by a significant margin of 25.5%, confirming the benefit of the compositionality objective in a real-world setting. The added regularization improves the performance of the baselines only moderately, by 4.6% (BASELINE-AUG) and 4.1% (BASELINE), respectively. In Fig. 4a, we see that COMP-FULL performs better than the baselines for every single category, improving performance by up to 32% (for stop sign). Fig. 4b gives results for varying amounts of training data (5, 10, 20, 50, 75, 100%).\nCOMP-FULL (blue-solid) clearly outperforms the baselines (orange and red curves) for all plotted amounts, with an increasing performance gap as training data increases.\nObject sizes and context. Fig. 4c gives the performance when testing the respective models trained on the training portion of MS-COCO-sub on all images of MS-COCO and evaluating them on object instances of different sizes (small, medium, large, all; see Sect. 4.1).\nWe observe that the compositional objective improves performance over the baselines consistently for all sizes. The improvement is most pronounced for large object instances (25%, COMP-FULL vs. BASELINE-AUG-REG), decreases for medium (9%, BASELINE-REG), and almost vanishes for small objects (3%, BASELINE-REG). This ordering is in line with the intuition that the compositional objective encourages activations to be context invariant: as context becomes more important with decreasing object size, the advantage of context invariance decreases.\nFig. 4d explicitly examines the role of context, by comparing the performance on the in- (Fig. 4d (bottom)) and out-of-context (Fig. 4d (top)) test sets defined in Sect. 4.1\n(Fig. 5 (b) gives examples). Indeed, COMP-FULL improves performance over the baselines in all cases: COMP-FULL outperforms BASELINE-AUG-REG by 27.1% (in-context) and BASELINE-AUG-REG by 11.2% (out-of-context). The relative performance ratio (Tab. 1) between in- and out-ofcontext objects is slightly more favorable for COMP-FULL (0.39) than for BASELINE-AUG-REG (0.37).\nLocalization accuracy. Fig. 5 and 6 give qualitative results that highlight two distinct properties of our compositional objective COMP-FULL. First, it leads to bottomup network activations that are better localized than for BASELINE-AUG, as indicated visually by the differences in masked and unmasked activations (Fig. 5). Second, it also leads to better localization when backtracing classification decisions to the input images, which we implement by applying guided backpropagation [39] (Fig. 6). Fig. 3h quantifies this on all test images of MS-COCO-sub, by computing the percentage of \u201cmass\u201d of the back-trace heat-map inside the ground-truth mask of the back-traced category, averaged over categories. COMP-FULL outperforms both BASELINEAUG and VGG [37] by considerable margins.\nDiscussion. To our knowledge, only [28] reports classification (not detection) performance on MS-COCO, achieving 62.8% mAP on the full set of 80 classes using fixed lower-layer weights from ImageNet pre-training [20] and an elaborate multi-scale, sliding-window network architecture.\nIn comparison, our COMP-FULL achieves 34% on 20 classes (Fig. 4c, \u2019all\u2019 column) when trained entirely from scratch using only a small fraction of the full data (6% with area above 7, 000) and a single, fixed scale window (the original image), outperforming the best baseline BASELINE-REG by 17%. We believe this to be an encouraging result that is complementary to the gains reported by [28] and leave the combination of both as a promising avenue for future work."}, {"heading": "5. Conclusion", "text": "We have introduced an enhanced CNN architecture and novel loss function based on the inductive bias of compositionality. It follows the intuition that the representation of part of an image should be similar to the corresponding part of the representation of that image and is implemented as additional layers and connections of an existing CNN.\nOur experiments indicate that the compositionality bias aids in the learning of representations that generalize better when training networks from scratch, and improves the performance in object recognition tasks on both synthetic and real-world data. Obvious next steps include the application to tasks that explicitly require spatial localization, such as image parsing, and combination with pre-trained networks.\nAcknowledgments. We thank John Bauer and Robert Hafner for support with experiment infrastructure."}], "references": [{"title": "Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In ICCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Context models and out-of-context objects", "author": ["M.J. Choi", "A. Torralba", "A.S. Willsky"], "venue": "Pattern Recognition Letters,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Contextual object detection using set-based classification", "author": ["R.G. Cinbis", "S. Sclarof"], "venue": "In ECCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "An empirical study of context in object detection", "author": ["S.K. Divvala", "D. Hoiem", "J.H. Hays", "A.A. Efros", "M. Hebert"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S.M.A. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "D. Szepesvari", "K. Kavukcuoglu", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Recognition using visual phrases", "author": ["A. Farhadi", "M.A. Sadeghi"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Towards scalable representations of object categories: Learning a hierarchy of parts", "author": ["S. Fidler", "A. Leonardis"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Context based object categorization: A critical survey", "author": ["C. Galleguillos", "S. Belongie"], "venue": "CVIU,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Object-centric representation learning from unlabeled videos", "author": ["R. Gao", "D. Jayaraman", "K. Grauman"], "venue": "In ACCV,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Exploring person context and local scene context for object detection", "author": ["S. Gupta", "B. Hariharan", "J. Malik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbelaez", "R. Girshick", "J. Malik"], "venue": "In ECCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbelez", "R. Girshick", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Learning with side information through modality hallucination", "author": ["J. Hoffman", "S. Gupta", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1962}, {"title": "Cortical feedback improves discrimination between figure and background by v1", "author": ["J.M. Hup", "A. James", "B.R. Payne", "S.G. Lomber", "P. Girard", "J. Bullier"], "venue": "v2 and v3 neurons. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Slow and steady feature analysis: Higher order temporal coherence in video", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Lei Ba"], "venue": "ICLR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J.H.P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "In ECCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information", "author": ["D. Marr"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1982}, {"title": "Using the forest to see the trees: A graphical model relating features, objects, and scenes", "author": ["K. Murphy", "A. Torralba", "W.T. Freeman"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "The role of context in object recognition", "author": ["A. Oliva", "A. Torralba"], "venue": "TRENDS in Cognitive Sciences,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Learning the compositional nature of visual object categories for recognition", "author": ["B. Ommer", "J.M. Buhmann"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Is object localization for free? weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Shared parts for deformable partbased models", "author": ["P. Ott", "M. Everingham"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Occlusion patterns for object class detection", "author": ["B. Pepik", "M. Stark", "P. Gehler", "B. Schiele"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "In ECCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Texture segregation causes early figure enhancement and later ground suppression in areas v1 and v4 ofvisual cortex", "author": ["J. Poort", "M.W. Self", "B. v. Vugt", "H. Malkki", "P.R. Roelfsema"], "venue": "Cerebral Cortex,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Object recognition by scene alignment", "author": ["B.C. Russell", "A. Torralba", "C. Liu", "R. Fergus", "W.T. Freeman"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Learning with hierarchical-deep models", "author": ["R. Salakhutdinov", "J.B. Tenenbaum", "A. Torralba"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Learning and-or templates for object recognition and detection", "author": ["Z. Si", "S.-c. Zhu"], "venue": "PAMI, 35:2189\u20132205,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Very deep convolutional networks for large scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C.-Y. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR-WS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "In NIPS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Towards deep compositional networks. arxiv, 2016", "author": ["D. Tabernik", "M. Kristan", "J.L. Wyatt", "A. Leonardis"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Semantic part segmentation using compositional model combining shape and appearance", "author": ["J. Wang", "A. Yuille"], "venue": "In CVPR,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Learning and-or models to represent context and occlusion for car detection and viewpoint estimation", "author": ["T. Wu", "B. Li", "S. Zhu"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Complexity of representation and inference in compositional models with part sharing", "author": ["A. Yuille", "R. Mottaghi"], "venue": "JMLR, 2016", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Part and appearance sharing: Recursive compositional models for multi-view multi-object detection", "author": ["L. Zhu", "Y. Chen", "A. Torralba", "W. Freeman", "A. Yuille"], "venue": "In CVPR,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In CVPR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 19, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 36, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 34, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 101, "endOffset": 121}, {"referenceID": 19, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 153, "endOffset": 157}, {"referenceID": 40, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 182, "endOffset": 190}, {"referenceID": 11, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 182, "endOffset": 190}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 214, "endOffset": 218}, {"referenceID": 17, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 237, "endOffset": 245}, {"referenceID": 42, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 237, "endOffset": 245}, {"referenceID": 5, "context": "Convolutional neural networks (CNNs) have shown remarkable performance in many computer vision tasks [21, 20, 42, 37, 35] including image classification [20], object class detection [41, 12], instance segmentation [13], image captioning [18, 44], and scene understanding [6].", "startOffset": 271, "endOffset": 274}, {"referenceID": 14, "context": "Specifically, the interleaving of locally connected filter and pooling layers [15] bears similarity to the visual cortex\u2019s interleaving of simple cells, which have localized receptive fields, and complex cells, which have wider receptive fields and greater local invariance.", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 9, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 16, "context": "Examples include learning representations from video sequences [2, 10, 17], encouraging the utilization of depth in-", "startOffset": 63, "endOffset": 74}, {"referenceID": 36, "context": "Figure 1: For a standard CNN (VGG, [37]), the presence of a nearby object (cup) greatly affects the activations in the region of an object of interest (airplane).", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "formation [14], and using physical interaction with the environment [31] to bias representations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "formation [14], and using physical interaction with the environment [31] to bias representations.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "It is also in line with findings from neuroscience that suggest separate processing of figure and ground regions in the visual cortex [16, 32].", "startOffset": 134, "endOffset": 142}, {"referenceID": 31, "context": "It is also in line with findings from neuroscience that suggest separate processing of figure and ground regions in the visual cortex [16, 32].", "startOffset": 134, "endOffset": 142}, {"referenceID": 36, "context": "1 visualizes the difference in activations between a CNN trained without (VGG [37]) and with our compositionality objective1).", "startOffset": 78, "endOffset": 82}, {"referenceID": 33, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 43, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 41, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 45, "context": "In contrast to previous work that designs compositional representations from the ground up [34, 45, 43, 47], our approach does not mandate any particular network architecture or parameterization \u2013 instead, it comes in the form of a modified training objective that can be applied to teach any standard CNN about compositionality in a soft manner.", "startOffset": 91, "endOffset": 107}, {"referenceID": 23, "context": "Compositional models have existed since the early days of computer vision [24] and have appeared mainly in two different varieties.", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 26, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 46, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 45, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 114, "endOffset": 129}, {"referenceID": 47, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 157, "endOffset": 165}, {"referenceID": 28, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 157, "endOffset": 165}, {"referenceID": 35, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 201, "endOffset": 209}, {"referenceID": 43, "context": "The first flavor focuses on the creation of hierarchical feature representations by means of statistical modeling [8, 27, 48, 47], reusable deformable parts [49, 29], or compositional graph structures [36, 45].", "startOffset": 201, "endOffset": 209}, {"referenceID": 37, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 165, "endOffset": 169}, {"referenceID": 41, "context": "The second flavor designs neural network-based representations in the form of recursive neural networks [38], imposing hierarchical priors on Deep Boltzman Machines [34], or introducing parametric network units that are themselves compositional [43].", "startOffset": 245, "endOffset": 249}, {"referenceID": 27, "context": "Recent work [28] constrains CNN activations to lie within object masks in the context of weakly-supervised localization.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 100, "endOffset": 107}, {"referenceID": 16, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 100, "endOffset": 107}, {"referenceID": 9, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 195, "endOffset": 199}, {"referenceID": 30, "context": "It has demonstrated improved performance when training from video sequences instead of still images [2, 17], assuming an object-centric view [10], integrating multimodal sensory side information [14], or even being in control of movement [31].", "startOffset": 238, "endOffset": 242}, {"referenceID": 8, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 25, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 4, "context": "It is well known that context plays a major role in visual recognition, both in human and artificial vision systems [9, 26, 5].", "startOffset": 116, "endOffset": 126}, {"referenceID": 24, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 196, "endOffset": 203}, {"referenceID": 3, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 196, "endOffset": 203}, {"referenceID": 2, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 238, "endOffset": 241}, {"referenceID": 6, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 29, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 10, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 44, "context": "Our environment tends to be highly regular, and making use of regularities in the occurrence of different object and scene classes has been shown to be beneficial for recognizing familiar objects [25, 4], objects in unusual circumstances [3], and recurring spatial configurations [7, 30, 11, 46].", "startOffset": 280, "endOffset": 295}, {"referenceID": 32, "context": "At the extreme, object classes can be successfully recognized even in the absence of local information by relying exclusively on scene context [33].", "startOffset": 143, "endOffset": 147}, {"referenceID": 22, "context": "In the following, we use object masks (as provided by standard data sets such as MS-COCO [23]) as the basis for compositionality.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "Their relative contributions are controlled by the hyperparameter \u03b3 \u2208 [0, 1], to yield", "startOffset": 70, "endOffset": 76}, {"referenceID": 18, "context": "Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Since Lc is of a standard form, we can optimize it like any CNN via SGD (specifically, using the ADAM optimizer [19] and Tensorflow [1]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "3) and real images (MS-COCO [23], Sect.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "We create two variants of the popular MNIST dataset [22], in analogy to the two aforementioned 3D object datasets.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "MS-COCO [23] constitutes a move away from \u201ciconic\u201d views of objects towards a dataset in which objects frequently occur in their respective natural", "startOffset": 8, "endOffset": 12}, {"referenceID": 39, "context": "Like BASELINE, but with dropout [40] and l2-regularization.", "startOffset": 32, "endOffset": 36}, {"referenceID": 36, "context": "256 VGG [37] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 39, "context": "We compare to the same baselines as before, plus two baselines with dropout [40] and l2-regularization (see Sect.", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "Second, it also leads to better localization when backtracing classification decisions to the input images, which we implement by applying guided backpropagation [39] (Fig.", "startOffset": 162, "endOffset": 166}, {"referenceID": 36, "context": "COMP-FULL outperforms both BASELINEAUG and VGG [37] by considerable margins.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "To our knowledge, only [28] reports classification (not detection) performance on MS-COCO, achieving 62.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "8% mAP on the full set of 80 classes using fixed lower-layer weights from ImageNet pre-training [20] and an elaborate multi-scale, sliding-window network architecture.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "We believe this to be an encouraging result that is complementary to the gains reported by [28] and leave the combination of both as a promising avenue for future work.", "startOffset": 91, "endOffset": 95}, {"referenceID": 36, "context": "VGG [37]", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "VGG [37]", "startOffset": 4, "endOffset": 8}, {"referenceID": 38, "context": "Figure 6: Backtracing classification activations (MS-COCO categories, denoted by column labels) to test images using guided backpropagation [39].", "startOffset": 140, "endOffset": 144}], "year": 2017, "abstractText": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via applicationspecific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific details of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.", "creator": "LaTeX with hyperref package"}}}