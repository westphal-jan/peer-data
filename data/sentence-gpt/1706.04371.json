{"id": "1706.04371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "A survey of dimensionality reduction techniques based on random projection", "abstract": "Dimensionality reduction techniques play important roles in the analysis of big data. Traditional dimensionality reduction approaches, such as Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA), have been studied extensively in the past few decades. However, as the dimension of huge data increases, the computational cost of traditional dimensionality reduction approaches grows dramatically and becomes prohibitively expensive.\n\n\n\n\nThe basic idea behind a linear dimensionality reduction approach is to develop a linear dimensionality reduction approach that uses the existing linear-dimensional methods. For example, the proposed problem is that the whole model can be modeled as a matrix of linear dimensions and then treated as a matrix of linear dimensions and an integral part of the matrix. However, it is possible to use linear dimensional models such as Dirac, Dirac, and Stagnation. Therefore, the approach that I believe will improve the current approach to dimensionality reduction is to consider some of the problems of large datasets. For example, you can divide the data into two parts by using only the same dimension, as in your standard way. However, you can be done with a different dimension, like in your regular way, but in the new dimension:\nIn an ordinary way, you can now have two part sets:\nI.e. you have a linear dimensionality reduction system that uses an integral part. However, it can be easily re-ordered in different parts (i.e. you have to be satisfied with one part in a time), such as in the new dimension:\nThis is a great way to solve this problem. In general, linear dimensions are not only efficient but have the benefit of allowing them to be re-ordered in different parts. Therefore, the problem of dimensionality reduction is a problem of many different parts that can be easily improved in a single time.\nThis is another important problem, however, for a large data structure. In this case, the problem of dimensionality reduction is called the \"big data\" principle. Because the structure is a set of data, it is important to maintain this structure in a simple way.\nThe problem of dimensionality reduction is a problem of multiple dimensions. We can take the example of \"a graph of linear dimensionality reduction\", for example, where we use the following formula:\n[x = 2 + x, y = 1 + y, z = 2 + x, y = 1 + y, z = 2 + y, z = 2 + y, z = 3 + y", "histories": [["v1", "Wed, 14 Jun 2017 09:13:33 GMT  (1095kb,D)", "http://arxiv.org/abs/1706.04371v1", null], ["v2", "Sat, 17 Jun 2017 04:09:17 GMT  (1095kb,D)", "http://arxiv.org/abs/1706.04371v2", null], ["v3", "Fri, 27 Oct 2017 10:59:10 GMT  (728kb,D)", "http://arxiv.org/abs/1706.04371v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haozhe xie", "jie li", "hanqing xue"], "accepted": false, "id": "1706.04371"}, "pdf": {"name": "1706.04371.pdf", "metadata": {"source": "CRF", "title": "A survey of dimensionality reduction techniques based on random projection", "authors": ["Haozhe Xiea", "Jie Lia", "Hanqing Xuea"], "emails": ["jieli@hit.edu.cn"], "sections": [{"heading": null, "text": "Dimensionality reduction techniques play important roles in the analysis of big data. Traditional dimensionality reduction approaches, such as Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA), have been studied extensively in the past few decades. However, as the dimension of huge data increases, the computational cost of traditional dimensionality reduction approaches grows dramatically and becomes prohibitive. It has also triggered the development of Random Projection (RP) technique which maps high-dimensional data onto low-dimensional subspace within short time. However, RP generates transformation matrix without considering intrinsic structure of original data and usually leads to relatively high distortion. Therefore, in the past few years, some approaches based on RP have been proposed to address this problem. In this paper, we summarized these approaches in different applications to help practitioners to employ proper approaches in their specific applications. Also, we enumerated their benefits and limitations to provide further references for researchers to develop novel RP-based approaches.\nKeywords: Random projection, Compressive sensing, Dimensionality reduction, High-dimensional data"}, {"heading": "1. Introduction", "text": "In machine learning and data mining scenarios, data usually has very high dimensionality [1]. For example, market basket data in the hypermarket is very high-dimensional, consisting of several thousand of merchandises. In text mining, each document is usually represented by a matrix whose dimensionality equals to the vocabulary size. In bioinformatics, gene expression profiles can also be considered as matrices with more than ten thousand of continuous values. Highdimensionality leads to burdensome computation and curseof-dimensionality issues. Therefore, dimensionality reduction techniques have often been applied in machine learning tasks to solve curse-of-dimensionality problem [2]. During the past decades, traditional dimensionality reduction techniques such as PCA [3, 4] and LDA [5] have been widely studied. However, as the data dimension increases, the computational cost of traditional dimensionality reduction approaches grows dramatically and becomes prohibitive. RP [6] is a simple and fast approach to reduce dimensionality, which projects the original high-dimensional matrix Xn\u00d7d onto a k-dimensional subspace using a random matrix Wd\u00d7k. It can be formulated as following:\nXRPn\u00d7k = Xn\u00d7dWd\u00d7k (1)\nThe key idea of RP is based on the Johnson\u2013Lindenstrauss lemma [7], which states that it is possible to project n points in a space of arbitrarily high dimension onto an O(log n)dimensional space, such that the pairwise distances between points are approximately preserved.\n\u2217Corresponding author Email address: jieli@hit.edu.cn (Jie Li)\nThus RP has attracted increasing attention in the past few years and has been employed in many machine learning scenarios, including classification [8, 9, 10], clustering [11, 12, 13], regression [14, 15, 16], manifold learning [17, 18, 19], and information retrieval [20, 21]. Although RP is much less expensive in computational cost, it often fails to capture the taskrelated information because latent space is generated without considering the intrinsic structure of original data. Various strategies are proposed to overcome this issue and improve the performance of RP. These strategies are organized into four categories: feature extraction, dimensionality increase, ensemble, and optimization theory (Figure 1).\nFeature extraction intends to construct informative and nonredundant features from a large set of data, which is the most commonly used strategy to improve the performance of RP. Schneider et al. [16] incorporated bag-of-words (BoW) model into RP to utilize consumer reviews for forecasting sales of products on Amazon. In order to gain higher performance in text classification tasks, Zhao et al. [10] developed two methods and achieved better balance between classification accuracy and computational complexity.\nIncreasing the dimensionality of the original feature space can sometimes improve the linear separability and the original features can be better represented in high-dimensional space. Ma et al. [22] firstly constructed an extremely high-dimensional feature space with rectangle filters. Then, RP was applied for dimensionality reduction. In bioinformatics, Alshamiri et al. [23] combined extreme learning machine (ELM) [24] with RP for data classification and clustering in gene expression datasets.\nSeveral studies concentrated on ensembles of decision trees have been proposed, well known as Random Forest [25] and AdaBoost [26]. Schclar et al. [27] performed a voting scheme\nPreprint submitted to Pattern Recognition June 15, 2017\nar X\niv :1\n70 6.\n04 37\n1v 1\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n17\non ensemble classifiers that were constructed by applying multiple RP instances to the same dataset. Zhang et al. [28] and Yoshioka et al. [29] used a similar method for drug-target interaction prediction and dysarthric speech recognition, respectively.\nOptimization theory methods treat the improvements of RP as an optimization problem. Drineas et al. [30] reduced the computational complexity of Lasso [31] with the help of RP.\nAlthough various approaches were developed to improve the performance of RP, there are still some issues need to be addressed. Practitioners also need some guidelines to select which strategy and approach to use in their application fields. Here we reviewed these approaches and summarized their benefits and limitations to provide references for further studies of RPbased methods."}, {"heading": "2. Feature extraction approaches to improve the performance of RP", "text": "Feature extraction transforms the data in the highdimensional space into a space with fewer dimensions, which is the most commonly used strategy to improve the performance of RP. It is often used before or after RP, which are called preprocessing or post-processing respectively (Figure 2). Most researchers prefer to use post-processing methods because feature extraction methods take less time in lower-dimensional subspace. Observe that RP rotates the original data to a random basis, which can be considered as a uniform sampling. Therefore, for approximately uniformly distributed data, it can be well represented after the usage of RP. The existing preprocessing and post-processing methods for RP in different application fields are listed in Table 1.\nIn many machine learning and pattern recognition systems, feature extractors that transform raw data into feature vectors are required to be carefully designed, especially in specific application fields. So important are feature extractors that they directly affect the performance of the developed methods. There are two kinds of feature extraction methods: general methods,\nsuch as PCA and LDA, which can be applied to many fields, and application-specific methods that are designed for coping with a specific problem, like Nonsubsampled Pyramid (NSP) [39] and constrained energy minimization (CEM) [40]."}, {"heading": "2.1. General methods", "text": "Traditional dimensionality reduction techniques aim at seeking dimensions with maximum discriminative power, whereas RP performs well in fast finding low dimension space. It is natural that the two kinds of methods are combined to solve dimensionality reduction problem. So in the past few years, a great deal of researches based on the two techniques have been developed.\nXie et al. [9] incorporated RP into PCA, LDA, and feature selection (FS) [41] to classify gene expression profiles of breast cancer. Experimental results demonstrated that the classification accuracy of RP can be significantly improved by FS, especially in the small-n-large-P datasets [42]. However, in some cases, FS takes longer time with the increase of data dimension.\nZhao et al. [10] proposed Semi-Random Projection (SRP) method with the purpose of finding a discriminative subspace while having feasible computational load. Different from RP whose values of transformation matrix are assigned randomly, the weights for transformation vectors of SRP are obtained by LDA. SRP method (see Figure 3) includes three steps. First, the original data matrix X \u2208 Rn\u00d7d is mapped onto a subspace X\u0302i \u2208 Rn\u00d7k using the randomly selected k features. Next, the data with k features is projected onto a single dimension hi using a transform vector W \u2208 Rk\u00d71 learned from LDA. The above procedure is repeated r times, and generates the following latent subspace H \u2208 Rn\u00d7r:\nH = [ h1|h2| . . . |hr ] (2)\nExperiments were performed on six datasets generated from the standard text 20 newsgroups corpus [43] for determining the top category of text. Experimental results indicated that the classification accuracy of SRP followed by PCA (SRP + PCA)\nincreased by about 25% compared to that of RP, but it was still lower than that of PCA.\nTo further improve the performance of SRP, Stacked SemiRandom Projection (SSRP) was developed. In the SSRP, SRP is stacked by feeding the output of previous SRP layer as the input of the subsequent SRP layer. Suppose the output of the k-th SRP layer is denoted by Hk with the dimensionality of dk, and the first layer is initialized with the original data H0 = X with the dimensionality of d0. The relationship between dimensionality of the k-th and the (k \u2212 1)-th SRP layer can be calculated as:\ndk = \u03b1b \u221a dk\u22121c (3)\nwhere \u03b1 is a hyperparameter. The output of the k-th SRP layer can be determined as:\nHk = 1\n1 + exp(\u2212[Wk]THk\u22121) (4)\nwhere Wk is the transformation vector of the k-th SRP layer learned from regularized LDA [44]. The overall process of SSRP is shown in Figure 4. The SSRP outperformed other methods, including PCA, sparse PCA [45], RP, RP + PCA, mSDA [46], and SRP + PCA on 5 of 6 datasets in classification accuracy. In addition, the computation cost of SSRP is much lower than that of PCA though it is 2 times higher than that of RP. Due to LDA is adopted, SRP and SSRP may easily overfit in the presence of labeling noise and are not applicable for non-linear problems [47].\nBoW model is a simplified representation used in natural language processing and computer vision [48, 32, 49]. The fact that the number of words in a BoW model usually reaches tens of millions, which explodes the dimensionality of predictor matrices. In the past few years, RP has been used to address this issue. Schneider et al. [48] proposed an attributes-based regression model in which historical data were used to forecast one-week-ahead rolling sales for tablet computers. Different from extant approaches [50, 51], the authors took customer reviews into account and used BoW model to analyze feedback of products. However, a key challenge of BoW is that millions of words in the bag lead to infeasible computation. To tackle this problem, RP was adopted for reducing the dimension of BOW predictor matrices. Assuming that the BoW matrix is denoted by BOWSn\u00d7d, where n and d are the number of reviews and words, respectively. RP maps original space onto a subspace B\u0302OWSn\u00d7k spanned by the k features. Compared to baseline model without BoW, the proposed model with BoW [48] had better predictive performance on the dataset named \u201cMarket Dynamics and User Generated Content about Tablet Computers\u201d [52].\nIn computer vision, the BoW model has been applied to texture research (including synthesis, classification, segmentation, compression, and shape from texture) [53, 54, 55, 56] by treating texture features as words. In BoW, texture images are statistically described as histograms over a dictionary of features. It has been proved that local texture feature descriptors learned\nfrom BoW were proved to be insensitive to local image perturbations such as rotation, affine changes and scale [32]. One limitation of existing methods without BoW [57, 58] is that the image patch features are not rotationally invariant. In order to address this limitation, Liu et al. [59] proposed a robust and powerful texture classifier based on RP and BoW. Firstly, RP is used to extract a small set of random features from local image patches. Then, random features are embedded into a BoW model to carry out texture classification. Finally, learning and classification are performed in the compressed domain. By comparison, the proposed method made an improvement by 10.38% and 3.32% in classification accuracy compared to local binary pattern (LBP) [57] and the combination of LBP and normalized Gabor filter (NGF) [58]. Furthermore, the proposed method took considerable less time and storage.\nTraditional machine learning techniques are limited in their ability to handle natural data in their row form. Deep learning [60, 61] has performed remarkably well, leaving traditional machine learning in the dust. Wu et al. [33] proposed a novel method based on RP and deep neural network (DNN) [62] for text recognition in natural scene images. First of all, convolutional neural network (CNN) [62] is adopted as a feature extractor to convert word images to a multi-layer CNN feature sequence with slicing windows. Then RP is used to map highdimensional CNN features onto subspace with low dimensionality. Finally, recurrent neural network (RNN) [63] used for decoding the embedded RP-CNN features is trained to recognize the text in the image. Multiple RNNs are ensembled by Recognizer Output Voting Error Reduction (ROVER) algorithm [64] so as to further improve the recognition rate. The authors implied that the recognition rate of RP-CNN features, with 85% dimensionality reduction, is similar to the original high-dimensional ones. In spite of the fact that CNNs and RNNs dramatically improved the performance in recognition rate, CNNs and RNNs are difficult to train and require lots of time in selecting hyperparameters.\nLow rank approximation plays a central role in data analysis. In mathematics, it is often desirable to find a good approximation of a given matrix with lower rank. Eigenvalue decomposition is a typical strategy, best known as Singular Value Decomposition (SVD) [65], but it usually leads to heavy computation. RP is a simple technique and has been widely used for accelerating the computation for such approximations. Assume that the original matrix can be formulated as X \u2208 Rn\u00d7k. The quality of the approximation depends on how well the method captures the important part of X. Martinsson [66] proposed an approximation method based on QR decomposition [67] and RP. First of all, a matrix Y \u2208 Rn\u00d7d is formed according to Y = (XXT)pXS, where p stands for the number of iterations, and S \u2208 Rk\u00d7d follows Gaussian distribution N(0, 1). Then, QR decomposition is applied: Y = QR. The low rank approximation of matrix X can be represented as X\u0303 = Q(QTX). The proposed method constructed a nearly optimal rank-k approximation with much lower time complexity. Alternatively, the sampling matrix S can also sampled from Subsampled Randomized Hadamard Transform (SRHT) [68], and the approximation method is known as structured RP [69]. Experimental\nresults revealed that the structured RP is faster but less accurate than the method proposed by Martinsson [66]."}, {"heading": "2.2. Application-specific methods", "text": "In some specific application fields, especially in image processing, specific feature extraction methods are often designed to obtain better experimental results. Arriaga et al. [34] proposed two RP-based methods: slide window RP (SWRP) and corner RP (CRP). In the SWRP, a corresponding location in the projected images is filled with a synthetic color. The synthetic color is generated from a random combination of colors in a window that slides over the original image (see Figure 5a). Motivated by the intuition that human visual systems are feature detectors for colors, lines, corners, and so on, the features from accelerated segment test (FAST) [70] corner detector is adopted in CRP to detect features in images. The projected image can be represented as a combination of multiple corner images (see Figure 5b). The authors indicated that SWRP performed better than CRP in classification accuracy. Compared to neural networks without SWRP, neural networks with SWRP have similar classification accuracy but much lower computational cost. However, the performance of SWRP may be serious affected for images consisting of similar colors since SWRP randomly combines colors in a sliding window.\nBy taking advantages of characteristics of RP that it is insensitive to noise in images, Qin et al. [35] proposed a method for suppressing clutters while enhancing targets in infrared images (see Figure 6). First, a signal decomposition algorithm named NSP [39] is adopted to decompose image into one lowfrequency subband and multiple high-frequency subbands. After K-scale NSP decomposition, K+1 subbands including one low-frequent subband and K high-frequency subbands are produced. The high-frequency subbands mainly contain targets and little clutters background, so it is easier to extract target information by dealing with high-frequency subbands. In order to preserve target information as much as possible, a 3D image cube is constructed by concatenating all the high-frequency subbands. The cube model can be expressed as:\nFK\u00d7M = ( f1, f2, . . . , fk, . . . , fK)T (5)\nwhere fk is the row vector representation of a subband and M is the number of pixels of the k-th high-frequency subband. Then, RP is used to project 3D image cube FK\u00d7M onto a low-dimensional subspace QS\u00d7M with the intention of reducing the spatial redundancy of the target and background information. Finally, Mahalanobis distance [71] is applied to remove background clutters in dimensionality-reduction highfrequency subbands obtained above. Compared to other stateof-art methods for background suppression of infrared small target images, the proposed model outperformed other methods including max-median [72], morphological (top-hat) [73], Phase Spectrum of Quaternion Fourier Transform (PQFT) [74] and Wavelet Transform (WRX) [75] in signal-to clutters ratio gain (SCRG) and background suppress factor (BSF) [76].\nFeng et al. [36] developed a new approach to detect hyperspectral target using the CEM method [40]. The objective of CEM is to design a finite impulse response (FIR) linear filter with L filter coefficients, and the FIR filter can be represented using an L-dimensional vector w = {w1,w2, . . . ,wL}. Let ri(i = 1, 2, . . . , n) be an L-dimensional sample pixel vector. The optimized target detector w can be given as:\nw\u2217 = R\u22121L\u00d7Ld\ndTR\u22121L\u00d7Ld (6)\nwhere RL\u00d7L = (1/n) \u2211n\ni=1 rirTi is the sample autocorrelation matrix of the target and d denotes the spectral signature of the target. In order to resolve the issue of curse of dimensionality in hyperspectral imagery, RP is incorporated. The noise suppression effect of RP is superior to that of maximum-noisefraction (MNF) [77] and PCA. Therefore, CEM method using dimensionality reduction RP (RP-CEM) outperformed MNFCEM and PCA-CEM in both detection accuracy and computation time. The drawback of CEM is that it can only detect single target because CEM simply treats the undesired targets as interference and does not take fully use of the known information. Target-Constrained Interference-Minimized Filter (TCIMF) [78] was developed to tackle this issue. It assumes pixels of an image are made up of three separate signal sources: D (desired targets), U (undesired targets), and I (interference), while CEM simply treats U as a part of I.\nLet D = [ d1, d2, . . . , dp ] and U = [ u1, u2, . . . , uq ] denote the desired-target signature and undesired-target signature, respectively. The spectral signature of the target d in Eq. 6 can be replaced with [ D,U ] . The optimal weight vector w can be formulated as:\nw\u2217 = R\u22121L\u00d7L\n[ D U ][ D U ]T R\u22121L\u00d7L [D U] [ 1p\u00d71 0q\u00d71 ] (7)\nwhere 1p\u00d71 is a p \u00d7 1 column vector with ones in its components, which is used to constrain the desired targets in D. Similarly, 0q\u00d71 is a q\u00d71 column vector with zeros in all components, which is used to suppress the undesired targets in U. Analogous to Feng et al. [36], Du et al. [37] conducted target detection by TCIMF, where RP is used for dimensionality reduction. Not only did RP reduce computation complexity, but also improved target-detection accuracy by decision fusion across multiple RP instances. Experimental results demonstrated that the detection accuracy of RP-TCIMF with a single run of RP was a bit lower than that of TCIMF, but it could be further improved by multiple runs.\nSubspace based spectrum estimation is often used for wide-\nband spectrum sensing. However, subspace based techniques which require eigen decomposition of original data are computationally expensive. To alleviate this issue, Majee et al. [38] applied low-rank approximation before MUltiple SIgnal Classification (MUSIC) [79] which is employed for wideband spectrum sensing. The low-rank approximation technique used in this work is based on Cholesky Factorization (CF) [80] and RP. Suppose X \u2208 Rn\u00d7n is an arbitrary square matrix. First of all, RP is performed as X\u0302 = XW, where W \u2208 Rn\u00d7k is a random projection matrix. Then \u03a6T is filled with k left singular values of X\u0302, and thus X\u0302 can be estimated as X\u0302\u2032 = \u03a6X\u03a6T. After that, CF is applied as X\u0302\u2032 = LLT, and D can be calculated as D = X\u0302\u2032\u03a6T(LT)\u22121. SVD is then applied as D = U\u03a3VT, and krank approximate of X can be obtained as XLR = U\u03a32VT. The authors concluded that the proposed method had a marginal reduction in time complexity. In addition, the spectrum sensing performance of the proposed method was comparable or superior to that of MUSIC in terms of probability of alarm."}, {"heading": "3. Increase dimensionality of original data to improve the performance of RP", "text": "In some studies, researchers acted in diametrically opposed ways. They firstly map the original dataset onto a higher dimensional feature space to better represent the original features. Then RP is applied to reduce dimensionality and save computational cost.\nMa et al. [22] proposed a robust method for face recognition, as illustrated in Figure 7. In the proposed method, multi-radius local binary pattern (MLBP) is firstly adopted to incorporate more structural facial feature. Then, a high-dimension multiscale and multi-radius LBP (MMLBP) space P \u2208 RL\u00d7m is obtained by convolving rectangle filters. The rectangle filters of a given face image with width w and height h can be defined as follows:\npu0,v0i = 1, u0 \u2264 u \u2264 wi, v0 \u2264 v \u2264 hi0, otherwise (8) where u0 and v0 represent the offset coordinates of the filter pi with width wi and height hi, respectively. It can be found that there are approximately L = m2 = (wh)2 (i.e. the number of possible locations times that of possible scales) exhaustive rectangle filters for each face image. Generally, a large multiscale rectangle filter matrix P can be obtained by stacking pi together, where pi \u2208 R1\u00d7m can be formulated as a row vector whose dimensionality equals to w \u00d7 h. Since L is usually up to 106 \u2212 1010, sparse RP [81] is taken for dimensionality reduction. The subspace can be formulated as Mn\u00d7m = Wn\u00d7LPL\u00d7m, where W is a transformation matrix generated by RP. The proposed method not only archived higher recognition rate but also showed better robustness to corruption, occlusion, and disguise compared to Randomface [82] and Eigenfaces [83], even in low dimensional spaces.\nThere have been lots of works concentrating on visual tracking [84, 85, 86], where RP are favored by researchers because of its computational effective and data independent characteristics. Zhang et al. [85, 86] proposed a tracking framework, named CT tracker, where rectangle filters following Eq. 8 were adopted to form a very high-dimensional multiscale image feature vector and RP was applied to compress samples of foreground targets and the background. The tracking window in the first frame is determined manually. To predict the target in the next frame, positive samples are sampled near the current target while negative samples are sampled far away from the current target. Both positive and negative samples are used to update the Bayes classifier. However, rectangle filters are sensitive to the presence of a few outliers in samples. To overcome this problem, a more stable model was proposed by Gao et al. [87], where rectangle filters were replaced by MSERs [88]. To robustly adapt the variations of target appearance, a least squares support vector machine (LS-SVM) [89] is employed. The authors stated that the proposed tracker outperformed the CT-based tracker [86] in terms of mean distance precision and mean frame rate (FPS).\nRecent studies have revealed that ELM performs well in regression and classification problems [24]. Additionally, compared to traditional algorithms such as back-propagation (BP)\nand support vector machine (SVM), ELM provides not only good generalization performance, but also fast learning speed. Alshamiri et al. [23] performed RP in conjunction with ELM for low-dimensional data classification. The method consists of two phases. Firstly, the original data X \u2208 Rn\u00d7d is projected onto the subspace spanned by L (L d) using ELM. The linear separability of the data often increases by mapping the data onto a high-dimensional ELM feature space. After that, RP is applied to reduce the dimensionality of the ELM feature space. There is a slight improvement in classification accuracy of ELM-RP compared to that of ELM. Since ELM consists of a single hidden layer, it is hard to encode complex things and archive satisfying accuracy. Also, on small-n-large-p datasets, ELM is prone to overfit because of a lack of training samples."}, {"heading": "4. Ensemble of multiple RP instances to improve the performance of RP", "text": "Inspired by the fact that an ensemble strategy can significantly improve the performance of weak classifiers, several algorithms were proposed based on ensembles of decision trees, well known as Random Forest [25] and AdaBoost [26]. To alleviate computational burdens of high-dimensional datasets, RP is incorporated as a preprocessing step before AdaBoost [90]. Furthermore, ensembles of multiple RP instances helps RP to yield more stable results.\nSchclar et al. [27] proposed an ensemble method based on RP and nearest-neighbor (NN) inducers [91]. First of all, K random matrices are generated by RP. Then, K training sets for ensemble classifiers are constructed by applying random matrices to the original dataset. After that, K NN classifiers are trained and the final classification result is produced via a voting scheme. The proposed method appeared to be more accurate compared to the non-ensemble NN classifier.\nZhang et al. [28] proposed an RP ensemble method which is analogous to the work of Schclar\u2019s et al. [27] for drugtarget interaction prediction, where \u201cPaDEL-Descriptor\u201d [92] was adopted as a feature detector. The authors stated that the proposed drug-target interaction prediction method achieved an improvement in accuracy by 4.5%-8.2% compared to the work [93]. Similarly, Yoshioka et al. [29] also proposed RP ensemble method for dysarthric speech recognition, in which an automatic speech recognition (ASR) system was adopted as a feature detector. As compared to the PCA-based feature projection method, the method proposed in [29] made an improvement by 5.23% in recognition rate.\nGondara [94] also proposed an ensemble classifier using RP. Different from the method proposed by Schclar\u2019s et al. [27] where the RP matrices are applied to the same feature space, in this method, the RP matrices are applied to random subsets of the original feature set. The authors demonstrated that the proposed method performed equally well or better than Random Forest and AdaBoost in classification accuracy.\nSo as to alleviate the high distortion in single run of clustering algorithm in feature spaces produced by RP, Fern et al. [11] investigated how RP could best be used for clustering,\nand proposed a cluster ensemble approach based on expectation\u2013maximization (EM) [95] and RP. In the proposed ensemble approach, EM generates a probabilistic model \u03b8 of a mixture of k Gaussian distribution after each run of RP. The final clusters are aggregated by measuring similarity of clusters in different clustering results, where the similarity of two clusters ci and c j can be defined as:\nsim(ci, c j) = min pi\u2208ci,p j\u2208c j P\u03b8i j (9)\nwhere P\u03b8i j denotes the probability of data point i and j belonging to the same cluster and can be calculated as:\nP\u03b8i j = k\u2211\nl=1\nP(l|i, \u03b8)P(l| j, \u03b8) (10)\nThe authors mentioned that the proposed ensemble method (RP + EM) was more robust and produced better clusters compared to PCA + EM."}, {"heading": "5. Use optimization theory to improve the performance of", "text": "RP\nThe optimization problem of high-dimensional datasets is computationally expensive. RP is an effective method and has been widely used to reduce the computational cost for optimization problems.\nLeast squares approximation has been widely used to find an approximate solution to an overdetermined system that has no exact solution. The Least-Square problem, also known as Lasso [31], can be formulated as:\n\u03b2\u2217 = argmin \u03b2\u2208Rd \u2225\u2225\u2225Y \u2212 X\u03b2\u2225\u2225\u222522 (11) where X = (x1, x2, . . . , xn) \u2208 Rn\u00d7d and Y = (y1, y2, . . . , yn)T \u2208 Rn represent the data matrix and corresponding response vector, respectively. \u03b2 \u2208 Rd is a vector denoting model coefficients. So as to speed up calculation of least square problem, Drineas et\nal. [30] applied RP matrices to reduce the dimensionality of original feature space, which can be formulated as follows:\n\u03b2\u2217 = argmin \u03b2\u2208Rd \u2225\u2225\u2225WHD(Y \u2212 X\u03b2)\u2225\u2225\u222522 (12) where W is a data-independent random matrix, H and D are produced by SRHT [68].\nZhang et al. [96] proposed a method that accelerated linear regression with the help of low-rank matrix approximation implemented with RP and QR decomposition, as introduced in Section 2.1. Let X\u0303 be the low-rank approximation of the original feature set, and thus the coefficient vector \u03b2 can be calculated according to the following recursive formula:\n\u03b2t+1 = argmin \u03b2\u2208Rd \u22121n (\u03b2 \u2212 \u03b2t)TX\u0303(Y \u2212 X\u0303T\u03b2t) +\u03a6t \u2225\u2225\u2225\u03b2\u2225\u2225\u22251 + \u03bb2 \u2225\u2225\u2225\u03b2 \u2212 \u03b2t\u2225\u2225\u222522 \n(13)\nwhere \u03bb represents the regularization term. \u03b20 is set to 0 and \u03a6t = min(\u03a6min,\u03a60\u03b7t) in which \u03b7 \u2208 (0, 1) controls the shrinkage speed of \u03a6t. As compared to existing methods excluding RP, such as PGH [97] and ADG [98], the proposed method made a significant reduction by 98.90% and 98.55% in computation time. Also, the convergence rate of the proposed method was comparable to that of PGH, and much higher than that of ADG.\nZhang et al. [99] tried to solve an optimization problem by recovering from the dual solution of the low-dimensional optimization problem. Assume a training sample and the corresponding binary class assignment are denoted by xi \u2208 Rd and yi \u2208 {\u22121,+1} (i = 1, 2, . . . , n), respectively. In general, a linear classifier \u03b2 \u2208 Rd can be learned by solving a regularized optimization problem as following:\n\u03b2\u2217 = argmin \u03b2\u2208Rd 1 n n\u2211 i=1 ` ( yi\u03b2Txi ) + \u03bb 2 \u2225\u2225\u2225\u03b2\u2225\u2225\u222522 (14) where `(\u00b7) is a differentiable convex loss function, and \u03bb and n\ndenote the regularization term and the number of training examples, respectively. Using conjugate function, Eq. 14 can be turned into a dual problem:\n\u03b1\u2217 = argmax \u03b1\u2208\u2126n\n\u2212 n\u2211\ni=1\n`\u2217 (\u03b1i) \u2212 1 \u03bbn (\u03b1 \u25e6 y)TXTX(\u03b1 \u25e6 y) (15)\nwhere \u03b1\u2217 represents the optimal solution in dual problem and `\u2217(\u00b7) is the convex conjugate of `(\u00b7). Let W \u2208 Rd\u00d7k be an RP matrix following Gaussian distributionN(0, 1/k), where k d. The primal problem can be solved in the low-dimensional space generated by RP, which can be formulated as following:\n\u03b3\u2217 = argmin \u03b3\u2208Rk 1 n n\u2211 i=1 ` ( yi\u03b3TWxi ) + \u03bb 2 \u2225\u2225\u2225\u03b3\u2225\u2225\u222522 (16) And the corresponding dual problem is\n\u03b4\u2217 = argmax \u03b4\u2208\u2126n\n\u2212 n\u2211\ni=1\n`\u2217 (\u03b4i) \u2212 1 \u03bbn (\u03b4 \u25e6 y)TXTWWTX(\u03b4 \u25e6 y) (17)\nThe optimal primal solution \u03b2\u2217 can be computed as following when given the optimal dual solution \u03b4\u2217 of the low-dimensional optimization problem:\n\u03b2\u2217 = \u2212 1 \u03bbn\nX ( \u03b4\u2217 \u25e6 y ) (18)\nCompared to the naive recover method where the primal solution was computed as \u03b2\u0303 = W\u03b3 [100], the authors stated that the proposed method had much lower relative recovery error and less computational time."}, {"heading": "6. Conclusions", "text": "RP is an efficient and powerful dimensionality reduction technique and has been developed and matured over the last 15 years. With the rapid increase of big data, RP has brought about tangible benefits to counteract the burdensome computation and has met the needs of real-time processing in some situations. Despite the fact that RP is computationally efficient, RP often introduces relatively high distortion. In order to solve this problem, various strategies have been proposed to improve the performance of the RP, which were summarized in this survey. Experimental results have proved that feature extraction methods including PCA, LDA, BoW and other application-specified methods can significantly improve the performance of RP. It has also been found that an ensemble of RP instances can improve the generalization ability of original features by bringing information diversity with randomness.\nAlthough RP yields satisfying performance in many scenarios, such as text classification, face recognition, and video tracking. In some other complex tasks including advertisement recommendation, cancer classification based on highthroughput microarray data, however, RP may not well capture the intrinsic structure of the original datasets and its performance need to be further improved."}, {"heading": "Acknowledgments", "text": "This work is partially supported by National Natural Science Foundation of China (61471147, 61371179), Natural Science Foundation of Heilongjiang Province (F2016016) and the Fundamental Research Funds for the Central Universities (HIT.NSRIF.2017037), the National Key Research and Development Program of China (2016YFC0901905)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Dimensionality reduction techniques play important roles in the analysis of big data. Traditional dimensionality reduction ap-<lb>proaches, such as Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA), have been studied extensively<lb>in the past few decades. However, as the dimension of huge data increases, the computational cost of traditional dimensionality<lb>reduction approaches grows dramatically and becomes prohibitive. It has also triggered the development of Random Projection<lb>(RP) technique which maps high-dimensional data onto low-dimensional subspace within short time. However, RP generates trans-<lb>formation matrix without considering intrinsic structure of original data and usually leads to relatively high distortion. Therefore,<lb>in the past few years, some approaches based on RP have been proposed to address this problem. In this paper, we summarized<lb>these approaches in different applications to help practitioners to employ proper approaches in their specific applications. Also, we<lb>enumerated their benefits and limitations to provide further references for researchers to develop novel RP-based approaches.", "creator": "LaTeX with hyperref package"}}}