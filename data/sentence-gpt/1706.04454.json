{"id": "1706.04454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al., as well as to support the hypothesis that both the bulk and outliers are distinct from the structure of the Hessian. However, it is not clear whether Sagun et al. show that the presence of a bulk or outliers in the shape of the Hessian is causally linked to the structure of the Hessian. Nevertheless, the physical evidence that has been provided provides a support for a hypothesis of gravitational wave-induced dark matter as a potential source of gravitational waves is now lacking.\n\n\n\n\n\n\nIn the present work, the theoretical proof for gravitational waves is demonstrated through a number of experiments, both on a case-by-case basis and in a separate case-by-case basis (see in the abstract above). The theoretical proof for gravitational waves is provided by theoretical methods that allow the identification of the properties of the gravitational waves. For example, we have previously demonstrated a novel way for detecting gravitational waves that can be formed at the moment by applying gravitational waves to the shape of the dark matter. As it turns out, we can only detect the existence of gravitational waves at a specific time, or it can only be expected to occur at a specific time, or it can only be expected to occur at a specific time, or it can only be expected to occur at a particular time, or it can only be expected to occur at a particular time, or it can only be expected to occur at a particular time. We have previously demonstrated the use of gravitational waves at a specific time. For example, we have demonstrated that in the present work, the general principle of gravitational waves is to allow for detection of gravitational waves that cannot be formed at the moment by applying gravitational waves to the shape of the dark matter. Such methods also have proven to be of some utility in detecting gravitational waves that can be formed at a specific time, or it can only be expected to occur at a particular time. In the present work, the general principle of gravitational waves is to allow for detection of gravitational waves that can be formed at a specific time, or it can only be expected to occur at a specific time, or it can only", "histories": [["v1", "Wed, 14 Jun 2017 12:50:00 GMT  (391kb,D)", "http://arxiv.org/abs/1706.04454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["levent sagun", "utku evci", "v ugur guney", "yann dauphin", "leon bottou"], "accepted": false, "id": "1706.04454"}, "pdf": {"name": "1706.04454.pdf", "metadata": {"source": "CRF", "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "authors": ["Levent Sagun", "Utku Evci"], "emails": ["sagun@cims.nyu.edu", "ue225@nyu.edu", "ugurguney@gmail.com", "yann@dauphin.io", "leonb@fb.com"], "sections": [{"heading": "1 Introduction", "text": "In this paper we study the geometry of the loss surface of supervised learning problems through the lens of their second order properties. To introduce the framework, suppose we are given data in the form of input-label pairs, D = {(xi, yi)}Ni=1 where x \u2208 Rd and y \u2208 R that are sampled i.i.d. from a possibly unknown distribution \u03bd, a model that is parametrized by w \u2208 RM ; so that the number of examples is N and the number of parameters of the system is M . Suppose also that there is a predictor f(w, x). The supervised learning process aims to solve for w so that f(w, x) \u2248 y. To make the \u2018\u2248\u2019 precise, we use a non-negative loss function that measures how close the predictor is to the true label, `(f(w, x), y). We wish to find a parameter w\u2217 such that:\nw\u2217 = arg minL(w), (1)\nar X\niv :1\n70 6.\n04 45\n4v 1\n[ cs\n.L G\n] 1\nwhere\nL(w) := 1 N N\u2211 i=1 `(f(w, xi), yi). (2)\nIn particular, one is curious about the relationship between L(w) and L\u2032(w) := \u222b `d(\u03bd). By the law of large numbers, at a given point w, Lw \u2192 L\u2032w almost surely as N \u2192 \u221e for fixed M . However in modern applications, especially in deep learning, the number of parameters M is comparable to the number of examples N (if not much larger). And the behaviour of the two quantities may be drastically different (for a recent analysis on provable estimates see [Mei et al., 2016]).\nA classical algorithm to find w\u2217 is gradient descent (GD), in which the optimization process is carried out using the gradient of L. A new parameter is found iteratively by taking a step in the direction of the negative gradient whose size is scaled with a constant step size \u03b7 that is chosen from line-search minimization. Two problems emerge: (1) Gradient computation can be expensive, (2) Line-search can be expensive. More involved algorithms, such as Newton type methods, make use of second order information [Nocedal and Wright, 2006]. Under sufficient regularity conditions we may observe: L(w + \u2206w) \u2248 L(w) + \u2206w\u2207L(w) + \u2206wT\u22072L(w)\u2206w. A third problem emerges beyond an even more expansive computational cost of the Hessian: (3) Most methods require the Hessian to be non-degenerate to a certain extent.\nWhen the gradients are computationally expensive, one can alternatively use it\u2019s stochastic version (SGD) that replaces the above gradient with the gradient of averages of losses over subsets (such a subset will be called the mini-batch) of D (see [Bottou, 2010] for a classical reference). The benefit of SGD on real-life time limits is obvious, and GD may be impractical for practical purposes in many problems. In any case, the stochastic gradient can be seen as an approximation to the true gradient, and hence it is important to understand how the two directions are related in the parameter space. Therefore, the discussion around the geometry of the loss surface can be enlightening in the comparison of the two algorithms: Does SGD locate solutions of a different nature than GD? Do they follow different paths? If so, which one is better in terms of generalization performance?\nFor the second problem of expensive line-search, there are two classical solutions: using a small, constant step size, or scheduling the step size according to a certain rule. In practice, in the context of deep learning, the values for both approaches are determined heuristically, by trial and error. More involved optimal step size choices involve some kind of second order information that can be obtained from the Hessian of the loss function [Schaul et al., 2013]. From a computational point of view, obtaining the Hessian is extremely expensive, however obtaining some of its largest and smallest eigenvalues and eigenvectors are not that expensive. Is it enough to know only those eigenvalues and eigenvectors that are large in magnitude? How do they change through SGD?\nFor the third problem, let\u2019s look at the Hessian a little closer. A critical point is defined by w such that ||\u2207L(w)|| = 0 and the nature of it can be determined by looking at the signs of its Hessian matrix. If all eigenvalues are positive the point is called a local minimum, if r of them are negative and the rest are positive, then it is called a saddle point with index r. At the critical point, the eigenvectors indicate the directions in which the value of the function locally changes. Moreover the changes are proportional to the corresponding -signed- eigenvalue. Under sufficient regularity conditions, it is rather straightforward to show that gradient based methods converge to points where gradient is zero. Recently Lee et al. [2016] showed that they indeed converge to minimizers. However, a significant and untested assumption to establish these convergence results is that the loss is non-degenerate. A relaxation of the above convergence to the case of non-isolated critical points can be found in [Panageas and Piliouras, 2016]. What about the critical points of machine learning loss functions? Do they satisfy the non-degeneracy assumptions? If they don\u2019t, can we still apply the results of provable theorems to gain intuition?"}, {"heading": "1.1 A historical overview", "text": "One of the first instances of the comparison of GD and SGD in the context of neural networks dates back to late eighties and early nineties. Bottou [1991] points out that large eigenvalues of the Hessian of the loss can create the illusion of a local minima and GD can get stuck there, it further claims that the help of the inherent noise in SGD may help getting out of this obstacle. The origin of this observation is due to Bourrely [1989], as well as numerical justifications. However, given the computational limits of the time, these experiments relied on low-dimensional neural networks with few hidden units. The picture may be drastically different in higher dimensions. In fact, provable\nresults in statistical physics tell us that, in certain real-valued non-convex functions, the local minima concentrate at an error level near that of the global minima. A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized.\nThese concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1.\nThe final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value. Such regions at the bottom of the landscape are called the flat minima, which can be considered as another way of saying a very wide minima. It is further noted that such minima have better generalization properties and a new loss function that makes use of the Hessian of the loss function has been proposed that targets the flat minima. The computational complexity issues have been attempted to be resolved using the R-operator of Pearlmutter [1994]. However, the new loss requires all the entries of the Hessian, and even with the R-operator it is unimaginably slow for today\u2019s large networks. More recently, an exact numerical calculation of the Hessian have been carried out by Sagun et al. [2016]. It turns out that the Hessian is degenerate at any given point including the randomly chosen initial point, that the spectrum of it is composed of two parts: (1) the bulk, and (2) the outliers. The bulk is mostly full of zero eigenvalues with a fast decaying tail, and the outliers are only a handful which appears to depend on the data. This implies that, locally, most directions in the weight space are flat, and leads to little or no change in the loss value, except for the directions of eigenvectors that correspond to the large eigenvalues of the Hessian. Based on exactly this observation combined with the K-replica method of the previous paragraph, a recent work produced promising practical results [Chaudhari et al., 2016]."}, {"heading": "1.2 Overview of results", "text": "In an attempt to develop an understanding for some of the classical and modern problems described above and hopefully to get further insight into non-convex optimization over high dimensional spaces, we study the loss function through its second order properties through the spectrum of its Hessian matrix.\nThe first observation is that the Hessian is not slightly singular but extremely so, having almost all of its eigenvalues at or near zero. And this observations seems to hold even at random points of the space. For example, let us consider an artificial neural network with two layers (totaling 5K parameters) and trained using gradient descent. Figure 1 shows the full spectrum of the Hessian at the random initial point of training and after the final training point. Intuitively, this kind of singularity should be expected to emerge if (1) the data is essentially low dimensional but lives in a larger ambient space, and (2) the model is over-parametrized. In the context of deep learning, over-parametrization is a key feature of the model, and in most cases the data is expected to be essentially on a low dimensional manifold.\nWe study the Hessian using a decomposition [LeCun et al., 1998] as a sum of two matrices,\u22072L(w) = G(w) +H(w) where G is the sample covariance matrix of the gradients of model outputs and H is the Hessian of the model outputs. We show that the approximation\u22072L(w) \u2248 G(w) becomes more and more accurate as training progresses, and in fact we can prove equality at the global minimum. This connection to the covariance of the gradients explains why having more parameters than samples causes such degeneracy of the Hessian. We further observe that the singularity holds true even when M is comparable to N . Also, we review results on the spectrum of sample covariance matrices,\n1A general remark is that the terms wide-narrow, high-low, are all relative. Everything can be scaled and skewed to take desired shapes. An example is the recent, Dinh et al. [2017], work shows how sharp minima can still generalize with proper modifications to the loss function. In this work we pay attention to fix as many variables as we can to get a consistent comparison across different setups.\nand attempt to relate those results to machine learning models. Finally, we empirically examine the spectrum, carefully tuning its components, to find uncover intricate dependencies within the data-architecture-algorithm triangle. We begin by discussing the decomposition of the Hessian."}, {"heading": "2 Generalized Gauss-Newton decomposition of the Hessian", "text": "In order to study its spectrum, we will describe how the Hessian can be decomposed into two meaningful matrices. Suppose the loss function is given as a composition of two functions, the model function f : RM \u00d7 Rd \u2212\u2192 R is the real valued output of a network that depends on the parameters and a given example; and the loss function ` : R\u00d7 R \u2212\u2192 R+ which is constrained to be convex in its first component 2. For ease of reading, we suppressed the dependencies of functions ` and f to data (x, y) and unless noted otherwise the gradients are with respect to w. The gradient of the loss per fixed sample is given by\n\u2207`(f(w)) = d ds `(s) \u2223\u2223 s=f(w) \u2207f(w) (3)\nand then the Hessian can be written as\n\u22072`(f(w)) = d 2\nds2 f(s)\n\u2223\u2223 s=f(w) \u2207f(w)\u2207f(w)T + d ds f(s) \u2223\u2223 s=f(w) \u22072f(w) (4)\nwhere ?T denotes the transpose operation, in which case is the gradient column-vector. Note that since ` is convex `\u2032\u2032(s) \u2265 0 we can take its square root. This allows us to define g,H , the scaled gradient and the scaled Hessian of f ,\ng(w) = \u221a `\u2032\u2032(f(w))\u2207f(w) (5)\nH(w) = `\u2032(f(w))\u22072f(w). (6) Combining these results we can rewrite the Hessian of the loss as follows:\n\u22072L(w) = 1 N N\u2211 i=1 gi(w)gi(w) T\n\ufe38 \ufe37\ufe37 \ufe38 G(w)\n+ 1\nN N\u2211 i=1\nHi(w)\ufe38 \ufe37\ufe37 \ufe38 H(w)\n(7)\nwhere the subscript i indicates the index of the example (x, y) \u2208 D. This way we see that the Hessian of the loss can be decomposed in two parts where the generalized Gauss-Newton matrix G is the average of rank one matrices obtained by gradients of outputs, and H is the average of the Hessians of outputs. Some classical examples are as follows:\n2Even then, in many cases the overall loss function is non-convex in the space of its parameters.\nExample 1. The mean-square loss when `(s, y) = (s\u2212 y)2 where s = f(w, x). Example 2. The hinge-loss when `(s, y) = max{0, sy}. Note that the hinge-loss is piecewise differentiable and its second derivative is zero. Example 3. Negative log-likelihood loss when `(sy, y) = \u2212sy+log \u2211 y\u2032 exp sy\u2032 and fy(w;x) = sy for classification problems. For instance, we can have y \u2208 {1, . . . , k} that indicates the correct class of the input x, and the machine outputs a vector of length k described by f(w, x) = (f1(w, x), . . . , fk(w, x)) \u2208 Rk before its fed to the softmax operator.\nWe have various choices for f :\n1. f(w, x) = \u3008w, x\u3009 for the Perceptron. 2. f(w, x) = (Wl . . .W1x) for a toy layered network. 3. f(w, x) = WTx for W an d\u00d7 k for k-class logistic regression. 4. f(w, x) = Wl\u03c3 \u25e6 . . . \u03c3 \u25e6W1x for a fully connected neural network with non-linearities\ndescribed by \u03c3 that acts on vectors entry-wise.\nThe output can also come from a convolutional network, or any other deep learning model, so such a decomposition is fairly general. What we can say about it may heavily depend on its specifics. In general, it doesn\u2019t appear to be so straightforward to discuss the spectrum of the sums of matrices by looking at the individual ones. Nevertheless, looking at the decomposition, we can still speculate on what we should expect. G is the sum of rank-1 matrices. The sum runs over data, and the data typically have redundancies. For instance, if the problem is classification, and the data is sampled from k clusters, then it is not unreasonable to expect that the non-trivial eigenvector of ggT \u2019s are somewhat aligned when the on the examples coming from the same cluster. This suggests that G may be approximated by a rank-k matrix. Moving to the second term: Looking at H we have the Hessian of f . The structure of this matrix depends on the architecture."}, {"heading": "2.1 The spectrum of the generalized Gauss-Newton matrix", "text": "In this section, we will show that the spectrum of the Generalized Gauss-Newton matrix can be characterized theoretically under some conditions. Suppose that we can express the scaled gradient as g = Tx with the matrix T \u2208M \u00d7 d depending only on the parameters w - which is the case for linear models. Then we can write G as\nG = 1\nN \u2211 i\u2208D gig T i = 1 N TXXTTT (8)\nwhere X = {x1, . . . , xN} is an d\u00d7N matrix Furthermore, without loss of generality, we assume that the examples are normalized such that the entries of X are independent with zero mean and unit variance. One of the first steps in studying G goes through understanding its principle components. In particular, we would like to understand how the eigenvalues and eigenvectors of G are related to the ones of \u03a3 where \u03a3 := E(G) = 1N TXX TTT = TTT .\nIn the simplest case, we have \u03a3 = Id so that the gradients are uncorrelated and the eigenvalues of G are distributed according to the Marc\u030cenko-Pastur law in the limit where N,M \u2192\u221e and \u03b1 := MN . The result dates back to sixties and can be found in [Marc\u030cenko and Pastur, 1967]. Note that if M > N then there are M \u2212 N trivial eigenvalues of G at zero. Also, the width of the nontrivial distribution essentially depends on the ratio \u03b1. Clearly, setting the expected covariance to identity is very limiting. One of the earliest relaxations appear in [Baik et al., 2005]. They prove a phase transition for the largest eigenvalues of the sample covariance matrix which has been known as the BBP phase transition. A case that may be useful for our setup is as follows: Theorem 1 (Baik, Arous, P\u00e9ch\u00e9, et al., 2005). If \u03a3 = diag(`, 1, . . . , 1), ` > 1, and M,N \u2192 \u221e with MN = \u03b1 \u2265 1. Let c = 1 + \u221a \u03b1, and let\u2019s call the top eigenvalue of the sample covariance matrix as \u03bbmax then:\n\u2022 If 1 \u2264 ` < c then \u03bbmax is at the right edge of the spectrum with Tracy-Widom fluctuations. \u2022 If c < ` then \u03bbmax is an outlier that is away from bulk centered at `(1 + \u03b1`\u22121 ) with Gaussian\nfluctuations.\nTypically, due to the correlations in the problem we don\u2019t have \u03a3 to be the identity matrix or a diagonal matrix with spikes. This makes the analysis of their spectrum a lot more difficult. A solution\nfor this slightly more general case with non-trivial correlations has been provided only recently by Bloemendal et al. [2016]. We will briefly review these results here see how they are related to the first term of the above decomposition. Theorem 2 (Bloemendal, Knowles, Yau, and Yin, 2016). If d = M , \u03a3 \u2212 Id has bounded rank, logN is comparable to logM , and entries of X are independent with mean zero and variance one, then the spectrum of \u03a3 can be precisely mapped to the one of G as M,N \u2192 \u221e for fixed \u03b1 = MN . Let K = min{M,N}, and the decomposition of the spectrum can be described as follows:\n\u2022 Zeros: M \u2212K many eigenvalues located at zero (if M > N ). \u2022 Bulk: Order K many eigenvalues are distributed according to Marc\u030cenko-Pastur law. \u2022 Right outliers: All eigenvalues of \u03a3 that exceed a certain value produce large-positive\noutlier eigenvalues to the right of the spectrum of G.\n\u2022 Left outliers: All eigenvalues of \u03a3 that are close to zero produce small outlier eigenvalues between 0 and the left edge of the bulk of G.\nMoreover, the eigenvectors of outliers of G are close to the corresponding ones of \u03a3."}, {"heading": "2.2 Applications and experiments on artificial data", "text": "Previous section essentially describes the way in which one obtains outlier eigenvalues in the sample covariance matrix assuming the population covariance is known. Here are some examples: Example 4 (Perceptron). The Hessian for the Perceptron with regular hinge-loss is the zero matrix. It has no local curvature, the weight updates are driven by the examples. Though this doesn\u2019t mean that the landscape is trivial: the landscape is composed of constant gradient patches whose boundaries are determined by the sign changes of f . Example 5 (Logistic regression). A slightly less trivial version could be considered for the log-loss `(s, y) = \u2212y log 11+e\u2212s \u2212 (1 \u2212 y) log(1 \u2212 1 1+e\u2212s ) and a single neuron with sigmoid. Note that `(s, y) is convex in s for fixed y, and we can apply the decomposition using ` and f(w, x) = \u3008w, x\u3009. In this case we have M = d and the assumptions can be checked easily for the matrix g. Also, note that the second part of the Hessian is zero since\u22072f(w, x) = 0. So the Hessian of the loss is just the first term: G. It is straightforward to calculate that the gradient per sample is of the form g = c(w, x, y)IdMx for a positive constant c = c(w, x, y) that depends on y. If we had only one class in the data, then the we wouldn\u2019t have the dependency on y and this case would fall into the classical Marc\u030chenko-Pastur law (left pane of 2). However once we have more than one class, we can\u2019t get rid of the y dependency and the spectrum changes. What should we expect with the dependency? It turns out that in that case the weights have one large outlier eigenvalue, and a bulk that\u2019s close to zero (right pane of 2). Example 6 (Toy layered network). Take the same ` from the previous example, and set f(w) = Wl . . .W1x where Wl is a column vector so that f is real valued, and w is the flattened weight vectors. Note that we can write both gi and Hi in weight-matrix times input form. We emphasize that the the weight-matrix may depend on the label of its input. We also note that Hi has a block structure that is separated among its layers and its terms are somewhat sparse.\nExample 7 (A network with non-linearity). The general structure of the Hessian should carry over to the case when the non-linearity is ReLU, too, since this case can be considered as a piece-wise matrix multiplication where the boundaries of the pieces depend on the data. As we have seen in the logistic regression case, this dependency can make or break things. Also, it is harder in the case of sigmoid non-linearities since now we may not have the nice matrix multiplication representation. Due to these considerations this case seems to be a non-trivial candidate for counting large eigenvalues.\nIn the subsequent experiment, we used a feed-forward neural network with a 100 dimensional input layer, two hidden layers each of which with 30 hidden units, and a k dimensional output layer that is combined with softmax for k-class classification. We sampled random k Gaussian clusters in the input space and normalized the data globally. Then we carried out the training for the following sets of parameters: k : {2, 5, 10, 20, 50}, algorithm: {GD, SGD}, non-linearity: {tanh, ReLU}, initial multipler for the covariance of the input distribution: {1, 10}. Then we counted the number of large eigenvalues according to three different cutoff methods: largest consecutive gap, largest consecutive ratio, and a heuristic method of determining the threshold by searching for the elbow in the scree plot (see Figure 4 right pane). In Table 1 we marked the ones that are off by \u00b11.\n0 10 20 30 40 50 Order of largest components\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCu m\nul at\niv e\nex pl\nai ne\nd va\nria nc\ne ra\ntio\nPrinciple components of the data\n%90 line %80 line 2 classes 5 classes 10 classes 20 classes 50 classes\nFigure 3: PCA of data.\n020406080100120 Order of largest eigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nEi ge\nnv al\nue s\n1e 2 Gaps in eigenvalue distribution - ReLU Heuristic threshold 2 5 10 20 50\nFigure 4: Scree plot."}, {"heading": "3 Larger scale experiments", "text": "In this section we show two experiments in an attempt to address the following questions: (1) How does the spectrum (and hence the geometry) change when we just increase the size of the system? (2) What does over-parametrization imply on the discussion around GD vs. SGD or large batch vs small batch?"}, {"heading": "3.1 MNIST and growing the network size", "text": "We test the effect of growing the size of the network with fixed data, architecture, and algorithm. We sample 1K examples from the MNIST dataset, and train all the networks with the same step size until a sufficient stopping condition is satisfied. Then we compute the exact Hessian and plot its eigenvalues in Figures 5 and 6. Note that we use different ways of plotting the left pane and the right pane: the left pane is scaled to indicate the ratios of eigenvalues whereas the right pane shows the counts. The fact that this particular choice creates consistent plots should not be surprising since this confirms our previous suspicions about the fact that adding neurons in the system proportionally enlarges the bulk, but the outliers on the right depend on data, so their number should not, in principle, change with the increasing dimensionality of the parameter space.\n0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 Percentage of small eigenvalues\n7\n6\n5\n4\n3\n2\n1\n0\nEi ge\nnv al\nue s\n1e 5 Left edge of the spectrum\n7960 23860 39760 55660\nFigure 5: Left edge - scaled.\n20406080100120 Order of large eigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\nEi ge\nnv al\nue s\n1e 1 Right edge of the spectrum 7960 23860 39760 55660\nFigure 6: Right edge."}, {"heading": "3.2 CIFAR10 and large/small batch comparison", "text": "A common way to plot training profiles in larger scale neural networks is to stop every epoch to reserve extra computational power to calculate various statistics of the model at its current position. This becomes problematic when one compares training with different batch sizes, primarily because the larger batch model takes fewer steps in a given epoch. Recall that the overall loss is averaged, therefore, for a fixed point in the weight space, the empirical average of the gradients is an unbiased estimator for the expected gradient. Hence it is reasonable to expect that the norms of the large batch methods match to the ones of the small batch. And for a fair comparison, one should use the same learning rate for both training procedures. This suggests that a better comparison between GD and SGD (or LB and SB) should be scaled with the number of steps, so that, on average both algorithms are able to take similar number of steps of comparable sizes. Moreover, since random initial points in high dimensional spaces are almost always orthogonal, the fact that training with random initial points may lead to different basins should not be a surprising observation. However, the observation that LB converges to tighter basins that is separated by wells from the wider basins sound by SB has been an observation that triggered attention. To test this idea even further we conduct the following experiment:\n1. Part I: Train full CIFAR10 data for a bare AlexNet (no momentum, no dropout, no batch normalization) with a batch-size of 1, 000. Record every 100 steps for 250 times.\n2. Part II: Continue training from the end point of the previous step with a smaller batch-size of 32. Everything else, including the constant learning rate is kept the same. And train another 250 periods each of which with 100 steps.\nThe key observation is the jump in the training and test losses, and a drop in the corresponding accuracies (Figure 7). Toward the end of Phase II the small batch reaches to a slightly better accuracy.\nAnd this looks in line with the observations in Keskar et al. [2016], in that, it appears that the LB solution and SB solutions are separated by a barrier, and that the latter of which generalizes better. Moreover, the line interpolations extending away from either end points appear to be confirming the sharpness of LB solution. However, we find the line interpolation connecting the end points of Part I and Part II turns out to not contain any barriers (Figure 8). This suggests that while the small and large batch method converge to different solutions, these solutions have been in the same basin all along. This raises the striking possibility that that other seemingly different solutions may be similarly connected by a flat region to form a larger basin.\nOne way to interpret the results goes through the Gauss-Newton decomposition introduced earlier. When we decrease the batch size, we increase the noise in the covariance of the gradients, and hence the first term starts to dominate. Even when the weight space has large flat regions, the fluctuations of the stochastic noise should be precisely in the directions of the large eigenvalues."}, {"heading": "4 Conclusion", "text": "We have shown that the level of singularity of the Hessian cannot be ignored. We use the generalized Gauss-Newton decomposition of the Hessian to argue the cluster of zero eigenvalues are to be expected in practical applications. This allows us to divide the process of training into two parts with initial fast decay and final slow progress. We examine the option that the inherent geometry at the bottom of the landscape could be accountable for the slow progress in the final phase due to its extremely flat structure.\nOne of the most striking implications of flatness may be the connected structure of the solution space. We may wonder whether two given solutions can be connected by a continuous path of solutions. This question have been explored in a recent work: in Freeman and Bruna [2016] it shown that for one hidden layer rectified neural networks the solution space is connected which is consistent with the flatness of the landscape.\nWe speculate that the classical notions of basins of attractions may not be the suitable objects to study for neural networks. Rather, we may look at the interiors of level sets (also called the excursion sets) of the landscape. Given a loss-level u the excursion set below u is the set of all points of the domain that has loss value less than or equal to u: A(u) = {w : w \u2208 RM&L(w) \u2264 u}. Exploring A(u) is challenging and gradient based methods seem to be less suitable for this task. For further research we will examine the distinction between the gradient based part and the excursion set exploration of training."}], "references": [{"title": "Random matrices and complexity of spin glasses", "author": ["Antonio Auffinger", "G\u00e9rard Ben Arous", "Ji\u0159\u00ed \u010cern\u1ef3"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Auffinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auffinger et al\\.", "year": 2013}, {"title": "Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices", "author": ["Jinho Baik", "G\u00e9rard Ben Arous", "Sandrine P\u00e9ch\u00e9"], "venue": "The Annals of Probability,", "citeRegEx": "Baik et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Baik et al\\.", "year": 2005}, {"title": "Energy landscapes for machine learning", "author": ["Andrew J Ballard", "Ritankar Das", "Stefano Martiniani", "Dhagash Mehta", "Levent Sagun", "Jacob D Stevenson", "David J Wales"], "venue": "Physical Chemistry Chemical Physics,", "citeRegEx": "Ballard et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ballard et al\\.", "year": 2017}, {"title": "On the principal components of sample covariance matrices", "author": ["Alex Bloemendal", "Antti Knowles", "Horng-Tzer Yau", "Jun Yin"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bloemendal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bloemendal et al\\.", "year": 2016}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of Neuro-N\u0131mes,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Parallelization of a neural network learning algorithm on a hypercube. Hypercube and distributed computers", "author": ["J Bourrely"], "venue": "Elsiever Science Publishing,", "citeRegEx": "Bourrely.,? \\Q1989\\E", "shortCiteRegEx": "Bourrely.", "year": 1989}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Pratik Chaudhari", "Anna Choromanska", "Stefano Soatto", "Yann LeCun", "Carlo Baldassi", "Christian Borgs", "Jennifer Chayes", "Levent Sagun", "Riccardo Zecchina"], "venue": "arXiv preprint arXiv:1611.01838,", "citeRegEx": "Chaudhari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2016}, {"title": "Sharp minima can generalize for deep nets", "author": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1703.04933,", "citeRegEx": "Dinh et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2017}, {"title": "Topology and geometry of deep rectified network optimization landscapes", "author": ["C Daniel Freeman", "Joan Bruna"], "venue": "arXiv preprint arXiv:1611.01540,", "citeRegEx": "Freeman and Bruna.,? \\Q2016\\E", "shortCiteRegEx": "Freeman and Bruna.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Efficient backprop", "author": ["Y LeCun", "L Bottou", "GB ORR", "K-R M\u00fcller"], "venue": "Lecture notes in computer science,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["Vladimir A Mar\u010denko", "Leonid Andreevich Pastur"], "venue": "Mathematics of the USSR-Sbornik,", "citeRegEx": "Mar\u010denko and Pastur.,? \\Q1967\\E", "shortCiteRegEx": "Mar\u010denko and Pastur.", "year": 1967}, {"title": "The landscape of empirical risk for non-convex losses", "author": ["Song Mei", "Yu Bai", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1607.06534,", "citeRegEx": "Mei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Numerical optimization, second edition", "author": ["Jorge Nocedal", "Stephen J Wright"], "venue": "Numerical optimization,", "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Gradient descent only converges to minimizers: Nonisolated critical points and invariant regions", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "Panageas and Piliouras.,? \\Q2016\\E", "shortCiteRegEx": "Panageas and Piliouras.", "year": 2016}, {"title": "Fast exact multiplication by the hessian", "author": ["Barak A Pearlmutter"], "venue": "Neural computation,", "citeRegEx": "Pearlmutter.,? \\Q1994\\E", "shortCiteRegEx": "Pearlmutter.", "year": 1994}, {"title": "Explorations on high dimensional landscapes", "author": ["Levent Sagun", "V U\u011fur G\u00fcney", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "ICLR 2015 Workshop Contribution,", "citeRegEx": "Sagun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Singularity of the hessian in deep learning", "author": ["Levent Sagun", "L\u00e9on Bottou", "Yann LeCun"], "venue": "arXiv preprint arXiv:1611.07476,", "citeRegEx": "Sagun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2016}, {"title": "No more pesky learning rates", "author": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "venue": "ICML (3),", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. [2016]: Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers.", "startOffset": 103, "endOffset": 123}, {"referenceID": 14, "context": "And the behaviour of the two quantities may be drastically different (for a recent analysis on provable estimates see [Mei et al., 2016]).", "startOffset": 118, "endOffset": 136}, {"referenceID": 15, "context": "More involved algorithms, such as Newton type methods, make use of second order information [Nocedal and Wright, 2006].", "startOffset": 92, "endOffset": 118}, {"referenceID": 5, "context": "When the gradients are computationally expensive, one can alternatively use it\u2019s stochastic version (SGD) that replaces the above gradient with the gradient of averages of losses over subsets (such a subset will be called the mini-batch) of D (see [Bottou, 2010] for a classical reference).", "startOffset": 248, "endOffset": 262}, {"referenceID": 20, "context": "More involved optimal step size choices involve some kind of second order information that can be obtained from the Hessian of the loss function [Schaul et al., 2013].", "startOffset": 145, "endOffset": 166}, {"referenceID": 16, "context": "A relaxation of the above convergence to the case of non-isolated critical points can be found in [Panageas and Piliouras, 2016].", "startOffset": 98, "endOffset": 128}, {"referenceID": 4, "context": "When the gradients are computationally expensive, one can alternatively use it\u2019s stochastic version (SGD) that replaces the above gradient with the gradient of averages of losses over subsets (such a subset will be called the mini-batch) of D (see [Bottou, 2010] for a classical reference). The benefit of SGD on real-life time limits is obvious, and GD may be impractical for practical purposes in many problems. In any case, the stochastic gradient can be seen as an approximation to the true gradient, and hence it is important to understand how the two directions are related in the parameter space. Therefore, the discussion around the geometry of the loss surface can be enlightening in the comparison of the two algorithms: Does SGD locate solutions of a different nature than GD? Do they follow different paths? If so, which one is better in terms of generalization performance? For the second problem of expensive line-search, there are two classical solutions: using a small, constant step size, or scheduling the step size according to a certain rule. In practice, in the context of deep learning, the values for both approaches are determined heuristically, by trial and error. More involved optimal step size choices involve some kind of second order information that can be obtained from the Hessian of the loss function [Schaul et al., 2013]. From a computational point of view, obtaining the Hessian is extremely expensive, however obtaining some of its largest and smallest eigenvalues and eigenvectors are not that expensive. Is it enough to know only those eigenvalues and eigenvectors that are large in magnitude? How do they change through SGD? For the third problem, let\u2019s look at the Hessian a little closer. A critical point is defined by w such that ||\u2207L(w)|| = 0 and the nature of it can be determined by looking at the signs of its Hessian matrix. If all eigenvalues are positive the point is called a local minimum, if r of them are negative and the rest are positive, then it is called a saddle point with index r. At the critical point, the eigenvectors indicate the directions in which the value of the function locally changes. Moreover the changes are proportional to the corresponding -signed- eigenvalue. Under sufficient regularity conditions, it is rather straightforward to show that gradient based methods converge to points where gradient is zero. Recently Lee et al. [2016] showed that they indeed converge to minimizers.", "startOffset": 249, "endOffset": 2415}, {"referenceID": 4, "context": "Bottou [1991] points out that large eigenvalues of the Hessian of the loss can create the illusion of a local minima and GD can get stuck there, it further claims that the help of the inherent noise in SGD may help getting out of this obstacle.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "Bottou [1991] points out that large eigenvalues of the Hessian of the loss can create the illusion of a local minima and GD can get stuck there, it further claims that the help of the inherent noise in SGD may help getting out of this obstacle. The origin of this observation is due to Bourrely [1989], as well as numerical justifications.", "startOffset": 0, "endOffset": 302}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 10, "context": "A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016].", "startOffset": 111, "endOffset": 132}, {"referenceID": 7, "context": "Based on exactly this observation combined with the K-replica method of the previous paragraph, a recent work produced promising practical results [Chaudhari et al., 2016].", "startOffset": 147, "endOffset": 171}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al.", "startOffset": 46, "endOffset": 97}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks.", "startOffset": 46, "endOffset": 123}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value.", "startOffset": 46, "endOffset": 1298}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value. Such regions at the bottom of the landscape are called the flat minima, which can be considered as another way of saying a very wide minima. It is further noted that such minima have better generalization properties and a new loss function that makes use of the Hessian of the loss function has been proposed that targets the flat minima. The computational complexity issues have been attempted to be resolved using the R-operator of Pearlmutter [1994]. However, the new loss requires all the entries of the Hessian, and even with the R-operator it is unimaginably slow for today\u2019s large networks.", "startOffset": 46, "endOffset": 1882}, {"referenceID": 0, "context": "A theoretical review on this can be found in [Auffinger et al., 2013], while Sagun et al. [2014] and Ballard et al. [2017] provide an experimental simulation as well as a numerical study for neural networks. They notably find that high error local minima traps do not appear when the model is over-parametrized. These concentration results can help explain why we find that the solutions attained by different optimizers like GD and SGD often have comparable training accuracies. However, while these methods find comparable solutions in terms of training error there is no guarantee they generalize equally. A recent work in this direction compares the generalization performance of small batch and large batch methods [Keskar et al., 2016]. They demonstrate that the large batch methods always generalize a little bit worse even when they have similar training accuracies. The paper further makes the observation that the basins found by small batch methods are wider, thereby contributing to the claim that wide basins, as opposed to narrow ones, generalize better 1. The final part of the historical account is devoted to the observation of flatness of the landscape in neural networks and it\u2019s consequences through the lens of the Hessian. In early nineties, Hochreiter and Schmidhuber [1997] remarks that there are parts of the landscape in which the weights can be perturbed without significantly changing the loss value. Such regions at the bottom of the landscape are called the flat minima, which can be considered as another way of saying a very wide minima. It is further noted that such minima have better generalization properties and a new loss function that makes use of the Hessian of the loss function has been proposed that targets the flat minima. The computational complexity issues have been attempted to be resolved using the R-operator of Pearlmutter [1994]. However, the new loss requires all the entries of the Hessian, and even with the R-operator it is unimaginably slow for today\u2019s large networks. More recently, an exact numerical calculation of the Hessian have been carried out by Sagun et al. [2016]. It turns out that the Hessian is degenerate at any given point including the randomly chosen initial point, that the spectrum of it is composed of two parts: (1) the bulk, and (2) the outliers.", "startOffset": 46, "endOffset": 2133}, {"referenceID": 11, "context": "We study the Hessian using a decomposition [LeCun et al., 1998] as a sum of two matrices,\u2207L(w) = G(w) +H(w) where G is the sample covariance matrix of the gradients of model outputs and H is the Hessian of the model outputs.", "startOffset": 43, "endOffset": 63}, {"referenceID": 8, "context": "An example is the recent, Dinh et al. [2017], work shows how sharp minima can still generalize with proper modifications to the loss function.", "startOffset": 26, "endOffset": 45}, {"referenceID": 13, "context": "The result dates back to sixties and can be found in [Mar\u010denko and Pastur, 1967].", "startOffset": 53, "endOffset": 80}, {"referenceID": 1, "context": "One of the earliest relaxations appear in [Baik et al., 2005].", "startOffset": 42, "endOffset": 61}, {"referenceID": 3, "context": "for this slightly more general case with non-trivial correlations has been provided only recently by Bloemendal et al. [2016]. We will briefly review these results here see how they are related to the first term of the above decomposition.", "startOffset": 101, "endOffset": 126}, {"referenceID": 10, "context": "And this looks in line with the observations in Keskar et al. [2016], in that, it appears that the LB solution and SB solutions are separated by a barrier, and that the latter of which generalizes better.", "startOffset": 48, "endOffset": 69}, {"referenceID": 9, "context": "This question have been explored in a recent work: in Freeman and Bruna [2016] it shown that for one hidden layer rectified neural networks the solution space is connected which is consistent with the flatness of the landscape.", "startOffset": 54, "endOffset": 79}], "year": 2017, "abstractText": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. [2016]: Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecturealgorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: a gradient based method appears to be first climbing uphill and then falling downhill between two points; whereas, in fact, they lie in the same basin.", "creator": "LaTeX with hyperref package"}}}