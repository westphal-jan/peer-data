{"id": "1706.04732", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Revenue Optimization with Approximate Bid Predictions", "abstract": "In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.\n\n\n\n\nTo demonstrate our approach to using a model of prediction using the model of prediction as a means for predicting a loss, we used a model known as an implicit regression model. A model that is designed to predict the outcome of a prediction is a model designed to predict a loss. A model that does not accurately predict the outcome of a prediction is a model designed to predict a loss. We used a model of prediction, but instead of looking at the results in a model that is fully predictive, we use the models from the model of the prediction to model how the distribution of the predicted cost is determined. We find that by measuring the price of the model, we can reduce the number of bids to a certain level, and reduce the number of bids that will result. To achieve this, we employ two approaches to reduce the cost of the prediction: 1) by limiting the number of bids to a certain level, and 2) by using the model of the prediction to model how the distribution of the prediction cost is determined, and by employing the model of the prediction to model how the distribution of the predicted cost is determined. We use the models from the model of the prediction to model how the distribution of the prediction cost is determined.\n\nIn this paper, we introduce two models based on the assumption that each model is a prediction for a given condition (Fig. 1), and define how the probability of prediction for a given condition is determined.\nThe model that we use is a prediction for a given condition (Fig. 1). A prediction for a given condition (Fig. 1) is the probability of a given condition (Fig. 1), and the probability of a given condition (Fig. 1). The probability of a given condition (Fig. 1) is the probability of a given condition (Fig. 1).\nWe apply these models as a means for predicting a loss in the probability of a given condition (Fig. 1). For each condition,", "histories": [["v1", "Thu, 15 Jun 2017 04:05:56 GMT  (64kb,D)", "http://arxiv.org/abs/1706.04732v1", "Submitted to NIPS 2017"]], "COMMENTS": "Submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["andr\\'es mu\\~noz medina", "sergei vassilvitskii"], "accepted": true, "id": "1706.04732"}, "pdf": {"name": "1706.04732.pdf", "metadata": {"source": "CRF", "title": "Revenue Optimization with Approximate Bid Predictions", "authors": ["Andr\u00e9s Mu\u00f1oz Medina", "Sergei Vassilvitskii"], "emails": ["sergeiv}@google.com"], "sections": [{"heading": "1 Introduction", "text": "A crucial task for revenue optimization in auctions is setting a good reserve (or minimum) price. Set it too low, and the sale may yield little revenue, set it too high and there may not be anyone willing to buy the item. The celebrated work by Myerson [1981] shows how to optimally set reserves in second price auctions, provided the value distribution of each bidder is known.\nIn practice there are two challenges that make this problem significantly more complicated. First, the value distribution is never known directly; rather, the auctioneer can only observe samples drawn from it. Second, in the context of ad auctions, the items for sale are heterogeneous, and there are literally trillions of different types of items being sold. It is therefore likely that a specific type of item has never been observed previously, and no information about its value is known.\nA standard machine learning approach that addresses the heterogeneity problem parametrizes each impression by a feature vector, with the underlying assumption that bids observed from auctions with similar features will be similar. In online advertising. these features encode, for instance, the ad size, whether the user is on mobile or desktop, etc.\nThe question is, then, how to use the features to set a good reserve price for a particular ad opportunity. On the face of it, this sounds like a standard machine learning question\u2014given a set of features, predict the value of the maximum bid. The difficulty comes from the shape of the loss function. Much of the machine learning literature is concerned with optimizing well behaved loss functions, such as squared loss, or hinge loss. The revenue function, on the other hand is non-continuous and strongly non-concave, making a direct attack a challenging proposition.\nIn this work we take a different approach and reduce the problem of finding good reserve prices to a prediction problem under the squared loss. In this way we can rely upon many widely available and scalable algorithms developed to minimize this objective. We proceed by using the predictor to define a judicious clustering of the data, and then compute the empirically maximizing reserve price for each group. Our reduction is simple and practical, and directly ties the revenue gained by the algorithm to the prediction error.\nar X\niv :1\n70 6.\n04 73\n2v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20"}, {"heading": "1.1 Related Work", "text": "Optimizing revenue in auctions has been a rich area of study, beginning with the seminal work of Myerson [1981] who introduced optimal auction design. Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions.\nMost of the classical literature on revenue optimization focuses on the design of optimal auctions when the bidding distribution of buyers is known. More recent work has considered the computational and information theoretic challenges in learning optimal auctions from data. A long line of work [Cole and Roughgarden, 2015, Devanur et al., 2016, Dhangwatnotai et al., 2015, Morgenstern and Roughgarden, 2015, 2016] analyzes the sample complexity of designing optimal auctions. The main contribution of this direction is to show that under fairly general bidding scenarios, a near-optimal auction can be designed knowing only a polynomial number of samples from bidders\u2019 valuations. Other authors, [Leme et al., 2016, Roughgarden and Wang, 2016] have focused on the computational complexity of finding optimal reserve prices from samples, showing that even for simple mechanisms the problem is often NP-hard to solve directly.\nAnother well studied approach to data-driven revenue optimization is that of online learning. Here, auctions occur one at a time, and the learning algorithm must compute prices as a function of the history of the algorithm. These algorithms generally make no distributional assumptions and measure their performance in terms of regret: the difference between the algorithm\u2019s performance and the performance of the best fixed reserve price in hindsight. Kleinberg and Leighton [2003] developed an online revenue optimization algorithm for posted-price auctions that achieves low regret. Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015].\nA natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information).\nThis problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by \u03c3 \u2208 [0, 1], thus limiting the applicability of these results\u2014online advertising auctions are normally parameterized by a large set of features. Finally, Cui et al. [2011] proposes partitioning data into clusters and solving the i.i.d. problem for each cluster. The crucial choice of a partition, however, is heuristic and thus provides no guarantees on the achievable revenue.\nOur results. We show that given a predictor of the bid with squared loss of \u03b72, we can construct a reserve function r that extracts all but g(\u03b7) revenue, for a simple increasing function g. (See Theorem 2 for the exact statement.) To the best of our knowledge, this is the first result that ties the revenue one can achieve directly to the quality of a standard prediction task. Our algorithm for computing r is scalable, practical, and efficient.\nAlong the way we show what kinds of distributions are amenable to revenue optimization via reserve prices. We prove that when bids are drawn i.i.d. from a distribution F , the ratio between the mean bid and the revenue extracted with the optimum monopoly reserve scales as O(logVar(F ))\u2014Theorem 5. This result refines the log h bound derived by Goldberg et al. [2001], and formalizes the intuition that reserve prices are more successful for low variance distributions."}, {"heading": "2 Setup", "text": "We consider a repeated posted price auction setup where every auction is parametrized by a feature vector x \u2208 X and a bid b \u2208 [0, 1]. Let D be a distribution over X \u00d7 [0, 1]. Let h : X \u2192 [0, 1], be a bid prediction function and denote by \u03b72 the squared loss incurred by h:\nE[(h(x)\u2212 b)2] = \u03b72.\nWe assume h is given, and make no assumption on the structure of h or how it is obtained; it can, for example, be learned from other data. Let S = ( (x1, b1), . . . , (xm, bm) ) \u223c D be a set of m i.i.d. samples drawn from D and denote by SX = (x1, . . . , xm) its projection on X . Given a price p let Rev(p, b) = p1b\u2265p denote the revenue obtained when the bidder bids b. For a reserve price function r : X \u2192 [0, 1] we let:\nR(r) = E (x,b)\u223cD\n[ Rev(r(x), b) ] and R\u0302(r) = 1\nm \u2211 (x,b)\u2208S Rev(r(x), b)\ndenote the expected and empirical revenue of reserve price function r. We also let B = E[b], B\u0302 = 1m \u2211m i=1 bi denote the expected and empirical bids, and S(r) = B \u2212 R(r), S\u0302(r) = B\u0302 \u2212 R\u0302(r) denote the expected and empirical separation between bid values and the revenue. Notice that for a given reserve price function r, S(r) corresponds to revenue left on the table. Our goal is, given S and h, to find a function r that maximizes R(r) or equivalently minimizes S(r)."}, {"heading": "2.1 Generalization Error", "text": "Note that in our set up we are only given samples from the distribution, D, but aim to maximize the expected revenue. Understanding the difference between the empirical performance of an algorithm and its expected performance, also known as the generalization error, is a key tenet of learning theory.\nAt a high level, the generalization error is a function of the training set size: larger training sets lead to smaller generalization error; and the inherent complexity of the learning algorithm: simple rules such as linear classifiers generalize better than more complex ones.\nIn this paper we characterize the complexity of a class G of functions by its growth function \u03a0. The growth function corresponds to the maximum number of binary labelings that can be obtained by G over all possible samples SX . It is closely related to the VC-dimension when the range of functions in G takes values in {0, 1} and to the pseudo-dimension [Morgenstern and Roughgarden, 2015, Mohri et al., 2012] when G takes values in R.\nWe can give a bound on the generalization error associated with minimizing the empirical separation over a class of functions G. The following theorem is an adaptation of Theorem 1 of Mohri and Medina [2014] to our particular setup.\nTheorem 1. Let \u03b4 > 0, with probability at least 1 \u2212 \u03b4 over the choice of the sample S the following bound holds uniformly for r \u2208 G\nS(r) \u2264 S\u0302(r) + 2 \u221a log 1/\u03b4\n2m + 4\n\u221a 2 log(\u03a0(G,m))\nm . (1)\nTherefore, in order to minimize the expected separation S(r) it suffices to minimize the empirical sepa-\nration S\u0302(r) over a class of functions G whose growth function grows polynomially in m."}, {"heading": "3 Warmup", "text": "In order to better understand the problem at hand, we begin by introducing a straighforward mechanism for transforming the hypothesis function h to a reserve price function r with guarantees on its achievable revenue.\nLemma 1. Let r : X \u2192 [0, 1] be defined by r(x) := max(h(x) \u2212 \u03b72/3, 0). The function r then satisfies S(r) \u2264 \u03b71/2 + 2\u03b72/3.\nProof. By definition of S and r we have:\nS(r) = E[b\u2212 r(x)1b\u2265r(x)] = E[b\u2212 r(x)] + E[r(x)1b<r(x)] \u2264 E[b\u2212 h(x)] + E[h(x)\u2212 r(x)] + P (b < r(x)) \u2264 \u03b71/2 + \u03b72/3 + P (h(x)\u2212 b > \u03b72/3) (2)\n\u2264 \u03b71/2 + \u03b72/3 + E [ (h(x)\u2212 b)2 ] \u03b74/3\n(3)\n= \u03b71/2 + 2\u03b72/3,\nwhere (2) is a consequence of Jensen\u2019s inequality and we used Markov\u2019s inequality in (3).\nThis surprisingly simple algorithm shows there are ways to obtain revenue guarantees from a simple regressor. To the best of our knowledge these is the first guarantee of its kind. The reader may be curious about the choice of \u03b72/3 as the offset in our reserve price function. We will show that the dependence on \u03b72/3 is not a simple artifact of our analysis, but a cost inherent to the problem of revenue optimization.\nWhile this simple algorithm achieves a good theoretical bound, since the offset is static it makes no distinction between those features x, where the error is low, and those where the error is high. Thus for predictors, h, whose error is not uniform (i.e. when there are parts of the input space where h performs well, and other parts where h performs poorly), the static offset is large across the board, losing a lot of revenue in the process. In Section 7 we explore this situation empirically, and show that the static offset algorithm performs poorly in practice.\nIn the remainder of the paper we will introduce an algorithm with data dependent bounds on S(r). Moreover, we will remove the dependence on \u03b71/2 from the bound allowing for a wider range of values of \u03b7 for which the bound on S(r) is meaningful."}, {"heading": "4 Results Overview", "text": "In principle to maximize revenue we need to find a class of functions G with small complexity, but that contains a function which approximately minimizes the empirical separation. The difficulty stems from the fact that the revenue function, Rev, is not continuous and highly non-concave\u2014a small change in the price, p, may lead to very large changes in revenue. This is the main reason why simply using the predictor h(x) as a proxy for a reserve function is a poor choice, even if its average error, \u03b72 is small. For example a function h, that is just as likely to over-predict by \u03b7 as to under predict by \u03b7 will have very small error, but lead to 0 revenue in half the cases.\nA solution on the other end of the spectrum would simply memorize the optimum prices from the sample S, setting r(xi) = bi. While this leads to optimal empirical revenue, a function class G containing r would satisfy \u03a0(G,m) = 2m, making the bound of Theorem 1 vacuous.\nIn this work we introduce a family G(h, k) of classes parameterized by k \u2208 N. This family admits an approximate minimizer that can be computed in polynomial time, has low generalization error, and achieves provable guarantees to the overall revenue.\nMore precisely, we show that given S, and a hypothesis h with expected squared loss of \u03b72:\n\u2022 For every k \u2265 1 there exists a set of functions G(h, k) such that \u03a0(G(h, k),m) = O(m2k).\n\u2022 For every k \u2265 1, there is a polynomial time algorithm that outputs rk \u2208 G(h, k) such that in the worst case scenario S\u0302(rk) is bounded by O(\n1 k2/3 + \u03b72/3 + 1 m1/6 ).\nEffectively, we show how to transform any classifier h with low squared loss, \u03b72, to a reserve price predictor that recovers all but O(\u03b72/3) revenue in expectation."}, {"heading": "4.1 Algorithm Description", "text": "In this section we give an overview of the algorithm that uses both the predictor h and the set of samples S to develop a pricing function r. Our approach has two steps. First we partition the set of feasible prices, 0 \u2264 p \u2264 1, into k partitions, C1, C2, . . . , Ck. The exact boundaries between partitions depend on the samples S and their predicted values, as given by h. For each partition we find the price that maximizes the empirical revenue in the partition. We let r(x) return the empirically optimum price in the partition that contains h(x).\nFor a more formal description, let Tk be the set of k-partitions of the interval [0, 1] that is:\nTk = {t = (t0, t1, . . . , tk\u22121, tk) | 0 = t0 < . . . < tk = 1}.\nWe define G(h, k) = {x 7\u2192 \u2211k\u22121 j=0 ri1tj\u2264h(x)<tj+1 | rj \u2208 [ti, tj+1] and t \u2208 Tk}. A function in G(h, k) chooses k level sets of h and k reserve prices. Given x, price rj is chosen if x falls on the j-th level set. It remains to define the function rk \u2208 G(h, k). Given a partition vector t \u2208 Tk, let the partition Ch = {Ch1 , . . . , Chk } of X be given by Chj = {x \u2208 X |tj\u22121 < h(x) \u2264 tj}. Let mj = |SX \u2229 Chj | be the number of elements that fall into the j-th partition.\nWe define the predicted mean and variance of each group Cj as\n\u00b5hj = 1\nmj \u2211 xi\u2208Chj h(xi) and (\u03c3 h j ) 2 = 1 mj \u2211 xi\u2208Chj (h(xi)\u2212 \u00b5j)2.\nWe are now ready to present algorithm RIC-h for computing rk \u2208 Hk.\nAlgorithm 1 Reserve Inference from Clusters Compute th \u2208 Tk that minimizes 1m \u2211k\u22121 j=0 mj\u03c3 h j .\nLet Ch = Ch1 , Ch2 , . . . , Chk be the induced partitions. For each j \u2208 1, . . . , k, set rj = maxr r \u00b7 |{i|bi \u2265 r \u2227 xi \u2208 Chj }|. Return x 7\u2192 \u2211k\u22121 j=0 rj1h(x)\u2208Chj .\nOur main theorem states that the separation of rk is bounded by the cluster variance of Ch. For a partition C = {C1, . . . , Ck} of X let \u03c3j denote the empirical variance of bids for auctions in Cj . We define the weighted empirical variance by:\n\u03a6(C) : = k\u2211 j=1 \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Ck (bi \u2212 bi\u2032)2 = 2 k\u2211 j=1 mj \u03c3\u0302j (4)\nTheorem 2. Let \u03b4 > 0 and let rk denote the output of Algorithm 1 then rk \u2208 G(h, k) and with probability at least 1\u2212 \u03b4 over the samples S:\nS\u0302(rk) \u2264 (3B\u0302)1/3 ( 1\n2m \u03a6(Ch)\n) \u2264 (3B\u0302)1/3 ( 1 2k + 2 ( \u03b72 + \u221a log 1/\u03b4 2m )1/2)2/3 .\nNotice that our bound is data dependent and only in the worst case scenario it behaves like \u03b72/3. In general it could be much smaller. We further validate this empirically in Section 7.\nWe also show that the complexity of G(h, k) admits a favorable bound. The proof is similar to that in Morgenstern and Roughgarden [2015]; we include it in Appendix D for completness.\nTheorem 3. The growth function of the class G(h, k) can be bounded as: \u03a0(G(h, k),m) \u2264 m 2k\u22121\nkk .\nWe can combine these results with Equation 1 and an easy bound on B\u0302 in terms of B to conclude:\nCorollary 1. Let \u03b4 > 0 and let rk denote the output of Algorithm 1 then rk \u2208 G(h, k) and with probability at least 1\u2212 \u03b4 over the samples S:\nS(rk) \u2264 (3B\u0302)1/3 ( 1\n2m \u03a6(Ch)\n) +O (\u221ak logm m ) \u2264 (3B)1/3\u03b72/3 +O ( 1 k2/3 + ( log 1/\u03b4 2m )1/6 + \u221a k logm m ) .\nSince B \u2208 [0, 1], this implies that when k = \u0398(m3/7), the separation is bounded by 1.45\u03b72/3 plus additional error factors that go to 0 with the number of samples, m, as O\u0303(m\u22122/7)."}, {"heading": "5 Bounding Separation", "text": "In this section we prove the main bound motivating our algorithm. This bound relates the variance of the bid distribution and the maximum revenue that can be extracted. It formally shows what makes a distribution amenable to revenue optimization.\nTo gain intuition for the kind of bound we are striving for, consider a bid distribution F . If the variance of F is 0, that is F is a point mass at some value v, then setting a reserve price to v leads to no separation. On the other hand, consider the equal revenue distribution, with F (x) = 1\u2212 1/x. Here any reserve price leads to revenue of 1. However, the distribution has unbounded expected bid and variance, so it is not too surprising that more revenue cannot be extracted. We make this connection precise, showing that after setting the optimal reserve price, the separation can be bounded by a function of the variance of the distribution.\nGiven any bid distribution F over [0, 1] we denote by G(r) = 1\u2212 limr\u2032\u2192r\u2212 F (r\u2032) the probability that a bid is greater than or equal to r. Finally, we will let R = maxr rG(r) denote the maximum revenue achievable when facing a bidder whose bids are drawn from distribution F . As before we denote by B = Eb\u223cF [b] the bid and by S = B \u2212R the expected separation of distribution F .\nTheorem 4. Let \u03c32 denote the variance of F . Then \u03c32 \u2265 2R2e SR \u2212B2 \u2212R2.\nLet x(q) = sup{x|G(x) \u2265 q} denote the pseudo-inverse of G. Notice in particular that when G is strictly decreasing then x = G\u22121. When it is clear from context we will refer to a distribution indistinctly by F , G or x.\nWe will use the following expressions for the expected bid and second moment of a distribution. The proof of this lemma is given in Appendix A\nLemma 2. The expected bid and second moments of any distribution F are given respectively by:\nB = \u222b 1 0 x(q)dq and s2 = \u222b 1 0 x(q)2dq.\nIn order to prove the bound of Theorem 4 holds, we consider the following optimization problem over the space of square integrable functions L2[0, 1]:\nmin x\u2208L2[0,1]\n1\n2 \u222b 1 0 x2(q)dq (5)\ns.t. \u222b 1 0 x(q) = B and R \u2265 q x(q) \u2200q.\nWe show that the value of this optimization problem is greater than 12 ( 2R2e S R \u2212R2 ) . Since any distribution x(q) achieving revenue R and separation S is feasible for (5) it follows that it must satisfy \u03c32 = s2 \u2212 B2 \u2265 2R2e S R \u2212B2 \u2212R2.\nProposition 1. The objective value of (5) is lower bounded by:\nB2\n2 + max v\u2208L2[0,1]:v\u22650 \u22121 2 (\u222b 1 0 (qv(q))2 \u2212 (\u222b 1 0 qv(q) )2) +B \u222b 1 0 qv(q)\u2212R \u222b 1 0 v(q). (6)\nProof. For any \u03bb \u2208 R and v \u2208 L2[0, 1] define the Lagrangian\nL(x, \u03bb, v) = 1\n2 \u222b 1 0 x(q)2 \u2212 \u03bb \u222b 1 0 x(q) + \u03bbB + \u222b 1 0 v(q)(q x(q)\u2212R).\nIt is immediate to see that optimization problem (5) is equivalent to\nmin x\u2208L2[0,1] max \u03bb\u2208R,v\u22650 L(x, \u03bb, v) \u2265 max \u03bb\u2208R,v\u22650 min x\u2208L2[0,1] L(x, \u03bb, v).\nBy taking variational derivatives of the function L with respect to x we see that the minimizing solution x(q) satisfies:\nx(q) = \u03bb\u2212 qv(q).\nReplacing this value in the function L we see that problem (5) is lower bounded by:\nmax \u03bb,v\u22650 \u03bbB \u2212R \u222b 1 0 v(q)\u2212 1 2 \u222b 1 0 (\u03bb\u2212 qv(q))2\nWe can solve for the unconstrained variable \u03bb to obtain \u03bb = B+ \u222b 1 0 qv(q). Replacing this value in the above expression yields:\nmax v\u22650 \u2212R \u222b 1 0 v(q) + 1 2 (\u222b 1 0 qv(q) +B )2 \u2212 1 2 \u222b 1 0 (qv(q))2.\nExpanding the quadratic term yields:\nB2\n2 + max v\u22650 \u22121 2 (\u222b 1 0 (qv(q))2 \u2212 (\u222b 1 0 qv(q) )2) +B \u222b 1 0 qv(q)\u2212R \u222b 1 0 v(q).\nTo obtain a lower bound on (6) we simply need to evaluate the objective function at a feasible function v. In particular we let\nv(q) = R\nq [1 s \u2212 1 q ] 1q>s (7)\nwith s = e\u2212 S R . Notice that v is clearly in L2[0, 1] and v \u2265 0. The choice of this function is highly motivated by the solution to the unconstrained version of problem (6).\nProposition 2. The optimization problem\nmax v\u2208L2[0,1]:v\u22650 \u22121 2 (\u222b 1 0 (qv(q))2 \u2212 (\u222b 1 0 qv(q) )2) +B \u222b 1 0 qv(q)\u2212R \u222b 1 0 v(q) (8)\nis lower bounded by 12\n( 2R2e S R \u2212R2 \u2212B2 ) .\nProof. Let v(q) be defined by (7). Using the fact that log s = \u2212 SR = R\u2212B R we have the following equalities:\u222b 1\n0\nqv(q) = R \u222b 1 s 1 s \u2212 1 q = R (1\u2212 s s + log(s) ) = R\ns \u2212B (9)\u222b 1\n0\nv(q) = R ( \u2212 log s s + ( 1\u2212 1 s )) = R s (s\u2212 1\u2212 log s) = B +R(s\u2212 2) s . (10)\nIn view of (9) and (10) we have that for all q \u2265 s\nq2v(q)\u2212 q \u222b 1 s v(q) = Rq s \u2212R\u2212 Rq s +Bq = Bq \u2212R\nTherefore, the objective function of (8) evaluated at v(q) is given by\n1\n2\n( B \u222b 1 0 qv(q)\u2212R \u222b 1 0 v(q) ) .\nReplacing (9) and (10) on the expression above we obtain:\n1\n2 (BR s \u2212B2 \u2212 RB +R 2(s\u2212 2) s ) = 1 2 (2R2 s \u2212R2 \u2212B2 ) = 1\n2\n( 2R2e S R \u2212R2 \u2212B2 )\nCorollary 2. The following bound holds for any distribution F:\nS \u2264 (3R)1/3\u03c32/3 \u2264 (3B)1/3\u03c32/3\nProof. By Theorem 4 and using a third order Taylor expansion we have:\n\u03c32 \u2265 2R2e SR \u2212R2 \u2212B2\n\u2265 2R2 ( 1 + S\nR +\nS2\n2R2 +\nS3\n6R3\n) \u2212B2 \u2212R2\n= 2R2 + 2RS + S2 + S3\n3R \u2212B2 \u2212R2\n= (S \u2212R)2 \u2212B2 + S 3 3R = S3 3R .\nThe proof follows by rearranging terms.\nIn Appendix C we show that this bound is in fact tight."}, {"heading": "5.1 Approximating Maximum Revenue", "text": "In their seminal work Goldberg et al. [2001] showed that when faced with a bidder drawing values distribution F on [1,M ] with mean B, an auctioneer setting the optimum monopoly reserve would recover at least \u2126(B/ logM) revenue. We show how to adapt the result of Theorem 4 to refine this approximation ratio as a function of the variance of F .\nTheorem 5. For any distribution F with mean B and variance \u03c32, the maximum revenue with monopoly reserves, R, satisfies: BR \u2264 4.78 + 2 log ( 1 + \u03c3 2 B2 ) Proof. Let \u03b1 = BR . Note that \u03b1 \u2265 1. We begin by dividing both sides of the statement of 4 by R 2:\n\u03c32 B2 \u03b12 + \u03b12 \u2212 2e\u03b1\u22121 \u2265 \u22121\nRearranging, we have: \u03c32\nB2 \u03b12 + \u03b12 \u2265 2e\u03b1\u22121 \u2212 1. (11)\nSince \u03b1 \u2265 1, e\u03b1\u22121 \u2265 1. Therefore, if Equation 11 holds, then:\n\u03c32 B2 \u03b12 + \u03b12 \u2265 e\u03b1\u22121\n\u21d4 \u03b1 \u221a 1 + \u03c32\nB2 \u2265 e\n\u03b1\u22121 2\n\u21d4 2 \u221a e \u221a 1 + \u03c32\nB2 \u2265 e\n\u03b1/2\n\u03b1/2\nSuppose ex/x \u2264 t for some fixed t \u2265 2 \u221a e. Note that the function ex/x is increasing in x for x \u2265 1. Moreover, at x = 2 log t > 1 we have ex/x = t2/(2 log t) \u2265 t, since t > 2 log t for t > 2. Therefore x \u2264 2 log t. In our situation, we can then conclude that\n\u03b1 \u2264 4 log ( 2 \u221a e \u221a 1 + \u03c32\nB2\n) < 4.78 + 2 log ( 1 + \u03c32\nB2\n) .\nNote that since \u03c32 \u2264M2 this always leads to a tighter bound on the revenue."}, {"heading": "5.2 Partition of X", "text": "Corollary 2 suggests clustering points in such a way that the variance of the bids in each cluster is minimized. Given a partition {C1, . . . , Ck} of X we denote by mj = |SX \u2229 Cj |, B\u0302j = 1mj \u2211 i:xi\u2208Cj bi, \u03c3\u0302 2 j =\n1 mj \u2211 i:xi\u2208Cj (bi \u2212 B\u0302j) 2. Let also rj = argmaxp>0 p|{bi > p|xi \u2208 Cj}| and R\u0302j = rj |{bi > rj |xi \u2208 Cj}|. Lemma 3. Let r(x) = \u2211k j=1 rj1x\u2208Cj then S\u0302(r) \u2264 ( 3B\u0302 m2 )1/3(\u2211k j=1mj \u03c3\u0302j )2/3 .\nProof. Let S\u0302j = B\u0302j\u2212R\u0302j , Corollary 2 applied to the empirical bid distribution in Cj yields S\u0302j\u2264(3B\u0302j)1/3\u03c3\u03022/3j . Multiplying by\nmj m , summing over all clusters and using Ho\u0308lder\u2019s inequality gives:\nS\u0302(r) = 1\nm k\u2211 j=1 mjSj \u2264 1 m k\u2211 j=1 (3B\u0302j) 1/3\u03c3\u0302 2/3 j mj \u2264 ( k\u2211 j=1 3mj m B\u0302j )1/3( k\u2211 j=1 mj m \u03c3\u0302j )2/3 ."}, {"heading": "6 Clustering Algorithm", "text": "In view of Lemma 3 and since the quantity B\u0302 is fixed, we can find a function minimizing the expected separation by finding a partition of X that minimizes the weighted variance \u03a6(C) defined Section 4.1. From the definition of \u03a6, this problem resembles a traditional k-means clustering problem with distance function d(xi, xi\u2032) = (bi \u2212 bi\u2032)2. Thus, one could use one of several clustering algorithms to solve it. Nevertheless, in order to allocate a new point x \u2208 X to a cluster, we would require access to the bid b which at evaluation time is unknown. Instead, we show how to utilize the predictions of h to define an almost optimal clustering of X .\nFor any partition C = {C1, . . . , Ck} of X define\n\u03a6h(C) = k\u2211 j=1 \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Ck (h(xi)\u2212 h(xi\u2032))2.\nNotice that 12m\u03a6h(C) is the function minimized by Algorithm 1. The following lemma bounds the cluster variance achieved by clustering bids according to their predictions.\nLemma 4. Let h be a function such that 1m \u2211m i=1(h(xi) \u2212 bi)2 \u2264 \u03b7\u03022, and let C\u2217 denote the partition that minimizes \u03a6(C). If Ch minimizes \u03a6h(C) then\n\u03a6(Ch) \u2264 \u03a6(C\u2217) + 4m\u03b7\u0302.\nProof. From definition of \u03a6(C) and a straightforward application of the triangle inequality we have: \u03a6(Ch) = k\u2211 j=1 \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Chj (bi \u2212 bi\u2032)2\n\u2264 k\u2211 j=1 \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Chj (h(xi)\u2212 h(xi\u2032))2 + \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Chj (h(xi)\u2212 bi)2 + \u221a \u2211 i,i\u2032:xi,xi\u2032\u2208Chj (h(xi\u2032)\u2212 bi\u2032)2 = \u03a6h(Ch) + 2 k\u2211 j=1 \u221a mj \u2211 i:xi\u2208Chj (h(xi)\u2212 bi)2\n\u2264 \u03a6h(Ch) + 2 \u221a m \u221a\u221a\u221a\u221a\u221a k\u2211 j=1 \u2211 i:xi\u2208Chj (h(xi)\u2212 bi)2,\nwhere we have used Cauchy-Schwarz inequality for the last line. Using the property of h we can further bound the above expression as\n\u03a6(Ch) \u2264 \u03a6h(Ch) + 2m\u03b7\u0302 \u2264 \u03a6h(C\u2217) + 2m\u03b7\u0302, (12)\nwhere we have used the fact that Ch minimizes \u03a6h. Proceeding in the same manner as before, it is easy to see that \u03a6h(C\u2217) \u2264 \u03a6(C\u2217) + 2m\u03b7\u0302. Replacing this bound in (12) we recover the statement of the lemma.\nCorollary 3. Let rk be the output of Algorithm 1. If \u2211m j=1(h(xi)\u2212 bi)2 \u2264 m\u03b7\u03022 then:\nS\u0302(rk) \u2264 (3B\u0302)1/3 ( 1\n2m \u03a6(Ch)\n)2/3 \u2264 ( 1\n2m \u03a6(C\u2217) + 2\u03b7\u0302\n)2/3 . (13)\nProof. It is easy to see that the elements Chj of Ch are of the form Cj = {x|tj \u2264 h(x) \u2264 tj+1} for t \u2208 Tk. Thus if rk is the hypothesis induced by the partition Ch, then rk \u2208 G(h, k). The result now follows by definition of \u03a6 and lemmas 3 and 4.\nThe proof of Theorem 2 is now straightforward. Define a partition C by xi \u2208 Cj if bi \u2208 [ j\u22121 k , j k ] . Since\n(bi \u2212 bi\u2032)2 \u2264 1k2 for bi, bi\u2032 \u2208 Cj we have\n\u03a6(C) \u2264 k\u2211 j=1 \u221a m2j k2 = m k . (14)\nFurthermore since E[(h(x)\u2212 b)2] \u2264 \u03b72, Hoeffding\u2019s inequality implies that with probability 1\u2212 \u03b4:\n1\nm m\u2211 i=1 (h(xi)\u2212 bi)2 \u2264 ( \u03b72 + \u221a log 1/\u03b4 m ) . (15)\nIn view of inequalities (14) and (15) as well as Corollary 3 we have: S\u0302(rk) \u2264 (3B\u0302)1/3 ( 1\n2m \u03a6(C) + 2\n( \u03b72 + \u221a log 1/\u03b4\n2m\n)1/2)2/3 \u2264 (3B\u0302)1/3 ( 1 2k + 2 ( \u03b72 + \u221a log 1/\u03b4 2m )1/2)2/3 This completes the proof of the main result. To implement the algorithm, note that the problem of minimizing \u03a6h(C) reduces to finding a partition t \u2208 Tk such that the sum of the variances within the partitions is minimized. It is clear that it suffices to consider points tj in the set B = {h(x1), . . . , h(xm)}. With this observation, a simple dynamic program leads to a polynomial time algorithm with an O(km2) running time (see Appendix B)."}, {"heading": "7 Experiments", "text": "We now compare the performance of our algorithm against the following baselines:\n1. The offset algorithm presented in Section 3, where instead of using the theoretical offset \u03b72/3 we find the optimal t maximizing the empirical revenue \u2211m i=1 ( h(xi)\u2212 t)1h(xi)\u2212t\u2264bi , note that this makes the\nalgorithm even better.\n2. The DC algorithm introduced by Mohri and Medina [2014], which represents the state of the art in learning a revenue optimal reserve price and optimizes the empirical \u03b3-Lipschitz approximation to the revenue function.\nSynthetic data. We begin by running experiments on synthetic data to demonstrate the regimes where each algorithm excels. We generate feature vectors xi \u2208 R10 with coordinates sampled from a mixture of lognormal distributions with means \u00b51 = 0, \u00b52 = 1, variance \u03c31 = \u03c32 = 0.5 and mixture parameter p = 0.5. Let 1 \u2208 Rd denote the vector with entries set to 1. Bids are generated according to two different scenarios:\nLinear Bids bi generated according to bi = max(x > i 1 + \u03b2i, 0) where \u03b2i is a Gaussian random variable with\nmean 0, and standard deviation \u03c3 \u2208 {0.01, 0.1, 1.0, 2.0, 4.0}.\nBimodal Bids bi generated according to the following rule: let si = max(x > i 1+\u03b2i, 0) if si > 30 then bi = 40+\u03b1i\notherwise bi = si. Here \u03b1i has the same distribution as \u03b2i.\nThe linear scenario demonstrates what happens when we have a good estimate of the bids. The bimodal scenario models a buyer, which for the most part will bid as a continuous function of features but that is interested in a particular set of objects (for instance retargeting buyers in online advertisement) for which she is willing to pay a much higher price.\nFor each experiment we generated a training dataset Strain, a holdout set Sholdout and a test set Stest each with 16,000 examples. The function h used by RIC-h and the offset algorithm is found by training a linear regressor over Strain. For efficiency, we ran RIC-h algorithm on quanitzations of predictions h(xi). Quantized predictions belong to one of 1000 buckets over the interval [0, 50].\nFinally, the choice of hyperparameters \u03b3 for the Lipchitz loss and k for the clustering algorithm was done by selecting the best performing parameter over the holdout set. Following the suggestions in [Mohri and Medina, 2014] we chose \u03b3 \u2208 {0.001, 0.01, 0.1, 1.0} and k \u2208 {2, 4, . . . , 24}.\nFigures 1(a),(b) show the average revenue of the three approaches across 20 replicas of the experiment as a function of log(\u03c3). Revenue is normalized so that the DC algorithm revenue is 1.0 when \u03c3 = 0.01. The error bars at one standard deviation are indistinguishable in the plot. It is not surprising to see that in the linear scenario, the DC algorithm of [Mohri and Medina, 2014] and the offset algorithm outperform RIC-h under low noise conditions. Both algorithms will recover a solution close to the true weight vector 1. In this case the offset is minimal, thus recovering virtually all revenue. On the other hand, even if we set the optimal reserve price for every cluster, the inherent variance of each cluster makes us leave some revenue on the table. Nevertheless, notice that as the noise increases all three algorithms seem to achieve the same revenue. This is due to the fact that the variance in each cluster is comparable with the error in the prediction function h.\nThe results are reversed for the bimodal scenario where RIC-h outperforms both algorithms under low noise. This is due to the fact that RIC-h recovers virtually all revenue obtained from high bids while the offset and DC algorithms must set conservative prices to avoid losing revenue from lower bids.\nAuction data. In practice, however, neither of the synthetic regimes is fully representative of the bidding patterns. In order to fully evaluate RIC-h, we collected sampled auction bid data from an AdExchange for 4 different publisher-advertiser pairs. For each pair we sampled 100,000 examples with a set of discrete and continuous features. After expressing the discrete features as one-hot encoded vectors we obtain feature vectors in Rd for d \u2208 [100, 200] depending on the publisher-buyer pair. For each experiment, we extract a random training sample of 20,0000 points as well as a holdout and test sample. We repeated this experiment 20 times and present the results on Figure 1 (c) where we have normalized the data so that the performance\nof the DC algorithm is always 1. The error bars represent one standard deviation from the mean revenue lift. Notice that our proposed algorithm achieves on average up to 30% improvement over the DC algorithm. Moreover, the simple offset strategy never outperforms the clustering algorithm, and in some cases achieves significantly less revenue."}, {"heading": "8 Conclusion", "text": "We provided a simple, scalable reduction of the problem of revenue optimization with side information to the well studied problem of minimizing the squared loss. Our reduction provides the first polynomial time algoritm with a quantifiable bound on the achieved revenue. In the analysis of our algorithm we also provided the first variance dependent lower bound on the revenue attained by setting optimal monopoly prices. Finally, we provided extensive empirical evidence of the advantages of RIC-h over the current state of theart."}, {"heading": "9 Acknowledgements", "text": "We thank Eric Balkanski, Renato Paes Leme, and Martin Pa\u0301l for helpful discussions and comments on earlier drafts of this work. We thank the anonymous reviewer for pointing out the simple algorithm presented in Section 3."}, {"heading": "A Additional proofs", "text": "Lemma 2. The expected bid and second moments of any distribution F are given respectively by:\nB = \u222b 1 0 x(q)dq and s2 = \u222b 1 0 x(q)2dq.\nProof. We show the result only for the mean as the proof for the second moment is similar. It is well known that for a positive random variable, the mean can be expressed as:\nB = \u222b \u221e 0 G(x)dx = \u222b \u221e 0 \u222b G(x) 0 dqdx = \u222b D dqdx.\nwhere D = {(x, q) | x > 0 and q \u2264 G(x)}. Let D\u2032 = {(x, q) | 0 \u2264 q \u2264 1 and x \u2264 x(q)}. It is immediate that D \u2282 D\u2032 as q \u2264 G(x) implies by definition that x \u2264 x(q). We can thus decompose the above integral as:\u222b\nD\ndqdx = \u222b D\u2032 dqdx\u2212 \u222b D\u2032\u2212D dqdx.\nThe proof will be complete by showing that D\u2032\u2212D has Lebesgue measure 0. Indeed, in that case the above expression reduces to: \u222b\nD\u2032 dqdx = \u222b 1 0 \u222b x(q) 0 dxdq = \u222b 1 0 x(q)dq.\nLet us then characterize points (x, q) \u2208 D\u2032 \u2212D. Notice that if (x, q) /\u2208 D then G(x) < q but this again by definition implies x \u2265 x(q). If (x, q) is also in D\u2032 then we must have x = x(q). From which we conclude that limx\u2032\u2192x\u2212 G(x\n\u2032) \u2265 q > G(x). Thus (x, y) \u2208 D\u2032 \u2212D implies that x is a discontinuity of G. Finally, since G is decreasing there can be at most a countable number of discontinuities and thus D\u2032 \u2212D has measure 0."}, {"heading": "B Dynamic Program", "text": "Lemma 5. Let y1 \u2264 . . . \u2264 ym, there exists an algorithm with time complexity in O(km2) that finds a set of indices i0 = 1 \u2264 i1 \u2264 . . . ik\u22121 \u2264 ik = m minimizing\n\u03a6(i0, . . . , ik) = l\u2211 j=1 \u221a\u221a\u221a\u221a\u221a ij\u2211 i,i\u2032=ij\u22121 (yi \u2212 y\u2032i)2\nProof. For every l \u2264 k and r \u2264 m define\nAl,r = min i0=1\u2264...\u2264il=r k\u2211 j=1 \u221a\u221a\u221a\u221a\u221a ij\u2211 i,i\u2032=ij\u22121 (yi \u2212 y\u2032i)2\nIt is clear that Ak,m is equal to the minimum of \u03a6. We now show that Al,r satisfies the recursion:\nAl,r = min r\u2032<r Al\u22121,r\u2032 + \u221a\u221a\u221a\u221a r\u2211 i,i\u2032=r\u2032+1 (yi \u2212 yi\u2032)2.\nLet i0 \u2264 . . . \u2264 il denote the set of indices defining Al,r notice that by definition\nAl\u22121,il\u22121 \u2264 l\u22121\u2211 j=1 \u221a\u221a\u221a\u221a\u221a ij\u2211 i,i\u2032=ij=1 (yi \u2212 yi\u2032)2.\nTherefore\nmin r\u2032<r Al\u22121,r\u2032 + \u221a\u221a\u221a\u221a r\u2211 i,i\u2032=r\u2032+1 (yi \u2212 yi\u2032)2 \u2264 Al\u22121,il\u22121 + \u221a\u221a\u221a\u221a r\u2211 i,i\u2032=il\u22121 (yi \u2212 y\u2032i)2\n\u2264 Al,r.\nThe reverse inequality is trivial. We have thus shown that calculating Ak,m requires finding all values Al,r and each value can be calculated in O(m) time. Therefore, the complexity of the algorithm is in O(km2)."}, {"heading": "C Lower Bounds", "text": "Lemma 6. For any > 0 there exists a distribution F such that B \u2212R \u2265 (3B)1/3\u03c32/3 \u2212 . Proof. Let R = 1 and consider a distribution G given by G(x) = 1 for x < 1 and G(x) = 1x for x \u2208 [R,M ]. We then have that the optimal revenue is given by 1 and the mean of this distribution is\nB = \u222b M 0 G(x) = 1 + logM.\nOn the other hand the second moment of the distribution is given by\n2 \u222b M 0 xG(x) = 1 + 2 ( 1\u2212 1 M ) .\nTherefore the variance of the distribution is given by: \u03c32 = 2 (\n1\u2212 1 M \u2212 logM \u2212 log\n2M\n2 ) Using the fact that 1M = e \u2212 logM and using the Taylor expansion of that term we see that the variance is roughly log 3M 3 + o(log\n3M)as M \u2192 1. Since B \u2212 R\u2217 = logM it follows that for any > 0 there exists M close to 1 such that B \u2212R\u2217 \u2265 (3R\u2217)1/3\u03c32/3 \u2212 ."}, {"heading": "D Complexity Bounds", "text": "Theorem 3. The growth function of the class G(h, k) can be bounded as:\n\u03a0(G(h, k),m) \u2264 m 2k\nkk . Proof. Let S \u2032 = ( (x1, z1), . . . , (xm, zm) ) denote a sample.Let G = { ( sign(g(x1) \u2212 z1), . . . , sign(g(xm) \u2212\nzm) ) |g \u2208 G(h, k)}. We proceed to bound the cardinality of G. Notice that a partition t \u2208 Tk can divide the set of predictions h(x1), . . . , h(xm) into at most m k\u22121 different ways. Indeed, this is immediate as a k-partition of [0, 1] is defined by k \u2212 1 points t1, . . . , tk\u22121 and each ti has at most m distinct places to be placed. Now, fix a partition t \u2208 Tk and let Ij = [tj\u22121, tj ]. Let (possibly after relabeling) h(x1), . . . , h(xmj ) denote the points that fall in interval Ij . Notice that all points falling in Ij share the same reserve price rj thus the number of labelings that can be obtained in interval j are equal to\u2223\u2223{(sign(rj \u2212 z1), . . . , sign(rj \u2212 zmj ))|rj \u2208 R}\u2223\u2223 = mj Therefore for a fixed partition there are at most \u220fk j=1mj \u2264 ( m k )k labelings. Since there are at most mk\u22121 partitions then we must have:\n\u03a0(G(h, k)) \u2264 m 2k\u22121\nkk"}], "references": [{"title": "Regret minimization for reserve prices in secondprice auctions", "author": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2015}, {"title": "Algorithmic pricing via virtual valuations", "author": ["Shuchi Chawla", "Jason D. Hartline", "Robert D. Kleinberg"], "venue": "In Proceedings 8th ACM Conference on Electronic Commerce (EC-2007),", "citeRegEx": "Chawla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2007}, {"title": "The sample complexity of revenue maximization", "author": ["Richard Cole", "Tim Roughgarden"], "venue": "CoRR, abs/1502.00963,", "citeRegEx": "Cole and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Cole and Roughgarden.", "year": 2015}, {"title": "Bid landscape forecasting in online ad exchange marketplace", "author": ["Ying Cui", "Ruofei Zhang", "Wei Li", "Jianchang Mao"], "venue": "In Proceedings of SIGKDD,", "citeRegEx": "Cui et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2011}, {"title": "The sample complexity of auctions with side information", "author": ["Nikhil R. Devanur", "Zhiyi Huang", "Christos-Alexandros Psomas"], "venue": "In Proceedings of STOC,", "citeRegEx": "Devanur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Devanur et al\\.", "year": 2016}, {"title": "Revenue maximization with a single sample", "author": ["Peerapong Dhangwatnotai", "Tim Roughgarden", "Qiqi Yan"], "venue": "Games and Economic Behavior,", "citeRegEx": "Dhangwatnotai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dhangwatnotai et al\\.", "year": 2015}, {"title": "Competitive auctions and digital goods", "author": ["Andrew V. Goldberg", "Jason D. Hartline", "Andrew Wright"], "venue": "In Proceedings of the Twelfth Annual Symposium on Discrete Algorithms, January", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Simple versus optimal mechanisms", "author": ["Jason D. Hartline", "Tim Roughgarden"], "venue": "In Proceedings 10th ACM Conference on Electronic Commerce (EC-2009),", "citeRegEx": "Hartline and Roughgarden.,? \\Q2009\\E", "shortCiteRegEx": "Hartline and Roughgarden.", "year": 2009}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank Thomson Leighton"], "venue": "In Proceedings of FOCS,", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "A field guide to personalized reserve prices", "author": ["Renato Paes Leme", "Martin P\u00e1l", "Sergei Vassilvitskii"], "venue": "In Proceedings of the 25th International Conference on World Wide Web, WWW 2016,", "citeRegEx": "Leme et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leme et al\\.", "year": 2016}, {"title": "Learning theory and algorithms for revenue optimization in second-price auctions with reserve", "author": ["Mehryar Mohri", "Andres Mu\u00f1oz Medina"], "venue": "In Proceedings of ICML,", "citeRegEx": "Mohri and Medina.,? \\Q2014\\E", "shortCiteRegEx": "Mohri and Medina.", "year": 2014}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "ISBN 026201825X,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "On the pseudo-dimension of nearly optimal auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2015}, {"title": "Learning simple auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "In Proceedings ofCOLT,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2016\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2016}, {"title": "Optimal auction design", "author": ["R. Myerson"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Myerson.,? \\Q1981\\E", "shortCiteRegEx": "Myerson.", "year": 1981}, {"title": "Minimizing regret with multiple reserves", "author": ["Tim Roughgarden", "Joshua R. Wang"], "venue": "In Proceedings of the 2016 ACM Conference on Economics and Computation, EC \u201916,", "citeRegEx": "Roughgarden and Wang.,? \\Q2016\\E", "shortCiteRegEx": "Roughgarden and Wang.", "year": 2016}, {"title": "Objective variables for probabilistic revenue maximization in second-price auctions with reserve", "author": ["Maja R. Rudolph", "Joseph G. Ellis", "David M. Blei"], "venue": "In Proceedings of WWW", "citeRegEx": "Rudolph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rudolph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "The celebrated work by Myerson [1981] shows how to optimally set reserves in second price auctions, provided the value distribution of each bidder is known.", "startOffset": 23, "endOffset": 38}, {"referenceID": 1, "context": "1 Related Work Optimizing revenue in auctions has been a rich area of study, beginning with the seminal work of Myerson [1981] who introduced optimal auction design.", "startOffset": 112, "endOffset": 127}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions.", "startOffset": 18, "endOffset": 39}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions.", "startOffset": 18, "endOffset": 75}, {"referenceID": 0, "context": "Follow up work by Chawla et al. [2007] and Hartline and Roughgarden [2009], among others, refined his results to increasingly more complex settings, taking into account multiple items, diverse demand functions, and weaker assumptions on the shape of the value distributions. Most of the classical literature on revenue optimization focuses on the design of optimal auctions when the bidding distribution of buyers is known. More recent work has considered the computational and information theoretic challenges in learning optimal auctions from data. A long line of work [Cole and Roughgarden, 2015, Devanur et al., 2016, Dhangwatnotai et al., 2015, Morgenstern and Roughgarden, 2015, 2016] analyzes the sample complexity of designing optimal auctions. The main contribution of this direction is to show that under fairly general bidding scenarios, a near-optimal auction can be designed knowing only a polynomial number of samples from bidders\u2019 valuations. Other authors, [Leme et al., 2016, Roughgarden and Wang, 2016] have focused on the computational complexity of finding optimal reserve prices from samples, showing that even for simple mechanisms the problem is often NP-hard to solve directly. Another well studied approach to data-driven revenue optimization is that of online learning. Here, auctions occur one at a time, and the learning algorithm must compute prices as a function of the history of the algorithm. These algorithms generally make no distributional assumptions and measure their performance in terms of regret: the difference between the algorithm\u2019s performance and the performance of the best fixed reserve price in hindsight. Kleinberg and Leighton [2003] developed an online revenue optimization algorithm for posted-price auctions that achieves low regret.", "startOffset": 18, "endOffset": 1685}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers.", "startOffset": 58, "endOffset": 85}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve.", "startOffset": 58, "endOffset": 753}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output.", "startOffset": 58, "endOffset": 1099}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information.", "startOffset": 58, "endOffset": 1258}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by \u03c3 \u2208 [0, 1], thus limiting the applicability of these results\u2014online advertising auctions are normally parameterized by a large set of features. Finally, Cui et al. [2011] proposes partitioning data into clusters and solving the i.", "startOffset": 58, "endOffset": 1597}, {"referenceID": 0, "context": "Their work was later extended to second-price auctions by Cesa-Bianchi et al. [2015]. A natural approach in both of these settings is to attempt to predict an optimal reserve price, equivalently the highest bid submitted by any of the buyers. While the problem of learning this reserve price is well understood for the simplistic model of buyers with i.i.d. valuations [Cesa-Bianchi et al., 2015, Devanur et al., 2016, Kleinberg and Leighton, 2003], the problem becomes much more challenging in practice, when the valuations of a buyer also depend on features associated with the ad opportunity (for instance user demographics, and publisher information). This problem is not nearly as well understood as its i.i.d. counterpart. Mohri and Medina [2014] provide learning guarantees and an algorithm based on DC programming to optimize revenue in second-price auctions with reserve. The proposed algorithm, however, does not easily scale to large auction datasets as each iteration involves solving a convex optimization problem. A smoother version of this algorithm is given by Rudolph et al. [2016]. However, being a highly non-convex problem, neither algorithm provides a guarantee on the revenue attainable by the algorithm\u2019s output. Devanur et al. [2016] give sample complexity bounds on the design of optimal auctions with side information. However, the authors consider only cases where this side information is given by \u03c3 \u2208 [0, 1], thus limiting the applicability of these results\u2014online advertising auctions are normally parameterized by a large set of features. Finally, Cui et al. [2011] proposes partitioning data into clusters and solving the i.i.d. problem for each cluster. The crucial choice of a partition, however, is heuristic and thus provides no guarantees on the achievable revenue. Our results. We show that given a predictor of the bid with squared loss of \u03b7, we can construct a reserve function r that extracts all but g(\u03b7) revenue, for a simple increasing function g. (See Theorem 2 for the exact statement.) To the best of our knowledge, this is the first result that ties the revenue one can achieve directly to the quality of a standard prediction task. Our algorithm for computing r is scalable, practical, and efficient. Along the way we show what kinds of distributions are amenable to revenue optimization via reserve prices. We prove that when bids are drawn i.i.d. from a distribution F , the ratio between the mean bid and the revenue extracted with the optimum monopoly reserve scales as O(logVar(F ))\u2014Theorem 5. This result refines the log h bound derived by Goldberg et al. [2001], and formalizes the intuition that reserve prices are more successful for low variance distributions.", "startOffset": 58, "endOffset": 2618}, {"referenceID": 10, "context": "The following theorem is an adaptation of Theorem 1 of Mohri and Medina [2014] to our particular setup.", "startOffset": 55, "endOffset": 79}, {"referenceID": 12, "context": "The proof is similar to that in Morgenstern and Roughgarden [2015]; we include it in Appendix D for completness.", "startOffset": 32, "endOffset": 67}, {"referenceID": 6, "context": "1 Approximating Maximum Revenue In their seminal work Goldberg et al. [2001] showed that when faced with a bidder drawing values distribution F on [1,M ] with mean B, an auctioneer setting the optimum monopoly reserve would recover at least \u03a9(B/ logM) revenue.", "startOffset": 54, "endOffset": 77}, {"referenceID": 10, "context": "Following the suggestions in [Mohri and Medina, 2014] we chose \u03b3 \u2208 {0.", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": "It is not surprising to see that in the linear scenario, the DC algorithm of [Mohri and Medina, 2014] and the offset algorithm outperform RIC-h under low noise conditions.", "startOffset": 77, "endOffset": 101}, {"referenceID": 10, "context": "The DC algorithm introduced by Mohri and Medina [2014], which represents the state of the art in learning a revenue optimal reserve price and optimizes the empirical \u03b3-Lipschitz approximation to the revenue function.", "startOffset": 31, "endOffset": 55}], "year": 2017, "abstractText": "In the context of advertising auctions, finding good reserve prices is a notoriously challenging learning problem. This is due to the heterogeneity of ad opportunity types and the non-convexity of the objective function. In this work, we show how to reduce reserve price optimization to the standard setting of prediction under squared loss, a well understood problem in the learning community. We further bound the gap between the expected bid and revenue in terms of the average loss of the predictor. This is the first result that formally relates the revenue gained to the quality of a standard machine learned model.", "creator": "LaTeX with hyperref package"}}}