{"id": "1706.05048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "A new look at clustering through the lens of deep convolutional neural networks", "abstract": "Classification and clustering have been studied separately in machine learning and computer vision. Inspired by the recent success of deep learning models in solving various vision problems (e.g., visual intelligence or visual intelligence), these models offer an overview of the best available methods of learning and classification and other fields of learning, including information analysis, statistical analysis, and more. We find that we are able to identify, by way of, a large degree of network connectivity during a network connection. Moreover, a very strong network connectivity has been achieved, as well as the degree of network connectivity in the network. We are also able to show that, in the case of data-mining, most information is generated from data-mining processes. This connectivity allows us to infer which tasks are being evaluated and which tasks are to be assessed, and to generate the most significant results. In contrast, the network connectivity in our model has been observed in the context of network clustering for long-term data-mining tasks. We have also found that, in contrast, the network connectivity in our model has been shown to be more complete than previously understood. Although these networks are highly parallel to the network connectivity, their connectivity in the network has also been suggested to be more stable.", "histories": [["v1", "Thu, 15 Jun 2017 19:10:50 GMT  (2081kb,D)", "http://arxiv.org/abs/1706.05048v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["ali borji", "aysegul dundar"], "accepted": false, "id": "1706.05048"}, "pdf": {"name": "1706.05048.pdf", "metadata": {"source": "CRF", "title": "A new look at clustering through the lens of deep convolutional neural networks", "authors": ["Ali Borji", "Aysegul Dundar"], "emails": ["aborji@crcv.ucf.edu", "adundar@purdue.edu"], "sections": [{"heading": "1 Introduction", "text": "Clustering, a.k.a unsupervised classification or nonparametric density estimation, is central to many data-driven domains and has been studied heavily in the past. The task in clustering is to group a given collection of unlabeled patterns into meaningful clusters such that objects within a cluster are more similar to each other than they are to objects in other clusters. Clustering provides a summary representation of data at a coarse level and is used widely in many disciplines (e.g., computer version, bioinformatics, text processing) for exploratory data analysis (a.k.a pattern mining) as well as representation learning (e.g., bag of words). Despite the introduction of thousands of clustering algorithms in the past [1], some challenges still remain. For instance, existing algorithms fall short in dealing with different cluster shapes, high dimensions, automatically determining the number of clusters or other parameters, large amounts of data, choosing the appropriate similarity measure, incorporating domain knowledge, and cluster evaluation. Further, no clustering algorithm can consistently win over other algorithms, handle all test cases, and perform at the level of humans.\nDeep neural networks have become a dominant approach to solve various tasks across many fields. They have been proven successful in several domains including computer vision [34], natural language\n\u2217Equal contribution\nar X\niv :1\n70 6.\n05 04\n8v 1\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nprocessing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].\nDeep Convolutional Neural Networks (CNNs) [23, 35] have been particularly successful over vision problems. One reason is that nearby pixels in natural scenes are highly correlated [30]. Further natural objects are compositional. These facts allow applying the same filters across spatial locations (and hence share weights), and build complex filters from simpler ones to detect high-level patterns (e.g., object parts, objects). We advocate that these properties are highly appealing when dealing with clustering problems. For instance, the classic two half moons example can be solved by applying a filter that is selective to each half moon. Or, when two clusters with different shapes overlap, the problem can be solved by having filters responding to each shape. Solving these cases is very challenging by just looking at local regions around points and being blind to the high-level patterns. Incorporating domain knowledge, while working in some cases, does not give a general solution for solving all clustering problems. The human visual system easily solves these 2D problems because it is a general system with a rich set of learned or evolved filters. We believe that deep CNNs, although imperfect models of the human vision as they lack feedback and lateral connections carry a huge promise for solving clustering tasks. Further, as we will argue, they offer a unified solution to both classification and clustering tasks.\nThe current demarcation between classification and clustering becomes murky when we notice that researchers often refer to human judgments in evaluating the outcomes of clustering algorithms. Indeed, humans learn quite a lot about the visual world during their life time. Moreover, the structure of the visual system has been fine-tuned through the evolution. Thus, certainly, there is a learning component involved which has been often neglected in formulating clustering algorithms. While this is sensible from an application point of view (e.g., pattern mining), not only it limits the pursuit for stronger algorithms but also narrows our understanding of human vision.\nLearning techniques have been utilized for clustering in the past (e.g., [3, 5, 47]), for example for tuning parameters (e.g., [3]). Deep networks have also been exploited for clustering (e.g., [29, 27, 60]). However, to our knowledge, while CNNs have been already adopted for image segmentation, so far they have not been exploited for generic clustering. Our goal is to investigate such possibility. To this end, instead of borrowing from clustering to do image segmentation, we follow the opposite direction and propose a deep learning based approach to clustering.\nOur method builds on the fully convolutional network literature, in particular, recent work on edge detection and semantic segmentation which utilize multi-scale local and non-local cues [50]. Thanks to a high volume of labeled data, high capacity of deep networks, powerful optimization algorithms, and high computational power, deep models win on these tasks. We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7]. Our study enriches our understanding of the concept of clustering and its relations with classification."}, {"heading": "2 Related Work", "text": "Data clustering algorithms. Clustering approaches can be broadly classified into hierarchical and partitional types [1]. Hierarchical approaches either start with a single big cluster and recursively split it into smaller ones (top-down) or begin with several small clusters, and then gradually merge them (bottom-up or agglomerative). A classic example of partitional methods is the k-means [40] algorithm which minimizes the sum of squared errors between data points and their nearest cluster centers. Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37]. Density-based algorithms, a class of partitional methods, classify points by identifying regions heavily populated with data. Some examples include DBSCAN [21], GDBSCAN [51], OPTICS [2], and CFSFDP [49]. Neural networks have also been utilized to build clustering algorithms (e.g., ARTMAP [9] and SOM [33]).\nBoundary assignment. This problem has also been studied under the names of boundary completion, border ownership, occlusion handling, figure-ground interpretation or segmentation in cognitive sciences, computational neuroscience (e.g., [65, 63]) and computer vision. It has long been recognized as an important human ability for scene understanding and perception. Here, the main goal is to tell which region a border belongs to or to identify an object in the image and separate it from the\nbackground. The Gestalt psychologists were among the first to study these processes (e.g., [32]). They pointed out that visual perception tends to assign dividing edges to objects, not to the background (e.g., Kanizsa triangle). Research in computer vision on detecting occlusion relations in natural images was stimulated by the construction of the BSDS border ownership dataset [48]. Deep learning methods have recently been exploited for solving the boundary assignment problem (e.g., [59]).\nSemantic segmentation. The goal in these techniques is to assign structured semantic labels \u2014 typically, object class labels \u2014 to individual pixels in images. This problem has been studied extensively over decades, yet remains challenging. This is mainly because objects can appear widely differently in images due to significant variations is the pose, scale, lighting, occlusion, background clutter, etc. However, in spite of such challenges, the techniques based on deep learning demonstrate impressive performance in the standard benchmark datasets such as PASCAL VOC [22] or MS COCO [39]. Most deep learning based approaches pose semantic segmentation as a pixelwise classification problem (e.g., [41, 11, 46]). Although these approaches have achieved good performance compared to previous methods, they demand a large number of segmentation groundtruths. Clustering scenarios we are studying in this paper are quite different than image segmentation and instance segmentation challenges. In clustering, the input is sparse and occlusion is see through, therefore a combination of detection and segmentation can not give the instance clusters.\nDeep learning for clustering. Currently, there is only a little work on exploiting deep neural network for clustering. Hsu and Kira [29] utilized pairwise constraints to train a neural network to perform clustering. In [10], deep belief networks [28] were used for non-parametric clustering. Wang et al. [60] proposed a task-specific deep architecture for clustering based on sparse coding. Some works have focused on learning representations (or embeddings) for clustering. Here the idea is to first transform the original data to a lower dimensional space with a nonlinear mapping f\u03b8 : X \u2192 Z, where \u03b8 are parameters and Z is the latent feature space. Adopting this, Xie et al. [61] proposed an approach for image clustering. Song et al. [56] defined a new objective function by considering the reconstruction error from an auto-encoder network and restricting the distance in the learned space between data and their corresponding cluster centers. Hershey et al. [27] proposed a deep clustering approach to solving acoustic source separation. Rather than directly estimating signals or masking functions, they trained a deep network to produce spectrogram embeddings. [10] explored the possibility of employing deep learning in graph clustering. They first learned a nonlinear embedding of the original graph by an autoencoder, followed by k-means algorithm on the embedding to obtain the final clustering result."}, {"heading": "3 Model Description", "text": "We are motivated by three observations. First, CNNs have been very successful over a variety of vision problems such as semantic segmentation, edge detection, and recognition. Second, CNNs learn representations through several stages of non-linear processing, akin to how the cortex adapts to represent the visual world [62]. Similar to other biological models of visual cortex (e.g., HMAX [53]), these models capture aspects of the organization of the visual ventral stream. Third, clustering methods are often evaluated against human perception motivating biologically inspired solutions.\nOur strategy parallels recent work in using CNNs for semantic segmentation. A crucial difference, however, is that cluster identities (class labels) are not important here. For instance, if a triangle and a circle exist in the image, the shape labels can be anything as long as clusters are correctly separated. Unlike previous work, instead of learning embeddings, we use back propagation via stochastic\ngradient descent to optimize a clustering objective to learn the mapping, which is parameterized by a deep neural network. In this way, there is no need to specify parameters such as the number of clusters, distance measure, scale, cluster centers, etc."}, {"heading": "3.1 The network architecture", "text": "Figure 1 shows the proposed deep network architecture which is based on the U-Net [50]; an encoderdecoder with skip connections. The input is a binary image with a single channel, also shown in Figure 2. Input is fed to five stacks of [convolution, convolutions, pooling] modules. These are followed by five stacks of decoder modules [convolution, convolution, upsampling]. Skip connections from mirrored layers in the encoder are fed to decoder stacks. Such skip connections recover the spatial image information which might have been lost through successive convolution and pooling operations in the encoder. Finally, three 1\u00d7 1 filters are applied to collapse the convolutional maps to three channels that can cluster three objects (one channel per cluster). Each convolution layer in the decoder module has 16 filters and is followed by a ReLU layer except the last convolution layer which uses Sigmoid activation. Then, the pointwise multiplication of the output and the input is calculated to generate the final cluster map (denoted as the output map in Figure 2). This multiplication removes the background pixels from the output, giving us only the prediction for points that are of interest."}, {"heading": "3.2 Training procedure and ground truth", "text": "To implement our model, we use the Keras [12] platform. We use 128\u00d7 128 binary images as inputs (see Figure 2). The corresponding ground truth map for each input is generated as follows: the points belonging to the topmost cluster are assigned label 0, descending top-down in the image, points belonging to the next cluster are assigned label 1, and so on. This process is repeated until all clusters are labeled. This way the labels are independent of the object shapes. This makes our training scheme different from classification and segmentation methods where each object is always assigned the exact same label. Notice that here we are mainly interested in separating the objects from each other rather than correctly classifying them. Despite this, as we will show later, the network is able to cluster objects with the same shape successfully. We use the mean squared error loss and train the network with the Adam optimizer [31]. Batch size is set to 16 and learning rate to 0.001."}, {"heading": "4 Experimental Evaluation", "text": ""}, {"heading": "4.1 Synthetic data", "text": "Here, we explain the process of generating the synthetic images to train and test our model.\nGeometric shape stimuli. Objects are parametrized using several variables including S = {Circle, Ring, Square, SquareRing, Bar}, O = {1, . . . ,m}, D = [200 300], and SC = [10 30]. They, in order, denote the set of possible shapes, the set of the possible number of objects to place in the image, the interval of point densities for an object, and the interval of possible object scales.\nTo generate an image, first a number k \u2208 O, indicating the number of objects is randomly drawn. The following is then repeated k times. Random density and scales are chosen for clusters. The cluster is randomly rotated \u03b8 degrees (\u03b8 \u2208 [0, 2\u03c0]) and shifted such that it remains within the image boundary. The output is then saved as a 128\u00d7 128 pixels binary image (1 for shape pixels and 0 for background) to be fed to CNN. Ground truth of clusters is saved for training and evaluation.\nGaussian mixture distribution. To randomly generate a Gaussian Mixture Density distribution, a 2D mean vector M = [x y] is randomly sampled (x, y \u2208 [20 100]). A random matrix A = [a b; c d] is generated with elements in [0 1]. The matrix A\u2032A which is symmetric and positive semidefinite is chosen as the covariance matrix for a Gaussian. The same is repeated to assign random mean and covariances matrices for m Gaussians (m \u2208 {2, 3}) in the image. After having the Gaussian distributions, we randomly sample D \u2208 [100 400] points from each Gaussian and form the input and output images as in the shapes case.\nSome sample generated images are shown in Figure 2. While it seems easy for humans to find the clusters in these images, as will be shown in the next section, our stimuli poses a serious challenge to current best clustering algorithms. In particular, current clustering algorithms fail when clusters occlude each other. The reason is that they lack a global understanding of the patterns."}, {"heading": "4.2 Evaluation metric", "text": "Since our purpose is to do clustering, and not classification, prediction score is not sensible to evaluate the performance. Instead, we define an intuitive straightforward evaluation score as follows. Given n points in the image, a binary matrix of size n2 is formed where each element indicates whether two points belong to the same cluster or not. A similar matrix is created for the prediction of each model. Notice that the order of points is preserved when constructing these matrices. Then, the Hamming distance between the ground truth matrix and the prediction matrix is calculated which determines the fraction of cases they do not agree with each other (i.e., error rate)."}, {"heading": "4.3 Benchmark algorithms", "text": "We used the following clustering methods as the benchmark algorithms: k-Means (KM) [40], minimizes the sum of squared errors between data points and their nearest cluster centers. It is a simple and widely used method.\nFuzzy C-Means (FCM) [20, 4] assigns soft labels to data points meaning that each data point can belong to more than one cluster with different degrees of membership.\nSpectral Clustering. We employ two spectral clustering algorithms. These algorithms perform a spectral analysis of the matrix of point-to-point similarities instead of estimating an explicit model of data distribution (as in k-Means). The first one is the Normalized Cut (NC) proposed by Shi and Malik [54]. Here, first, a graph is built from the image (pixels as nodes, edges as similarities between pixels). Then, algorithm cuts the graph into two subgraphs. The second algorithm, known as Ng-Jordan-Weiss (NJW) [45] is a simple and significant example of spectral clustering which analysis the eigenvectors of the Laplacian of the similarity matrix. Spectral clustering has been shown to work well on data that is connected but not necessarily compact or clustered within convex boundaries.\nMean shift (MS) [15] iteratively seeks the modes of a density function from discrete samples of that function. Mean Shift performs as follows. First, it fixes a window around each data point. Then, computes the mean of data within each window. Finally, shifts the window to the mean and repeats till convergence.\nClustering by fast search and find of density peaks (CFSFDP) [49] method seeks the modes or peaks of a distribution. It works as follows: 1) For each point, its local density is computed (i.e., number of points in its neighborhood), 2) For each point, its distance to all the points with higher\ndensity is computed and the minimum value is sought, 3) A plot is made showing the minimum distance for a point as a function of the density. The \u201coutliers\u201d in this plot are the cluster centers, 5) Finally, each point is assigned to the same cluster of its nearest neighbor of higher density. The input to this algorithms is the pairwise distance matrix. To find outliers (i.e., cluster centers), we find the point [maxx maxy] in this plot and then find q closest points to this point with q being equal to number of ground truth clusters. We use the Gaussian cut off kernel in this model.\nAll algorithms are provided with the actual number of clusters that exist in the input image, except the CNN and MS algorithm which are supposed to automatically determine the number of clusters in the process of clustering. Euclidean distance is used in both k-Means and FCM as the distance measure. All algorithms are optimized for their best performance in each experiment (e.g., by varying the type of affinity, scale, and normalization). The last three algorithms have been very successful for solving the perceptual grouping problem in computer vision."}, {"heading": "4.4 Experiments", "text": "We run a total of eight experiments. The first five experiments aim to evaluate the performance of the proposed method with respect to the benchmark algorithms. The last three experiments test the robustness of the proposed method.\nIn our first experiment, we generate images with 2 objects randomly chosen from 5 shapes. Shape parameters are randomly drawn from all possible choices. We use 1800 training images and 200 testing images. The goal here is to study the effect of cluster heterogeneity on the results. In the second experiment, we generate 200 test images with 2 objects from the same shape type (one of the fives shapes). We use the network trained in our first experiment. This experiment concentrates on the proposed method\u2019s ability to cluster shapes that look very similar to each other. This case is important since CNNs are known to be very good at generalizing.\nIn the third experiment, we generate images with 3 objects randomly chosen from 3 shapes (Ring, SquareRing, and Bar). Shape parameters are randomly drawn from all possible choices. We use 2700 training images. Experiment 4 considers a more challenging case of having 3 objects sampled from 5 shapes. Here, the training set contains 7000 images. Experiment 5 uses randomly generated Gaussian clusters. Number of the clusters in each image varies between 2 to 3.\nExperiment 6 tests the behavior of the network trained with 3 objects when applied to cluster images with only 2 shapes. Experiment 7 investigates the generalization power using different amounts of training data for both shapes and Gaussian distributions while experiment 8 tests the robustness of our model to noise. In experiment 7, a model is first trained over a variable number of n images (n \u2208 {1, 10, 100, 1000, 3000, 7000}) and is then tested over 200 test images. To measure the lower bound, we also test a randomly initialized network that has not seen any training sample. In experiment 8, we randomly switch some points in the background to 1 (i.e., making them shape points). Figure 5a shows some examples. The pre-trained models from the first and fifth experiments, trained over noiseless data, are applied to the noisy test images. We do not set a threshold in the output so the noises also receive a cluster id. In this experiment, we are interested in analyzing\nthe effect of injecting additional noise on the real clusters. To measure the performance over noisy images, we discard the noise pixels in the evaluation."}, {"heading": "4.5 Results and discussion", "text": "Table 1 shows the results of the first five experiments. CNN wins over all models in all experiments with a large margin (e.g., about 7% improvement over the second best in experiment 1). This large margin hints that maybe even the best optimization of the compared algorithms will not be able to compete with the proposed CNN. All other models perform about the same.\nTable 1: Quantitative comparison of different clustering algorithms. The best one is highlighted in bold. First row in each experiments shows the mean and the second row shows the standard deviation.\nModel CNN kM FCM NJW SC MS CFS\nExp 1 0.916 0.850 0.858 0.816 0.805 0.748 0.771 0.131 0.173 0.165 0.220 0.213 0.181 0.207 Exp 2 0.895 0.849 0.859 0.813 0.811 0.749 0.756 0.151 0.176 0.168 0.224 0.217 0.192 0.212 Exp 3 0.895 0.770 0.771 0.721 0.760 0.714 0.728 0.098 0.107 0.103 0.138 0.139 0.116 0.134 Exp 4 0.871 0.798 0.804 0.703 0.739 0.714 0.729 0.117 0.120 0.117 0.190 0.153 0.167 0.130\nExp 5 0.920 0.868 0.870 0.844 0.853 0.838 0.878 0.112 0.138 0.135 0.158 0.145 0.176 0.152\nResults of the experiment 1, the first row of Table 1, show that CNN is able to successfully cluster the data over the easy cases when two different objects are in the image. Yet, these images challenge the other algorithms. Over images with the same objects (experiment 2), results drop compared to experiment 1 but CNN still outperforms other models. Our model sometimes fails when objects touch each other since points in the shared area can belong to any of the objects.\nFigure 3(A) illustrates the output of models over 4 examples, each with 3 objects drawn from 3 shapes (experiment 3). In the first example, the bar slightly touches the rotated square. While CNN is capable of handling this case, other models bleed around the point of touch. In example two, the square is occluded with the bar. Again, while CNN is able to correctly assign the cluster labels to occluded objects, other models are drastically hindered. Our model scores 81.2% while other models perform no better than 70%. Similar patterns can be observed over the other two examples. These findings also hold over images with varying numbers of objects, and parameters (e.g., 5 objects, 5 shapes) as well as Gaussian clusters (see Figure 3(B)).\nResults are lower in experiment 4, images with 3 objects from 5 shapes, compared to the first two experiments. We find that performance drops as we add more objects or increase the shape variety.\nIn experiment 5, we use images that can have 2 or 3 different Gaussian clusters both in training and testing data. We do not give the number of clusters to CNN and MS algorithms during testing. As it can be seen, similar to shapes, CNN is superior to other approaches in clustering Gaussian distributions.\nIn experiment 6, we further investigate the behavior of CNN to different number of clusters in images. Applying a network trained over 3 objects to cluster 2 objects (over shapes), shows a drop in performance compared to the situation where the network was trained on 2 objects (from 91.6% to\n88%). The accuracy is still high and better than other clustering algorithms. This result suggests that CNN model is not fitted to a certain task and learns about what constitutes a cluster.\nResults of the 7th experiment are shown in Figure 4. The left and right panels in this figure show models\u2019 predictions for shapes and Gaussian clusters respectively. Expectedly, it shows that the model fails without training (i.e., model with its weights randomly initialized). Here, the model achieves 34% accuracy. To our surprise, training with only a single sample leads to a better than chance performance of 62.7% (chance is 50%) for shapes. Training with only 10 samples gives a descent accuracy (68.3%). Increasing the training size to 100 leads to a performance comparable to some algorithms (NJW). With 1000 examples, the model is as good as k-Means and FCM and better than other methods. This result is illustrated in Figure 5b.B. Results on shape and Gaussian clusters follow a similar trend. In sum, results of experiment 7 suggest that our model is quite efficient in learning from a few samples and generalizes well to unseen cases.\nFinally, Figure 5a illustrates the results of the 8th experiment which is the analysis of noise. Two inputs, corrupted with 3 levels of noise, are shown in the figure for both shape and Gaussian clusters. Figure 5b.A shows a gradual decrease in performance by increasing the amount of noise. Net-\nwork is affected by noise more when the clusters are Gaussians since it is a harder task as can be seen from Figure 5a. Even with highly degraded images especially for shapes, the model does reasonably well. Therefore, our model, unlike other methods, is robust to noise."}, {"heading": "5 Conclusion", "text": "We argued that deep neural networks, especially CNNs, hold a great promise for data clustering. We are motivated by the fact that human vision (and learning) is a general system capable of solving both classification and clustering tasks thus blurring the current dichotomy in treating these problems. Our results show that CNNs can successfully handle complex and occluded clusters much better than other algorithms. This means that a learning mechanism, unsupervised or with minimal supervision, seems inevitable in capturing complex cluster shapes.\nWhile our formulation is supervised, feeding the labels to the network is not always consistent. This is where our work differs from semantic segmentation and instance level segmentation. We exploited the mean squared loss to train the network. It might be possible to define other loss functions to teach the network more efficiency using less number of training data or even with weaker labels. One possibility is the pairwise accuracy that we used here for evaluation. Instead of correctly classifying labels, the emphasis can be placed on correctly predicting whether two points belong to the same cluster, regardless of cluster identities (i.e., class labels may vary).\nNotice that while here we focused on synthetic stimuli, variations of the proposed CNN architecture, have been successfully applied to natural image segmentation. Thus, CNNs offer a unified solution\nthat can be applied to different data modalities and even to higher dimensional data. Further work is needed to extend this line of work to higher dimensions, and more versatile types of cluster shapes (e.g., free form curves, Gestalt examples, density-based clusters). In this regard, adopting CNNs trained on natural images containing a rich set of intermediate- and high-level patterns can give invaluable insights.\nIn sum, our results provide encouragement for researchers seeking unified theoretical explanations for supervised and unsupervised categorization but raise a range of challenging theoretical questions. We emphasized more on the capacity of hierarchical frameworks and compositionality to capture complex structures rather than the learning algorithms. Indeed, further discussion and research are needed for training CNNs in unsupervised or weakly supervised manners. Some new works on learning representations from videos (e.g., predicting future frames [57]) are particularly interesting. Further, research on unsupervised training of spiking neural networks [42] and CNNs (e.g., using Hebb rule [58]), along with computational modeling (e.g., [52, 36]), and experimental studies on mechanisms of human vision (e.g., [62, 16]), will hopefully converge to computational vision algorithms that are capable of solving a variety of tasks, including classification and clustering."}], "references": [{"title": "Data clustering: algorithms and applications", "author": ["Charu C Aggarwal", "Chandan K Reddy"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Optics: ordering points to identify the clustering structure", "author": ["Mihael Ankerst", "Markus M Breunig", "Hans-Peter Kriegel", "J\u00f6rg Sander"], "venue": "In ACM Sigmod record,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Learning spectral clustering", "author": ["Francis R Bach", "Michael I Jordan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Pattern recognition with fuzzy objective function algorithms", "author": ["James C Bezdek"], "venue": "Springer Science & Business Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning to segment", "author": ["Eran Borenstein", "Shimon Ullman"], "venue": "Vision-ECCV", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "State-of-the-art in visual attention modeling", "author": ["Ali Borji", "Laurent Itti"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Human vs. computer in scene and object recognition", "author": ["Ali Borji", "Laurent Itti"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study", "author": ["Ali Borji", "Dicky N Sihite", "Laurent Itti"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Artmap: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network", "author": ["Gail A Carpenter", "Stephen Grossberg", "John H Reynolds"], "venue": "Neural networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Deep learning with nonparametric clustering", "author": ["Gang Chen"], "venue": "arXiv preprint arXiv:1501.03084,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L Yuille"], "venue": "In ICLR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Spectral graph theory, volume 92", "author": ["Fan RK Chung"], "venue": "American Mathematical Soc.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["Dorin Comaniciu", "Peter Meer"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Neural networks and neuroscience-inspired computer vision", "author": ["David Daniel Cox", "Thomas Dean"], "venue": "Current Biology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Series B (methodological),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1977}, {"title": "Untangling invariant object recognition", "author": ["James J DiCarlo", "David D Cox"], "venue": "Trends in cognitive sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "A fuzzy relative of the isodata process and its use in detecting compact well-separated", "author": ["Joseph C Dunn"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1973}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["Martin Ester", "Hans-Peter Kriegel", "J\u00f6rg Sander", "Xiaowei Xu"], "venue": "In Kdd,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham", "Luc Van Gool", "Christopher KI Williams", "John Winn", "Andrew Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition", "author": ["Kunihiko Fukushima", "Sei Miyake"], "venue": "In Competition and cooperation in neural nets,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1982}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["John R Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Neural network-based clustering using pairwise constraints", "author": ["Yen-Chang Hsu", "Zsolt Kira"], "venue": "arXiv preprint arXiv:1511.06321,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision., volume 39", "author": ["Aapo Hyv\u00e4rinen", "Jarmo Hurri", "Patrick O Hoyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In NIPS,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1998}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2001}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Least squares quantization in pcm", "author": ["Stuart Lloyd"], "venue": "IEEE transactions on information theory,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1982}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In CVPR,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Unsupervised learning of visual features through spike timing dependent plasticity", "author": ["Timoth\u00e9e Masquelier", "Simon J Thorpe"], "venue": "PLoS Comput Biol,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In NIPS,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2002}, {"title": "Learning to segment object candidates", "author": ["Pedro O Pinheiro", "Ronan Collobert", "Piotr Dollar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Learning to refine object segments", "author": ["Pedro O Pinheiro", "Tsung-Yi Lin", "Ronan Collobert", "Piotr Doll\u00e1r"], "venue": "In ECCV,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Figure/ground assignment in natural images", "author": ["Xiaofeng Ren", "Charless C Fowlkes", "Jitendra Malik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "Clustering by fast search and find of density", "author": ["Alex Rodriguez", "Alessandro Laio"], "venue": "peaks. Science,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Density-based clustering in spatial databases: The algorithm gdbscan and its applications", "author": ["J\u00f6rg Sander", "Martin Ester", "Hans-Peter Kriegel", "Xiaowei Xu"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1998}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2007}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2016}, {"title": "Auto-encoder based data clustering", "author": ["Chunfeng Song", "Feng Liu", "Yongzhen Huang", "Liang Wang", "Tieniu Tan"], "venue": "In Iberoamerican Congress on Pattern Recognition,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Bottom-up deep learning using the hebbian principle, 2016", "author": ["Aseem Wadhwa", "Upamanyu Madhow"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2016}, {"title": "Doc: Deep occlusion recovering from a single image", "author": ["Peng Wang", "Alan Yuille"], "venue": "arXiv preprint arXiv:1511.06457,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}, {"title": "Learning a task-specific deep architecture for clustering", "author": ["Zhangyang Wang", "Shiyu Chang", "Jiayu Zhou", "Meng Wang", "Thomas S Huang"], "venue": "In Proceedings of the 2016 SIAM International Conference on Data Mining,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2016}, {"title": "Unsupervised deep embedding for clustering analysis", "author": ["Junyuan Xie", "Ross Girshick", "Ali Farhadi"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex", "author": ["Daniel LK Yamins", "Ha Hong", "Charles F Cadieu", "Ethan A Solomon", "Darren Seibert", "James J DiCarlo"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Border ownership from intracortical interactions in visual area", "author": ["Li Zhaoping"], "venue": "v2. Neuron,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2005}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr"], "venue": "In ICCV,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2015}, {"title": "Coding of border ownership in monkey visual cortex", "author": ["Hong Zhou", "Howard S Friedman", "R\u00fcdiger Von Der Heydt"], "venue": "Journal of Neuroscience,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Despite the introduction of thousands of clustering algorithms in the past [1], some challenges still remain.", "startOffset": 75, "endOffset": 78}, {"referenceID": 29, "context": "They have been proven successful in several domains including computer vision [34], natural language \u2217Equal contribution ar X iv :1 70 6.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 95, "endOffset": 103}, {"referenceID": 24, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 95, "endOffset": 103}, {"referenceID": 35, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 149, "endOffset": 157}, {"referenceID": 57, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 149, "endOffset": 157}, {"referenceID": 5, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 178, "endOffset": 184}, {"referenceID": 7, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 178, "endOffset": 184}, {"referenceID": 22, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 203, "endOffset": 207}, {"referenceID": 32, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 227, "endOffset": 231}, {"referenceID": 23, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 252, "endOffset": 256}, {"referenceID": 37, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 278, "endOffset": 282}, {"referenceID": 49, "context": "processing [14], and speech recognition [17] for tasks such as scene and object classification [34, 26], pixel-level labeling for image segmentation [41, 64], modeling attention [6, 8], image generation [24], robot arm control [38], speech recognition [25], playing Atari games [43] and beating the Go champion [55].", "startOffset": 311, "endOffset": 315}, {"referenceID": 21, "context": "Deep Convolutional Neural Networks (CNNs) [23, 35] have been particularly successful over vision problems.", "startOffset": 42, "endOffset": 50}, {"referenceID": 30, "context": "Deep Convolutional Neural Networks (CNNs) [23, 35] have been particularly successful over vision problems.", "startOffset": 42, "endOffset": 50}, {"referenceID": 27, "context": "One reason is that nearby pixels in natural scenes are highly correlated [30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": ", [3, 5, 47]), for example for tuning parameters (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": ", [3, 5, 47]), for example for tuning parameters (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 41, "context": ", [3, 5, 47]), for example for tuning parameters (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 26, "context": ", [29, 27, 60]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 25, "context": ", [29, 27, 60]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 53, "context": ", [29, 27, 60]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 44, "context": "Our method builds on the fully convolutional network literature, in particular, recent work on edge detection and semantic segmentation which utilize multi-scale local and non-local cues [50].", "startOffset": 187, "endOffset": 191}, {"referenceID": 46, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 55, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 47, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 17, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 30, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 29, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 24, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 6, "context": "We are also strongly inspired by the works showing the high resemblance between human vision mechanisms and CNNs from behavioral, electrophysiological, and computational aspects [52, 62, 53, 19, 35, 34, 26, 7].", "startOffset": 178, "endOffset": 209}, {"referenceID": 0, "context": "Clustering approaches can be broadly classified into hierarchical and partitional types [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 34, "context": "A classic example of partitional methods is the k-means [40] algorithm which minimizes the sum of squared errors between data points and their nearest cluster centers.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37].", "startOffset": 60, "endOffset": 68}, {"referenceID": 38, "context": "Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37].", "startOffset": 60, "endOffset": 68}, {"referenceID": 16, "context": "Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37].", "startOffset": 116, "endOffset": 120}, {"referenceID": 31, "context": "Some frequently used algorithms include spectral clustering [13, 44], expectation maximization (EM) [18], Meanshift [15], and non-negative matrix factorization (NMF) [37].", "startOffset": 166, "endOffset": 170}, {"referenceID": 19, "context": "Some examples include DBSCAN [21], GDBSCAN [51], OPTICS [2], and CFSFDP [49].", "startOffset": 29, "endOffset": 33}, {"referenceID": 45, "context": "Some examples include DBSCAN [21], GDBSCAN [51], OPTICS [2], and CFSFDP [49].", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "Some examples include DBSCAN [21], GDBSCAN [51], OPTICS [2], and CFSFDP [49].", "startOffset": 56, "endOffset": 59}, {"referenceID": 43, "context": "Some examples include DBSCAN [21], GDBSCAN [51], OPTICS [2], and CFSFDP [49].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": ", ARTMAP [9] and SOM [33]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 58, "context": ", [65, 63]) and computer vision.", "startOffset": 2, "endOffset": 10}, {"referenceID": 56, "context": ", [65, 63]) and computer vision.", "startOffset": 2, "endOffset": 10}, {"referenceID": 44, "context": "Figure 1: U-Net architecture [50] adopted in this work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 42, "context": "Research in computer vision on detecting occlusion relations in natural images was stimulated by the construction of the BSDS border ownership dataset [48].", "startOffset": 151, "endOffset": 155}, {"referenceID": 52, "context": ", [59]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "However, in spite of such challenges, the techniques based on deep learning demonstrate impressive performance in the standard benchmark datasets such as PASCAL VOC [22] or MS COCO [39].", "startOffset": 165, "endOffset": 169}, {"referenceID": 33, "context": "However, in spite of such challenges, the techniques based on deep learning demonstrate impressive performance in the standard benchmark datasets such as PASCAL VOC [22] or MS COCO [39].", "startOffset": 181, "endOffset": 185}, {"referenceID": 35, "context": ", [41, 11, 46]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": ", [41, 11, 46]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 40, "context": ", [41, 11, 46]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": "Hsu and Kira [29] utilized pairwise constraints to train a neural network to perform clustering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "In [10], deep belief networks [28] were used for non-parametric clustering.", "startOffset": 3, "endOffset": 7}, {"referenceID": 53, "context": "[60] proposed a task-specific deep architecture for clustering based on sparse coding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[61] proposed an approach for image clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[56] defined a new objective function by considering the reconstruction error from an auto-encoder network and restricting the distance in the learned space between data and their corresponding cluster centers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed a deep clustering approach to solving acoustic source separation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] explored the possibility of employing deep learning in graph clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "Second, CNNs learn representations through several stages of non-linear processing, akin to how the cortex adapts to represent the visual world [62].", "startOffset": 144, "endOffset": 148}, {"referenceID": 47, "context": ", HMAX [53]), these models capture aspects of the organization of the visual ventral stream.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "Figure 1 shows the proposed deep network architecture which is based on the U-Net [50]; an encoderdecoder with skip connections.", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "We use the mean squared error loss and train the network with the Adam optimizer [31].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": ",m}, D = [200 300], and SC = [10 30].", "startOffset": 29, "endOffset": 36}, {"referenceID": 27, "context": ",m}, D = [200 300], and SC = [10 30].", "startOffset": 29, "endOffset": 36}, {"referenceID": 18, "context": "To randomly generate a Gaussian Mixture Density distribution, a 2D mean vector M = [x y] is randomly sampled (x, y \u2208 [20 100]).", "startOffset": 117, "endOffset": 125}, {"referenceID": 0, "context": "A random matrix A = [a b; c d] is generated with elements in [0 1].", "startOffset": 61, "endOffset": 66}, {"referenceID": 34, "context": "We used the following clustering methods as the benchmark algorithms: k-Means (KM) [40], minimizes the sum of squared errors between data points and their nearest cluster centers.", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Fuzzy C-Means (FCM) [20, 4] assigns soft labels to data points meaning that each data point can belong to more than one cluster with different degrees of membership.", "startOffset": 20, "endOffset": 27}, {"referenceID": 3, "context": "Fuzzy C-Means (FCM) [20, 4] assigns soft labels to data points meaning that each data point can belong to more than one cluster with different degrees of membership.", "startOffset": 20, "endOffset": 27}, {"referenceID": 48, "context": "The first one is the Normalized Cut (NC) proposed by Shi and Malik [54].", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "The second algorithm, known as Ng-Jordan-Weiss (NJW) [45] is a simple and significant example of spectral clustering which analysis the eigenvectors of the Laplacian of the similarity matrix.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "Mean shift (MS) [15] iteratively seeks the modes of a density function from discrete samples of that function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 43, "context": "Clustering by fast search and find of density peaks (CFSFDP) [49] method seeks the modes or peaks of a distribution.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "Classification and clustering have been studied separately in machine learning and computer vision. Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution. We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints. Instead, we emphasize on the compositionality of the real world structures and objects. In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms. The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance. This, by no means, suggests that other methods do not hold merits. For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).", "creator": "LaTeX with hyperref package"}}}