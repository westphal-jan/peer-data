{"id": "1706.05083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation", "abstract": "This work presents a novel approach to jointly tackling Automatic Post-Editing (APE) and Word-Level Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features which have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models which use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.\n\n\n\nIn the second part, we consider the ability to write a sentence of two sentences of two sentences of two sentences of two sentences. The first sentence is a given sentence with the following description:\nFirst sentence: the sentence contains the following sentence:\nSecond sentence: the sentence contains the following sentence:\nThird sentence: the sentence contains the following sentence:\nFourth sentence: the sentence contains the following sentence:\nTo create the model, we use the following two sentences:\nThird sentence: the sentence contains the following sentence:\nThe second sentence: the sentence contains the following sentence:\nA few lines of output:\nThen we write the sentence:\nSo: The first sentence:\nThen, we write the sentence:\nThe second sentence: the sentence contains the following sentence:\nA few lines of output:\nFinally, the second sentence: the sentence contains the following sentence:\nThe third sentence: the sentence contains the following sentence:\nSo: The second sentence: the sentence contains the following sentence:\nSo: The second sentence: the sentence contains the following sentence:\nFor a few lines of output:\nThe third sentence: the sentence contains the following sentence:\nThe second sentence: the sentence contains the following sentence:\nThe third sentence: the sentence contains the following sentence:\nThis is a simplified version of the example above. It is based on multiple pre-written sentences, but is made possible by using the same methods of NMT. The model also includes several examples of machine translation in the first part.\nIn the third part of this series, we", "histories": [["v1", "Thu, 15 Jun 2017 20:47:03 GMT  (80kb,D)", "https://arxiv.org/abs/1706.05083v1", "APE/QE System Description Paper for WMT/CMT 2017"], ["v2", "Sat, 15 Jul 2017 12:38:48 GMT  (81kb,D)", "http://arxiv.org/abs/1706.05083v2", "APE/QE System Description Paper for WMT/CMT 2017"]], "COMMENTS": "APE/QE System Description Paper for WMT/CMT 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris hokamp"], "accepted": false, "id": "1706.05083"}, "pdf": {"name": "1706.05083.pdf", "metadata": {"source": "CRF", "title": "Ensembling Factored Neural Machine Translation Models for Automatic Post-Editing and Quality Estimation", "authors": ["Chris Hokamp"], "emails": ["chokamp@computing.dcu.ie"], "sections": [{"heading": "1 Introduction", "text": "Translation destined for human consumption often must pass through multiple editing stages. In one common scenario, human translators correct machine translation (MT) output, correcting errors and omissions until a perfect translation has been produced. Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (Green et al., 2013).\nA relatively recent line of research has tried to build models which correct errors in MT automatically (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016). Automatic Post-Editing (APE) typically views the\nsystem that produced the original translation as a black box, which cannot be modified or inspected. An APE system has access to the same data that a human translator would see: a source sentence and a translation hypothesis. The job of the system is to output a corrected hypothesis, attempting to fix errors made by the original translation system. This can be viewed as a sequence-tosequence task (Sutskever et al., 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al., 2016). However, APE intuitively tries to make the minimum number of edits required to transform the hypothesis into a satisfactory translation, because we would like our system to mimic human translators in attempting to minimize the time spent correcting each MT output. This additional constraint on APE models differentiates the task from multisource MT.\nThe Word Level QE task is ostensibly a simpler version of APE, where a system must only decide whether or not each word in an MT hypothesis belongs in the post-edited version \u2013 it is not necessary to propose a fix for errors. Most recent work has considered word-level QE to be a sequence labeling task, and employed the standard tools of structured prediction to solve it, i.e. structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016). However, Martins et al. (2017) recently proposed a new method of word-level QE using APE, which simply uses an APE system to produce a \"pseudo-post-edit\" given a source sentence and an MT hypothesis. Their approach, which we call APE-QE, is the basis of the work presented here. In APE-QE, the original MT hypothesis is then aligned with the pseudo-postedit from the APE system using word level editar X iv :1 70 6.\n05 08\n3v 2\n[ cs\n.C L\n] 1\n5 Ju\nl 2 01\n7\ndistance, and words which correspond to Insert or Delete operations are labeled as incorrect. Note that this also corresponds exactly to the way QE datasets are currently created, with the only difference being that human post-edits are typically used to create gold-standard data (Bojar et al., 2015).\nA key similarity between the QE and APE tasks is that both use information from two sequences: (1) the original source input, and (2) an MT hypothesis. Martins et al. (2017), showed that APE systems with no knowledge about the QE task already provide a very strong baseline for QE. Because the essential training data for the APE and QE tasks is identical, consisting of parallel triples of (SRC,MT, PE), it is also natural to consider these tasks as two subtasks that make use of a single underlying model.\nIn this work, we explicitly design ensembles of NMT models for both word-level QE, and APE. This approach builds upon the approach presented in Martins et al. (2017), by incorporating features which have proven effective for Word Level QE as \"factors\" in the input to Neural Machine Translation (NMT) systems. We achieve state-of-the-art results in both Automatic Post-Editing and WordLevel Quality Estimation, matching the performance of much more complex QE systems, and significantly outperforming the current state-ofthe-art in APE.\nThe main contributions of this work are:\n\u2022 Novel Input Representations for Neural APE models\n\u2022 New tuned ensembles for APE-QE\n\u2022 An open-source decoder supporting ensembles of models with different inputs1\nThe following sections discuss our approach to creating hybrid models for APE-QE, which should be able to solve both tasks with minimal modification."}, {"heading": "2 Related Work", "text": "Two important lines of research have recently made breakthroughs in QE and APE.\n1code avaiable at https://github.com/ chrishokamp/constrained_decoding"}, {"heading": "2.1 Automatic Post-Editing", "text": "APE and QE training datasets consist of (SRC,MT, PE) triples, where the post-edited reference is created by a human translator in the workflow described above. However, publicly available APE datasets are relatively small in comparison to parallel datasets used to train machine translation systems. Junczys-Dowmunt and Grundkiewicz (2016) introduce a method for generating a large synthetic training dataset from a parallel corpus of (SRC,REF ) by first translating the reference to the source language, and then translating this \"pseudo-source\" back into the target language, resulting in a \u201cpseudohypothesis\" which is likely to be more similar to the reference than a direct translation from source to target. The release of this synthetic training data was a major contribution towards improving APE.\nJunczys-Dowmunt and Grundkiewicz (2016) also present a framework for ensembling SRC\u2192 PE and SRC \u2192 PE NMT models together, and tuning for APE performance. Our work extends this idea with several new input representations, which are inspired by the goal of solving both QE and APE with the same model."}, {"heading": "2.2 Quality Estimation", "text": "Martins et al. (2016) introduced a stacked architecture, using a very large feature set within a structured prediction framework to achieve a large jump in the state of the art for Word-Level QE. Some features are actually the outputs of standalone feedforward and recurrent neural network models, which are then stacked into the final system. Although their approach creates a very good final model, the training and feature extraction steps are quite complicated. An additional disadvantage of this approach is that it requires \"jackknifing\" the training data for the standalone models that provide features to the stacked model, in order to avoid overfitting in the stacked ensemble. This requires training k versions of each model type, where k is the number of jackknife splits.\nOur approach is most similar to Martins et al. (2017), the major differences are: we do not use any internal features from the original MT system, and we do not need to \"jackknife\" in order to create a stacked ensemble. Using only NMT with attention, we are able to surpass the state-of-theart in APE and match it in QE."}, {"heading": "2.3 Factored Inputs", "text": "Alexandrescu and Kirchoff (2006) introduced linguistic factors for neural language models. The core idea is to learn embeddings for linguistic features such as part-of-speech (POS) tags and dependency labels, augmenting the word embeddings of the input with additional features. Recent work has shown that NMT performance can also be improved by concatenating embeddings for additional word-level \"factors\" to source-word input embeddings (Sennrich and Haddow, 2016). The input representation ej for each source input xj with factors F thus becomes Eq. 1:\nej =\n|F |n\nk=1\nEkxjk (1)\nwhere f\nindicates vector concatenation, Ek is the embedding matrix of factor k, and xjk is a one hot vector for the k-th input factor."}, {"heading": "3 Models", "text": "In this section we describe the five model types used for APE-QE, as well as the ensembles of these models which turn out to be the bestperforming overall. We design several features to be included as inputs to APE. The operating hypothesis is that that features which haven proven useful for Quality Estimation should also have a positive impact upon APE performance.\nOur baseline models are the same models used in Junczys-Dowmunt (2016)2. The authors provide trained SRC \u2192 PE and MT \u2192 PE models, which correspond to the last four checkpoints from fine-tuning the models on the 500K training data concatenated with the task internal APE data upsampled 20 times. These models are referred to as SRC and MT."}, {"heading": "3.1 Word Alignments", "text": "Previous work has shown that alignment information between source and target is a critical component of current state-of-the-art word level QE systems (Kreutzer et al., 2015; Martins et al., 2016). The sequential inputs for structured prediction, as well as the feedforward and recurrent models in existing work obtain the source-side features for each target word using the word-alignments provided by the WMT task organizers. However, this information is not likely to be available in many real-world usecases for Quality Estimation, and the use of this information also means that the MT system used to produce the hypotheses is not actually a \"black box\", which is part of the definition of the QE task. Clearly, access to the word-alignment information of an SMT system provides a lot of insight into the underlying model.\nBecause our models rely upon synthetic training data, and because we wish to view the MT system as a true black-box, we instead use the SRC NMT system to obtain these alignments. The attention model for NMT produces a normalized vector of weights at each timestep, where the weights can be viewed as the \"alignment probabilities\" for each source word (Bahdanau et al., 2014). In order to obtain the input representation shown in table 3, we use the source word with the highest weight from the attention model as an additional factor in the input to another MT-aligned\u2192 PE system.\n2These models have been made available by the authors at https://amunmt.github.io/examples/ postedit/\nThe MT-aligned\u2192 PE system thus depends upon the SRC \u2192 PE system to produce the additional alignment factor."}, {"heading": "3.2 Inputting Both Source and Target", "text": "Following Crego et al. (2016), we train a model which takes the concatenated source and MT as input. The two sequences are separated by a special BREAK token. We refer to this system as SRC+MT."}, {"heading": "3.3 Part-of-Speech and Dependency Labels", "text": "Sennrich and Haddow (2016) showed that information such as POS tags, NER labels, and syntactic roles can be included in the input to NMT models, generally improving performance. Inspired by this idea, we select some of the top performing features from Martins et al. (Martins et al., 2016), and include them as input factors to the\nSRC+MT-factor model. The base representation is the concatenated SRC+MT (again with a special BREAK token). For each word in the English source and the German hypothesis, we obtain the part-of-speech tag, the dependency relation, and the part-of-speech of the head word, and include these as input factors. For both English and German, we use spaCy3 to extract these features for all training, development, and test data. The resulting model is illustrated in figure 1."}, {"heading": "3.4 Extending Factors to Subword Encoding", "text": "Our NMT models use subword encoding (Sennrich et al., 2016), but the additional factors are computed at the word level. Therefore, the factors must also be segmented to match the BPE segmentation. We use the {BILOU}- prefixes common in sequence-labeling tasks such as NER to extend\n3https://spacy.io/\nfactor vocabularies and map each word-level factor to the subword segmentation of the source or target text.\nTable 3 shows the input representations for each of the model types using an example from the WMT 2016 test data."}, {"heading": "3.5 Ensembling NMT Models", "text": "We average the parameters of the four best checkpoints of each model type, and create an ensemble of the resulting five models, called Avg-All Baseline. We then tune this ensemble for TER (APE) and F1-Mult (QE), using MERT (Och, 2003). The tuned models are called Avg-All APE-Tuned and Avg-All QE-Tuned, respectively. After observing that source-only models have the best singlemodel QE performance (see section 5), we created a final F1-Mult tuned ensemble, consisting of the four individual SRC models, and the averaged models from each other type (an ensemble of eight models total), called 4-SRC+Avg-All QE-Tune."}, {"heading": "3.6 Tuning", "text": "Table 2 shows the final weights for each ensemble type after tuning. In line with the two-model ensemble presented in Martins et al. (2017), tuning models for F1-Mult results in much more weight being allocated to the SRC model, while TER tuning favors models with access to the MT hypothesis."}, {"heading": "4 Experiments", "text": "All of our models are trained using Nematus (Sennrich et al., 2017). At inference time we use our own decoder, which supports weighted loglinear ensembles of Nematus models4. Following Junczys-Dowmunt and Grundkiewicz (2016), we\n4https://github.com/chrishokamp/ constrained_decoding\nfirst train each model type on the large (4M) synthetic training data, then fine tune using the 500K dataset, concatenated with the task-internal training data upsampled 20x. Finally, for SRC+MT and SRC+MT-factor we continued fine-tuning each model for a small number of iterations using the min-risk training implementation available in Nematus (Shen et al., 2016). Table 4 shows the best dev result after each stage of training.\nFor both APE and QE, we use only the taskspecific training data provided for the WMT 2017 APE task, including the extra synthetic training data5. However, note that the SpaCy models used to extract features for the factored models are trained with external data \u2013 we only use the offthe-shelf models provided by the SpaCy developers.\nTo convert the output sequence from an APE system into OK,BAD labels for QE, we use the APE hypothesis as a \"pseudo-reference\", which is then aligned with the original MT hypothesis using TER (Snover et al., 2006)."}, {"heading": "5 Results", "text": "Table 1 shows the results of our experiments using the WMT 16 development and test sets. For each system, we measure performance on BLEU and TER, which are the metrics used in APE task, and also on F1-Mult, which is the primary metric used for the Word Level QE task. Overall tagging accuracy is included as a secondary metric for QE.\nAll systems with input factors significantly improve APE performance over the baselines. For QE, the trends are less clear, but point to a key difference between optimizing for TER vs. F1_product: F1_product optimization probably lowers the threshold for \"changing\" a word, as opposed to copying it from the MT hypothesis. This hypothesis is supported by the observation that the source-only APE system outperforms all other single models on the QE metrics. Because the source-only systems cannot resort to copying words from the input, they are forced to make the best guess about the final output, and words which are more likely to be wrong are less likely to be present in the output. If input factors were used with a source-only APE system, the performance on word-level QE could likely be further improved. However, this hypothesis needs more\n5http://www.statmt.org/wmt17/ape-task. html\nanalysis and experimentation to confirm."}, {"heading": "6 Conclusion", "text": "This work has presented APE-QE, unifying models for APE and word-level QE by leveraging the flexibility of NMT to take advantage of informative features from QE. Models with different input representations are ensembled together and tuned for either APE or QE, achieving state of the art performance in both tasks. The complementary nature of these tasks points to future avenues of exploration, such as joint training using both QE labels and reference translations, as well as the incorporation of other features as input factors."}, {"heading": "Acknowledgments", "text": "This project has received funding from Science Foundation Ireland in the ADAPT Centre for Dig-\nital Content Technology (www.adaptcentre.ie) at Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund and the European Union Horizon 2020 research and innovation programme under grant agreement 645452 (QT21). Marcin JunczysDowmunt provided essential guidance on the tuning implementation for APE and QE ensembles."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu", "Katrin Kirchhoff."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. Association for Computational Linguistics, Strouds-", "citeRegEx": "Alexandrescu and Kirchhoff.,? 2006", "shortCiteRegEx": "Alexandrescu and Kirchhoff.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Kevin Knight, Ani Nenkova, and Owen Rambow, editors, HLT-NAACL. The Association for Compu-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "The efficacy of human postediting for language translation", "author": ["Spence Green", "Jeffrey Heer", "Christopher D. Manning."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, New", "citeRegEx": "Green et al\\.,? 2013", "shortCiteRegEx": "Green et al\\.", "year": 2013}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "Proceedings of the First Conference on Machine Translation. Association for", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Quality estimation from scratch (quetch): Deep learning for word-level translation quality estimation", "author": ["Julia Kreutzer", "Shigehiko Schamoni", "Stefan Riezler."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine", "citeRegEx": "Kreutzer et al\\.,? 2015", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2015}, {"title": "MARMOT: A toolkit for translation quality estimation at the word level", "author": ["Varvara Logacheva", "Chris Hokamp", "Lucia Specia."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoro\u017e,", "citeRegEx": "Logacheva et al\\.,? 2016", "shortCiteRegEx": "Logacheva et al\\.", "year": 2016}, {"title": "Unbabel\u2019s participation in the wmt16 word-level translation quality estimation shared task", "author": ["Andr\u00e9 F.T. Martins", "Ram\u00f3n Astudillo", "Chris Hokamp", "Fabio Kepler."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computa-", "citeRegEx": "Martins et al\\.,? 2016", "shortCiteRegEx": "Martins et al\\.", "year": 2016}, {"title": "Pushing the limits of translation quality estimation", "author": ["Andr\u00e9 FT Martins", "Marcin Junczys-Dowmunt", "Fabio N Kepler", "Ram\u00f3n Astudillo", "Chris Hokamp"], "venue": null, "citeRegEx": "Martins et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2017}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proc. of the Annual Meeting on Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Nematus: a toolkit", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Au-", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Rule-based translation with statistical phrase-based post-editing", "author": ["Michel Simard", "Nicola Ueffing", "Pierre Isabelle", "Roland Kuhn."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. pages 203\u2013206.", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Exploiting objective annotations for measuring translation post-editing effort", "author": ["Lucia Specia."], "venue": "Proceedings of the 15th Conference of the European Association for Machine Translation. pages 73\u201380.", "citeRegEx": "Specia.,? 2011", "shortCiteRegEx": "Specia.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. MIT Press, Cam-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Kevin Knight, Ani Nenkova, and Owen Rambow, editors, NAACL HLT 2016,", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (Green et al.", "startOffset": 116, "endOffset": 130}, {"referenceID": 4, "context": "Several studies has shown that this process, referred to as \"post-editing\", is faster than translation from scratch (Specia, 2011), or interactive machine translation (Green et al., 2013).", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": "A relatively recent line of research has tried to build models which correct errors in MT automatically (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 104, "endOffset": 185}, {"referenceID": 5, "context": "A relatively recent line of research has tried to build models which correct errors in MT automatically (Simard et al., 2007; Bojar et al., 2015; Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 104, "endOffset": 185}, {"referenceID": 18, "context": "This can be viewed as a sequence-tosequence task (Sutskever et al., 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 19, "context": ", 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 65, "endOffset": 108}, {"referenceID": 3, "context": ", 2014), and is also similar to multi-source machine translation (Zoph and Knight, 2016; Firat et al., 2016).", "startOffset": 65, "endOffset": 108}, {"referenceID": 7, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016).", "startOffset": 193, "endOffset": 239}, {"referenceID": 8, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016).", "startOffset": 193, "endOffset": 239}, {"referenceID": 7, "context": "structured predictors such as CRFs or structured SVMs, which take advantage of sparse representations and very large feature sets, as well as dependencies between labels in the output sequence (Logacheva et al., 2016; Martins et al., 2016). However, Martins et al. (2017) recently proposed a new method of word-level QE using APE, which simply uses an APE system to produce a \"pseudo-post-edit\" given a source sentence and an MT hypothesis.", "startOffset": 194, "endOffset": 272}, {"referenceID": 8, "context": "Martins et al. (2017), showed that APE systems with no knowledge about the QE task already provide a very strong baseline for QE.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "in Martins et al. (2017), by incorporating features which have proven effective for Word Level QE as \"factors\" in the input to Neural Machine Translation (NMT) systems.", "startOffset": 3, "endOffset": 25}, {"referenceID": 5, "context": "Junczys-Dowmunt and Grundkiewicz (2016) introduce a method for generating a large synthetic training dataset from a parallel corpus of (SRC,REF ) by first translating the reference to the source language, and then translating this \"pseudo-source\" back into the target language, resulting in a \u201cpseudohypothesis\" which is likely to be more similar to", "startOffset": 0, "endOffset": 40}, {"referenceID": 8, "context": "Our approach is most similar to Martins et al. (2017), the major differences are: we do not use any internal features from the original MT system, and we do not need to \"jackknife\" in order to create a stacked ensemble.", "startOffset": 32, "endOffset": 54}, {"referenceID": 12, "context": "Recent work has shown that NMT performance can also be improved by concatenating embeddings for additional word-level \"factors\" to source-word input embeddings (Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 187}, {"referenceID": 6, "context": "nent of current state-of-the-art word level QE systems (Kreutzer et al., 2015; Martins et al., 2016).", "startOffset": 55, "endOffset": 100}, {"referenceID": 8, "context": "nent of current state-of-the-art word level QE systems (Kreutzer et al., 2015; Martins et al., 2016).", "startOffset": 55, "endOffset": 100}, {"referenceID": 1, "context": "The attention model for NMT produces a normalized vector of weights at each timestep, where the weights can be viewed as the \"alignment probabilities\" for each source word (Bahdanau et al., 2014).", "startOffset": 172, "endOffset": 195}, {"referenceID": 2, "context": "Following Crego et al. (2016), we train a model which takes the concatenated source and MT as input.", "startOffset": 10, "endOffset": 30}, {"referenceID": 8, "context": "(Martins et al., 2016), and include them as input factors to the SRC+MT-factor model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Our NMT models use subword encoding (Sennrich et al., 2016), but the additional factors are computed at the word level.", "startOffset": 36, "endOffset": 59}, {"referenceID": 10, "context": "We then tune this ensemble for TER (APE) and F1-Mult (QE), using MERT (Och, 2003).", "startOffset": 70, "endOffset": 81}, {"referenceID": 8, "context": "In line with the two-model ensemble presented in Martins et al. (2017), tuning models for F1-Mult results in much more weight", "startOffset": 49, "endOffset": 71}, {"referenceID": 11, "context": "All of our models are trained using Nematus (Sennrich et al., 2017).", "startOffset": 44, "endOffset": 67}, {"referenceID": 5, "context": "Following Junczys-Dowmunt and Grundkiewicz (2016), we", "startOffset": 10, "endOffset": 50}, {"referenceID": 14, "context": "Finally, for SRC+MT and SRC+MT-factor we continued fine-tuning each model for a small number of iterations using the min-risk training implementation available in Nematus (Shen et al., 2016).", "startOffset": 171, "endOffset": 190}, {"referenceID": 16, "context": "system into OK,BAD labels for QE, we use the APE hypothesis as a \"pseudo-reference\", which is then aligned with the original MT hypothesis using TER (Snover et al., 2006).", "startOffset": 149, "endOffset": 170}], "year": 2017, "abstractText": "This work presents a novel approach to Automatic Post-Editing (APE) and WordLevel Quality Estimation (QE) using ensembles of specialized Neural Machine Translation (NMT) systems. Word-level features that have proven effective for QE are included as input factors, expanding the representation of the original source and the machine translation hypothesis, which are used to generate an automatically post-edited hypothesis. We train a suite of NMT models that use different input representations, but share the same output space. These models are then ensembled together, and tuned for both the APE and the QE task. We thus attempt to connect the state-of-the-art approaches to APE and QE within a single framework. Our models achieve state-of-the-art results in both tasks, with the only difference in the tuning step which learns weights for each component of the ensemble.", "creator": "LaTeX with hyperref package"}}}