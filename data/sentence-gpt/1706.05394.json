{"id": "1706.05394", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "A Closer Look at Memorization in Deep Networks", "abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. gradient-based adversarial performance by introducing a linear, nonlinear approach that involves training neural networks to solve complex problems as well as by embedding a linear learning framework into neural networks that are often highly sensitive to noise. We show that learning more complex networks using more efficient training algorithms, but also using more efficient training and learning algorithms that are not as efficient as those with fewer inputs (i.e., less). As our models show, the results suggest that neural networks are capable of doing better than conventional learning algorithms. Furthermore, this leads to more complicated training and learning algorithms, as more complex networks can be built on relatively low-cost techniques that allow for greater accuracy. This is particularly significant because of their high accuracy in learning and memory. We also note that a higher percentage of learned learning tasks can be performed as a generalization approach to solving complex problems as well as more complex tasks. Finally, this model predicts the effect of neural networks to learn complex problems from the loss of memory. Our results demonstrate that this model predicts how training neural networks can be used to generate better learning training. These findings indicate that learning learned to solve complex problems is more robust in learning tasks than learning to solve complex problems from the loss of memory.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Fri, 16 Jun 2017 18:11:09 GMT  (8495kb,D)", "http://arxiv.org/abs/1706.05394v1", "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work"], ["v2", "Sat, 1 Jul 2017 14:26:51 GMT  (8495kb,D)", "http://arxiv.org/abs/1706.05394v2", "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work"]], "COMMENTS": "Appears in Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, and David Krueger contributed equally to this work", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["devansh arpit", "stanislaw k jastrzebski", "nicolas ballas", "david krueger", "emmanuel bengio", "maxinder s kanwal", "tegan maharaj", "asja fischer", "aaron c courville", "yoshua bengio", "simon lacoste-julien"], "accepted": true, "id": "1706.05394"}, "pdf": {"name": "1706.05394.pdf", "metadata": {"source": "META", "title": "A Closer Look at Memorization in Deep Networks", "authors": ["Devansh Arpit", "Stanis\u0142aw Jastrz\u0119bski", "Nicolas Ballas", "David Krueger", "Emmanuel Bengio", "Maxinder S. Kanwal", "Tegan Maharaj", "Asja Fischer", "Aaron Courville", "Yoshua Bengio", "Simon Lacoste-Julien"], "emails": ["<david.krueger@umontreal.ca>."], "sections": [{"heading": "1. Introduction", "text": "The traditional view of generalization holds that a model with sufficient capacity (e.g. more parameters than training examples) will be able to \u201cmemorize\u201d each example, overfitting the training set and yielding poor generalization to validation and test sets (Goodfellow et al., 2016). Yet deep neural networks (DNNs) often achieve excellent generalization performance with massively over-parameterized models. This phenomenon is not well-understood.\n*Equal contribution 1Montr\u00e9al Institute for Learning Algorithms, Canada 2Universit\u00e9 de Montr\u00e9al, Canada 3Jagiellonian University, Krakow, Poland 4McGill University, Canada 5University of California, Berkeley, USA 6Polytechnique Montr\u00e9al, Canada 7University of Bonn, Bonn, Germany 8CIFAR Fellow 9CIFAR Senior Fellow. Correspondence to: <david.krueger@umontreal.ca>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nFrom a representation learning perspective, the generalization capabilities of DNNs are believed to stem from their incorporation of good generic priors (see, e.g., Bengio et al. (2009)). Lin & Tegmark (2016) further suggest that the priors of deep learning are well suited to the physical world. But while the priors of deep learning may help explain why DNNs learn to efficiently represent complex real-world functions, they are not restrictive enough to rule out memorization.\nOn the contrary, deep nets are known to be universal approximators, capable of representing arbitrarily complex functions given sufficient capacity (Cybenko, 1989; Hornik et al., 1989). Furthermore, recent work has shown that the expressiveness of DNNs grows exponentially with depth (Montufar et al., 2014; Poole et al., 2016). These works, however, only examine the representational capacity, that is, the set of hypotheses a model is capable of expressing via some value of its parameters.\nBecause DNN optimization is not well-understood, it is unclear which of these hypotheses can actually be reached by gradient-based training (Bottou, 1998). In this sense, optimization and generalization are entwined in DNNs. To account for this, we formalize a notion of the effective capacity (EC) of a learning algorithm A (defined by specifying both the model and the training procedure, e.g.,\u201ctrain the LeNet architecture (LeCun et al., 1998) for 100 epochs using stochastic gradient descent (SGD) with a learning rate of 0.01\u201d) as the set of hypotheses which can be reached by applying that learning algorithm on some dataset. Formally, using set-builder notation:\nEC(A) = {h | \u2203D such that h \u2208 A(D)} ,\nwhere A(D) represents the set of hypotheses that is reachable by A on a dataset D1.\nOne might suspect that DNNs effective capacity is sufficiently limited by gradient-based training and early stopping to resolve the apparent paradox between DNNs\u2019 excellent generalization and their high representational capacity. However, the experiments of Zhang et al. (2017) suggest that this is not the case. They demonstrate that DNNs are\n1 Since A can be stochastic, A(D) is a set.\nar X\niv :1\n70 6.\n05 39\n4v 1\n[ st\nat .M\nL ]\n1 6\nJu n\n20 17\nable to fit pure noise without even needing substantially longer training time. Thus even the effective capacity of DNNs may be too large, from the point of view of traditional learning theory.\nBy demonstrating the ability of DNNs to \u201cmemorize\u201d random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets. Intuitively, a brute-force memorization approach to fitting data does not capitalize on patterns shared between training examples or features; the content of what is memorized is irrelevant. A paradigmatic example of a memorization algorithm is k-nearest neighbors (Fix & Hodges Jr, 1951). Like Zhang et al. (2017), we do not formally define memorization; rather, we investigate this intuitive notion of memorization by training DNNs to fit random data.\nMain Contributions\nWe operationalize the definition of \u201cmemorization\u201d as the behavior exhibited by DNNs trained on noise, and conduct a series of experiments that contrast the learning dynamics of DNNs on real vs. noise data. Thus, our analysis builds on the work of Zhang et al. (2017) and further investigates the role of memorization in DNNs.\nOur findings are summarized as follows:\n1. There are qualitative differences in DNN optimization behavior on real data vs. noise. In other words, DNNs do not just memorize real data (Section 3).\n2. DNNs learn simple patterns first, before memorizing (Section 4). In other words, DNN optimization is content-aware, taking advantage of patterns shared by multiple training examples.\n3. Regularization techniques can differentially hinder memorization in DNNs while preserving their ability to learn about real data (Section 5)."}, {"heading": "2. Experiment Details", "text": "We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.) datasets. We investigate two classes of models: 2-layer multi-layer perceptrons (MLPs) with rectifier linear units (ReLUs) on MNIST and convolutional neural networks (CNNs) on CIFAR10. If not stated otherwise, the MLPs have 4096 hidden units per layer and are trained for 1000 epochs with SGD and learning rate 0.01. The CNNs are a small Alexnet-style CNN2 (as in Zhang et al. (2017)), and are trained using\n2Input \u2192 Crop(2,2) \u2192 Conv(200,5,5) \u2192 BN \u2192 ReLU \u2192 MaxPooling(3,3) \u2192 Conv(200,5,5) \u2192 BN\u2192 ReLU\u2192 MaxPool-\nSGD with momentum=0.9 and learning rate of 0.01, scheduled to drop by half every 15 epochs.\nFollowing Zhang et al. (2017), in many of our experiments we replace either (some portion of) the labels (with random labels), or the inputs (with i.i.d. Gaussian noise matching the real dataset\u2019s mean and variance) for some fraction of the training set. We use randX and randY to denote datasets with (100%, unless specified) noisy inputs and labels (respectively)."}, {"heading": "3. Qualitative Differences of DNNs Trained on Random vs. Real Data", "text": "Zhang et al. (2017) empirically demonstrated that DNNs are capable of fitting random data, which implicitly necessitates some high degree of memorization. In this section, we investigate whether DNNs employ similar memorization strategy when trained on real data. In particular, our experiments highlight some qualitative differences between DNNs trained on real data vs. random data, supporting the fact that DNNs do not use brute-force memorization to fit real datasets."}, {"heading": "3.1. Easy Examples as Evidence of Patterns in Real Data", "text": "A brute-force memorization approach to fitting data should apply equally well to different training examples. However, if a network is learning based on patterns in the data, some examples may fit these patterns better than others. We show that such \u201ceasy examples\u201d (as well as correspondingly \u201chard examples\u201d) are common in real, but not in random, datasets. Specifically, for each setting (real data, randX, randY), we train an MLP for a single epoch starting from 100 different random initializations and shufflings of the data. We find that, for real data, many examples are consistently classified (in)correctly after a single epoch, suggesting that different examples are significantly easier or harder in this sense. For noise data, the difference between examples is much less, indicating that these examples are fit (more) independently. Results are presented in Figure 1.\nFor randX, apparent differences in difficulty are well modeled as random Binomial noise. For randY, this is not the case, indicating some use of shared patterns. Visualizing first-level features learned by a CNN supports this hypothesis (Figure 2).\ning(3,3) \u2192 Dense(384) \u2192 BN \u2192 ReLU \u2192 Dense(192) \u2192 BN \u2192 ReLU \u2192 Dense(#classes) \u2192 Softmax. Here Crop(. , .) crops height and width from both sides with respective values."}, {"heading": "3.2. Loss-Sensitivity in Real vs. Random Data", "text": "To further investigate the difference between real and fully random inputs, we propose a proxy measure of memorization via gradients. Since we cannot measure quantitatively how much each training sample x is memorized, we instead measure the effect of each sample on the average loss. That is, we measure the norm of the loss gradient with respect to a previous example x after t SGD updates. Let Lt be the loss after t updates; then the sensitivity measure is given by\ngtx = \u2016\u2202Lt/\u2202x\u20161 .\nThe parameter update from training on x influences all future Lt indirectly by changing the subsequent updates on different training examples. We denote the average over gtx after T steps as g\u0304x, and refer to it as loss-sensitivity. Note that we only report `1-norm results, but that results stay very similar using `2-norm and infinity norm.\nWe compute gtx by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015). Unlike Maclaurin et al. (2015), we only use this procedure to compute gtx, and do not modify the training procedure in any way.\nWe find that for real data, only a subset of the training set has high g\u0304x, while for random data, g\u0304x is high for virtually all examples. We also find a different behavior when each example is given a unique class; in this scenario, the network has to learn to identify each example uniquely, yet still behaves differently when given real data than when given random data as input.\nWe visualize (Figure 3) the spread of g\u0304x as training progresses by computing the Gini coefficient over x\u2019s. The Gini coefficient (Gini, 1913) is a measure of the inequality among values of a frequency distribution; a coefficient of 0 means exact equality (i.e., all values are the same), while a coefficient of 1 means maximal inequality among values. We observe that, when trained on real data, the network has a high g\u0304x for a few examples, while on random data the network is sensitive to most examples. The difference between the random data scenario, where we know the neural network needs to do memorization, and the real data scenario, where we\u2019re trying to understand what happens, leads us to believe that this measure is indeed sensitive to memorization. Additionally, these results suggest that when being trained on real data, the neural network probably does not memorize, or at least not in the same manner it needs to for random data.\nIn addition to the different behaviors for real and random data described above, we also consider a class specific losssensitivity: g\u0304i,j = E(x,y)1/T \u2211T t |\u2202Lt(y = i)/\u2202xy=j |, where Lt(y = i) is the term in the crossentropy sum corresponding to class i. We observe that the loss-sensitivity w.r.t. class i for training examples of class j is higher when i = j, but more spread out for real data (see Figure 4). An interpretation of this is that for real data there are more interesting cross-category patterns that can be learned than for random data.\nFigure 3 and 4 were obtained by training a fully-connected network with 2 layers of 16 units on 1000 downscaled 14\u00d7 14 MNIST digits using SGD."}, {"heading": "3.3. Capacity and Effective Capacity", "text": "In this section, we investigate the impact of capacity and effective capacity on learning of datasets having different amounts of random input data or random labels."}, {"heading": "3.3.1. EFFECTS OF CAPACITY AND DATASET SIZE ON VALIDATION PERFORMANCES", "text": "In a first experiment, we study how overall model capacity impacts the validation performances for datasets with different amounts of noise. On MNIST, we found that the optimal validation performance requires a higher capacity model in the presence of noise examples (see Figure 5). This trend was consistent for noise inputs on CIFAR10, but\nwe did not notice any relationship between capacity and validation performance on random labels on CIFAR10.\nThis result contradicts the intuitions of traditional learning theory, which suggest that capacity should be restricted, in order to enforce the learning of (only) the most regular patterns. Given that DNNs can perfectly fit the training set in any case, we hypothesize that that higher capacity allows the network to fit the noise examples in a way that does not interfere with learning the real data. In contrast, if we were simply to remove noise examples, yielding a smaller (clean) dataset, a lower capacity model would be able to achieve optimal performance."}, {"heading": "3.3.2. EFFECTS OF CAPACITY AND DATASET SIZE ON TRAINING TIME", "text": "Our next experiment measures time-to-convergence, i.e. how many epochs it takes to reach 100% training accuracy. Reducing the capacity or increasing the size of the dataset slows down training as well for real as for noise\ndata3. However, the effect is more severe for datasets containing noise, as our experiments in this section show (see Figure 6).\nEffective capacity of a DNN can be increased by increasing the representational capacity (e.g. adding more hidden units) or training for longer. Thus, increasing the number of hidden units decreases the number of training iterations needed to fit the data, up to some limit. We observe stronger diminishing returns from increasing representational capacity for real data, indicating that this limit is lower, and a smaller representational capacity is sufficient, for real datasets.\nIncreasing the number of examples (keeping representational capacity fixed) also increases the time needed to memorize the training set. In the limit, the representational capacity is simply insufficient, and memorization is not feasible. On the other hand, when the relationship between inputs and outputs is meaningful, new examples sim-\n3 Regularization can also increase time-to-convergence; see section 5.\nply give more (possibly redundant) clues as to what the input \u2192 output mapping is. Thus, in the limit, an idealized learner should be able to predict unseen examples perfectly, absent noise. Our experiments demonstrate that time-toconvergence is not only longer on noise data (as noted by Zhang et al. (2017)), but also, increases substantially as a function of dataset size, relative to real data. Following the reasoning above, this suggests that our networks are learning to extract patterns in the data, rather than memorizing."}, {"heading": "4. DNNs Learn Patterns First", "text": "This section aims at studying how the complexity of the hypotheses learned by DNNs evolve during training for real data vs. noise data. To achieve this goal, we build on the intuition that the number of different decision regions into which an input space is partitioned reflects the complexity of the learned hypothesis (Sokolic et al., 2016). This notion is similar in spirit to the degree to which a function can scatter random labels: a higher density of decision boundaries in the data space allows more samples to be scattered.\nTherefore, we estimate the complexity by measuring how densely points on the data manifold are present around the model\u2019s decision boundaries. Intuitively, if we were to randomly sample points from the data distribution, a smaller fraction of points in the proximity of a decision boundary suggests that the learned hypothesis is simpler."}, {"heading": "4.1. Critical Sample Ratio (CSR)", "text": "Here we introduce the notion of a critical sample, which we use to estimate the density of decision boundaries as discussed above. A critical sample of a model is a datapoint x such that there exist a point x\u0302 in the proximity of x where the model predicts a different label from that of x. Specifically, consider a classification network\u2019s output\nvector f(x) = (f1(x), . . . , fk(x)) \u2208 Rk for a given input sample x \u2208 Rn from the data manifold. We call x a critical sample if there exists a point x\u0302 such that,\narg max i fi(x) 6= arg max j fj(x\u0302) (1)\ns.t. \u2016x\u2212 x\u0302\u2016\u221e \u2264 r\nwhere r is a fixed box size. Note that, unlike with adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label, which is not meaningfully defined for random data.\nFollowing the above argument relating complexity to decision boundaries, a higher number of critical samples indicatives a more complex hypothesis. Thus we measure complexity as the critical sample ratio (CSR), that is, the fraction of data-points for which we can find a critical sample: #critical samples|D| .\nTo identify whether a given data point x is a critical samples, we search for such an x\u0302 within a box of radius r. To perform this search, we propose using Langevin dynamics applied to the fast gradient sign method (FGSM, Goodfellow et al. (2014)) as shown in algorithm 14. We refer to this method as Langevin adversarial sample search (LASS). While the FGSM search algorithm can get stuck at a points with zero gradient, LASS explores the box more thoroughly. Specifically, a problem with first order gradient search methods (like FGSM) is that there might exist training points where the gradient is 0, but with a large 2nd derivative corresponding to a large change in prediction in the neighborhood. The noise added by the LASS algorithm during the search enables escaping from such points.\n4In our experiments, we set \u03b1 = 0.25, \u03b2 = 0.2 and \u03b7 is samples from standard normal distribution.\nAlgorithm 1 Langevin Adversarial Sample Search (LASS) Require: x \u2208 Rn, \u03b1, \u03b2, r, noise process \u03b7 Ensure: x\u0302\n1: converged = FALSE 2: x\u0303\u2190 x; x\u0302\u2190 \u2205 3: while not converged or max iter reached do 4: \u2206 = \u03b1 \u00b7 sign(\u2202fk(x)\u2202x ) + \u03b2 \u00b7 \u03b7 5: x\u0303\u2190 x\u0303 + \u2206 6: for i \u2208 [n] do\n7: x\u0303i \u2190 {\nxi + r \u00b7 sign(x\u0303i \u2212 xi) if |x\u0303i \u2212 xi| > r x\u0303i otherwise\n8: end for 9: if arg maxi f(x) 6= arg maxi f(x\u0303) then\n10: converged = TRUE 11: x\u0302\u2190 x\u0303 12: end if 13: end while"}, {"heading": "4.2. Critical Samples Throughout Training", "text": "We now show that the number of critical samples is much higher for a deep network (specifically, a CNN) trained on noise data compared with real data. To do so, we mea-\nsure the number of critical samples in the validation set5, throughout training6. Results are shown in Figure 9. A\n5 We also measure the number of critical samples in the training sets. Since we train our models using log loss, training points are pushed away from the decision boundary even after the network learns to classify them correctly. This leads to an initial rise and then fall of the number of critical samples in the training sets.\n6We use a box size of 0.3, which is small enough in a 0-255 pixel scale to be unnoticeable by a human evaluator. Different values for r were tested but did not change results qualitatively\nhigher number of critical samples for models trained on noise data compared with those trained on real data suggests that the learned decision surface is more complex for noise data (randX and randY). We also observe that the CSR increases gradually with increasing number of epochs and then stabilizes. This suggests that the networks learn gradually more complex hypotheses during training for all three datasets.\nIn our next experiment, we evaluate the performance and critical sample ratio of datasets with 20% to 80% of the training data replaced with either input or label noise. Results for MNIST and CIFAR-10 are shown in Figures 7 and 8, respectively. For both randX and randY datasets, the CSR is higher for noisier datasets, reflecting the higher level of complexity of the learned prediction function. The final and maximum validation accuracies are also both lower for noisier datasets, indicating that the noise examples interfere somewhat with the networks ability to learn about the real data.\nMore significantly, for randY datasets (Figures 7(b) and 8(b)), the network achieves maximum accuracy on the validation set before achieving high accuracy on the training set. Thus the model first learns the simple and general patterns of the real data before fitting the noise (which results in decreasing validation accuracy). Furthermore, as the model moves from fitting real data to fitting noise, the CSR greatly increases, indicating the need for more complex hypotheses to explain the noise. Combining this result with our results from Section 3.1, we conclude that real data examples are easier to fit than noise."}, {"heading": "5. Effect of Regularization on Learning", "text": "Here we demonstrate the ability of regularization to degrade training performance on data with random labels, while maintaining generalization performance on real data. Zhang et al. (2017) argue that explicit regularizations are not the main explanation of good generalization performance, rather SGD based optimization is largely responsible for it. Our findings extend their claim and indicate that explicit regularizations can substantially limit the speed of memorization of noise data without significantly impacting learning on real data.\nWe compare the performance of CNNs trained on CIFAR10 and randY with the following regularizers: dropout (with dropout rates in range 0-0.9), input dropout (range 0- 0.9), input Gaussian noise (with standard deviation in range 0-5), hidden Gaussian noise (range 0-0.3), weight decay (range 0-1) and additionally dropout with adversarial training (with weighting factor in range 0.2-0.7 and dropout in\nand lead to the same conclusions\nrate range 0.03-0.5).7 We train a separate model for every combination of dataset, regularization technique, and regularization parameter.\nThe results are summarized in Figure 10. For each combination of dataset and regularization technique, the final training accuracy on randY (x-axis) is plotted against the best validation accuracy on CIFAR-10 from amongst the models trained with different regularization parameters (yaxis). Flat curves indicate that the corresponding regularization technique can reduce memorization when applied on random labeling, while resulting in the same validation accuracy on the clean validation set. Our results show that different regularizers target memorization behavior to different extent \u2013 dropout being the most effective. We find that dropout, especially coupled with adversarial training, is best at hindering memorization without reducing the model\u2019s ability to learn. Figure 11 additionally shows this effect for selected experiments (i.e. selected hyperparameter values) in terms of train loss."}, {"heading": "6. Related Work", "text": "Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017). We make heavy use of their methodology of studying DNN training in the context of noise datasets. Zhang et al. (2017) show that DNNs can perfectly fit noise and thus that their generalization ability cannot be explained through traditional statistical learning theory (e.g., see (Vapnik & Vapnik, 1998; Bartlett et al., 2005)). We agree with this finding, but show in addition that the degree of memorization and generalization in DNNs depends not only on the architecture and training\n7We perform adversarial training using critical samples found by LASS algorithm with default parameters.\nprocedure (including explicit regularizations), but also on the training data itself 8.\nAnother direction we investigate is the relationship between regularization and memorization. Zhang et al. (2017) argue that explicit and implicit regularizers (including SGD) might not explain or limit shattering of random data. In this work we show that regularizers do control the speed at which DNNs memorize.\nA number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015). Previous work also suggests that SGD prioritizes the learning of simple hypothesis first. Sjoberg et al. (1995) showed that, for linear models, SGD first learns models with small `2 parameter norm. More generally, the efficacy of early stopping shows that SGD first learns simpler models (Yao et al., 2007). We extend these results, showing that DNNs trained with SGD learn patterns before memorizing, even in the presence of noise examples.\nVarious previous works have analyzed explanations for the generalization power of DNNs. Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.g. implemented by the network structure of a CNN or MLP) control the speed of learning at each layer. Neyshabur et al. (2014) note that the number of parameters does not control the effective capacity of a DNN, and that the reason for DNNs\u2019 generalization is unknown. We supplement this result by showing how the impact of representational capacity changes with varying noise levels. While exploring the effect of noise samples on learning dynamics has a long tradition (Bishop, 1995; An, 1996), we are the first to examine relationships between the fraction of noise samples\n8We conclude the latter part based on experimental findings in sections 3 and 4.2\nand other attributes of the learning algorithm, namely: capacity, training time and dataset size.\nMultiple techniques for analyzing the training of DNNs have been proposed before, including looking at generalization error, trajectory length evolution (Raghu et al., 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al., 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016). Instead of measuring the sharpness of the loss for the learned hypothesis, we investigate the complexity of the learned hypothesis throughout training and across different datasets and regularizers, as measured by the critical sample ratio. Critical samples are closely related to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014); the differences are: 1) critical samples are indifferent to any ground truth, only noting changes in a model\u2019s prediction, 2) critical samples refer to real data-points; adversarial examples refer to the adversarial points near real data-points. More similarly to critical samples, virtual adversarial training (VAT) (Miyato et al., 2015) looks at changes in the predictive distribution. Specifically, VAT penalizes changes in f(x) resulting from small changes in x, using the KL-divergence.\nTwo contemporary works perform in-depth explorations of topics related to our work. Bojanowski & Joulin (2017) show that predicting random noise targets can yield state of the art results in unsupervised learning, corroborating our findings in Section 3.1, especially Figure 2. Koh & Liang (2017) use influence functions to measure the impact on parameter changes during training, as in our Section 3.2. They explore several promising applications for this technique, including generation of adversarial training examples."}, {"heading": "7. Conclusion", "text": "Our empirical exploration demonstrates qualitative differences in DNN optimization on noise vs. real data, all of which support the claim that DNNs trained with SGDvariants first use patterns, not brute force memorization, to fit real data. However, since DNNs have the demonstrated ability to fit noise, it is unclear why they find generalizable solutions on real data; we believe that the deep learning priors including distributed and hierarchical representations likely play an important role. Our analysis suggests that memorization and generalization in DNNs depend on network architecture and optimization procedure, but also on the data itself. We hope to encourage future research on how properties of datasets influence the behavior of deep learning algorithms, and suggest a data-dependent understanding of DNN capacity as a research goal."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Akram Erraqabi and Jason Jo for helpful discussions. SJ was supported by Grant No. DI 2014/016644 from Ministry of Science and Higher Education, Poland. DA was supported by IVADO, CIFAR and NSERC. EB was financially supported by the Samsung Advanced Institute of Technology (SAIT). MSK and SJ were supported by MILA during the course of this work. We acknowledge the computing resources provided by ComputeCanada and CalculQuebec. Experiments were carried out using Theano (Theano Development Team, 2016) and Keras (Chollet et al., 2015)."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["An", "Guozhong"], "venue": "Neural computation,", "citeRegEx": "An and Guozhong.,? \\Q1996\\E", "shortCiteRegEx": "An and Guozhong.", "year": 1996}, {"title": "Local rademacher complexities", "author": ["Bartlett", "Peter L", "Bousquet", "Olivier", "Mendelson", "Shahar"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Learning deep architectures for ai", "author": ["Bengio", "Yoshua"], "venue": "Foundations and trends\u00ae in Machine Learning,", "citeRegEx": "Bengio and Yoshua,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua", "year": 2009}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Bishop", "Chris M"], "venue": "Neural computation,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "Unsupervised Learning by Predicting Noise", "author": ["P. Bojanowski", "A. Joulin"], "venue": "ArXiv e-prints,", "citeRegEx": "Bojanowski and Joulin,? \\Q2017\\E", "shortCiteRegEx": "Bojanowski and Joulin", "year": 2017}, {"title": "Online learning and stochastic approximations", "author": ["Bottou", "L\u00e9on"], "venue": "On-line learning in neural networks,", "citeRegEx": "Bottou and L\u00e9on.,? \\Q1998\\E", "shortCiteRegEx": "Bottou and L\u00e9on.", "year": 1998}, {"title": "Entropy-sgd: Biasing gradient descent into wide valleys", "author": ["Chaudhari", "Pratik", "Choromanska", "Anna", "Soatto", "Stefano", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1611.01838,", "citeRegEx": "Chaudhari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chaudhari et al\\.", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["Cybenko", "George"], "venue": "Mathematics of Control, Signals, and Systems (MCSS),", "citeRegEx": "Cybenko and George.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko and George.", "year": 1989}, {"title": "Discriminatory analysis-nonparametric discrimination: consistency properties", "author": ["Fix", "Evelyn", "Hodges Jr.", "Joseph L"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Fix et al\\.,? \\Q1951\\E", "shortCiteRegEx": "Fix et al\\.", "year": 1951}, {"title": "Variabilita e mutabilita", "author": ["Gini", "Corrado"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Gini and Corrado.,? \\Q1913\\E", "shortCiteRegEx": "Gini and Corrado.", "year": 1913}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Hardt", "Moritz", "Recht", "Benjamin", "Singer", "Yoram"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White", "Halbert"], "venue": "Neural networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "An empirical analysis of deep network loss surfaces", "author": ["Im", "Daniel Jiwoong", "Tao", "Michael", "Branson", "Kristin"], "venue": "arXiv preprint arXiv:1612.04010,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Keskar", "Nitish Shirish", "Mudigere", "Dheevatsa", "Nocedal", "Jorge", "Smelyanskiy", "Mikhail", "Tang", "Ping Tak Peter"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Understanding blackbox predictions via influence functions", "author": ["Koh", "Pang Wei", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1703.04730,", "citeRegEx": "Koh et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Koh et al\\.", "year": 2017}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Why does deep and cheap learning work so well", "author": ["Lin", "Henry W", "Tegmark", "Max"], "venue": "arXiv preprint arXiv:1608.08225,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Gradient-based hyperparameter optimization through reversible learning", "author": ["Maclaurin", "Dougal", "Duvenaud", "David K", "Adams", "Ryan P"], "venue": "In ICML, pp", "citeRegEx": "Maclaurin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maclaurin et al\\.", "year": 2015}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Miyato", "Takeru", "Maeda", "Shin-ichi", "Koyama", "Masanori", "Nakae", "Ken", "Ishii", "Shin"], "venue": "stat, 1050:25,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Kernel analysis of deep networks", "author": ["Montavon", "Gr\u00e9goire", "Braun", "Mikio L", "M\u00fcller", "KlausRobert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Montavon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Montavon et al\\.", "year": 2011}, {"title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "author": ["Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan"], "venue": "arXiv preprint arXiv:1412.6614,", "citeRegEx": "Neyshabur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neyshabur et al\\.", "year": 2014}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Overtraining, regularization and searching for a minimum, with application to neural networks", "author": ["J. Sjoberg", "J. Sjoeberg", "J. Sj\u00f6berg", "L. Ljung"], "venue": "International Journal of Control,", "citeRegEx": "Sjoberg et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Sjoberg et al\\.", "year": 1995}, {"title": "Robust large margin deep neural networks", "author": ["Sokolic", "Jure", "Giryes", "Raja", "Sapiro", "Guillermo", "Rodrigues", "Miguel RD"], "venue": "arXiv preprint arXiv:1605.08254,", "citeRegEx": "Sokolic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sokolic et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian J", "Fergus", "Rob"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Statistical learning theory, volume 1", "author": ["Vapnik", "Vladimir Naumovich", "Vlamimir"], "venue": null, "citeRegEx": "Vapnik et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik et al\\.", "year": 1998}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["Wilson", "D Randall", "Martinez", "Tony R"], "venue": "Neural Networks,", "citeRegEx": "Wilson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2003}, {"title": "On early stopping in gradient descent learning", "author": ["Yao", "Yuan", "Rosasco", "Lorenzo", "Caponnetto", "Andrea"], "venue": "Constructive Approximation,", "citeRegEx": "Yao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2007}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 12, "context": "On the contrary, deep nets are known to be universal approximators, capable of representing arbitrarily complex functions given sufficient capacity (Cybenko, 1989; Hornik et al., 1989).", "startOffset": 148, "endOffset": 184}, {"referenceID": 16, "context": ",\u201ctrain the LeNet architecture (LeCun et al., 1998) for 100 epochs using stochastic gradient descent (SGD) with a learning rate of 0.", "startOffset": 31, "endOffset": 51}, {"referenceID": 30, "context": "However, the experiments of Zhang et al. (2017) suggest that this is not the case.", "startOffset": 28, "endOffset": 48}, {"referenceID": 30, "context": "By demonstrating the ability of DNNs to \u201cmemorize\u201d random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets.", "startOffset": 65, "endOffset": 85}, {"referenceID": 30, "context": "By demonstrating the ability of DNNs to \u201cmemorize\u201d random noise, Zhang et al. (2017) also raise the question whether deep networks use similar memorization tactics on real datasets. Intuitively, a brute-force memorization approach to fitting data does not capitalize on patterns shared between training examples or features; the content of what is memorized is irrelevant. A paradigmatic example of a memorization algorithm is k-nearest neighbors (Fix & Hodges Jr, 1951). Like Zhang et al. (2017), we do not formally define memorization; rather, we investigate this intuitive notion of memorization by training DNNs to fit random data.", "startOffset": 65, "endOffset": 497}, {"referenceID": 30, "context": "Thus, our analysis builds on the work of Zhang et al. (2017) and further investigates the role of memorization in DNNs.", "startOffset": 41, "endOffset": 61}, {"referenceID": 16, "context": "We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 16, "context": "We perform experiments on MNIST (LeCun et al., 1998) and CIFAR10 (Krizhevsky et al.) datasets. We investigate two classes of models: 2-layer multi-layer perceptrons (MLPs) with rectifier linear units (ReLUs) on MNIST and convolutional neural networks (CNNs) on CIFAR10. If not stated otherwise, the MLPs have 4096 hidden units per layer and are trained for 1000 epochs with SGD and learning rate 0.01. The CNNs are a small Alexnet-style CNN2 (as in Zhang et al. (2017)), and are trained using", "startOffset": 33, "endOffset": 469}, {"referenceID": 30, "context": "Following Zhang et al. (2017), in many of our experiments we replace either (some portion of) the labels (with random labels), or the inputs (with i.", "startOffset": 10, "endOffset": 30}, {"referenceID": 18, "context": "We compute g x by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015). Unlike Maclaurin et al.", "startOffset": 117, "endOffset": 141}, {"referenceID": 18, "context": "We compute g x by unrolling t SGD steps and applying backpropagation over the unrolled computation graph, as done by Maclaurin et al. (2015). Unlike Maclaurin et al. (2015), we only use this procedure to compute g x, and do not modify the training procedure in any way.", "startOffset": 117, "endOffset": 173}, {"referenceID": 30, "context": "Our experiments demonstrate that time-toconvergence is not only longer on noise data (as noted by Zhang et al. (2017)), but also, increases substantially as a function of dataset size, relative to real data.", "startOffset": 98, "endOffset": 118}, {"referenceID": 25, "context": "To achieve this goal, we build on the intuition that the number of different decision regions into which an input space is partitioned reflects the complexity of the learned hypothesis (Sokolic et al., 2016).", "startOffset": 185, "endOffset": 207}, {"referenceID": 26, "context": "Note that, unlike with adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label, which is not meaningfully defined for random data.", "startOffset": 44, "endOffset": 91}, {"referenceID": 10, "context": "Note that, unlike with adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) the above definition depends only on the predicted label arg maxi fi(x) of x, and not the true label, which is not meaningfully defined for random data.", "startOffset": 44, "endOffset": 91}, {"referenceID": 10, "context": "To perform this search, we propose using Langevin dynamics applied to the fast gradient sign method (FGSM, Goodfellow et al. (2014)) as shown in algorithm 14.", "startOffset": 107, "endOffset": 132}, {"referenceID": 30, "context": "Zhang et al. (2017) argue that explicit regularizations are not the main explanation of good generalization performance, rather SGD based optimization is largely responsible for it.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": ", see (Vapnik & Vapnik, 1998; Bartlett et al., 2005)).", "startOffset": 6, "endOffset": 52}, {"referenceID": 29, "context": "Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017). We make heavy use of their methodology of studying DNN training in the context of noise datasets.", "startOffset": 73, "endOffset": 93}, {"referenceID": 29, "context": "Our work builds on the experiments and challenges the interpretations of Zhang et al. (2017). We make heavy use of their methodology of studying DNN training in the context of noise datasets. Zhang et al. (2017) show that DNNs can perfectly fit noise and thus that their generalization ability cannot be explained through traditional statistical learning theory (e.", "startOffset": 73, "endOffset": 212}, {"referenceID": 30, "context": "Zhang et al. (2017) argue that explicit and implicit regularizers (including SGD) might not explain or limit shattering of random data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015).", "startOffset": 185, "endOffset": 205}, {"referenceID": 29, "context": "More generally, the efficacy of early stopping shows that SGD first learns simpler models (Yao et al., 2007).", "startOffset": 90, "endOffset": 108}, {"referenceID": 11, "context": "A number of arguments support the idea that SGD-based learning imparts a regularization effect, especially with a small batch size (Wilson & Martinez, 2003) or a small number of epochs (Hardt et al., 2015). Previous work also suggests that SGD prioritizes the learning of simple hypothesis first. Sjoberg et al. (1995) showed that, for linear models, SGD first learns models with small ` parameter norm.", "startOffset": 186, "endOffset": 319}, {"referenceID": 20, "context": "Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Montavon et al. (2011) use kernel methods to analyze the complexity of deep learning architectures, and find that network priors (e.g. implemented by the network structure of a CNN or MLP) control the speed of learning at each layer. Neyshabur et al. (2014) note that the number of parameters does not control the effective capacity of a DNN, and that the reason for DNNs\u2019 generalization is unknown.", "startOffset": 0, "endOffset": 258}, {"referenceID": 22, "context": "Multiple techniques for analyzing the training of DNNs have been proposed before, including looking at generalization error, trajectory length evolution (Raghu et al., 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al.", "startOffset": 153, "endOffset": 173}, {"referenceID": 23, "context": ", 2016), analyzing Jacobians associated to different layers (Wang; Saxe et al., 2013), or the shape of the loss minima found by SGD (Im et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 13, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 6, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 14, "context": ", 2013), or the shape of the loss minima found by SGD (Im et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016).", "startOffset": 54, "endOffset": 116}, {"referenceID": 26, "context": "Critical samples are closely related to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014); the differences are: 1) critical samples are indifferent to any ground truth, only noting changes in a model\u2019s prediction, 2) critical samples refer to real data-points; adversarial examples refer to the adversarial points near real data-points.", "startOffset": 61, "endOffset": 108}, {"referenceID": 10, "context": "Critical samples are closely related to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014); the differences are: 1) critical samples are indifferent to any ground truth, only noting changes in a model\u2019s prediction, 2) critical samples refer to real data-points; adversarial examples refer to the adversarial points near real data-points.", "startOffset": 61, "endOffset": 108}, {"referenceID": 19, "context": "More similarly to critical samples, virtual adversarial training (VAT) (Miyato et al., 2015) looks at changes in the predictive distribution.", "startOffset": 71, "endOffset": 92}], "year": 2017, "abstractText": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.", "creator": "LaTeX with hyperref package"}}}