{"id": "1706.05826", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2017", "title": "Capacity Releasing Diffusion for Speed and Locality", "abstract": "Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics. Because they spread mass agnostically at each step in an iterative manner, they can sometimes spread mass \"too aggressively,\" thereby failing to find the \"right\" clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical spectral diffusion process. (We propose that we identify the right clusters by sampling one another using a different technique. For instance, it will be used to transfer information from one dataset to another in addition to to providing a better data transfer to another dataset.) The CRD can be used in large-scale computational models such as modeling the distribution of individual subgroups. It can be used in large-scale functional programming such as machine learning, neural networks, and computer programs such as artificial intelligence.\n\n\n\n\n\n\n\nIn general, the CRD does not provide the correct parameters for a cluster. Although the technique is widely used in computer science and computer vision, it is widely used by a wide variety of researchers, including mathematicians and statisticians. The CRD is an interesting technique that enables multiple clusters, in particular large-scale datasets, for multiple analyses to evaluate a cluster, including multiple clusters. It is possible for the CRD to be applied to hundreds of thousands of different datasets, and to detect over several millions of distinct clusters across a cluster. In this case, it should be used in large-scale applications such as machine learning, data analysis, and applied mathematics.\n\nThis type of technique is very versatile. However, it has been widely used by various computational systems for a wide variety of applications. For example, the CRD can be used to investigate and classify two clusters of multiple data in the same way as the two data types in the same way. For example, in the case of an individual cluster, the CRD can be used to investigate multiple clusters of multiple data for the same reason. A more common approach to CRD is to test multiple clusters of several different subgroups, such as machine learning, data analysis, and applied mathematics. In this case, it should be used in large-scale applications such as machine learning, data analysis, and applied mathematics. For example, in the case of a single cluster, the CRD can be used to investigate multiple clusters of multiple data for the same reason. For example, in the case of a single cluster, the", "histories": [["v1", "Mon, 19 Jun 2017 08:18:04 GMT  (136kb,D)", "http://arxiv.org/abs/1706.05826v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.IR", "authors": ["di wang", "kimon fountoulakis", "monika henzinger", "michael w mahoney", "satish rao"], "accepted": true, "id": "1706.05826"}, "pdf": {"name": "1706.05826.pdf", "metadata": {"source": "META", "title": "Capacity Releasing Diffusion for Speed and Locality", "authors": ["Di Wang", "Kimon Fountoulakis", "Monika Henzinger", "Michael W. Mahoney", "Satish Rao"], "emails": ["<wangd@eecs.berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics, perhaps most conspicuously in the area of spectral clustering (Cheeger, 1969; Donath & Hoffman, 1973; von Luxburg, 2006; Shi & Malik, 2000), community detection in networks (Ng et al., 2001; White & Smyth, 2005; Leskovec et al., 2009; Jeub et al., 2015), so-called manifold learning (Belkin & Niyogi, 2003; Mahoney et al., 2012), and PageRank-based spectral\n1EECS, UC Berkeley, Berkeley, CA, USA 2ICSI and Statistics, UC Berkeley, Berkeley, CA, USA 3Computer Science, University of Vienna, Vienna, Austria. Correspondence to: Di Wang <wangd@eecs.berkeley.edu>.\nranking in web ranking (Page et al., 1999; Gleich, 2015). Particularly relevant for our results are local/personalized versions of PageRank (Jeh & Widom, 2003) and local/distributed versions of spectral clustering (Spielman & Teng, 2004; Andersen et al., 2006; Andersen & Peres, 2009). These latter algorithms can be used to find provablygood small-sized clusters in very large graphs without even touching the entire graph; they have been implemented and applied to billion-node graphs (Shun et al., 2016); and they have been used to characterize the clustering and community structure in a wide range of social and information networks (Leskovec et al., 2009; Jeub et al., 2015).\nSomewhat more formally, we will use the term diffusion on a graph to refer to a process that spreads mass among vertices by sending mass along edges step by step according to some rule. With this interpretation, classical spectral diffusion spreads mass by distributing the mass on a given node equally to the neighbors of that node in an iterative manner. A well-known problem with spectral methods is that\u2014due to their close relationship with random walks\u2014they sometimes spread mass \u201ctoo aggressively,\u201d and thereby they don\u2019t find the \u201cright\u201d partition. In theory, this can be seen with so-called Cockroach Graph (Guattery & Miller, 1998; von Luxburg, 2006). In practice, this is seen by the extreme sensitivity of spectral methods to high-degree nodes and other structural heterogeneities in real-world graphs constructed from very noisy data (Leskovec et al., 2009; Jeub et al., 2015). More generally, it is well-known that spectral methods can be very sensitive to a small number of random edges, e.g., in small-world graphs, that \u201cshort circuit\u201d very distant parts of the original graph, as well as other noise properties in realistic data. Empirically, this is well-known to be a particular problem when there are moderately good\u2014but not very good\u2014clusters in the data, a situation that is all too common in machine learning and data analysis applications (Jeub et al., 2015).\nHere, we introduce a novel Capacity Releasing Diffusion (CRD) Process to address this problem. Our CRD Process is a type of diffusion that spreads mass according to a carefully-constructed push-relabel rule, using techniques that are well-known from flow-based graph algorithms, but modified here to release the capacity of edges to transmit mass. Our CRD Process has better properties with respect to limiting the spread of mass inside local well-connected\nar X\niv :1\n70 6.\n05 82\n6v 1\n[ cs\n.D S]\n1 9\nJu n\n20 17\nclusters. It does so with improved running time properties. We show that this yields improved local clustering algorithms, both in worst-case theory and in empirical practice."}, {"heading": "1.1. Capacity Releasing Diffusion (CRD)", "text": "We start by describing the generic CRD process in Figures 1 and 2, which lays down the dynamics of spreading mass across the graph. Importantly, this dynamical process is independent of any particular task to which it may be applied. Later (in Section 2) we also present a concrete CRD algorithm for the specific task of local clustering that exploits the dynamics of the generic CRD process1.\nThe entire CRD process (Figure 1) repeatedly applies the generic CRD inner process (which we call a CRD step), and then it doubles the amount of mass at all vertices between invocations. A CRD step starts with each vertex u having mass m(u) \u2264 2d(u), where d(u) is the degree of u, and spreads the mass so that at the end each vertex u has mass m(u) \u2264 d(u). Observe that, essentially, each CRD step\nspreads the mass to a region of roughly twice the volume comparing to the previous step.\n1The relation between the generic CRD process and the CRD algorithm for local graph clustering is analogous to the relation between local random walks and a local spectral graph partitioning algorithm such as that of Andersen et al. (2006).\nThe generic CRD inner process (Figure 2) implements a modification of the classic \u201cpush-relabel\u201d algorithm (Goldberg & Tarjan, 1988; 2014) for routing a source-sink flow. The crucial property of our process (different from the standard push-relabel) is that edge capacity is made available to the process slowly by releasing. That is, we only allow l(u) units of mass to move across any edge (u, v), where l(u) is the label (or height) maintained by the CRD inner process. Thus, edge capacity is released to allow mass to cross the edge as the label of the endpoint rises. As we will see, this difference is critical to the theoretical and empirical performance of the CRD algorithm."}, {"heading": "1.2. Example: Classical Versus Capacity Releasing", "text": "To give insight into the differences between classical spectral diffusion and our CRD Process, consider the graph in Figure 3. There is a \u201ccluster\u201d B, which consists of k paths, each of length l, joined at a common node u. There is one edge from u to the rest of the graph, and we assume the other endpoint v has very high degree such that the vast majority of the mass arriving there is absorbed by its neighbors in B. While idealized, such an example is not completely unrealistic (Leskovec et al., 2009; Jeub et al., 2015).\nConsider first classical spectral diffusion, with a random walk starting from some vertex in B. This process requires \u2126(`2) steps to spread probability mass to a constant fraction of the nodes on the paths, and in this many steps, the expected number of times to visit u is \u2126(`). Because of the edge to v, each time we visit u, we have a \u2126(1/k) chance of leaving B. Thus, when ` is \u2126(k), the random walk is expected to leave B and never return, i.e., the classical diffusion will leak out all the probability mass before even spreading beyond a constant fraction of B.\nConsider next our CRD Process, starting with mass at the vertex u \u2208 B (which would be a worst-case starting node in B for CRD). Assume that at some point the mass is spread along z neighboring vertices on each of the k paths. To continue the spread to 2z vertices in the next CRD step, the labels will be raised to (at most) 2z to allow the mass to spread over the path of length 2z. This enables the spread along the paths, but it only releases a capacity of 2z to the exiting edge (u, v). Since in this call, a total of 2zk mass is in the set B, at most 1/k of the mass escapes. After\nlog ` CRD steps, the mass is spread over all the k length-` paths, and only a (2 log `)/k fraction of the mass has escaped from B. Thus if ` = \u2126(k), as before, a factor of k/ log ` less mass has escaped from B with the CRD Process than with the classical diffusion process.\nWithout the releasing, however, the mass escapingB would be large, as even raising the label of vertex u to 1 would allow an arbitrary amount of mass to leak out.\nFinally, note that the `2 mixing time makes spectral diffusions a \u2126(`) factor slower than CRD. This drawback of spectral techniques can perhaps be resolved using sophisticated methods such as evolving sets (Andersen & Peres, 2009), though it comes easily with CRD."}, {"heading": "1.3. Our Main Results", "text": "We provide theoretical and empirical evidence that our CRD algorithm is superior to classical diffusion methods at finding clusters, with respect to noise tolerance, recovery accuracy, cut conductance, and running time. Here the cut conductance \u03a6(S) of a cut (S, S\u0304) is \u03a6(S) := |E(S,S\u0304)| min(vol(S),vol(V \\S)) , where E(S, S\u0304) denotes the set of edges between S and S\u0304,2 and the volume vol(S) is the sum of the degrees of the vertices in S. In all these measures, we break the quadratic Cheeger barrier for classical diffusions (explained below) while presenting a local algorithm (i.e., an algorithm whose running time depends on the volume of the cluster found and not of the whole graph).\nOur first main result (Section 2) presents a CRD algorithm and its running time. The CRD algorithm is a parameterized specialization of the generic CRD Process, where we limit the maximum label of vertices, as well as the maximum edge capacity. We prove that this specialization is efficient, in that it runs in time linear in the total mass and the label limit, and it either succeeds in spreading the mass or it leaves all unspread mass at nodes with high label. This property is analogous to the ispoerimetric capacity control provided by local spectral methods, and it is important for locating cluster bottlenecks. We use this crucially in our context to find low conductance clusters.\nOur second main result (Section 3) concerns the use of the CRD algorithm to find good local clusters in large graphs. Our result posits the existence of a \u201cgood\u201d clusterB, which satisfies certain conditions (Assumption 1 and 2) that naturally capture the notion of a local structure. The rather weak Assumption 1 states that B\u2019s internal connectivity \u03c6S(B) (see Section 3 for definition) is a constant factor better (i.e., larger) than the conductance \u03c6(B). Assumption 2 states that we have a smoothness condition which needs that any subset T \u2282 B has polylog(vol(B)) times more\n2Unless otherwise noted, when speaking of the conductance of a cut S, we assume S to be the side of minimum volume.\nneighbors inB\u2212T than in V \u2212B. Under these conditions, we can recover B starting from any vertex in B.\nBoth assumptions formalize the idea that the signal of the local structure is stronger than the noise of the cluster by some moderately large factor. More specifically, Assumption 1 roughly says that the weakest signal of any subset of B is a constant times stronger than the average noise of B; and Assumption 2 roughly says the signal of any subset is polylog(vol(B)) times stronger than the noise of the subset.\nWe note that Assumption 1 is significantly weaker than the factor in Zhu et al. (2013), where it is shown how to localize a cluster B such that \u03c6S(B) \u2265 \u221a \u03c6(B). Their condition is considerably stricter than our condition on the ratio between \u03c6S(B) and \u03c6(B), especially when \u03c6(B) is small, as is common. Their algorithm relies on proving that a classical diffusion starting at a typical node keeps most of its mass inside of B. However, they do not need something like our smoothness condition.\nWith the additional smoothness condition, we break the dependence on \u221a \u03c6(B) that is central to all approaches using spectral diffusions, including Zhu et al. (2013), for the first time with a local algorithm. In particular, comparing to Zhu et al. (2013), under their parameter settings (but with the smoothness condition), we identify a cluster with \u221a \u03c6(B) times less error, and we have a 1/ \u221a \u03c6(B) speedup in running time. This improvement is (up to a log `-factor) consistent with the behavior in the example of the previous section where the improvement is k/ log ` = O(1/( \u221a 1/\u03c6(B) log `)) as \u03c6(B) = 1/k` and ` = \u2126(k).\nWe note that with the additional smoothness condition, our theoretical results hold for any starting node vs in B, in contrast to prior spectral-based results which only work when starting from a \u201cgood\u201d node (where only a constant fraction of the nodes inB are good). We expect the smoothness condition to be an artifact of our analysis, i.e., similar results actually hold when starting at good nodes inB, even without this assumption.\nOur third main result (Section 4) is an empirical illustration of our method. We consider several social and information networks studied previously that are particularly challenging for spectral methods. In particular, while graphs that have upward-sloping NCPs (Network Community Profiles) have good small clusters (Leskovec et al., 2009; Jeub et al., 2015), denser social networks with flat NCPs do not have any very-good conductance clusters of any size. They do, however, often have moderately-good clusters, but these are very difficult for spectral methods to identify (Jeub et al., 2015). Our empirical results show that our CRD-based local clustering algorithm is better able to identify and extract in a strongly local running time moderately\ngood quality clusters from several such social networks."}, {"heading": "1.4. Previous Work: Low Conductance Cuts, Diffusions, and Multicommodity Flow", "text": "Spectral algorithms for computing eigenvalues use some variant of repeated matrix multiplication, which for graphs is a type of classical diffusion. For the Laplacian of a graph, the convergence rate is O(1/\u03bb2), where \u03bb2 is the second smallest eigenvalue by of this matrix. The Lanczos method improves this rate to O( \u221a 1/\u03bb2) by cleverly and efficiently combining different iterations of the diffusions. See, e.g., Orecchia et al. (2012) for more details on this.\nOne application of such a computation is to find a low conductance cut in a graph. The second eigenvector for G can be used to find a cut of conductance O( 1\u03bb2 ) (Cheeger, 1969; Donath & Hoffman, 1973). Let \u03c6G be the minimum conductance in the graph. In his work, Cheeger already observed that random-walk based diffusion can make a \u0398(1/ \u221a \u03c6G) error in estimating the conductance, informally known as the (quadratic) Cheeger barrier, and illustrated in our example. This, combined with the fact that \u03bb2 = O(1/\u03c6G), gives a spectral method to find anO(\u03c6 1/2 G ) conductance cut in G.\nSpielman-Teng (2004) used local versions of diffusions (i.e., those with small support) to compute recursive decompositions efficiently, and then they used locality to produce linear time partitioning algorithms. Andersen, Chung and Lang (2006) developed an improved version that adjusts the standard diffusion by having mass settled at vertices, resulting in significantly improved bounds to O( \u221a \u03c6G log n) on the conductance of the returned cut (B, B\u0304) in time O\u0303(vol(B)\u03c6G ). Allen-Zhu, Lattanzi and Mirrokni (2013) analyzed the behavior of the same algorithm under certain well-connected conditions. The EvoCut algorithm of Andersen and Peres (2009) improved the running time of this method to O\u0303(vol(B)\u221a\n\u03c6G ). As all these methods are\nbased on spectral diffusion, their performance with respect to conductance is subject to the Cheeger barrier. Other processes have been proposed for random walks that mix faster, e.g., non-backtracking random walks (Alon et al., 2007). These too are subject to the Cheeger barrier asymptotically. Our result is the first to break this barrier in any broad setting, where classical spectral methods fail.\nMulticommodity flow based methods are able to find clusters of conductance O(\u03c6G log n) (Leighton & Rao, 1988), bypassing the limit inherent in purely spectral methods. A semidefinite programming approach, which can be viewed as combining multicommodity flow and spectral methods, yields cuts of conductance O(\u03c6G \u221a log n) (Arora et al., 2009). These algorithms are very non-local, e.g., in the sense that their running time depends on the size of the whole graph, and it is not clear that they can be meaning-\nfully localized. We do, however, use well-known flowbased ideas in our algorithm. In particular, recall that push-relabel and in general \u201cshortest-path\u201d based methods have a celebrated history in algorithms (Goldberg & Tarjan, 2014). Using levels to release capacity, however, as we do in our algorithm, is (to our knowledge) completely new."}, {"heading": "2. Capacity Releasing Diffusion", "text": "In this section, we describe our algorithm which implements a specific version of the generic CRD Process. In particular, it has some modifications for efficiency reasons, and it terminates the diffusion when it finds a bottleneck during the process. The algorithm iteratively calls a subroutine CRD-inner, which implements one CRD step.\nFor efficiency reasons, CRD-inner doesn\u2019t necessarily carry out a full CRD step, where a full CRD step means every node u has at most d(u) mass at termination. In particular, CRD-inner only makes a certain amount of \u201ceffort\u201d (which is tuned by a parameter \u03c6) to spread the mass, and if there is a bottleneck in the form of a cut that requires \u201ctoo much effort\u201d for the diffusion to get through, then CRDinner may leave excess mass on nodes, i.e.,m(v) > d(v) at termination. More specifically, given \u03c6, CRD-inner guarantees to overcome any bottleneck of conductance \u2126(\u03c6), i.e., if it doesn\u2019t carry out a full CRD step, then it returns a cut of conductance O(\u03c6) as a certificate. We will discuss CRD-inner with more detail in Section 2.2."}, {"heading": "2.1. CRD Algorithm", "text": "Given a starting node vs, the CRD algorithm (Algorithm 1) is essentially the CRD Process starting from vs, as described in Figure 1. The algorithm takes as input a parameter \u03c6, which is used to tune CRD-inner. Since CRDinner may stop short of a full CRD step due to a bottleneck, we remove any excess mass remaining on nodes after calling CRD-inner. Due to the excess removal, we may discard mass as the algorithm proceeds. In particular, as we start with 2d(vs) mass, and double the amount after every CRD step, the amount of mass after the j-th doubling is 2d(vs) \u00b7 2j if we never remove excess. When the actual amount of mass is significantly smaller than 2d(vs) \u00b7 2j , there must be a bottleneck (K, K\u0304) during the last CRD step, such that K contains a large fraction of the mass (and of the excess) and such that CRD-inner cannot push any more mass from K to K\u0304. We terminate the CRD algorithm when this happens, as the mass and, as we can show, thus the volume of K must be large, while there are few edges between K and K\u0304. Thus K is a low-conductance cluster around vs. Formally, the algorithm takes input parameters \u03c4 and t, and it terminates either when the amount of mass drops below \u03c4(2d(vs) \u00b7 2j) after iteration j, or after iteration t if the former never happens. It returns the mass on the nodes (i.e., m(\u00b7)), as well as the cut K returned by the\nAlgorithm 1 CRD Algorithm(G, vs, \u03c6, \u03c4, t)\n. Initialization: m(vs) = d(vs),m(v) = 0, \u2200v 6= vs; j = 0. . For j = 0, . . . , t . . m(v)\u2190 2m(v), \u2200v . . Assertion: m(v) \u2264 2d(v), \u2200v . . Call CRD-inner with G,m(\u00b7), \u03c6, get cut Kj (Kj empty if CRD-inner finishes full CRD step). . . m(v)\u2190 min(m(v), d(v)), \u2200v\n. . If \u2211 vm(v) \u2264 \u03c4(2d(vs) \u00b7 2 j) . . . Return m(\u00b7), and K def= Kj . Terminate. . End For . Return m(\u00b7),K def= Kt.\nlast CRD-inner call in the former termination state.\nThe running time of our CRD algorithm is local (i.e., proportional to the volume of the region it spreads mass to, rather than the volume of the entire graph). In particular, each CRD-inner call takes time linear in the amount of mass, and as the amount of mass increases geometrically before we terminate, the running time of the CRD algorithm is dominated by the last CRD-inner call."}, {"heading": "2.2. CRD Inner Procedure", "text": "Now we discuss the CRD-inner subroutine (Algorithm 2), which aims to carry out one CRD step. In particular, each node v has m(v) \u2264 2d(v) mass at the beginning, and CRD-inner tries to spread the mass so each node v has m(v) \u2264 d(v) mass at the end. Not surprisingly, as the CRD step draws intuition from flow routing, our CRD-inner can be viewed as a modification of the classic push-relabel algorithm.\nAs described in Figure 2, we maintain a label l(v) for each node v, and the net mass being pushed along each edge. Although the graph is undirected, we consider each edge e = {u, v} as two directed arcs (u, v) and (v, u), and we use m(u, v) to denote the net mass pushed from u to v (during the current CRD-inner invocation). Under this notation, we have m(u, v) = \u2212m(v, u). We denote |m(\u00b7)| def= \u2211 vm(v) as the total amount of mass, ex(v) def = max(m(v)\u2212 d(v), 0) as the amount of excess on v, and we let \u03c6 be the input parameter tuning the \u201ceffort\u201d made by CRD-inner (which will be clear shortly).\nAs noted earlier, to make CRD-inner efficient, we deviate from the generic CRD step. In particular, we make the following modifications:\n1. The label of any node can be at most h = 3 log |m(\u00b7)|/\u03c6. If v is raised to level h, but still has excess mass, CRD-inner leaves the excess on v, and won\u2019t work on v any more. Formally, v is active if l(v) < h and ex(v) > 0. We keep a list Q of all active\nnodes, and terminate CRD-inner when Q is empty. 2. In addition to capacity releasing, the net mass along\nany edge can be at mostC = 1/\u03c6. Formally, for an arc (v, u), its effective capacity is c\u0302(v, u) def= min(l(v), C), and its residual capacity is rm(v, u) def = c\u0302(v, u) \u2212 m(v, u). The arc (v, u) is eligible iff l(v) > l(u) (i.e., downhill) and rm(v, u) > 0. We only push mass along eligible arcs.\n3. We enforce m(v) \u2264 2d(v) for all v through the execution. This is assumed at the start, and we never push mass to v if that would result in m(v) > 2d(v).\nThe parameter \u03c6 in the first two modifications limits the work done by CRD-inner, and it captures how hard CRDinner will try to carry out the full CRD step (e.g., when h,C are infinitely large, CRD-inner implements the full CRD step). Given any \u03c6, CRD-inner makes enough effort by allowing nodes to have height up to h and by using the above edge capacities to overcome bottlenecks of conductance \u2126(\u03c6) during the diffusion process. If it doesn\u2019t finish the full CRD step, then it returns a cut of conductanceO(\u03c6) as certificate.\nAnother motivation of tuning with parameter \u03c6 is to keep the diffusion local. Since CRD-inner doesn\u2019t try to get through low-conductance bottlenecks, the diffusion tends to spread mass over well-connected region, instead of pushing mass out of a bottleneck. This guarantees that the work performed is linear in the volume of the returned cluster, i.e., that it is a strongly local algorithm, since only a small fraction of mass can leak out of the cluster.\nThe third modification guarantees when CRD-inner terminates with a lot of excess on nodes, the excess won\u2019t be concentrated on a few nodes, as no node can have more mass than twice its degree, and thus the cut returned must contain a large region.\nWe have the following theorem for CRD-inner. Theorem 1. Given G,m(\u00b7), and \u03c6 \u2208 (0, 1], such that |m(\u00b7)| \u2264 vol(G), and \u2200v : m(v) \u2264 2d(v) at the start, CRD-inner terminates with one of the following cases:\n1. CRD-inner finishes the full CRD step: \u2200v : m(v) \u2264 d(v).\n2. There are nodes with excess, and we can find a cut A of conductance O(\u03c6). Moreover, \u2200v \u2208 A : 2d(v) \u2265 m(v) \u2265 d(v), and \u2200v \u2208 A\u0304 : m(v) \u2264 d(v).\nThe running time is O(|m(\u00b7)| log(|m(\u00b7)|)/\u03c6).\nProof sketch. Let l(\u00b7) be the labels of nodes at termination. First note all nodes with excess must be on level h. Moreover, since we only push from a node v if it has excess (i.e., m(v) \u2265 d(v)), once a node has at least d(v)\nAlgorithm 2 CRD-inner(G,m(\u00b7),\u03c6)\n. Initialization: . . \u2200{v, u} \u2208 E, m(u, v) = m(v, u) = 0; \u2200v, l(v) = 0 . . Q = {v|m(v) > d(v)}, h = 3 log |m(\u00b7)| \u03c6 . While Q is not empty . . Let v be the lowest labeled node in Q. . . Push/Relabel(v). . . If Push/Relabel(v) pushes mass along (v, u) . . . If v becomes in-active, remove v from Q . . . If u becomes active, add u to Q . . Else If Push/Relabel(v) increases l(v) by 1 . . . If l(v) = h, remove v from Q.\nPush/Relabel(v) . If there is any eligible arc (v, u) . . Push(v, u). . Else . . Relabel(v).\nPush(v, u) . \u03c8 = min (ex(v), rm(v, u), 2d(u)\u2212m(u)) . Push \u03c8 units of mass from v to u: m(v, u)\u2190 m(v, u) + \u03c8,m(u, v)\u2190 m(u, v)\u2212 \u03c8; m(v)\u2190 m(v)\u2212 \u03c8,m(u)\u2190 m(u) + \u03c8.\nRelabel(v) . l(v)\u2190 l(v) + 1.\nmass, it always has at least d(v) mass. Note further that l(v) \u2265 1 if and only if ex(v) > 0 at some point during the process. Thus, we know the following: l(v) = h \u21d2 2d(v) \u2265 m(v) \u2265 d(v); h > l(v) \u2265 1 \u21d2 m(v) = d(v); l(v) = 0\u21d2 m(v) \u2264 d(v).\nLet Bi = {v|l(v) = i}. Since the total amount of mass |m(\u00b7)| is at most the volume of the graph, if B0 = \u2205 or Bh = \u2205, then we have case (1) of the theorem.\nOtherwise, both Bh and B0 are non-empty. Let the level cut Si = \u222ahj=iBj be the set of nodes with label at least i. We have h level cuts Sh, . . . , S1, where vol(Sh) \u2265 1, and Sj \u2286 Si if j > i. The conductance of these cuts, when we go from Sh down to S1, lower bounds how much the volume grows from Sh to S1. If all these cuts have \u2126(\u03c6) conductance, by our choice of h, the volume of S1 will be much larger than |m(\u00b7)|. This gives a contradiction, since any node v \u2208 S1 has m(v) \u2265 d(v), and we don\u2019t have enough mass. It follows that at least one of the level cuts has conductance O(\u03c6).\nAs to the running time, the graph G is given implicitly, and we only acess the list of edges of a node when it is active. Each active node v has d(v) mass, and the total amount of mass is |m(\u00b7)|, so the algorithm touches a region of volume at most |m(\u00b7)|. Thus, the running time has linear dependence on |m(\u00b7)|. Using an amortization argument one can show that the total work of the subroutine (in the worst case) is O(|m(\u00b7)|h) = O(|m(\u00b7)| log(|m(\u00b7)|)/\u03c6).\nThere are certain details in the implementation of CRDinner that we don\u2019t fully specify, such as how to check if v has any outgoing eligible arcs, and how (and why) we pick the active node with lowest label. These details are important for the running time to be efficient, but don\u2019t change the dynamics of the diffusion process. Most of these details are standard to push-relabel framework, and we include them (as well as the detailed proof of Theorem ??) in Appendix A."}, {"heading": "3. Local Graph Clustering", "text": "In this section, we provide theoretical evidence that the CRD algorithm can identify a good local cluster in a large graph if there exists one around the starting node. We define set conductance, \u03c6S(B) (or internal connectivity) of a set B \u2282 V is the minimum conductance of any cut in the induced subgraph on B.\nInformally, for a \u201cgood\u201d cluster B, any inside bottleneck should have larger conductance than \u03c6(B), and nodes in B should be more connected to other nodes inside B than to nodes outside. We capture the intuition formally as follows.\nAssumption 1. \u03c31 def = \u03c6S(B)\u03c6(B) \u2265 \u2126(1). Assumption 2. There exists \u03c32 \u2265 \u2126(1), such that any T \u2282 B with volB(T ) \u2264 volB(B)/2 satisfies\n|E(T,B \\ T )| |E(T, V \\B)| log vol(B) log 1\u03c6S(B) \u2265 \u03c32.\nFollowing prior work in local clustering, we formulate the goal as a promise problem, where we assume the existence of an unknown target good cluster B \u2282 V satisfying Assumption 1 and 2. In the context of local clustering, we also assume vol(B) \u2264 vol(G)/2. Similar to prior work, we assume the knowledge of a node vs \u2208 B, and rough estimates (i.e., within constant factor) of the value of \u03c6S(B) and vol(B). We use the CRD algorithm with vs as the starting node, \u03c6 = \u0398(\u03c6S(B)), \u03c4 = 0.5, and t = \u0398(log vol(B)d(vs) ). With the parameters we use, the algorithm will terminate due to too much excess removed, i.e., |m(\u00b7)| \u2264 \u03c4(2d(vs) \u00b7 2j) after some iteration j. The region where the diffusion spreads enough mass will be a good approximation of B.\nTheorem 2. Starting from any vs \u2208 B, with the above parameters, when the CRD algorithm terminates, if we let S = {v|m(v) \u2265 d(v)}, then we have:\n1. vol(S \\B) \u2264 O( 1\u03c3 ) \u00b7 vol(B) 2. vol(B \\ S) \u2264 O( 1\u03c3 ) \u00b7 vol(B)\nwhere \u03c3 = min(\u03c31, \u03c32) \u2265 \u2126(1), with the \u03c31, \u03c32 from Assumption 1 and 2. The running time is O(vol(B) log vol(B)\u03c6S(B) ).\nThe theorem states that the cluster recovered by the CRD algorithm has both good (degree weighted) precision and\nrecall with respect to B; and that the stronger the \u201csignal\u201d (relative to the \u201cnoise\u201d), i.e., the larger \u03c31, \u03c32, the more accurate our result approximates B.\nIf the goal is to minimize conductance, then we can run one extra iteration of the CRD algorithm after termination with a smaller value for \u03c6 (not necessarily \u0398(\u03c6S(B)) as used in previous iterations). In this case, we have the following. Theorem 3. If we run the CRD algorithm for one extra iteration, with \u03c6 \u2265 \u2126(\u03c6(B)), then CRD-inner will end with case (2) of Theorem 1. LetK be the cut returned. We have:\n1. vol(K \\B) \u2264 O(\u03c6(B)\u03c6 ) \u00b7 vol(B) 2. vol(B \\K) \u2264 O( \u03c6(B)\u03c6S(B) ) \u00b7 vol(B) 3. \u03c6(K) \u2264 O(\u03c6)\nThe running time is O(vol(B) log vol(B)\u03c6 ).\nNow we can search for the smallest \u03c6 that gives case (2) of Theorem 1, which must give a cut of conductance within an O(1) factor of the best we can hope for (i.e., \u03c6(B)). If we search with geometrically decreasing \u03c6 values, then the running time is O(vol(B) log vol(B)/\u03c6(B)).\nTheorem 2 and 3 hold due to the particular flow-based dynamics of the CRD algorithm, which tends to keep the diffusion local, without leaking mass out of a bottleneck.\nFormally, for each CRD step, we can bound the total amount of mass that starts on nodes in B, and leaves B at any point during the diffusion. We have the following lemma, a sketch of the proof of which is given. We include the full proof in Appendix B. Lemma 1. In the j-th CRD step, let Mj be the total amount of mass in B at the start, and let Lj be the amount of mass that ever leaves B during the diffusion. Then Lj \u2264 O( 1\u03c32 log vol(B) ) \u00b7Mj , when Mj \u2264 volB(B)/2; and Lj \u2264 O( 1\u03c31 ) \u00b7Mj , when Mj \u2265 volB(B)/2.\nProof sketch. We have two cases, corresponding to whether the diffusion already spread a lot of mass over B.\nIn the first case, if Mj \u2265 volB(B)/2, then we use the upper bound 1/\u03c6 that is enforced on the net mass over any edge to limit the amount of mass that can leak out. In particular Lj \u2264 O(vol(B)\u03c6(B)/\u03c6S(B)), since there are vol(B)\u03c6(B) edges from B to B\u0304, and \u03c6 = \u0398(\u03c6S(B)) in CRD-inner. As Mj \u2265 \u2126(vol(B)), we have Lj \u2264 O( 1\u03c31 ) \u00b7Mj .\nThe second case is when Mj \u2264 volB(B)/2. In this case, a combination of Assumption 2 and capacity releasing controls the leakage of mass. Intuitively, there are still many nodes in B to which the diffusion can spread mass. For the nodes in B with excess on them, when they push their excess, most of the downhill directions go to nodes inside B. As a consequence of capacity releasing, only a small fraction of mass will leak out.\nTheorem 2 and 3 follow from straightforward analysis of the total amount of leaked mass at termination. We sketch the ideas for the proof of Theorem 2, with the full proof in Appendix B.\nProof sketch. Since we use \u03c6 = \u0398(\u03c6S(B)) when we call CRD-inner, the diffusion will be able to spread mass over nodes inside B, since there is no bottleneck with conductance smaller than \u03c6S(B) in B.\nThus, before every node v in B has d(v) mass on it (in which case we say v is saturated), there will be no excess on nodes in B at the end of a CRD step. Consequently, the amount of mass in B only decreases (compared to the supposed 2d(vs) \u00b7 2j amount in the j-th CRD step) due to mass leaving B.\nAs long as the total amount Mj of mass in B at the start of a CRD step is less than volB(B)/2, the mass loss to B\u0304 is at most a O(1/(\u03c32 log vol(B))) fraction of the mass in B each CRD step. After O(log vol(B)) CRD steps, Mj reaches vol(B)B/2, and only a O(1/\u03c32) fraction of mass has left B so far. After O(1) more CRD steps, there will be enough mass to saturate all nodes in B, and each of these CRD steps looses at most a O(1/\u03c31) fraction of the mass to B\u0304. Thus we loose at most a O(1/\u03c3) fraction of mass before all nodes in B are saturated.\nOnce the diffusion has saturated all nodes in B, the amount of mass in B will be 2vol(B) at the start of every subsequent CRD step. At most vol(B)\u03c6(B)/\u03c6S(B) \u2264 O(vol(B)/\u03c3) mass can leave B, and nodes in B can hold vol(B) mass, so there must be a lot of excess (in B) at the end. Thus, the CRD algorithm will terminate in at most 2 more CRD steps, since the amount of mass almost stops growing due to excess removal.\nAt termination, the amount of mass is \u0398(vol(B)), and only O(1/\u03c3) fraction of the mass is in B\u0304. Since S = {v|m(v) \u2265 d(v)}, and the total mass outside is O(vol(B)/\u03c3), we get claim (1) of the theorem. In our simplified argument, all nodes in B have saturated sinks (i.e., vol(B \\ S) = 0) at termination. We get the small loss in claim (2) when we carry out the argument in more detail.\nThe amount of mass grows geometrically before the CRD algorithm terminates, so the running time is dominated by the last CRD step. The total amount of mass is O(vol(B)) in the last CRD step, and the running time follows Theorem 1 with \u03c6 = \u0398(\u03c6S(B)). The proof of Theorem 3 is very similar to Theorem 2, and the conductance guarantee follows directly from Theorem 1."}, {"heading": "4. Empirical Illustration", "text": "We have compared the performance of the CRD algorithm (Algorithm 1), the Andersen-Chung-Lang local spec-\ntral algorithm (ACL) (2006), and the flow-improve algorithm (FlowImp) (Andersen & Lang, 2008). Given a starting node vs and teleportation probability \u03b1, ACL is a local algorithm that computes an approximate personalized PageRank vector, which is then used to identify local structure via a sweep cut. FlowImp is a flow-based algorithm that takes as input a set of reference nodes and finds a cluster around the given reference set with small conductance value. Note that we only couple FlowImp with ACL. The reason is that, while FlowImp needs a very good reference set as input to give meaningful results in our setting, it can be used as a \u201cclean up\u201d step for spectral methods, since they give good enough output. Note also that FlowImp has running time that depends on the volume of the entire graph, as it optimizes a global objective, while our CRD algorithm takes time linear in the volume of the local region explored.\nWe compare these methods on 5 datasets, one of which is a synthetic grid graph. For the 4 real-world graphs, we use the Facebook college graphs of John Hopkins (Hop.), Rice, Simmons (Sim.), and Colgate, as introduced in Traud et al. (2012). Each graph in the Facebook dataset comes along with some features, e.g., \u201cdorm 217,\u201d and \u201cclass year 2009.\u201d We consider a set of nodes with the same feature as a \u201cground truth\u201d cluster, e.g., students of year 2009. We filter out very noisy features via some reasonable thresholds, and we run our computations on the the remaining features. We include the details of the graph and feature selection in Appendix C. The clusters of the features we use are shown in Table 1.\nWe filter bad clusters from all the ground truth clusters, by setting reasonable thresholds on volume, conductance, and gap (which is the ratio between the spectral gap of the induced graph of cluster, and the cut conductance of the cluster). Details about the selection of the ground truth clusters are in Appendix refsxn:empirical-appendix. In Table 1, we show the size and conductance of the clusters of the features used in our experiments.\nFor the synthetic experiment, we measure performance by conductance; the smaller the better. For real-world experiments, we use precision and recall. We also compare to ACLopt which \u201ccheats\u201d in the sense that it uses ground\ntruth to choose the parameter \u03b1 with best F1-score (a combination of precision and recall). A detailed discussion on parameter tuning of the algorithms is given in Appendix C.\nFor the synthetic data, we use a grid graph of size 60\u00d7 60. We add noise to the grid by randomly connecting two vertices. We illustrate the performance of the algorithms versus probability of random connection in Figure 4. The range of probabilities was chosen consistent with theory. As expected, CRD outperforms ACL in the intermediate range, and the two method\u2019s performances meet at the endpoints. One view of this is that the random connections initially adds noise to the local structure and eventually destroys it. CRD is more tolerant to this noise process.\nSee Table 2 for results for real-world data. We run algorithms starting at each vertex in a random sample of half the vertices in each cluster and report the median.\nFor clusters with good but not great conductance (e.g., Rice 2009, Colgate 2008), CDR outperforms ACL and has nearly identical performance to FlowImp (which, recall, is\na global algorithm). This is a consequence of CDR avoiding the trap of leaking mass out of the local structure, in contrast to ACL, which leaks a large fraction of mass. For clusters with great conductance, all methods perform very well; and all methods perform poorly when the conductance of the clusters gets close to 0.5. We include more detailed plots and discussion in Appendix C.\nHere again, as with the synthetic data, we see that for high conductance sets (which do not have good local structure) and very good conductance sets (which have excellent local structure), all methods perform similarly. In the intermediate range, i.e., when there are moderately good but not very good quality clusters, CDR shows distinct advantages, as suggested by the theory."}, {"heading": "Acknowledgements", "text": "SR and DW are supported by the National Science Foundation under Grant CCF-1528174 and CCF-1535989. MM and KF would like to thank the Army Research Office and the Defense Advanced Research Projects Agency for partial support of this work. MH has received funding from the European Research Council under the European Union\u2019s Seventh Framework Programme (FP/20072013)/ERC Grant Agreement no. 340506."}, {"heading": "A. CRD Inner Procedure", "text": "We first fill in the missing details in the CRD-inner subroutine (Algorithm 3).\nNote an ineligible arc (v, u) must remain ineligible until the next relabel of v, so we only need to check each arc out of v once between consecutive relabels. We use current(v) to keep track of the arcs out of v that we have checked since the last relabel of v. We always pick an active vertex v with the lowest label. Then for any eligible arc (v, u), we know m(u) \u2264 d(u), so we can push at least 1 along (v, u) (without violating m(u) \u2264 2d(u)), which is crucial to bound the total work.\nWe keep the list Q in non-decreasing order of the vertices\u2019 labels, for efficient look-up of the lowest labeled active vertex, and Add, Remove, Shift are the operations to maintain this order. Note these operations can be implemented to take O(1) work. In particular, when we add a node u to Q, it will always be the active node with lowest label, so will be put at the beginning. We only remove the first element v from Q, and when we shift a node v in Q, we know l(v) increases by exactly 1. To maintain Q, we simply need to pick two linked lists, one containing all the active nodes with non-decreasing labels, and another linked list containing one pointer for each label value, as long as there is some\nAlgorithm 3 CRD-inner(G,m(\u00b7),\u03c6)\n. Initialization: . . \u2200{v, u} \u2208 E, m(u, v) = m(v, u) = 0. . . Q = {v|m(v) > d(v)}, h = 3 log |m(\u00b7)|\u03c6 . . \u2200v, l(v) = 0, and current(v) is the first edge in v\u2019s list of incident edges. . While Q is not empty . . Let v be the lowest labeled vertex in Q. . . Push/Relabel(v). . . If Push/Relabel(v) pushes mass along (v, u) . . . If v becomes in-active, Remove(v,Q) . . . If u becomes active, Add(u,Q) . . Else If Push/Relabel(v) increases l(v) by 1 . . . If l(v) < h, Shift(v,Q) . . . Else Remove(v,Q)\nPush/Relabel(v) . Let {v, u} be current(v). . If arc (v, u) is eligible, then Push(v, u). . Else . . If {v, u} is not the last edge in v\u2019s list of edges. . . . Set current(v) be the next edge of v. . . Else (i.e., {v, u} is the last edge of v) . . . Relabel(v), and set current(v) be the first\nedge of v\u2019s list of edges.\nPush(v, u) . Assertion: rm(v, u) > 0, l(v) \u2265 l(u) + 1. ex(v) > 0,m(u) < 2d(u). . \u03c8 = min (ex(v), rm(v, u), 2d(u)\u2212m(u)) . Send \u03c8 units of mass from v to u: m(v, u)\u2190 m(v, u) + \u03c8,m(u, v)\u2190 m(u, v)\u2212 \u03c8. m(v)\u2190 m(v)\u2212 \u03c8,m(u)\u2190 m(u) + \u03c8.\nRelabel(v) . Assertion: v is active, and \u2200u \u2208 V , rm(v, u) > 0 =\u21d2 l(v) \u2264 l(u). . l(v)\u2190 l(v) + 1.\nactive node with that label, and the pointer contains the position of first such active node in Q. Maintaining this two lists together can give O(1) time Add, Remove, Shift.\nNow we proceed to prove the main theorem of CRD-inner.\nTheorem 1. Given G,m(\u00b7), and \u03c6 \u2208 (0, 1], such that |m(\u00b7)| \u2264 vol(G), and \u2200v : m(v) \u2264 2d(v) at the start, CRD-inner terminates with one of the following cases:\n1. CRD-inner finishes the full CRD step: \u2200v : m(v) \u2264 d(v).\n2. There are nodes with excess, and we can find a cut A of conductance O(\u03c6). Moreover, \u2200v \u2208 A : 2d(v) \u2265 m(v) \u2265 d(v), and \u2200v \u2208 A\u0304 : m(v) \u2264 d(v).\nThe running time is O(|m(\u00b7)| log(|m(\u00b7)|)/\u03c6).\nProof. Let l(\u00b7) be the labels of vertices at termination, and let Bi = {v|l(v) = i}. We make the following observations: l(v) = h \u21d2 2d(v) \u2265 m(v) \u2265 d(v); h > l(v) \u2265 1\u21d2 m(v) = d(v); l(v) = 0\u21d2 m(v) \u2264 d(v).\nSince |m(\u00b7)| \u2264 vol(G), if B0 = \u2205, it must be |m(\u00b7)| = vol(G), and every v has m(v) = d(v), so we get case (1). If Bh = \u2205, we also get case (1).\nIf Bh, B0 6= \u2205, let Si = \u222ahj=iBj be the set of nodes with label at least i. We have h level cuts Sh, . . . , S1, where vol(Sh) \u2265 1, and Sj \u2286 Si if j > i. We claim one of these level cuts must have conductance O(\u03c6). For any Si, we divide the edges from Si to Si into two groups: 1) edge across one level (i.e., from node in Bi to node in Bi\u22121), and 2) edges across more than one level. Let z1(i), z2(i) be the number of edges in the two groups respectively, and define \u03c6g(i) def = zg(i)/vol(Si) for g = 1, 2.\nFirst we show that, there must be a i\u2217 between h and h/2 such that \u03c61(i\u2217) \u2264 \u03c6. By contradiction, if \u03c61(i) > \u03c6 for all i = h, . . . , h/2, since vol(Si\u22121) \u2265 vol(Si)(1 + \u03c61(Si)), we get vol(Sh/2) \u2265 (1 + \u03c6)h/2vol(Sh). With h = 3 log |m(\u00b7)|/\u03c6, we have vol(Sh/2) \u2265 \u2126(|m(\u00b7)|3/2), and since nodes in Sh/2 are all saturated, we get a contradiction since we must have vol(Sh/2) \u2264 |m(\u00b7)|.\nNow we consider any edge {v, u} counted in z2(i\u2217) (i.e., v \u2208 Si\u2217 , u \u2208 Si\u2217 , l(v)\u2212 l(u) \u2265 2). Since i\u2217 \u2265 h/2 > 1/\u03c6, c\u0302(v, u) = 1/\u03c6. l(v)\u2212l(u) > 2 suggests rm(v, u) = 0, thus m(v, u) = 1/\u03c6 (i.e., 1/\u03c6 mass pushed out of Si\u2217 along each edge counted in z2(i\u2217)). Each edge counted in z1(i\u2217) can have at most 1/\u03c6 mass pushed into Si\u2217 , and at most 2vol(Si\u2217) mass can start in Si\u2217 , then we know\nz2(i \u2217)/\u03c6 \u2264 z1(i\u2217)/\u03c6+ 2vol(Si\u2217)\nWe will let A be Si\u2217 , and we have\n\u03c6(A) = z1(i\n\u2217) + z2(i \u2217)\nvol(Si\u2217) \u2264 4\u03c6 = O(\u03c6)\nHere we assume Si\u2217 is the smaller side of the cut to compute the conductance. If this is not the case, i.e. vol(Si\u2217) > vol(G)/2, we just carry out the same argument as above, but run the region growing argument from level h/4 up to level h/2, and get a low conductance cut, and still let A to be the side containing Sh. The additional properties of elements in A follows from Sh \u2286 A \u2286 Sh/4.\nNow we proceed to the running time. The initialization takes O(|m(\u00b7)|). Subsequently, each iteration takes O(1) work. We will first attribute the work in each iteration to either a push or a relabel. Then we will charge the work on pushes and relabels to the absorbed mass, such that each unit of absorbed mass gets charged O(h) work. Recall the absorbed mass at v are the first up to d(v) mass starting at or pushed into v, and these mass never leave v, as the algorithm only pushes excess mass. This will prove the result, as there are at most |m(\u00b7)| units of (absorbed) mass in total.\nIn each iteration of Unit-Flow, the algorithm picks a lowest labeled active node v. If Push/Relabel(v) ends with a push of \u03c8 mass, we charge O(\u03c8) to that push operation. Since \u03c8 \u2265 1, we charged the push enough to cover the work in that iteration. If the call to Push/Relabel(v) doesn\u2019t push, we charge the O(1) work of the iteration to the next relabel of v (or the last relabel if there is no next relabel). The latter can happen at most d(v) times between consecutive relabels of v, so each relabel of v is charged O(d(v)) work.\nWe now charge the work on pushes and relabels to the absorbed mass. Note each time we relabel v, there are d(v) units of absorbed mass at v, so we charge theO(d(v)) work on the relabel to the absorbed mass, and each unit gets charged O(1). There is at most h relabels of v, so each unit of absorbed mass is charged O(h) in total by all the relabels.\nFor the work on pushes, we consider the potential function \u039b = \u2211 v ex(v)l(v). \u039b is always non-negative, and as we only push excess mass downhill, each push of \u03c8 units of mass decrease \u039b by at least \u03c8, so we can charge the work on pushes to the increment of \u039b. It only increases at relabel. When we relabel v, \u039b is increased by ex(v). Since ex(v) \u2264 d(v), we can charge O(1) to each unit of absorbed mass at v to cover \u039b\u2019s increment. In total we can charge all pushes (via \u039b) to absorbed mass, and each unit is charged with O(h).\nIf we need to compute the cut A in case (2), the running time is O(vol(S1)), which is O(|m(\u00b7)|)."}, {"heading": "B. Local Clustering", "text": "Recall we assume B to satisfy the following conditions.\nAssumption 1. \u03c31 def = \u03c6S(B)\u03c6(B) \u2265 \u2126(1). Assumption 2. There exists \u03c32 \u2265 \u2126(1), such that any T \u2282 B with volB(T ) \u2264 volB(B)/2 satisfies\n|E(T,B \\ T )| |E(T, V \\B)| log vol(B) log 1\u03c6S(B) \u2265 \u03c32.\nNow we proceed to prove the main lemma.\nLemma 1. In the j-th CRD step, let Mj be the total amount of mass in B at the start, and let Lj be the amount of mass that ever leaves B during the diffusion. Then Lj \u2264 O( 1\u03c32 log vol(B) ) \u00b7Mj , when Mj \u2264 volB(B)/2; and Lj \u2264 O( 1\u03c31 ) \u00b7Mj , when Mj \u2265 volB(B)/2. Proof. For simplicity, we assume once a unit of mass leaves B, it is never routed back. Intuitively, mass coming back into B should only help the algorithm, and indeed the results don\u2019t change without this assumption. We denote |Mj(S)| as the amount of mass on nodes in a set S at the start of the CRD-inner call.\nWe have two cases, corresponding to whether the diffusion already spread a lot of mass over B. If Mj \u2265 volB(B)/2, we use the upperbound 1/\u03c6 that is enforced on the net mass over any edge to limit the amount of mass that can leak out. In particularLj \u2264 O(vol(B)\u03c6(B)/\u03c6S(B)), since there are vol(B)\u03c6(B) edges from B to B\u0304, and \u03c6 = \u0398(\u03c6S(B)) in CRD-inner. As Mj \u2265 \u2126(vol(B)), we have Lj \u2264 O( 1\u03c31 ) \u00b7 Mj .\nThe second case is when Mj \u2264 volB(B)/2. In this case, a combination of Assumption 2 and capacity releasing controls the leakage of mass. Intuitively, there are still many nodes in B that the diffusion can spread mass to. For the nodes in B with excess on them, when they push their excess, most of the downhill directions go to nodes inside B. As a consequence of capacity releasing, only a small fraction of mass will leak out.\nIn particular, let l(\u00b7) be the labels on nodes when CRDinner finishes, we consider Bi = {v \u2208 B|l(v) = i} and the level cuts Si = {v \u2208 B|l(v) \u2265 i} for i = h, . . . , 1. As Mj \u2264 volB(B)/2, we know vol(Sh) \u2264 vol(Sh\u22121) \u2264 . . . \u2264 vol(S1) \u2264 volB(B)/2. In this case, we can use Assumption 2 on all level cuts Sh, . . . , S1. Moreover, for a node v \u2208 Bi, the \u201d\u2018effective\u201d\u2019 capacity of an arc from v to B\u0304 is min(i, 1/\u03c6). Formally, we can bound Lj by the total (effective) outgoing capacity, which is\nh\u2211 i=1 |E(Bi, B\u0304)| \u00b7min(i, 1 \u03c6 ) = 1 \u03c6\u2211 i=1 |E(Si, B\u0304)| (1)\nwhere h is the bound on labels used in unit flow.\nWe design a charging scheme to charge the above quantity (the right hand side) to the mass in \u2206j(B), such that each unit of mass is charged O(1/(\u03c32 log vol(B))). It follows that Lj \u2264 O( 1\u03c32 log vol(B) ) \u00b7 |\u2206j(B)|.\nRecall that, |E(Si, B\u0304)| \u2264 |E(Si,B\\Si)|\u03c32 log vol(B) log(1/\u03c6) from Assumption 2. We divide edges in E(Si, B \\ Si) into two groups: : 1) edges across one level, and 2) edges across more than one level. Let z1(i), z2(i) be the number of edges in the two groups respectively.\nIf z1(i) \u2265 |E(Si, B \\ Si)|/3, we charge 3/(\u03c32 log vol(B) log(1/\u03c6)) to each edge in group 1. These edges in turn transfer the charge to the absorbed mass at their endpoints in Bi. Since each node v in level i \u2265 1 has d(v) absorbed mass, each unit of absorbed mass is charged O(1/(\u03c32 log vol(B) log(1/\u03c6))). Note that the group 1 edges of different level i\u2019s are disjoint, so each unit of absorbed mass will only be charged once this way.\nIf z1(i) \u2264 |E(Si, B \\ Si)|/3, we know z2(i) \u2212 z1(i) \u2265 |E(Si, B \\ Si)|/3. Group 2 edges in total send at least (i \u2212 1)z2(i) mass from Si to B \\ Si, and at most (i \u2212 1)z1(i) of these mass are pushed into Si by group 1 edges. Thus, there are at least (i \u2212 1)|E(Si, B \\ Si)|/3 mass that start in Si, and are absorbed by nodes at level below i (possibly outside B). In particular, this suggests |Mj(Si)| \u2265 (i \u2212 1)|E(Si, B \\ Si)|/3, and we split the total charge |E(Si, B\u0304)| evenly on these mass, so each unit of mass is charged O(1/(i\u03c32 log vol(B) log(1/\u03c6))). Since we sum from i = 1/\u03c6 to 1 in (RHS of) Eqn (1), we charge some mass multiple times (as Si\u2019s not disjoint), but we can bound the total charge by \u22111/\u03c6 i=1 1 i \u00b7 O(1/(\u03c32 log vol(B) log(1/\u03c6))), which is O(1/(\u03c32 log vol(B))). This completes the proof.\nNow we fill in some details for the proof of Theorem 2.\nTheorem 2. Starting from any vs \u2208 B, with the above parameters, when the CRD algorithm terminates, if we let S = {v|m(v) \u2265 d(v)}, then we have:\n1. vol(S \\B) \u2264 O( 1\u03c3 ) \u00b7 vol(B) 2. vol(B \\ S) \u2264 O( 1\u03c3 ) \u00b7 vol(B)\nwhere \u03c3 = min(\u03c31, \u03c32) \u2265 \u2126(1), with the \u03c31, \u03c32 from Assumption 1 and 2. The running time is O(vol(B) log vol(B)\u03c6S(B) ).\nProof. Since we use \u03c6 = \u0398(\u03c6S(B)) when we call CRDinner, we are guaranteed that CRD-inner will make enough effort to get through bottleneck of conductance \u2126(\u03c6S(B)), so the diffusion should be able to spread mass completely over B, since any cut inside B has conductance at least \u03c6S(B). Thus, there should be no excess remaining on nodes in B at the end of CRD-inner, unless every node v in B has m(v) mass already (i.e., all nodes in B are saturated).\nFormally, consider the proof of Theorem 1, but with everything with respect to the induced graph of B. If there is no excess pushed into B from outside, we can use the exact arguments in the proof of Theorem 1 to show that either there is no excess on any node in B at the end of the CRD-inner call, or there is a cut of conductance O(\u03c6), or all nodes in B are saturated. By assumption, the second case is not possible, so we won\u2019t remove excess supply between CRD-inner calls before all nodes in B are saturated. If we consider supply pushed back into B from outside, we can show that the amount of excess on nodes in B at the end is at most the amount of mass pushed into B. Since we already counted the mass leaving B as lost, we don\u2019t need to worry about them again when we remove the mass.\nConsequently, before all nodes in B are saturated, the amount of mass in B only decreases (compared to the supposed 2d(vs) \u00b7 2j amount in iteration j) due to mass leaving B, which we can bound ( Lemma 1) by a O(1/(\u03c3 log vol(B))) fraction of the mass in B each iteration. We will have enough mass to spread over all nodes in B in O(log vol(B)) iterations, so we lose O(1/\u03c3) fraction of mass before all nodes in B are saturated.\nOnce the diffusion has saturated all nodes inB, the amount of mass in B will be 2vol(B) at the start of every subsequent CRD-inner call. At most vol(B)\u03c6(B)/\u03c6S(B) \u2264 O(vol(B)/\u03c3) mass can leave B, and nodes in B can hold vol(B) mass, so there must be a lot of excess (in B) at the end. Thus, the CRD algorithm will terminate in at most 2 more iterations after all nodes in B are saturated, since the amount of mass almost stops growing due to excess removal.\nAt termination, the amount of mass is \u0398(vol(B)), and only O(1/\u03c3) fraction of the mass is in B\u0304. Since S = {v|m(v) \u2265 d(v)}, and the total mass outside is O(vol(B)/\u03c3), we get claim (1) of the theorem. In our simplified argument, all nodes in B have saturated sinks (i.e., vol(B \\ S) = 0) at termination. We get the small loss in claim (2) when we carry out the argument more rigorously.\nThe amount of mass grows geometrically before the CRD algorithm terminates, so the running time is dominated by the last CRD-inner call. The total amount of mass is O(vol(B)) in the last iteration, and the running time follows Theorem 1 with \u03c6 = \u0398(\u03c6S(B))"}, {"heading": "C. Empirical Set-up and Results", "text": "C.1. Datasets\nWe chose the graphs of John Hopkins, Rice, Simmons and Colgate universities/colleges. The actual IDs of the graphs in Facebook100 dataset are Johns Hopkins55, Rice31, Simmons81 and Colgate88. These graphs are anonymized\nFacebook graphs on a particular day in September 2005 for student social networks. The graphs are unweighted and they represent \u201cfriendship ties\u201d. The data form a subset of the Facebook100 dataset from (Traud et al., 2012). We chose these 4 graphs out of 100 due to their large assortativity value in the first column of Table A.2 in (Traud et al., 2012), where the data were first introduced and analyzed. Details about the graphs are shown is Table 3.\nEach graph in the Facebook dataset comes along with 6 features, i.e., second major, high school, gender, dorm, major index and year. We construct \u201cground truth\u201d clusters by using the features for each node. In particular, we consider nodes with the same value of a feature to be a cluster, e.g., students of year 2009. We loop over all possible clusters and consider as ground truth the ones that have volume larger than 1000, conductance smaller than 0.5 and gap larger than 0.5. Filtering results in moderate scale clusters for which the internal volume is at least twice as much as the volume of the edges that leave the cluster. Additionally, gap at least 0.5 means that the smallest nonzero eigenvalue of the normalized Laplacian of the subgraph defined by the cluster is at least twice larger than the conductance of the cluster in the whole graph. The clusters per graph that satisfy the latter constraints are shown in Table 1.\nWe resort to social networks as our motivation is to test our algorithm against \u201dnoisy\u201d clusters, and for social networks it is well known that reliable ground truth is only weakly related to good conductance clusters, and thus certain commonly-used notions of ground truth would not provide falsifiable insight into the method (Jeub et al., 2015). As analyzed in the original paper that introduced these datasets (Traud et al., 2012), only year and dorm features give non-trivial \u201dassortativity coefficients\u201d, which is a \u201dlocal measure of homophily\u201d. This agrees with the ground truth clusters we find, which also correspond to features of year and dorm.\nC.2. Performance criteria and parameter tuning\nFor real-world Facebook graphs since we calculate the ground truth clusters in Table 1 then we measure performance by calculating precision and recall for the output clusters of the algorithms.\nWe set the parameters of CRD to \u03c6 = 1/3 for all experiments. At each iteration we use sweep cut on the labels\nreturned by the CRD-inner subroutine to find a cut of small conductance, and over all iterations of CRD we return the cluster with the lowest conductance.\nACL has two parameters, the teleportation parameter \u03b1 and a tolerance parameter . Ideally the former should be set according to the reciprocal of the mixing time of a a random walk within the target cluster, which is equal to the smallest nonzero eigenvalue of the normalized Laplacian for the subgraph that corresponds to the target cluster. Let us denote the eigenvalue with \u03bb. In our case the target cluster is a ground truth cluster from Table 1. We use this information to set parameter \u03b1. In particular, for each node in the clusters in Table 1 we run ACL 4 times where \u03b1 is set based on a range of values in [\u03bb/2, 2\u03bb] with a step of (2\u03bb\u2212 \u03bb/2)/4. The tolerance parameter is set to 10\u22127 for all experiments in order to guarantee accurate solutions for the PageRank linear system. For each parameter setting we use sweep cut to find a cluster of low conductance, and over all parameter settings we return the cluster with the lowest conductance value as an output of ACL.\nFor real-world experiments we show results for ACLopt. In this version of ACL, for each parameter setting of \u03b1 we use sweep cut algorithm to obtain a low conductance cluster and then we compute its precision and recall. Over all parameter settings we keep the cluster with the best F1score; a combination of precision and recall. This is an extra level of supervision for the selection of the teleportation parameter \u03b1, which is not possible in practice since it requires ground truth information. However, the performance of ACLopt demonstrates the performance of ACL in case that we could make optimal selection of parameter \u03b1 among the given range of parameters (which also includes ground truth information) for the precision and recall criteria.\nFinally, we set the reference set of FlowI to be the output set of best conductance of ACL out of its 4 runs for each node. By this we aim to obtain an improved cluster to ACL in terms of conductance. Note that FlowI is a global algorithm, which means that it accesses the information from the whole graph compared to CRD and ACL which are local algorithms.\nC.3. Real-world experiments\nFor clusters in Table 1 we sample uniformly at random half of their nodes. For each node we run CRD, ACL and ACL+FlowI. We report the results using box plots, which graphically summarizes groups of numerical data using quartiles. In these plot the orange line is the median, the blue box below the median is the first quartile, the blue box above the median is the third quartile, the extended long lines below and above the box are the maximum and minimum values and the circles are outliers.\nThe results for John Hopkins university are shown in Figure 5. Notice in this figure that CRD performs better than ACL and ACLopt, which both use ground truth information, see parameter tuning in Subsection C.2. CRD performs similarly to ACL+FlowI, where FlowI is a global algorithm, but CRD is a local algorithm. Overall all methods have large medians for this graph because the clusters with dorm 217 and year 2009 are clusters with low conductance compared to the ones in other universities/colleges which we will discuss in the remaining experiments of this subsection.\nThe results for Rice university are shown in Figure 6. Notice that both clusters of dorm 203 and year 2009 for Rice university are worse in terms of conductance compared to the clusters of John Hopkins university. Therefore the performance of the methods is decreased. For the cluster of dorm 203 with conductance 0.46 CRD has larger median than ACL, ACLopt and ACL+Flow in terms of precision. The latter methods obtain larger median for recall, but this is because ACL leaks lots of probability mass outside of the ground truth cluster since as indicated by its large conductance value many nodes in this cluster are connected externally. For cluster of year 2009 CRD outperforms ACL, which fails to recover the cluster because it leaks mass outside the cluster, FlowI corrects the problem and locates the correct cluster at the expense of touching the whole graph. Notice that all methods have a significant amount of variance and outliers, which is also explained by the large conductance values of the clusters.\nThe results for Simmons college are shown in Figure 7. Notice that Simmons college in Table 1 has two clusters, one with poor conductance 0.47 for students of year 2007 and one low conductance 0.1 for students of year 2009. The former with conductance 0.47 means that the internal volume is nearly half the volume of the outgoing edges. This has a strong implication in the performance of CRD, ACL and ACLopt which get median precision about 0.5. This happens because the methods push half of the flow (CRD) and half of the probability mass (ACL) outside the ground truth cluster, which results in median precision 0.5. ACL achieves about 20% more (median) recall than CRD but this is because ACL touched more nodes than CRD during execution of the algorithm. Notice that ACL+FlowI fails for the cluster of year 2007, this is because FlowI is a global algorithm, hence it finds a cluster that has low conductance but it is not the ground truth cluster. The second cluster of year 2009 has low conductance hence all methods have large median performance with CRD being slightly better than ACL, ACLopt and ACL+FlowI.\nThe results for Colgate university are shown in Figure 8. The interesting property of the clusters in Table 1 for Colgate university is that their conductance varies from low 0.1 to large 0.48. Therefore in Figure 8 we see a smooth tran-\nsition of performance for all methods from poor to good performance. In particular, for the cluster of year 2006 the conductance is 0.48 and CRD, ACL and ACLopt perform poorly by having median precision about 50%, recall is slightly better for ACL but this is because we allow it touch a bigger part of the graph. ACL+FlowI fails to locate the cluster. For the cluster of year 2007 the conductance is 0.41 and the performance of CRD, ACL and ACLopt is increased with CRD having larger (median) precision and ACL having larger (median) recall as in the previous cluster. Conductance is smaller for the cluster of year 2008, for which we observe substantially improved performance for CRD with large median precision and recall. On the contrary, ACL, ACLopt and ACL+FlowI have nearly 30% less median precision in the best case and similar median recall, but only because a large amount of probability mass is leaked and a big part of the graph is touched which includes the ground truth cluster. Finally, the cluster of year 2009 has low conductance 0.11 and all methods have good performance for precision and recall.\nFigure 7. Precision and recall results for Simmons college\nFigure 8. Precision and recall results for Colgate university"}], "references": [{"title": "Non-backtracking random walks mix faster", "author": ["Alon", "Noga", "Benjamini", "Itai", "Lubetzky", "Eyal", "Sodin", "Sasha"], "venue": "Communications in Contemporary Mathematics,", "citeRegEx": "Alon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2007}, {"title": "An algorithm for improving graph partitions", "author": ["R. Andersen", "K. Lang"], "venue": "SODA", "citeRegEx": "Andersen and Lang,? \\Q2008\\E", "shortCiteRegEx": "Andersen and Lang", "year": 2008}, {"title": "Finding sparse cuts locally using evolving sets", "author": ["Andersen", "Reid", "Peres", "Yuval"], "venue": "STOC", "citeRegEx": "Andersen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andersen et al\\.", "year": 2009}, {"title": "Local graph partitioning using PageRank vectors", "author": ["Andersen", "Reid", "Chung", "Fan", "Lang", "Kevin"], "venue": "FOCS", "citeRegEx": "Andersen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Andersen et al\\.", "year": 2006}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["Arora", "Sanjeev", "Rao", "Satish", "Vazirani", "Umesh"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Arora et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2009}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation,", "citeRegEx": "Belkin and Niyogi,? \\Q2003\\E", "shortCiteRegEx": "Belkin and Niyogi", "year": 2003}, {"title": "A lower bound for the smallest eigenvalue of the Laplacian", "author": ["Cheeger", "Jeff"], "venue": "In Proceedings of the Princeton conference in honor of Professor S. Bochner,", "citeRegEx": "Cheeger and Jeff.,? \\Q1969\\E", "shortCiteRegEx": "Cheeger and Jeff.", "year": 1969}, {"title": "Lower bounds for the partitioning of graphs", "author": ["Donath", "William E", "Hoffman", "Alan J"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Donath et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Donath et al\\.", "year": 1973}, {"title": "PageRank beyond the web", "author": ["D.F. Gleich"], "venue": "SIAM Review,", "citeRegEx": "Gleich,? \\Q2015\\E", "shortCiteRegEx": "Gleich", "year": 2015}, {"title": "A new approach to the maximum-flow problem", "author": ["Goldberg", "Andrew V", "Tarjan", "Robert E"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goldberg et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 1988}, {"title": "Efficient maximum flow algorithms", "author": ["Goldberg", "Andrew V", "Tarjan", "Robert Endre"], "venue": "Commun. ACM,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "On the quality of spectral separators", "author": ["S. Guattery", "G.L. Miller"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Guattery and Miller,? \\Q1998\\E", "shortCiteRegEx": "Guattery and Miller", "year": 1998}, {"title": "Scaling personalized web search", "author": ["Jeh", "Glen", "Widom", "Jennifer"], "venue": "In Proceedings of the 12th international conference on World Wide Web,", "citeRegEx": "Jeh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jeh et al\\.", "year": 2003}, {"title": "Think locally, act locally: Detection of small, medium-sized, and large communities in large networks", "author": ["L.G.S. Jeub", "P. Balachandran", "M.A. Porter", "P.J. Mucha", "M.W. Mahoney"], "venue": "Physical Review E,", "citeRegEx": "Jeub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jeub et al\\.", "year": 2015}, {"title": "An approximate maxflow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms", "author": ["Leighton", "Tom", "Rao", "Satish"], "venue": "In FOCS,", "citeRegEx": "Leighton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Leighton et al\\.", "year": 1988}, {"title": "Community structure in large networks: Natural cluster sizes and the absence of large well-defined clusters", "author": ["J. Leskovec", "K.J. Lang", "A. Dasgupta", "M.W. Mahoney"], "venue": "Internet Mathematics,", "citeRegEx": "Leskovec et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2009}, {"title": "A local spectral method for graphs: with applications to improving graph partitions and exploring data graphs locally", "author": ["M.W. Mahoney", "L. Orecchia", "N.K. Vishnoi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mahoney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahoney et al\\.", "year": 2012}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Proceedings of the 15th Annual Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Approximating the exponential, the Lanczos method and an \u00d5(m)-time spectral algorithm for balanced separator", "author": ["Orecchia", "Lorenzo", "Sachdeva", "Sushant", "Vishnoi", "Nisheeth K"], "venue": null, "citeRegEx": "Orecchia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Orecchia et al\\.", "year": 2012}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["Page", "Lawrence", "Brin", "Sergey", "Motwani", "Rajeev", "Winograd", "Terry"], "venue": "Technical report, Stanford InfoLab,", "citeRegEx": "Page et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Page et al\\.", "year": 1999}, {"title": "A spectral clustering approach", "author": ["S. White", "P. Smyth"], "venue": "bernetics,", "citeRegEx": "White and Smyth,? \\Q2006\\E", "shortCiteRegEx": "White and Smyth", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": ", 2015), so-called manifold learning (Belkin & Niyogi, 2003; Mahoney et al., 2012), and PageRank-based spectral", "startOffset": 37, "endOffset": 82}, {"referenceID": 19, "context": "ranking in web ranking (Page et al., 1999; Gleich, 2015).", "startOffset": 23, "endOffset": 56}, {"referenceID": 8, "context": "ranking in web ranking (Page et al., 1999; Gleich, 2015).", "startOffset": 23, "endOffset": 56}, {"referenceID": 3, "context": "Particularly relevant for our results are local/personalized versions of PageRank (Jeh & Widom, 2003) and local/distributed versions of spectral clustering (Spielman & Teng, 2004; Andersen et al., 2006; Andersen & Peres, 2009).", "startOffset": 156, "endOffset": 226}, {"referenceID": 15, "context": ", 2016); and they have been used to characterize the clustering and community structure in a wide range of social and information networks (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 139, "endOffset": 181}, {"referenceID": 13, "context": ", 2016); and they have been used to characterize the clustering and community structure in a wide range of social and information networks (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 139, "endOffset": 181}, {"referenceID": 15, "context": "In practice, this is seen by the extreme sensitivity of spectral methods to high-degree nodes and other structural heterogeneities in real-world graphs constructed from very noisy data (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 185, "endOffset": 227}, {"referenceID": 13, "context": "In practice, this is seen by the extreme sensitivity of spectral methods to high-degree nodes and other structural heterogeneities in real-world graphs constructed from very noisy data (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 185, "endOffset": 227}, {"referenceID": 13, "context": "Empirically, this is well-known to be a particular problem when there are moderately good\u2014but not very good\u2014clusters in the data, a situation that is all too common in machine learning and data analysis applications (Jeub et al., 2015).", "startOffset": 216, "endOffset": 235}, {"referenceID": 2, "context": "The relation between the generic CRD process and the CRD algorithm for local graph clustering is analogous to the relation between local random walks and a local spectral graph partitioning algorithm such as that of Andersen et al. (2006). The generic CRD inner process (Figure 2) implements a modification of the classic \u201cpush-relabel\u201d algorithm (Goldberg & Tarjan, 1988; 2014) for routing a source-sink flow.", "startOffset": 216, "endOffset": 239}, {"referenceID": 15, "context": "While idealized, such an example is not completely unrealistic (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 63, "endOffset": 105}, {"referenceID": 13, "context": "While idealized, such an example is not completely unrealistic (Leskovec et al., 2009; Jeub et al., 2015).", "startOffset": 63, "endOffset": 105}, {"referenceID": 15, "context": "In particular, while graphs that have upward-sloping NCPs (Network Community Profiles) have good small clusters (Leskovec et al., 2009; Jeub et al., 2015), denser social networks with flat NCPs do not have any very-good conductance clusters of any size.", "startOffset": 112, "endOffset": 154}, {"referenceID": 13, "context": "In particular, while graphs that have upward-sloping NCPs (Network Community Profiles) have good small clusters (Leskovec et al., 2009; Jeub et al., 2015), denser social networks with flat NCPs do not have any very-good conductance clusters of any size.", "startOffset": 112, "endOffset": 154}, {"referenceID": 13, "context": "They do, however, often have moderately-good clusters, but these are very difficult for spectral methods to identify (Jeub et al., 2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 18, "context": ", Orecchia et al. (2012) for more details on this.", "startOffset": 2, "endOffset": 25}, {"referenceID": 0, "context": ", non-backtracking random walks (Alon et al., 2007).", "startOffset": 32, "endOffset": 51}, {"referenceID": 4, "context": "A semidefinite programming approach, which can be viewed as combining multicommodity flow and spectral methods, yields cuts of conductance O(\u03c6G \u221a log n) (Arora et al., 2009).", "startOffset": 153, "endOffset": 173}], "year": 2017, "abstractText": "Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics. Because they spread mass agnostically at each step in an iterative manner, they can sometimes spread mass \u201ctoo aggressively,\u201d thereby failing to find the \u201cright\u201d clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical spectral diffusion process. As an application, we use our CRD Process to develop an improved local algorithm for graph clustering. Our local graph clustering method can find local clusters in a model of clustering where one begins the CRD Process in a cluster whose vertices are connected better internally than externally by an O(log n) factor, where n is the number of nodes in the cluster. Thus, our CRD Process is the first local graph clustering algorithm that is not subject to the well-known quadratic Cheeger barrier. Our result requires a certain smoothness condition, which we expect to be an artifact of our analysis. Our empirical evaluation demonstrates improved results, in particular for realistic social graphs where there are moderately good\u2014but not very good\u2014clusters.", "creator": "LaTeX with hyperref package"}}}