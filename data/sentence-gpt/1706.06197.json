{"id": "1706.06197", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2017", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting", "abstract": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. If the model parameter is a gradient, the gradient is computed to update the model parameters. For example, it will be used to adjust the size of the slope of the gradient.\n\n\n\n\n\n\n\nA typical gradient model is presented here with a linear slope and the slope is a normal direction and the slope is a flat slope with a slope. A horizontal axis is computed as usual. It is shown that the gradient has a linear slope (with the gradient's slope at the end). This linear gradient, as the gradient changes from a gradient to a height, is the most effective approach to learning and it is not the easiest way to change that orientation.\n\n\nIn a similar way, the gradient is computed as usual. In the forward propagation, only a small subset of the full gradient is computed to update the model parameters. For example, it is shown that the gradient has a linear slope (with the gradient's slope at the end). This linear gradient, as the gradient changes from a gradient to a height, is the most effective approach to learning and it is not the easiest way to change that orientation.\nThe forward propagation is computed as usual. In the forward propagation, only a small subset of the full gradient is computed to update the model parameters. For example, it is shown that the gradient has a linear slope (with the gradient's slope at the end). This linear gradient, as the gradient changes from a gradient to a height, is the most effective approach to learning and it is not the easiest way to change that orientation.\nIn this view, the direction of the gradient is computed as usual. In the forward propagation, only a small subset of the full gradient is computed to update the model parameters. For example, it is shown that the gradient has a linear slope (with the gradient's slope at the end). This linear gradient, as the gradient changes from a gradient to a height, is the most effective approach to learning and it is not the easiest way to change that orientation.\nThe forward propagation is computed as usual. In the forward propagation, only a small subset of the full gradient is computed to update the model parameters. For example, it is shown that the gradient has a linear slope (with the gradient's slope at the end). This linear gradient, as the gradient changes from a gradient", "histories": [["v1", "Mon, 19 Jun 2017 22:36:33 GMT  (153kb)", "http://arxiv.org/abs/1706.06197v1", "ICML 2017"], ["v2", "Wed, 5 Jul 2017 01:34:50 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v2", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v3", "Mon, 30 Oct 2017 09:48:41 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v3", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"], ["v4", "Tue, 31 Oct 2017 02:04:52 GMT  (158kb)", "http://arxiv.org/abs/1706.06197v4", "Accepted by The 34th International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.CV", "authors": ["xu sun", "xuancheng ren", "shuming ma", "houfeng wang"], "accepted": true, "id": "1706.06197"}, "pdf": {"name": "1706.06197.pdf", "metadata": {"source": "META", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting", "authors": ["Xu Sun", "Xuancheng Ren", "Shuming Ma", "Houfeng Wang"], "emails": ["<xusun@pku.edu.cn>."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n06 19\n7v 1\n[ cs\n.L G\n] 1\n9 Ju\nn 20\n17\nneural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given."}, {"heading": "1. Introduction", "text": "Neural network learning is typically slow, where back propagation usually dominates the computational cost during the learning process. Back propagation entails a high computational cost because it needs to compute full gradients and update all model parameters in each learning step. It is not uncommon for a neural network to have a massive number of model parameters.\nIn this study, we propose a minimal effort back propagation method, which we call meProp, for neural network learning. The idea is that we compute only a very small but critical portion of the gradient information, and update only the corresponding minimal portion of the parameters in each learning step. This leads to sparsified gradients, such that only highly relevant parameters are updated and other pa-\n1School of Electronics Engineering and Computer Science, Peking University, China 2MOE Key Laboratory of Computational Linguistics, Peking University, China. Correspondence to: Xu Sun <xusun@pku.edu.cn>.\nrameters stay untouched. The sparsified back propagation leads to a linear reduction in the computational cost.\nTo realize our approach, we need to answer two questions. The first question is how to find the highly relevant subset of the parameters from the current sample in stochastic learning. We propose a top-k search method to find the most important parameters. Interestingly, experimental results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations. The proposed method is general-purpose and it is independent of specific models and specific optimizers (e.g., Adam and AdaGrad).\nThe second question is whether or not this minimal effort back propagation strategy will hurt the accuracy of the trained models. We show that our strategy does not degrade the accuracy of the trained model, even when a very small portion of the parameters is updated. More interestingly, our experimental results reveal that our strategy actually improves the model accuracy in most cases. Based on our experiments, we find that it is probably because the minimal effort update does not modify weakly relevant parameters in each update, which makes overfitting less likely, similar to the dropout effect.\nThe contributions of this work are as follows:\n\u2022 We propose a sparsified back propagation technique for neural network learning, in which only a small\nsubset of the full gradient is computed to update the model parameters. Experimental results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations.\n\u2022 Surprisingly, our experimental results reveal that the accuracy of the resulting models is actually improved,\nrather than degraded. We demonstrate this effect by conducting experiments on different deep learning models (LSTM andMLP), various optimizationmethods (Adam and AdaGrad), and diverse tasks (natural language processing and image recognition)."}, {"heading": "2. Proposed Method", "text": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. During back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are \u201cquantized\u201d so that only the top-k components in terms of magnitude are kept. We first present the proposed method and then describe the implementation details."}, {"heading": "2.1. meProp", "text": "Forward propagation of neural network models, including feedforward neural networks, RNN, LSTM, consists of linear transformations and non-linear transformations. For simplicity, we take a computation unit with one linear transformation and one non-linear transformation as an example:\ny = Wx (1)\nz = \u03c3(y) (2)\nwhere W \u2208 Rn\u00d7m, x \u2208 Rm, y \u2208 Rn, z \u2208 Rn, m is the dimension of the input vector, n is the dimension of the output vector, and \u03c3 is a non-linear function (e.g., relu, tanh, and sigmoid). During back propagation, we need to compute the gradient of the parameter matrix W and the input vector x:\n\u2202z\n\u2202Wij = \u03c3\n\u2032 ix T j (1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m) (3)\n\u2202z \u2202xi =\n\u2211\nj\nWTij\u03c3 \u2032 j (1 \u2264 j \u2264 n, 1 \u2264 i \u2264 m) (4)\nwhere \u03c3 \u2032 i \u2208 R n means \u2202zi\n\u2202yi . We can see that the computa-\ntional cost of back propagation is directly proportional to the dimension of output vector n.\nThe proposedmeProp uses approximate gradients by keeping only top-k elements based on the magnitude values. That is, only the top-k elements with the largest absolute values are kept. For example, suppose a vector v = \u30081, 2, 3,\u22124\u3009, then top2(v) = \u30080, 0, 3,\u22124\u3009. We denote the indices of vector \u03c3 \u2032\n(y)\u2019s top-k values as {t1, t2, ..., tk}(1 \u2264 k \u2264 n), and the approximate gradient of the parameter matrixW and input vector x is:\n\u2202z\n\u2202Wij \u2190 \u03c3\n\u2032 ix T j if i \u2208 {t1, t2, ..., tk} else 0 (5)\n\u2202z\n\u2202xi \u2190\n\u2211\nj\nWTij\u03c3 \u2032 j if j \u2208 {t1, t2, ..., tk} else 0 (6)\nAs a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost.\nFigure 1 is an illustration of meProp for a single computation unit of neural models. The original back propagation uses the full gradient of the output vectors to compute the gradient of the parameters. The proposed method selects the top-k values of the gradient of the output vector, and backpropagates the loss through the corresponding subset of the total model parameters.\nAs for a complete neural network framework with a loss L, the original back propagation computes the gradient of the parameter matrixW as:\n\u2202L \u2202W = \u2202L \u2202y \u00b7 \u2202y \u2202W (7)\nwhile the gradient of the input vector x is:\n\u2202L \u2202x = \u2202y \u2202x \u00b7 \u2202L \u2202y (8)\nThe proposed meProp selects top-k elements of the gradient \u2202L \u2202y to approximate the original gradient, and passes\nthem through the gradient computation graph according to the chain rule. Hence, the gradient ofW goes to:\n\u2202L \u2202W \u2190 topk( \u2202L \u2202y ) \u00b7 \u2202y \u2202W (9)\nwhile the gradient of the vector x is:\n\u2202L \u2202x \u2190 \u2202y \u2202x \u00b7 topk( \u2202L \u2202y ) (10)\nFigure 2 shows an illustration of the computational flow of meProp. The forward propagation is the same as traditional forward propagation, which computes the output vector via a matrix multiplication operation between two input tensors. The original back propagation computes the full gradient for the input vector and the weight matrix. For meProp, back propagation computes an approximate gradient by keeping top-k values of the backward flowed gradient and masking the remaining values to 0.\nFigure 3 further shows the computational flow of meProp for the mini-batch case."}, {"heading": "2.2. Implementation", "text": "We have coded two neural network models, including an LSTM model for part-of-speech (POS) tagging, and\na feedforward NN model (MLP) for transition-based dependency parsing and MNIST image recognition. We use the optimizers with automatically adaptive learning rates, including Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011). In our implementation, we make no modification to the optimizers, although there are many zero elements in the gradients.\nMost of the experiments on CPU are conducted on the framework coded in C# on our own. This framework builds a dynamic computation graph of the model for each sample, making it suitable for data in variable lengths. A typical training procedure contains four parts: building the computation graph, forward propagation, back propagation, and parameter update. We also have an implementation based on the PyTorch framework for GPU based experiments."}, {"heading": "2.2.1. WHERE TO APPLY MEPROP", "text": "The proposed method aims to reduce the complexity of the back propagation by reducing the elements in the computationally intensive operations. In our preliminary observations, matrix-matrix or matrix-vector multiplication consumed more than 90% of the time of back propagation. In our implementation, we apply meProp only to the back propagation from the output of the multiplication to its in-\nputs. For other element-wise operations (e.g., activation functions), the original back propagation procedure is kept, because those operations are already fast enough compared with matrix-matrix or matrix-vector multiplication operations.\nIf there are multiple hidden layers, the top-k sparsification needs to be applied to every hidden layer, because the sparsified gradient will again be dense from one layer to another. That is, in meProp the gradients are sparsified with a top-k operation at the output of every hidden layer.\nWhile we apply meProp to all hidden layers using the same k of top-k, usually the k for the output layer could be different from the k for the hidden layers, because the output layer typically has a very different dimension compared with the hidden layers. For example, there are 10 tags in the MNIST task, so the dimension of the output layer is 10, and we use an MLP with the hidden dimension of 500. Thus, the best k for the output layer could be different from that of the hidden layers.\n2.2.2. CHOICE OF TOP-k ALGORITHMS\nInstead of sorting the entire vector, we use the well-known min-heap based top-k selection method, which is slightly changed to focus on memory reuse. The algorithm has a\ntime complexity of O(n log k) and a space complexity of O(k)."}, {"heading": "3. Related Work", "text": "Riedmiller and Braun (1993) proposed a direct adaptive method for fast learning, which performs a local adaptation of the weight update according to the behavior of the error function. Tollenaere (1990) also proposed an adaptive acceleration strategy for back propagation. Dropout (Srivastava et al., 2014) is proposed to improve training speed and reduce the risk of overfitting. Sparse coding is a class of unsupervised methods for learning sets of over-complete bases to represent data efficiently (Olshausen & Field, 1996). Ranzato et al. (2006) proposed a sparse autoencoder model for learning sparse over-complete features. The proposed method is quite different compared with those prior studies on back propagation, dropout, and sparse coding.\nThe sampled-output-loss methods (Jean et al., 2015) are limited to the softmax layer (output layer) and are only based on random sampling, while our method does not have those limitations. The sparsely-gated mixture-ofexperts (Shazeer et al., 2017) only sparsifies the mixtureof-experts gated layer and it is limited to the specific set-\nting of mixture-of-experts, while our method does not have those limitations. There are also prior studies focusing on reducing the communication cost in distributed systems (Seide et al., 2014; Dryden et al., 2016), by quantizing each value of the gradient from 32-bit float to only 1- bit. Those settings are also different from ours."}, {"heading": "4. Experiments", "text": "To demonstrate that the proposed method is generalpurpose, we perform experiments on different models (LSTM/MLP), various training methods (Adam/AdaGrad), and diverse tasks.\nPart-of-Speech Tagging (POS-Tag): We use the standard benchmark dataset in prior work (Collins, 2002), which is derived from the Penn Treebank corpus, and use sections 0-18 of the Wall Street Journal (WSJ) for training (38,219 examples), and sections 22-24 for testing (5,462 examples). The evaluation metric is per-word accuracy. A popular model for this task is the LSTM model\n(Hochreiter & Schmidhuber, 1997),1 which is used as our baseline.\nTransition-based Dependency Parsing (Parsing): Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation. We follow the standard split of the corpus and use sections 2-21 as the training set (39,832 sentences, 1,900,056 transition examples),2 section 22 as the development set (1,700 sentences, 80,234 transition examples) and section 23 as the final test set (2,416 sentences, 113,368 transition examples). The evaluation metric is unlabeled attachment score (UAS). We implement a parser using MLP following Chen and Manning (2014), which is used as our baseline.\nMNIST Image Recognition (MNIST): We use the MNIST handwritten digit dataset (LeCun et al., 1998) for evaluation. MNIST consists of 60,000 28\u00d728 pixel training images and additional 10,000 test examples. Each image contains a single numerical digit (0-9). We select the first 5,000 images of the training images as the development set and the rest as the training set. The evaluation metric is per-image accuracy. We use the MLP model as the baseline."}, {"heading": "4.1. Experimental Settings", "text": "We set the dimension of the hidden layers to 500 for all the tasks. For POS-Tag, the input dimension is 1 (word)\u00d7 50 (dim per word)+ 7 (features)\u00d7 20 (dim per feature) = 190, and the output dimension is 45. For Parsing, the input\n1In this work, we use the bi-directional LSTM (Bi-LSTM) as the implementation of LSTM.\n2A transition example consists of a parsing context and its optimal transition action.\ndimension is 48 (features) \u00d7 50 (dim per feature) = 2400, and the output dimension is 25. For MNIST, the input dimension is 28 (pixels per row)\u00d7 28 (pixels per column)\u00d7 1 (dim per pixel) = 784, and the output dimension is 10. As discussed in Section 2, the optimal k of top-k for the output layer could be different from the hidden layers, because their dimensions could be very different. For Parsing and MNIST, we find using the same k for the output and the hidden layers works well, and we simply do so. For another task, POS-Tag, we find the the output layer should use a different k from the hidden layers. For simplicity, we do not apply meProp to the output layer for POS-Tag, because in this task we find the computational cost of the output layer is almost negligible compared with other layers.\nThe hyper-parameters are tuned based on the development data. For the Adam optimization method, we find the default hyper-parameters work well on development sets, which are as follows: the learning rate \u03b1 = 0.001, and \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1 \u00d7 10 \u22128. For the AdaGrad learner, the learning rate is set to \u03b1 = 0.01, 0.01, 0.1 for POS-Tag, Parsing, and MNIST, respectively, and \u01eb = 1 \u00d7 10\u22126. The experiments on CPU are conducted on a computer with the INTEL(R) Xeon(R) 3.0GHz CPU. The experiments on GPU are conducted on NVIDIA GeForce GTX 1080."}, {"heading": "4.2. Experimental Results", "text": "In this experiment, the LSTM is based on one hidden layer and the MLP is based on two hidden layers (experiments on more hidden layers will be presented later). We conduct experiments on different optimization methods, including AdaGrad and Adam. Since meProp is applied to the linear transformations (which entail the major computational cost), we report the linear transformation related backprop time as Backprop Time. It does not include nonlinear activations, which usually have only less than 2% computational cost. The total time of back propagation, in-\ncluding non-linear activations, is reported as Overall Backprop Time. Based on the development set and prior work, we set the mini-batch size to 1 (sentence), 10,000 (transition examples), and 10 (images) for POS-Tag, Parsing, and MNIST, respectively. Using 10,000 transition examples for Parsing follows Chen and Manning (2014).\nTable 1 shows the results based on different models and different optimizationmethods. In the table,mePropmeans applying meProp to the corresponding baseline model, h = 500 means that the hidden layer dimension is 500, and k = 20 means that meProp uses top-20 elements (among 500 in total) for back propagation. Note that, for fair comparisons, all experiments are first conducted on the development data and the test data is not observable. Then, the optimal number of iterations is decided based on the optimal score on development data, and the model of this iteration is used upon the test data to obtain the test scores.\nAs we can see, applying meProp can substantially speed up the back propagation. It provides a linear reduction in the computational cost. Surprisingly, results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More surprisingly, the accuracy of the resulting models is actually improved rather than decreased. The main reason could be that the minimal effort update does not modify weakly relevant parameters, which makes overfitting less likely, similar to the dropout effect.\nTable 2 shows the overall forward propagation time, the overall back propagation time, and the training time by summing up forward and backward propagation time. As we can see, back propagation has the major computational cost in training LSTM/MLP.\nThe results are consistent among AdaGrad and Adam. The results demonstrate that meProp is independent of specific optimization methods. For simplicity, in the following experiments the optimizer is based on Adam."}, {"heading": "4.3. Varying Backprop Ratio", "text": "In Figure 4 (left), we vary the k of top-k meProp to compare the test accuracy on different ratios of meProp backprop. For example, when k=5, it means that the backprop ratio is 5/500=1%. The optimizer is Adam. As we can see, meProp achieves consistently better accuracy than the baseline. The best test accuracy of meProp, 98.15% (+0.33), is actually better than the one reported in Table 1."}, {"heading": "4.4. Top-k vs. Random", "text": "It will be interesting to check the role of top-k elements. Figure 4 (middle) shows the results of top-k meProp vs. random meProp. The random meProp means that random elements (instead of top-k ones) are selected for back propagation. As we can see, the top-k version works better than the random version. It suggests that top-k elements contain the most important information of the gradients."}, {"heading": "4.5. Varying Hidden Dimension", "text": "We still have a question: does the top-k meProp work well simply because the original model does not require that big dimension of the hidden layers? For example, the meProp (topk=5) works simply because the LSTM works well with the hidden dimension of 5, and there is no need to use the hidden dimension of 500. To examine this, we perform experiments on using the same hidden dimension as k, and the results are shown in Table 3. As we can see, however, the results of the small hidden dimensions are much worse than those of meProp.\nIn addition, Figure 4 (right) shows more detailed curves by varying the value of k. In the figure, different k gives different backprop ratio for meProp and different hidden dimension ratio for LSTM/MLP. As we can see, the answer to that question is negative: meProp does not rely on redundant hidden layer elements."}, {"heading": "4.6. Adding Dropout", "text": "Since we have observed that meProp can reduce overfitting of deep learning, a natural question is that if meProp is reducing the same type of overfitting risk as dropout.\nThus, we use development data to find a proper value of the dropout rate on those tasks, and then further add meProp to check if further improvement is possible.\nTable 4 shows the results. As we can see, meProp can achieve further improvement over dropout. In particular, meProp has an improvement of 0.46 UAS on Parsing. The results suggest that the type of overfitting that meProp reduces is probably different from that of dropout. Thus, a model should be able to take advantage of both meProp and dropout to reduce overfitting."}, {"heading": "4.7. Adding More Hidden Layers", "text": "Another question is whether or not meProp relies on shallow models with only a few hidden layers. To answer this question, we also perform experiments on more hidden layers, from 2 hidden layers to 5 hidden layers. We find setting the dropout rate to 0.1 works well for most cases of different numbers of layers. For simplicity of comparison, we set the same dropout rate to 0.1 in this experiment. Table 5 shows that adding the number of hidden layers does not hurt the performance of meProp."}, {"heading": "4.8. Speedup on GPU", "text": "For implementing meProp on GPU, the simplest solution is to treat the entire mini-batch as a \u201cbig training example\u201d, where the top-k operation is based on the averaged values of all examples in the mini-batch. In this way, the big sparse matrix of the mini-batch will have consistent sparse patterns among examples, and this consistent sparse matrix can be transformed into a small dense matrix by remov-\ning the zero values. We call this implementation as simple unified top-k. This experiment is based on PyTorch.\nDespite its simplicity, Table 6 shows the good performance of this implementation, which is based on the mini-batch size of 50. We also find the speedup on GPU is less significant when the hidden dimension is low. The reason is that our GPU\u2019s computational power is not fully consumed by the baseline (with small hidden layers), so that the normal back propagation is already fast enough,making it hard for meProp to achieve substantial speedup. For example, supposing a GPU can finish 1000 operations in one cycle, there could be no speed difference between a method with 100 and a method with 10 operations. Indeed, we find MLP (h=64) and MLP (h=512) have almost the same GPU speed even on forward propagation (i.e., without meProp), while theoretically there should be an 8x difference. With GPU, the forward propagation time of MLP (h=64) and MLP (h=512) is 572ms and 644ms, respectively. This provides evidence for our hypothesis that our GPU is not fully consumed with the small hidden dimensions.\nThus, the speedup test on GPU is more meaningful for the heavy models, such that the baseline can at least fully consume the GPU\u2019s computational power. To check this, we test the GPU speedup on synthetic data of matrix multiplication with a larger hidden dimension. Indeed, Table 7 shows that meProp achieves much higher speed than the traditional backprop with the large hidden dimension. Furthermore, we test the GPU speedup on MLP with the large hidden dimension (Dryden et al., 2016). Table 8 shows that meProp also has substantial GPU speedup on MNIST with the large hidden dimension. In this experiment, the speedup is based on Overall Backprop Time (see the prior definition). Those results demonstrate that meProp can achieve\ngood speedup on GPU when it is applied to heavy models.\nFinally, there are potentially other implementation choices of meProp on GPU. For example, another natural solution is to use a big sparse matrix to represent the sparsified gradient of the output of a mini-batch. Then, the sparse matrix multiplication library can be used to accelerate the computation. This could be an interesting direction of future work."}, {"heading": "4.9. Related Systems on the Tasks", "text": "The POS tagging task is a well-known benchmark task, with the accuracy reports from 97.2% to 97.4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015). Our method achieves 97.31% (Table 4).\nFor the transition-based dependency parsing task, existing approaches typically can achieve the UAS score from 91.4 to 91.5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010). As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.5 UAS. Chen and Manning (2014) achieves 92.0 UAS using neural networks. Our method achieves 91.99 UAS (Table 4).\nFor MNIST, the MLP based approaches can achieve 98\u2013 99% accuracy, often around 98.3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010). Our method achieves 98.37% (Table 5). With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012). Our method can also be improved with those additional techniques, which, however, are not the focus of this paper."}, {"heading": "5. Conclusions", "text": "The back propagation in deep learning tries to modify all parameters in each stochastic update, which is inefficient and may even lead to overfitting due to unnecessary modification of many weakly relevant parameters. We propose a minimal effort back propagation method (meProp), in which we compute only a very small but critical portion of\nthe gradient, and modify only the corresponding small portion of the parameters in each update. This leads to very sparsified gradients to modify only highly relevant parameters for the given training sample. The proposed meProp is independent of the optimization method. Experiments show that meProp can reduce the computational cost of back propagation by one to two orders of magnitude via updating only 1\u20134% parameters, and yet improve the model accuracy in most cases."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the anonymous reviewers for insightful comments and suggestions on this paper. This work was supported in part by National Natural Science Foundation of China (No. 61673028), National High Technology Research and Development Program of China (863 Program, No. 2015AA015404), and an Okawa Research Grant (2016)."}], "references": [{"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Danqi", "Manning", "Christopher D"], "venue": "In Proceedings of EMNLP\u201914,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Deep, big, simple neural nets for handwritten digit recognition", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Gambardella", "Luca Maria", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Multi-column deep neural networks for image classification", "author": ["Ciresan", "Dan C", "Meier", "Ueli", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of CVPR\u201912,", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Collins", "Michael"], "venue": "In Proceedings of EMNLP\u201902,", "citeRegEx": "Collins and Michael.,? \\Q2002\\E", "shortCiteRegEx": "Collins and Michael.", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel P"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Communication quantization for dataparallel training of deep neural networks", "author": ["Dryden", "Nikoli", "Moon", "Tim", "Jacobs", "Sam Ade", "Essen", "Brian Van"], "venue": "In Proceedings of the 2nd Workshop on Machine Learning in HPC Environments,", "citeRegEx": "Dryden et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dryden et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John C", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Liang", "Sagae", "Kenji"], "venue": "In Proceedings of ACL\u201910,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang", "Zhiheng", "Xu", "Wei", "Yu", "Kai"], "venue": "CoRR, abs/1508.01991,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Jarrett", "Kevin", "Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "LeCun", "Yann"], "venue": "In Proceeding of ICCV\u201909,", "citeRegEx": "Jarrett et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean", "S\u00e9bastien", "Cho", "KyungHyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In Proceedings of ACL/IJCNLP\u201915,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Nivre", "Joakim", "Hall", "Johan", "Nilsson", "Jens", "Chanev", "Atanas", "Eryigit", "G\u00fclsen", "K\u00fcbler", "Sandra", "Marinov", "Svetoslav", "Marsi", "Erwin"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Natural image statistics and efficient coding", "author": ["Olshausen", "Bruno A", "Field", "David J"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Olshausen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen et al\\.", "year": 1996}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Ranzato", "Marc\u2019Aurelio", "Poultney", "Christopher S", "Chopra", "Sumit", "LeCun", "Yann"], "venue": "In NIPS\u201906,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Riedmiller", "Martin", "Braun", "Heinrich"], "venue": "In Proceedings of IEEE International Conference on Neural Networks", "citeRegEx": "Riedmiller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 1993}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts", "author": ["Shazeer", "Noam", "Mirhoseini", "Azalia", "Maziarz", "Krzysztof", "Davis", "Andy", "Le", "Quoc V", "Hinton", "Geoffrey E", "Dean", "Jeff"], "venue": "layer. CoRR,", "citeRegEx": "Shazeer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen", "Libin", "Satta", "Giorgio", "Joshi", "Aravind K"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Simard", "Patrice Y", "Steinkraus", "Dave", "Platt", "John C"], "venue": "In Proceedings of ICDR\u201903,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Structure regularization for structured prediction", "author": ["Sun", "Xu"], "venue": "In NIPS\u201914,", "citeRegEx": "Sun and Xu.,? \\Q2014\\E", "shortCiteRegEx": "Sun and Xu.", "year": 2014}, {"title": "Supersab: fast adaptive back propagation with good scaling properties", "author": ["Tollenaere", "Tom"], "venue": "Neural networks,", "citeRegEx": "Tollenaere and Tom.,? \\Q1990\\E", "shortCiteRegEx": "Tollenaere and Tom.", "year": 1990}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Toutanova", "Kristina", "Klein", "Dan", "Manning", "Christopher D", "Singer", "Yoram"], "venue": "In Proceedings of HLT-NAACL\u201903,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Learning with lookahead: Can history-based models rival globally optimized models", "author": ["Tsuruoka", "Yoshimasa", "Miyao", "Yusuke", "Kazama", "Jun\u2019ichi"], "venue": "In Proceedings of CoNLL\u201911,", "citeRegEx": "Tsuruoka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2011}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Yue", "Clark", "Stephen"], "venue": "In Proceedings of EMNLP\u201908,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "We use the optimizers with automatically adaptive learning rates, including Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011).", "startOffset": 113, "endOffset": 133}, {"referenceID": 17, "context": "Ranzato et al. (2006) proposed a sparse autoencoder model for learning sparse over-complete features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "The sampled-output-loss methods (Jean et al., 2015) are limited to the softmax layer (output layer) and are only based on random sampling, while our method does not have those limitations.", "startOffset": 32, "endOffset": 51}, {"referenceID": 19, "context": "The sparsely-gated mixture-ofexperts (Shazeer et al., 2017) only sparsifies the mixtureof-experts gated layer and it is limited to the specific set-", "startOffset": 37, "endOffset": 59}, {"referenceID": 5, "context": "There are also prior studies focusing on reducing the communication cost in distributed systems (Seide et al., 2014; Dryden et al., 2016), by quantizing each value of the gradient from 32-bit float to only 1bit.", "startOffset": 96, "endOffset": 137}, {"referenceID": 14, "context": "Transition-based Dependency Parsing (Parsing): Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation.", "startOffset": 104, "endOffset": 125}, {"referenceID": 14, "context": "Transition-based Dependency Parsing (Parsing): Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation. We follow the standard split of the corpus and use sections 2-21 as the training set (39,832 sentences, 1,900,056 transition examples), section 22 as the development set (1,700 sentences, 80,234 transition examples) and section 23 as the final test set (2,416 sentences, 113,368 transition examples). The evaluation metric is unlabeled attachment score (UAS). We implement a parser using MLP following Chen and Manning (2014), which is used as our baseline.", "startOffset": 105, "endOffset": 568}, {"referenceID": 13, "context": "MNIST Image Recognition (MNIST): We use the MNIST handwritten digit dataset (LeCun et al., 1998) for evaluation.", "startOffset": 76, "endOffset": 96}, {"referenceID": 5, "context": "Furthermore, we test the GPU speedup on MLP with the large hidden dimension (Dryden et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 25, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 20, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 26, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 4, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 9, "context": "4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 3, "endOffset": 124}, {"referenceID": 15, "context": "5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010).", "startOffset": 2, "endOffset": 64}, {"referenceID": 15, "context": "As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.", "startOffset": 64, "endOffset": 84}, {"referenceID": 15, "context": "5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010). As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.5 UAS. Chen and Manning (2014) achieves 92.", "startOffset": 24, "endOffset": 189}, {"referenceID": 13, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 21, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 1, "context": "3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).", "startOffset": 3, "endOffset": 66}, {"referenceID": 10, "context": "With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012).", "startOffset": 103, "endOffset": 147}, {"referenceID": 2, "context": "With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012).", "startOffset": 103, "endOffset": 147}], "year": 2017, "abstractText": "We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1\u20134% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.", "creator": "LaTeX with hyperref package"}}}