{"id": "1706.07138", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2017", "title": "Generating Long-term Trajectories Using Deep Hierarchical Networks", "abstract": "We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when myopic modeling lead to the desired behavior. The key difficulty is that conventional approaches are \"shallow\" models that only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts. Our approach has proven well suited for sports science majors. We are now exploring the impact of hierarchical policy classes that allow us to implement hierarchical policy-based model predictions, rather than merely model a single individual individual agent. We believe that this approach is particularly effective when you consider how models and their models can support multiple outcomes of both sports.\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 21 Jun 2017 23:05:52 GMT  (2935kb,D)", "http://arxiv.org/abs/1706.07138v1", "Published in NIPS 2016"]], "COMMENTS": "Published in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stephan zheng", "yisong yue", "jennifer hobbs"], "accepted": true, "id": "1706.07138"}, "pdf": {"name": "1706.07138.pdf", "metadata": {"source": "CRF", "title": "Generating Long-term Trajectories Using Deep Hierarchical Networks", "authors": ["Stephan Zheng", "Yisong Yue"], "emails": ["stzheng@caltech.edu", "yyue@caltech.edu", "plucey@stats.com"], "sections": [{"heading": null, "text": "1 Introduction\nModeling long-term behavior is a key challenge in many learning problems that require complex decision-making. Consider a sports player determining a movement trajectory to achieve a certain strategic position. The space of all such trajectories is prohibitively large, and precludes conventional approaches, such as those based on Markovian dynamics.\nMany decision problems can be naturally modeled as requiring high-level, long-term macro-goals, which span time horizons much longer than the timescale of low-level micro-actions (cf. [1, 2]). A natural example for such macro-micro behavior occurs in spatiotemporal games, such as basketball where players execute complex trajectories. The microactions of each agent are to move around the court and if they have the ball, dribble, pass or shoot the ball. These micro-actions operate at the millisecond scale, whereas their macro-goals, such as \"maneuver behind these 2 defenders towards the basket\", span multiple seconds. Figure 1\ndepicts an example from a professional basketball game, where the player must make a sequence of movements (micro-actions) in order to reach a specific location on the basketball court (macro-goal).\nIntuitively, agents need to trade-off between short-term and long-term behavior: often sequences of individually reasonable micro-actions do not form a cohesive trajectory towards a macro-goal. For instance, in Figure 1 the player (green) takes a highly non-linear trajectory towards his macro-goal of positioning near the basket. As such, conventional approaches are not well suited for these settings, as they generally use a single (low-level) state-action planner, which is only successful when myopic or short-term decision-making leads to the desired behavior.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n70 6.\n07 13\n8v 1\n[ cs\n.L G\n] 2\n1 Ju\nn 20\nIn this paper, we propose a novel class of hierarchical policy models, which we instantiate using recurrent neural networks, that can simultaneously reason about both macro-goals and micro-actions. Our model utilizes an attention mechanism through which the macro-planner guides the micro-planner. Our model is further distinguished from previous work on hierarchical policies by dynamically predicting macro-goals instead of following fixed goals, which gives additional flexibility to our model class that can be fitted to data (rather than having the macro-goals be hand-crafted).\nWe showcase our approach in a case study on learning to imitate demonstrated behavior in professional basketball. Our primary result is that our approach generates significantly more realistic player trajectories compared to non-hierarchical baselines, as judged by professional sports analysts. We also provide a comprehensive qualitative and quantitive analysis, e.g., showing that incorporating macro-goals can actually improve 1-step micro-action prediction accuracy."}, {"heading": "2 Related Work", "text": "The reinforcement learning community has largely focused on non-hierarchical, or \u201cshallow\u201d, policies such as those based on Markovian or linear dynamics (cf. [3, 4, 2]). By and large, such policy classes are only shown to be effective when the optimal action can be found via short-term planning. Previous research has instead focused on issues such as how to perform effective exploration, plan over parameterized action spaces, or deal with non-convexity issues from using deep neural networks within the policy model. In contrast, we focus on developing hierarchical policices that can effectively generate realistic long-term plans in complex settings such as basketball gameplay.\nThe use of hierarchical models to decompose macro-goals from micro-actions is relatively common in the planning community (cf. [5, 1, 6]). For instance, the winning team in 2015 RoboCup Simulation Challenge [6] used a manually constructed hierarchical planner to solve MDPs with a set of fixed sub-tasks. In contrast, we study how to learn a hierarchical planner from a large amount of expert demonstrations that can adapt its policy in non-Markovian environments with dynamic macro-goals.\nA somewhat related line of research aims to develop efficient planners for factored MDPs [7], e.g., by learning value functions over factorized state spaces for multi-agent systems. It may be possible that such a factorization approach is also applicable for learning our hierarchical structure. However, it is less clear how to combine such a factorization method with utilizing deep networks.\nOur hierarchical model bears affinity to other attention models for deep networks that have mainly been applied to natural language processing, image recognition and combinations thereof [8]. In contrast to previous work which focuses on attention models of the input, our attention model is applied to the output by integrating control from both the macro-planner and the micro-planner.\nRecent work on generative models for sequential data [9], such as handwriting generation, have combined latent variables with an RNN\u2019s hidden state to capture temporal variability in the input. In our work, we instead aim to learn semantically meaningful latent variables that are external to the RNN and reason about long-term behavior and goals.\nOur model shares conceptual similarities to the Dual Process framework [10], which decomposes cognitive processes into fast, unconscious behavior (System 1) and slow, conscious behavior (System 2). This separation reflects our policy decomposition into a macro and micro part. Other related work in neuroscience and cognitive science include hierarchical models of learning by imitation [11]."}, {"heading": "3 Long-Term Trajectory Planning", "text": "We are interested in learning policies that can produce high quality trajectories, where quality is some global measure of the trajectory (e.g., realistic trajectories as in Figure 1). We first set notation:\n\u2022 Let S,A denote the state, action space. Macro policies also use a goal space G. \u2022 At time t, an agent i is in state sit \u2208 S and takes action ait \u2208 A. The state and action vectors\nare st = { sit } players i, at = { ait } players i. The history of events is ht = {(su, au)}0\u2264u<t.\n\u2022 Let \u03c0(st, ht) denote a policy that maps state and history to a distribution over actions P (at|st, ht). If \u03c0 is deterministic, the distribution is peaked around a specific action. We\nalso abuse notation to sometimes refer to \u03c0 as deterministically taking the most probable action \u03c0(st, ht) = argmaxa\u2208AP (a|st, ht) \u2013 this usage should be clear from context.\nOur main research question is how to design a policy class that can capture the salient properties of how expert agents execute trajectories. In particular, we present a general policy class that utilizes a goal space G to guide its actions to create such trajectory histories. We show in Section 4 how to instantiate this policy class as a hierarchical network that uses an attention mechanism to combine macro-goals and micro-actions. In our case study on modeling basketball behavior (Section 5.1), we train such a policy to imitate expert demonstrations using a large dataset of tracked basketball games.\n3.1 Incorporating Macro-Goals\nOur main modeling assumption is that a policy simultaneously optimizes behavior hierarchically on multiple well-separated timescales. We consider two distinct timescales (macro and micro-level), although our approach could in principle be generalized to even more timescales. During an episode [t0, t1], an agent i executes a sequence of micro-actions ( ait ) t\u22650 that leads to a macrogoal git \u2208 G. We do not assume that the start and end times of an episode are fixed. For instance, macro-goals can change before they are reached. We finally assume that macro-goals are relatively static on the timescale of the micro-actions, that is: dgit/dt 1. Figure 2 depicts an example of an agent with two unique macro-goals over a 50-frame trajectory. At every timestep t, the agent executes a micro-action ait, while the macro-goals git change more slowly.\nWe model the interaction between a micro-action ait and a macro-goal g i t through a raw micro-action uit \u2208 A that is independent of the macro-goal. The micro-policy is then defined via a joint distribution over the raw micro-action and macro-goal:\nait = \u03c0micro(s i t, h i t) = argmaxaP micro(a|sit, hit) (1) Pmicro(ait|sit, hit) = \u222b dudgP (ait|u, g)P (u, g|sit, hit). (2)\nHere, we model the conditional distribution P (a|u, g) as a non-linear factorization:\nait = \u03c8(u i t, \u03c6(g i t)), P (a i t|uit, git) = \u03a8 ( P raw(uit),\u03a6 ( Pmacro(git) )) , (3)\nwhere \u03c6,\u03a6 are non-linear transfer functions and \u03c8,\u03a8 are synthesis functions - we will elaborate on the specific choices of \u03a6,\u03a8 in Section 4. In this work, we do not condition (3) explicitly on st, ht, as we found this did not make a significant difference in our experiments. This hierarchical decomposition is visualized in Figure 3 and can be generalized to multiple scales l using multiple macro-goals gl and transfer functions \u03c6l."}, {"heading": "4 Hierarchical Policy Network", "text": "Figure 3 depicts a high-level overview of our hierarchical policy class for long-term spatiotemporal planning. In this work, both the raw micro-policy and macro-policy are instantiated as recurrent convolutional neural networks, and the raw action and macro-goals are combined through an attention mechanism, which we elaborate on below.\nAttention mechanism for integrating macro-goals and micro-actions.\nWe model the joint distribution to implement an attention mechanism over the action space A, that is, a non-linear weight function on A. For this, \u03a8 = is a Hadamard product and the transfer function \u03c6maps a macro-goal g into an attention variable \u03c6(g), whose distribution acts as an attention. Suppressing indices i, t for clarity, the conditional distribution (3) becomes\nP (a|u, g) = \u03b4a,u,\u03c6(g)P raw(u|s, h) \u03a6 (Pmacro(g|s, h)) (Hadamard), (4) Intuitively, this structure captures the trade-off between the macro- and raw micro-policy. On the one hand, the raw micro-policy \u03c0raw aims for short-term optimality, On the other hand, the macro-policy \u03c0macro can attend via \u03a6 to sequences of actions that lead to a macro-goal and bias the agent towards good long-term behavior. The difference between u and \u03c6(g) thus reflects the trade-off that the hierarchical policy learns between actions that are good for either short-term or long-term goals.\nDiscretization and deep neural architecture. In general, when using continuous (stochastic) latent variables g, learning the model (1) is intractable, and one would have to resort to approximation methods. Hence, to reduce complexity we discretize the state-action and latent spaces. In spatiotemporal settings, such as for basketball, a state sit \u2208 S can naturally be represented as a 1-hot occupancy vector of the basketball court. We then pose goal states git as sub-regions of the court that i wants to reach, defined at a coarser resolution than S. With this choice, it is natural to instantiate the macro and micro-policies as convolutional recurrent neural networks, which can capture both predictive spatial patterns, non-Markovian temporal dynamics and implement the non-linear attention \u03a6.\nMulti-stage learning. Given a training set D of sequences of state-action tuples (st, a\u0302t), where the a\u0302t are 1-hot labels, the hierarchical policy network can be trained by solving an optimization problem\n\u03b8\u2217 = argmin \u03b8 \u2211 D T\u2211 t=1 Lt(st, ht, a\u0302t; \u03b8). (5)\nGiven the hierarchical structure of our model class, we decompose the loss Lt (dropping the index t):\nL(s, h, a\u0302; \u03b8) = Lmacro (s, h, g; \u03b8) + Lmicro (s, h, a\u0302; \u03b8) +R(\u03b8), (6)\nLmicro(s, h, a\u0302; \u03b8) = A\u2211 k=1 a\u0302k log [P raw(uk|s, h; \u03b8) \u00b7 \u03a6(\u03c6(g)k|s, h; \u03b8)] , (7)\nwhere Rt(\u03b8) regularizes the model weights \u03b8 and k indexes A discrete action-values. Although we have ground truths a\u0302t for the observable micro-actions, in general we do not have labels for the macro-goals gt that induce optimal long-term planning. As such, one would have to appeal to separate solution methods to compute the posterior P (gt|st, ht) which minimizes Lt,macro (st, ht; gt, \u03b8). To reduce this complexity and given the non-convexity of (7), we instead follow a multi-stage learning approach with a set of weak labels g\u0302t, \u03c6\u0302t for the macro-goals gt and attention masks \u03c6t = \u03c6(gt). We assume access to such weak labels and only use them in the initial training phases. In this approach, we first train the raw micro-policy, macro-policy and attention individually, freezing the other parts of the network. The policies \u03c0micro, \u03c0macro and attention \u03c6 can be trained using standard cross-entropy minimization with the labels a\u0302t, g\u0302t and \u03c6\u0302t, respectively. In the final stage we fine-tune the entire network on objective (5), using only Lt,micro and R. We found this approach beneficial to find a good initialization for the fine-tuning phase and to achieve high-quality long-term planning performance.1 Another advantage of this approach is that we can train the model with gradient descent in all stages.\n1As ut and \u03c6(gt) enter symmetrically into the objective (7), it is possible, in principle, that the network converges to a symmetric phase where the predictions ut and \u03c6(gt) become identical along the entire trajectory. However, our experiments suggest that our multi-stage learning approach separates timescales well between the micro- and macro policy and prevents the network from settling in the redundant symmetric phase."}, {"heading": "5 Case Study on Modeling Basketball Behavior", "text": "We applied our approach to modeling basketball behavior data. In particular, we focus on imitating the players\u2019 movements, which is a challenging problem in the spatiotemporal planning setting."}, {"heading": "5.1 Experimental Setup", "text": "We validated the hierarchical policy network (HPN) by learning a basketball movement policy that predicts as the micro-action the instantaneous velocity vit = \u03c0micro(s i t, h i t).\nTraining data. We trained the HPN on a large dataset of tracking data from professional basketball games [13]. 2 The dataset consists of possessions of variable length: each possession is a sequence of tracking coordinates sit = ( xit, y i t ) for each player i, recorded at 25 Hz, where one team has continuous possession of the ball. Since possessions last between 50 and 300 frames, we sub-sampled every 4 frames and used a fixed input sequence length of 50 to make training feasible. Spatially, we discretized the left half court using 400 \u00d7 380 cells of size 0.25ft \u00d7 0.25ft. For simplicity, we modeled every player identically using a single policy network. The resulting input data for each possession is grouped into 4 channels: the ball, the player\u2019s location, his teammates, and the opposing team. After this pre-processing, we extracted 130,000 tracks for training and 13,000 as a holdout set.\nLabels. We extracted micro-action labels v\u0302it = sit+1 \u2212 sit as 1-hot vectors in a grid of 17\u00d7 17 unit cells. Additionally, we constructed a set of weak macro-labels g\u0302t, \u03c6\u0302t by heuristically segmenting each track using its stationary points. The labels g\u0302t were defined as the next stationary point. For \u03c6\u0302t, we used 1-hot velocity vectors vit,straight along the straight path from the player\u2019s location s i t to the macro-goal git. We refer to the supplementary material for additional details.\nModel hyperparameters. To generate smooth rollouts while sub-sampling every 4 frames, we simultaneously predicted the next 4 micro-actions at, . . . , at+3. A more general approach would model the dependency between look-ahead predictions as well, e.g. P (\u03c0t+\u2206+1|\u03c0t+\u2206). However, we found that this variation did not outperform baseline models. We selected a network architecture to balance performance and feasible training-time. The macro and micro-planner use GRU memory cells [14] and a memory-less 2-layer fully-connected network as the transfer function m, as depicted in Figure 4. We refer to the supplementary material for more details.\nBaselines. We compared the HPN against two natural baselines.\n1. A policy with only a raw micro-policy and memory (GRU-CNN) and without memory (CNN).\n2. A hierarchical policy network H-GRU-CNN-CC without an attention mechanism, which instead learns the final output from a concatenation of the raw micro- and macro-policy.\nRollout evaluation. To evaluate the quality of our model, we generated rollouts (st;h0,r0) with burnin period r0, These are generated by 1) feeding a ground truth sequence of states h0,r0 = (s0, . . . , sr0)\n2A version of the dataset is available at https://www.stats.com/data-science/.\nto the policy network and 2) for t > r0, iteratively predicting the next action at and updating the game-state st \u2192 st+1, using ground truth locations for the other agents."}, {"heading": "5.2 How Realistic are the Generated Trajectories?", "text": "The most holistic way to evaluate the trajectory rollouts is via visual analysis. Simply put, would a basketball expert find the rollouts by HPN more realistic than those by the baselines? We begin by first visually analyzing some rollouts, and then present our human preference study results.\nVisualization. Figure 5 depicts example rollouts for our HPN approach and one example rollout for a baseline. Every rollout consists of two parts: 1) an initial ground truth phase from the holdout set and 2) a continuation by either the HPN or ground truth. We note a few salient properties. First, the HPN does not memorize tracks, as the rollouts differ from the tracks it has trained on. Second, the HPN generates rollouts with a high dynamic range, e.g. they have realistic curves, sudden changes of directions and move over long distances across the court towards macro-goals. For instance, HPN tracks do not move towards macro-goals in unrealistic straight lines, but often take a curved route, indicating that the policy balances moving towards macro-goals with short-term responses to the current state (see also Figure 6b). In contrast, the baseline model often generates more constrained behavior, such as moving in straight lines or remaining stationary for long periods of time.\nHuman preference study. Our primary empirical result is a preference study eliciting judgments on the relative quality of rollout trajectories between HPN and baselines or ground truth. We recruited seven experts (professional sports analysts) and eight knowledgeable non-experts (e.g., college basketball players) as judges. Because all the learned policies perform better with a \u201cburn-in\u201d period, we first animated with the ground truth for 20 frames, and then extrapolated with a policy for 30 frames. During extrapolation, the other nine players do not animate. For each test case, the users were shown an animation of two rollout extrapolations of a specific player\u2019s movement: one generated by the HPN, the other by a baseline or ground truth. The judges then chose which rollout looked more realistic. Please see the supplementary material for details of the study.\nTable 1 shows the preference study results. We tested 25 scenarios (some corresponding to scenarios in Figure 6b). HPN won the vast majority of comparisons against the baselines using expert judges,\nModel comparison Experts Non-Experts All W/T/L Avg Gain W/T/L Avg Gain W/T/L Avg Gain VS-CNN 21 / 0 / 4 0.68 15 / 9 / 1 0.56 21 / 0 / 4 0.68 VS-GRU-CNN 21 / 0 / 4 0.68 18 / 2 / 5 0.52 21 / 0 / 4 0.68 VS-H-GRU-CNN-CC 22 / 0 / 3 0.76 21 / 0 / 4 0.68 21 / 0 / 4 0.68 VS-GROUND TRUTH 11 / 0 / 14 -0.12 10 / 4 / 11 -0.04 11 / 0 / 14 -0.12\n(a) Predicted distributions for attention masks \u03c6 (left column), velocity (micro-action) \u03c0micro (middle) and weighted velocity \u03c6(g) \u03c0micro (right) for basketball players. The center corresponds to 0 velocity. (b) Rollout tracks and predicted macro-goals gt (blue boxes). The HPN starts the rollout after 20 frames. Macro-goal box intensity corresponds to relative prediction frequency during the trajectory.\nFigure 6: Left: Visualizing how the attention mask generated from the macro-planner interacts with the microplanner \u03c0micro. Row 1 and 2: the micro-planner \u03c0micro decides to stay stationary, but the attention \u03c6 goes to the left. The weighted result \u03c6 \u03c0micro is to move to the left, with a magnitude that is the average. Row 3) \u03c0micro wants to go straight down, while \u03c6 boosts the velocity so the agent bends to the bottom-left. Row 4) \u03c0micro goes straight up, while \u03c6 goes left: the agent goes to the top-left. Row 5) \u03c0micro remains stationary, but \u03c6 prefers to move in any direction. As a result, the agent moves down. Right: The HPN dynamically predicts macro-goals and guides the micro-planner in order to reach them. The macro-goal predictions are stable over a large number of timesteps. The HPN sometimes predicts inconsistent macro-goals. For instance, in the bottom right frame, the agent moves to the top-left, but still predicts the macro-goal to be in the bottom-left sometimes.\nwith slightly weaker preference using non-expert judges. HPN was also competitive with the ground truth. These results suggest that the HPN is able to generate high-quality trajectories that significantly improve on baselines, and approach the ground truth quality for our extrapolation setting."}, {"heading": "5.3 Analyzing Macro- and Micro-planner Integration", "text": "Our model integrates the macro- and micro-planner by converting the macro-goal into an attention mask on the micro-action output space, which intuitively guides the micro-planner towards the macro-goal. We now inspect our macro-planner and attention mechanism to verify this behavior.\nFigure 6a depicts how the macro-planner \u03c0macro guides the micro-planner \u03c0micro through the attention \u03c6, by attending to the direction in which the agent can reach the predicted macro-goal. The attention\nmodel and micro-planner differ in semantic behavior: the attention favors a wider range of velocities and larger magnitudes, while the micro-planner favors smaller velocities.\nFigure 6b depicts predicted macro-goals by HPN along with rollout tracks. In general, we see that the rollouts are guided towards the predicted macro-goals. However, we also observe that the HPN makes some inconsistent macro-goal predictions, which suggests there is still ample room for improvement."}, {"heading": "5.4 Benchmark Analysis", "text": "We finally evaluated using a number of benchmark experiments, with results shown in Table 2. These experiments measure quantities that are related to overall quality, albeit not holistically. We first evaluated the 4-step look-ahead accuracy of the HPN for micro-actions ait+\u2206,\u2206 = 0, 1, 2, 3. On this benchmark, the HPN outperforms all baseline policy networks when not using weak labels \u03c6\u0302 to train the attention mechanism, which suggests that using a hierarchical model can noticeably improve the short-term prediction accuracy over non-hierarchical baselines.\nWe also report the prediction accuracy on weak labels g\u0302, \u03c6\u0302, although they were only used during pretraining, and we did not fine-tune for accuracy on them. Using weak labels one can tune the network for both long-term and short-term planning, whereas all non-hierarchical baselines are optimized for short-term planning only. Including the weak labels \u03c6\u0302 can lower the accuracy on short-term prediction, but increases the quality of the on-policy rollouts. This trade-off can be empirically set by tuning the number of weak labels used during pre-training."}, {"heading": "6 Limitations and Future Work", "text": "We have presented a hierarchical policy class for generating long-term trajectories that models both macro-goals and micro-actions, and integrates them using a novel attention mechanism. We showed significant improvement over non-hierarchical baselines in a case study on basketball player behavior.\nThere are several notable limitations to our HPN model. First, we did not consider all aspects of basketball gameplay, such as passing and shooting. We also modeled all players using a single policy whereas in reality player behaviors vary (although the variability can be low-dimensional [13]). We also only modeled offensive players, and another interesting direction is modeling defensive players and integrating adversarial reinforcement learning [15] into our approach. These issues limited the scope of the rollouts in our preference study, and it would be interesting to consider extended settings.\nIn order to focus on our HPN model class, we considered only the imitation learning setting. More broadly, many planning problems require collecting training data via exploration [4], which can be more challenging. One interesting scenario is having two adversarial policies learn to be strategic against each other through repeatedly game-play in a basketball simulator. Furthermore, in general it can be difficult to acquire the appropriate weak labels to initialize the macro-planner training.\nFrom a technical perspective, using attention in the output space may be applicable to other architectures. More sophisticated applications may require multiple levels of output attention masking."}], "references": [{"title": "PUMA: Planning Under Uncertainty with Macro-Actions", "author": ["Ruijie He", "Emma Brunskill", "Nicholas Roy"], "venue": "In Twenty-Fourth AAAI Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Deep reinforcement learning in parameterized action space", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S. Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Online planning for large markov decision processes with hierarchical decomposition", "author": ["Aijun Bai", "Feng Wu", "Xiaoping Chen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Efficient Solution Algorithms for Factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A recurrent latent variable model for sequential data", "author": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Dual-Process Theories of Higher Cognition Advancing the Debate", "author": ["Jonathan St B.T. Evans", "Keith E. Stanovich"], "venue": "Perspectives on Psychological Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Learning by imitation: A hierarchical approach", "author": ["Richard W Byrne", "Anne E Russon"], "venue": "Behavioral and brain sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning Fine- Grained Spatial Models for Dynamic Sports Play Prediction", "author": ["Yisong Yue", "Patrick Lucey", "Peter Carr", "Alina Bialkowski", "Iain Matthews"], "venue": "In IEEE International Conference on Data Mining (ICDM),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Cooperative multi-agent learning: The state of the art", "author": ["Liviu Panait", "Sean Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2]).", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[3, 4, 2]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[3, 4, 2]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 1, "context": "[3, 4, 2]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[5, 1, 6]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 0, "context": "[5, 1, 6]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[5, 1, 6]).", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "For instance, the winning team in 2015 RoboCup Simulation Challenge [6] used a manually constructed hierarchical planner to solve MDPs with a set of fixed sub-tasks.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "A somewhat related line of research aims to develop efficient planners for factored MDPs [7], e.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Our hierarchical model bears affinity to other attention models for deep networks that have mainly been applied to natural language processing, image recognition and combinations thereof [8].", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "Recent work on generative models for sequential data [9], such as handwriting generation, have combined latent variables with an RNN\u2019s hidden state to capture temporal variability in the input.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Our model shares conceptual similarities to the Dual Process framework [10], which decomposes cognitive processes into fast, unconscious behavior (System 1) and slow, conscious behavior (System 2).", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Other related work in neuroscience and cognitive science include hierarchical models of learning by imitation [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 11, "context": "Batch-normalization (bn) [12] is applied to stabilize training.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "We trained the HPN on a large dataset of tracking data from professional basketball games [13].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "The macro and micro-planner use GRU memory cells [14] and a memory-less 2-layer fully-connected network as the transfer function m, as depicted in Figure 4.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "We also modeled all players using a single policy whereas in reality player behaviors vary (although the variability can be low-dimensional [13]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "We also only modeled offensive players, and another interesting direction is modeling defensive players and integrating adversarial reinforcement learning [15] into our approach.", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "More broadly, many planning problems require collecting training data via exploration [4], which can be more challenging.", "startOffset": 86, "endOffset": 89}], "year": 2017, "abstractText": "We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when myopic modeling lead to the desired behavior. The key difficulty is that conventional approaches are \u201cshallow\u201d models that only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and shortterm goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.", "creator": "LaTeX with hyperref package"}}}