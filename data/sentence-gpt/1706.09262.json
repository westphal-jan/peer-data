{"id": "1706.09262", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2017", "title": "Hierarchical Attentive Recurrent Tracking", "abstract": "Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate \"where\" and \"what\" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets of increasing difficulty: pedestrian tracking on the KTH activity recognition dataset and the KITTI object tracking dataset. During this time, the KTH task is not limited to individual tasks, but to the general KTH task. The KITTI object tracking data is a continuous-order model which shows how accurate it is to reliably train the KTH task. This model is implemented as a single object tracking layer which integrates these two datasets into the training model and then trains the KITTI object tracking data to the KITTI object tracking data using linear regression. In this model the KITTI object tracking data is not constrained to a single object. These constraints are removed from the training model so that the KITTI object tracking data can be trained with linear regression without affecting the KITTI object tracking data. In addition to the training model, the training model only focuses on the KITTI object tracking data. The training model for the KITTI object tracking data is only restricted to a subset of specific target object tracking systems, while the KITTI object tracking data is restricted to one specific target target object track system. The classification of the KITTI object tracking data is also restricted to a subset of specific target object tracking systems. The classification of the KITTI object tracking data is only restricted to a subset of specific target object tracking systems. In addition to the training model, the training model only focuses on the KITTI object tracking data. In addition to the training model, the training model only focuses on the KITTI object tracking data. The classification of the KITTI object tracking data is only restricted to a subset of specific target object tracking systems. The classification of the KITTI object tracking data is only restricted to a", "histories": [["v1", "Wed, 28 Jun 2017 13:00:14 GMT  (7743kb,D)", "http://arxiv.org/abs/1706.09262v1", "Submitted to NIPS 2017. Code will be available atthis https URLand qualitative results are available atthis https URL"], ["v2", "Tue, 5 Sep 2017 14:35:08 GMT  (7771kb,D)", "http://arxiv.org/abs/1706.09262v2", "Published as a conference paper at NIPS 2017. Code is available atthis https URLand qualitative results are available atthis https URL"]], "COMMENTS": "Submitted to NIPS 2017. Code will be available atthis https URLand qualitative results are available atthis https URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["adam r kosiorek", "alex bewley", "ingmar posner"], "accepted": true, "id": "1706.09262"}, "pdf": {"name": "1706.09262.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Attentive Recurrent Tracking", "authors": ["Adam R. Kosiorek", "Alex Bewley"], "emails": ["adamk@robots.ox.ac.uk", "bewley@robots.ox.ac.uk", "ingmar@robots.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In computer vision, the task of class-agnostic object tracking is challenging since no target-specific model can be learnt a priori and yet the model has to handle target appearance changes, varying lighting conditions and occlusion. To make it even more difficult, the tracked object often constitutes but a small fraction of the visual field. The remaining parts may contain distractors, which are visually salient objects resembling the target but hold no relevant information. Despite this fact, recent models often process the whole image, exposing them to noise and increases in associated computational cost or use heuristic methods to decrease the size of search regions. This in contrast to human visual perception, which does not process the visual field in its entirety, but rather acknowledges it briefly and focuses on processing small fractions thereof, which we dub visual attention.\nAttention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7]. While they improve efficiency [22] and performance on state-of-the-art machine learning benchmarks [27], their architecture is much simpler than that of the mechanisms found in the human visual cortex [5]. Attention has also been long studied by neuroscientists [18], who believe that it is crucial for visual perception and cognition [4], since it is inherently tied to the architecture of the visual cortex and can affect the information flow inside it. Whenever more than one visual stimulus is present in the receptive field of a neuron, all the stimuli compete for computational resources due to the limited processing capacity. Visual attention can lead to suppression of distractors, by reducing\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n09 26\n2v 1\n[ cs\n.C V\n] 2\n8 Ju\nthe size of the receptive field of a neuron and by increasing sensitivity at a given location in the visual field (spatial attention). It can also amplify activity in different parts of the cortex, which are specialised in processing different types of features, leading to response enhancement w. r. t. those features (appearance attention). The functional separation of the visual cortex is most apparent in two distinct processing pathways. After leaving the eye, the sensory inputs enter the prefrontal cortex (known as V1) and then split into the dorsal stream, responsible for estimating spatial relationships (where), and the ventral stream, which targets appearance-based features (what).\nInspired by the general architecture of the human visual cortex and the role of attention mechanisms, this work presents a biologically-inspired recurrent model for single object tracking in videos (cf. section 3). Tracking algorithms typically use simple motion models and heuristics to decrease the size of the search region. It is interesting to see whether neuroscientific insights can aid our computational efforts, thereby improving the efficiency and performance of single object tracking. It is worth noting that visual attention can be induced by the stimulus itself (due to, e. g., high contrast) in a bottom-up fashion or by back-projections from other brain regions and working memory as top-down influence. The proposed approach exploits this property to create a feedback loop that steers the three layers of visual attention mechanisms in our hierarchical attentive recurrent tracking (HART) framework, see Figure 1. The first stage immediately discards spatially irrelevant input, while later stages focus on producing deictic filters to emphasise visual features particular to the object of interest.\nBy factoring the problem into its constituent parts, we arrive at a familiar statistical domain; namely that of maximum likelihood estimation (MLE). This follows from our interest in estimating the distribution over object locations, in a sequence of images, given the initial location from whence our tracking commenced. Formally, given a sequence of images x1:T \u2208 RH\u00d7W\u00d73 and an initial location for the tracked object given by a bounding box b1 \u2208 R4, the conditional probability distribution factorises as\np(b2:T | x1:T ,b1) = = \u222b p(h1 | x1,b1) T\u220f t=2 \u222b p(bt | ht)p(ht | xt,bt\u22121,ht\u22121) dht dh1, (1)\nwhere we assume that motion of an object can be described by a Markovian state ht. Our bounding box estimates are given by b\u03022:T , found by the MLE of the model parameters. In sum, our contributions are threefold: Firstly, a hierarchy of attention mechanisms that leads to suppressing distractors and computational efficiency is introduced. Secondly, a biologically plausible combination of attention mechanisms and recurrent neural networks is presented for object tracking. Finally, our attentionbased tracker is demonstrated using real-world sequences in challenging scenarios where previous recurrent attentive trackers have failed.\nNext we briefly review related work before describing how information flows through the components of our hierarchical attention in Section 3. Section 3 details the losses applied to guide attention. Section 5 presents experiments on KTH, KITTI and ImageNet video datasets with comparison to related neural network based trackers. Section 6 discusses the results and intriguing properties of our framework and Section 7 concludes the work. Code and results will be available online1.\n1https://github.com/akosiorek/hart"}, {"heading": "2 Related Work", "text": "A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [22, 12]. Such a paradigm has the intriguing property that the computational complexity is proportional to the number of steps as opposed to the image size. Furthermore, the fovea centralis in the retina of primates is structured with maximum visual acuity in the centre and decaying resolution towards the periphery, Cheung et al. [4] show that if spatial attention is capable of zooming, a regular grid sampling is sufficient. Jaderberg et al. [14] introduced the spatial transformer network (STN) which provides a fully differentiable means of transforming feature maps, conditioned on the input itself. Eslami et al. [7] use the STN as a form of attention in combination with a recurrent neural network (RNN) to sequentially locate and identify objects in an image. Moreover, Eslami et al. [7] use a latent variable to estimate the presence of additional objects, allowing the RNN to adapt the number of time-steps based on the input. Our spatial attention mechanism is based on the two dimensional Gaussian grid filters of [16] which is both fully differentiable and more biologically plausible than the STN.\nWhilst focusing on a specific location has its merits, focusing on particular appearance features might be as important. A policy with feedback connections can learn to adjust filters of a convolutional neural network (CNN), thereby adapting them to features present in the current image and improving accuracy [25]. De Brabandere et al. [6] introduced dynamic filter network (DFN), where filters for a CNN are computed on-the-fly conditioned on input features, which can reduce model size without performance loss. Karl et al. [17] showed that an input-dependent state transitions can be helpful for learning latent Markovian state-space system. While not the focus of this work, we follow this concept in estimating the expected appearance of the tracked object.\nIn the context of single object tracking, both attention mechanisms and RNNs appear to be perfectly suited, yet their success has mostly been limited to simple monochromatic sequences with plain backgrounds [16]. Cheung [3] applied STNs [14] as attention mechanisms for real-world object tracking, but failed due to exploding gradients potentially arising from the difficulty of the data. Ning et al. [23] achieved competitive performance by using features from an object detector as inputs to a long-short memory network (LSTM), but requires processing of the whole image at each time-step. Two recent state-of-the-art trackers employ convolutional Siamese networks which can be seen as an RNN unrolled over two time-steps [13, 26]. Both methods explicitly process small search areas around the previous target position to produce a bounding box offset [13] or a correlation response map with the maximum corresponding to the target position [26]. We acknowledge the recent work2 of Gordon et al. [10] which employ an RNN based model and use explicit cropping and warping as a form of non-differentiable spatial attention. The work presented in this paper is closest to [16] where we share a similar spatial attention mechanism which is guided through an RNN to effectively learn a motion model that spans multiple time-steps. The next section describes our additional attention mechanisms in relation to their biological counterparts."}, {"heading": "3 Hierarchical Attention", "text": "Inspired by the architecture of the human visual cortex, we structure our system around working memory responsible for storing the motion pattern and an appearance description of the tracked object. If both quantities are known, it would be possible to compute the expected location of the object at the next time step. Given a new frame, however, it is not immediately apparent which visual features correspond to the appearance description. If we were to pass them on to an RNN, it would have to implicitly solve a data association problem. As it is non-trivial, we prefer to model it explicitly by outsourcing the computation to a separate processing stream conditioned on the expected appearance. This results in a location-map, making it possible to neglect features inconsistent with our memory of the tracked object. We now proceed with describing the information flow in our model.\nGiven attention parameters at, the spatial attention module extracts a glimpse gt from the input image xt. We then apply appearance attention, parametrised by appearance \u03b1t and comprised of V1 and dorsal and ventral streams, to obtain object-specific features vt, which are used to update the hidden state ht of an LSTM. The LSTM\u2019s output is then decoded to predict both spatial and appearance attention parameters for the next time-step along with a bounding box correction \u2206b\u0302t for\n2[10] only became available at the time of submitting this paper.\nthe current time-step. Spatial attention is driven by top-down signal at, while appearance attention depends on top-down \u03b1t as well as bottom-up (contents of the glimpse gt) signals. Bottom-up signals have local influence and depend on stimulus salience at a given location, while top-down signals incorporate global context into local processing. This attention hierarchy, further enhanced by recurrent connections, mimics that of the human visual cortex [18]. We now describe the individual components of the system.\nSpatial Attention Our spatial attention mechanism is similar to the one used by Kaho\u00fa et al. [16]. Given an input image xt \u2208 RH\u00d7W , it creates two matrices Axt \u2208 Rw\u00d7W and A y t \u2208 Rh\u00d7H , respectively. Each matrix contains one Gaussian per row; the width and positions of the Gaussians determine which parts of the image are extracted as the attention glimpse. Formally, the glimpse gt \u2208 Rh\u00d7w is defined as\ngt = A y t xt (A x t ) T . (2)\nAttention is described by centres \u00b5 of the Gaussians, their variances \u03c32 and strides \u03b3 between centers of Gaussians of consecutive rows of the matrix, one for each axis. In contrast to the work by Kaho\u00fa et al. [16], only centres and strides are estimated from the hidden state of the LSTM, while the variance depends solely on the stride. This prevents excessive aliasing during training caused when predicting a small variance (w. r. t. strides) leading to smoother convergence. The relationship between variance and stride is approximated using linear regression with polynomial basis functions (up to 4th order) before training the whole system. The glimpse size we use depends on the experiment. Appearance Attention This stage transforms the attention glimpse gt into a fixed-dimensional vector vt comprising appearance and spatial information about the tracked object. Its architecture depends on the experiment. In general, however, we implement V1 : Rh\u00d7w \u2192 Rhv\u00d7wv\u00d7cv as a number of convolutional and max-pooling layers. They are shared among later processing stages, which corresponds to the primary visual cortex in humans [5]. Processing then splits into ventral and dorsal streams. The ventral stream is implemented as a CNN, and handles visual features and outputs feature maps \u03bdt. The dorsal stream, implemented as a DFN, is responsible for handling spatial relationships. Let MLP(\u00b7) denote a multi-layered perceptron. The dorsal stream uses appearance \u03b1t to dynamically compute convolutional filters \u03c8a\u00d7b\u00d7ct , where the superscript denotes the size of the filters and the number of feature maps, as\n\u03a8t = { \u03c8hi\u00d7bi\u00d7cit }K i=1 = MLP(\u03b1t). (3)\nThe filters with corresponding nonlinearities form K convolutional layers applied to the output of V1. Finally, a convolutional layer with a 1\u00d7 1 kernel and a sigmoid non-linearity is applied to transform the output into a spatial Bernoulli distribution st. Each value in st represents the probability of the tracked object occupying the corresponding location. The location map of the dorsal stream is combined with appearance-based features extracted by the ventral stream, to imitate the distractor-suppressing behaviour of the human brain. It also prevents drift and allows occlusion handling, since object appearance is not overwritten in the hidden state when input does not contain features particular to the tracked object. Outputs of both streams are combined as3\nvt = MLP(vec(\u03bdt st)), (4) with being the Hadamard product. State Estimation Our approach relies upon being able to predict future object appearance and location, and therefore it heavily depends upon state estimation. We use an LSTM, which can learn to trade-off spatio-temporal and appearance information in a data-driven fashion. It acts like a working memory, enabling the system to be robust to occlusions and oscillating object appearance e. g., when an object rotates and comes back to the original orientation.\not,ht = LSTM(vt,ht\u22121), (5)\n\u03b1t+1,\u2206at+1,\u2206b\u0302t = MLP(ot, vec(st)), (6)\nat+1 = at + tanh(c)\u2206at+1, (7)\nb\u0302t = at + \u2206b\u0302t (8)\nEquations (5) to (8) detail the state updates. Spatial attention at time t is formed as a cumulative sum of attention updates from times t = 1 to t = T , where c is a learnable parameter initialised to a small value to constrain the size of the updates early in training. Since the spatial-attention mechanism is trained to predict where the object is going to go (section 4), the bounding box b\u0302t is estimated relative to attention at time t."}, {"heading": "4 Loss", "text": "We train our system by minimising a loss function comprised of a: tracking loss term, a set of terms for auxiliary tasks and regularisation terms. Auxiliary tasks are essential for real-world data, since convergence does not occur without them. They also speed up learning and lead to better performance for simpler datasets. Unlike the auxiliary tasks used by Jaderberg et al. [15], ours are relevant for our main objective \u2014 object tracking. In order to limit the number of hyperparameters, we automatically learn loss weighting. The loss L(\u00b7) is given by\nLHART(D, \u03b8) = \u03bbtLt(D, \u03b8) + \u03bbsLs(D, \u03b8) + \u03bbaLa(D, \u03b8) +R(\u03bb) + \u03b2R(D, \u03b8), (9)\nwith dataset D = { (x1:T ,b1:T ) i }M i=1\n, network parameters \u03b8, regularisation terms R(\u00b7), adaptive weights \u03bb = {\u03bbt, \u03bbs, \u03bbd} and a regularisation weight \u03b2. We now present and justify components of our loss, where expectations E[\u00b7] are evaluated as an empirical mean over a minibatch of samples{ xi1:T ,b i 1:T }M i=1 , where M is the batch size.\nTracking To achieve the main tracking objective (localising the object in the current frame), we base the first loss term on Intersection-over-Union (IoU) of the predicted bounding box w. r. t. the ground truth, where the IoU of two bounding boxes is defined as IoU(a, b) = a\u2229ba\u222ab = area of overlap area of union . The IoU is invariant to object and image scale, making it a suitable proxy for measuring the quality of localisation. Even though it (or an exponential thereof) does not correspond to any probability\n3vec : Rm\u00d7n \u2192 Rmn is the vecorisation operator, which stacks columns of a matrix into a column vector.\ndistribution (as it cannot be normalised), it is often used for evaluation [20]. We follow the work by Yu et al. [28] and express the loss term as the negative log of IoU:\nLt(D, \u03b8) = Ep(b\u03021:T |x1:T ,b1) [ \u2212 log IoU(b\u0302t,bt) ] , (10)\nwith IoU clipped for numerical stability. Spatial Attention Spatial attention singles out the tracked object from the image. To estimate its parameters, the system has to predict the object\u2019s motion. In case of an error, especially when the attention glimpse does not contain the tracked object, it is difficult to recover. As the probability of such an event increases with decreasing size of the glimpse, we employ two loss terms. The first one constrains the predicted attention to cover the bounding box, while the second one prevents it from becoming too large, with logarithmic arguments are appropriately clipped to avoid numerical instabilities:\nLs(D, \u03b8) = Ep(a1:T |x1:T ,b1) [ \u2212 log ( at \u2229 bt\narea(bt)\n) \u2212 log(1\u2212 IoU(at,xt)) ] . (11)\nAppearance Attention The purpose of appearance attention is to suppress distractors while keeping full view of the tracked object e. g., focus on a particular pedestrian moving within a group. To guide this behaviour, we put a loss on appearance attention that encourages picking out only the tracked object. Let \u03c4(at,bt) : R4 \u00d7 R4 \u2192 {0, 1}hv\u00d7wv be a target function. Given the bounding box b and attention a, it outputs a binary mask of the same size as the output of V1. The mask corresponds to the the glimpse g, with the value equal to one at every location where the bounding box overlaps with the glimpse and equal to zero otherwise. If we take H(p, q) = \u2212 \u2211 z p(z) log q(z) to be the cross-entropy, the loss reads\nLa(D, \u03b8) = Ep(a1:T ,s1:T |x1:T ,b1)[H(\u03c4(at,bt), st)]. (12)\nRegularisation We apply the L2 regularisation to the model parameters \u03b8 and to the expected value of dynamic parameters \u03c8t(\u03b1t) as R(D, \u03b8) = 12\u2016\u03b8\u2016 2 2 + 1 2 \u2225\u2225Ep(\u03b11:T |x1:T ,b1)[\u03a8t | \u03b1t]\u2225\u222522. Adaptive Loss Weights To avoid hyper-parameter tuning, we follow the work by Kendall et al. [19] and learn the loss weighting \u03bb. After initialising the weights with a vector of ones, we add the following regularisation term to the loss function: R(\u03bb) = \u2212 \u2211 i log(\u03bb \u22121 i )."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 KTH Pedestrian Tracking", "text": "Kaho\u00fa et al. [16] performed a pedestrian tracking experiment on the KTH activity recognition dataset [24] as a real-world case-study. We replicate this experiment for comparison. We use code provided by the authors for data preparation and we also use their pre-trained feature extractor. Unlike them, we did not need to upscale ground-truth bounding boxes by a factor of 1.5 and then downscale them again for evaluation. We follow the authors and set the glimpse size (h,w) = (28, 28). We\nreplicate the training procedure exactly, with the exception of using the RMSProp optimiser [9] with learning rate of 3.33 \u00d7 10\u22125 and momentum set to 0.9 instead of the stochastic gradient descent with momentum. The original work reported an IoU of 55.03% on average, on test data, while the presented work achieves an average IoU score of 77.11%, reducing the relative error by almost a factor of two. Figure 4 presents qualitative results."}, {"heading": "5.2 Scaling to Real-World Data: KITTI", "text": "Since we demonstrated that pedestrian tracking is feasible using the proposed architecture, we proceed to evaluate our model in a more challenging multi-class scenario on the KITTI dataset [8]. It consists of 21 high resolution video sequences with multiple instances of the same class posing as potential distractors. We split all sequences into 80/20 sequences for train and test sets, respectively. As images in this dataset are much more varied, we implement V1 as the first three convolutional layers of a modified AlexNet [1]. The original AlexNet takes inputs of size 227\u00d7 227 and downsizes them to 14\u00d7 14 after conv3 layer. Since too low resolution would result in low tracking performance, and we did not want to upsample the extracted glimpse, we decided to replace the initial stride of four with one and to skip one of the max-pooling operations to conserve spatial dimensions. This way, our feature map has the size of 14\u00d7 14\u00d7 384 with the input glimpse of size (h,w) = (56, 56). We apply dropout with probability 0.25 at the end of V1. The ventral stream is comprised of a single convolutional layer with a 1 \u00d7 1 kernel and five output feature maps. The dorsal stream has two dynamic filter layers with kernels of size 1\u00d7 1 and 3\u00d7 3, respectively and five feature maps each. We used 100 hidden units in the RNN with orthogonal initialisation and Zoneout [21] with probability set to 0.05. The system was trained via curriculum learning [2], by starting with sequences of length five and increasing sequence length every 13 epochs, with epoch length decreasing with increasing sequence length. We used the same optimisation settings, with the exception of the learning rate, which we set to 3.33\u00d7 10\u22126. Table 1 and Figure 5 contain results of different variants of our model and of the RATM tracker by Kaho\u00fa et al. [16] related works. Spatial Att does not use appearance attention, nor loss on attention parameters. App Att does not apply any loss on appearance attention, while HART uses all described modules; it is also our biggest model with 1.8 million parameters. Qualitative results in the form of a video with bounding boxes and attention are available online 4. We implemented the RATM tracker of Kaho\u00fa et al. [16] and trained with the same hyperparameters as our framework, since both are closely related. It failed to learn even with the initial curriculum of five time-steps, as RATM cannot integrate the frame xt into the estimate of bt (it predicts location at the next time-step). Furthermore, it uses feature-space distance between ground-truth and predicted attention glimpses as the error measure, which is insufficient on a dataset with rich backgrounds. It did better when we initialised its feature extractor with weights of our trained model but, despite passing a few stags of the curriculum, it achieved very poor final performance.\n(a) The model with appearance attention loss (top) learns to focus on the tracked object, which prevents an ID swap when a pedestrian is occluded by another one (bottom).\n(b) Three examples of glimpses and locations maps for a model with and without appearance loss (left to right). Attention loss forces the appearance attention to pick out only the tracked object, thereby suppressing distractors.\nFigure 6: Glimpses and corresponding location maps for models trained with and without appearance loss. The appearance loss encourages the model to learn foreground/background segmentation of the input glimpse."}, {"heading": "6 Discussion", "text": "The experiments in the previous section show that it is possible to track real-world objects with a recurrent attentive tracker. While similar to the tracker by Kaho\u00fa et al. [16], our approach uses additional building blocks, specifically: (i) bounding-box regression loss, (ii) loss on spatial attention, (iii) appearance attention with an additional loss term, and (iv) combines all of these in a unified approach. We now discuss properties of these modules.\nSpatial Attention Loss prevents Vanishing Gradients Our early experiments suggest that using only the tracking loss causes an instance of the vanishing gradient problem. Early in training, the system is not able to estimate object\u2019s motion correctly, leading to cases where the extracted glimpse does not contain the tracked object or contains only a part thereof. In such cases, the supervisory signal is only weakly correlated with the model\u2019s input, which prevents learning. Even when the object is contained within the glimpse, the gradient path from the loss function is rather long, since any teaching signal has to pass to the previous timestep through the feature extractor stage. Penalising attention parameters directly seems to solve this issue.\nIs Appearance Attention Loss Necessary? Given enough data and sufficiently high model capacity, appearance attention should be able to filter out irrelevant input features before updating the working memory. In general, however, this behaviour can be achieved faster if the model is constrained to do so by using an appropriate loss. Figure 6 shows examples of glimpses and corresponding location maps for a model with and without loss on the appearance attention. In fig. 6a the model with loss on appearance attention is able to track a pedestrian even after it was occluded by another human. Figure 6b shows that, when not penalised, location map might not be very object-specific and can miss the object entirely (left-most figure). By using the appearance attention loss, we not only improve results but also make the model more interpretable.\nSpatial Attention Bias is Always Positive To condition the system on the object\u2019s appearance and make it independent of the starting location, we translate the initial bounding box to attention parameters, to which we add a learnable bias, and create the hidden state of LSTM from corresponding visual features. In out experiments, this bias always converged to positive values favouring attention glimpse slightly larger than the object bounding box. It suggests that, while discarding irrelevant features is desirable for object tracking, the system as a whole learns to trade off attention responsibility between the spatial and appearance based attention modules.\n4https://youtu.be/Vvkjm0FRGSs"}, {"heading": "7 Conclusion", "text": "Inspired by the cascaded attention mechanisms found in the human visual cortex, this work presented a neural attentive recurrent tracking architecture suited for the task of object tracking. Beyond the biological inspiration, the proposed approach has a desirable computational cost and increased interpretability due to location maps, which select features essential for tracking. Furthermore, by introducing a set of auxiliary losses we are able to scale to challenging real world data, outperforming predecessor attempts and approaching state-of-the-art performance. Future research will look into extending the proposed approach to multi-object tracking, as unlike many single object tracking, the recurrent nature of the proposed tracker offer the ability to attend each object in turn."}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "Geoffrey E. Hinton"], "venue": "In NIPS, pages 1097\u20131105,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Neural Attention for Object Tracking", "author": ["Brian Cheung"], "venue": "In GPU Technology Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Emergence of foveal image sampling from learning to attend", "author": ["Brian Cheung", "Eric Weiss", "Bruno Olshausen"], "venue": "in visual scenes. Arxiv,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Theoretical neuroscience : computational and mathematical modeling of neural systems", "author": ["Peter. Dayan", "L.F. Abbott"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models", "author": ["S.M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "David Szepesvari", "Koray Kavukcuoglu", "Geoffrey E. Hinton"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Vision meets robotics: The KITTI dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Real-Time Recurrent Regression Networks for Object Tracking", "author": ["Daniel Gordon", "Ali Farhadi", "Dieter Fox"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adri\u00e0 Puigdom\u00e8nech Badia", "Karl Moritz Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": "URL http: //dx.doi.org/10.1038/nature20101http://10.0.4.14/nature20101http://www.nature", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "DRAW: A Recurrent Neural Network For Image Generation", "author": ["K Gregor", "I Danihelka", "A Graves", "D Wierstra"], "venue": "arXiv preprint arXiv:", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning to track at 100 FPS with deep regression networks", "author": ["David Held", "Sebastian Thrun", "Silvio Savarese"], "venue": "In European Conference on Computer Vision Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1946}, {"title": "Spatial Transformer Networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "venue": "In Nips, pages 1\u201314,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Reinforcement Learning with Unsupervised Auxiliary Tasks. In arXiv:1611.05397, nov 2016", "author": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "venue": "ISBN 2004012439", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "RATM: Recurrent Attentive Tracking", "author": ["Samira Ebrahimi Kaho\u00fa", "Vincent Michalski", "Roland Memisevic"], "venue": "Model. Iclr,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "author": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "venue": "In International Conference 9  on Learning Representation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "Mechanisms of visual attention in the human cortex", "author": ["Sabine Kastner", "Leslie G Ungerleider"], "venue": "Annual Reviews of Neuroscience,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. arXiv:1705.07115, may 2017", "author": ["Alex Kendall", "Yarin Gal", "Roberto Cipolla"], "venue": "URL http://arxiv.org/abs/1705", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "The Visual Object Tracking VOT2016 challenge results", "author": ["Matej Kristan", "Jiri Matas", "Ale\u0161 Leonardis", "Michael Felsberg", "Luk Cehovin", "Gustavo Fern\u00e1ndez", "Tom\u00e1\u0161 Voj\u00ed", "Gustav H\u00e4ger", "Georg Nebehay", "Roman Pflugfelder", "Abhinav Gupta", "Adel Bibi", "Alan Luke\u017ei\u010d", "Alvaro Garcia-Martin", "Amir Saffari", "Philip H S Torr", "Qiang Wang", "Rafael Martin-Nieto", "Rengarajan Pelapur", "Richard Bowden", "Chun Zhu", "Stefan Becker", "Stefan Duffner", "Stephen L Hicks", "Stuart Golodetz", "Sunglok Choi", "Tianfu Wu", "Thomas Mauthner", "Tony Pridmore", "Weiming Hu", "Wolfgang H\u00fcbner", "Xiaomeng Wang", "Xin Li", "Xinchu Shi", "Xu Zhao", "Xue Mei", "Yao Shizeng", "Yang Hua", "Yang Li", "Yang Lu", "Yuezun Li", "Zhaoyun Chen", "Zehua Huang", "Zhe Chen", "Zhe Zhang", "Zhenyu He", "Zhibin Hong"], "venue": "In European Conference on Computer Vision Workshop,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Aaron Courville", "Chris Pal"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Recurrent Models of Visual Attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking", "author": ["Guanghan Ning", "Zhi Zhang", "Chen Huang", "Zhihai He", "Xiaobo Ren", "Haohong Wang"], "venue": "arXiv preprint arXiv:1607.05781,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Recognizing human actions: A local SVM approach", "author": ["Christian Schuldt", "Ivan Laptev", "Barbara Caputo"], "venue": "In Proceedings - International Conference on Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Deep Networks with Internal Selective Attention through Feedback Connections", "author": ["Marijn Stollenga", "Jonathan Masci", "Faustino Gomez", "Juergen Schmidhuber"], "venue": "In arXiv preprint arXiv:", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "End-to-end representation learning for Correlation Filter based tracking", "author": ["Jack Valmadre", "Luca Bertinetto", "Jo\u00e3o F. Henriques", "Andrea Vedaldi", "Philip H.S. Torr"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "UnitBox: An Advanced Object Detection Network", "author": ["Jiahui Yu", "Yuning Jiang", "Zhangyang Wang", "Zhimin Cao", "Thomas Huang"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 24, "context": "Attention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7].", "startOffset": 99, "endOffset": 107}, {"referenceID": 11, "context": "Attention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7].", "startOffset": 99, "endOffset": 107}, {"referenceID": 8, "context": "Attention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7].", "startOffset": 173, "endOffset": 184}, {"referenceID": 9, "context": "Attention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7].", "startOffset": 173, "endOffset": 184}, {"referenceID": 5, "context": "Attention mechanisms have recently been explored in machine learning in a wide variety of contexts [27, 14], often providing new capabilities to machine learning algorithms [11, 12, 7].", "startOffset": 173, "endOffset": 184}, {"referenceID": 19, "context": "While they improve efficiency [22] and performance on state-of-the-art machine learning benchmarks [27], their architecture is much simpler than that of the mechanisms found in the human visual cortex [5].", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "While they improve efficiency [22] and performance on state-of-the-art machine learning benchmarks [27], their architecture is much simpler than that of the mechanisms found in the human visual cortex [5].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "While they improve efficiency [22] and performance on state-of-the-art machine learning benchmarks [27], their architecture is much simpler than that of the mechanisms found in the human visual cortex [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 15, "context": "Attention has also been long studied by neuroscientists [18], who believe that it is crucial for visual perception and cognition [4], since it is inherently tied to the architecture of the visual cortex and can affect the information flow inside it.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Attention has also been long studied by neuroscientists [18], who believe that it is crucial for visual perception and cognition [4], since it is inherently tied to the architecture of the visual cortex and can affect the information flow inside it.", "startOffset": 129, "endOffset": 132}, {"referenceID": 19, "context": "A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [22, 12].", "startOffset": 133, "endOffset": 141}, {"referenceID": 9, "context": "A number of recent studies have demonstrated that visual content can be captured through a sequence of spatial glimpses or foveation [22, 12].", "startOffset": 133, "endOffset": 141}, {"referenceID": 3, "context": "[4] show that if spatial attention is capable of zooming, a regular grid sampling is sufficient.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[14] introduced the spatial transformer network (STN) which provides a fully differentiable means of transforming feature maps, conditioned on the input itself.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] use the STN as a form of attention in combination with a recurrent neural network (RNN) to sequentially locate and identify objects in an image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] use a latent variable to estimate the presence of additional objects, allowing the RNN to adapt the number of time-steps based on the input.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Our spatial attention mechanism is based on the two dimensional Gaussian grid filters of [16] which is both fully differentiable and more biologically plausible than the STN.", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "A policy with feedback connections can learn to adjust filters of a convolutional neural network (CNN), thereby adapting them to features present in the current image and improving accuracy [25].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "[17] showed that an input-dependent state transitions can be helpful for learning latent Markovian state-space system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "In the context of single object tracking, both attention mechanisms and RNNs appear to be perfectly suited, yet their success has mostly been limited to simple monochromatic sequences with plain backgrounds [16].", "startOffset": 207, "endOffset": 211}, {"referenceID": 2, "context": "Cheung [3] applied STNs [14] as attention mechanisms for real-world object tracking, but failed due to exploding gradients potentially arising from the difficulty of the data.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "Cheung [3] applied STNs [14] as attention mechanisms for real-world object tracking, but failed due to exploding gradients potentially arising from the difficulty of the data.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "[23] achieved competitive performance by using features from an object detector as inputs to a long-short memory network (LSTM), but requires processing of the whole image at each time-step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Two recent state-of-the-art trackers employ convolutional Siamese networks which can be seen as an RNN unrolled over two time-steps [13, 26].", "startOffset": 132, "endOffset": 140}, {"referenceID": 23, "context": "Two recent state-of-the-art trackers employ convolutional Siamese networks which can be seen as an RNN unrolled over two time-steps [13, 26].", "startOffset": 132, "endOffset": 140}, {"referenceID": 10, "context": "Both methods explicitly process small search areas around the previous target position to produce a bounding box offset [13] or a correlation response map with the maximum corresponding to the target position [26].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Both methods explicitly process small search areas around the previous target position to produce a bounding box offset [13] or a correlation response map with the maximum corresponding to the target position [26].", "startOffset": 209, "endOffset": 213}, {"referenceID": 7, "context": "[10] which employ an RNN based model and use explicit cropping and warping as a form of non-differentiable spatial attention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The work presented in this paper is closest to [16] where we share a similar spatial attention mechanism which is guided through an RNN to effectively learn a motion model that spans multiple time-steps.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "[10] only became available at the time of submitting this paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This attention hierarchy, further enhanced by recurrent connections, mimics that of the human visual cortex [18].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], only centres and strides are estimated from the hidden state of the LSTM, while the variance depends solely on the stride.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "They are shared among later processing stages, which corresponds to the primary visual cortex in humans [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "[15], ours are relevant for our main objective \u2014 object tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Figure 4: Tracking results on KTH dataset [24].", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "distribution (as it cannot be normalised), it is often used for evaluation [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "[28] and express the loss term as the negative log of IoU:", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] and learn the loss weighting \u03bb.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] performed a pedestrian tracking experiment on the KTH activity recognition dataset [24] as a real-world case-study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[16] performed a pedestrian tracking experiment on the KTH activity recognition dataset [24] as a real-world case-study.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "[16] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Since we demonstrated that pedestrian tracking is feasible using the proposed architecture, we proceed to evaluate our model in a more challenging multi-class scenario on the KITTI dataset [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "As images in this dataset are much more varied, we implement V1 as the first three convolutional layers of a modified AlexNet [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 18, "context": "We used 100 hidden units in the RNN with orthogonal initialisation and Zoneout [21] with probability set to 0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "The system was trained via curriculum learning [2], by starting with sequences of length five and increasing sequence length every 13 epochs, with epoch length decreasing with increasing sequence length.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "[16] related works.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] and trained with the same hyperparameters as our framework, since both are closely related.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16], our approach uses additional building blocks, specifically: (i) bounding-box regression loss, (ii) loss on spatial attention, (iii) appearance attention with an additional loss term, and (iv) combines all of these in a unified approach.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate \u201cwhere\u201d and \u201cwhat\u201d processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets of increasing difficulty: pedestrian tracking on the KTH activity recognition dataset and the KITTI object tracking dataset.", "creator": "LaTeX with hyperref package"}}}