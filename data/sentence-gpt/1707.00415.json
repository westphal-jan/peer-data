{"id": "1707.00415", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2017", "title": "Dual Supervised Learning", "abstract": "Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. English-to-English translations. However, in a study of the comparative literature on learning in the UK, an English-language language analysis showed that the effect of supervised learning on the number of participants in the study is still uncertain. The majority of the study participants were from the UK, who had studied for more than 10 years prior, who had been at the time of the study. The effect of supervised learning on the number of participants in the study on the number of participants in the study is unclear, but a few years later, participants in the study were in the UK, and the UK experienced increased number of years of supervised learning. These findings also indicate that supervised learning affects the number of participants in the study. One factor affecting the number of participants in the study is the difficulty in interpreting the word \u2010 (or other words), as well as the fact that supervised learning affects the number of participants in the study. As in the previous paper, the problem of supervised learning was the fact that it is difficult to interpret the word \u2010 (or other words), as well as the fact that supervised learning affects the number of participants in the study. One factor affecting the number of participants in the study is the difficulty in interpreting the word \u2010 (or other words), as well as the fact that supervised learning affects the number of participants in the study. This can lead to poor quality of life and a shortage of jobs and resources. In contrast, supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. This can lead to poor quality of life and a shortage of jobs and resources. In contrast, supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. This can lead to poor quality of life and a shortage of jobs and resources. In contrast, supervised learning affects the number of participants in the study. The problem of supervised learning affects the number of participants in the study. This can lead to poor quality of life and a shortage of jobs and resources. In contrast, supervised learning affects the number of participants in the study", "histories": [["v1", "Mon, 3 Jul 2017 06:19:31 GMT  (1152kb,D)", "http://arxiv.org/abs/1707.00415v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yingce xia", "tao qin", "wei chen", "jiang bian", "nenghai yu", "tie-yan liu"], "accepted": true, "id": "1707.00415"}, "pdf": {"name": "1707.00415.pdf", "metadata": {"source": "META", "title": "Dual Supervised Learning", "authors": ["Yingce Xia", "Tao Qin", "Wei Chen", "Jiang Bian", "Nenghai Yu", "Tie-Yan Liu"], "emails": ["<taoqin@microsoft.com>."], "sections": [{"heading": "1. Introduction", "text": "Deep learning brings state-of-the-art results to many artificial intelligence tasks, such as neural machine translation (Wu et al., 2016), image classification (He et al., 2016b;c), image generation (van den Oord et al., 2016b;a), speech recognition (Graves et al., 2013; Amodei et al., 2016), and speech generation/synthesis (Oord et al., 2016).\nInterestingly, we find that many of the aforementioned AI tasks are emerged in dual forms, i.e., the input and output of one task are exactly the output and input of the other task respectively. Examples include translation from language A to language B vs. translation from language B to A, image classification vs. image generation, and speech\n1School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China 2Microsoft Research, Beijing, China. Correspondence to: Tao Qin <taoqin@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nrecognition vs. speech synthesis. Even more interestingly (and somehow surprisingly), this natural duality is largely ignored in the current practice of machine learning. That is, despite the fact that two tasks are dual to each other, people usually train them independently and separately. Then a question arises: Can we exploit the duality between two tasks, so as to achieve better performance for both of them? In this work, we give a positive answer to the question.\nTo exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task and its dual task. The primal task takes a sample from space X as input and maps to space Y , and the dual task takes a sample from space Y as input and maps to space X . Using the language of probability, the primal task learns a conditional distribution P (y|x; \u03b8xy) parameterized by \u03b8xy , and the dual task learns a conditional distribution P (x|y; \u03b8yx) parameterized by \u03b8yx, where x \u2208 X and y \u2208 Y . In the new scheme, the two dual tasks are jointly learned and their structural relationship is exploited to improve the learning effectiveness. We name this new scheme as dual supervised learning (briefly, DSL).\nThere could be many different ways of exploiting the duality in DSL. In this paper, we use it as a regularization term to govern the training process. Since the joint probability P (x, y) can be computed in two equivalent ways: P (x, y) = P (x)P (y|x) = P (y)P (x|y), for any x \u2208 X , y \u2208 Y , ideally the conditional distributions of the primal and dual tasks should satisfy the following equality:\nP (x)P (y|x; \u03b8xy) = P (y)P (x|y; \u03b8yx). (1) However, if the two models (conditional distributions) are learned separately by minimizing their own loss functions (as in the current practice of machine learning), there is no guarantee that the above equation will hold. The basic idea of DSL is to jointly learn the two models \u03b8xy and \u03b8yx by minimizing their loss functions subject to the constraint of Eqn.(1). By doing so, the intrinsic probabilistic connection between \u03b8yx and \u03b8xy are explicitly strengthened, which is supposed to push the learning process towards the right direction. To solve the constrained optimization problem of DSL, we convert the constraint Eqn.(1) to a penalty term by using the method of Lagrange multipliers (Boyd & Vandenberghe, 2004). Note that the penalty term could also be seen as a data-dependent regularization term.\nar X\niv :1\n70 7.\n00 41\n5v 1\n[ cs\n.L G\n] 3\nJ ul\n2 01\n7\nTo demonstrate the effectiveness of DSL, we apply it to three artificial intelligence applications 1:\n(1) Neural Machine Translation (NMT) We first apply DSL to NMT, which formulates machine translation as a sequence-to-sequence learning problem, with the sentences in the source language as inputs and those in the target language as outputs. The input space and output space of NMT are symmetric, and there is almost no information loss while mapping from x to y or from y to x. Thus, symmetric tasks in NMT fits well into the scope of DSL. Experimental studies illustrate significant accuracy improvements by applying DSL to NMT: +2.07/0.86 points measured by BLEU scores for English\u2194French translation, +1.37/0.12 points for English\u2194Germen translation and +0.74/1.69 points on English\u2194Chinese.\n(2) Image Processing We then apply DSL to image processing, in which the primal task is image classification and the dual task is image generation conditioned on category labels. Both tasks are hot research topics in the deep learning community. We choose ResNet (He et al., 2016b) as our baseline for image classification, and PixelCNN++(Salimans et al., 2017) as our baseline for image generation. Experimental results show that on CIFAR-10, DSL could reduce the error rate of ResNet-110 from 6.43 to 5.40 and obtain a better image generation model with both clearer images and smaller bits per dimension. Note that these primal and dual tasks do not yield a pair of completely symmetric input and output spaces since there is information loss while mapping from an image to its class label. Therefore, our experimental studies reveal that DSL can also work well for dual tasks with information loss.\n(3) Sentiment Analysis Finally, we apply DSL to sentiment analysis, in which the primal task is sentiment classification (i.e., to predict the sentiment of a given sentence) and the dual one is sentence generation with given sentiment polarity. Experiments on the IMDB dataset show that DSL can improve the error rate of a widely-used sentiment classification model by 0.9 point, and can generate sentences with clearer/richer styles of sentiment expression.\nAll of above experiments on real artificial intelligence applications have demonstrated that DSL can improve practical performance of both tasks, simultaneously."}, {"heading": "2. Framework", "text": "In this section, we formulate the problem of dual supervised learning (DSL), describe an algorithm for DSL, and discuss its connections with existing learning schemes and\n1In our experiments, we chose the most cited models with either open-source codes or enough implementation details, to ensure that we can reproduce the results reported in previous papers. All of our experiments are done on a single Telsa K40m GPU.\nits application scope."}, {"heading": "2.1. Problem Formulation", "text": "To exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task that takes a sample from space X as input and maps to space Y , and a dual task takes a sample from space Y as input and maps to space X .\nAssume we have n training pairs {(xi, yi)}ni=1 i.i.d. sampled from the space X \u00d7 Y according to some unknown distribution P . Our goal is to reveal the bi-directional relationship between the two inputs x and y. To be specific, we perform the following two tasks: (1) the primal learning task aims at finding a function f : X 7\u2192 Y such that the prediction of f for x is similar to its real counterpart y; (2) the dual learning task aims at finding a function g : Y 7\u2192 X such that the prediction of g for y is similar to its real counterpart x. The dissimilarity is penalized by a loss function. Given any (x, y), let `1(f(x), y) and `2(g(y), x) denote the loss functions for f and g respectively, both of which are mappings from X \u00d7 Y to R.\nA common practice to design (f, g) is the maximum likelihood estimation based on the parameterized conditional distributions P (\u00b7|x; \u03b8xy) and P (\u00b7|y; \u03b8yx):\nf(x; \u03b8xy) , arg max y\u2032\u2208Y\nP (y\u2032|x; \u03b8xy),\ng(y; \u03b8yx) , arg max x\u2032\u2208X\nP (x\u2032|y; \u03b8yx),\nwhere \u03b8xy and \u03b8yx are the parameters to be learned.\nBy standard supervised learning, the primal model f is learned by minimizing the empirical risk in space Y:\nmin\u03b8xy (1/n) \u2211n i=1`1(f(xi; \u03b8xy), yi);\nand dual model g is learned by minimizing the empirical risk in space X :\nmin\u03b8yx(1/n) \u2211n i=1`2(g(yi; \u03b8yx), xi).\nGiven the duality of the primal and dual tasks, if the learned primal and dual models are perfect, we should have\nP (x)P (y|x; \u03b8xy) = P (y)P (x|y; \u03b8yx) = P (x, y),\u2200x, y.\nWe call this property probabilistic duality, which serves as a necessary condition for the optimality of the learned two dual models.\nBy the standard supervised learning scheme, probabilistic duality is not considered during the training, and the primal and the dual models are trained independently and separately. Thus, there is no guarantee that the learned dual models can satisfy probabilistic duality. To tackle this problem, we propose explicitly reinforcing the empirical probabilistic duality of the dual modes by solving the fol-\nlowing multi-objective optimization problem instead:\nobjective 1: min \u03b8xy\n(1/n) \u2211n i=1`1(f(xi; \u03b8xy), yi),\nobjective 2: min \u03b8yx\n(1/n) \u2211n i=1`2(g(yi; \u03b8yx), xi),\ns.t. P (x)P (y|x; \u03b8xy) = P (y)P (x|y; \u03b8yx),\u2200x, y,\n(2)\nwhere P (x) and P (y) are the marginal distributions. We call this new learning scheme dual supervised learning (abbreviated as DSL).\nWe provide a simple theoretical analysis which shows that DSL has theoretical guarantees in terms of generalization bound. Since the analysis is straightforward, we put it in Appendix A."}, {"heading": "2.2. Algorithm Description", "text": "In practical artificial intelligence applications, the groundtruth marginal distributions are usually not available. As an alternative, we use the empirical marginal distributions P\u0302 (x) and P\u0302 (y) to fulfill the constraint in Eqn.(2).\nTo solve the DSL problem, following the common practice in constraint optimization, we introduce Lagrange multipliers and add the equality constraint of probabilistic duality into the objective functions. First, we convert the probabilistic duality constraint into the following regularization term (with the empirical marginal distributions included):\n`duality =(log P\u0302 (x) + logP (y|x; \u03b8xy) \u2212 log P\u0302 (y)\u2212 logP (x|y; \u03b8yx))2.\n(3)\nThen, we learn the models of the two tasks by minimizing the weighted combination between the original loss functions and the above regularization term. The algorithm is shown in Algorithm 1.\nIn the algorithm, the choice of optimizers Opt1 and Opt2 is quite flexible. One can choose different optimizers such as Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014), or SGD for different tasks, depending on common practice in the specific task and personal preferences."}, {"heading": "2.3. Discussions", "text": "The duality between tasks has been used to enable learning from unlabeled data in (He et al., 2016a). As an early attempt to exploit the duality, this work actually uses the exterior connection between dual tasks, which helps to form a closed feedback loop and enables unsupervised learning. For example, in the application of machine translation, the primal task/model first translates an unlabeled English sentence x to a French sentence y\u2032; then, the dual task/model translates y\u2032 back to an English sentence x\u2032; finally, both the primal and the dual models get optimized by minimiz-\nAlgorithm 1 Dual Supervise Learning Algorithm Input: Marginal distributions P\u0302 (xi) and P\u0302 (yi) for any i \u2208 [n]; Lagrange parameters \u03bbxy and \u03bbyx; optimizers Opt1 and Opt2; repeat\nGet a minibatch of m pairs {(xj , yj)}mj=1; Calculate the gradients as follows:\nGf = \u2207\u03b8xy (1/m) \u2211m j=1 [ `1(f(xj ; \u03b8xy), yj)\n+ \u03bbxy`duality(xj , yj ; \u03b8xy, \u03b8yx) ] ;\nGg = \u2207\u03b8yx(1/m) \u2211m j=1 [ `2(g(yj ; \u03b8yx), xj)\n+ \u03bbyx`duality(xj , yj ; \u03b8xy, \u03b8yx) ] ;\n(4)\nUpdate the parameters of f and g: \u03b8xy \u2190 Opt1(\u03b8xy, Gf ), \u03b8yx \u2190 Opt2(\u03b8yx, Gg).\nuntil models converged\ning the difference between x\u2032 with x. In contrast, by making use of the intrinsic probabilistic connection between the primal and dual models, DSL takes an innovative attempt to extend the benefit of duality to supervised learning.\nWhile `duality can be regarded as a regularization term, it is data dependent, which makes DSL different from Lasso (Tibshirani, 1996) or SVM (Hearst et al., 1998), where the regularization term is data-independent. More accurately speaking, in DSL, every training sample contributes to the regularization term, and each model contributes to the regularization of the other model.\nDSL is different from the following three learning schemes: (1) Co-training focuses on single-task learning and assumes that different subsets of features can provide enough and complementary information about data, while DSL targets at learning two tasks with structural duality simultaneously and does not yield any prerequisite or assumptions on features. (2) Multi-task learning requires that different tasks share the same input space and coherent feature representation while DSL does not. (3) Transfer Learning uses auxiliary tasks to boost the main task, while there is no difference between the roles of two tasks in DSL, and DSL enables them to boost the performance of each other simultaneously.\nWe would like to point that there are several requirements to apply DSL to a certain scenario: (1) Duality should exist for the two tasks. (2) Both the primal and dual models should be trainable. (3) P\u0302 (X) and P\u0302 (Y ) in Eqn. (3) should be available. If these conditions are not satisfied, DSL might not work very well. Fortunately, as we have discussed in the paper, many machine learning tasks related to image, speech, and text satisfy these conditions."}, {"heading": "3. Application to Machine Translation", "text": "We first apply our dual supervised learning algorithm to machine translation and study whether it can improve the translation qualities by utilizing the probabilistic duality of dual translation tasks. In the following of the section, we perform experiments on three pairs of dual tasks 2: English\u2194French (En\u2194Fr), English\u2194Germany (En\u2194De), and English\u2194Chinese (En\u2194Zh)."}, {"heading": "3.1. Settings", "text": "Datasets We employ the same datasets as used in (Jean et al., 2015) to conduct experiments on En\u2194Fr and En\u2194De. As a part of WMT\u201914, the training data consists of 12M sentences pairs for En\u2194Fr and 4.5M for En\u2194De, respectively (WMT, 2014). We combine newstest2012 and newstest2013 together as the validation sets and use newstest2014 as the test sets. For the dual tasks of En\u2194Zh, we use 10M sentence pairs obtained from a commercial company as training data. We leverage NIST2006 as the validation set and NIST2008 as well as NIST2012 as the test sets3. Note that, during the training of all three pairs of dual tasks, we drop all sentences with more than 50 words.\nMarginal Distributions P\u0302 (x) and P\u0302 (y) We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}. More details about such language modeling approach can be referred to Appendix B.\nModel We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015). The word embedding dimension is 620 and the number of hidden node is 1000. Regarding the vocabulary size of the source and target language, we set it as 30k, 50k, and 30k for En\u2194Fr, En\u2194De, and En\u2194Zh, respectively. The out-of-vocabulary words are replaced by a special token UNK. Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch. We implement the whole NMT learning system based on an open source code4.\n2Since both tasks in each pair are symmetric, they play the same role in the dual supervised learning framework. Consequently, any one of the dual tasks can be viewed as the primal task while the other as the dual task.\n3The three NIST datasets correspond to Zh\u2192En translation task, in which each Chinese sentence has four English references. To build the test set for En\u2192Zh, we use the Chinese sentence with one randomly picked English sentence to form up a En\u2192Zh validation/test pair.\n4https://github.com/nyu-dl/dl4mt-tutorial\nEvaluation Metrics The translation qualities are measured by tokenized case-sensitive BLEU (Papineni et al., 2002) scores, which is implemented by (multi bleu, 2015). The larger the BLEU score is, the better the translation quality is. During the evaluation process, we use beam search with beam width 12 to generate sentences. Note that, following the common practice, the Zh\u2192En is evaluated by case-insensitive BLEU score.\nTraining Procedure We initialize the two models in DSL (i.e., the \u03b8xy and \u03b8yx) by using two warm-start models, which is generated by following the same process as (Jean et al., 2015). Then, we use SGD with the minibatch size of 80 as the optimization method for dual training. During the training process, we first set the initial learning rate \u03b7 to 0.2 and then halve it if the BLEU score on the validation set cannot grow for a certain number of mini batches. In order to stabilize parameters, we will freeze the embedding matrix once halving learning rates can no long improve the BLEU score on the validation set. The gradient clip is set as 1.0, 5.0 and 1.0 during the training for En\u2194Fr, En\u2194De, and En\u2194Zh, respectively (Pascanu et al., 2013). The value of both \u03bbxy and \u03bbyx in Algorithm 1 are set as 0.01 according to empirical performance on the validation set. Note that, during the optimization process, the LSTM-based language models will not be updated."}, {"heading": "3.2. Results", "text": "Table 1 shows the BLEU scores on the dual tasks by the DSL method with that by the baseline RNNSearch method. Note that, in this table, we use (MT08) and (MT12) to denote results carried out on NIST2008 and NIST2012, respectively. From this table, we can find that, on all these three pairs of symmetric tasks, DSL can improve the performance of both dual tasks, simultaneously.\nTo better understand the effects of applying the probabilistic duality constraint as the regularization, we compute the `duality on the test set by DSL compared with RNNSearch. In particular, after applying DSL to En\u2192Fr, the `duality decreases from 1545.68 to 1468.28, which also indicates that\nthe two models become more coherent in terms of probabilistic duality.\n(Jean et al., 2015) proposed an effective post-process technique, which can achieve better translation performance by replacing the \u201cUNK\u201d with the corresponding word-level translations. After applying this technique into DSL, we report its results on En\u2192Fr in Table 2, compared with several baselines with the same model structures as ours that also integrate the \u201cUNK\u201d post-processing technique. From this table, it is clear to see that DSL can achieve better performance than all baseline methods.\nIn the previous experiments, we use a warm-start approach in DSL using the models trained by RNNSearch. Actually, we can use stronger models for initialization to achieve even better accuracy. We conduct a light experiment to verify this. We use the models trained by (He et al., 2016a) as the initializations in DSL on En\u2194Fr translation. We find that BLEU score can be improved from 34.83 to 35.95 for En\u2192Fr translation, and from 32.94 to 33.40 for Fr\u2192En translation.\nEffects of \u03bb There are two hyperparameters \u03bbxy and \u03bbyx in our DSL algorithm. We conduct some experiments to investigate their effects. Since the input and output space are symmetric, we set \u03bbxy = \u03bbyx = \u03bb and plot the validation accuracy of different \u03bb\u2019s in Figure 1(a). From this figure, we can see that both En\u2192Fr and Fr\u2192En reach the best performance when \u03bb = 10\u22122, and thus the results of DSL reported in Table 1 are obtained with \u03bb = 10\u22122. Moreover, we find that, within a relatively large interval of \u03bb, DSL outperforms standard supervised learning, i.e., the point with \u03bb = 0. We also plot\nthe BLEU scores for \u03bb = 10\u22122 on the validation and test sets in Figure 1(b) with respect to training iterations. We can see that, in the first couple of rounds, the test BLEU curves fluctuate with large variance. The reason is that two separately initialized models of dual tasks yield are not consistent with each other, i.e., Eqn. (1) does not hold, which causes the declination of the performance of both models as they play as the regularizer for each other. As the training goes on, two models become more consistent and finally boost the performance of each other.\nCase studies Table 3 shows a couple of translation examples produced by RNNSearch compared with DSL. From this table, we find that DSL demonstrates three major advantages over RNNSearch. First, by leveraging the structural duality of sentences, DSL can result in the improvement of mutual translation, e.g. \u201cwhen it comes to\u201d and \u201clorsqu qu\u2019il s\u2019agit de\u201d, which better fit the semantics expressed in the sentences. Second, DSL can consider more contextual information in translation. For example, in Fr\u2192En, une soci\u00e9t\u00e9 is translated to company, however, in the baseline, it is translated to society. Although the word level translation is not bad, it should definitely be translated as \u201ccompany\u201d given the contextual semantics. Furthermore, DSL can better handle the plural form. For example, DSL can correctly translate \u201cthe French are the worst\u201d, which are of plural form, while the baseline deals with it by singular form."}, {"heading": "4. Application to Images Processing", "text": "In the domain of image processing, image classification (image\u2192label) and image generation (label\u2192image) are in\nthe dual form. In this section, we apply our dual supervised learning framework to these two tasks and conduct experimental studies based on a public dataset, CIFAR10 (Krizhevsky & Hinton, 2009), with 10 classes of images. In our experiments, we employ a popular method, ResNet5, for image classification and a most recent method, PixelCNN++6, for image generation. Let X denote the image space and Y denote the category space related to CIFAR10."}, {"heading": "4.1. Settings", "text": "Marginal Distributions In our experiments, we simply use the uniform distribution to set the marginal distribution P\u0302 (y) of 10-class labels, which means the marginal distribution of each class equals 0.1. The image distribution P\u0302 (x) is usually defined as \u220fm i=1 P{xi|x<i}, where all pixels of the image is serialized and xi is the value of the i-th pixel of an m-pixel image. Note that the model can predict xi only based on the previous pixels xj with index j < i. We use the PixelCNN++, which is so far the best algorithm, to model the image distribution.\nModels For the task of image classification, we choose 32- layer ResNet (denoted as ResNet-32) and 110-layer ResNet (denoted as ResNet-110) as two baselines, respectively, in order to examine the power of DSL on both relatively simple and complex models. For the task of image generation, we use PixelCNN++ again. Compared to the PixelCNN++ used for modeling distribution, the difference lies in the training process: When used for image generation given a certain class, PixelCNN++ takes the class label as an additional input, i.e., it tries to characterize \u220fm i=1 P{xi|x<i, y}, where y is the 1-hot label vector.\nEvaluation Metrics We use the classification error rates to measure the performance of image classification. We use bits per dimension (briefly, bpd) (Salimans et al., 2017), to assess the performance of image generation. In particular, for an image x with label y, the bpd is defined as:\n\u2212 (\u2211Nx i=1 logP (xi|x<i, y) ) / ( Nx log(2) ) , (5)\nwhere Nx is the number of pixels in image x. By using the dataset CIFAR-10, Nx is 3072 for any image x, and we will report the average bpd on the test set.\nTraining Procedure We first initialize both the primal and the dual models with the ResNet model and PixelCNN++ model pre-trained independently and separately. We obtain a 32-layer ResNet with error rate of 7.65 and a 110-layer ResNet with error rate of 6.54 as the pre-trained models for image classification. The error rates of these two pretrained models are comparable to results reported in (He\n5https://github.com/tensorflow/models/tree/master/resnet 6https://github.com/openai/pixel-cnn\net al., 2016b). We generate a pre-trained conditional image generation model with the test bpd of 2.94, which is the same as reported in (Salimans et al., 2017). For DSL training, we set the initial learning rate of image classification model as 0.1 and that of image generation model as 0.0005. The learning rates follow the same decay rules as those in (He et al., 2016b) and (Salimans et al., 2017). The whole training process takes about two weeks before convergence. Note that experimental results below are based on the training with \u03bbxy = (30/3072)2 and \u03bbyx = (1.2/3072)2."}, {"heading": "4.2. Results on Image Classification", "text": "Table 4 compares the error rates of two image classification models, i.e., DSL vs. Baseline, on the test set. From this table, we find that, with using either ResNet-32 or ResNet-110, DSL achieves better accuracy than the baseline method.\nInterestingly, we observe from Table 4 that, DSL leads to higher relative performance improvement on the ResNet110 over the ResNet-32. We hypothesize one possible reason is that, due to the limited training data, an appropriate regularization can benefit more to the 110-layer ResNet with higher model complexity, and the dualityoriented regularization `duality indeed plays this role and consequently gives rise to higher relative improvement."}, {"heading": "4.3. Results on Image Generation", "text": "Our further experimental results show that, based on ResNet-110, DSL can decrease the test bpd from 2.94 (baseline) to 2.93 (DSL), which is a new state-of-the-art result on CIFAR-10. Indeed, it is quite difficult to improve bpd by 0.01 which though seems like a minor change. We also find that, there is no significant improvement on test bpd based on ResNet-32. An intuitive explanation is that, since ResNet-110 is stronger than ResNet-32 in modeling the conditional probability P (y|x), it can better help the task of image generation through the constraint/regularization of the probabilistic duality.\nAs pointed out in (Theis et al., 2015), bpd is not the only evaluation rule of image generation. Therefore, we further conduct a qualitative analysis by comparing images generated by dual supervised learning with those by the base-\nline model for each of image categories, some examples of which are shown in Figure 2.\nEach row in Figure 2 corresponds to one category in CIFAR-10, the five images in the left side are generated by the baseline model, and the five ones in the right side are generated by the model trained by DSL. From this figure, we find that DSL generally generates images with clearer and more distinguishable characteristics regarding the corresponding category. Specifically, those right five images in Row 3, 4, and 6 can illustrate more distinguishable characteristics of birds, cats and dogs respectively, which is mainly due to benefits of introducing the probabilistic duality into DSL. But, there are still some cases that neither the baseline model nor DSL can perform well, like deers it Row 5 and frogs in Row 7. One reason is that the bpd of images in the category of deer and frogs are 3.17 and 3.32, which are significant larger than the average 2.94. This shows that the images of these two categories are harder to generate."}, {"heading": "5. Application to Sentiment Analysis", "text": "Finally, we apply the dual supervised learning framework to the domain of sentiment analysis. In this domain, the primal task, sentiment classification (Maas et al., 2011; Dai & Le, 2015), is to predict the sentiment polarity label of a given sentence; and the dual task, though not quite apparent but really existed, is sentence generation based on a sentiment polarity. In this section, let X denote the sentences and Y denote the sentiment related to our task."}, {"heading": "5.1. Experimental Setup", "text": "Dataset Our experiments are performed based on the IMDB movie review dataset (IMDB, 2011), which consists of 25k training and 25k test sentences. Each sentence in this dataset is associated with either a positive or a negative sentiment label. We randomly sample a subset of 3750 sentences from the training data as the validation set for hyperparameter tuning and use the remaining training data for model training.\nMarginal Distributions We simply use the uniform distribution to set the marginal distribution P\u0302 (y) of polarity labels, which means the marginal distribution of positive or negative class equals 0.5. On the other side, we take advantage of the LSTM-based language modeling to model the marginal distribution P\u0302 (x) of a sentence x. The test perplexities (Bengio et al., 2003) of the obtained language model is 58.74.\nModel Implementation We leverage the widely used LSTM (Dai & Le, 2015) modeling approach for sentiment classification7 model. We set the embedding dimension as 500 and the hidden layer size as 1024. For sentence generation, we use another LSTM model with W ewEwxt\u22121 + W e sEsy as input, where xt\u22121 denotes the t\u22121\u2019th word,Ew andEs represent the embedding matrices for word and sentiment label respectively, and W \u2019s represent the connections between embedding matrix and LSTM cells. A sentence is generated word by word sequentially, and the probability that word xt is generated is proportional to exp(W dwEwxt\u22121 +W d s Esy +Whht\u22121), where ht\u22121 is the hidden state outputted by LSTM. Note the W \u2019s and the E\u2019s are the parameters to learn in training. In the following, we call the model for sentiment based sentence generation as contextual language model (briefly, CLM).\nEvaluation Metrics We measure the performance of sentiment classification by the error rate, and that of sentence generation, i.e., CLM, by test perplexity.\nTraining Procedure To obtain baseline models, we use Adadelta as the optimization method to train both the sentiment classification and sentence generation model. Then, we use them to initialization the two models for DSL. At the beginning of DSL training, we use plain SGD with an initial learning rate of 0.2 and then decrease it to 0.02 for both models once there is no further improvement on the validation set. For each (x, y) pair, we set \u03bbxy = (5/lx)2 and \u03bbyx = (0.5/lx)2, where lx is the length of x. The whole training process of DSL takes less than two days.\n7Both supervised and semi-supervised sentiment classification are studied in (Dai & Le, 2015). We focus on supervised learning here. Therefore, we do not compare with the models trained with semi-supervised (labeled + unlabeled) data."}, {"heading": "5.2. Results", "text": "Table 5 compares the performance of DSL with the baseline method in terms of both the error rates of sentiment classification and the perplexity of sentence generation. Note that the test error of the baseline classification model, which is 10.10 as shown in the table, is comparable to the recent results as reported in (Dai & Le, 2015). We have two observations from the table. First, DSL can reduce the classification error by 0.90 without modifying the LSTMbased model structure. Second, DSL slightly improves the perplexity for sentence generation, but the improvement is not very significant. We hypothesize the reason is that the sentiment label can merely supply at most 1 bit information such that the perplexity difference between the language model (i.e., the marginal distribution P\u0302 (x)) and CLM (i.e., the conditional distribution P (x|y)) are not large, which limits the improvement brought by DSL.\nQualitative analysis on sentence generation\nIn addition to quantitative studies as shown above, we further conduct qualitative analysis on the performance of sentence generation. Table 6 demonstrates some examples of generated sentences based on sentiment labels. From this table, we can find that both the baseline model and DSL succeed in generating sentences expressing the certain sentiment. The baseline model prefers to produce the sentence with those words yielding high-frequency in the training data, such as the \u201cthe plot is simple/predictable, the acting is great/bad\u201d, etc. This is because the sentence generation model itself is essentially a language model based generator, which aims at catching the high-frequency words in the training data. Meanwhile, since the training of CLM in DSL can leverage the signals provided by the classifier, DSL makes it more possible to select those words, phrases, or textual patterns that can present more specific and more intense sentiment, such as \u201cnothing but good, 10/10, don\u2019t waste your time\u201d, etc. As a result, the CLM in DSL can generate sentences with richer expressions for sentiments."}, {"heading": "5.3. Discussions", "text": "In previous experiments, we start DSL training with welltrained primal and dual models. We conduct some further experiments to verify whether warm start is a must for DSL. (1) We train DSL from a warm-start sentence generator and a cold-start (randomly initialized) sentence classifier. In this case, DSL achieves a classification error of 9.44%, which is better than the baseline classifier in Ta-\nble 5. (2) We train DSL from a warm-start classifier and a cold-start sentence generator. The perplexity of the generator after DSL training reach 58.79, which is better than the baseline generator. (3) We train DSL from both cold-start models. The final classification error is 9.50% and the perplexity of the generator is 58.82, which are both better than the baselines. These results show that the success of DSL does not necessarily require warm-start models, although they can speed up the training of DSL."}, {"heading": "6. Conclusions and Future Work", "text": "Observing the existence of structure duality among many AI tasks, we have proposed a new learning framework, dual supervised learning, which can greatly improve the performance for both the primal and the dual tasks, simultaneously. We have introduced a probabilistic duality term to serve as a data-dependent regularizer to better guide the training. Empirical studies have validated the effectiveness of dual supervised learning.\nThere are multiple directions to explore in the future. First, we will test dual supervised learning on more dual tasks, such as speech recognition and speech synthesis. Second, we will enrich theoretical study to better understand dual supervised learning. Third, it is interesting to combine dual supervised learning with unsupervised dual learning (He et al., 2016a) to leverage unlabeled data so as to further improve the two dual tasks. Fourth, we will combine dual supervised learning with dual inference (Xia et al., 2017) so as to leverage structural duality to enhance both the training and inference procedures.\nAppendix"}, {"heading": "A. Theoretical Analysis", "text": "As we know, the final goal of the dual learning is to give correct predictions for the unseen test data. That is to say, we want to minimize the (expected) risk of the dual models, which is defined as follows8:\nR(f, g) = E [ `1(f(x), y) + `2(g(y), x)\n2\n] ,\u2200f \u2208 F , g \u2208 G,\nwhereF = {f(x; \u03b8xy); \u03b8xy\u2208\u0398xy}, G = {g(x; \u03b8yx); \u03b8yx \u2208 \u0398yx}, \u0398xy and \u0398yx are parameter spaces, and the E is taken over the underlying distribution P . Besides, let D denote the product space of the two models satisfying probabilistic duality, i.e., the constraint in Eqn.(4). For ease of reference, defineHdual as (F \u00d7 G) \u2229 D.\nDefine the empirical risk on the n sample as follows: for any f \u2208 F , g \u2208 G,\nRn(f, g) = 1\nn \u2211n i=1 `1(f(xi), yi) + `2(g(yi), xi) 2 .\nFollowing (Bartlett & Mendelson, 2002), we introduce Rademacher complexity for dual supervised learning, a measure for the complexity of the hypothesis. Definition 1. Define the Rademacher complexity of DSL, RDSLn , as follows:\nRDSLn = E z,\u03c3\n[ sup\n(f,g)\u2208Hdual\n\u2223\u2223 1 n n\u2211 i=1 \u03c3i ( `1(f(xi), yi)+`2(g(yi), xi) )\u2223\u2223], where z = {z1, z2, \u00b7 \u00b7 \u00b7 , zn} \u223c Pn, zi = (xi, yi) in which xi \u2208 X and yi \u2208 Y , \u03c3 = {\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3m} are i.i.d sampled with P (\u03c3i = 1) = P (\u03c3i = \u22121) = 0.5.\nBased on RDSLn , we have the following theorem for dual supervised learning:\nTheorem 1 ((Mohri et al., 2012)). Let 12`1(f(x), y) + 1 2`2(g(y), x) be a mapping from X \u00d7 Y to [0, 1]. Then, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212\u03b4, the following inequality holds for any (f, g) \u2208 Hdual,\nR(f, g) \u2264 Rn(f, g) + 2RDSLn + \u221a 1\n2n ln(\n1 \u03b4 ). (6)\nSimilarly, we define the Rademacher complexity for the standard supervised learning RSLn under our framework by replacing theHdual in Definition 1 byF\u00d7G. With probability at least 1 \u2212 \u03b4, the generation error bound of supervised learning is smaller than 2RSLn + \u221a 1 2n ln( 1 \u03b4 ).\n8The parameters \u03b8xy and \u03b8yx in the dual models will be omitted when the context is clear.\nSince Hdual \u2208 F \u00d7 G, by the definition of Rademacher complexity, we have RDSLn \u2264 RSLn . Therefore, DSL enjoys a smaller generation error bound than supervised learning.\nThe approximation of dual supervised learning is defined as\nR(f\u2217F , g \u2217 F )\u2212R\u2217 (7)\nin which\nR(f\u2217F , g \u2217 F ) = inf R(f, g), s.t. (f, g) \u2208 Hdual; R\u2217 = inf R(f, g).\nThe approximation error for supervised learning is similarly defined.\nDefine Py|x = {P (y|x; \u03b8xy)|\u03b8xy \u2208 \u0398xy}, Px|y = {P (x|y; \u03b8yx)|\u03b8yx \u2208 \u0398yx}. Let P \u2217y|x and P \u2217 x|y denote the two conditional probabilities derived from P . We have the following theorem:\nTheorem 2. If P \u2217y|x \u2208 Py|x and P \u2217 x|y \u2208 Px|y , then supervised learning and DSL has the same approximation error.\nProof. By definition, we can verify both of the two approximation errors are zero."}, {"heading": "B. Details about the Language Models for Marginal Distributions", "text": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}. The embedding dimension and hidden node are both 1024. We apply 0.5 dropout to the input embedding and the last hidden layer before softmax. The validation perplexities of the language models are shown in Table 7, where the validation sets are the same.\nFor the marginal distributions for sentences of sentiment classification, we choose the LSTM language model again like those for machine translation applications. The two differences are: (i) the vocabulary size is 10000; (ii) the word embedding dimension is 500. The perplexity of this language model is 58.74."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "In 33rd International Conference on Machine Learning,", "citeRegEx": "Chrzanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chrzanowski et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of machine learning research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Andrew M", "Le", "Quoc V"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Dual learning for machine translation", "author": ["He", "Di", "Xia", "Yingce", "Qin", "Tao", "Wang", "Liwei", "Yu", "Nenghai", "Liu", "Tie-Yan", "Ma", "Wei-Ying"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Support vector machines", "author": ["Hearst", "Marti A", "Dumais", "Susan T", "Osuna", "Edgar", "Platt", "John", "Scholkopf", "Bernhard"], "venue": "IEEE Intelligent Systems and their Applications,", "citeRegEx": "Hearst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hearst et al\\.", "year": 1998}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean", "S\u00e9bastien", "Cho", "Kyunghyun", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "In ACL,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Foundations of machine learning", "author": ["Mohri", "Mehryar", "Rostamizadeh", "Afshin", "Talwalkar", "Ameet"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Wavenet: A generative model for raw audio", "author": ["Oord", "Aaron van den", "Dieleman", "Sander", "Zen", "Heiga", "Simonyan", "Karen", "Vinyals", "Oriol", "Graves", "Alex", "Kalchbrenner", "Nal", "Senior", "Andrew", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "WeiJing"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications", "author": ["Salimans", "Tim", "Karpathy", "Andrej", "Chen", "Xi", "P. Kingma", "Diederik", "Bulatov", "Yaroslav"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Salimans et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2017}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen", "Shiqi", "Cheng", "Yong", "He", "Zhongjun", "Wei", "Wu", "Hua", "Sun", "Maosong", "Liu", "Yang"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "Martin", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In Interspeech, pp", "citeRegEx": "Sundermeyer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Espeholt", "Lasse", "Vinyals", "Oriol", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In 33rd International Conference on Machine Learning,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Dual inference for machine learning", "author": ["Xia", "Yingce", "Bian", "Jiang", "Qin", "Tao", "Yu", "Nenghai", "Liu", "TieYan"], "venue": "In The 26th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Xia et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2017}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": ", 2016b;a), speech recognition (Graves et al., 2013; Amodei et al., 2016), and speech generation/synthesis (Oord et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 15, "context": ", 2016), and speech generation/synthesis (Oord et al., 2016).", "startOffset": 41, "endOffset": 60}, {"referenceID": 18, "context": ", 2016b) as our baseline for image classification, and PixelCNN++(Salimans et al., 2017) as our baseline for image generation.", "startOffset": 65, "endOffset": 88}, {"referenceID": 9, "context": "While `duality can be regarded as a regularization term, it is data dependent, which makes DSL different from Lasso (Tibshirani, 1996) or SVM (Hearst et al., 1998), where the regularization term is data-independent.", "startOffset": 142, "endOffset": 163}, {"referenceID": 10, "context": "Datasets We employ the same datasets as used in (Jean et al., 2015) to conduct experiments on En\u2194Fr and En\u2194De.", "startOffset": 48, "endOffset": 67}, {"referenceID": 20, "context": "Marginal Distributions P\u0302 (x) and P\u0302 (y) We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 89, "endOffset": 137}, {"referenceID": 13, "context": "Marginal Distributions P\u0302 (x) and P\u0302 (y) We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 89, "endOffset": 137}, {"referenceID": 1, "context": "Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).", "startOffset": 113, "endOffset": 155}, {"referenceID": 10, "context": "Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).", "startOffset": 113, "endOffset": 155}, {"referenceID": 1, "context": "Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.", "startOffset": 76, "endOffset": 118}, {"referenceID": 10, "context": "Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.", "startOffset": 76, "endOffset": 118}, {"referenceID": 16, "context": "com/nyu-dl/dl4mt-tutorial Evaluation Metrics The translation qualities are measured by tokenized case-sensitive BLEU (Papineni et al., 2002) scores, which is implemented by (multi bleu, 2015).", "startOffset": 117, "endOffset": 140}, {"referenceID": 10, "context": ", the \u03b8xy and \u03b8yx) by using two warm-start models, which is generated by following the same process as (Jean et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 17, "context": "0 during the training for En\u2194Fr, En\u2194De, and En\u2194Zh, respectively (Pascanu et al., 2013).", "startOffset": 64, "endOffset": 86}, {"referenceID": 10, "context": "(Jean et al., 2015) proposed an effective post-process technique, which can achieve better translation performance by replacing the \u201cUNK\u201d with the corresponding word-level translations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "[1] (Jean et al., 2015); [2] (Shen et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 19, "context": ", 2015); [2] (Shen et al., 2016)", "startOffset": 13, "endOffset": 32}, {"referenceID": 18, "context": "We use bits per dimension (briefly, bpd) (Salimans et al., 2017), to assess the performance of image generation.", "startOffset": 41, "endOffset": 64}, {"referenceID": 18, "context": "94, which is the same as reported in (Salimans et al., 2017).", "startOffset": 37, "endOffset": 60}, {"referenceID": 18, "context": ", 2016b) and (Salimans et al., 2017).", "startOffset": 13, "endOffset": 36}, {"referenceID": 21, "context": "As pointed out in (Theis et al., 2015), bpd is not the only evaluation rule of image generation.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "The test perplexities (Bengio et al., 2003) of the obtained language model is 58.", "startOffset": 22, "endOffset": 43}, {"referenceID": 25, "context": "Fourth, we will combine dual supervised learning with dual inference (Xia et al., 2017) so as to leverage structural duality to enhance both the training and inference procedures.", "startOffset": 69, "endOffset": 87}, {"referenceID": 14, "context": "Theorem 1 ((Mohri et al., 2012)).", "startOffset": 11, "endOffset": 31}, {"referenceID": 20, "context": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 32, "endOffset": 80}, {"referenceID": 13, "context": "We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as \u220fTx i=1 P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index < i indicates {1, 2, \u00b7 \u00b7 \u00b7 , i \u2212 1}.", "startOffset": 32, "endOffset": 80}], "year": 2017, "abstractText": "Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach dual supervised learning. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.", "creator": "LaTeX with hyperref package"}}}