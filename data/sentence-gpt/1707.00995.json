{"id": "1707.00995", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2017", "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "abstract": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe as areas of the source word or image region.", "histories": [["v1", "Tue, 4 Jul 2017 13:57:04 GMT  (1158kb,D)", "http://arxiv.org/abs/1707.00995v1", "Accepted to EMNLP 2017"]], "COMMENTS": "Accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jean-benoit delbrouck", "st\\'ephane dupont"], "accepted": true, "id": "1707.00995"}, "pdf": {"name": "1707.00995.pdf", "metadata": {"source": "CRF", "title": "An empirical study on the effectiveness of images in Multimodal Neural Machine Translation", "authors": ["Jean-Benoit Delbrouck"], "emails": ["stephane.dupont}@umons.ac.be"], "sections": [{"heading": "1 Introduction", "text": "In machine translation, neural networks have attracted a lot of research attention. Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mechanism is learned to focus on different parts of the input sentence while decoding. Attention mechanisms have shown to work with other modalities too, like images, where their are able to learn to attend the salient parts of an image, for instance when generating text captions (Xu et al., 2015). For such applications, Convolutional Neural Networks (CNNs) such as Deep Residual (He et al.,\n2016) have shown to work best to represent images.\nMultimodal models of texts and images empower new applications such as visual question answering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications.\nIn the field of Machine Translation (MT), the efficient integration of multimodal information still remains a challenging task. It requires combining diverse modality vector representations with each other. These vector representations, also called context vectors, are computed in order the capture the most relevant information in a modality to output the best translation of a sentence.\nTo investigate the effectiveness of information obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community1. The best results of NMT model were those of Huang et al. (2016) who used LSTM fed with global visual features or multiple regional visual features followed by rescoring. Recently, Calixto et al. (2017) proposed a doubly-attentive decoder that outperformed this baseline with less data and without rescoring.\nOur paper is structured as follows. In section 2, we briefly describe our NMT model as well as the conditional GRU activation used in the decoder. We also explain how multi-modalities can be implemented within this framework. In the following sections (3 and 4), we detail three attention mechanisms and explain how we tweak them to work as well as possible with images. Finally, we report and analyze our results in section 5 then conclude in section 6.\n1http://www.statmt.org/wmt16/multimodal-task.html\nar X\niv :1\n70 7.\n00 99\n5v 1\n[ cs\n.C L\n] 4\nJ ul\n2 01\n7"}, {"heading": "2 Neural Machine Translation", "text": "In this section, we detail the neural machine translation architecture by Bahdanau et al. (2014), implemented as an attention-based encoder-decoder framework with recurrent neural networks (\u00a72.1). We follow by explaining the conditional GRU layer (\u00a72.2) - the gating mechanism we chose for our RNN - and how the model can be ported to a multimodal version (\u00a72.3)."}, {"heading": "2.1 Text-based NMT", "text": "Given a source sentence X = (x1, x2, . . . , xM ), the neural network directly models the conditional probability p(Y |X) of its translation Y = (y1, y2, . . . , yN ). The network consists of one encoder and one decoder with one attention mechanism. The encoder computes a representation C for each source sentence and a decoder generates one target word at a time and by decomposing the following conditional probability :\nlog p(Y |X) = n\u2211 t=1 log p(yt|y < t, C) (1)\nEach source word xi and target word yi are a column index of the embedding matrix EX and EY . The encoder is a bi-directional RNN with Gated Recurrent Unit (GRU) layers (Chung et al., 2014; Cho et al., 2014), where a forward RNN \u2212\u2192 \u03a8 enc reads the input sequence as it is ordered (from x1 to xM ) and calculates a sequence of forward hidden states ( \u2212\u2192 h 1, \u2212\u2192 h 2, . . . , \u2212\u2192 hM ). A backward RNN \u2190\u2212 \u03a8 enc reads the sequence in the reverse order (from xM to x1), resulting in a sequence of backward hidden states ( \u2190\u2212 hM , \u2190\u2212 hM\u22121, . . . , \u2190\u2212 h 1). We obtain an annotation for each word xi by concatenating the forward and backward hidden state ht = [ \u2212\u2192 h t; \u2190\u2212 h t]. Each annotation ht contains the summaries of both the preceding words and the following words. The representation C for each source sentence is the sequence of annotations C = (h1,h2, . . . ,hM ).\nThe decoder is an RNN that uses a conditional GRU (cGRU, more details in \u00a72.2) with an attention mechanism to generate a word yt at each time-step t. The cGRU uses it\u2019s previous hidden state st\u22121, the whole sequence of source annotations C and the previously decoded symbol yt\u22121 in order to update it\u2019s hidden state st :\nst = cGRU (st\u22121, yt\u22121, C) (2)\nIn the process, the cGRU also computes a timedependent context vector ct. Both st and ct are further used to decode the next symbol. We use a deep output layer (Pascanu et al., 2014) to compute a vocabulary-sized vector :\not = Lo tanh(Lsst +Lcct +LwEY [yt\u22121]) (3)\nwhere Lo, Ls, Lc, Lw are model parameters. We can parameterize the probability of decoding each word yt as:\np(yt|yt\u22121, st, ct) = Softmax(ot) (4)\nThe initial state of the decoder s0 at time-step t = 0 is initialized by the following equation :\ns0 = finit(hM ) (5)\nwhere finit is a feedforward network with one hidden layer."}, {"heading": "2.2 Conditional GRU", "text": "The conditional GRU 2 consists of two stacked GRU activations called REC1 and REC2 and an attention mechanism fatt in between (called ATT in the footnote paper). At each time-step t, REC1 firstly computes a hidden state proposal st based on the previous hidden state st\u22121 and the previously emitted word yt\u22121:\nz\u2032t = \u03c3 ( W \u2032zEY [yt\u22121] +U \u2032 zst\u22121 ) r\u2032t = \u03c3 ( W \u2032rEY [yt\u22121] +U \u2032 rst\u22121\n) s\u2032t = tanh ( W \u2032EY [yt\u22121] + r \u2032 t (U \u2032st\u22121)\n) s\u2032t =(1\u2212 z\u2032t) s\u2032t + z\u2032t st\u22121 (6)\nThen, the attention mechanism computes ct over the source sentence using the annotations sequence C and the intermediate hidden state proposal s\u2032t:\nct = fatt ( C, s\u2032t ) (7)\n() Finally, the second recurrent cell REC2, computes the hidden state st of the cGRU by looking at the intermediate representation s\u2032t and context vector ct:\nzt =\u03c3 ( Wzct +Uzs \u2032 t ) rt =\u03c3 ( Wrct +Urs \u2032 t\n) st =tanh ( Wct + rt (Us\u2032t)\n) st =(1\u2212 zt) st + zt s\u2032t (8)\n2https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf"}, {"heading": "2.3 Multimodal NMT", "text": "Recently, Calixto et al. (2017) proposed a doubly attentive decoder (referred as the \u201dMNMT\u201d model in the author\u2019s paper) which can be seen as an expansion of the attention-based NMT model proposed in the previous section. Given a sequence of second a modality annotations I = (a1,a2, . . . ,aL), we also compute a new context vector based on the same intermediate hidden state proposal s\u2032t:\nit = f \u2032 att ( I, s\u2032t ) (9)\nThis new time-dependent context vector is an additional input to a modified version of REC2 which now computes the final hidden state st using the intermediate hidden state proposal s\u2032t and both time-dependent context vectors ct and it :\nzt =\u03c3 ( Wzct +Wzit +Uzs \u2032 t ) rt =\u03c3 ( Wrct +Writ +Urs \u2032 t\n) st =tanh ( Wct +Wit + rt (Us\u2032t)\n) st =(1\u2212 zt) st + zt s\u2032t (10)\nThe probabilities for the next target word (from equation 3) also takes into account the new context vector it:\nLo tanh(Lsst +Lcct +Liit +LwEY [yt\u22121]) (11) where Li is a new trainable parameter. In the field of multimodal NMT, the second modality is usually an image computed into feature maps with the help of a CNN. The annotations a1, a2, . . . , aL are spatial features (i.e. each annotation represents features for a specific region in the image) . We follow the same protocol for our experiments and describe it in section 5."}, {"heading": "3 Attention-based Models", "text": "We evaluate three models of the image attention mechanism f \u2032att of equation 7. They have in common the fact that at each time step t of the decoding phase, all approaches first take as input the annotation sequence I to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word yt. Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image."}, {"heading": "3.1 Soft attention", "text": "Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) but has been widely used for translation tasks ever since. One should note that it slightly differs from Bahdanau et al. (2014) where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation 7. This mechanism has also been successfully investigated for the task of image description generation (Xu et al., 2015) where a model generates an image\u2019s description in natural language. It has been used in multimodal translation as well (Calixto et al., 2017), for which it constitutes a state-of-the-art.\nThe idea of the soft attentional model is to consider all the annotations when deriving the context vector it. It consists of a single feedforward network used to compute an expected alignment et between modality annotation al and the target word to be emitted at the current time step t. The inputs are the modality annotations and the intermediate representation of REC1 s\u2032t:\net,l = v T tanh(Uas \u2032 t +Waal) (12)\nThe vector et has length L and its l-th item contains a score of how much attention should be put on the l-th annotation in order to output the best word at time t. We compute normalized scores to create an attention mask \u03b1t over annotations:\n\u03b1t,i = exp(et,i)\u2211L j=1 exp(et,j)\n(13)\nit = L\u2211 i=1 \u03b1t,iai (14)\nFinally, the modality time-dependent context vector it is computed as a weighted sum over the annotation vectors (equation 14). In the above expressions, vT , Ua andWa are trained parameters."}, {"heading": "3.2 Hard Stochastic attention", "text": "This model is a stochastic and sampling-based process where, at every timestep t, we are making a hard choice to attend only one annotation. This corresponds to one spatial location in the image. Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process on images.\nThe mechanism fatt is now a function that returns a sampled intermediate latent variables \u03b3t,i based upon a multinouilli distribution parameterized by \u03b1:\n\u03b3t \u223c Multinoulli({\u03b11,...,L}) (15)\nwhere \u03b3t,i an indicator one-hot variable which is set to 1 if the i-th annotation (out of L) is the one used to compute the context vector it:\np(\u03b3t,l = 1|\u03b3 < t, I) =\u03b1t,l (16)\nit = L\u2211 i=1 \u03b3t,iai (17)\nContext vector it is now seen as the random variable of this distribution. We define the variational lower bound L(\u03b3) on the marginal log evidence log p(y|I) of observing the target sentence y given modality annotations I .\nL(\u03b3) = \u2211 \u03b3 p(\u03b3|I) log p(y|\u03b3, I)\n\u2264 log \u2211 \u03b3 p(\u03b3|I)p(y|\u03b3, I)\n= log p(y|I) (18)\nThe learning rules can be derived by taking derivatives of the above variational free energy\nL(\u03b3) with respect to the model parameterW :\n\u2202L \u2202W = \u2211 \u03b3 p(\u03b3|I)\n[ \u2202 log p(y|\u03b3, I)\n\u2202W +\nlog p(y|\u03b3, I)\u2202 log p(\u03b3|I) \u2202W ] (19)\nIn order to propagate a gradient through this process, the summation in equation 19 can then be approximated using Monte Carlo based sampling defined by equation 16:\n\u2202L \u2202W \u2248 1 N N\u2211 n=1\n[ \u2202 log p(y|\u03b3\u0303n, I)\n\u2202W +\nlog p(y|\u03b3\u0303n, I)\u2202 log p(\u03b3\u0303 n|I)\n\u2202W\n] (20)\nTo reduce variance of the estimator in equation 20, we use a moving average baseline estimated as an accumulated sum of the previous log likelihoods with exponential decay upon seeing the k-th mini-batch:\nbk = 0.9\u00d7 bk\u22121 + 0.1\u00d7 log p(y|\u03b3\u0303k, I) (21)"}, {"heading": "3.3 Local Attention", "text": "In this section, we propose a local attentional mechanism that chooses to focus only on a small\nsubset of the image annotations. Local Attention has been used for text-based translation (Luong et al., 2015) and is inspired by the selective attention model of Gregor et al. (2015) for image generation. Their approach allows the model to select an image patch of varying location and zoom. Local attention uses instead the same \u201dzoom\u201d for all target positions and still achieved good performance. This model can be seen as a trade-off between the soft and hard attentional models. The model picks one patch in the annotation sequence (one spatial location) and selectively focuses on a small window of context around it. Even though an image can\u2019t be seen as a temporal sequence, we still hope that the model finds points of interest and selects the useful information around it. This approach has an advantage of being differentiable whereas the stochastic attention requires more complicated techniques such as variance reduction and reinforcement learning to train as shown in section 3.2. The soft attention has the drawback to attend the whole image which can be difficult to learn, especially because the number of annotations L is usually large (presumably to keep a significant spatial granularity).\nMore formally, at every decoding step t, the model first generates an aligned position pt. Context vector it is derived as a weighted sum over the annotations within the window [pt \u2212 D; pt + D] where D is a fixed model parameter chosen empirically3. These selected annotations correspond to a squared region in the attention maps around pt. The attention mask \u03b1t is of size 2D + 1. The model predicts pt as an aligned position in the annotation sequence (referred as Predictive alignment (local-m) in the author\u2019s paper) according to the following equation:\npt = S \u00b7 sigmoid(vT tanh(Uas\u2032t)) (22)\nwhere vT and Ua are both trainable model parameters and S is the annotation sequence length |I|. Because of the sigmoid, pt \u2208 [0, S]. We use equation 12 and 13 respectively to compute the expected alignment vector et and the attention mask \u03b1t. In addition, a Gaussian distribution centered around pt is placed on the alphas in order to favor\n3We pick D = |ai|/4 = 49\nannotations near pt:\n\u03b1t,i = \u03b1t,i exp\n( \u2212 (i\u2212 pt) 2\n2\u03c32\n) (23)\nwhere standard deviation \u03c3 = D2 . We obtain context vector it by following equation 14.\nschwimmt im Wasser ."}, {"heading": "4 Image attention optimization", "text": "Three optimizations can be added to the attention mechanism regarding the image modality. All lead to a better use of the image by the model and improved the translation scores overall.\nAt every decoding step t, we compute a gating scalar \u03b2t \u2208 [0, 1] according to the previous decoder state st\u22121:\n\u03b2t = \u03c3(W\u03b2st\u22121 + b\u03b2) (24)\nIt is then used to compute the time-dependent image context vector :\nit = \u03b2t L\u2211 l=1 \u03b1t,lal (25)\nXu et al. (2015) empirically found it to put more emphasis on the objects in the image descriptions generated with their model.\nWe also double the output size of trainable parameters Ua, Wa and vT in equation 12 when it comes to compute the expected annotations over the image annotation sequence. More\nformally, given the image annotation sequence I = (a1,a2, . . . ,aL),ai \u2208 RD, the tree matrices are of size D \u00d7 2D, D \u00d7 2D and 2D \u00d7 1 respectively. We noticed a better coverage of the objects in the image by the alpha weights.\nLastly, we use a grounding attention inspired by Delbrouck and Dupont (2017). The mechanism merge each spatial location ai in the annotation sequence I with the initial decoder state s0 obtained in equation 5 with non-linearity :\nI \u2032 =(f(a1 + s0), f(a2 + s0), . . . , f(aL + s0)) (26)\nwhere f is tanh function. The new annotations go through a L2 normalization layer followed by two 1 \u00d7 1 convolutional layers (of size D \u2192 512, 512 \u2192 1 respectively) to obtain L \u00d7 1 weights, one for each spatial location. We normalize the weights with a softmax to obtain a soft attention map \u03b1. Each annotation ai is then weighted according to its corresponding \u03b1i:\nI =(\u03b11a1,\u03b12a2, . . . ,\u03b1LaL) (27)\nThis method can be seen as the removal of unnecessary information in the image annotations according to the source sentence. This attention is used on top of the others - before decoding - and is referred as \u201dgrounded image\u201d in Table 1."}, {"heading": "5 Experiments", "text": "For this experiments on Multimodal Machine Translation, we used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities. For each image, one of the English descriptions was selected and manually translated into German by a professional translator. As training and development data, 29,000 and 1,014 triples are used respectively. A test set of size 1000 is used for metrics evaluation."}, {"heading": "5.1 Training and model details", "text": "All our models are build on top of the nematus framework (Sennrich et al., 2017). The encoder is a bidirectional RNN with GRU, one 1024D single-layer forward and one 1024D single-layer backward RNN. Word embeddings for source and target language are of 620D and trained jointly with the model. Word embeddings and other non-recurrent matrices are initialized by sampling\nfrom a Gaussian N (0, 0.012), recurrent matrices are random orthogonal and bias vectors are all initialized to zero.\nTo create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size 14 \u00d7 14 \u00d7 1024 at its res4f layer (He et al., 2016). In our experiments, our decoder operates on the flattened 196 \u00d7 1024 (i.e L \u00d7 D). We also apply dropout with a probability of 0.5 on the embeddings, on the hidden states in the bidirectional RNN in the encoder as well as in the decoder. In the decoder, we also apply dropout on the text annotations hi, the image features ai, on both modality context vector and on all components of the deep output layer before the readout operation. We apply dropout using one same mask in all time steps (Gal and Ghahramani, 2016).\nWe also normalize and tokenize English and German descriptions using the Moses tokenizer scripts (Koehn et al., 2007). We use the byte pair encoding algorithm on the train set to convert space-separated tokens into subwords (Sennrich et al., 2016), reducing our vocabulary size to 9226 and 14957 words for English and German respectively.\nAll variants of our attention model were trained with ADADELTA (Zeiler, 2012), with minibatches of size 80 for our monomodal (text-only) NMT model and 40 for our multimodal NMT. We apply early stopping for model selection based on BLEU4 : training is halted if no improvement on the development set is observed for more than 20 epochs. We use the metrics BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006) to evaluate the quality of our models\u2019 translations."}, {"heading": "5.2 Quantitative results", "text": "We notice a nice overall progress over Calixto et al. (2017) multimodal baseline, especially when using the stochastic attention. With improvements of +1.51 BLEU and -2.2 TER on both precision-oriented metrics, the model shows a strong similarity of the n-grams of our candidate translations with respect to the references. The more recall-oriented metrics METEOR scores\nare roughly the same across our models which is expected because all attention mechanisms share the same subsequent step at every time-step t, i.e. taking into account the attention weights of previous time-step t \u2212 1 in order to compute the new intermediate hidden state proposal and therefore the new context vector it. Again, the largest improvement is given by the hard stochastic attention mechanism (+0.4 METEOR): because it is modeled as a decision process according to the previous choices, this may reinforce the idea of recall. We also remark interesting improvements when using the grounded mechanism, especially for the soft attention. The soft attention may benefit more of the grounded image because of the wide range of spatial locations it looks at, especially compared to the stochastic attention. This motivates us to dig into more complex grounding techniques in order to give the machine a deeper understanding of the modalities.\nNote that even though our baseline NMT model is basically the same as Calixto et al. (2017), our experiments results are slightly better. This is probably due to the different use of dropout and subwords. We also compared our results to Caglayan et al. (2016) because our multimodal models are nearly identical with the major ex-\nception of the gating scalar (cfr. section 4). This motivated some of our qualitative analysis and hesitation towards the current architecture in the next section."}, {"heading": "5.3 Qualitative results", "text": "For space-saving and ergonomic reasons, we only discuss about the hard stochastic and soft attention, the latter being a generalization of the local attention. As we can see in Figure 7, the soft attention model is looking roughly at the same region of the image for every decoding step t. Because the words \u201dhund\u201d(dog), \u201dwald\u201d(forest) or \u201dweg\u201d(way) in left image are objects, they benefit from a high gating scalar. As a matter of fact, the attention mechanism has learned to detect the objects within a scene (at every time-step, whichever word we are decoding as shown in the right image) and the gating scalar has learned to decide whether or not we have to look at the picture (or more accurately whether or not we are translating an object). Without this scalar, the translation scores undergo a massive drop (as seen in Caglayan et al. (2016)) which means that the attention mechanisms don\u2019t really understand the more complex relationships between objects, what is really happening in the scene. Surprisingly, the\ngating scalar happens to be really low in the stochastic attention mechanism: a significant amount of sentences don\u2019t have a summed gating scalar \u2265 0.10. The model totally discards the image in the translation process.\nIt is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders { \u2190\u2212 \u03a8 enc, \u2212\u2192 \u03a8 enc} combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \u201dgets lost\u201d, the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:\nRef: Ein Kind sitzt auf den Schultern einer Frau und klatscht . Mono: Ein Kind sitzt auf den Schultern einer Frau und schla\u0308ft . Soft: Ein Kind , das sich auf der Schultern eines Frau reitet , fa\u0308hrt auf den Schultern . Hard: Ein Kind in der Haltung , wa\u0308hrend er auf den Schultern einer Frau fa\u0308hrt .\nThe monomodal translation has a sentence-level BLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45 respectively. Figure 8 shows the attention maps for both mechanism. Nevertheless, one has to concede that the use of images indubitably helps the translation as shown in the score tabular."}, {"heading": "6 Conclusion and future work", "text": "We have tried different attention mechanism and tweaks for the image modality. We showed improvements and encouraging results overall on the Flickr30K Entities dataset. Even though we identified some flaws of the current attention mechanisms, we can conclude pretty safely that images are an helpful resource for the machine in a translation task. We are looking forward to try out richer and more suitable features for multimodal translation (ie. dense captioning features). Another interesting approach would be to use visually grounded word embeddings to capture visual notions of semantic relatedness."}, {"heading": "7 Acknowledgements", "text": "This work was partly supported by the Chist-Era project IGLU with contribution from the Belgian Fonds de la Recherche Scientique (FNRS), contract no. R.50.11.15.F, and by the FSO project VCYCLE with contribution from the Belgian Waloon Region, contract no. 1510501."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Ba et al\\.,? 2015", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Doubly-attentive decoder for multi-modal neural machine translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR abs/1702.01287. http://arxiv.org/abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Multimodal compact bilinear pooling for multimodal neural machine translation", "author": ["Jean-Benoit Delbrouck", "Stephane Dupont."], "venue": "arXiv preprint arXiv:1703.08084 https://arxiv.org/pdf/1703.08084.pdf.", "citeRegEx": "Delbrouck and Dupont.,? 2017", "shortCiteRegEx": "Delbrouck and Dupont.", "year": 2017}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi30k: Multilingual english-german image descriptions pages 70\u201374", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Rezende", "Daan Wierstra."], "venue": "Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gregor et al\\.,? 2015", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Attention-based multimodal neural machine translation", "author": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer."], "venue": "Proceedings of the First Conference on Machine Translation, Berlin, Germany.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "koray kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Nematus: a Toolkit for Neural Machine", "author": ["Rico Sennrich", "Orhan Firat", "Kyunghyun Cho", "Alexandra Birch", "Barry Haddow", "Julian Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u201daubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2017}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "A shared task on multimodal machine translation and crosslingual image description", "author": ["Lucia Specia", "Stella Frank", "Khalil Sima\u2019an", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "David Blei and Francis Bach, editors,", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 1, "context": "Recently, the attention-based encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014) has been largely adopted.", "startOffset": 56, "endOffset": 103}, {"referenceID": 23, "context": "Attention mechanisms have shown to work with other modalities too, like images, where their are able to learn to attend the salient parts of an image, for instance when generating text captions (Xu et al., 2015).", "startOffset": 194, "endOffset": 211}, {"referenceID": 11, "context": "For such applications, Convolutional Neural Networks (CNNs) such as Deep Residual (He et al., 2016) have shown to work best to represent images.", "startOffset": 82, "endOffset": 99}, {"referenceID": 20, "context": "To investigate the effectiveness of information obtained from images, a multimodal machine translation shared task (Specia et al., 2016) has been addressed to the MT community1.", "startOffset": 115, "endOffset": 136}, {"referenceID": 11, "context": "The best results of NMT model were those of Huang et al. (2016) who used LSTM fed with global visual features or multiple regional visual features followed by rescoring.", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "Recently, Calixto et al. (2017) proposed a doubly-attentive decoder that outperformed this baseline with less data and without rescoring.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "In this section, we detail the neural machine translation architecture by Bahdanau et al. (2014), implemented as an attention-based encoder-decoder framework with recurrent neural networks (\u00a72.", "startOffset": 74, "endOffset": 97}, {"referenceID": 16, "context": "We use a deep output layer (Pascanu et al., 2014) to compute a vocabulary-sized vector :", "startOffset": 27, "endOffset": 49}, {"referenceID": 3, "context": "Recently, Calixto et al. (2017) proposed a doubly attentive decoder (referred as the \u201dMNMT\u201d model in the author\u2019s paper) which can be seen as an expansion of the attention-based NMT model proposed in the previous section.", "startOffset": 10, "endOffset": 32}, {"referenceID": 23, "context": "This mechanism has also been successfully investigated for the task of image description generation (Xu et al., 2015) where a model generates an image\u2019s description in natural language.", "startOffset": 100, "endOffset": 117}, {"referenceID": 3, "context": "It has been used in multimodal translation as well (Calixto et al., 2017), for which it constitutes a state-of-the-art.", "startOffset": 51, "endOffset": 73}, {"referenceID": 20, "context": "Soft attention has firstly been used for syntactic constituency parsing by Vinyals et al. (2015) but has been widely used for translation tasks ever since.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": "One should note that it slightly differs from Bahdanau et al. (2014) where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation 7.", "startOffset": 46, "endOffset": 69}, {"referenceID": 14, "context": "Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 0, "context": "Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 23, "context": ", 2015) and later extended to image description generation (Xu et al., 2015).", "startOffset": 59, "endOffset": 76}, {"referenceID": 0, "context": ", 2014; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process on images.", "startOffset": 8, "endOffset": 160}, {"referenceID": 13, "context": "Local Attention has been used for text-based translation (Luong et al., 2015) and is inspired by the selective attention model of Gregor et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 10, "context": ", 2015) and is inspired by the selective attention model of Gregor et al. (2015) for image generation.", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": "Lastly, we use a grounding attention inspired by Delbrouck and Dupont (2017). The mechanism merge each spatial location ai in the annotation sequence I with the initial decoder state s0 obtained in equation 5 with non-linearity :", "startOffset": 49, "endOffset": 77}, {"referenceID": 8, "context": "For this experiments on Multimodal Machine Translation, we used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities.", "startOffset": 85, "endOffset": 107}, {"referenceID": 17, "context": "All our models are build on top of the nematus framework (Sennrich et al., 2017).", "startOffset": 57, "endOffset": 80}, {"referenceID": 11, "context": "To create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size 14 \u00d7 14 \u00d7 1024 at its res4f layer (He et al., 2016).", "startOffset": 166, "endOffset": 183}, {"referenceID": 9, "context": "We apply dropout using one same mask in all time steps (Gal and Ghahramani, 2016).", "startOffset": 55, "endOffset": 81}, {"referenceID": 18, "context": "We use the byte pair encoding algorithm on the train set to convert space-separated tokens into subwords (Sennrich et al., 2016), reducing our vocabulary size to 9226 and 14957 words for English and German respectively.", "startOffset": 105, "endOffset": 128}, {"referenceID": 24, "context": "All variants of our attention model were trained with ADADELTA (Zeiler, 2012), with minibatches of size 80 for our monomodal (text-only) NMT model and 40 for our multimodal NMT.", "startOffset": 63, "endOffset": 77}, {"referenceID": 15, "context": "We use the metrics BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 7, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al.", "startOffset": 16, "endOffset": 43}, {"referenceID": 19, "context": ", 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006) to evaluate the quality of our models\u2019 translations.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "We notice a nice overall progress over Calixto et al. (2017) multimodal baseline, especially when using the stochastic attention.", "startOffset": 39, "endOffset": 61}, {"referenceID": 2, "context": "BLEU\u2191 METEOR\u2191 TER\u2193 Monomodal (text only) Caglayan et al. (2016) 32.", "startOffset": 41, "endOffset": 64}, {"referenceID": 2, "context": "BLEU\u2191 METEOR\u2191 TER\u2193 Monomodal (text only) Caglayan et al. (2016) 32.50 49.2 Calixto et al. (2017) 33.", "startOffset": 41, "endOffset": 97}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.", "startOffset": 11, "endOffset": 34}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.82 45.0 Huang et al. (2016) 36.", "startOffset": 11, "endOffset": 65}, {"referenceID": 2, "context": "Multimodal Caglayan et al. (2016) 27.82 45.0 Huang et al. (2016) 36.50 54.1 Calixto et al. (2017) 36.", "startOffset": 11, "endOffset": 98}, {"referenceID": 3, "context": "We pick Calixto et al. (2017) scores as baseline and report our results accordingly (green for improvement and red for deterioration).", "startOffset": 8, "endOffset": 30}, {"referenceID": 2, "context": "Note that even though our baseline NMT model is basically the same as Calixto et al. (2017), our experiments results are slightly better.", "startOffset": 70, "endOffset": 92}, {"referenceID": 2, "context": "We also compared our results to Caglayan et al. (2016) because our multimodal models are nearly identical with the major exception of the gating scalar (cfr.", "startOffset": 32, "endOffset": 55}, {"referenceID": 2, "context": "Without this scalar, the translation scores undergo a massive drop (as seen in Caglayan et al. (2016)) which means that the attention mechanisms don\u2019t really understand the more complex relationships between objects, what is really happening in the scene.", "startOffset": 79, "endOffset": 102}], "year": 2017, "abstractText": "In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image \u2192 German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.", "creator": "LaTeX with hyperref package"}}}