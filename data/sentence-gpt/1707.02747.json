{"id": "1707.02747", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jul-2017", "title": "Robust Imitation of Diverse Behaviors", "abstract": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train in.\n\n\n\n\n\nThis article was originally published in PNAS Web Science.", "histories": [["v1", "Mon, 10 Jul 2017 08:46:14 GMT  (2138kb,D)", "http://arxiv.org/abs/1707.02747v1", null], ["v2", "Fri, 14 Jul 2017 09:31:26 GMT  (1189kb,D)", "http://arxiv.org/abs/1707.02747v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ziyu wang", "josh merel", "scott reed", "greg wayne", "nando de freitas", "nicolas heess"], "accepted": true, "id": "1707.02747"}, "pdf": {"name": "1707.02747.pdf", "metadata": {"source": "CRF", "title": "Robust Imitation of Diverse Behaviors", "authors": ["Ziyu Wang", "Josh Merel", "Scott Reed", "Greg Wayne", "Nando de Freitas", "Nicolas Heess"], "emails": ["ziyu@google.com", "jsmerel@google.com", "reedscot@google.com", "gregwayne@google.com", "nandodefreitas@google.com", "heess@google.com"], "sections": [{"heading": "1 Introduction", "text": "Building versatile embodied agents, both in the form of real robots and animated avatars, capable of a wide and diverse set of behaviors is one of the long-standing challenges of AI. State-of-the-art robots cannot compete with the effortless variety and adaptive flexibility of motor behaviors produced by toddlers. Towards addressing this challenge, in this work we combine several deep generative approaches to imitation learning in a way that accentuates their individual strengths and addresses their limitations. The end product of this is a robust neural network policy that can imitate a large and diverse set of behaviors using few training demonstrations.\nWe first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders. The first decoder is a multi-layer perceptron (MLP) policy mapping a trajectory embedding and the current state to a continuous action vector. The second is a dynamics model mapping the embedding and previous state to the present state, while modelling correlations among states with a WaveNet [36]. Experiments with a 9 DoF Jaco robot arm and a 9 DoF 2D biped walker, implemented in the MuJoCo physics engine [35], show that the VAE learns a structured semantic embedding space, which allows for smooth policy interpolation.\nWhile supervised policies that condition on demonstrations (such as our VAE or the recent approach of Duan et al. [6]) are powerful models for one-shot imitation, they require large training datasets in order to work for non-trivial tasks. They also tend to be brittle and fail when the agent diverges too much from the demonstration trajectories. These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].\n\u2217Joint First authors.\nar X\niv :1\n70 7.\n02 74\n7v 1\n[ cs\n.L G\n] 1\n0 Ju\nl 2 01\n7\nRecently, Ho and Ermon [11] showed a way to overcome the brittleness of supervised imitation using another type of deep generative model called Generative Adversarial Networks (GANs) [8]. Their technique, called Generative Adversarial Imitation Learning (GAIL) uses reinforcement learning, allowing the agent to interact with the environment during training. GAIL allows one to learn more robust policies with fewer demonstrations, but adversarial training introduces another difficulty called mode collapse [7]. This refers to the tendency of adversarial generative models to cover only a subset of modes of a probability distribution, resulting in a failure to produce adequately diverse samples. This will cause the learned policy to capture only a subset of control behaviors (which can be viewed as modes of a distribution), rather than allocating capacity to cover all modes.\nRoughly speaking, VAEs can model diverse behaviors without dropping modes, but do not learn robust policies, while GANs give us robust policies but insufficiently diverse behaviors. In section 3, we show how to engineer an objective function that takes advantage of both GANs and VAEs to obtain robust policies capturing diverse behaviors. In section 4, we show that our combined approach enables us to learn diverse behaviors for a 9 DoF 2D biped and a 62 DoF humanoid, where the VAE policy alone is brittle and GAIL alone does not capture all of the diverse behaviors."}, {"heading": "2 Background and Related Work", "text": "We begin our brief review with generative models. One canonical way of training generative models is to maximize the likelihood of the data: max \u2211 i log p\u03b8(xi). This is equivalent to minimizing the Kullback-Leibler divergence between the distribution of the data and the model: DKL(pdata(\u00b7)||p\u03b8(\u00b7)). For highly-expressive generative models, however, optimizing the loglikelihood is often intractable.\nOne class of highly-expressive yet tractable models are the auto-regressive models which decompose the log likelihood as log p(x) = \u2211 i log p\u03b8(xi|x<i). Auto-regressive models have been highly effective in both image and audio generation [37, 36].\nInstead of optimizing the log-likelihood directly, one can introduce a parametric inference model over the latent variables, q\u03c6(z|x), and optimize a lower bound of the log-likelihood:\nEq\u03c6(z|xi) [log p\u03b8(xi|z)]\u2212DKL (q\u03c6(z|xi)||p(z)) \u2264 log p(x). (1) For continuous latent variables, this bound can be optimized efficiently via the re-parameterization trick [14, 24]. This class of models are often referred to as VAEs.\nGANs, introduced by Goodfellow et al. [8], have become very popular. GANs use two networks: a generator G and a discriminator D. The generator attempts to generate samples that are indistinguishable from real data. The job of the discriminator is then to tell apart the data and the samples, predicting 1 with high probability if the sample is real and 0 otherwise. More precisely, GANs optimize the following objective function\nmin G max D Epdata(x) [logD(x)] + Ep(z) [log(1\u2212D(G(z))] . (2)\nAuto-regressive models, VAEs and GANs are all highly effective generative models, but have different trade-offs. GANs were noted for their ability to produce sharp image samples, unlike the blurrier samples from contemporary VAE models [8]. However, unlike VAEs and autoregressive models trained via maximum likelihood, they suffer from the mode collapse problem [7]. Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain. Like GANs, autoregressive models produce sharp and at times realistic image samples [37], but they tend to be slow to sample from and unlike VAEs do not immediately provide a latent vector representation of the data. This is why we used VAEs to learn representations of demonstration trajectories.\nWe turn our attention to imitation. Imitation is the problem of learning a control policy that mimics a behavior provided via a demonstration. It is natural to view imitation learning from the perspective of generative modeling. However, unlike in image and audio modeling, in imitation the generation process is constrained by the environment and the agent\u2019s actions, with observations becoming accessible through interaction. Imitation learning brings its own unique challenges.\nIn this paper, we assume that we have been provided with demonstrations {\u03c4i}i where the i-th trajectory of state-action pairs is \u03c4i = {xi1, ai1, \u00b7 \u00b7 \u00b7 , xiTi , a i Ti }. These trajectories may have been produced by either an artificial or natural agent.\nAs in generative modeling, we can easily apply maximum likelihood to imitation learning. For instance, if the dynamics are tractable, we can maximize the likelihood of the states directly: max\u03b8 \u2211 i \u2211Ti t=1 log p(x i t+1|xit, \u03c0\u03b8(xit)). If a model of the dynamics is unavailable, we can instead\nmaximize the likelihood of the actions: max\u03b8 \u2211 i \u2211Ti t=1 log \u03c0\u03b8(a i t|xit). The latter approach is what we referred to as behavioral cloning (BC) in the introduction.\nWhen demonstrations are plentiful, BC is effective [22, 27, 6]. Without abundant data, BC is known to be inadequate [25, 26, 11]. The inefficiencies of BC stem from the sequential nature of the problem. When using BC, even the slightest errors in mimicking the demonstration behavior can quickly accumulate as the policy is unrolled. A good policy should correct for mistakes made previously, but for BC to achieve this, the corrective behaviors have to appear frequently in the training data.\nGAIL [11] avoids some of the pitfalls of BC by allowing the agent to interact with the environment and learn from these interactions. It constructs a reward function using GANs to measure the similarity between the policy-generated trajectories and the expert trajectories. As in GANs, GAIL adopts the following objective function\nmin \u03b8 max \u03c8 E\u03c0E [logD\u03c8(x, a)] + E\u03c0\u03b8 [log(1\u2212D\u03c8(x, a))] , (3)\nwhere \u03c0E denotes the expert policy that generated the demonstration trajectories.\nTo avoid differentiating through the system dynamics, policy gradient algorithms are used to train the policy by maximizing the discounted sum of rewards r\u03c8(xt, at) = \u2212 log(1 \u2212 D\u03c8(xt, at)). Maximizing this reward, which may differ from the expert reward, drives \u03c0\u03b8 to expert-like regions of the state-action space. In practice, trust region policy optimization (TRPO) is used to stabilize the learning process [28]. GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.\nThe literature on imitation including BC, apprenticeship learning and inverse reinforcement learning is vast. We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13]. Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.\nIn graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20]. Yet, as pointed out in a recent hierarchical control paper [21], the design of such controllers often requires significant human insight. Our focus is on flexible, general imitation methods."}, {"heading": "3 A Generative Modeling Approach to Imitating Diverse Behaviors", "text": ""}, {"heading": "3.1 Behavioral cloning with variational autoencoders suited for control", "text": "In this section, we follow a similar approach to Duan et al. [6], but opt for stochastic VAEs as having a distribution q\u03c6(z|x1:T ) to better regularize the latent space. In our VAE, an encoder maps a demonstration sequence to an embedding vector z. Given z, we decode both the state and action trajectories as shown in Figure 1. To train the model, we minimize the following loss:\nL(\u03b1,w, \u03c6; \u03c4i)=\u2212Eq\u03c6(z|xi1:Ti ) [ Ti\u2211 t=1 log \u03c0\u03b1(a i t|xit, z)+log pw(xit+1|xit, z) ] +DKL ( q\u03c6(z|xi1:Ti)||p(z) ) Our encoder q uses a bi-directional LSTM. To produce the final embedding, it calculates the average of all the outputs of the second layer of this LSTM before applying a final linear transformation to generate the mean and standard deviation of an Gaussian. We take one sample from this Gaussian as our demonstration encoding.\nThe action decoder is an MLP that maps both the state and the embedding to the parameters of a Gaussian. The state decoder is similar to a conditional WaveNet model [36]. In particular, it conditions on the embedding z and previous state xt\u22121 to generate the vector xt autoregressively. That is, the autoregression is over the components of the vector xt. Wavenet lessens the load of the\nencoder which no longer has to carry information that can be captured by modeling auto-correlations between components of the state vector . Finally, instead of a Softmax, we use a mixture of Gaussians as the output of the WaveNet."}, {"heading": "3.2 Diverse generative adversarial imitation learning", "text": "As pointed out earlier, it is hard for BC policies to mimic experts under environmental perturbations. Our solution to obtain more robust policies from few demonstrations, which are also capable of diverse behaviors, is to build on GAIL. Specifically, to enable GAIL to produce diverse solutions, we condition the discriminator on the embeddings generated by the VAE encoder and integrate out the GAIL objective with respect to the variational posterior q\u03c6(z|x1:T ). Specifically, we train the discriminator by optimizing the following objective\nmax \u03c8 E\u03c4i\u223c\u03c0E { Eq(z|xi1:Ti ) [ 1\nTi Ti\u2211 t=1 logD\u03c8(x i t, a i t|z) + E\u03c0\u03b8 [log(1\u2212D\u03c8(x, a|z))]\n]} . (4)\nA related work [18] introduces a conditional GAIL objective to learn controllers for multiple behaviors from state trajectories, but the discriminator conditions on an annotated class label, as in conditional GANs [19].\nWe condition on unlabeled trajectories, which have been passed through a powerful encoder, and hence our approach is capable of one-shot imitation learning. Moreover, the VAE encoder enables us to obtain a continuous latent embedding space where interpolation is possible, as shown in Figure 3.\nSince our discriminator is conditional, the reward function is also conditional: rt\u03c8(xt, at|z) = \u2212 log(1 \u2212D\u03c8(xt, at|z)). We also clip the reward so that it is upper-bounded. Conditioning on z allows us to generate an infinite number of reward functions each of them tailored to imitating a different trajectory. Policy gradients, though mode seeking, will not cause collapse into one particular mode due to the diversity of reward functions.\nTo better motivate our objective, let us temporarily leave the context of imitation learning and consider the following alternative value function for training GANs\nmin G max D V (G,D) = \u222b y p(y) \u222b z q(z|y) [ logD(y|z) + \u222b y\u0302 G(y\u0302|z) log(1\u2212D(y\u0302|z))dy\u0302 ] dydz.\nThis function is a simplification of our objective function. Furthermore, it satisfies the following property.\nLemma 1. Assuming that q computes the true posterior distribution that is q(z|y) = p(y|z)p(z)p(y) , then\nV (G,D) = \u222b z p(z) [\u222b y p(y|z) logD(y|z)dy + \u222b x\u0302 G(y\u0302|z) log(1\u2212D(y\u0302|z))dy\u0302 ] dz.\nAlgorithm 1 Diverse generative adversarial imitation learning. INPUT: Demonstration trajectories {\u03c4i}i and VAE encoder q. repeat\nfor j \u2208 {1, \u00b7 \u00b7 \u00b7 , n} do Sample trajectory \u03c4j from the demonstration set and sample zj \u223c q(\u00b7|xj1:Tj ). Run policy \u03c0\u03b8(\u00b7|zj) to obtain the trajectory \u03c4\u0302j . end for Update policy parameters via TRPO with rewards rjt (x j t , a j t |zj) = \u2212 log(1\u2212D\u03c8(x j t , a j t |zj)). Update discriminator parameters from \u03c8i to \u03c8i+1 with gradient:\n\u2207\u03c8  1n n\u2211 j=1  1 Tj Tj\u2211 t=1 logD\u03c8(x j t , a j t |zj) +  1 T\u0302j T\u0302j\u2211 t=1 log(1\u2212D\u03c8(x\u0302jt , a\u0302 j t |zj))  until Max iteration or time reached.\nIf we further assume an optimal discriminator [8], the cost optimized by the generator then becomes\nC(G) = 2 \u222b z p(z)JSD [p( \u00b7 |z) ||G( \u00b7 |z)] dz \u2212 log 4, (5)\nwhere JSD stands for the Jensen-Shannon divergence. We know that GANs approximately optimize this divergence, and it is well documented that optimizing it leads to mode seeking behavior [34].\nThe objective defined in (5) alleviates this problem. Consider an example where p(x) is a mixture of Gaussians and p(z) describes the distribution over the mixture components. In this case, the conditional distribution p(x|z) is not multi-modal, and therefore minimizing the Jensen-Shannon divergence is no longer problematic. In general, if the latent variable z removes most of the ambiguity, we can expect the conditional distributions to be close to uni-modal and therefore our generators to be non-degenerate. In light of this analysis, we would like q to be as close to the posterior as possible and hence our choice of training q with VAEs.\nWe now turn our attention to some algorithmic considerations. We can use the VAE policy \u03c0\u03b1(at|xt, z) to accelerate the training of \u03c0\u03b8(at|xt, z). One possible route is to initialize the weights \u03b8 to \u03b1. However, before the policy behaves reasonably, the noise injected into the policy for exploration (when using stochastic policy gradients) can cause poor initial performance. Instead, we fix \u03b1 and structure the conditional policy as follows\n\u03c0\u03b8( \u00b7 |x, z) = N ( \u00b7 |\u00b5\u03b8(x, z) + \u00b5\u03b1(x, z), \u03c3\u03b8(x, z)) , where \u00b5\u03b1 is the mean of the VAE policy. Finally, the policy parameterized by \u03b8 is optimized with TRPO [28] while holding parameters \u03b1 fixed, as shown in Algorithm 1."}, {"heading": "4 Experiments", "text": "The primary focus of our experimental evaluation is to demonstrate that the architecture allows learning of robust controllers capable of producing the full spectrum of demonstration behaviors for a diverse range of challenging control problems. We consider three bodies: a 9 DoF robotic arm, a 9 DoF planar walker, and a 62 DoF complex humanoid (56-actuated joint angles, and a freely translating and rotating 3d root joint). While for the reaching task BC is sufficient to obtain a working controller, for the other two problems our full learning procedure is critical.\nWe analyze the resulting embedding spaces and demonstrate that they exhibit rich and sensible structure that an be exploited for control. Finally, we show that the encoder can be used to capture the gist of novel demonstration trajectories which can then be reproduced by the controller.\nAll experiments are conducted with the MuJoCo physics engine [35]. For details of the simulation and the experimental setup please see appendix."}, {"heading": "4.1 Robotic arm reaching", "text": "We first demonstrate the effectiveness of our VAE architecture and investigate the nature of the learned embedding space on a reaching task with a simulated Jaco arm. The physical Jaco is a robotics arm developed by Kinova Robotics.\nTo obtain demonstrations, we trained 60 independent policies to reach to random target locations2 in the workspace starting from the same initial configuration. We generated 30 trajectories from each of the first 50 policies. These serve as training data for the VAE model (1500 training trajectories in total). The remaining 10 policies were used to generate test data.\nThe reaching task is relatively simple, so with this amount of data the VAE policy is fairly robust. After training, the VAE encodes and reproduces the demonstrations as shown in Figure 2. Representative examples can be found in the video in the supplemental material.\nTo further investigate the nature of the embedding space we encode two trajectories. Next, we construct the embeddings of interpolating policies by taking convex combinations of the embedding vectors of the two trajectories. We condition the VAE policy on these interpolating embeddings and execute it. The results of this experiment are illustrated with a representative pair in Figure 3. We observe that interpolating in the latent space indeed corresponds to interpolation in task (trajectory endpoint) space, highlighting the semantic meaningfulness of the discovered latent space."}, {"heading": "4.2 2D Walker", "text": "We found reaching behavior to be relatively easy to imitate, presumably because it does not involve much physical contact. As a more challenging test we consider bipedal locomotion. We train 60 neural network policies for a 2d walker to serve as demonstrations3. These policies are each trained to move at different speeds both forward and backward depending on a label provided as additional input to the policy. Target speeds for training were chosen from a set of four different speeds (m/s): -1, 0, 1, 3. For the distribution of speeds that the trained policies actually achieve see Figure 4, top right). Besides the target speed the reward function imposes few constraints on the behavior. The resulting policies thus form a diverse set with several rather idiosyncratic movement styles. While for most purposes this diversity is undesirable, for the present experiment we consider it a feature.\n2See appendix for details 3See section A.2 in the appendix for details.\nWe trained our model with 20 episodes per policy (1200 demonstration trajectories in total, each with a length of 400 steps or 10s of simulated time). In this experiment our full approach is required: training the VAE with BC alone can imitate some of the trajectories, but it performs poorly in general, presumably because our relatively small training set does not cover the space of trajectories sufficiently densely. On this generated dataset, we also train policies with GAIL using the same architecture and hyper-parameters. Due to the lack of conditioning, GAIL does not reproduce coherently trajectories. Instead, it simply meshes different behaviors together. In addition, the policies trained with GAIL also exhibit dramatically less diversity; see video.\nA general problem of adversarial training is that there is no easy way to quantitatively assess the quality of learned models. Here, since we aim to imitate particular demonstration trajectories that were trained to achieve particular target speed(s) we can use the difference between the speed of the demonstration trajectory the trajectory produced by the decoder as a surrogate measure of the quality of the imitation (cf. also [11]).\nThe general quality of the learned model and the improvement achieved by the adversarial stage of our training procedure are quantified in Fig. 4. We draw 660 trajectories (11 trajectories each for all 60 policies) from the training set, compute the corresponding embedding vectors using the encoder, and use both the VAE policy as well as the improved policy from the adversarial stage to imitate each of the trajectories. We determine the absolute values of the difference between the average speed of the demonstration and the imitation trajectories (measured in m/s). As shown in Fig. 4 the adversarial training greatly improves reliability of the controller as well as the ability of the model to accurately match the speed of the demonstration. Video of our agent imitating a diverse set of behaviors can be found in the supplemental material.\nTo assess generalization to novel trajectories we encode and subsequently imitate trajectories not contained in the training set. The supplemental video contains several representative examples, demonstrating that the style of movement is successfully imitated for previously unseen trajectories.\nFinally, we analyze the structure of the embedding space. We embed training trajectories and perform dimensionality reduction with t-SNE [38]. The result is shown in Fig. 4. It reveals a clear clustering according to movement speeds thus recovering the nature of the task context for the demonstration trajectories. We further find that trajectories that are nearby in embedding space tend to correspond to similar movement styles even when differing in speed."}, {"heading": "4.3 Complex humanoid", "text": "We consider a humanoid body of high dimensionality that poses a hard control problem. The construction of this body and associated control policies is described in [18], and is briefly summarized in the appendix (section A.3) for completness. We generate training trajectories with the existing controllers, which can produce instances of one of six different movement styles (see section A.3). Examples of such trajectories are shown in Fig. 5 and in the supplemental video.\nThe training set consists of 250 random trajectories from 6 different neural network controllers that were trained to match 6 different movement styles from the CMU motion capture data base4. Each trajectory is 334 steps or 10s long. We use a second set of 5 controllers from which we generate trajectories for evaluation (3 of these policies were trained on the same movement styles as the policies used for generating training data).\nSurprisingly, despite the complexity of the body, supervised learning is quite effective at producing sensible controllers: The VAE policy is reasonably good at imitating the demonstration trajectories, although it lacks the robustness to be practically useful. Adversarial training dramatically improves the stability of the controller. We analyze the improvement quantitatively by computing the percentage of the humanoid falling down before the end of an episode while imitating either training or test policies. The results are summarized in Figure 5 right. The figure further shows sequences of frames of representative demonstration and associated imitation trajectories. Videos of demonstration and imitation behaviors can be found in the supplemental video.\nFor practical purposes it is desirable to allow the controller to transition from one behavior to another. We test this possibility in an experiment similar to the one for the Jaco arm: We determine the embedding vectors of pairs of demonstration trajectories, start the trajectory by conditioning on the first embedding vector, and then transition from one behavior to the other half-way through the episode by blending their embeddings over a window of 20 control steps. Although not always successful the learned controller often transitions robustly, despite not having been trained to do so. Representative examples of these transitions can be found in the supplemental video."}, {"heading": "5 Conclusions", "text": "We have proposed an approach for imitation learning that combines the favorable properties of techniques for density modeling with latent variables (VAEs) with those of GAIL. The result is a model that learns, from a moderate number of demonstration trajectories (1) a semantically well structured embedding of behaviors, (2) a corresponding multi-task controller that allows to robustly execute diverse behaviors from this embedding space, as well as (3) an encoder that can map new trajectories into the embedding space and hence allows for one-shot imitation.\nOur experimental results demonstrate that our approach can work on a variety of control problems, and that it scales even to very challenging ones such as the control of a simulated humanoid with a large number of degrees of freedoms.\n4See appendix for details."}, {"heading": "A Details of the experiments", "text": "We trained the random reaching policies with deep deterministic policy gradients (DDPG, [31, 16]) to reach to random positions in the workspace. Simulations were ran for 2.5 secs or 50 steps. For more details on the hyper-parameters and network configuration, please refer to Table 1.\nA.2 Walker\nThe demonstration policies were trained to reach different speeds. Target speeds were chosen from a set of four different speeds (m/s) -1, 0, 1, 3. For each target speed in {\u22121, 0, 1, 3}, we trained 12 policies. Another 12 policies are each trained to achieve three target speeds -1, 0, and 1 depending on a context label. Finally 12 policies are each trained to achieve three target speeds -1, 0, and 3 depending on a context label. For each target speed group, a grid search over two parameters are performed: the initial log sigma for the policy and random seeds. We use 4 initial log sigma values: 0, -1, -2, -3 and three seeds.\nFor more details on the hyper-parameters and network configurations used see Tables 1 3, and 4.\nA.3 Humanoid\nThe model of the humanoid body was generated from subject 8 from the CMU database, also used in [18]. It has 56 actuated joint-angles and a freely translating and rotating root. The actions specified for the body correspond to torques of the joint angles.\nWe generate training trajectories from six different neural network controllers trained to imitate six different movement styles (simple walk, cat style, chicken style, drunk style, and old style). Policies were produced which demonstrate robust, generalized behavior in the style of a simple walk or single motion capture clip from various styles [18]. For evaluation we use a second set of five different policies that had been independently trained on a partially overlapping set of movement styles (drunk style, normal style, old style, sexy-swagger style, strong style).\nFor more details on the hyper-parameters and network configurations used see Tables 1 3, and 4. To assist the training of the discriminator, we only use a subset of features as inputs to our discriminator following [18]. This subset includes mostly features pertaining to end-effector positions."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 57(5):469\u2013483,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "Preprint arXiv:1701.07875,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Model-based adversarial imitation learning", "author": ["N. Baram", "O. Anschel", "S. Mannor"], "venue": "Preprint arXiv:1612.02179,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "BEGAN: Boundary equilibrium generative adversarial networks", "author": ["D. Berthelot", "T. Schumm", "L. Metz"], "venue": "Preprint arXiv:1703.10717,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Robot programming by demonstration", "author": ["A. Billard", "S. Calinon", "R. Dillmann", "S. Schaal"], "venue": "Springer handbook of robotics, pages 1371\u20131394.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "One-shot imitation learning", "author": ["Y. Duan", "M. Andrychowicz", "B. Stadie", "J. Ho", "J. Schneider", "I. Sutskever", "P. Abbeel", "W. Zaremba"], "venue": "Preprint arXiv:1703.07326,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "NIPS 2016 tutorial: Generative adversarial networks", "author": ["I. Goodfellow"], "venue": "Preprint arXiv:1701.00160,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, pages 2672\u20132680,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional LSTM networks for improved phoneme classification and recognition", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "Artificial Neural Networks: Formal Models and Their Applications\u2013ICANN 2005, pages 753\u2013753,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Boundary-seeking generative adversarial networks", "author": ["R.D. Hjelm", "A.P. Jacob", "T. Che", "K. Cho", "Y. Bengio"], "venue": "Preprint arXiv:1702.08431,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "NIPS, pages 4565\u20134573,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Imitation learning: A survey of learning methods", "author": ["A. Hussein", "M.M. Gaber", "E. Elyan", "C. Jayne"], "venue": "ACM Computing Surveys, 50(2):21,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Auto-encoding variational bayes", "author": ["D. Kingma", "M. Welling"], "venue": "Preprint arXiv:1312.6114,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Imitating driver behavior with generative adversarial networks", "author": ["A. Kuefler", "J. Morton", "T. Wheeler", "M. Kochenderfer"], "venue": "Preprint arXiv:1701.06699,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Continuous control with deep reinforcement learning", "author": ["T. Lillicrap", "J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv:1509.02971,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y.K. Lau", "Z. Wang", "S.P. Smolley"], "venue": "Preprint ArXiv:1611.04076,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning human behaviors from motion capture by adversarial imitation", "author": ["J. Merel", "Y. Tassa", "TB. Dhruva", "S. Srinivasan", "J. Lemmon", "Z. Wang", "G. Wayne", "N. Heess"], "venue": "Preprint arXiv:1707.02201,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "Preprint arXiv:1411.1784,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Contact-aware nonlinear control of dynamic characters", "author": ["U. Muico", "Y. Lee", "J. Popovi\u0107", "Z. Popovi\u0107"], "venue": "SIGGRAPH,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "DeepLoco: Dynamic locomotion skills using hierarchical deep reinforcement learning", "author": ["X.B. Peng", "G. Berseth", "K. Yin", "M. van de Panne"], "venue": "In SIGGRAPH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["D.A. Pomerleau"], "venue": "Neural Computation, 3(1):88\u201397,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Loss-sensitive generative adversarial networks on Lipschitz densities", "author": ["G.J. Qi"], "venue": "Preprint arXiv:1701.06264,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "A. Bagnell"], "venue": "AIStats,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "AIStats,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Policy distillation", "author": ["A. Rusu", "S. Colmenarejo", "C. Gulcehre", "G. Desjardins", "J. Kirkpatrick", "R. Pascanu", "V. Mnih", "K. Kavukcuoglu", "R. Hadsell"], "venue": "Preprint arXiv:1511.06295,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Synthesis of controllers for stylized planar bipedal walking", "author": ["D. Sharon", "M. van de Panne"], "venue": "In ICRA,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Deterministic policy gradient algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Simulating biped behaviors from human motion data", "author": ["K.W. Sok", "M. Kim", "J. Lee"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Third-person imitation learning", "author": ["B.C. Stadie", "P. Abbeel", "I. Sutskever"], "venue": "Preprint arXiv:1703.01703,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "Preprint arXiv:1511.01844,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "MuJoCo: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "IROS, pages 5026\u20135033,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "Preprint arXiv:1609.03499,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Conditional image generation with pixelCNN decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "MAGAN: Margin adaptation for generative adversarial networks", "author": ["R. Wang", "A. Cully", "H. Jin Chang", "Y. Demiris"], "venue": "Preprint arXiv:1704.03817,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 13, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 51, "endOffset": 59}, {"referenceID": 23, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 51, "endOffset": 59}, {"referenceID": 11, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 28, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 8, "context": "We first introduce a variational autoencoder (VAE) [14, 24] for supervised imitation, consisting of a bi-directional LSTM [12, 29, 9] encoder mapping demonstration sequences to embedding vectors, and two decoders.", "startOffset": 122, "endOffset": 133}, {"referenceID": 35, "context": "The second is a dynamics model mapping the embedding and previous state to the present state, while modelling correlations among states with a WaveNet [36].", "startOffset": 151, "endOffset": 155}, {"referenceID": 34, "context": "Experiments with a 9 DoF Jaco robot arm and a 9 DoF 2D biped walker, implemented in the MuJoCo physics engine [35], show that the VAE learns a structured semantic embedding space, which allows for smooth policy interpolation.", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "[6]) are powerful models for one-shot imitation, they require large training datasets in order to work for non-trivial tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 24, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 115, "endOffset": 123}, {"referenceID": 25, "context": "These limitations of supervised learning for imitation, also known as behavioral cloning (BC) [22], are well known [25, 26].", "startOffset": 115, "endOffset": 123}, {"referenceID": 10, "context": "Recently, Ho and Ermon [11] showed a way to overcome the brittleness of supervised imitation using another type of deep generative model called Generative Adversarial Networks (GANs) [8].", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "Recently, Ho and Ermon [11] showed a way to overcome the brittleness of supervised imitation using another type of deep generative model called Generative Adversarial Networks (GANs) [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 6, "context": "GAIL allows one to learn more robust policies with fewer demonstrations, but adversarial training introduces another difficulty called mode collapse [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 36, "context": "Auto-regressive models have been highly effective in both image and audio generation [37, 36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 35, "context": "Auto-regressive models have been highly effective in both image and audio generation [37, 36].", "startOffset": 85, "endOffset": 93}, {"referenceID": 13, "context": "(1) For continuous latent variables, this bound can be optimized efficiently via the re-parameterization trick [14, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 23, "context": "(1) For continuous latent variables, this bound can be optimized efficiently via the re-parameterization trick [14, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 7, "context": "[8], have become very popular.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "GANs were noted for their ability to produce sharp image samples, unlike the blurrier samples from contemporary VAE models [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "However, unlike VAEs and autoregressive models trained via maximum likelihood, they suffer from the mode collapse problem [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 3, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 16, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 22, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 38, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 9, "context": "Recent work has focused on alleviating mode collapse in image modeling [2, 4, 17, 23, 39, 10], but so far these have not been demonstrated in the control domain.", "startOffset": 71, "endOffset": 93}, {"referenceID": 36, "context": "Like GANs, autoregressive models produce sharp and at times realistic image samples [37], but they tend to be slow to sample from and unlike VAEs do not immediately provide a latent vector representation of the data.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 26, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 5, "context": "When demonstrations are plentiful, BC is effective [22, 27, 6].", "startOffset": 51, "endOffset": 62}, {"referenceID": 24, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 25, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 10, "context": "Without abundant data, BC is known to be inadequate [25, 26, 11].", "startOffset": 52, "endOffset": 64}, {"referenceID": 10, "context": "GAIL [11] avoids some of the pitfalls of BC by allowing the agent to interact with the environment and learn from these interactions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "In practice, trust region policy optimization (TRPO) is used to stabilize the learning process [28].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 97, "endOffset": 100}, {"referenceID": 32, "context": "GAIL has become a popular choice for imitation learning [15] and there already exist model-based [3] and third-person [33] extensions.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 0, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 12, "context": "We cannot cover this literature at the level of detail it deserves, and instead refer readers to recent authoritative surveys on the topic [5, 1, 13].", "startOffset": 139, "endOffset": 149}, {"referenceID": 10, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 32, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 5, "context": "Inspired by recent works, including [11, 33, 6], we focus on taking advantage of the dramatic recent advances in deep generative modelling to learn high-dimensional policies capable of learning a diverse set of behaviors from few demonstrations.", "startOffset": 36, "endOffset": 47}, {"referenceID": 29, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 31, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 19, "context": "In graphics, a significant effort has been devoted to the design physics controllers that take advantage of motion capture data, or key-frames and other inputs provided by animators [30, 32, 40, 20].", "startOffset": 182, "endOffset": 198}, {"referenceID": 20, "context": "Yet, as pointed out in a recent hierarchical control paper [21], the design of such controllers often requires significant human insight.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "[6], but opt for stochastic VAEs as having a distribution q\u03c6(z|x1:T ) to better regularize the latent space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "The state decoder is similar to a conditional WaveNet model [36].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "A related work [18] introduces a conditional GAIL objective to learn controllers for multiple behaviors from state trajectories, but the discriminator conditions on an annotated class label, as in conditional GANs [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "A related work [18] introduces a conditional GAIL objective to learn controllers for multiple behaviors from state trajectories, but the discriminator conditions on an annotated class label, as in conditional GANs [19].", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "If we further assume an optimal discriminator [8], the cost optimized by the generator then becomes", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "We know that GANs approximately optimize this divergence, and it is well documented that optimizing it leads to mode seeking behavior [34].", "startOffset": 134, "endOffset": 138}, {"referenceID": 27, "context": "Finally, the policy parameterized by \u03b8 is optimized with TRPO [28] while holding parameters \u03b1 fixed, as shown in Algorithm 1.", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "All experiments are conducted with the MuJoCo physics engine [35].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "also [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "We embed training trajectories and perform dimensionality reduction with t-SNE [38].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "The construction of this body and associated control policies is described in [18], and is briefly summarized in the appendix (section A.", "startOffset": 78, "endOffset": 82}], "year": 2017, "abstractText": "Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.", "creator": "LaTeX with hyperref package"}}}