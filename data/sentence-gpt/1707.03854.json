{"id": "1707.03854", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2017", "title": "Estimating the unseen from multiple populations", "abstract": "Given samples from a distribution, how many new elements should we expect to find if we continue sampling this distribution? This is an important and actively studied problem, with many applications ranging from unseen species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population $j$ has an unknown distribution $D_j$ from which we observe $n_j$ samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. For example, if $j$ has no values $n_j$ from which to select a sample, then $j$ samples are found within that sample $d_j$ from which to select a sample. We use the two-sample technique to estimate the number of distinct species to find in each sample $n_j$ sample, where we use two-sample method to estimate population $d_j$ from which to select a sample. The resulting error is a significant decrease in the number of individual species from the sample $n_j$ to which we use two-sample method to estimate population $d_j$ from which to select a sample. We used the three-sample approach to estimate the number of distinct species to select the sample of a representative population with a high-sample sampling rate. This difference in population estimates has no impact on the quality of populations with population estimates. We describe an error in estimation of different estimates in each sample, where the probability is less than 3% in every sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 12 Jul 2017 18:26:19 GMT  (1207kb,D)", "http://arxiv.org/abs/1707.03854v1", "13 pages, 3 figures, appearing at the International Conference on Machine Learning 2017 (ICML 2017)"]], "COMMENTS": "13 pages, 3 figures, appearing at the International Conference on Machine Learning 2017 (ICML 2017)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["aditi raghunathan", "gregory valiant", "james zou"], "accepted": true, "id": "1707.03854"}, "pdf": {"name": "1707.03854.pdf", "metadata": {"source": "META", "title": "Estimating the unseen from multiple populations", "authors": ["Aditi Raghunathan", "Gregory Valiant", "James Zou"], "emails": ["tir@stanford.edu>,", "<valiant@stanford.edu>,", "<jamesz@stanford.edu>."], "sections": [{"heading": "1. Introduction", "text": "Given samples from a distribution, many settings in machine learning and statistics involves estimating properties of the unseen portion of the distribution, i.e. elements in the support of the distribution that are not observed in the samples collected so far. One important example of estimating the unseen is the problem of predicting the number of distinct new elements in additional samples collected. This question is famously illustrated by the case of Corbet\u2019s butterflies. Alexander Corbet was a British naturalist who\n1Stanford University, Stanford, CA 2Chan Zuckerberg Biohub, San Francisco, CA. Correspondence to: Aditi Raghunathan <aditir@stanford.edu>, Gregory Valiant <valiant@stanford.edu>, James Zou <jamesz@stanford.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nspent two years in Malaya trapping butterflies. He found 118 rare species of butterflies for which he found only one specimen, another 74 species with two specimens, 44 with three specimens, etc. Corbet was naturally interested in the butterflies that are heretofore unseen. In particular, he wanted to estimate how many distinct new species of butterflies he can expect to discover if he were to conduct a new expedition to Malaya\u2014such an estimate could help determine whether a new experiment is warranted. GoodToulmin, extending earlier work of Ronald Fisher, came up with the remarkable estimate that the number of new species Corbet can expect to find is simply the alternating sum 118 - 74 + 44 - ... The Good-Toulmin estimator sparked the investigation into how to estimate the discovery rate of new elements and this remains an active area of research. Estimating the discovery rate has many important applications beyond the original species collection setting. In genomics, for example, an important question is: given the genetic variation already identified in the genomes of individuals from some population (say, East Asia), how many additional mutations do we expect to find by sequencing the genomes of additional individuals from East Asia. An accurate answer to this question can improve the cohort design of new population sequencing experiments.\nPredicting the number of new elements is a particular instance of estimating the unseen. In other applications, one may want to estimate different statistics that also depend on the currently unobserved elements. For example, one may want to predict how many new elements will be observed at least twice (for reproducibility) or at most three times (if the focus is on rare elements). More generally, one may want to estimate the histogram of the underlying distribution, which summarizes the frequency distribution of all the elements (see Sec. 2 for precise definition) and from which these other statistics can be derived.\nThe unseen estimation literature has focused on the setting where there is a single distribution which generate current samples as well as any future samples. In practice, we often have multiple distinct distributions and we observe varying number of samples from each distribution. In the genomics example above, in addition to sequencing data from East Asians, we also have genome sequences of individuals from Europe, Africa, etc. The relevant question is: given we currently have the genomes of ni individuals\nar X\niv :1\n70 7.\n03 85\n4v 1\n[ cs\n.L G\n] 1\n2 Ju\nl 2 01\n7\nfrom population i, i \u2208 {1, ...,m}, and we have identified all the genetic variants in this group, how many total new mutations do we expect to find if we sequence additional bi individuals from population i. Moreover, given a finite budget Nnew of new genomes that we can sequence, how should we allocate this budget across the different populations to maximize the expected number of new mutations oberved? Similarly, suppose Corbet had also collected butterflies in Brunei and Indonesia, in addition to Malaya. Then he might want to know how many totally new species he can expect to find if he was to spend, say, another six month in Malaya and one year in Brunei. He might also be interested in estimating the joint frequency distribution of butterflies across all three regions.\nOur contributions. In this paper, we address the general problem of estimating the unseen when we have samples from multiple populations, each corresponding to a potentially distinct distribution. Despite being very natural, this multi-population problem has not been systematically studied to the best of our knowledge. We derive a multi-population generalization of the Good-Toulmin estimator for the expected number of new elements. Surprisingly, we prove that the accuracy of our extrapolation estimator is independent of the number of populations. Moreover, it achieves the optimal super-linear extrapolation rate. Next, we develop an efficient optimization method to estimate the more general multi-population joint frequency distribution. This complements our extrapolation estimator, and outperforms the generalized Good-Toulmin estimator in most settings. This more general approach also enables predictions for other statistics of interest. We systematically validate these two algorithms on synthetic data as well as real datasets from population genetics and from English books. Moreover, we illustrate that by estimating the joint frequency distribution, we can significantly improve the discovery power under a budget constraint."}, {"heading": "2. Related works", "text": "The problem of estimating the properties of the unobserved portion of a distribution, given n samples, and the related problem of estimating the number of new domain elements that are likely to be observed if an additional cn samples are collected, dates back to works of I.J. Good and A. Turing (Good, 1953), and R.A. Fisher (Fisher et al., 1943). This was quickly followed by (Good & Toulmin, 1956), which introduced the Good-Toulmin estimator. While the Good-Toulmin estimator is always unbiased, the variance increases rapidly for c \u2265 1. Subsequent works, including (Efron & Thisted, 1976) have suggested \u201csmoothing\u201d approaches that tradeoff the bias and variance for this type of approach. The recent work of Orlitsky et al. (2016) describes a clever variant that achieves good performance for\nc = O(log n). This ability to accurately estimate the number of domain elements seen in a second sample of size up to O(n log n), where n denotes the size of the original sample, was concurrently shown via a different approach in (Valiant & Valiant, 2016). This logarithmic factor extrapolation matches the lower bounds of (Valiant & Valiant, 2011), to constant factors. The linear estimators that we propose in Section 4 for the multiple population setting, and their analysis, are extensions of the smoothed GoodToulmin estimators of (Orlitsky et al., 2016).\nA different approach to this problem was proposed by Efron & Thisted (1976), who considered a linearprogramming approach to estimating this property by implicitly finding a label-less representation of the underlying distribution that was consistent with the observed frequency counts, then returning the support size of this distribution. This approach was adapted and rigorously analyzed in (Valiant & Valiant, 2011; 2013), who showed that it provably yields an accurate representation of the frequency distribution of the underlying distribution, which can subsequently be leveraged to yield estimates of distributional properties, including entropy, distance metrics between distributions, and approximations for the number of new elements that would be observed in larger samples. Recent works (Valiant & Valiant, 2016; Zou et al., 2016) also established that this approach can accurately estimate the number of new elements that will be observed in samples of size up to O(n log n). Our optimization-based algorithm, described in Section 5, generalizes this approach."}, {"heading": "3. Definitions and examples", "text": "Let \u2126 denote the domain, and D1, ..., Dm denote m probability distributions over \u2126. Di represents the frequency of elements in population i. Note that it is not restrictive to assume that the populations share the same domain \u2126 since different Di\u2019s may have distinct supports. We model the multi-population unseen estimation as a two stage process. In the first period, we observe nj independent samples from the j-th population, {Xji } j=1,...,m i=1,...,nj\n. This is the seen data. In period two, which is in the future, we will sample additional tjnj samples from the j-th population, {Y ji } j=1,...,m i=1,...,tjnj . The period two samples are unseen and we would like to estimate some statistic U({Y ji }, {X j i }). We can think of tj \u2265 0 as the extrapolation factors. If tj is large, then we will obtain many more samples from population j in the second period compared to what we have, and the problem of estimating U could be more challenging. We can take tj as given for the purpose of estimating U . We later discuss how we to leverage our estimator of U to optimize the tj\u2019s in order to maximize the number of new discoveries. Note that in general, the nj\u2019s and tj\u2019s can differ arbitrarily across the populations.\nA particularly important statistic is U = the total number of new elements in {Y ji } that are not observed in the period one samples {Xji }. A good estimator for this U quantifies the expected information gain of the second period. In the one population setting, this statistic is the focus of Good-Toulmin and a large number of papers. Other useful choices of U could be the number of distinct new elements that are observed at least twice in {Y ji }, which could be relevant if we want some reproducibility.\nBeyond estimating these single parameters, we could also hope to use the samples {Xji } to estimate the histogram of D1, ..., Dm. The multi-population histogram, defined below, captures all of the information about the populations, other than the labels of the domain. Definition 3.1 (Multi-population histogram). Given a collection of m distributions D1, . . . , Dm over a common domain \u2126, the corresponding multi-population histogram H is a mapping from [0, 1]m \\ 0m 7\u2192 N \u222a {0}. For each \u03b1 = (\u03b11, \u03b12, . . . \u03b1m) \u2208 [0, 1]m \\ 0m, H(\u03b1) = |{y \u2208 \u2126 | Dj(y) = \u03b1j , 1 \u2264 j \u2264 m}|, where Di(y) is the probability mass of domain element y. in the ith distribution Di.\nAny symmetric multi-population statistic\u2014one that is invariant to permuting the labels of the domain\u2014is a function of only the histogram. Such statistics include distance metrics between the distributions/populations, measures of the entropy of the populations, and the number of new elements that one is likely to observe in a second batch of samples. The multi-population histogram is also of intrinsic interest; in population genetics, H is exactly the joint frequency distribution of mutations, and reveals information about demographic history (e.g. historical variations in population size) and selective pressures. One benefit of focusing on the histogram is that, while it does not contain as much information as the actual labeled distributions, it can often be accurately recovered even when given too few samples to learn the (labeled) distributions to any significant accuracy (Valiant & Valiant, 2011).\nBoth for directly predicting U and estimating H , we rely on a label-less representation of the samples, termed the fingerprint of {Xji }. The fingerprint of the samples is the analog of the histogram of the distributions, and captures all the information of {Xji } that is relevant for estimating symmetric statistics. Definition 3.2 (Multi-population fingerprint). Given the samples {Xji }, its fingerprint is an m-dimensional tensor \u03a6 whose i1...im-th entry, \u03c6i1...im , is the number of distinct elements observed exactly ij times in the samples from population j. Here each ij can range from 0 to nj . Example 3.1. Suppose we have five samples from Population 1, (A,B,C,E, F ), and seven from Population 2, (A,B,D,E,E, F, F ). The corresponding 2-dimensional fingerprint of this data is given by the following matrix:\n0 1 2 0 \u00b7 1 0 1 1 2 2\nThe (1, 1) entry is 2 because A,B are observed once in each set of samples; the (1, 0) entry is 1 because exactly one element, C, is observed once in the samples from Population 1 and zero times in the samples from Population 2. By convention, we omit the (0, 0) element."}, {"heading": "4. A linear estimator", "text": "Unbiased estimator. Given the empirical fingerprints \u03a6 and the extrapolation factors tj , j = 1, ...,m, we define the following estimator\nU\u0302 = \u2212 \u2211\ni1,...,im: \u2211 ij>0  m\u220f j=1 (\u2212tj)ij \u03c6i1...im . (1)\nU\u0302 is a weighted alternating sum of the empirical fingerprints where the weights are determined by the extrapolation factors tj .\nProposition 4.1. For any number of populations m, and any extrapolation factors tj \u2265 0, j = 1, ...m, U\u0302 is an unbiased estimator of U .\nProof of the proposition appears in Appendix 7.\nU\u0302 is linear in the fingerprint entries. Its computational cost is linear in the total number of period one samples, n = \u2211 j nj , since there can be at most n non-zero fingerprint entries. To build more intuition for U\u0302 , we illustrate its application in two simple settings.\nExample 4.2. Consider the setting where all m distribution are identical, i.e. all the samples are drawn from the same discrete distribution D. Let tj = 1,\u2200j for simplicity. After rearranging terms, U\u0302 can be written as\nU\u0302 = \u2211 k=1\n(\u22121)k+1  \u2211\n(i1,...,im): \u2211 ij=k \u03c6i1...im  . Because the populations are identical, the sum in the parenthesis is just the number of elements that are observed k times from all the samples so far. Hence the general estimator U\u0302 reduces to the one dimensional Good-Toulmin estimator when all m populations are identical.\nExample 4.3. Suppose the supports of the distributionsDi are disjoint. Then the only possible non-zero fingerprint entries are \u03c6i1...im where exactly one of the ij is great than 0 and all the other ij\u2019s are zero. For simplicity, assume tj = 1 for all j. Then U\u0302 = \u2211k j=1 \u2211 i(\u22121)k+1\u03c6ki , where \u03c6ki is the marginal fingerprint entry of the number of elements that are observed i times in population k. Hence\nwhen the populations are disjoint, the expected number of new elements is the sum of the expected number of new elements in each population. When the populations have overlapping support, we have the nontrivial interaction terms due to the cross-population fingerprint entries.\nGeneral weighted linear estimator. While U\u0302 is unbiased, its variance could be large if some of the extrapolation factors tj\u2019s are greater than 1. This is because the powers of tj appear in Eqn. 1. To address this issue, we introduce a general class of multi-population weighted linear estimators. U\u0302W = \u2212 \u2211\nij : \u2211 ij>0  m\u220f j=1 (\u2212tj)ij \u03c6i1,...,imW (i1, . . . , im).\nWe focus on a particular weighting scheme, which is an extension of that introduced in (Orlitsky et al., 2016): W (i1, i2, . . . im) = P ( L \u2265 \u2211 j\u2208A ij ) where L \u223c Poi(r) and A = {j : tj > 1} are the populations that we would like to extrapolate beyond the original sample size. If tj \u2264 1 \u2200j, then W = 1 and U\u0302W is just the unbiased estimator U\u0302 . The Poisson rate r is a tuning parameter that determines the bias/variance tradeoff of U\u0302W . As r increases, all the weights approaches 1 and U\u0302W approaches the unbiased estimator U\u0302 . As r decreases, the fingerprint entries \u03c6i1...im with some large ij\u2019s\u2014which are also the terms with high variance\u2014are weighted by a factor that is close to 0. This reduces the total variance of U\u0302W at the cost of introducing bias. We will see how to set r as a function of the nj\u2019s and tj\u2019s in order to minimize the overall estimation error. In the rest of the paper, unless otherwise specified, we will use U\u0302W to denote the multi-population linear estimator with Poisson weights.\nPerformance guarantee of the weighted estimator. We use relative mean squared error, E [(\nU\u0302W\u2212U\u2211 njtj\n)2] , to quan-\ntify the performance of U\u0302W . This is a natural error metric, because \u2211 njtj is the number of samples in period two and we care about how the error in the predicted number of new elements scales with the number of samples. Without loss of generality, we can relabel the populations so that t1 = maxjtj . We are especially interested in the setting when t1 \u2265 1 (i.e. large extrapolation). Proposition 4.4. Suppose t1 = maxj tj \u2265 1 and the Poisson rate is r = log( \u2211 j nj(tj+1))\n2t1 , then\nE ( U\u0302W \u2212 U\u2211 njtj )2 \u2264 (n1t1 +\u2211j nj n1t1 ) n \u22121/t1 1 . (2)\nRemark 4.5 (log extrapolation factor). Suppose the ratio n1\u2211\nj nj is bounded, then Prop. 4.4 guarantees that for\nany > 0, we can achieve E [(\nU\u0302W\u2212U\u2211 njtj\n)2] \u2264 with\nt1 = O(log n1/ log(1/ )). This means that U\u0302W has low relative error even when the largest extrapolation factor t1 is logarithmic in its initial sample size n1. Remark 4.6 (no dependence on m). Note that the relative error in Eqn. 2 does not depend on the number of populations m. This is somewhat surprising since the number of terms in U\u0302W potentially grows exponentially with m and the variance of each fingerprint entry \u03c6i1...im also increases as the number of population increases. This population agnostic property of U\u0302W guarantees its accuracy even when m is arbitrarily large. Remark 4.7 (lower bound). Here we have focused on a specific form of the estimator U\u0302W where the weights W of the fingerprint entries correspond to the tail probability of Poisson distributions. A natural question is whether there exists a different form of the weights or a different estimator altogether that can consistently be more accurate than our current U\u0302W . The answer is essentially no due to the following lower bound for one population extrapolation (Orlitsky et al., 2016; Valiant & Valiant, 2011): There exists universal constants c, c\u2032 such that for all estimators U\u0302 , if the extrapolation factor t > c, then \u2203 distribution such\nthat E [(\nU\u0302\u2212U nt )2] > \u223c n\u2212c \u2032/t. Here n is the number of sam-\nples drawn from this distribution in period one. This lower bound implies that in order to guarantee that the relative error is less than in general, the extrapolation factor can be at most O(log n/ log(1/ )), matching Prop. 4.4.\nOutline of the proof of Prop. 4.4 (detailed analysis is in the Appendix). To analyze the relative error, we separately quantify the bias and variance of U\u0302W in terms of nj , tj , r. Lemma 4.8 (Bias). Let r denote the rate of the Poisson weights, then\n\u2223\u2223\u2223E[U\u0302W \u2212 U ]\u2223\u2223\u2223 \u2264 \u2211 j\u2208A nj(tj + 1)  e\u2212r Lemma 4.9 (Variance). Without loss of generality, let t1 = maxj tj and suppose t1 \u2265 1 then\nVar(U\u0302W \u2212 U) \u2264 \u2211 nje 2r(t1\u22121) + \u2211 j njtj .\nTo obtain the optimal r given in the statement of Prop. 4.4, we set r to balance the squared bias and variance."}, {"heading": "5. Estimating the multi-population frequency distribution", "text": "While we have a linear estimator for the number of unseen elements in a new sample, it is challenging to con-\nstruct good estimators of other statistics (e.g. number of new elements observed \u2265 2) directly from the fingerprints. As discussed in Sec. 3, we can also take the less direct approach of first trying to estimating the true underlying multi-population histogram. Given an accurate reconstruction of this underlying histogram, we can then estimate any symmetric statistic of the future samples. We discuss some of the uses of such a representation in Section 5.\nRecovering the frequency distribution The core of our algorithm to recover the multi-population histogram is a natural extension of the single population algorithm presented in Valiant & Valiant (2011; 2013).\nEstimating the multi-population histogram: Core Approach. Input: Multi-population fingerprint \u03a6 of samples, Output: Two estimates, H\u0302counts and H\u0302ll of histogram corresponding to the distributions underlying fingerprint \u03a6.\n\u2022 Compute H\u0302counts and H\u0302ll minimizing the following expressions:\nH\u0302counts = arg min H \u2211 i 1\u221a 1 + \u03a6i |\u03a6i \u2212 \u03a6\u0302(H)i|.\nH\u0302ll = arg max H \u2211 i log poi(\u03a6i, \u03a6\u0302(H)i),\nWhere [\u03a6\u0302(H)]i = \u2211 \u03b1 H(\u03b1) m\u220f j=1 bino(\u03b1j , nj , ij).\nThe intuition behind these two optimization problems is the following. The histogram corresponding to a set of distributions is an unlabeled representation of the underlying distributions, hence it makes intuitive sense to try to recover the histogram that maximizes the likelihood of the unlabeled representation of the samples, namely the fingerprint \u03a6. Recent work (Acharya et al., 2016) provided rigorous support for this intuition. In general, however, this likelihood might be difficulty to compute. Nevertheless, an efficiently computable proxy for this likelihood can be obtained by treating the distribution of the fingerprint, corresponding to a histogram H , as a product distribution, with \u03a6i1,...,im distributed according to the Poisson distribution with appropriate expectation EH [\u03a6i1,...,im ]. The recent central limit theorem for \u201cPoisson Multinomials\u201d from (Valiant & Valiant, 2011) provides at least some corroboration for the reasonableness of having a proxy for the log-likelihood that decomposes linearly across the different elements of \u03a6. The motivation for the 1\u221a\n1+\u03a6i scaling\non the first proxy likelihood function is that this expression penalizes discrepancies between the observed and expected\nfingerprint entries according to a rough approximation of the standard deviation of that fingerprint entry, as the variance of a Poisson random variable is equal to its expectation, and the observed fingerprint entry is an approximation for the expected fingerprint entry given the true underlying histogram.\nThe work (Valiant & Valiant, 2013) focused on recovering H\u0302counts, as this optimization problem can be formulated as a linear program, whose variables correspond to a fine discretization of the potential support of the histogram. Unfortunately, in the present multi-distribution setting, the number of variables required by this linear programming approach would scale exponentially with the number of distributions in question. Even for fingerprints derived from modest-sized samples from two distributions, the resulting linear program becomes impractical.\nInstead of pursuing the linear programming based approach, we instead propose a black-box optimization approach to finding a histogram that optimizes either of the two proxy likelihood functions. In this optimization approach, the dimensionality of the optimization problem is specified by the user, and corresponds to the number of (i1, . . . , im) tuples for which the returned histogram H\u0302 is nonzero. Denoting this quantity by s, the resulting optimization problem can be regarded as the problem of specifying s vectors (h1, \u03b11,1, . . . , \u03b11,m), . . . , (hs, \u03b1s,1, . . . , \u03b1s,m). These s vectors are then interpreted as a histogram H with H(\u03b1j) = hj for all j \u2208 {1, . . . , s}, and H(\u03b1) = 0 for all other vectors \u03b1.\nThe one additional modification that leads to a substantial improvement in runtime is to only evaluate the proxy likelihood expressions for fingerprint entries \u03a6i1,...,im \u2265 2. The intuition for this is two-fold. First, the number of vectors (i1, . . . , im) for which \u03a6i1,...,im = 0 will scale exponentially with m, as opposed to scaling as some parameter of the sample sizes; this is clearly undesirable. Second, given that we wish to avoid evaluating the contribution to the proxy likelihood from fingerprint entries that are zero, we must now be careful in dealing with fingerprint entries that are equal to 1. Suppose we have 1 element with true probability in and suppose we observe that fingerprint entry \u03a6i = 1, and the other fingerprints near i are 0. Since we are maximizing the likelihood that \u03a6i = 1 (without taking into account the nearby 0 entries), we would assign roughly \u221a i elements to probability in which is undesirable. Removing the ones largely resolves this issue. Note that the \u03a6j = 2 entries do not cause as much of an issue, as such collisions are unlikely to occur in regions of the fingerprint in which there is not a significant number of domain elements.\nIn this one-distribution example, a constraint on the total probability mass being 1 would resolve this issue, though\nanalogs of this issue in the multiple distribution setting cannot be resolved in this way. Hence, we adopt the crude, but effective approach of viewing all the empirical fingerprint entries that are equal to 1 as being reflective of an element in the underlying set of distributions whose probability is close to the empirical probability of the corresponding element. We summarize the complete algorithm below:\nEstimating the multi-population histogram: Full Algorithm. Input: Multi-population fingerprint \u03a6 derived from samples from m distributions of respective sizes n1, . . . , nm. Output: Two estimates, H\u0302counts and H\u0302ll of histogram corresponding to the distributions underlying fingerprint \u03a6.\n\u2022 Remove fingerprint entries that are 1, and add to empirical portion of histogram:\n1. Initialize m-distribution histogram H\u0302emp to be identically zero.\n2. For each vector i = (i1, . . . , im) such that \u03a6i = 1, set H\u0302emp( i1n1 , . . . , im nm ) = 1.\n\u2022 Compute H\u0302counts and H\u0302ll minimizing the following expressions:\nH\u0302counts = arg min H \u2211 i:\u03a6(i)\u22652 1\u221a 1 + \u03a6i |\u03a6i \u2212 \u03a6\u0302(H)i|.\nH\u0302ll = arg max H \u2211 i:\u03a6(i)\u22652 log poi(\u03a6i, \u03a6\u0302(H)i.\nWhere \u03a6\u0302(H)i = \u2211 \u03b1 H(\u03b1) m\u220f j=1 bino(\u03b1j , nj , ij).\nSubject to the constraint that, together with H\u0302emp, the total mass in all the distributions is 1. Namely for all i \u2208 {1, . . . ,m},\u2211\n\u03b1 \u03b1iH\u0302ll(\u03b1) + \u2211 \u03b1 \u03b1iH\u0302\u2217(\u03b1) = 1.\n\u2022 Return the concatenation of the empirical portion of the histogram and the portion returned by the optimization: H\u0302count := H\u0302count + H\u0302emp, and H\u0302ll := H\u0302ll + H\u0302emp\nLeveraging H\u0302 for approximating the value of additional data. An accurate representation of the histogram corresponding to the multi-population distribution underlying a given set of observations can be leveraged to estimate a number of useful properties. These properties include estimating the number of new domain elements that would\nlikely be seen given additional samples from the populations. Specifically, given a histogram H\u0302 , corresponding to m populations, we can estimate the expected number of distinct elements that will be observed in samples from the m populations of respective sizes n1, . . . , nm via the simple formula: E[num observed] = \u2211 \u03b1 H\u0302(\u03b1) ( 1\u2212 m\u220f i=1 (1\u2212 \u03b1i)ni ) . (3)\nAn accurate approximation to the histogram can also be leveraged to answer many other questions about the populations that can not be readily addressed via the linear estimators of Section 4. These include tasks such as estimating the amount of data that must be collected to capture, say, 99% of the mass of the distributions in question."}, {"heading": "6. Experiments", "text": "Evaluating the weighted linear estimator for large m. We empirically evaluated the performance of the weighted linear estimator U\u0302W . The experiments were conducted for three types of distributions\u2014Uniform, Dirichlet and Geometric\u2014that are commonly used to evaluate extrapolation algorithms. Each experiment contains m = 100 populations. We have a total of 3000 distinct elements. In the Uniform setting, each population has support on 100 elements that are randomly sampled from the 3000. For Dirichlet, each population also has support on 100 random elements (from the 3000), and the weights on these 100 elements are sampled from a Dirichlet prior. For the Geometric experiments, each population corresponds to a random ordering of the 3000 elements and the k-th element is assigned probability \u221d (1\u2212 p)kp. In period one, ten samples are observed in each of the 100 populations. In period two, 95 randomly chosen populations have extrapolation factor t \u2208 [0, 1] and five populations have extrapolation factor 10t. This simulates the setting where we can obtain substantially more samples from a subset of the populations.\nFigure 1(a, b, c) shows the results of the experiments for Uniform, Dirichlet(1) and Geometric with p = 0.05 respectively. The results for other parameter settings are qualitatively similar. The black curves indicate the true number of distinct new elements we expect to observe in period two by sampling from the true underlying distributions. The red curves are the predictions of the weighted linear estimator (shaded regions indicate one standard deviation across 100 experiments). In all three settings, U\u0302W provides accurate estimate with low variance when the maximum extrapolation factor is relatively small (\u2264 3). For Uniform and Geometric distributions, the accuracy is high up to 10 fold extrapolation. For Zipf, the bias is low but variance becomes large for the maximum extrapolation factor around 10. The downward bias in the predictions\nis due to the weighting scheme. The relative error of the weighted estimator, ( U\u0302W\u2212U\u2211 njtj )2 , is 0.09, 0.08 and 0.08 for the Uniform, Dirichlet and Geometric distributions when\nthe maximum extrapolation factor is 10. This confirms the theoretical results of Prop. 4.4 on the accuracy of the weighted linear estimator.\nEvaluating the histogram estimators. We first validated the performance of H\u0302count and H\u0302ll on a three population setting with synthetic data. The true population consists of three uniform distributions over 200k elements, whose supports have 100k elements in common, and 100k elements unique to each distribution. In Figure 1(d), the x-axis corresponds to the number of samples we observe from each population, and the y-axis indicates the earthmover distance (EMD) between H\u0302count, H\u0302ll and the true histogram. As a baseline, we also compute the EMD between the empirical histogram of the observed samples and the true histogram. H\u0302count and H\u0302ll performed roughly equally well and both are substantially better than the empirical estimator especially when the number of observed samples is small. Figure 1(e) illustrates the extrapolation accuracy of our histogram estimators. We estimated H\u0302count and H\u0302ll using 16K from each population, and then used Eqn. 3 to estimate the number of unseen elements in additional samples. We tested two different settings: 1) when the additional samples are equally drawn from the three populations, and 2) a skewed mixture where 5/6 of the new samples are from population 1 and 1/12 each are drawn from population 2 and 3. H\u0302count and H\u0302ll gave extremely accurate predictions. In comparison, the weighted linear estimator U\u0302W was accurate for the initial extrapolations but has downward bias when the extrapolation increases, consistent with Fig. 1(ac).\nAdditionally, we evaluate the performance of H\u0302count on a real dataset, in which we sampled words from three books\u2013 Hamlet (32K total words), Treasure Island (40K) and The Sun Also Rises (72K). We used the true word frequencies (over the entire text) as the true histogram. We sampled a small number of words (equal in all books) either randomly or from a contiguous block of text and used H\u0302counts to predict the total number of distinct words in total in all three books. In Figure 2(a), the red line is the true value, and blue and green lines are predictions based on H\u0302count derived from samples of either random words, or words occurring in a random contiguous block of text, respectively. We obtain accurate estimates using a fraction of words (10K from each book). The estimates based on independent samples of words is more accurate than that based on contiguous blocks of text\u2014likely due to correlation in words that occur near each other.\nOptimizing discovery rate. Given the estimated histogram H\u0302count or H\u0302ll, we can optimize the allocation of new samples across the populations to maximize the number of unseen elements we can expect to discover given a bound on \u2211 j tjnj . To illustrate, we obtained genome sequencing data of 45K individuals from the Exome Aggregation Consortium (Lek et al., 2016). The individuals come from four ancestries: Europeans, Africans, East Asians and Latinos. We used all the observed mutations from the 45K\nsamples to construct a four population frequency distribution. For the experiment, we treat this as the ground truth and sampled 105 mutations from each population to obtain \u201cseen\u201d data. Suppose we have budget to sample 3 \u00d7 106 variants (10 fold extrapolation from current sample size), how should we allocate these new samples across the four populations in order to maximize the number of new variants discovered? We use H\u0302count to predict the extrapolation curves for three scenarios: 1. all the samples are allocated to Europeans (current genomic studies are heavily enriched of Europeans); 2. the samples are evenly allocated across the four populations; 3) we explicitly optimize the factors tj using H\u0302count. The dotted curves in Fig. 2(b) correspond to the predictions, and the solid curves are the actual numbers using the true distribution, showing good agreement. Optimization using H\u0302count led to 10 % increase in the number of new variants discovered. This is a simplistic example (there are many other factors in the design of real cohorts) but it illustrate the potential power in having multi-population histogram estimates. In Appendix Fig. 3, we also show that H\u0302count gives accurate predictions for a different statistic\u2014the number of new variants we expect to find at least twice in the new samples."}, {"heading": "7. Discussion", "text": "We introduce and formalize the problem of multipopulation unseen estimation. We provide a weighted linear estimator for the number of new elements and a general optimization algorithm to estimate the multi-population histogram. These two approaches have complementary strength. The weighted linear estimator U\u0302W specifically estimates the number of unseen elements. It\u2019s accuracy is independent of the number of populations, m, and it is worst-case optimal. This can be a good method especially when m is large and the extrapolation factor is small compared to log of the number of observed samples. When the extrapolation is larger, however, U\u0302W is consistently downward biased due to its variance-reducing weights. For relatively small number of populations (m = 2, 3, 4) and larger extrapolation factors, the unseen predictions of our histogram estimators, H\u0302count and H\u0302ll are significantly more accurate than U\u0302W . While both likely have comparable worst-case performance, the linear estimator nearly always incurs this worst-case loss and is largely incapable of extrapolating beyond this worst-case logarithmic factor. In contrast, the histogram-based estimators seem to perform well for much larger extrapolation factors on all of the distributions that we considered. H\u0302count and H\u0302ll are computationally more expensive than U\u0302W , but are still tractable for many applications\u2014each run of our experiments took less than 20 minutes on a single laptop."}, {"heading": "Acknowledgments", "text": "Gregory Valiant\u2019s contributions were supported by NSF CAREER CCF-1351108 and a Sloan Research Fellowship. James Zou is a Chan Zuckerberg Biohub investigator and is supported by NSF CISE-1657155."}, {"heading": "A. Proof of Prop. 4.1 and Prop. 4.4", "text": "Proof of Prop. 4.1. For each element x \u2208 X , let \u03bbx,j = njpx,j , where px,j is the probability of x in population j. We have\nE[U ] = \u2211 x e\u2212 \u2211 j \u03bbx,j ( 1\u2212 e\u2212 \u2211 j tj\u03bbx,j ) .\nThe first term in the sum is the probability that x is not observed in period one and the second term is the probability that x is observed at least once in period two. Taylor expand the second term followed by Binomial expansion gives\nE[U ] = \u2211 x e\u2212 \u2211 j \u03bbx,j \u221e\u2211 i=1 (\u22121)i+1 ( \u2211 j tj\u03bbx,j) i i!\n= \u2212 \u2211 x e\u2212 \u2211 j \u03bbx,j \u2211 i1,...,im: \u2211 ij>0 m\u220f j=1 (\u2212tj\u03bbx,j)ij ij !\n= \u2212 \u2211\ni1,...,im: \u2211 ij>0 \u2211 x e\u2212 \u2211 j \u03bbx,j m\u220f j=1 (\u2212tj\u03bbx,j)ij ij !\n= \u2212 \u2211\ni1,...,im: \u2211 ij>0  m\u220f j=1 (\u2212tj)ij E[\u03c6i1...im ].\nIt\u2019s easy to see that U\u0302 is an unbiased estimator of the last expression.\nWeighting the fingerprints reduces the variance of the estimator at the cost of introducing bias. We analyze the bias and variance of U\u0302W separately. The proof follows the strategy of the analysis for the one population setting in (Orlitsky et al., 2016).\nLemma A.1 (Lemma 4.8 restated). Let n = \u2211m j=1 nj denote the total number of samples in period one and r denote the\nrate of the Poisson weights, then \u2223\u2223\u2223E[U\u0302W \u2212 U ]\u2223\u2223\u2223 \u2264 \u2211 j\u2208A nj(tj + 1)  e\u2212r Proof. For each element x, its contribution to E[U\u0302W ] can be written as\n\u2212e\u2212 \u2211m j=1 \u03bbx,j  \u2211 i1,...,im m\u220f j=1 (\u2212tj)ij \u03bb ij x,j ij ! P L \u2265\u2211 j\u2208A ij \u2212 1 \n= \u2212e\u2212 \u2211m j=1 \u03bbx,j  \u2211 ij :j 6\u2208A \u220f j 6\u2208A (\u2212tj)ij \u03bb ij x,j ij !  \u2211 ij :j\u2208A \u220f j\u2208A (\u2212tj)ij \u03bb ij x,j ij ! P L \u2265\u2211 j\u2208A ij \u2212 1 \n= \u2212e\u2212 \u2211m j=1 \u03bbx,j e\u2212\u2211j 6\u2208A tj\u03bbx,j \u221e\u2211 i=0 P(L \u2265 i) i! \u2212\u2211 j\u2208A tj\u03bbx,j i \u2212 1 \nWe use the following two facts from (Orlitsky et al., 2016).\nFact 1 For all y > 0 and for any random variable L,\u2223\u2223\u2223\u2223\u2223\u2212 \u221e\u2211 i=0 P(L \u2265 i) i! (\u2212y)i + e\u2212y \u2223\u2223\u2223\u2223\u2223 \u2264 maxs\u2264y \u2223\u2223\u2223\u2223E [ (\u2212s)LL! ]\u2223\u2223\u2223\u2223 (1\u2212 e\u2212y) .\nFact 2 If L \u223c Poi(r), then \u2223\u2223\u2223\u2223E [ (\u2212s)LL! ]\u2223\u2223\u2223\u2223 \u2264 e\u2212r.\nTherefore, the contribution of x to E[U\u0302W \u2212 U ] is\ne\u2212 \u2211m j=1 \u03bbx,j\u2212 \u2211 j 6\u2208A tj\u03bbx,j \u2212 \u221e\u2211 i=0 P(L \u2265 i) i! \u2212\u2211 j\u2208A tj\u03bbx,j i + e\u2212\u2211j\u2208A tj\u03bbx,j  \u2264 (1\u2212 e\u2212\u2211j\u2208A tj\u03bbx,j) e\u2212r\nwhere we used Facts 1 and 2 with y = \u2211 j\u2208A tj\u03bbx,j and the fact that e \u2212 \u2211m j=1 \u03bbx,j\u2212 \u2211 j 6\u2208A tj\u03bbx,j \u2264 1.\nNow summing over x \u2208 X , we have\u2223\u2223\u2223E[U\u0302W \u2212 U ]\u2223\u2223\u2223 \u2264 \u2211 x ( 1\u2212 e\u2212 \u2211 j\u2208A tj\u03bbx,j ) e\u2212r\n\u2264 \u2211 x ( 1\u2212 e\u2212 \u2211 j\u2208A(tj+1)\u03bbx,j ) e\u2212r\n= \u2211 x [ e\u2212 \u2211 j\u2208A \u03bbx,j ( 1\u2212 e\u2212 \u2211 j\u2208A tj\u03bbx,j ) + 1\u2212 e\u2212 \u2211 j\u2208A \u03bbx,j ] e\u2212r\n= ( E[UA] + E[\u03a6A+] ) e\u2212r\n\u2264 \u2211 j\u2208A nj(tj + 1)  e\u2212r where \u03a6A+ is the total number of distinct elements observed in period one for subpopulations j \u2208 A and UA is the number of new elements observed in period two for j \u2208 A.\nLemma 4.8 quantifies the bias of the weighted estimator. Next we quantify its variance.\nLemma A.2 (Lemma 4.9 restated). Without loss of generality, let t1 = maxj tj and suppose t1 \u2265 1 then\nVar(U\u0302W \u2212 U) \u2264 ne2r(t1\u22121) + \u2211 j njtj .\nProof. Let Nx,j be the random variable corresponding to the number of times x is found in population j during period one. Let N \u2032x,j be the random variable corresponding to the number of times x is found in population j during period two.\nDefine h(i1, ..., im) = \u2212 \u220fm j=1(\u2212tij )ijP ( L \u2265 \u2211 j\u2208A ij ) .\nFor every element x, its contribution to Var(U\u0302W \u2212 U) is\nVar  \u2211 i1,...,im \u220f j 1Nx,j=ij h(i1, ..., im)\u2212 \u220f j 1Nx,j=0 1\u2212\u220f j 1N \u2032x,j=0  \u2264 E\n \u2211 i1,...,im \u220f j 1Nx,j=ij h(i1, ..., im)\u2212 \u220f j 1Nx,j=0 1\u2212\u220f j 1N \u2032x,j=0 2\n= E  \u2211 i1,...,im \u220f j 1Nx,j=ij h(i1, ..., im)2 + \u220f j 1Nx,j=0 1\u2212\u220f j 1N \u2032x,j=0  .\nThe last equality follows because the cross-term vanishes since the events Nx,j = 0,\u2200j and Nx,j = ij , \u2211 j ij > 0 are disjoint. Summing over all x gives\nVar(U\u0302W \u2212 U) \u2264 E[\u03a6+] sup i1,...,im h(i1, ..., im) 2 + E[U ] (4)\n\u2264 n sup i1,...,im h(i1, ..., im) 2 + \u2211 j njtj . (5)\nMoreover we have\n|h(i1, ..., im)| = \u220f j t ij j P L \u2265\u2211 j\u2208A ij  \u2264 t \u2211 j\u2208A ij\n1 P L \u2265\u2211 j ij  \u2264 er(t1\u22121)\nwhere we have used the following fact:\nFact 3 If L \u223c Poi(r) and t \u2265 1, then for all i > 0\ntiP(L \u2265 i) \u2264 er(t\u22121).\nNote that only tmax \u2265 1 is assumed here; the other tj\u2019s could be less than 1.\nPutting the last two lemmas together, we have\nLemma A.3. Let t1 = maxj tj \u2265 1, then\nE [ (U\u0302W \u2212 U)2 ] \u2264 ne2r(t1\u22121) + \u2211 j njtj + \u2211 j\u2208A nj(tj + 1) 2 e\u22122r\nBecause t1 \u2265 1, Lemma A.3 implies that\nE [ (U\u0302W \u2212 U)2 ] \u2264 (n+ \u2211 j njtj)e 2r(t1\u22121) + (n+ \u2211 j njtj) 2e\u22122r.\nThe two terms on the RHS are equal when r = log(n+\n\u2211 j njtj)\n2tmax . Using this value of r, we have\nE ( U\u0302W \u2212 U\u2211 njtj )2 \u2264 (n+ t1n\u0304 tn\u0304 )2 (n+ t1n\u0304) \u22121/t1\n\u2264 ( n+ t1n\u0304\nt1n\u0304\n)2 n\u0304\u22121/t1\n\u2264 ( n+ t1n1 t1n1 )2 n \u22121/t1 1\nwhere n\u0304 \u2261 \u2211 j njtj/t1. This completes the proof of Prop. 4.4."}, {"heading": "B. Multi-population Earth Mover\u2019s Distance", "text": "We define a natural distance metric on multi-population histograms, which is a measure of the extent to which the corresponding distributions are similar, up to a relabeling of the elements:\nDefinition B.1. Given two m-population histograms, H,H \u2032, the multi-population earthmover distance dW (H,H \u2032) is defined as the minimum over all schemes of moving the histogram elements in H to yield H \u2032, where the cost of moving c histogram elements from \u03b1 \u2208 [0, 1]m to \u03b1\u2032 is c 12m\u2016\u03b1\u2212 \u03b1 \u2032\u20161 = \u2211m i=1 |\u03b1i \u2212 \u03b1\u2032i|. To ensure that such a scheme exists, we regard there being an infinitude of elements that occur with probability zero in all populations, H(0) = H \u2032(0) =\u221e.\nNote that for all pairs of histograms H,H \u2032 it holds that dW (H,H \u2032) \u2208 [0, 1], with dW (H,H \u2032) = 0 if and only if the distributions corresponding to H and H \u2032 are identical, up to relabeling the domain elements. The following example illustrates the above definition:\nExample B.1. Consider a 3 population distribution corresponding to a three uniform distributions over 2n elements, where n of the elements are common to all 3 populations, and the other elements are unique. This corresponds to histogram H defined by H(1/2n, 1/2n, 1/2n) = n, H(1/2n, 0, 0) = n, H(0, 1/2n, 0) = n, H(0, 0, 1/2n) = n. Consider a second histogram H \u2032 corresponding to three uniform distributions over a common set of n elements, H \u2032(1/n, 1/n, 1/n) = n, and H \u2032(\u03b1) = 0 for all \u03b1 6= (1/n, 1/n, 1/n). The EMD\ndW (H,H \u2032) =\n1\n2 \u00b7 3\n( n 3\n2n + 3n\n1\n2n\n) = 1\n2 ,\nSince we can make H \u2032 from H by moving n histogram elements from (1/2n, 1/2n, 1/2n) to (1/n, 1/n, 1/n) at a per-unitcost of \u2016( 12n , 1 2n , 1 2n )\u2212 ( 1 n , 1 n , 1 n )\u20161 = 3 2n , and then moving the remaining 3n elements of H to (0, 0, 0) at a per-unit-cost of 1/2n."}, {"heading": "C. Additional experiments", "text": "We tested the prediction accuracy of H\u0302count on a different statistic: the number of elements we expect to find at least twice in the new samples, see Fig. 3."}], "references": [{"title": "A unified maximum likelihood approach for optimal distribution property estimation", "author": ["Acharya", "Jayadev", "Das", "Hirakendu", "Orlitsky", "Alon", "Suresh", "Ananda Theertha"], "venue": "arXiv preprint arXiv:1611.02960,", "citeRegEx": "Acharya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2016}, {"title": "Estimating the number of unsen species: How many words did shakespeare know", "author": ["Efron", "Bradley", "Thisted", "Ronald"], "venue": "Biometrika, pp", "citeRegEx": "Efron et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Efron et al\\.", "year": 1976}, {"title": "The relation between the number of species and the number of individuals in a random sample of an animal population", "author": ["Fisher", "Ronald A", "Corbet", "A Steven", "Williams", "Carrington B"], "venue": "The Journal of Animal Ecology,", "citeRegEx": "Fisher et al\\.,? \\Q1943\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 1943}, {"title": "The number of new species, and the increase in population coverage, when a sample is increased", "author": ["IJ Good", "Toulmin", "GH"], "venue": null, "citeRegEx": "Good et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Good et al\\.", "year": 1956}, {"title": "The population frequencies of species and the estimation of population parameters", "author": ["Good", "Irving J"], "venue": null, "citeRegEx": "Good and J.,? \\Q1953\\E", "shortCiteRegEx": "Good and J.", "year": 1953}, {"title": "Optimal prediction of the number of unseen species", "author": ["Orlitsky", "Alon", "Suresh", "Ananda Theertha", "Wu", "Yihong"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Orlitsky et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Orlitsky et al\\.", "year": 2016}, {"title": "Estimating the unseen: an n/log (n)-sample estimator for entropy and support size, shown optimal via new clts", "author": ["Valiant", "Gregory", "Paul"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Valiant et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Valiant et al\\.", "year": 2011}, {"title": "Instance optimal learning of discrete distributions", "author": ["Valiant", "Gregory", "Paul"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "Valiant et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Valiant et al\\.", "year": 2016}, {"title": "Estimating the unseen: improved estimators for entropy and other properties", "author": ["Valiant", "Paul", "Gregory"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Valiant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Valiant et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "Fisher (Fisher et al., 1943).", "startOffset": 7, "endOffset": 28}, {"referenceID": 5, "context": "The linear estimators that we propose in Section 4 for the multiple population setting, and their analysis, are extensions of the smoothed GoodToulmin estimators of (Orlitsky et al., 2016).", "startOffset": 165, "endOffset": 188}, {"referenceID": 2, "context": "Fisher (Fisher et al., 1943). This was quickly followed by (Good & Toulmin, 1956), which introduced the Good-Toulmin estimator. While the Good-Toulmin estimator is always unbiased, the variance increases rapidly for c \u2265 1. Subsequent works, including (Efron & Thisted, 1976) have suggested \u201csmoothing\u201d approaches that tradeoff the bias and variance for this type of approach. The recent work of Orlitsky et al. (2016) describes a clever variant that achieves good performance for c = O(log n).", "startOffset": 8, "endOffset": 418}, {"referenceID": 5, "context": "We focus on a particular weighting scheme, which is an extension of that introduced in (Orlitsky et al., 2016): W (i1, i2, .", "startOffset": 87, "endOffset": 110}, {"referenceID": 5, "context": "The answer is essentially no due to the following lower bound for one population extrapolation (Orlitsky et al., 2016; Valiant & Valiant, 2011): There exists universal constants c, c\u2032 such that for all estimators \u00db , if the extrapolation factor t > c, then \u2203 distribution such", "startOffset": 95, "endOffset": 143}, {"referenceID": 0, "context": "Recent work (Acharya et al., 2016) provided rigorous support for this intuition.", "startOffset": 12, "endOffset": 34}], "year": 2017, "abstractText": "Given samples from a distribution, how many new elements should we expect to find if we continue sampling this distribution? This is an important and actively studied problem, with many applications ranging from unseen species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population j has an unknown distributionDj from which we observe nj samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. Surprisingly, we prove that our estimator\u2019s accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions. We validate our methods and theory through extensive experiments. Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.", "creator": "LaTeX with hyperref package"}}}