{"id": "1707.05000", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "In-Order Transition-based Constituent Parsing", "abstract": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing. The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its construction, and a strong encoder over the input. The top-down parsers also benefit from a strong decoder over the input, and the top-down parsers benefit from multiple parses of the same form. The top-down parsers do not provide the same information as the top-down parsers, but both have an overall representation of the content, meaning they do not always accurately identify which of the sub-problems. The top-down parsers generally produce more output than the top-down parsers, but with low decoding information for a constituent hierarchy, this representation can be misleading: the parsers generally produce much less output than the top-down parsers, but this representation can be misleading. A simple example is a simple example of a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a parser with a", "histories": [["v1", "Mon, 17 Jul 2017 04:27:11 GMT  (144kb,D)", "http://arxiv.org/abs/1707.05000v1", "Accepted by TACL"]], "COMMENTS": "Accepted by TACL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiangming liu", "yue zhang"], "accepted": true, "id": "1707.05000"}, "pdf": {"name": "1707.05000.pdf", "metadata": {"source": "CRF", "title": "In-Order Transition-based Constituent Parsing", "authors": ["Jiangming Liu", "Yue Zhang"], "emails": ["zhang}@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe\nand Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree.\nThe process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence \u201cThe little boy\u201d is first read, and then an NP recognized for the word sequence. After the system reads the verb \u201clikes\u201d and an its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3\u00a9\u2192 4\u00a9\u2192 5\u00a9\u2192 2\u00a9\u2192 7\u00a9\u2192 9\u00a9\u2192 10\u00a9\u2192 8\u00a9\u2192 6\u00a9\u2192 11\u00a9\u2192 1\u00a9. When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages.\nOn the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree. Given the sentence in Figure 1, a top-down\n1The action sequence is taken on unbinarized trees.\nar X\niv :1\n70 7.\n05 00\n0v 1\n[ cs\n.C L\n] 1\n7 Ju\nl 2 01\nshift-reduce parser takes the action sequence in Table 2(b) to build the output, where an S is first made and then an NP is generated. After that, the system makes decision to read the word sequence \u201cThe little boy\u201d to complete the NP. The full order of recognition for the tree nodes is 1\u00a9\u2192 2\u00a9\u2192 3\u00a9\u2192 4\u00a9\u2192 5\u00a9\u2192 6\u00a9\u2192 7\u00a9\u2192 8\u00a9\u2192 9\u00a9\u2192 10\u00a9\u2192 11\u00a9. The top-down lookahead guidance contributes to non-local disambiguation. However, it is difficult to generate a constituent before its sub constituents have been realized, since no explicit features can be extracted from their subtree structures. Thanks to the use of recurrent neural networks, which makes it possible to represent a sentence globally before syntactic tree construction, seminal work of neural top-down parsing directly generates bracketed constituent trees using sequence-to-sequence models (Vinyals et al., 2015). Dyer et al. (2016) design set of top-down transition actions for standard\ntransition-based parsing. In this paper, we propose a novel transition system for constituent parsing, mitigating issues of both bottom-up and top-down systems by finding a compromise between bottom-up constituent information and top-down lookahead information. The process of the proposed constituent parsing can be regarded as in-order traversal over a tree. Given the sentence in Figure 1, the system takes the action sequence in Table 2(c) to build the output. The system reads the word \u201cThe\u201d and then projects an NP, which is based on bottom-up evidence. After this, based on the projected NP, the system reads the word sequence \u201clittle boy\u201d, with top-down guidance from NP. Similarly, based on the completed constituent \u201c(NP The little boy)\u201d, the system projects an S, with the bottom-up evidence.\nWith the S and the word \u201clikes\u201d, the system projects an VP, which can serve as top-down guidance. The full order of recognition for the tree nodes is 3\u00a9\u2192 2\u00a9\u2192 4\u00a9\u2192 5\u00a9\u2192 1\u00a9\u2192 7\u00a9\u2192 6\u00a9\u2192 9\u00a9\u2192 8\u00a9\u2192 10\u00a9\u2192 11\u00a9. Compared to post-order traversal, in-order traversal can potentially resolve non-local ambiguity better by top-down guidance. Compared to pre-order traversal, in-order traversal can potentially resolve local ambiguity better by bottom-up evidence.\nFurthermore, in-order traversal is psycholinguistically motivated (Roark et al., 2009; Steedman, 2000). Empirically, a human reader comprehends sentences by giving lookahead guesses for constituents. For example, when reading a word \u201clikes\u201d, a human reader could guess that it could be a start of a constituent VP, instead of waiting to read the object \u201cred tomatoes\u201d, which is the procedure of a bottom-up system.\nWe compare our system with the two baseline systems (i.e. a top-down system and a bottomup system) under the same neural transition-based framework of Dyer et al. (2016). Our final models outperform both of the bottom-up and top-down transition-based constituent parsing by achieving a 91.8 F1 in English and a 86.1 F1 in Chinese for greedy fully-supervised parsing, respectively. Furthermore, our final model obtains a 93.6 F1 with supervised reranking (Choe and Charniak, 2016) and a 94.2 F1 with semi-supervised reranking, achieving the state-of-the-art results on constituent parsing on English benchmark. By converting to Stanford dependencies, our final model achieves the state-ofthe-art results on dependency parsing by obtaining a 96.2% UAS and a 95.2% LAS. To our knowledge, we are the first to systematically compare top-down and bottom-up constituent parsing under the same neural framework. We release our code at https://github.com/LeonCrashCode/ InOrderParser."}, {"heading": "2 Transition-based constituent parsing", "text": "Transition-based constituent parsing takes a leftto-right scan of the input sentence, where a stack is used to maintain partially constructed phrasestructures, while the input words are stored in a buffer. Formally, a state is defined as [\u03c3, i, f ], where\n\u03c3 is the stack, i is the front index of the buffer, and f is a bool value showing if the parsing is finished. At each step, a transition action is applied to consume an input word or construct a new phrase-structure. Different parsing systems employ their own sets of actions."}, {"heading": "2.1 Bottom-up system", "text": "We take the bottom-up system of Sagae and Lavie (2005) as our bottom-up baseline. Given a state, the set of transition actions are\n\u2022 SHIFT: pop the front word from the buffer, and push it onto the stack.\n\u2022 REDUCE-L/R-X: pop the top two constituents off the stack, combine them into a new constituent with label X, and push the new constituent onto the stack.\n\u2022 UNARY-X: pop the top constituent off the stack, raise it to a new constituent with label X, and push the new constituent onto the stack.\n\u2022 FINISH: pop the root node off the stack and ends parsing.\nThe bottom-up parser can be summarized as the deductive system in Figure 3(a). Given the sentence with the binarized syntactic tree in Figure 1(b), the sequence of actions SHIFT, SHIFT, SHIFT, REDUCE-R-NP, REDUCE-R-NP, SHIFT, SHIFT, SHIFT, REDUCE-R-NP, REDUCE-L-VP, SHIFT, REDUCE-L-S, REDUCE-R-S and FINISH, can be used to construct its constituent tree."}, {"heading": "2.2 Top-down system", "text": "We take the top-down system of Dyer et al. (2016) as our top-down baseline. Given a state, the set of transition actions are\n\u2022 SHIFT: pop the front word from the buffer, and push it onto the stack.\n\u2022 NT-X: open a nonterminal with label X on top of the stack.\n\u2022 REDUCE: repeatedly pop completed subtrees or terminal symbols from the stack until an open nonterminal is encountered, and then this open NT is popped and used as the label of a\nnew constituent that has the popped subtrees as its children. This new completed constituent is pushed onto the stack as a single composite item.\nThe deduction system for the process is shown in Figure 3(b)2. Given the sentence in Figure 1, the sequence of actions NT-S, NT-NP, SHIFT, SHIFT, SHIFT, REDUCE, NT-VP, SHIFT, NT-NP, SHIFT, SHIFT, REDUCE, REDUCE, SHIFT and REDUCE,\n2Due to unary decision, we use completed marks to make finish decision, except for top-down system.\ncan be used to construct its constituent tree."}, {"heading": "3 In-order system", "text": "We propose a novel in-order system for transitionbased constituent parsing. Similar to the bottom-up and top-down systems, the in-order system maintains a stack and a buffer for representing a state. The set of transition actions are defined as:\n\u2022 SHIFT: pop the front word from the buffer, and push it onto the stack.\n\u2022 PJ-X: project a nonterminal with label X on top of the stack.\n\u2022 REDUCE: repeatedly pop completed subtrees or terminal symbols from the stack until an projected nonterminal is encountered, and then this projected nonterminal is popped and used as the label of a new constituent, and furthermore, one more item on the top of stack is popped as the leftmost child of the new constituent and the popped subtrees as its rest children. This new completed constituent is pushed onto the stack as a single composite item.\n\u2022 FINISH: pop the root node off the stack and ends parsing.\nThe deduction system for the process is shown in Figure 3(c). Given the sentence in Figure 1, the sequence of actions SHIFT, PJ-NP, SHIFT, SHIFT, REDUCE, PJ-S, SHIFT, PJ-VP, SHIFT, PJ-NP, SHIFT, REDUCE, REDUCE, SHIFT, REDUCE, FINISH can be used to construct its constituent tree.\nVariants The in-order system can be generalized into variants by modifying k, the number of leftmost nodes traced before the parent node. For example, given the tree \u201c(S a b c d)\u201d, the traversal is \u201ca S b c d\u201d if k = 1 while the traversal is \u201ca b S c d\u201d if k = 2. We name each variant with a certain k value as k-in-order systems. In this paper, we only investigate the in-order system with k = 1, the 1-inorder system. Note that the top-down parser can be regarded as a special case of a generalized version of the in-order parser with k = 0, and the bottom-up parser can be regarded as a special case with k =\u221e."}, {"heading": "4 Neural parsing model", "text": "We employ the stack-LSTM parsing model of Dyer et al. (2016) for the three types of transition-based parsing systems in Section 2.1, 2.2 and 3, respectively, where a stack-LSTM is used to represent the stack, a stack-LSTM is used to represent the buffer, and a vanilla LSTM is used to represent the action history, as shown in Figure 4."}, {"heading": "4.1 Word representation", "text": "We follow Dyer et al. (2015), representing each word using three different types of embeddings, including pretrained word embedding, ewi , which is not fine-tuned during training of the parser, randomly initialized embeddings ewi , which is finetuned, and the randomly initialized part-of-speech embeddings, which is fine-tuned. The three embeddings are concatenated, and then fed to nonlinear layer to derive the final word embedding:\nxi = f(Winput[epi ; ewi ; ewi ] + binput),\nwhere Winput and binput are model parameters, wi and pi denote the form and the POS tag of the ith input word, respectively, and f is an nonlinear function. In this paper, we use ReLu for f ."}, {"heading": "4.2 Stack representation", "text": "We employ a bidirectional LSTM as the composition function to represent constituents on stack3. For top-down parsing and in-order parsing, following Dyer et al. (2016), as shown in Figure 5(a), the composition representation scomp is computed as:\nscomp = (LSTMfwd[ent, s0, ..., sm]; LSTMbwd[ent, sm, ..., s0]),\nwhere ent is the representation of a non-terminal, sj , j \u2208 [0,m] is the jth child node, and m is the number of the child nodes. For bottom-up parsing, we make use of head information in the composition function by requiring the order that the head node is always before the non-head node in the bidirectional LSTM, as shown in Figure 5(b)4. The bina-\n3To be fair, we use bidirectional LSTM as composition function for all parsing systems\n4A bidirectional LSTM consists of two LSTMs, making it balanced for composition. However, they have different parameters so that one represents information of head-first while other represents information of head-last.\nrized composition is computed as:\nsbcomp = (LSTMfwd[ent, sh, so]; LSTMbwd[ent, so, sh]),\nwhere sh and so is the representation of head and non-head node, respectively."}, {"heading": "4.3 Greedy action classification", "text": "Given a sentence w0, w1, ..., wn\u22121, where wi is the ith word, and n is the length of the sentence, our parser makes local action classification decisions incrementally. For the kth parsing state like [sj , ..., s1, s0, i, false], the probability distribution of the current action p is:\np = SOFTMAX(W [hstk;hbuf ;hah] + b), (*)\nwhere W and b are model parameters, the representation of stack information hstk is\nhstk = stack-LSTM[s0, s1, ..., sj ],\nthe representation of buffer information hbuf is\nhbuf = stack-LSTM[xi, xi+1, ..., xn],\nx is the word representation, and the representation of action history hah is\nhah = LSTM[eactk\u22121 , eactk\u22122 , ..., eact0 ],\nwhere eactk\u22121 is the representation of action in k-1th parsing state.\nTraining Our models are trained to minimize a cross-entropy loss objective with an l2 regularization term, defined by\nL(\u03b8) = \u2212 \u2211 i \u2211 j log paij + \u03bb 2 ||\u03b8||2,\nwhere \u03b8 is the set of parameters, paij is the probability of the jth action in the ith training example given by the model and \u03bb is a regularization hyperparameter (\u03bb = 10\u22126). We use stochastic gradient descent with 0.1 initialized learning rate with 0.05 learning rate decay."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data", "text": "We empirically compare our bottom-up, top-down and in-order parsers. The experiments are carried on both English and Chinese. For English data, we use the standard benchmark of WSJ sections in PTB (Marcus et al., 1993), where the sections 2-21 are taken for training data, section 22 for development data and section 23 for test for both dependency parsing and constituency parsing. We adopt the pretrained English word embeddings generated on the AFP portion of English Gigaword.\nFor Chinese data, we use the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). We use articles 001- 270 and 440-1151 for training, articles 301-325 for system development, and articles 271-300 for final performance evaluation. We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus,\nThe POS tags in both the English data and the Chinese data are automatically assigned as the same as the work of Dyer et al. (2016), using Stanford tagger. We follow the work of Choe and Charniak (2016) and adopt the AFP portion of English Gigaword as the extra resources for the semi-supervised reranking."}, {"heading": "5.2 Settings", "text": "Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1. Reranking experiments Following the same reranking setting of Dyer et al. (2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (sec-\ntion 4), respectively, with exponentiation strategy (\u03b1 = 0.8) by using probability distribution of action (equation *). We adopt the reranker of Choe and Charniak (2016) as both our English fullysupervised reranker and semi-supervised reranker, and the generative reranker of Dyer et al. (2016) as our Chinese supervised reranker."}, {"heading": "5.3 Development experiments", "text": "Table 2 shows the development results of the three parsing systems. The bottom-up system performs slightly better than the top-down system. The inorder system outperforms both the bottom-up and the top-down system."}, {"heading": "5.4 Results", "text": "Table 3 shows the parsing results on the English test dataset. We find that the bottom-up parser and the top-down parser have similar results under the greedy setting, and the in-order parser outperforms both of them. Also, with supervised reranking, the in-order parser achieves the best results.\nEnglish constituent results We compare our models with previous work, as shown in Table 4. With the fully-supervision setting5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang,\n5Here, we only consider the work of single model\n2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results. With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al. (2017), which extend the work of Dyer et al. (2016) by adding gated attention mechanism on composition functions. With the semi-supervised setting, the in-order parser outperforms the best semi-supervised parser (Choe and Charniak, 2016) by achieving 94.2 F1.\nEnglish dependency results As shown in Table 5, by converting to Stanford Dependencies, without additional training data, our models achieves similar performance with the state-of-the-art system (Choe and Charniak, 2016); with the same additional training data, our models achieves new state-of-the-art results on dependency parsing by achieving 96.2% UAS and 95.2% LAS on standard benchmark.\nChinese constituent results Table 6 shows the final results on Chinese test dataset. The inorder parser achieves the best results under hte fully-\nsupervised setting. With the supervised reranking, the in-order parser outperforms the state-of-the-art models by achieving 88.0 F1.\nChinese dependency results As shown in Table 7, by converting the results to dependencies6, our final model achieves the best results among transitionbased parsing, and obtains comparable results to the state-of-the-art graph-based models.\n6The Penn2Malt tool is used with Chinese head rules https://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html"}, {"heading": "6 Analysis", "text": "We analyze the results of section 23 in WSJ given by our model (i.e. in-order parser) and two baseline models (i.e. the bottom-up parser and the top-down parser) against the sentence length, the span length and the constituent type, respectively."}, {"heading": "6.1 Influence of sentence length", "text": "Figure 6 shows the F1 scores of the three parsers on sentences of different lengths. Compared to the top-down parser, the bottom-up parser perform better on the short sentences with the length falling in the range [20-40]. This is likely because the bottom-up parser takes advantages of rich local features from partially-built trees, which are useful for parsing short sentences. But these local structures are can be insufficient for parsing long sentences due to error propagation. On the other hand, the top-down parser obtain better results on long sentences with the length falling in the range [40-50]. This is because, as the length of sentences increase, lookahead features become rich and they could be correctly represented by the LSTM, which is beneficial for parsing non-local structures. We find that the in-order parser performs the best for both short and long sentences, showing the advantages of integrating bottom-up and top-down information."}, {"heading": "6.2 Influence of span length", "text": "Figure 7 shows the F1 scores of the three parsers on spans of different lengths. The trend of performances of the two baseline parsers are similar. Compared to the baseline parsers, the in-order parser obtains significant improvement on long spans. It is linguistically because the in-order traversal over a\ntree allows constituent types of spans to be correctly projected based on the information of the beginning (leftmost nodes) of the spans, and then the projected constituents constrain long span construction, which is different from the top-down parser, which generates constituent types of spans without trace of the spans."}, {"heading": "6.3 Influence of constituent type", "text": "Table 7 shows the F1 scores of the three parsers on frequent constituent types. The bottom-up parser performs better than the top-down parser on con-\nstituent types including NP, S, SBAR, QP. We find that the prediction of these constituent types requires explicitly modeling of bottom-up structures. In other words, bottom-up information is necessary for us to know if the span can be a noun phrase (NP) or sentence (S) for example. On the other hand, the top-down parser has better performance on WHNP, which can be due to the reason that a WHNP starts with a certain question word, which makes the prediction is easy without bottom-up information. The in-order parser performs the best on all constituent types, which demonstrates that the in-order parser can benefit from both bottom-up and top-down information."}, {"heading": "6.4 Examples", "text": "We give output examples from the test set to qualitatively compare the performances of the three parsers using the fully-supervised model without reranking, as shown in Table 9. For example, given the sentence #2006, the bottom-up and the in-order parsers give both correct results. However, the top-down parser makes an incorrect decision to generate an S, which leads subsequent incorrect decisions on VP to complete S. Sentence pattern ambiguaty allows top-down guidance to over-parsing the sentence by recognizing the word \u201cPlans\u201d as a verb, while more bottom-up information is useful for the local disambiguation.\nGiven the sentence #308, the bottom-up parser prefers construction of local constituents such as \u201conce producers and customers\u201d, ignoring the possible clause SBAR, however, which is captured by the in-order parser, because the parser projects a constituent SBAR from the word \u201cstick\u201d and continues to complete the clause, showing that top-down lookahead information is necessary for non-local disambiguation. The in-order parser gives the correct output for the sentence #2066 and the sentence #308, showing that it can benefit from bottom-up\nand top-down information. In the sentence #1715, there are coordinated objects such as \u201cinvestors uneasy\u201d and \u201cthe corporations more vulnerable\u201d. All of the three parsers can recognize coordination. However, the top-down and the bottom-up parsers incorrectly recognize the \u201cThis has both made investors uneasy\u201d as a complete sentence. The top-down parser incorrectly generates S, marked in red, at a early stage, leaving no choice but to follow this incorrect non-terminal. The bottom-up parser without lookahead information makes incorrect local decisions. In contrast, the in-order parser reads the word \u201cand\u201d and projects a non-terminal S for coordination after completing \u201c(S investors uneasy)\u201d. On the other hand, the in-order parser is confused to project for the word \u201cmade\u201d or the word \u201cboth\u201d into a VP, which we think could be addressed by using a in-order system variant with k=2 described in section 3."}, {"heading": "7 Related work", "text": "Our work is related to left corner parsing. Rosenkrantz and Lewis (1970) formalize this in automata theory, which have appeared frequently in the compiler literature. Roark and Johnson (1999) apply the strategy into parsing. Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013). In contrast, we propose a novel general transition-based in-order constituent parsing system.\nNeural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016). Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto topdown system, which is the same as our top-down parser. Watanabe and Sumita (2015) employ treeLSTM to model the complex representation in stack in bottom-up system. We are the first to investigate in-order traversal by designing a novel transitionbased system under the same neural structure model framework."}, {"heading": "8 Conclusion", "text": "We proposed a novel psycho-linguistically motivated constituent parsing system based on the inorder traversal over syntactic trees, aiming to find a compromise between bottom-up constituent information and top-down lookahead information. On standard WSJ benchmark, our in-order system outperforms bottom-up parsing on non-local ambiguity and top-down parsing on local decision. The resulting parser achieves the state-of-the-art constituent parsing results by obtaining 94.2 F1 and dependency parsing results by obtaining 96.2% UAS and 95.2%\nLAS."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their detailed and constructive comments. Yue Zhang is the corresponding author."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "ACL, pages 2442\u20132452.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith."], "venue": "EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Coarse-tofine n-best parsing and MaxEnt discriminative reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A maximum-entropy-inspired parser", "author": ["Eugene Charniak."], "venue": "NAACL, pages 132\u2013139. Association for Computational Linguistics.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "EMNLP, pages 740\u2013750, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Bi-directional attention with agreement for dependency parsing", "author": ["Hao Cheng", "Hao Fang", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "EMNLP.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "EMNLP.", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "ICLR.", "citeRegEx": "Dozat and Manning.,? 2017", "shortCiteRegEx": "Dozat and Manning.", "year": 2017}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "ACL.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "NAACL.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Pcfg models of linguistic tree representations", "author": ["Mark Johnson."], "venue": "Computational Linguistics, 24(4):613\u2013 632.", "citeRegEx": "Johnson.,? 1998", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association of Computational Linguistics, 4:313\u2013327.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In EACL,", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Lstm ccg parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "NAACL, pages 221\u2013231.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "author": ["Jiangming Liu", "Yue Zhang."], "venue": "Transactions of the Association of Computational Linguistics, 5:45\u201358.", "citeRegEx": "Liu and Zhang.,? 2017", "shortCiteRegEx": "Liu and Zhang.", "year": 2017}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient probabilistic top-down and left-corner parsing", "author": ["Brian Roark", "Mark Johnson."], "venue": "ACL, pages 421\u2013428. Association for Computational Linguistics.", "citeRegEx": "Roark and Johnson.,? 1999", "shortCiteRegEx": "Roark and Johnson.", "year": 1999}, {"title": "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing", "author": ["Brian Roark", "Asaf Bachrach", "Carlos Cardenas", "Christophe Pallier."], "venue": "EMNLP, pages 324\u2013333. Association for Computa-", "citeRegEx": "Roark et al\\.,? 2009", "shortCiteRegEx": "Roark et al\\.", "year": 2009}, {"title": "Robust probabilistic predictive syntactic processing: motivations, models, and applications", "author": ["Brian Roark."], "venue": "Ph.D. thesis.", "citeRegEx": "Roark.,? 2001", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "Deterministic left corner parsing", "author": ["Daniel J. Rosenkrantz", "Philip M. Lewis."], "venue": "IEEE Conference Record of 11th Annual Symposium on Switching and Automata Theory, pages 139\u2013152. IEEE.", "citeRegEx": "Rosenkrantz and Lewis.,? 1970", "shortCiteRegEx": "Rosenkrantz and Lewis.", "year": 1970}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "IWPT, pages 125\u2013132. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "Broad-coverage parsing using human-like memory constraints", "author": ["William Schuler", "Samir AbdelRahman", "Tim Miller", "Lane Schwartz."], "venue": "Computational Linguistics, 36(1):1\u201330.", "citeRegEx": "Schuler et al\\.,? 2010", "shortCiteRegEx": "Schuler et al\\.", "year": 2010}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "ACL, pages 440\u2013448. Association for Computational Linguistics.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "An analysis of frequency-and memory-based processing costs", "author": ["Marten Van Schijndel", "William Schuler."], "venue": "HLT-NAACL, pages 95\u2013105.", "citeRegEx": "Schijndel and Schuler.,? 2013", "shortCiteRegEx": "Schijndel and Schuler.", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "ICLR.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS tagging and transition-based constituent parsing in Chinese with non-local features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "ACL, pages 733\u2013742, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Feature optimization for constituent parsing via neural networks", "author": ["Zhiguo Wang", "Haitao Mi", "Nianwen Xue."], "venue": "ACL-IJCNLP, pages 1138\u20131147, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transitionbased neural constituent parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "ACL, pages 1169\u2013 1179.", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "Lstm shift-reduce ccg parsing", "author": ["Wenduan Xu."], "venue": "EMNLP.", "citeRegEx": "Xu.,? 2016", "shortCiteRegEx": "Xu.", "year": 2016}, {"title": "The Penn Chinese treebank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Martha Palmer."], "venue": "Natural Language Engineering, 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based parsing of the chinese treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "IWPT, pages 162\u2013171. Association for Computational Linguistics.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "ACL, pages 434\u2013443.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 24, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 36, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 37, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 33, "context": "There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al.", "startOffset": 93, "endOffset": 184}, {"referenceID": 12, "context": ", 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017).", "startOffset": 56, "endOffset": 97}, {"referenceID": 16, "context": ", 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017).", "startOffset": 56, "endOffset": 97}, {"referenceID": 37, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 33, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 8, "context": "When making local decisions, rich information is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation.", "startOffset": 92, "endOffset": 160}, {"referenceID": 14, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 20, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 3, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 18, "context": "However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017).", "startOffset": 92, "endOffset": 169}, {"referenceID": 24, "context": "In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003).", "startOffset": 116, "endOffset": 139}, {"referenceID": 7, "context": "In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003).", "startOffset": 191, "endOffset": 206}, {"referenceID": 30, "context": "Thanks to the use of recurrent neural networks, which makes it possible to represent a sentence globally before syntactic tree construction, seminal work of neural top-down parsing directly generates bracketed constituent trees using sequence-to-sequence models (Vinyals et al., 2015).", "startOffset": 262, "endOffset": 284}, {"referenceID": 11, "context": "Dyer et al. (2016) design set of top-down transition actions for standard stack buffer action node [] [The little .", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "Furthermore, in-order traversal is psycholinguistically motivated (Roark et al., 2009; Steedman, 2000).", "startOffset": 66, "endOffset": 102}, {"referenceID": 28, "context": "Furthermore, in-order traversal is psycholinguistically motivated (Roark et al., 2009; Steedman, 2000).", "startOffset": 66, "endOffset": 102}, {"referenceID": 6, "context": "6 F1 with supervised reranking (Choe and Charniak, 2016) and a 94.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "a top-down system and a bottomup system) under the same neural transition-based framework of Dyer et al. (2016). Our final models outperform both of the bottom-up and top-down transition-based constituent parsing by achieving a 91.", "startOffset": 93, "endOffset": 112}, {"referenceID": 24, "context": "We take the bottom-up system of Sagae and Lavie (2005) as our bottom-up baseline.", "startOffset": 32, "endOffset": 55}, {"referenceID": 11, "context": "We take the top-down system of Dyer et al. (2016) as our top-down baseline.", "startOffset": 31, "endOffset": 50}, {"referenceID": 11, "context": "We employ the stack-LSTM parsing model of Dyer et al. (2016) for the three types of transition-based parsing systems in Section 2.", "startOffset": 42, "endOffset": 61}, {"referenceID": 11, "context": "We follow Dyer et al. (2015), representing each word using three different types of embeddings, including pretrained word embedding, ewi , which is not fine-tuned during training of the parser, randomly initialized embeddings ewi , which is finetuned, and the randomly initialized part-of-speech embeddings, which is fine-tuned.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "For top-down parsing and in-order parsing, following Dyer et al. (2016), as shown in Figure 5(a), the composition representation scomp is computed as:", "startOffset": 53, "endOffset": 72}, {"referenceID": 19, "context": "For English data, we use the standard benchmark of WSJ sections in PTB (Marcus et al., 1993), where the sections 2-21 are taken for training data, section 22 for development data and section 23 for test for both dependency parsing and constituency parsing.", "startOffset": 71, "endOffset": 92}, {"referenceID": 35, "context": "1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005).", "startOffset": 37, "endOffset": 55}, {"referenceID": 9, "context": "We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus, The POS tags in both the English data and the Chinese data are automatically assigned as the same as the work of Dyer et al. (2016), using Stanford tagger.", "startOffset": 212, "endOffset": 231}, {"referenceID": 3, "context": "We follow the work of Choe and Charniak (2016) and adopt the AFP portion of English Gigaword as the extra resources for the semi-supervised reranking.", "startOffset": 31, "endOffset": 47}, {"referenceID": 9, "context": "Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1.", "startOffset": 107, "endOffset": 126}, {"referenceID": 9, "context": "Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1. Reranking experiments Following the same reranking setting of Dyer et al. (2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (sec-", "startOffset": 107, "endOffset": 258}, {"referenceID": 3, "context": "(2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (sec-", "startOffset": 20, "endOffset": 36}, {"referenceID": 3, "context": "We adopt the reranker of Choe and Charniak (2016) as both our English fullysupervised reranker and semi-supervised reranker, and the generative reranker of Dyer et al.", "startOffset": 34, "endOffset": 50}, {"referenceID": 3, "context": "We adopt the reranker of Choe and Charniak (2016) as both our English fullysupervised reranker and semi-supervised reranker, and the generative reranker of Dyer et al. (2016) as our Chinese supervised reranker.", "startOffset": 34, "endOffset": 175}, {"referenceID": 26, "context": "With the fully-supervision setting5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang,", "startOffset": 105, "endOffset": 144}, {"referenceID": 37, "context": "With the fully-supervision setting5, the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang,", "startOffset": 105, "endOffset": 144}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.", "startOffset": 75, "endOffset": 96}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.", "startOffset": 75, "endOffset": 119}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.4 Vinyals et al. (2015) 90.", "startOffset": 75, "endOffset": 146}, {"referenceID": 15, "context": "Here, we only consider the work of single model Model F1 fully-supervision Socher et al. (2013) 90.4 Zhu et al. (2013) 90.4 Vinyals et al. (2015) 90.7 Watanabe and Sumita (2015) 90.", "startOffset": 75, "endOffset": 178}, {"referenceID": 15, "context": "7 Shindo et al. (2012) 91.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "1 Durrett and Klein (2015) 91.", "startOffset": 2, "endOffset": 27}, {"referenceID": 6, "context": "1 Durrett and Klein (2015) 91.1 Dyer et al. (2016) 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.3 Liu and Zhang (2017) 91.", "startOffset": 2, "endOffset": 51}, {"referenceID": 5, "context": "2 Cross and Huang (2016) 91.3 Liu and Zhang (2017) 91.7 Top-down parser 91.2 Bottom-up parser 91.3 In-order parser 91.8 reranking Huang (2008) 91.", "startOffset": 2, "endOffset": 143}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.", "startOffset": 2, "endOffset": 30}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.", "startOffset": 2, "endOffset": 60}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.", "startOffset": 2, "endOffset": 84}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.3 Kuncoro et al. (2017) 93.", "startOffset": 2, "endOffset": 111}, {"referenceID": 2, "context": "7 Charniak and Johnson (2005) 91.5 Choe and Charniak (2016) 92.6 Dyer et al. (2016) 93.3 Kuncoro et al. (2017) 93.6 Top-down parser 93.3 Bottom-up parser 93.3 In-order parser 93.6 semi-supervised reranking Choe and Charniak (2016) 93.", "startOffset": 2, "endOffset": 231}, {"referenceID": 10, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results.", "startOffset": 72, "endOffset": 118}, {"referenceID": 18, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results.", "startOffset": 72, "endOffset": 118}, {"referenceID": 13, "context": "With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al.", "startOffset": 85, "endOffset": 98}, {"referenceID": 6, "context": "With the semi-supervised setting, the in-order parser outperforms the best semi-supervised parser (Choe and Charniak, 2016) by achieving 94.", "startOffset": 98, "endOffset": 123}, {"referenceID": 6, "context": "English dependency results As shown in Table 5, by converting to Stanford Dependencies, without additional training data, our models achieves similar performance with the state-of-the-art system (Choe and Charniak, 2016); with the same additional training data, our models achieves new state-of-the-art results on dependency parsing by achieving 96.", "startOffset": 195, "endOffset": 220}, {"referenceID": 8, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results. With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al. (2017), which extend the work of Dyer et al.", "startOffset": 73, "endOffset": 313}, {"referenceID": 8, "context": "2016; Watanabe and Sumita, 2015) and the stateof-the-art hybrid parsers (Durrett and Klein, 2015; Liu and Zhang, 2017), achieving the state-of-theart results. With the reranking setting, the in-order parser outperforms the best discrete parser (Huang, 2008) and have the same performance of Kuncoro et al. (2017), which extend the work of Dyer et al. (2016) by adding gated attention mechanism on composition functions.", "startOffset": 73, "endOffset": 358}, {"referenceID": 3, "context": "9 Cheng et al. (2016) \u2020 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.", "startOffset": 2, "endOffset": 51}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.", "startOffset": 2, "endOffset": 90}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.7 94.0 Kuncoro et al. (2017) -re 95.", "startOffset": 2, "endOffset": 123}, {"referenceID": 0, "context": "5 Andor et al. (2016) 94.6 92.8 Dyer et al. (2016) -re 95.6 94.4 Dozat and Manning (2017)\u2020 95.7 94.0 Kuncoro et al. (2017) -re 95.7 94.5 Choe and Charniak (2016) -sre 95.", "startOffset": 2, "endOffset": 162}, {"referenceID": 28, "context": "Parser F1 fully-supervision Zhu et al. (2013) 83.", "startOffset": 28, "endOffset": 46}, {"referenceID": 25, "context": "2 Wang et al. (2015) 83.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "2 Dyer et al. (2016) 84.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": "2 Dyer et al. (2016) 84.6 Liu and Zhang (2017) 85.", "startOffset": 2, "endOffset": 47}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.", "startOffset": 9, "endOffset": 37}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.", "startOffset": 9, "endOffset": 61}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.", "startOffset": 9, "endOffset": 165}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.6 Wang and Xue (2014) 86.", "startOffset": 9, "endOffset": 190}, {"referenceID": 2, "context": "1 rerank Charniak and Johnson (2005) 82.3 Dyer et al. (2016) 86.9 Top-down parser 86.9 Bottom-up parser 87.5 In-order parser 88.0 semi-supervision Zhu et al. (2013) 85.6 Wang and Xue (2014) 86.3 Wang et al. (2015) 86.", "startOffset": 9, "endOffset": 214}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.", "startOffset": 2, "endOffset": 70}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.6 86.1 Cheng et al. (2016) \u2020 88.", "startOffset": 2, "endOffset": 100}, {"referenceID": 1, "context": "0 Ballesteros et al. (2016) 87.7 86.2 Kiperwasser and Goldberg (2016) 87.6 86.1 Cheng et al. (2016) \u2020 88.1 85.7 Dozat and Manning (2017) \u2020 89.", "startOffset": 2, "endOffset": 137}, {"referenceID": 22, "context": "Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013).", "startOffset": 91, "endOffset": 159}, {"referenceID": 25, "context": "Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013).", "startOffset": 91, "endOffset": 159}, {"referenceID": 9, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 12, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al.", "startOffset": 155, "endOffset": 196}, {"referenceID": 16, "context": "Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017) constituent (Dyer et al., 2016; Kuncoro et al., 2017) and CCG parsing (Xu, 2016; Lewis et al.", "startOffset": 155, "endOffset": 196}, {"referenceID": 34, "context": ", 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016).", "startOffset": 24, "endOffset": 54}, {"referenceID": 17, "context": ", 2017) and CCG parsing (Xu, 2016; Lewis et al., 2016).", "startOffset": 24, "endOffset": 54}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014).", "startOffset": 46, "endOffset": 70}, {"referenceID": 33, "context": "This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016).", "startOffset": 118, "endOffset": 164}, {"referenceID": 12, "context": "This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016).", "startOffset": 118, "endOffset": 164}, {"referenceID": 14, "context": "Rosenkrantz and Lewis (1970) formalize this in automata theory, which have appeared frequently in the compiler literature.", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "Roark and Johnson (1999) apply the strategy into parsing.", "startOffset": 10, "endOffset": 25}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto topdown system, which is the same as our top-down parser.", "startOffset": 47, "endOffset": 257}, {"referenceID": 4, "context": "Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto topdown system, which is the same as our top-down parser. Watanabe and Sumita (2015) employ treeLSTM to model the complex representation in stack in bottom-up system.", "startOffset": 47, "endOffset": 365}], "year": 2017, "abstractText": "Both bottom-up and top-down strategies have been used for neural transition-based constituent parsing. The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree, where bottom-up strategies and top-down strategies take post-order and pre-order traversal over trees, respectively. Bottom-up parsers benefit from rich features from readily built partial parses, but lack lookahead guidance in the parsing process; top-down parsers benefit from non-local guidance for local decisions, but rely on a strong encoder over the input to predict a constituent hierarchy before its construction. To mitigate both issues, we propose a novel parsing system based on in-order traversal over syntactic trees, designing a set of transition actions to find a compromise between bottom-up constituent information and top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark.", "creator": "LaTeX with hyperref package"}}}