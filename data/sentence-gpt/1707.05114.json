{"id": "1707.05114", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jul-2017", "title": "Towards Bidirectional Hierarchical Representations for Attention-based Neural Machine Translation", "abstract": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem: It provides a new approach to learning how to learn to avoid being forced to repeat a set of repeated word sequences that were previously associated with high-level word repetition. To maximize the predictive probability of target words, it uses a model that uses a single set of word patterns to assess whether they are informative as well as highly relevant (i.e., if one word were highly relevant) for the task (i.e., if one word was highly relevant, if one word were highly relevant, if one word were highly relevant, if one word were highly relevant). The model is available online (www.saves.org/index.html). To address this limitation, we used a method that measures a group of word types using a weighted average of each word type for the task. It is proposed that each word type is represented as one that represents each specific word type and then measures the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of the relative importance of", "histories": [["v1", "Mon, 17 Jul 2017 12:09:08 GMT  (205kb)", "http://arxiv.org/abs/1707.05114v1", "Accepted for publication at EMNLP 2017"]], "COMMENTS": "Accepted for publication at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["baosong yang", "derek f wong", "tong xiao", "lidia s chao", "jingbo zhu"], "accepted": true, "id": "1707.05114"}, "pdf": {"name": "1707.05114.pdf", "metadata": {"source": "CRF", "title": "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation", "authors": ["Baosong Yang", "Derek F. Wong", "Tong Xiao", "Lidia S. Chao", "Jingbo Zhu"], "emails": ["nlp2ct.baosong@gmail.com,", "derekfw@umac.mo,", "lidiasc@umac.mo,", "xiaotong@mail.neu.edu.cn", "zhujingbo@mail.neu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 7.\n05 11\n4v 1\n[ cs\n.C L\n] 1\n7 Ju\nl 2 01\n7\nThis paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks."}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT) automatically learns the abstract features of and semantic relationship between the source and target sentences, and has recently given state-of-the-art results for various translation tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). The most widely used model is the encoderdecoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense representation, followed by a decoding process which generates the target translation. By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on\n\u2217Corresponding author\nthe context vector alone. From a model architecture perspective, prior studies of the attentive encoder-decoder translation model are mainly divided into two types.\nThe sequence-to-sequence model treats a sentence as a sequence of tokens. The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the\ntranslation performance.\nThe tree-to-sequence model encodes a source sentence according to a given syntactic tree over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases \u201ctake up\u201d1 and \u201ca position\u201d2 have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7, the current approaches do not account for the differences in meaning which arise as a result of ignoring the neighboring context as well as the remote context, i.e. hNP7 \u2190 hPP8 (sibling) and hVP3 \u2190 hNP7 (child of sibling). More specifically, at the encoding step t, the generated phrase is based on the results at the previous time steps ht\u22121 and ht\u22122, but has no information about the parent phrases ht\u2032 for t \u2032 > t.\nTo address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models (Eriguchi et al., 2016). In contrast to the model of Eriguchi et al. (2016), we first use a bidirectional RNN (Schuster and Paliwal, 1997) at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.1). Secondly, we propose a bidirectional tree-based encoder (described in Section 3.2), in which the original bottom-up encoding model is extended using an additional top-down encoding process. In the bidirectional hierarchical model, the vector representations of the sentence, phrases as well as words, are therefore based on the global context rather than local information.\nTo effectively leverage hierarchical representations in generating the target words, we adopt a variant weighted tree-based attention mechanism (described in Section 3.4) in which a time-dependent gating scalar is used to control the proportion of conditional information be-\n1Take up has the meanings of start doing something new, use space/time, accept an offer, etc.\n2Position has the meanings of location, job offer, rank/status, etc.\ntween the word and phrase vectors. To alleviate the out-of-vocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the treebased model (as described in Section 3.3). Experimental results for the NIST English-toChinese translation task reveal that the proposed model significantly outperforms the vanilla treebased (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.1)."}, {"heading": "2 Tree-Based Neural Machine Translation", "text": "A neural machine translation system (NMT) aims to use a single neural network to build a translation model, which is trained to maximize the conditional distribution of sentence pairs using a parallel training corpus (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b,a). By incorporating syntactic information, the treebased NMT exploits an additional syntactic structure of the source sentence to improve the translation. Since most existing NMTs generate one target word at a time, given a source sentence x = (x1, ..., xN ) and its corresponding syntactic tree tr, the conditional probability of a target sentence y = (y1, ..., yM ) is formally expressed as:\np(y | x, tr) = M\u220f\n1\np(yj | y1, ..., yj\u22121, x, tr; \u03b8),\nwhere \u03b8 represents the model parameters. A treebased NMT consists of a tree-based encoder and a decoder."}, {"heading": "2.1 Tree-Based Encoder", "text": "In a tree-based encoder, the source language x is encoded according to a given syntactic structure tr of the sentence. As shown in Figure 2, Eriguchi et al. (2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al., 2015) to generate the phrase representations in a bottom-up fashion. In the present study, we utilize the gated recurrent unit (GRU) (Cho et al., 2014b) instead of an LSTM, in view of its comparable performance (Chung et al., 2014) and since it yields even better results for certain tasks (Jo\u0301zefowicz et al., 2015). The lexical annotation vectors (hl1, ..., h l N ) are sequentially generated by using a GRU. The i-th leaf node vector is calculated as:\nhli = f l GRU (xi, h l i\u22121), (1)\nwhere xi is the i-th source word embedding and hli\u22121 denotes the previous hidden state. The parent hidden state h\u2191i,j summarizes its left child h \u2191 i,k and right child h\u2191k+1,j (i < k < j) by applying the tree-GRU (Zhou et al., 2016) as follows:\nz\u2191i,j = \u03c3(U L (z)h \u2191 i,k + U R (z)h \u2191 k+1,j + b \u2191 (z)) r\u2191i,k = \u03c3(U L (rL)h \u2191 i,k + U R (rL)h \u2191 k+1,j + b \u2191 (rL))\nr\u2191k+1,j = \u03c3(U L (rR)h \u2191 i,k + U R (rR)h \u2191 k+1,j + b \u2191 (rR))\nh\u0303\u2191i,j = tanh(U L (h)(r \u2191 i,k \u2299 h \u2191 i,k)\n+ UR(h)(r \u2191 k+1,j \u2299 h \u2191 k+1,j) + b \u2191 (h))\nh\u2191i,j = z \u2191 i,jh\u0303 \u2191 i,j + (1\u2212 z \u2191 i,j)(h \u2191 i,k + h \u2191 k+1,j),\nwhere z\u2191i,j is the update gate; r \u2191 i,k, r \u2191 k+1,j are the reset gates for the left and right children; h\u0303\u2191i,j denotes the candidate activation; UL(\u00b7) and U R (\u00b7) represent weight matrices; b\u2191(\u00b7) denote bias vectors; \u03c3 is the logistic sigmoid function; and the operator \u2299 denotes element-wise multiplication between vectors. The phrase representations are recursively built in an upward direction."}, {"heading": "2.2 Decoding with a Tree-Based Attention Mechanism", "text": "In generating the target words, we employ a sequential decoder with an input-feeding method\n(Luong et al., 2015) and attention mechanism (Bahdanau et al., 2015). The conditional probability of the j-th target word yj is calculated using a non-linear function fsoftmax:\np(yj | y1, ..., yj\u22121, x, tr; \u03b8) = fsoftmax(cj),\nwhere cj is the composite hidden state, which consists of a target hidden state sj and a context vector dj:\ncj = ftanh([sj, dj ]).\nGiven the previous target word yj\u22121, the concatenation of the previous hidden state sj\u22121 and the previous context vector cj\u22121 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network:\nsj = f dec gru(yj\u22121, [sj\u22121, cj\u22121]).\nThe context vector dj is computed using an attention model which is used to softly summarize the attended part of the source-side representations. Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors:\ndj =\nN\u2211\ni=1\n\u03b1j(i)h l i +\nN\u22121\u2211\nk=1\n\u03b1j(k)h p k, (2)\nwhere hli is the i-th hidden state of the source word at leaf level, and hpk is the k-th hidden state of the source phrase. The weight \u03b1j(t) of node t is computed by:\n\u03b1j(t) = exp(et)\u2211N\ni=1 exp(e l i) + \u2211N\u22121 k=1 exp(e p k)\net = (Va) T tanh(Uasj +Waht + ba),\nwhere ht is the hidden state of the node. Va, Ua, Wa and ba are the model parameters."}, {"heading": "3 The Bidirectional Hierarchical Model", "text": "Although the tree-based encoder of Eriguchi et al. (2016) has shown certain advantages in translation tasks involving distant language pairs, e.g. English-Japanese, the representation of a phrase relies solely on its child nodes, and the word representation at leaf level only takes into account the sequential information. We argue that the incorporation of more hierarchical information into the representations may contribute to an improvement in the translation. In particular, the use of global\ninformation can help in distinguishing the differences between word meanings. Based on this hypothesis, we propose an alternative architecture, the bidirectional hierarchical model, to enhance the source-side representations."}, {"heading": "3.1 Bidirectional Leaf-Node Encoding", "text": "As discussed in Section 1, the unidirectional recurrent neural network reads an input sequence in order, from the first symbol to the last. In order to generate leaf node annotation vectors which jointly take into account both preceding and following annotations, we exploit a bidirectional RNN encoder (Bahdanau et al., 2015). The hidden state of the i-th leaf node hli is the concatenation of the forward and backward vectors:\nhli = [ \u2212\u2192 h li, \u2190\u2212 h li],\nwhere \u2212\u2192 h li is obtained by a rightward GRU, as shown in Equation 1, and a leftward GRU calculates \u2190\u2212 h li, as follows:\n\u2190\u2212 h li = f \u2190 GRU (xi, \u2190\u2212 h li\u22121),\nwhere \u2190\u2212 h li\u22121 is the previous hidden state."}, {"heading": "3.2 Bidirectional Tree-Node Encoding", "text": "Since the hidden states of leaf nodes are derived in a sequential, context-sensitive way, by generating phrase annotations in a bottom-up fashion, the sequential context can be propagated to tree nodes. However, the learned annotation vectors still fail to capture global information from the upper nodes. To enhance the representations with global semantic information, we propose to use a standard GRU recurrent network to update representations in a top-down fashion, as shown in Figure 3. The annotation vectors, which are learned by the previous encoding steps, are fed to the updating process.\nFirst, we treat the bottom-up hidden state of root h\u2191root, which covers the global meaning as well as the syntactic information of the source sentence, as the initial state of the top-down GRU network:\nh\u2193root = h \u2191 root.\nGiven an updated hidden state of the parent node h\u2193i,j , the hidden states of left and right children h \u2193 i,k and h\u2193k+1,j are calculated as:\nh\u2193i,k = f ld GRU (h \u2191 i,k, h \u2193 i,j)\nh\u2193k+1,j = f rd GRU (h \u2191 k+1,j , h \u2193 i,j),\nwhere h\u2191i,k and h \u2191 k+1,j are the left and right child annotation vectors generated via the bottomup tree-GRU network. Contrary to the similar top-down encoding for sentiment classification (Kokkinos and Potamianos, 2017), which uses same weighting parameters to handle both left and right child nodes, f ldGRU and f rd GRU with different parameters are applied in the proposed model to distinguish the left and right structural information. According to the definition of a GRU (Cho et al., 2014b), f ldGRU uses an update gate z \u2193 i,k, a reset gate r\u2193i,k and a candidate activation h\u0303 \u2193 i,k to generate h\u2193i,k, as follows:\nz\u2193i,k = \u03c3(W ld (z)h \u2191 i,k + U ld (z)h \u2193 i,j + b ld (z)) r\u2193i,k = \u03c3(W ld (r)h \u2191 i,k + U ld (r)h \u2193 i,j + b ld (r)) h\u0303\u2193i,k = tanh(W ld (h)h \u2191 i,k + U ld (h)(r \u2193 i,k \u2299 h \u2193 i,j) + b ld (h)) h\u2193i,k = (1\u2212 z \u2193 i,k)h \u2193 i,j + z \u2193 i,kh\u0303 \u2193 i,k, (3)\nwhere W ld(\u00b7) and U ld (\u00b7) represent weight matrices, and bld(\u00b7) denote bias vectors. f rd GRU is defined in a similar way.\nFrom a linguistic point of view, in the top-down GRU network, the reset gate is able to retain the useful global information and drop irrelevant information from the parent state h\u2193i,j , while the proportions of the global context from the top-down state h\u2193i,j , and the local context from the bottomup state h\u2191i,k are controlled by the update gate. As it covers both the partial meaning of the phrase and the whole meaning of the sentence, h\u2193i,k is regarded as the final representation of nodei,k:\nhpi,k = h \u2193 i,k.\nWith the propagation of information from root to leaf nodes, the i-th leaf node representation is updated as:\nhli = h \u2193 i .\nAs each source-side hidden state of the leaf nodes and tree nodes carries the hierarchical information of the sentence, we interpret such an encoded state as a hierarchical representation."}, {"heading": "3.3 Handling Out-of-Vocabulary: Tree-Based Rare Word Encoding", "text": "In NMT, the translation of rare words and unknown words is an open problem, since the computational cost increases with the size of the vocabulary. Sennrich et al. (2016) proposed a simple and effective approach to handling out-ofvocabulary by representing rare words as a sequence of sub-word units, which are segmented using byte-pair encoding (BPE) (Gage, 1994).\nWe propose a variant tree-based rare word encoding approach which extends the tree-based model to the sub-word level. Sub-word units are encoded following an additional binary lexical tree. For a sentence x = (x1, ..., xi, ..., xN ), BPE segments the word xi into a sequence of sub-word units (x1i , ..., x n i ). The binary lexical tree is simply built by composing two nodes in a rightwards fashion, (((x1i , x 2 i ), x 3 i )...), x n i ), as shown in Figure 4. From the i-th leaf node, the original syntactic tree is extended downwards using the binary lexical tree, and the set of leaf nodes are replenished as x = (x1, ..., x 1 i , x 2 i , ..., x n i , ..., xN ). Subword units can therefore be regarded as leaf nodes, and can be encoded using the proposed encoder, as illustrated in Figure 5. The experimental results in Section 4.1 demonstrate the effectiveness of this simple approach."}, {"heading": "3.4 Decoder with Weighted Variant of Attention Mechanism", "text": "Since each representation carries both local and global information, in this case, attending fairly to the lexical and phrase representations in each decoding step may cause the problem of overtranslation (repeatedly attending and translating the same constituent of a sentence). An alternative approach is to balance the attentive information between the lexical and phrase vectors in the context vector. To effectively leverage these hierarchical representations, we propose a weighted variant of the tree-based attention mechanism (the original is defined in Equation 2). Formally, the calculation of the context vector dj at step j is modified as:\ndj = (1\u2212 \u03b2j)\nn\u2211\ni=1\n\u03b1j(i)h l i + \u03b2j\nn\u22121\u2211\nk=1\n\u03b1j(k)h p k (4)\nwhere \u03b2j \u2208 [0, 1] is used to weight the expected importance of the representations. Inspired by work on a multi-modal NMT (Calixto et al., 2017) which exploits a gating scalar (Xu et al., 2015) to weight the image context vector, we use such a scalar in our model in order to dynamically adapt the weighting scalar. The gating scalar \u03b2j at step j is calculated by :\n\u03b2j = \u03c3(W\u03b2cj\u22121 + b\u03b2),\nwhere W\u03b2 and b\u03b2 represent the model parameters. In contrast with \u03b1, which denotes the correspondence between each source annotation and\nthe current target hidden state, \u03b2 is dominated by the target composite hidden state alone. In other words, \u03b2 is a time-dependent scalar in relation to the current target word, and therefore enables the attention model to explicitly quantify how far the leaf and no-leaf states contribute to the word prediction at each time step. In the proposed model, the phrase and lexical context vectors are learned by a single attention model, meaning that they are dependent, and the gating scalar weights the phrase and lexical context vectors in complementary fashion, as shown in Equation 4. This distinguishes the model from that introduced by Calixto et al. (2017), in which the context vectors of the source sentence and image (bi-modal) are measured using two independent attention models and the gating scalar is merely used to weight the image context vector."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data", "text": "We evaluate the proposed model on an Englishto-Chinese translation task. For reasons of computational efficiency, we extracted 1.4M sentence pairs, in which the maximum length of the sentence was 40, from the LDC parallel corpus3 as our training data. The models were developed using NIST mt08 data and were examined using NIST mt04, mt05, and mt06 data. The number of sentences in each dataset is shown in Table 1. On the English side, we used the constituent parser (Zeng et al., 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012).\nTo avoid data sparsity, words referring to time, date and number, which are low in frequency, are generalized as \u2018$time\u2019, \u2018$date\u2019 and \u2018$number\u2019. In addition, as described in Section 3.3, the vocab-\n3Our training data was selected from LDC2000T46, LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08 and LDC2005T10.\nularies are further compressed by segmenting the rare words into sub-word units using BPE."}, {"heading": "4.2 Experimental Settings", "text": "As shown in Table 2, which gives the statistics of the token types, we limit the source and target vocabulary size to 40,000, in order to cover all the English and Chinese tokens. The dimensions of word embedding and hidden layer are respectively set as 620 and 1,000. Due to the concatenation in the bidirectional leaf-node encoding, the dimensions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500. In order to prevent over-fitting, the training data is shuffled following each epoch. Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate. We set the mini-batch size to 16 and the beam search size to 5. The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002). In order to give an equitable comparison, all the NMT models used for comparison are implemented or re-implemented using GRU in our code, based on dl4mt4."}, {"heading": "4.3 Enhanced Hierarchical Representations", "text": "Firstly, the effectiveness of the enhanced hierarchical representations is evaluated through a set of experiments, the results of which are summarized in Table 3.\nCompared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.1) shows better performance. This also reveals that the future context at leaf level can contribute to word prediction. Secondly, although the representations of leaf nodes are learned in a sequential, context-sensitive way, the translation quality is further improved by considering the global semantic information in the top-down encoding (Section 3.2).\n4 https://github.com/nyu-dl/dl4mt-tutorial\nBy incorporating the above enhancements into the model, the proposed hierarchical encoder yields significant improvements over both the sequential and the tree-based models. The problem of OOV is alleviated by further extending the treebased model to sub-word level (Section 3.3). In addition, we evaluate our tree-based rare word encoding method against the conventional rare word encoding (Sennrich et al., 2016) using the sequential encoder (Bahdanau et al., 2015). The empirical results confirm that our proposed tree-based BPE method achieves performance comparable to that of the standard BPE in the sequential model, but is applicable to the tree-based NMT model.\nOverall, the proposed hierarchical encoder has demonstrated the ability to effectively model source-side representations from both the sequential and structural context. The NMT systems based on the proposed model significantly outperform those of conventional models using the sequential encoder and the tree-based encoder."}, {"heading": "4.4 Weighted Attention Model", "text": "As discussed in Section 3.4, in order to effectively leverage hierarchical representations in generating the target word, we adopt a variant weighted treebased attention mechanism which incorporates a scalar to control the proportion of conditional information between the word and phrase vectors. By manually or automatically varying the weight \u03b2, the utilization of the weighted attention model is assessed for four cases:\n\u2022 \u03b2 = 0.0: We manually set the weight of phrase vectors to 0.0; in other words, the decoder is forced to ignore the phrase vectors.\nThe final translation is therefore generated by merely summarizing the leaf vectors.\n\u2022 \u03b2 = 0.5: The representations of non-leaf nodes and leaf nodes participate equally in\nthe translation process. The decoder of this case therefore employs the same attention mechanism as that of the original model (Section 2.2).\n\u2022 \u03b2 = 1.0: In the reverse of the first case, the weight of the leaf nodes is manually set to\n0.0. Thus, only the phrase vectors are used to predict the target words.\n\u2022 Gating scalar (GS): A gating scalar is used for dynamically learning to control the pro-\nportion in which the lexical and phrase contexts contribute to the generation of the target words (Section 3.4).\nThe experimental results are shown in Table 4. The model which attends only to lexical annotation vectors (\u03b2 = 0.0) gives slightly better performance than that which uses equal weights for lexical and phrase vectors (\u03b2 = 0.5). The use of global information contributes to distinguishing the differences between word meanings, although the similar semantic information in the lexical and phrase representations aggravates the\nover-translation problem observed in the translation results. However, we found that the model which attends only to phrase representations tends to generate shorter translation of an average of 21.13 words in length, as shown in the last column of the first row of Table 4. Furthermore, the model that neglects the leaf representations (\u03b2 = 1.0) is likely to underperform the others that are also conditioned on the leaf nodes. Even though the phrase representations are derived from the lexical level via a bottom-up encoding, we believe it is unable to fully capture the lexical information of the source sentence. Through the use of the gating scalar, the hierarchical model achieves progressive improvements, as shown in Tables 3 and 4, the problem of over-translation is also alleviated. The representations of non-leaf nodes can be regarded as supplements in the translation process."}, {"heading": "5 Qualitative Analysis", "text": "Figure 6 shows an English sentence and its binary tree representation, together with the corresponding Chinese translations produced by the different\nNMT models. All the models successfully give the correct Chinese translation \u201c\u8be5 \u7ec4\u7ec7 \u4e0d\u4f1a\u201d for the first three words of the English sentence \u201cthe organization wouldn\u2019t\u201d. Differences appear in the translation of the fourth word, and these lead to markedly different meanings. The translation \u201c\u4f7f\u7528 \u5176 \u6210\u5458\u56fd \u4ee5\u5916 \u7684 \u6b66\u88c5\u529b\u91cf\u201d output by the sequential model, means \u201cuse the armed forces other than its member states\u201d where \u201cother than its member states\u201d is incorrectly interpreted as a complement to \u201carmed forces\u201d. This is caused by the intrinsic limitations of the sequential model, whereby it is unable to properly interpret the syntactic relationship of words. By explicitly incorporating the syntactic information, both the proposed hierarchical model and the tree-based model can accurately attend to the dashed section of Figure 6, and the translations can be correctly generated to reflect the meaning of the source sentence. The distinction between the translations produced by the original tree-based model and our hierarchical model is the interpretation of the words \u201careas outside\u201d. The tree-based model interprets it into \u201c\u5883 \u5916 (outside)\u201d, while our model correctly translates it into \u201c\u4ee5\u5916\u7684\u5730\u533a (areas outside)\u201d. We believe that, with the help of global and local contextual information, our model is able to capture the long dependencies as well as the short dependencies.\nWe conducted an in-depth analysis of the BPE segmented units of rare words. It was observed that the sub-word units could be categorized into three groups. The first group of units involve the phonetic Romanization (Pinyin) of Chinese. In translation, these are simply transliterated into\nthe corresponding Chinese characters. As shown in the second row of Table 5, \u201cLiu/jing/min\u201d is a person\u2019s name. The segmented units are the phonetic representations. Both models can successfully transliterate this into the Chinese equivalent, \u201c\u5218/\u656c/\u6c11\u201d. The second group of subword units are likely to represent the word morphemes. The words are segmented into sub-word units, which are to some extent close to the linguistic word stems and suffixes. For example, the word \u201cadventurer\u201d is segmented into \u201cadventur/er\u201d, which is correctly translated into the Chinese translations \u201c\u63a2\u9669/\u5bb6\u201d and \u201c\u63a2\u9669/\u8005\u201d respectively by the hierarchical and sequential models, while the third group of sub-word units offer no linguistic interpretation. It is easy to see, using the BPE algorithm, that the identification of subword units is merely based on their frequency in the training data, with the result that not all units are well-formed linguistic morphemes. However, an interesting finding arises regarding the translation of these segmented units. In the sequential model, the word is incorrectly translated; however, it can be correctly translated by the hierarchical model. Taking \u201chi/k/ed\u201d as an example, the sequential model gives an incorrect translation \u201c\u53d1\u751f(happened)\u201d, while the hierarchical model translates it into \u201c\u4e0a\u5347(rise)\u201d which is a synonym of \u201chiked\u201d. This result indicates that in our hierarchical model, the parent node of hierarchical representation for sub-word units \u201chi/k/ed\u201d is better able to capture the meaning of the word as a whole; this cannot be captured independently by the sequential model."}, {"heading": "6 Conclusion", "text": "In this paper, we propose an improved NMT system with a novel bidirectional hierarchical encoder, which enhances the source-side representations of a sentence, that is, both phrases and words, with local and global context information. By introducing a tree-based rare word encoding, the hierarchical model is extended to sub-word level in order to alleviate the problem of OOVs. To effectively leverage the enhanced hierarchical representations, we also propose a weighted variant of the attention model which dynamically adjusts the proportion of conditional information between the lexical and phrase annotation vectors. Experimental results for NIST English-Chinese translation tasks demonstrate that the proposed model significantly outperforms the vanilla tree-based and sequential NMT models."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the National Natural Science Foundation of China (Grant No. 61672555), a Multiyear Research Grant from the University of Macau (Grant Nos. MYRG2017-00087-FST, MYRG201500175-FST and MYRG2015-00188-FST) and the Science and Technology Development Fund of Macau (Grant No. 057/2014/A). The work of Tong Xiao and Jingbo Zhu was supported in part by the National Natural Science Foundation of China (Grant Nos. 61672138 and 61432013), the Fundamental Research Funds for the Central Universities, and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation", "author": ["Iacer Calixto", "Qun Liu", "Nick Campbell."], "venue": "CoRR, abs/1702.01287.", "citeRegEx": "Calixto et al\\.,? 2017", "shortCiteRegEx": "Calixto et al\\.", "year": 2017}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Advances in neural information pro-", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Tree-to-SequenceAttentional Neural Machine Translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "A New Algorithm for Data Compression", "author": ["Philip Gage."], "venue": "The C Users Journal, 12(2):23\u201338.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural Computation, 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Structural Attention Neural Networks for Improved Sentiment Analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos."], "venue": "CoRR, abs/1701.01811.", "citeRegEx": "Kokkinos and Potamianos.,? 2017", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "WeiJing Zhu."], "venue": "Proceedings", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional Recurrent Neural Networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Linguistic Input Features Improve Neural Machine Translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation, pages 83\u201390.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Syntactically Guided Neural Machine Translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 299\u2013305.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation", "author": ["Tong Xiao", "Jingbo Zhu", "Hao Zhang", "Qiang Li."], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics, Sys-", "citeRegEx": "Xiao et al\\.,? 2012", "shortCiteRegEx": "Xiao et al\\.", "year": 2012}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "Proceedings of the 32nd International", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Graph-Based Lexicon Regularization for PCFG With Latent Annotations", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(3):441\u2013450.", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Lexicon Expansion for Latent Variable Grammars", "author": ["Xiaodong Zeng", "Derek F. Wong", "Lidia S. Chao", "Isabel Trancoso", "Liangye He", "Qiuping Huang."], "venue": "Pattern Recognition Letters, (42):47\u201355.", "citeRegEx": "Zeng et al\\.,? 2014", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "author": ["Yao Zhou", "Cong Liu", "Yan Pan."], "venue": "Proceedings of 26th International Conference on Computational Linguistics, pages 2912\u2013 2922.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 18, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 0, "context": "tion tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 11, "endOffset": 90}, {"referenceID": 18, "context": "decoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on", "startOffset": 38, "endOffset": 61}, {"referenceID": 18, "context": "The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b).", "startOffset": 181, "endOffset": 224}, {"referenceID": 3, "context": "The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b).", "startOffset": 181, "endOffset": 224}, {"referenceID": 14, "context": "(2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly,", "startOffset": 59, "endOffset": 87}, {"referenceID": 0, "context": "Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly,", "startOffset": 9, "endOffset": 32}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015).", "startOffset": 193, "endOffset": 234}, {"referenceID": 19, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015).", "startOffset": 193, "endOffset": 234}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al.", "startOffset": 194, "endOffset": 325}, {"referenceID": 5, "context": "these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the", "startOffset": 194, "endOffset": 353}, {"referenceID": 19, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 5, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 25, "context": "The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases.", "startOffset": 33, "endOffset": 93}, {"referenceID": 5, "context": "To address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models (Eriguchi et al., 2016).", "startOffset": 161, "endOffset": 184}, {"referenceID": 14, "context": "(2016), we first use a bidirectional RNN (Schuster and Paliwal, 1997) at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.", "startOffset": 41, "endOffset": 69}, {"referenceID": 5, "context": ", 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases \u201ctake up\u201d and \u201ca position\u201d have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7, the current approaches do not account for the differences in meaning which arise as a result of ignoring the neighboring context as well as the remote context, i.e. hNP7 \u2190 hPP8 (sibling) and hVP3 \u2190 hNP7 (child of sibling). More specifically, at the encoding step t, the generated phrase is based on the results at the previous time steps ht\u22121 and ht\u22122, but has no information about the parent phrases ht\u2032 for t \u2032 > t. To address the above problems, we propose a novel architecture, a bidirectional hierarchical encoder, which extends the existing attentive treestructured models (Eriguchi et al., 2016). In contrast to the model of Eriguchi et al. (2016), we first use a bidirectional RNN (Schuster and Paliwal, 1997) at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following contexts (described in Section 3.", "startOffset": 8, "endOffset": 1203}, {"referenceID": 5, "context": "Figure 2: The tree-based model of Eriguchi et al. (2016) comprising a structured and sequential encoder.", "startOffset": 34, "endOffset": 57}, {"referenceID": 16, "context": "To alleviate the out-of-vocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the treebased model (as described in Section 3.", "startOffset": 160, "endOffset": 183}, {"referenceID": 5, "context": "Experimental results for the NIST English-toChinese translation task reveal that the proposed model significantly outperforms the vanilla treebased (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al.", "startOffset": 148, "endOffset": 171}, {"referenceID": 0, "context": ", 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.", "startOffset": 34, "endOffset": 57}, {"referenceID": 8, "context": "(2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al.", "startOffset": 56, "endOffset": 109}, {"referenceID": 7, "context": "(2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al.", "startOffset": 56, "endOffset": 109}, {"referenceID": 19, "context": ", 2000) recurrent neural network (RNN) to encode the lexical nodes and a treeLSTM (Tai et al., 2015) to generate the phrase representations in a bottom-up fashion.", "startOffset": 82, "endOffset": 100}, {"referenceID": 3, "context": "In the present study, we utilize the gated recurrent unit (GRU) (Cho et al., 2014b) instead of an LSTM, in view of its comparable performance (Chung et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 4, "context": ", 2014b) instead of an LSTM, in view of its comparable performance (Chung et al., 2014) and since it yields even better results for certain tasks (J\u00f3zefowicz et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 9, "context": ", 2014) and since it yields even better results for certain tasks (J\u00f3zefowicz et al., 2015).", "startOffset": 66, "endOffset": 91}, {"referenceID": 2, "context": "As shown in Figure 2, Eriguchi et al. (2016) employed a forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 25, "context": "The parent hidden state h\u2191i,j summarizes its left child h \u2191 i,k and right child h\u2191k+1,j (i < k < j) by applying the tree-GRU (Zhou et al., 2016) as follows:", "startOffset": 125, "endOffset": 144}, {"referenceID": 12, "context": "In generating the target words, we employ a sequential decoder with an input-feeding method (Luong et al., 2015) and attention mechanism (Bahdanau et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 0, "context": ", 2015) and attention mechanism (Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 55}, {"referenceID": 12, "context": "Given the previous target word yj\u22121, the concatenation of the previous hidden state sj\u22121 and the previous context vector cj\u22121 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network:", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors:", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Although the tree-based encoder of Eriguchi et al. (2016) has shown certain advantages in translation tasks involving distant language pairs, e.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "In order to generate leaf node annotation vectors which jointly take into account both preceding and following annotations, we exploit a bidirectional RNN encoder (Bahdanau et al., 2015).", "startOffset": 163, "endOffset": 186}, {"referenceID": 11, "context": "Contrary to the similar top-down encoding for sentiment classification (Kokkinos and Potamianos, 2017), which uses same weighting parameters to handle both left and right child nodes, f ld GRU and f rd GRU with different parameters are applied in the proposed model to distinguish the left and right structural information.", "startOffset": 71, "endOffset": 102}, {"referenceID": 3, "context": "According to the definition of a GRU (Cho et al., 2014b), f ld GRU uses an update gate z \u2193 i,k, a reset gate r i,k and a candidate activation h\u0303 \u2193 i,k to generate h\u2193i,k, as follows:", "startOffset": 37, "endOffset": 56}, {"referenceID": 6, "context": "(2016) proposed a simple and effective approach to handling out-ofvocabulary by representing rare words as a sequence of sub-word units, which are segmented using byte-pair encoding (BPE) (Gage, 1994).", "startOffset": 188, "endOffset": 200}, {"referenceID": 15, "context": "Sennrich et al. (2016) proposed a simple and effective approach to handling out-ofvocabulary by representing rare words as a sequence of sub-word units, which are segmented using byte-pair encoding (BPE) (Gage, 1994).", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Inspired by work on a multi-modal NMT (Calixto et al., 2017) which exploits a gating scalar (Xu et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": ", 2017) which exploits a gating scalar (Xu et al., 2015) to weight the image context vector, we use such a scalar in our model in order to dynamically adapt the weighting scalar.", "startOffset": 39, "endOffset": 56}, {"referenceID": 1, "context": "This distinguishes the model from that introduced by Calixto et al. (2017), in which the context vectors of the source sentence and image (bi-modal) are measured using two independent attention models and the gating scalar is merely used to weight the image context vector.", "startOffset": 53, "endOffset": 75}, {"referenceID": 20, "context": "On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012).", "startOffset": 105, "endOffset": 124}, {"referenceID": 5, "context": ", 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al.", "startOffset": 114, "endOffset": 137}, {"referenceID": 22, "context": "Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate.", "startOffset": 60, "endOffset": 74}, {"referenceID": 13, "context": "The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002).", "startOffset": 90, "endOffset": 113}, {"referenceID": 5, "context": "Compared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.", "startOffset": 46, "endOffset": 69}, {"referenceID": 16, "context": "In addition, we evaluate our tree-based rare word encoding method against the conventional rare word encoding (Sennrich et al., 2016) using the sequen-", "startOffset": 110, "endOffset": 133}, {"referenceID": 0, "context": "tial encoder (Bahdanau et al., 2015).", "startOffset": 13, "endOffset": 36}], "year": 2017, "abstractText": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "creator": "LaTeX with hyperref package"}}}