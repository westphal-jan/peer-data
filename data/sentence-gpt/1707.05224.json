{"id": "1707.05224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2017", "title": "Detection, Recognition and Tracking of Moving Objects from Real-time Video via Visual Vocabulary Model and Species Inspired PSO", "abstract": "In this paper, we address the basic problem of recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.", "histories": [["v1", "Fri, 2 Jun 2017 11:09:10 GMT  (1572kb)", "http://arxiv.org/abs/1707.05224v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["kumar s ray", "anit chakraborty", "sayandip dutta"], "accepted": false, "id": "1707.05224"}, "pdf": {"name": "1707.05224.pdf", "metadata": {"source": "CRF", "title": "FFECTIVE recognition of objects for tracking in real-time video stream and processing of data involve integration of background modelling, shadow removal, foreground modelling", "authors": [], "emails": ["ksray@isical.ac.in)", "ianitchakraborty@gmail.com).", "sayandip199309@gmail.com)"], "sections": [{"heading": null, "text": "recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.\nIndex Terms\u2014 Background Modelling, Bag of Words, Cubic SVM, Foreground Modelling, Object Detection, Object recognition, PSO, Shadow Removal, Visual Vocabulary.\nI. INTRODUCTION\nFFECTIVE recognition of objects for tracking in real-time\nvideo stream and processing of data involve integration of background modelling, shadow removal, foreground modelling\nand proper detection of objects. Recognition of the detected\nobjects is done by extracting the features obtained from the principle of bag of words (BOW). Extracted feature sets are\ntracked in the successive test frames via species inspired PSO.\nAlthough, various detection and tracking algorithms exist, still; object detection, recognition, effective tracking of feature\nsets and adoptability of handling occlusions and other noise are still a standing challenge in the field of computer vision. In\nKumar S. Ray is with Indian Statistical Institute, 203 B.T Road, Kolkata\n108, India. (e-mail: ksray@isical.ac.in)\nAnit Chakraborty is with Indian Statistical Institute, 203 B. T Road,\nKolkata \u2013 108, India. (e-mail: ianitchakraborty@gmail.com). Sayandip Dutta is with Indian Statistical Institute, 203 B.T Road, Kolkata 108, India. (e-mail: sayandip199309@gmail.com)\ntracking algorithm, satisfactory training model is of utmost\nimportance. In recent years, several attentions [51, 49, 43, 38,\n32, 30, 17, 18, 20] have been given in this direction to achieve\nand share the goal of this paper. Generally, appearance based\ntracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].\nSeveral tracking algorithms based on static appearance\nmodels exists, which are either trained using only the first few consecutive set of iterations or defined manually [56], [36], [7],\n[35]. These algorithmic frameworks often fail to deal with momentous appearance changes, which cause nonlinear\ntransformation of the appearance of a given object. Such\nchallenges lead to difficulty when there is an absence of sufficient amount of priori knowledge. In our paper, we have\nconsidered several appearances of a given training object of our\ninterest, which can capture the momentous and nonlinear changes of the appearances, which are explained in length in\nSection III and IV.\nHere, we use the Visual Vocabulary Model using Bag of\nWords to extract the necessary features of certain instances of\nobjects through rigorous high level training. Subsequently, we apply the features to the test datasets to recognize and locate our\nobjects in the video scenes. Using visual instance occurrence and their probabilistic presence to imply a certain domain, we\nobtain optimum accuracy in domain recognition as well.\nThe contributions of this paper are: \u2022 Background modelling and extraction of astute shadow\nfree images using color invariant approach.\n\u2022 Foreground modelling using morphological operators for\neffective reconstruction of the image.\n\u2022 Adoptability of handling occlusions and other noises\npresent in the test datasets.\n\u2022 Extraction of the features of the objects captured in the\nblobs via the principle of Bag of Words.\n\u2022 Classification of the objects in our domain of interest using\nprobabilistic word occurrence for domain recognition.\n\u2022 Tracking of the recognized objects via species inspired\nPSO.\nE\nThe organization of the paper constitutes: review of related\nworks in the field of object detection and recognition,\nespecially, based on supervised learning are briefly described in\nthe following section (II). Section III explains the proposed method for detection, recognition and tracking, specifically,\nsection III (D) describes the concept of Visual Vocabulary\nModel for object recognition. Experimental results on several datasets and the comparative analysis with some state-of-the-\nart algorithms are presented in section IV. Section V concludes\nthe paper and discusses the future possibilities for further improvements."}, {"heading": "II. BRIEF REVIEW OF RELATED WORKS", "text": "Numerous color histograms based tracking algorithms [52,\n55] have been proposed in recent years. The mean shift tracking algorithm has been extended by Collins [53] with the scale\nvariation of object of interests in a video frame. Perez et al. [55] used color histogram in addition with a particle filter [57] for\ntracking of objects in video frames. A spatiogram based\napproach has been proposed to capture spatial relationships of statistical properties of pixel, in Birchfield and Rangarajan [45].\nHe et al. [7] developed a locality sensitive histogram at each pixel for finer distribution of the visual feature points for object\ntracking in video scenes. Histograms of oriented gradients\n(HOGs) [42] is proposed for object tracking [34] in addition to the integral histogram [44]. Covariance region descriptors [39]\nbased approaches were introduced for tracking, to combine\ndifferent features. Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance\nbased tracking of objects [17], [38], [19], [9]. Spatio-temporal\nrepresentation combined with genetic algorithm has also been\nused for feature extraction [1]. Recently pixel based\nsegmentations have been applied [2] to handle tracking.\nVarious generative models have been proposed for multiple\nobject tracking in past years. In [58] and [59], Sparse\nGenerative Appearance Modeling is implemented to build an appearance model of objects. Gaussian Mixture Models\n(GMM) [60] [62], are popular generative approach for tracking. Apart from GMM, several other mixture models have been used\nin tracking in earlier days, such as finite mixture models [63-\n68]. Priebe et al. [67] introduced an algorithm based on recursive mixture density estimation. To extract time-invariant\ncharacteristics, the authors of [69] present a Bayesian Tracking approach using autoregressive Hidden Markov Model (AR-\nHMM) for robust visual tracking.\nIn recent years, discriminative models are leading the way in the field of object tracking [49], [33]. In this method, a binary\nclassifier is trained from the input video sequence, for\nseparation of target and background. The classifiers that have been extensively used for object tracking are: ranking SVM\n[18], semi-boosting [30], support vector machine (SVM) [49], boosting [38], structured output SVM [19], and online multi-\ninstance boosting [17]. In [49], a trained SVM classifier is\nintegrated for tracking, to tackle appearance based changes with varying illumination.\nA confidence map [43] in each frame is built using a discriminative feature combination, learned online, for\nseparation of background from target objects. Larese et al. [3]\nhave used SIFT descriptor to discriminate the patterns and then\nused it to build Bag of Words model and finally classified with\nSVM.\nVarious tracking codes are available for evaluation with\nsignificant effort of the authors, e.g., MIL [17], OAB [38], IVT\n[32], L1 [26], TLD [23] and likes."}, {"heading": "III. PROPOSED METHOD", "text": "Accurate detection of the objects of interest across multiple\nframes and tracking of the recognized objects are still a\nchallenge. In order to do that, first we model the background and remove the hard shadows from the background to extract\nthe exact area occupied by the object in a frame. Next, we model\nthe foreground and subtract the background model without shadow to obtain the blob of an object. Before recognizing the\nobject inside the blob, we train a machine learning inspired Visual Vocabulary Model with a set of objects which can\nrepresent our domain of interest for recognition and tracking. In\nboth the cases, i.e., in case of training stage we extract the features of the objects of the training data by principle of Bag\nof Words and in the testing phase, we use the same technique to extract the features of the objects of the test data. After\nclassification of extracted feature sets, we track the features of\ninterest of the recognized objects in successive video frames via Species inspired PSO."}, {"heading": "A. Background Modeling", "text": "Background modelling is an integral part of our algorithm. This part consists of pretreatment of each frame of the video sequence, followed by temporal image analysis. We update the background continuously so as to adapt to the changing background and other variations. We set a background adaptive threshold in order to differentiate between foreground and background objects. Finally, we apply certain domain centric morphological operators to the updated background to obtain smooth background images.\nIn [31], Li et al. proposed an idea for background modelling. In our work, we introduced some modification of the same work and proceed as follows:\nAt each time step an image \ud835\udc3c\ud835\udc5a \ud835\udc61 is obtained by subtracting two successive video frames and \ud835\udc39\ud835\udc5a \ud835\udc61 can be obtained by subtracting the current video frame with the background model. To deal with sudden illumination variation an AND-OR operation is performed over \ud835\udc3c\ud835\udc5a \ud835\udc61 and \ud835\udc39\ud835\udc5a \ud835\udc61 . (Fig. 1)\na. Pretreatment\nTransformation of color image to grayscale is defined as:\ngray = 0.299R + 0.587G + 0.114B. (1)\nb. Temporal image analysis:\nThe extracted frame I t is compared with its previous frame It \u22121 in order to obtain \ud835\udc3c\ud835\udc5a \ud835\udc61 by predicting the similarity between the two consecutive pixel values of frames It (x, y) and It-1(x, y), expressed using radiometric similarity R (I t (x,y), I t \u22121 (x,y)) [31]:\nR(It(x,y), It\u22121(x,y))=\n\ud835\udc38[\ud835\udc4a(\ud835\udc3c\ud835\udc61(\ud835\udc65,\ud835\udc66)\ud835\udc4a(\ud835\udc3c\ud835\udc61\u22121(\ud835\udc65,\ud835\udc66))]\u2212\ud835\udc38[\ud835\udc4a(\ud835\udc3c\ud835\udc61(\ud835\udc65,\ud835\udc66)]\ud835\udc38[\ud835\udc4a(\ud835\udc3c\ud835\udc61\u22121(\ud835\udc65,\ud835\udc66))]\n\u221a\ud835\udc37[\ud835\udc4a(\ud835\udc3c\ud835\udc61(\ud835\udc65,\ud835\udc66)]\ud835\udc37[\ud835\udc4a(\ud835\udc3c\ud835\udc61\u22121(\ud835\udc65,\ud835\udc66)] . (2)\nMean and variance of the pixel intensities captured in a\nparticular window of a specific video frame W; E[W], D[W].\nPixel centers are compared between the succeeding images (I t (x, y), I t \u22121 (x, y)). Temporal binary image of the moving object (\ud835\udc3c\ud835\udc5a) has a radiometric similarity value, formally expressed as: \ud835\udc3c\ud835\udc5a(\ud835\udc65, \ud835\udc66) = { 1, \ud835\udc56\ud835\udc53 \ud835\udc45(\ud835\udc65, \ud835\udc66) > \ud835\udc47\ud835\udc4f 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 . (3) Similarly, \ud835\udc39\ud835\udc5a \ud835\udc61 is formulated on a hypothesis based on the difference threshold (\ud835\udc47\ud835\udc4f), between background frame and the current frame, formally:\n\ud835\udc39\ud835\udc5a \ud835\udc61 = { 1, \ud835\udc56\ud835\udc53 |\ud835\udc3c\ud835\udc61(\ud835\udc65, \ud835\udc66) \u2212 \ud835\udc35\ud835\udc61(\ud835\udc65, \ud835\udc66)| > \ud835\udc47\ud835\udc4f 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 . (4) The pixels (x,y) of moving objects are formulated by operating on \ud835\udc3c\ud835\udc5a(\ud835\udc65, \ud835\udc66) and \ud835\udc39 \ud835\udc61(\ud835\udc65, \ud835\udc66): \ud835\udc40\ud835\udc61(\ud835\udc65, \ud835\udc66) = { 1, \ud835\udc56\ud835\udc53 (\ud835\udc3c\ud835\udc5a(\ud835\udc65, \ud835\udc66) \u2229 \ud835\udc39\n\ud835\udc61(\ud835\udc65, \ud835\udc66)) = 1) 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 . (5)\nThe moving pixels in video frames are identified by \ud835\udc40\ud835\udc61(\ud835\udc65, \ud835\udc66).\nc. Background updating\nBackground model is updated with newly arrived information from previous frame using the first-order recursive filter: Bt +1 (x, y) = Bt (x, y) +\u03b1 \u00d7 (I t (x, y) \u2013 B t (x, y)), (6) where \u03b1 is an arbitrary adaptation coefficient.\nIn our implementation, a vector history V, with the six last\nvalues updated cumulatively, is considered as: V = [E(t), E (t \u22121), E (t \u2212 2), E (t \u2212 3), E(t \u2212 4), E (t \u2212 5)]. (7) At time t, the mean value of the frame is E(t).\nFor each frame, we calculate proper learning rate \ud835\udefc, based on this vector: \ud835\udefc = \ud835\udc4e + \ud835\udc4f |\ud835\udc38(\ud835\udc61)\u2212\ud835\udc38(\ud835\udc61\u22125)|\nmax (\ud835\udc38(\ud835\udc61),\ud835\udc38(\ud835\udc61\u22125)) , (8)\nTypically, \ud835\udc4e ranges from 0.04 to 0.06. Here a is chosen to be 0.05. For a given value of b and the gain as stated by |\ud835\udc38(\ud835\udc61)\u2212\ud835\udc38(\ud835\udc61\u22125)| max (\ud835\udc38(\ud835\udc61),\ud835\udc38(\ud835\udc61\u22125)) , usually we obtain a small value of \ud835\udefc. The slope of the gain variation curve, is denoted by b, where the gain is represented by [ |\ud835\udc38(\ud835\udc61)\u2212\ud835\udc38(\ud835\udc61\u22125)|\nmax (\ud835\udc38(\ud835\udc61),\ud835\udc38(\ud835\udc61\u22125)) ].\nd. Adaptive threshold: The formulation of the noise is described as follows:\n\ud835\udc5d(\ud835\udc5b) = 1\n\u221a2\ud835\udf0b\ud835\udf0e \ud835\udc52\ud835\udc65\ud835\udc5d {\u2212\n(\ud835\udc5b\u2212\ud835\udf07)2\n2\ud835\udf0e2 }, (9)\nwhere p(n) is the probability of the background pixel.\nLet d be a pixel of the image, the gray histogram of the pixel is h(d), and background pixels and foreground pixels are denoted by IB and IF respectively. Probability of a background pixel misidentified as foreground pixel and vice versa are as follows: \ud835\udc43\ud835\udc39|\ud835\udc35 = \u2211 \ud835\udc5d(\ud835\udc51 | \ud835\udc35)\ud835\udc51\u2208\ud835\udc3c\ud835\udc39 and \ud835\udc43\ud835\udc35|\ud835\udc39 = \u2211 \ud835\udc5d(\ud835\udc51 | \ud835\udc39)\ud835\udc51\u2208\ud835\udc3c\ud835\udc35 , (10) where Pd| B is the probability of background pixel and Pd| F is the probability of foreground pixel.\nOur goal is to minimize Pd|B and Pd|F as much as possible.\nThe Min PF|B is significant, as after morphological operation in the post-process, PB|F will be smaller. \ud835\udc5d(\ud835\udc35) is the priori probability of the background as calculated from gray histogram of the image \ud835\udc3c\ud835\udc5a \ud835\udc61 . \ud835\udc5d(\ud835\udc35) = \u2211 \u210e(\ud835\udc51) \u00b5 = 0\ud835\udc47\ud835\udc51=\u2212\ud835\udc47 . (11) Following is the noise of the variance model, as expressed: \ud835\udf0e = \u2211 \u210e2(\ud835\udc51)/\ud835\udc5d(\ud835\udc35)\ud835\udc47\ud835\udc51=\u2212\ud835\udc47 . (12) A fitting criterion describes the threshold value defined as, \ud835\udc52\ud835\udc40\ud835\udc56\ud835\udc5b = \u2211 (\ud835\udc5d(\ud835\udc35)\ud835\udc5d(\ud835\udc51|\ud835\udc35) \u2212 \u210e(\ud835\udc51)) 2 \ud835\udc51\u2208\ud835\udc37 . (13)\ne. Morphologic image operation:\nApart from moving objects, binary images contain a lot of residual noise. The object detection fails fatally because of many pixel holes in the image. The morphological operators help to remove the holes present and many of the noise by smoothening the edges of the blob. The morphological operators used in this experiment are Adaptive Kernel operator, Gaussian operator and Laplacian Filter."}, {"heading": "B. Shadow removal", "text": "As mentioned in [41] by Xu et al., the shadow removal\napproach is pictorially described in Fig. 2.\nThe r,g,b normalization is formulated as follows: \ud835\udc5f\u2032 = \ud835\udc5f\n\u221a\ud835\udc5f2+\ud835\udc542+\ud835\udc4f2 , (14)\n\ud835\udc54\u2032 = \ud835\udc54\n\u221a\ud835\udc5f2+\ud835\udc542+\ud835\udc4f2 , (15)\n\ud835\udc4f\u2032 = \ud835\udc4f\n\u221a\ud835\udc5f2+\ud835\udc542+\ud835\udc4f2 , (16)\nwhere r, g, b are input image color channels, r\u2019,b\u2019,g\u2019 construct the shadow-free color invariant image.\nApplication of Gaussian smooth filter suppresses the high frequency textures in both invariant and original images. These smoothed color images are converted to gray scale, following the HSV color model definition, to detect the edges, formally: \ud835\udc38\ud835\udc5c\ud835\udc5f\ud835\udc56 = \u2016\ud835\udc52\ud835\udc51\ud835\udc54\ud835\udc52(\ud835\udc3c\ud835\udc5c\ud835\udc5f\ud835\udc56)\u2016, \ud835\udc38\ud835\udc56\ud835\udc5b\ud835\udc63(\ud835\udc56) = \u2016\ud835\udc52\ud835\udc51\ud835\udc54\ud835\udc52(\ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc63(\ud835\udc56))\u2016, (17) where \ud835\udc38\ud835\udc5c\ud835\udc5f\ud835\udc56 is the edge of the original image after applying smooth filter and \ud835\udc3c\ud835\udc5c\ud835\udc5f\ud835\udc56 is the original image. \ud835\udc38\ud835\udc56\ud835\udc5b\ud835\udc63(\ud835\udc56) is the edge of the color invariant image after applying smooth filter and \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc63(\ud835\udc56)is the color invariant image. Here, the values of i (i.e. the invariant image index) are 1 and 2. The hard shadow edge mask is constructed by choosing the strong edges of original images that are absent in the invariant images. Thus, we get:\n\ud835\udc3b\ud835\udc46(\ud835\udc65, \ud835\udc66) = {\n1, \ud835\udc38\ud835\udc5c\ud835\udc5f\ud835\udc56(\ud835\udc65, \ud835\udc66) > \ud835\udc611, &\nmin \ud835\udc56 (\ud835\udc38\ud835\udc56\ud835\udc5b\ud835\udc63(\ud835\udc56)(\ud835\udc65, \ud835\udc66) < \ud835\udc612)\n0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52\n, (18)\nwhere t1, t2 are thresholds, set manually, and assessed hard shadow edge mask is HS(x,y). In (18), t1 maps the selected shadow edges to the strong edges of the subsequent hard shadows in images. t2 selects edges belonging only to shadows."}, {"heading": "C. Foreground Modelling and Reconstruction", "text": "In the present work, we have introduced a new concept of foreground modeling. This is essential to recognize and detect the objects from static or moving background variants.\nWe consider Poisson equation solution to reinforce the shadow portions from the derivative frames. To begin with, shadow edge masks of the two kinds and are merged as: mask = VS | HS, (19) where VS represents vague shadow and HS represents hard shadow. Furthermore, masking is applied to the gradient field: \u2207\ud835\udc56\u2032 = \ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 \u22c5 \u2207\ud835\udc56 = (\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 \u22c5 \ud835\udc3a\ud835\udc65 , \ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 \u22c5 \ud835\udc3a\ud835\udc66) , (20) where \ud835\udc3a\ud835\udc65 and \ud835\udc3a\ud835\udc66 are gradient field along the axis. The clipped derivatives are denoted as Gx\u2019 and Gy\u2019 and the calculation of the scalar is represented as: \ud835\udc51\ud835\udc56\ud835\udc63\ud835\udc3a = \ud835\udf15\ud835\udc3a\ud835\udc65 \u2032\n\ud835\udf15\ud835\udc65 + \ud835\udf15\ud835\udc3a\ud835\udc66\n\u2032\n\ud835\udf15\ud835\udc66 . (21)\nFinally, shadow image restoration is done by working out\nthe well-known Poisson equation: \u22072s = divG, (22) where, s is the calculated shadow image. As we perform the\nprocess in logarithmic domain, an exponent operation is added to the image: \ud835\udc46(\ud835\udc65, \ud835\udc66) = exp(\ud835\udc60(\ud835\udc65, \ud835\udc66) \u2212 max \ud835\udc65,\ud835\udc66 (\ud835\udc60(\ud835\udc65, \ud835\udc66))). (23)\nShadow extraction from original image, as formulated:\nr(x, y) = i(x, y) \u2212 s(x, y), (24) \ud835\udc45(\ud835\udc65, \ud835\udc66) = exp(\ud835\udc5f(\ud835\udc65, \ud835\udc66) \u2212 max \ud835\udc65,\ud835\udc66 (\ud835\udc5f(\ud835\udc65, \ud835\udc66))), (25) where S and R are the shadow image and shadow free image respectively, after reconstruction. Figure 3. (a) shows the actual video frames; column (b) is the modelled background, column (c) corresponds to the foreground modelling. The figures visible are accepted as foreground objects.\nD. Visual Vocabulary Model for Object Recognition\nVisual Vocabulary Model is a machine learning based image classification model, specifically, handling images as documents, by labelling specific features as words. By observing presence of such feature key words in an image, Visual Vocabulary Model can predict the domain of the image and by their arrangement in an image, it can recognize different objects.\nWe have adopted this method as it yields satisfactory results with limited training samples, and the accuracy only increases with the increase in training datasets. In our paper, scene recognition is essential to narrow down the objects that can be present in a scene.\nRecognition using Visual Vocabulary consists of selection of\ndistinct key words and normalization of the surrounding\nregional content. Assigning a descriptor to the normalized content, helps matching the captured objects which are of our\ndomain of interest.\na. Visual keyword localization:\nTo localize the keywords, first step is to extract the features of the object of interest such that they are distinct and invariant under different scale and illumination based conditions even with the presence of noise.\nTo construct the Visual Vocabulary a corpus of training images need to be stored in the feature space in order to model the descriptor instances. Subsequently, the modelled descriptors have to be clustered for quantization of the feature space into distinctive visual words, where the visual words signify the center of the cluster. For given video frames, the closest matching visual key points are recognized with the corresponding features. The bag of words feature sets can be used to define the principle description of any given videoframe. This process can be divided into three steps, namely; tokenization, counting and normalization. In tokenization phase, similar feature patches are labelled via kmean clustering. In the counting phrase, the number of tokens are counted to get an estimate of the scene. Finally, different tokens are assigned with different weightage based on their arrangement, which differentiates between various objects.\nb. Classification:\nExtracted features are classified in order to distinguish the objects of interest (say, human) from any other objects present in the videoframe. We have used Nonlinear (cubic) Support Vector Machine (SVM) as the feature classifier. Cubic SVM bundles consider consecutive triples of words for classification. In our case, cubic SVM performed better than all other types of SVM as well as other classifiers. Polynomial kernel for cubic SVM is: \ud835\udc3e(\ud835\udc65, \ud835\udc66) = (\ud835\udc65\ud835\udc47\ud835\udc66 + \ud835\udc50)3. (26) Here x and y are input vector features, calculated from the training samples. A free parameter, c  0, is indicating how far the equation is from homogeneity.\nc. Detection of feature of test objects:\nIn this section, we have followed the similar approach of bag\nwords to extract the textual distribution as feature of the test\nobjects in the video scenes. During classification process,\nsimilar feature in different objects can lead to uncertainty in feature assignment. Dealing with uncertainty, implicit shape\nmodel has been proposed (ISM). In ISM algorithm, extraction\nof local features and matching it to the Visual Vocabulary is done using soft matching. At the time of validation of our\nclassification process we used the notion of soft computing\nwhich is basically a heuristic interpretation of our matching threshold.\nThe following equation expresses the contribution of a feature f, at location l, at position x in the object class o\ud835\udc5b with matching visual keywords (C\ud835\udc56) indicating its potentiality of belonging to the class o\ud835\udc5b . Thus, we get: \ud835\udc5d(\ud835\udc5c\ud835\udc5b , \ud835\udc65|\ud835\udc53, \ud835\udc59) = \u2211 \ud835\udc56 \ud835\udc5d(\ud835\udc5c\ud835\udc5b , \ud835\udc65|\ud835\udc36\ud835\udc56 , \ud835\udc59) \ud835\udc5d(\ud835\udc36\ud835\udc56|\ud835\udc53), (27) where \ud835\udc5d(\ud835\udc5c\ud835\udc5b , \ud835\udc65|\ud835\udc53, \ud835\udc59) indicates the probability of feature f at frame location l to belong to the class of o\ud835\udc5b at image position x.\nThe weights are populated in continuous 3D weighing region for object position x= (x, y, s). For visual keyword \ud835\udc36\ud835\udc56, the first term of the right-hand side of (27) indicates the stored occurrence distribution, which is weighted by the second term, the probability of feature f, belonging to the exact class of \ud835\udc36\ud835\udc56. Mean-shift mode estimation with a kernel K, along with scale-adaptive kernel, is used to obtain the maxima in this space: \ud835\u0302\udc5d(\ud835\udc5c\ud835\udc5b , \ud835\udc65) = 1\n\ud835\udc49b(\ud835\udc65\ud835\udc60) \u2211 \u2211 \ud835\udc5d(\ud835\udc5c\ud835\udc5b, \ud835\udc65\ud835\udc57|\ud835\udc53\ud835\udc58, \ud835\udc59\ud835\udc58)\ud835\udc3e (\n\ud835\udc65\u2212\ud835\udc65\ud835\udc57\n\ud835\udc4f(\ud835\udc65\ud835\udc60) )\ud835\udc57\ud835\udc58 . (28)\nKernel bandwidth is denoted by b, and volume is denoted by \ud835\udc49\ud835\udc4f, which are varied over the radius of the kernel. In order to fix the hypothesized interest object, size and scale coordinate \ud835\udc99\ud835\udc60 is parallelly updated. This strategy makes it easier to deal with partial occlusions and also typically requires fewer training examples.\nThe pictorial structure model represents any object of interest as collection of parts, connected in pairs, and defined by a graph G = (V, E), where the nodes V = {\ud835\udc631, \u2026 , \ud835\udc63\ud835\udc5b} defines the parts and the edges (\ud835\udc63\ud835\udc56 , \ud835\udc63\ud835\udc57) \ud835\udf16 \ud835\udc38 describes the corresponding connections. L = {\ud835\udc591, \u2026 , \ud835\udc59\ud835\udc5b} be a certain arrangement of part frame locations. Then the matching of the model to a video frame is formulated using an energy minimization function:\n\ud835\udc3f\u2217 = argmin \ud835\udc3f (\u2211 \ud835\udc5a\ud835\udc56(\ud835\udc59\ud835\udc56) \ud835\udc5b \ud835\udc56=1 + \u2211 \ud835\udc51\ud835\udc56,\ud835\udc57(\ud835\udc59\ud835\udc56 , \ud835\udc59\ud835\udc57)(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57)\u2208\ud835\udc38 ). (29)\nThe matching cost is \ud835\udc5a\ud835\udc56(\ud835\udc59\ud835\udc56) at location \ud835\udc59\ud835\udc56, with the placing part \ud835\udc63\ud835\udc56 and deformation cost between two corresponding part locations represented by \ud835\udc51\ud835\udc56,\ud835\udc57(\ud835\udc59\ud835\udc56 , \ud835\udc59\ud835\udc57): \ud835\udc51\ud835\udc56,\ud835\udc57(\ud835\udc59\ud835\udc56 , \ud835\udc59\ud835\udc57) = (\ud835\udc47\ud835\udc56\ud835\udc57(\ud835\udc59\ud835\udc56) \u2212 \ud835\udc47\ud835\udc57\ud835\udc56(\ud835\udc59\ud835\udc57)) \ud835\udc47 \ud835\udc40\ud835\udc56\ud835\udc57 \u22121 (\ud835\udc47\ud835\udc56\ud835\udc57(\ud835\udc59\ud835\udc56) \u2212 \ud835\udc47\ud835\udc57\ud835\udc56(\ud835\udc59\ud835\udc57)), (30)\nwhere \ud835\udc51\ud835\udc56,\ud835\udc57 is Mahalanobis Distance between transformed locations \ud835\udc47\ud835\udc56\ud835\udc57(\ud835\udc59\ud835\udc56) and \ud835\udc47\ud835\udc57\ud835\udc56(\ud835\udc59\ud835\udc57), \ud835\udc40\ud835\udc56\ud835\udc57 being the diagonal covariance. The root node optimum location stated as: \ud835\udc591 \u2217 = argmin\n\ud835\udc591 (\ud835\udc5a1(\ud835\udc591) + \u2211 \ud835\udc37\ud835\udc5a\ud835\udc56(\ud835\udc471\ud835\udc56(\ud835\udc591))\n\ud835\udc5b \ud835\udc56=2 ), (31)\nwhere \ud835\udc37\ud835\udc5a\ud835\udc56(\ud835\udc471\ud835\udc56(\ud835\udc591)) represents a sum of the root node\u2019s response as a distance-transformed version of each child node.\nThus, we get:\n\ud835\udc591 \u2217 = argmin \ud835\udc591 (\ud835\udc5a1(\ud835\udc591) + \u2211 min \ud835\udc59\ud835\udc56 \ud835\udc5b\ud835\udc56=2 \ud835\udc5a\ud835\udc56(\ud835\udc59\ud835\udc56) + ||\ud835\udc59\ud835\udc56 \u2212\n\ud835\udc471\ud835\udc56(\ud835\udc591)||\ud835\udc40\ud835\udc56\ud835\udc57 2 ) . (32)\nIn (Fig. 4), visual word tokenization is explained and furthermore, the visual word occurrence frequency is explained in the following section. It signifies the number of time a particular word vector represents its class of object in varied frames. This is evidently the parameter that decides the characteristics of a particular object.\nTo validate the classification, cross validation approach is applied by randomly splitting the array of objects into train data subsets and test data subsets. Then it creates a confusion matrix for comparing the correct and incorrect classification of features for training of Visual Vocabulary Model. The sum along the principle diagonal of the confusion matrix represents the number of correctly classified objects. The ratio between correctly classified object to the total number of objects to be identified gives the validation accuracy.\nd. Adaption of Discriminative model based on Pyramid Matching Kernel (PMK) approach:\nIn this paper, we have essentially considered discriminative model for tracking the objects with different appearance (Fig. 9). For further improvement of our validation score by approximating the similarity measures, we are modelling a\nlinear time matching function, represented by the Pyramid Match Kernel (PMK) model to bridge the feature sets to the variable cardinalities. Let the input of a histogram pyramid be X \u03f5 S where \u03a8(X) = [\ud835\udc3b0(\ud835\udc4b), \u2026 , \ud835\udc3b\ud835\udc3f\u22121(\ud835\udc7f)], number of pyramid levels expressed as L. The histogram vector of point X is defined by \ud835\udc3b\ud835\udc56(\ud835\udc7f). Similarity between two input set of features Y and Z is expressed as: \ud835\udf05\ud835\udc43\ud835\udc40\ud835\udc3e(\u03a8(\ud835\udc4c),\u03a8(\ud835\udc4d)) = \u2211 \ud835\udf14\ud835\udc56 (\ud835\udc3c(\ud835\udc3b\ud835\udc56(\ud835\udc4c), \ud835\udc3b\ud835\udc56(\ud835\udc4d)) \u2212 \ud835\udc3f\u22121 \ud835\udc56=0\n\ud835\udc3c(\ud835\udc3b\ud835\udc56\u22121(\ud835\udc4c), \ud835\udc3b\ud835\udc56\u22121(\ud835\udc4d))), (33)\nwhere \ud835\udc3c(\ud835\udc3b\ud835\udc56(\ud835\udc4c), \ud835\udc3b\ud835\udc56(\ud835\udc4d))signifies the histogram intersection of two input set of features Y and Z at ith level of the pyramid."}, {"heading": "E. Region of Interest Tracking", "text": "The species inspired PSO framework provides an effective\nway to track multiple object that are detected and recognized from aforementioned method (visual word features). First, for singular object tracking, following analogies need to be assumed:\n\u2022 The groundtruth of an object and surrounding region can\nbe considered as ecological properties.\n\u2022 State space particles correspond to a particular species. \u2022 Each particle\u2019s observation likelihood and fitness\ncapability of a particular species is analogous.\nFor multiple object tracking, these postulates can be easily extended by creating a tracker for each object. These trackers are managed independently. In case of occlusion, support regions of concerning objects may overlap, which implies, the intersectional area between two species are elementary to both. Subsequently, the repulsion and competition among the species arise as both of them aspire to the same resource, the stronger one has higher probability of winning the competition.\nDuring the course of video scene, there may be overlap between two object areas due to occlusion and the related features between them may become ambiguous. To handle this hindrance, we design a multiple-species-based PSO algorithm. The principle idea behind this approach [19], is to divide the groundtruth particles of the object into various species according to the species object numbers and successfully model the relations and the partial visibility among varied species. Detailed description of the species inspired PSO algorithm is briefly described in the following sections.\na. Problem Construction:\nLet us consider, M number of objects, surrounded with N number of particles, constitute a set \ud835\udf12 = { \ud835\udc65\ud835\udc61,\ud835\udc58 \ud835\udc56,\ud835\udc5b , \ud835\udc56 = 1, \u2026 , \ud835\udc41, \ud835\udc58 = 1,\u2026 ,\ud835\udc40}, \ud835\udcde = { \ud835\udc5c\ud835\udc61,\ud835\udc58 \ud835\udc56,\ud835\udc5b, \ud835\udc56 = 1, \u2026 , \ud835\udc41, \ud835\udc58 = 1,\u2026 ,\ud835\udc40}. Here t is the 2- D translation parameter. Formula of multiple object tracking is as follows: \ud835\udf12\u2217 = argmax \ud835\udc56 \ud835\udc5d(\ud835\udcde|\ud835\udf4c) (34)\nBy independently maximizing of the individual observation likelihood, the above optimization may be simplified, in case of no occlusion.\nIn case of no occlusion, the above optimization may be simplified by maximizing the individual observation likelihood independently (here, we drop the superscript i, n for simplicity):\n\ud835\udc65\ud835\udc61,\ud835\udc58 \u2217 = argmax\n\ud835\udc65\ud835\udc61,\ud835\udc58 \ud835\udc5d(\ud835\udc5c\ud835\udc61,\ud835\udc58|\ud835\udc65\ud835\udc61,\ud835\udc58), \ud835\udc58 = 1, \u2026 ,\ud835\udc40 (35)\nb. Competition Model:\nWhen different object obscure one another, there is an\noverlap between corresponding support regions. In these\ncircumstances, the competition between two objects elevates to subjugate the overlapping part (Fig. 5). In order to effectively\ndesign the competition phenomenon, the visual problem needs to be merged with the competition process.\nTo evaluate the fitness value on the overlapping part as the competition ability, the overlapping part is viewed at a whole and projected onto the learned subspace corresponding to each object. We define the power of each object or species in following manner: \ud835\udc5d\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc58 = \ud835\udc5d(\ud835\u0302\udc5c\ud835\udc61,\ud835\udc58|\ud835\udc65\ud835\udc61,\ud835\udc58) = \ud835\udc52\ud835\udc65\ud835\udc5d (\u2212\u2016\ud835\u0302\udc5c\ud835\udc61,\ud835\udc58 \u2212 \ud835\u0302\udc48\ud835\udc58\ud835\u0302\udc48\ud835\udc58 \ud835\udc47\ud835\u0302\udc5c\ud835\udc61,\ud835\udc58\u2016 2 ), (36)\nwhere k and \ud835\u0302\udc48\ud835\udc58 are the overlapping part of the object and its corresponding subspace respectively. In a similar way, the interactive likelihood of object \ud835\udc581 over the overlapping regions can be calculated: \ud835\udc5d(\ud835\u0302\udc5c\ud835\udc61,\ud835\udc581|\ud835\udc65\ud835\udc61,\ud835\udc581 , \ud835\udc65\ud835\udc61,\ud835\udc582)\u23df = \ud835\udc5d\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc581\n\u2211 \ud835\udc5d\ud835\udc5c\ud835\udc64\ud835\udc52\ud835\udc5f\ud835\udc58\ud835\udc56\ud835\udc56=1,2 . (37)\n\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc63\ud835\udc52 \ud835\udc59\ud835\udc56\ud835\udc58\ud835\udc52\ud835\udc59\ud835\udc56\u210e\ud835\udc5c\ud835\udc5c\ud835\udc51\nThe mutual likelihood of each species describes the competition ability. Higher the competition ability of a species more like it is to win the competition. It means that the species which won the competition is more likely to be of the object that was occluding the other object species involved.\nc. Annealed Gaussian Based PSO (AGPSO)\nAn annealed Gaussian based PSO algorithm [21] is considered in this paper, as in conventional PSO requires careful and fine tuning of various parameters. In this algorithm, the particles and corresponding velocities are updated in below mentioned manner: \ud835\udc63\ud835\udc56,\ud835\udc5b+1 = |\ud835\udc5f1|(\ud835\udc5d \ud835\udc56 \u2212 \ud835\udc65\ud835\udc56,\ud835\udc5b) + |\ud835\udc5f2|(\ud835\udc54 \u2212 \ud835\udc65 \ud835\udc56,\ud835\udc5b) + \ud835\udf16 (38)\n\ud835\udc65\ud835\udc56,\ud835\udc5b+\ud835\udc56 = \ud835\udc65\ud835\udc56,\ud835\udc5b + \ud835\udc63\ud835\udc56,\ud835\udc5b+1, (39) where |\ud835\udc5f1| and |\ud835\udc5f2| being the absolute values of the samples from Gaussian probability distribution N (0, 1). This is zeromean Gaussian disturbance that stops the algorithm from getting trapped in local optima. With the help of adaptive simulated annealing, the covariance matrix of \ud835\udf16 is changed [34]: \u2211 =\ud835\udf16 \u2211\ud835\udc52\n\u2212\ud835\udc50\ud835\udc5b. (40) Here, a transition distribution is predefined and \u03a3 is its covariance matrix, annealing constant c, and iteration number n. The components in \u03a3 decrease in proportion to the iteration number which results in a fast rate of convergence. When \ud835\udc581 and \ud835\udc582 occlude each other at time t, a repulsion force is added to the evolution process of particles, and subsequently the iteration step for \ud835\udc581 becomes as follows:\n\ud835\udc63\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b+1 = |\ud835\udc5f1|(\ud835\udc5d\ud835\udc61,\ud835\udc581 \ud835\udc56 \u2212 \ud835\udc65\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b ) + |\ud835\udc5f2|(\ud835\udc54\ud835\udc61,\ud835\udc581 \u2212 \ud835\udc65\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b ) +\n|\ud835\udc5f3|\ud835\udc39\ud835\udc582,\ud835\udc581\u20d7\u20d7 \u20d7\u20d7 \u20d7\u20d7 \u20d7\u20d7 \u20d7\u20d7  \u20d7 + \ud835\udf16 (41) \ud835\udc65\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b+1 = \ud835\udc65\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b + \ud835\udc63\ud835\udc61,\ud835\udc581 \ud835\udc56,\ud835\udc5b+1 , (42)\nwhere the parameter \ud835\udc5f3 is Gaussian random number sampling from N (0, 1). The third term on the right-hand side of the above equation depicts the shared effect between object \ud835\udc582 and \ud835\udc581. In other words, the competition phenomenon on the observation level has been modelled in this paper. Also, the competition model of state space has been modelled to drive the evolution process of the species in the right direction.\nd. Updating of the Appearance Model Selectively\nIn most of the tracking algorithms [24], [29], appearance models are not updated during occlusion. However, the appearance of the object under occlusion may change, and that can cause the tracker to fail to recapture the object appearance after it is not occluded anymore. A selective updating algorithm is implemented to cope with the appearance changes during occlusion: pixels belonging to the visual part of the objects are cumulatively updated in the normal way and pixels that are part of the overlapping region (Fig. 5) are projected onto the subsequent subspace of each object. Then the errors due to the reconstruction are calculated. If this error is smaller than a predefined threshold for pixels inside the overlapping area, then it is again updated in the subsequent subspace.\nDue to this careful modelling of the updating strategy, the appearance changes can be easily accommodated, allowing more persistent tracking throughout the video stream."}, {"heading": "IV. EXPERIMENTAL RESULTS AND ANALYSIS", "text": "Proposed method is evaluated on benchmark datasets and compared it with the existing state-of-the art tracking algorithms. Brief description of the experimental datasets is shown followed by the experimental parameter settings and analysis.\nAll the tests were done on an Intel 5th Gen core i7, 2.10 GHz\nprocessor with 6 Gigabytes of RAM and 2 Gigabytes NVDIA GeForce GPU and the algorithm was implemented using\nMATLAB\u201916 development tool."}, {"heading": "A. Experimental Datasets", "text": "We evaluate the proposed algorithm on benchmark TB100 sequences, namely, iLIDS [4], VIVID [46], Walking2 [50] and Woman [40]. In the following section, we discuss the various attributes of the aforementioned datasets.\nThe primary challenges in the iLIDS dataset [4] (imagery Library for Intelligent Detection Systems) is the Scale Variation (SV), In Plane Rotation (IPR), Occlusion (OCC), Low Resolution (LR) and Illumination Variation (IV). This video consists of a total number of four people walking to, from and across the camera view, with the camera mounted at an isometric angle, where only one man carries a trolley. The video is 10 minutes long, at 25 frames per second, with each frame having a dimension of 720x570 pixels, which we have scaled down to 180x144 for low resolution analysis.\nThe woman dataset [40] is much more challenging, as one has to handle Illumination variation, Scale variation, Occlusion, Deformation (DEF), Motion Blur (MB), Fast Motion (FM) and Out Plane Rotation (OPR). It has 597 video frames of 352x288 resolution. Here the camera view follows a woman walking past several cars.\nThe VIVID dataset [46] is part of DARPA VIVID program consisting 9 video sequences. Here we deal with one of those nine videos, namely, RedTeam. The video contains challenges such as; Scale Variation, Occlusion, In Plane Rotation, Out Plane Rotation and Low Resolution. This is an aerial footage of a car driving on a straight road then turning a corner at the end forming long shadow cast of the object. Therefore, shadow removal gives a better result in tracking.\nWalking2 dataset [50] contains difficulties like Scale Variation, Occlusion and Low Resolution. The video contains 500 frames, each of dimension 384x288 pixels. The video is of some people walking down the corridor of an office interior. This video is in many ways similar to the iLIDS dataset. In the following section, we demonstrate the accuracy of our algorithm and susceptibility in handling all the challenges offered by these benchmark datasets."}, {"heading": "B. Experimental Setting", "text": "All the aforementioned datasets consist of RGB color channels, which are more memory extensive for image processing. So, we temporarily convert them to grayscale using the equation: gray = 0.299R + 0.587G + 0.114B.\nFurthermore, we update the background model with the process mentioned in (14) (15) and (16) of our proposed method.\nThen we process each frame with their original RGB color channels and reconstruct a new image by feeding only the normalized R and G color channel values. This normalized image removes the shadow component from the image, which essentially, helps in accurate tracking. After this step, we convert this image into a binary image through background subtraction [Figure 6.(b)]. We dilate the image by employing some morphological operations. However, this image does not preserve the edges satisfactorily enough. Thus, we extract the binary image with edges intact, by simple background subtraction [Figure 6. (c)]. Then we pointwise multiply these images to obtain the complete shadow free image [Figure6. (d)] with edges preserved.\nFor training, we consider the domain with train data sets and we use Visual Vocabulary features. We\u2019re extracting key points and calculating the probable occurrences in the subsequent frames (Fig. 7). Continuing with this process, we extract all the key points of certain domain oriented objects from different angle-posture-view high definition picture datasets for better accuracy.\nVisual Vocabulary using Cubic SVM based classification covers two other parameters, namely domain detection of the scene and object identification for further video frames, which are independent to camera axis orientation, camera background relationship and surroundings. It provides recognition accuracy of roughly 93.3%.\nThen we use the extracted and trained features to the Species inspired PSO for accurate and content aware tracking. Following this process, as we can see in (Fig. 8), the unattended luggage is also recognized. Here the luggage is a rigid object which remains stationary for most of the video and when it is moved by any human, it forms a connected blob with that human. Normally, in such cases, PSO cannot track successfully, but because of the feature driven input to the Species inspired PSO, we get the results shown below."}, {"heading": "C. Analysis and Evaluation", "text": "We test our algorithm on the dataset mentioned above with the aforementioned setting. The confusion matrix (Fig. 10) shows that we have reached up to 85.33% accuracy. Here, we consider INRIA Person Dataset for training of our Visual Vocabulary Model from various angle and postures for accurate detection and recognition, as portrayed in Fig. 9.\nMore training always leads to higher accuracy. Our training data here consisted of partial photos of train that had almost similar dimension like cars. That is why our classifier has maximum confusion in this situation, as reflected by the confusion matrix.\nOther plots like, ROC curve and scatter point data show the classifier performance and the plot between True Positive Rate and False Positive Rate. Prediction model curve shows the structure containing a classification object and a function for prediction. This structure allows to make predictions for data models that include principal component analysis (PCA).\nApplying the extracted features to the test datasets, using Visual Key points, prediction of new objects of interest in the video sequence is done. Subsequently, using the trained model as a reference to recognize newly arrived objects, as shown in the (Fig. 11), with a validation accuracy of roughly 93.13%.\nTrain models using machine learning learners are applied in the video sequences and the algorithm predicts with recognition, the objects of interests, present in the consequent video frames, as stated above.\nDomain recognition of the test sequence also being predicted by the probability distribution of presence of objects [i.e. Visual Key Points] in the scene, as shown in the [Fig. 12]\nNow, we compare the detection accuracy of our algorithm with different benchmark methods as shown in Table 1, 2, 3, 4.\nNext, a comparative analysis of the Frame Per Second (FPS), provided in [5], is demonstrated in Table 5. We implement our algorithm on the VIVID dataset [46], and compare it with various state-of-the-art methods to obtain the necessary data for Table 5.\nIn all these cases, our algorithm performs competitively better than all the popular existing approaches. This is due to the fact that other algorithms are effective to deal with certain challenges offered by the datasets, but they are not susceptible enough to cope up with all the challenges of datasets, as stated earlier in section IV (A). Table 6 represent the comparative performance of different trackers with our proposed algorithm. In Table 6, each entry has a numerator term which represents tracking score and the denominator term represents the false positive in tracking.\nA careful inspection of Table. 6 reveals the fact that in all the cases our algorithm performs better than the existing ones. Fig. 14 graphically depicts the above stated fact about the performance measure which indicates the proposed algorithm\u2019s flexibility in adapting with real-life challenges.\nFig. 13 further shows a comparative study on the performance of our algorithm with respect to tracking accuracy and the size of Visual Vocabulary Model. Thus, the\nproposed algorithm performs very satisfactorily over different challenging attributes of different images and illumination variations of the video frames.\nAs it can be seen in (Fig. 14), our algorithm performs best on Walking2 datasets, which is expected because of fewer number of challenging attributes. It performs well on iLIDS and VIVID datasets despite the low-resolution video frames.\nTable 6. Performance of different algorithms on different attributes\nTrackers\nFeatures\nASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed\nmethod\nScale Variation (SV) 54.0\n/ 3.9\n47.9 / 5.9\n47.1 / 5.3\n44.5 / 6.5\n44.8/5.7 48.5\n/ 5.3\n58.0/4.1\nIn Plane Rotation (IPR)\n52.1 / 4.1\n50.7 / 5.1\n46.4 / 5.3\n45.7 / 5.9\n43.7/5.9 47.1\n/ 5.5\n50.7/3.9\nOcclusion (OCC) 56.0\n/ 3.8\n52.7 / 5.1\n49.3 / 5.1\n47.6 / 5.8\n47.4/5.5 51.2\n/ 5.1\n60.9/1.6\nIllumination Variation (IV)\n59.6 / 3.0\n53.0 / 4.7\n51.2 / 4.8\n47.1 / 5.6\n47.8/5.8 51.4\n/ 5.2\n59.9/2.7"}, {"heading": "V. CONCLUSION", "text": "This paper presents object detection, recognition of the detected objects based on Visual Vocabulary Model and tracking of the recognized objects using Species inspired PSO. We train different objects separately in several images with multiple aspects and camera viewpoints to find the best key word points for recognition. Subsequently, we verify the extracted features of the train images. These key word points are applied to the regions based on visual feature point analysis. The comparative analysis is done using visual key word points. We present similarity measures using PMK approach [Section IV, D(d)] for feature matching. The object is satisfactorily detected. After detection of the object, the recognition of the specific object of our interest is done in section IV (D). Finally, the features of the recognized objects are tracked by the Species inspired PSO, which can also efficiently handle the tracking under partial occlusions as shown in Fig 8. The performance measure of the proposed algorithm is done with respect to available benchmark data [4, 46, 50, 40] and we obtain very satisfactory and competitive results. In future, we have a plan to modify the detection and recognition scheme based on the theory of SP (Simplicity & Power) Intelligence."}], "references": [{"title": "Track and Segment: An Iterative Unsupervised Approach for Video Object Proposals", "author": ["F. Xiao", "Y.J. Lee"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Finding local leaf vein patterns for legume characterization and classification", "author": ["M.G. Larese", "P.M. Granitto"], "venue": "Machine Vision and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A Method for Detecting Long Term Left Baggage Based on Heat Map", "author": ["P. Foggia", "A. Greco", "A Saggese", "M. Vento"], "venue": "VISAPP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Object Tracking Benchmark", "author": ["Y. Wu", "J. Lim", "M. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visual tracking via probability continuous outlier model", "author": ["D. Wang", "H. Lu"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014, pp. 3478\u20133485.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual tracking via locality sensitive histograms", "author": ["S. He", "Q. Yang", "R.W.H. Lau", "J. Wang", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 2427\u2013 2434.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Least soft-thresold squares tracking", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 2371\u20132378.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "Proc. 12th Eur. Conf. Comput. Vis., 2012, pp. 864\u2013877.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust tracking via weakly supervised ranking SVM", "author": ["Y. Bai", "M. Tang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1854\u20131861.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1822\u2013 1829.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Distribution fields for tracking", "author": ["L. Sevilla-Lara", "E. Learned-Miller"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1910\u20131917.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Real time robust l1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "Proc.  Fig. 14. Accuracy vs Visual Vocabulary of the proposed algorithm, plotted on 4 datasets.  12 IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1830\u2013 1837.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally orderless tracking", "author": ["S. Oron", "A. Bar-Hillel", "D. Levi", "S. Avidan"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1940\u20131947.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 2042\u20132049.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Online robust image alignment via iterative convex optimization", "author": ["Y. Wu", "B. Shen", "H. Ling"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1808\u20131814.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 7, pp. 1619\u20131632, Aug. 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking using compressive sensing", "author": ["H. Li", "C. Shen", "Q. Shi"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H.S. Torr"], "venue": "Proc. IEEE Int. Conf. Comput. Vis., 2011, pp. 263\u2013270.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["X. Mei", "H. Ling"], "venue": "PAMI 33,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Context tracker: Exploring supporters and distracters in unconstrained environments", "author": ["T.B. Dinh", "N. Vo", "G. Medioni"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 1177\u2013 1184.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust tracking using local sparse appearance model and K-selection", "author": ["B. Liu", "J. Huang", "L. Yang", "C. Kulikowsk"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 1313\u20131320.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking using autoregressive hidden markov model", "author": ["Z. Kalal", "J. Matas", "K. Mikolajczyk", "\u201cP-N Learning: Bootstrapping binary classifiers by structural constraints", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2010", "pp. 49\u201356. [26] X. Mei", "H. Ling", "\u201cRobust visual tracking using l1 minimization", "\u201d in Proc. 12th IEEE Int. Conf. Comput. Vis.", "2009", "pp. 1436\u20131443. [27] S. Stalder", "H. Grabner", "L. van Gool", "\u201cBeyond semisupervised tracking: Tracking should be as simple as detection", "but not simpler than recognition", "\u201d in Proc. 12th IEEE Int. Conf. Comput. Vis. Workshop", "2009", "pp. 1409\u20131416. [28] P. Dolla \u0301r", "B. Babenko", "S. Belongie", "P. Perona", "Z. Tu", "\u201cMultiple Component Learning for Object Detection", "\u201d Proc. European Conf. Computer Vision", "2008. [29] P. Felzenszwalb", "D. McAllester", "D. Ramanan", "\u201cA Discriminatively Trained", "Multiscale", "Deformable Part Model", "\u201d Proc. IEEE Conf. Computer Vision", "Pattern Recognition", "H. 2008. [30] Grabner", "C. Leistner", "Bischof", "D.H.: Semi-supervised On-Line Boosting for Robust Tracking. In: Forsyth", "P. Torr", "Zisserman", "A. (eds.) ECCV 2008", "Part I. LNCS", "vol. 5302", "pp. 234\u2013247. Springer", "Heidelberg (2008) [31] G. Li", "Y. Wang", "W. Shu", "Real-time moving object detection for video monitoring systems", "International Symposium on Intelligent Information Technology Application", "2008. [32] D. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang", "\u201cIncremental learning for robust visual tracking", "\u201d Int. J. Comput. Vis.", "vol. 77", "no. 1", "pp. 125\u2013141", "2008. [33] S. Avidan", "\u201cEnsemble tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 29", "no. 2", "pp. 261\u2013271", "Feb. 2007. [34] F. Tang", "S. Brennan", "Q. Zhao", "H. Tao", "\u201cCo-Tracking using semi- supervised support vector machines", "\u201d in Proc. 11th IEEE Conf. Comput. Vis. Pattern Recognit.", "2007", "pp. 1\u20138. [35] A. Adam", "E. Rivlin", "I. Shimshoni", "\u201cRobust Fragments- Based Tracking Using the Integral Histogram", "\u201d Proc. IEEE Conf. Computer Vision", "Pattern Recognition", "vol. 1", "pp. 798-805", "2006. [36] V. Lepetit", "P. Fua", "\u201cKeypoint Recognition Using Randomized Trees", "\u201d IEEE Trans. Pattern Analysis", "Machine Intelligence", "vol. 28", "no. 9", "pp. 1465-1479", "Sept. 2006. [37] D. Nister", "H. Stewenius", "\u201cScalable Recognition with a Vocabulary Tree\u201d", "Department of Computer Science", "University of Kentucky", "2006. [38] H. Grabner", "M. Grabner", "H. Bischof", "\u201cReal-time tracking via on-line boosting", "\u201d in Proc. British Mach. Vis. Conf.", "2006", "pp. 6.1\u2013 6.10. [39] O. Tuzel", "F. Porikli", "P. Meer", "\u201cRegion covariance: A fast descriptor for detection", "classification", "\u201d in Proc. 9th Eur. Conf. Comput. Vis.", "2006", "pp. 589\u2013600. [40] A. Adam", "E. Rivlin", "I. Shimshoni", "\u201cRobust fragmentsbased tracking using the integral histogram", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2006", "pp. 798\u2013805. [41] L. Xu", "F. Qi", "R. Jiang", "\"Shadow Removal from a Single Image\"", "Proc. IEEE Int'l Conf. Intelligent Systems Design", "Applications", "pp. 1049-1054", "2006. [42] N. Dalal", "B. Triggs", "\u201cHistograms of oriented gradients for human detection", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 886\u2013893. [43] R.T. Collins", "Y. Liu", "M. Leordeanu", "\u201cOnline selection of discriminative tracking features", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 27", "no. 10", "pp. 1631\u20131643", "Oct. 2005. [44] F. Porikli", "\u201cIntegral histogram: A fast way to extract histograms in cartesian spaces", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 829\u2013836. [45] S.T. Birchfield", "S. Rangarajan", "\u201cSpatiograms versus histograms for region-based tracking", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2005", "pp. 1158\u20131163. [46] R. Collins", "X. Zhou", "S.K. Teh", "\u201cAn open source tracking testbed", "evaluation web site", "\u201d in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance", "2005", "pp. 17\u2013 24. [47] P. Viola", "M.J. Jones", "\u201cRobust real-time face detection", "\u201d Int. J. Comput. Vis.", "vol. 57", "no. 2", "pp. 137\u2013154", "2004. [49] S. Avidan", "\u201cSupport vector tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 26", "no. 8", "pp. 1064\u20131072", "Aug. 2004. [50] R.B. Fisher", "\u201cThe PETS04 surveillance ground-truth data sets", "\u201d in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance", "2004", "A. pp. 1\u20135. [51] Jepson", "D. Fleet", "Maraghi", "T.: Robust online appearance models for visual tracking. PAMI 25", "1296\u20131311 (2003) [52] D. Comaniciu", "V. Ramesh", "P. Meer", "\u201cKernel-based object tracking", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 25", "no. 5", "pp. 564\u2013577", "May 2003. [53] R.T. Collins", "\u201cMean-shift blob tracking through scale space", "\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.", "2003", "pp. 234\u2013240.  13 [54] T. Ojala", "M. Pietik\u00e4inen", "T. M\u00e4enp\u00e4\u00e4", "\u201cMultiresolution grayscale", "rotation invariant texture classification with local binary patterns", "\u201d IEEE Trans. Pattern Anal. Mach. Intell.", "vol. 24", "no. 7", "pp. 971\u2013987", "Jul. 2002. [55] P. P_erez", "C. Hue", "J. Vermaak", "M. Gangnet", "\u201cColorbased probabilistic tracking", "\u201d in Proc. 7th Eur. Conf. Comput. Vis.", "2002", "pp. 661\u2013675 [56] M. Isard", "J. Maccormick", "\u201cBramble: A Bayesian Multiple-Blob Tracker", "\u201d Proc. IEEE Int\u2019l Conf. Computer Vision", "vol. 2", "pp. 34-41", "2001. [57] M. Isard", "A. Blake", "\u201cCondensation-Conditional density propagation for visual tracking", "\u201d Int. J. Comput. Vis.", "vol. 29", "no. 1", "pp. 5\u201328", "1998. [58] D. Riahi", "G.-A. Bilodeau", "\u201cMultiple object tracking based on sparse generative appearance modeling", "\u201d in Image Processing (ICIP)", "2015 IEEE International Conference on. IEEE", "2015", "pp. 4017\u20134021. [59] Chenglong Bao", "Yi Wu", "Haibin Ling", "Hui Ji", "\u201cReal time robust l1 tracker using accelerated proximal gradient approach", "\u201d in CVPR. IEEE", "2012", "pp. 1830\u20131837. [60] C. Stauffer", "W.E.L. Grimson", "\"Adaptive Background Mixture Models for Real-Time Tracking\"", "Proc. Computer Vision", "Pattern Recognition 1999 (CVPR '99)", "1999-June [62] S. McKenna", "Y. Raja", "S. Gong", "\"Tracking Colour Objects Using Adaptive Mixture Models\"", "Image", "Vision Computing J.", "vol. 17", "pp. 223-229", "1999. [63] C. Bishop", "Neural Networks for Pattern Recognition", "Oxford University Press", "New York", "1995. [64] B.S. Everitt", "D.J. Hand", "Finite Mixture Distributions", "Chapman", "Hall", "New York", "1981. [65] G.J. McLachlan", "K.E. Basford", "Mixture Models: Inference", "Applications to Clustering", "Marcel Dekker Inc.", "New York", "1988. [66] C.E. Priebe", "Adaptive mixtures", "Journal of the American Statistics Association 89 (427) (1994) 796\u2013806. [67] C.E.Priebe", "D.J. Maxchette", "Adaptive mixtures: recursive nonparametric pattern recognition", "Pattern Recognition 24 (12) (1991) 1197\u20131209. [68] C.E. Priebe", "D.J. Marchette", "Adaptive mixture density estimation", "Pattern Recognition 26 (5) (1993) 771\u2013785. [69] D. Park", "J. Kwon", "K. Lee"], "venue": "CVPR, pp. 1964\u20131971, IEEE, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1964}], "referenceMentions": [{"referenceID": 15, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 16, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 18, "context": "In recent years, several attentions [51, 49, 43, 38, 32, 30, 17, 18, 20] have been given in this direction to achieve and share the goal of this paper.", "startOffset": 36, "endOffset": 72}, {"referenceID": 18, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 84, "endOffset": 100}, {"referenceID": 16, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 84, "endOffset": 100}, {"referenceID": 15, "context": "Generally, appearance based tracking algorithms are of two types mainly; generative [51, 32, 20, 18] and discriminative [49, 43, 38, 30, 17].", "startOffset": 120, "endOffset": 140}, {"referenceID": 5, "context": "Several tracking algorithms based on static appearance models exists, which are either trained using only the first few consecutive set of iterations or defined manually [56], [36], [7], [35].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "[7] developed a locality sensitive histogram at each pixel for finer distribution of the visual feature points for object tracking in video scenes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Local binary patterns (LBP) [54] as well as Haar-like features [47] have been proposed for appearance based tracking of objects [17], [38], [19], [9].", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "Recently pixel based segmentations have been applied [2] to handle tracking.", "startOffset": 53, "endOffset": 56}, {"referenceID": 16, "context": "[18], semi-boosting [30], support vector machine (SVM) [49], boosting [38], structured output SVM [19], and online multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18], semi-boosting [30], support vector machine (SVM) [49], boosting [38], structured output SVM [19], and online multi-", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "instance boosting [17].", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": ", MIL [17], OAB [38], IVT", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[32], L1 [26], TLD [23] and likes.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "The principle idea behind this approach [19], is to divide the groundtruth particles of the object into various species according to the species object numbers and successfully model the relations and the partial visibility among varied species.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "Annealed Gaussian Based PSO (AGPSO) An annealed Gaussian based PSO algorithm [21] is considered in this paper, as in conventional PSO requires careful and fine tuning of various parameters.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "We evaluate the proposed algorithm on benchmark TB100 sequences, namely, iLIDS [4], VIVID [46], Walking2 [50] and Woman [40].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "The primary challenges in the iLIDS dataset [4] (imagery Library for Intelligent Detection Systems) is the Scale Variation (SV), In Plane Rotation (IPR), Occlusion (OCC), Low Resolution (LR) and Illumination Variation (IV).", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Next, a comparative analysis of the Frame Per Second (FPS), provided in [5], is demonstrated in Table 5.", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "Detection Accuracy on iLIDS Dataset [4]", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "ASLA [11] 2012 79.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "24% DFT [12] 2012 81.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "36% MIL [17] 2009 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "29% PCOM [6] 2014 79.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "87% LSS [8] 2013 78.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "ASLA [11] 2012 86.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "3% DFT [12] 2012 85.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "9% MIL [17] 2009 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "7% LSS [8] 2013 87.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "ASLA [11] 2012 88.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "2% DFT [12] 2012 87.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "8% MIL [17] 2009 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 88.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "3% LSS [8] 2013 90.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "ASLA [11] 2012 87.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "8% DFT [12] 2012 88.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "9% MIL [17] 2009 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "2% PCOM [6] 2014 88.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "4% LSS [8] 2013 89.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Trackers Features ASLA [11] DFT [12] IVT [32] MIL [17] PCOM [6] LSS [8] Proposed method Scale Variation (SV) 54.", "startOffset": 68, "endOffset": 71}], "year": 2017, "abstractText": "In this paper, we address the basic problem of recognizing moving objects in video images using Visual Vocabulary model and Bag of Words and track our object of interest in the subsequent video frames using species inspired PSO. Initially, the shadow free images are obtained by background modelling followed by foreground modeling to extract the blobs of our object of interest. Subsequently, we train a cubic SVM with human body datasets in accordance with our domain of interest for recognition and tracking. During training, using the principle of Bag of Words we extract necessary features of certain domains and objects for classification. Subsequently, matching these feature sets with those of the extracted object blobs that are obtained by subtracting the shadow free background from the foreground, we detect successfully our object of interest from the test domain. The performance of the classification by cubic SVM is satisfactorily represented by confusion matrix and ROC curve reflecting the accuracy of each module. After classification, our object of interest is tracked in the test domain using species inspired PSO. By combining the adaptive learning tools with the efficient classification of description, we achieve optimum accuracy in recognition of the moving objects. We evaluate our algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative analysis of our algorithm against the existing state-of-the-art trackers shows very satisfactory and competitive results.", "creator": "Microsoft\u00ae Word 2016"}}}