{"id": "1707.06480", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jul-2017", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones", "abstract": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.\n\n\n\nThe word-level RNN learning method of quantifying word-level RNN using the term \"reinforcement learning\" is based on data from four languages.\n\nFor example, we have learned that a vocabulary for words that are well understood and understood by most people can be interpreted with only a few examples and thus it is not feasible to infer the overall performance of a word-level RNN training strategy.\nIn other words, we have learned that word-level RNN training techniques can produce better word-level RNN training strategies.\nRNN training strategies\nWe have learnt that the \"best\" word-level RNN training methods outperform the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nWe have learned that a word-level RNN training strategy outperform the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nIn other words, we have learned that a word-level RNN training strategy outperforms the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nWe have learned that a word-level RNN training strategy outperforms the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nIn other words, we have learned that a word-level RNN training strategy outperforms the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nFor example, we have learned that a word-level RNN training strategy outperforms the \"best\" word-level RNN training strategies because they do not perform optimally and by fewer and more people than the ones that are studied.\nTo summarize, the good word-level RNN training strategies outperform the \"best\" word-level RNN training strategies because they do not perform optimally and", "histories": [["v1", "Thu, 20 Jul 2017 12:46:09 GMT  (75kb,D)", "http://arxiv.org/abs/1707.06480v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.NE stat.ML", "authors": ["zhenisbek assylbekov", "rustem takhanov", "bagdat myrzakhmetov", "jonathan washington"], "accepted": true, "id": "1707.06480"}, "pdf": {"name": "1707.06480.pdf", "metadata": {"source": "CRF", "title": "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones", "authors": ["Zhenisbek Assylbekov", "Rustem Takhanov"], "emails": ["zhassylbekov@nu.edu.kz", "rustem.takhanov@nu.edu.kz", "bagdat.myrzakhmetov@nu.edu.kz", "@swarthmore.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017). This is a promising approach, and we propose the following direction related to it: We would like to make sure that in the pursuit of the most fine-grained representations one has not missed possible intermediate ways of segmentation, e.g., by syllables. Syllables, in our opinion, are better supported as linguistic units of language than single characters. In most languages, words can be naturally split into syllables:\nES: el par-la-men-to a-po-yo\u0301 la en-mien-da RU: par-la-ment pod-der-z\u030cal po-prav-ku (EN: the parliament supported the amendment)\nBased on this observation, we attempted to determine whether syllable-aware NLM has any advantages over character-aware NLM. We experimented with a variety of models but could not find any evidence to support this hypothesis: splitting words into syllables does not seem to improve the language modeling quality when compared to splitting into characters. However, there are some positive findings: while our best syllable-aware\nlanguage model achieves performance comparable to the competitive character-aware model, it has 18%\u201333% fewer parameters and is 1.2\u20132.2 times faster to train."}, {"heading": "2 Related Work", "text": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schu\u0308tze, 2015). However, not much work has been done on syllable-level or syllable-aware NLM. Mikolov et al. (2012) show that subword-level language models outperform character-level ones.2 They keep the most frequent words untouched and split all other words into syllable-like units. Our approach differs mainly in the following aspects: we make predictions at the word level, use a more linguistically sound syllabification algorithm, and consider a variety of more advanced neural architectures.\nWe have recently come across a concurrent paper (Vania and Lopez, 2017) where the authors systematically compare different subword units (characters, character trigrams, BPE (Sennrich et al., 2016), morphemes) and different representation models (CNN, Bi-LSTM, summation) on languages with various morphological typology. However, they do not consider syllables, and they experiment with relatively small models on small data sets (0.6M\u20131.4M tokens).\n1Subword-level LMs rely on subword-level inputs and make predictions at the level of subwords; subword-aware LMs also rely on subword-level inputs but make predictions at the level of words.\n2Not to be confused with character-aware ones, see the previous footnote.\nar X\niv :1\n70 7.\n06 48\n0v 1\n[ cs\n.C L\n] 2\n0 Ju\nl 2 01\n7"}, {"heading": "3 Syllable-aware word embeddings", "text": "Let W and S be finite vocabularies of words and syllables respectively. We assume that both words and syllables have already been converted into indices. Let ES \u2208 R|S|\u00d7dS be an embedding matrix for syllables \u2014 i.e., it is a matrix in which the sth row (denoted as s) corresponds to an embedding of the syllable s \u2208 S . Any word w \u2208 W is a sequence of its syllables (s1, s2, . . . , snw), and hence can be represented as a sequence of the corresponding syllable vectors:\n[s1, s2, . . . , snw ]. (1)\nThe question is: How shall we pack the sequence (1) into a single vector x \u2208 RdW to produce a better embedding of the word w?3 In our case \u201cbetter\u201d means \u201cbetter than a character-aware embedding of w via the Char-CNN model of Kim et al. (2016)\u201d. Below we present several viable approaches."}, {"heading": "3.1 Recurrent sequential model (Syl-LSTM)", "text": "Since the syllables are coming in a sequence it is natural to try a recurrent sequential model:\nht = f(st,ht\u22121), h0 = 0, (2)\nwhich converts the sequence of syllable vectors (1) into a sequence of state vectors h1:nw . The last\n3The same question applies to any model that segments words into a sequence of characters or other subword units.\nstate vector hnw is assumed to contain the information on the whole sequence (1), and is therefore used as a word embedding for w. There is a big variety of transformations from which one can choose f in (2); however, a recent thorough evaluation (Jozefowicz et al., 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architectures on almost all tasks, and we decided to use it for our experiments. We will refer to this model as Syl-LSTM."}, {"heading": "3.2 Convolutional model (Syl-CNN)", "text": "Inspired by recent work on character-aware neural language models (Kim et al., 2016) we decided to try this approach (Char-CNN) on syllables. Our case differs mainly in the following two aspects:\n1. The set of syllables S is usually bigger than the set of characters C,4 and also the dimensionality dS of syllable vectors is expected to be greater than the dimensionality dC of character vectors. Both of these factors result in allocating more parameters on syllable embeddings compared to character embeddings. 2. On average a word contains fewer syllables than characters, and therefore we need narrower convolutional filters for syllables. This results in spending fewer parameters per convolution. This means that by varying dS and the maximum width of convolutional filters L we can still fit the parameter budget of Kim et al. (2016) to allow fair comparison of the models.\nLike in Char-CNN, our syllable-aware model, which is referred to as Syl-CNN-[L], utilizes maxpooling and highway layers (Srivastava et al., 2015) to model interactions between the syllables. The dimensionality of a highway layer is denoted by dHW."}, {"heading": "3.3 Linear combinations", "text": "We also considered using linear combinations of syllable-vectors to represent the word embedding:\nx = \u2211nw\nt=1 \u03b1t(st) \u00b7 st. (3)\nThe choice for \u03b1t is motivated mainly by the existing approaches (discussed below) which proved to be successful for other tasks. Syl-Sum: Summing up syllable vectors to get a word vector can be obtained by setting \u03b1t(st) = 1.\n4In languages with alphabetic writing systems.\nThis approach was used by Botha and Blunsom (2014) to combine a word and its morpheme embeddings into a single word vector. Syl-Avg: A simple average of syllable vectors can be obtained by setting \u03b1t(st) = 1/nw. This can be also called a \u201ccontinuous bag of syllables\u201d in an analogy to a CBOW model (Mikolov et al., 2013), where vectors of neighboring words are averaged to get a word embedding of the current word. Syl-Avg-A: We let the weights \u03b1t in (3) be a function of parameters (a1, . . . , an) of the model, which are jointly trained together with other parameters. Here n = maxw{nw} is a maximum word length in syllables. In order to have a weighted average in (3) we apply a softmax normalization:\n\u03b1t = softmax(a)t = exp(at)\u2211n \u03c4=1 exp(a\u03c4 )\n(4)\nSyl-Avg-B: We can let \u03b1t depend on syllables and their positions:\n\u03b1t = \u03b1t(st) = softmax(ast + b)t\nwhere A \u2208 RdS\u00d7n (with elements as,t) is a set of parameters that determine the importance of each syllable type in each (relative) position, b \u2208 Rn is a bias, which is conditioned only on the relative position. This approach is motivated by recent work on using an attention mechanism in the CBOW model (Ling et al., 2015a).\nWe feed the resulting x from (3) into a stack of highway layers to allow interactions between the syllables."}, {"heading": "3.4 Concatenation (Syl-Concat)", "text": "In this model we simply concatenate syllable vectors (1) into a single word vector:\nx = [s1; s2; . . . ; snw ;0;0; . . . ;0\ufe38 \ufe37\ufe37 \ufe38 n\u2212nw ]\nWe zero-pad x so that all word vectors have the same length n \u00b7 dS to allow batch processing, and then we feed x into a stack of highway layers."}, {"heading": "4 Word-level language model", "text": "Once we have word embeddings x1:k for a sequence of words w1:k we can use a word-level RNN language model to produce a sequence of states h1:k and then predict the next word according to the probability distribution\nPr(wk+1|w1:k) = softmax(hkW + b),\nwhere W \u2208 RdLM\u00d7|W|, b \u2208 R|W|, and dLM is the hidden layer size of the RNN. Training the model involves minimizing the negative log-likelihood over the corpus w1:K :\n\u2212 \u2211K\nk=1 log Pr(wk|w1:k\u22121) \u2212\u2192 min (5)\nAs was mentioned in Section 3.1 there is a huge variety of RNN architectures to choose from. The most advanced recurrent neural architectures, at the time of this writing, are recurrent highway networks (Zilly et al., 2017) and a novel model which was obtained through a neural architecture search with reinforcement learning (Zoph and Le, 2017). These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart. However, to make our results directly comparable to those of Kim et al. (2016) we select a two-layer LSTM and regularize it as in Zaremba et al. (2014)."}, {"heading": "5 Experimental Setup", "text": "We search for the best model in two steps: first, we block the word-level LSTM\u2019s architecture and pre-select the three best models under a small parameter budget (5M), and then we tune these three best models\u2019 hyperparameters under a larger budget (20M). Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M. The architectural choices are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three best-performing models from the preselection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: \u2022 dS \u223c U(20, 650),5 \u2022 log(dHW) \u223c U(log(160), log(2000)), \u2022 log(dLM) \u223c U(log(300), log(2000)), with the restriction dS < dLM. The total parameter budget is kept at 20M to allow for easy comparison to the results of Kim et al. (2016). Then these three best models (with their hyperparameters tuned on PTB) are trained and evaluated on small- (DATAS) and medium-sized (DATA-L) data sets in six languages.\n5U(a, b) stands for a uniform distribution over (a, b).\nOptimizaton is performed in almost the same way as in the work of Zaremba et al. (2014). See Appendix B for details. Syllabification: The true syllabification of a word requires its grapheme-to-phoneme conversion and then splitting it into syllables based on some rules. Since these are not always available for lessresourced languages, we decided to utilize Liang\u2019s widely-used hyphenation algorithm (Liang, 1983)."}, {"heading": "6 Results", "text": "The results of the pre-selection are reported in Table 1. All syllable-aware models comfortably outperform the Char-CNN when the budget is limited to 5M parameters. Surprisingly, a pure word-level model,6 LSTM-Word, also beats the character-aware one under such budget. The three best configurations are Syl-Concat, Syl-Sum, and Syl-CNN-3 (hereinafter referred to as Syl-CNN), and tuning their hyperparameters under 20M parameter budget gives the architectures in Table 2. The results of evaluating these three models on small (1M tokens) and medium-sized (17M\u2013 57M tokens) data sets against Char-CNN for different languages are provided in Table 3. The models demonstrate similar performance on small data, but Char-CNN scales significantly better on medium-sized data. From the three syllable-aware models, Syl-Concat looks the most advantageous as it demonstrates stable results and has the least\n6When words are directly embedded into RdW through an embedding matrix EW \u2208 R|W|\u00d7dW .\n7Syl-CNN results on DATA-L are not reported since computational resources were insufficient to run these configurations.\nnumber of parameters. Therefore in what follows we will make a more detailed comparison of SylConcat with Char-CNN. Shared errors: It is interesting to see whether Char-CNN and Syl-Concat are making similar errors. We say that a model gives an error if it assigns a probability less than p\u2217 to a correct word from the test set. Figure 2 shows the percentage of errors which are shared by Syl-Concat and CharCNN depending on the value of p\u2217. We see that the\nvast majority of errors are shared by both models even when p\u2217 is small (0.01). PPL breakdown by token frequency: To find out how Char-CNN outperforms Syl-Concat, we partition the test sets on token frequency, as computed on the training data. We can observe in Figure 3 that, on average, the more frequent the word is, the bigger the advantage of Char-CNN over Syl-Concat. The more Char-CNN sees a word in different contexts, the more it can learn about this word (due to its powerful CNN filters). SylConcat, on the other hand, has limitations \u2013 it cannot see below syllables, which prevents it from extracting the same amount of knowledge about the word.\nPCA of word embeddings: The intrinsic advantage of Char-CNN over Syl-Concat is also supported by the following experiment: We took word embeddings produced by both models on the English PTB, and applied PCA to them.8 Regardless of the threshold percentage of variance to retain, the embeddings from Char-CNN always have more principal components than the embeddings from Syl-Concat (see Table 4). This means that Char-CNN embeds words into higher dimensional space than Syl-Concat, and thus can better distinguish them in different contexts. LSTM limitations: During the hyperparameters tuning we noticed that increasing dS , dHW and dLM from the optimal values (in Table 2) did not result in better performance for Syl-Concat. Could it be due to the limitations of the word-level LSTM (the topmost layer in Fig. 1)? To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5). Moreover, increasing dLM from 439 to 650 did result in better performance for Syl-Concat. Optimization details are given in Appendix B. Comparing syllable and morpheme embeddings: It is interesting to compare morphemes and syllables. We trained Morfessor 2.0 (Creutz and\n8We equalized highway layer sizes dHW in both models to have same dimensions for embeddings. In both cases, word vectors were standardized using the z-score transformation.\nLagus, 2007) in its default configuration on the PTB training data and used it instead of the syllabifier in our models. Interestingly, we got \u22483K unique morphemes, whereas the number of unique syllables was\u22486K. We then trained all our models on PTB under 5M parameter budget, keeping the state size of the word-level LSTM at 300 (as in our pre-selection step for syllable-aware models). The reduction in number of subword types allowed us to give them higher dimensionality dM = 100 (cf. dS = 50).9\nConvolutional (Morph-CNN-3) and additive (Morph-Sum) models performed better than others with test set PPLs 83.0 and 83.9 respectively. Due to limited amount of time, we did not perform a thorough hyperparameter search under 20M budget. Instead, we ran two configurations for MorphCNN-3 and two configurations for Morph-Sum with hyperparameters close to those, which were optimal for Syl-CNN-3 and Syl-Sum correspondingly. All told, our best morpheme-aware model is Morph-Sum with dM = 550, dHW = 1100, dLM = 550, and test set PPL 79.5, which is practically the same as the result of our best syllable-aware model Syl-Concat (79.4). This makes Morph-Sum a notable alternative to CharCNN and Syl-Concat, and we defer its thorough study to future work. Source code: The source code for the models discussed in this paper is available at https://github.com/zh3nis/lstm-syl."}, {"heading": "7 Conclusion", "text": "It seems that syllable-aware language models fail to outperform competitive character-aware ones. However, usage of syllabification can reduce the total number of parameters and increase the training speed, albeit at the expense of languagedependent preprocessing. Morphological segmentation is a noteworthy alternative to syllabification: a simple morpheme-aware model which sums morpheme embeddings looks promising, and its study is deferred to future work.\n9M stands for morphemes."}, {"heading": "A Pre-selection", "text": "In all models with highway layers there are two of them and the non-linear activation of any highway layer is a ReLU. LSTM-Word: dW = 108, dLM = 300. Syl-LSTM: dS = 50, dLM = 300. Syl-CNN-[L]: dS = 50, convolutional filter widths are [1, . . . , L], the corresponding convolutional filter depths are [c\u00b7l]Ll=1, dHW = c\u00b7(1+. . .+ L). We experimented with L = 2, 3, 4. The corresponding values of c are chosen to be 120, 60, 35 to fit the total parameter budget. CNN activation is tanh. Linear combinations: We give higher dimensionality to syllable vectors here (compared to other models) since the resulting word vector will have the same size as syllable vectors (see (3)). dS = 175, dHW = 175 in all models except the Syl-Avg-B, where we have dS = 160, dHW = 160. Syl-Concat: dS = 50, dHW = 300."}, {"heading": "B Optimization", "text": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013). We backpropagate for 70 time steps on DATA-S and for 35 time steps on DATA-L using stochastic gradient descent where the learning rate is initially set to 1.0 and halved if the perplexity does not decrease on the validation set after an epoch. We use batch sizes of 20 for DATA-S and 100 for DATA-L. We train for 50 epochs on DATA-S and for 25 epochs on DATA-L, picking the best-performing model on the validation set. Parameters of the models are randomly initialized uniformly in [\u22120.05, 0.05], except the forget bias of the word-level LSTM, which is initialized to 1. For regularization we use dropout (Srivastava et al., 2014) with probability 0.5 between wordlevel LSTM layers and on the hidden-to-output softmax layer. We clip the norm of the gradients (normalized by minibatch size) at 5. These choices were guided by previous work on wordlevel language modeling with LSTMs (Zaremba et al., 2014).\nTo speed up training on DATA-L we use a sampled softmax (Jean et al., 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al., 2016). Although Kim et al. (2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave\net al., 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014). RHN-based models are optimized as in Zilly et al. (2017), except that we unrolled the networks for 70 time steps in truncated BPTT, and dropout rates were chosen to be as follows: 0.2 for the embedding layer, 0.7 for the input to the gates, 0.7 for the hidden units and 0.2 for the output activations."}, {"heading": "C Sizes and speeds", "text": "On DATA-S, Syl-Concat has 28%\u201333% fewer parameters than Char-CNN, and on DATA-L the reduction is 18%\u201327% (see Fig. 4).\nTraining speeds are provided in the Table 6. Models were implemented in TensorFlow, and were run on NVIDIA Titan X (Pascal)."}, {"heading": "Acknowledgements", "text": "We gratefully acknowledge the NVIDIA Corporation for their donation of the Titan X Pascal GPU used for this research. The work of Bagdat Myrzakhmetov has been funded by the Committee of Science of the Ministry of Education and Science of the Republic of Kazakhstan under the targeted program O.0743 (0115PK02473). The authors would like to thank anonymous reviewers and Aibek Makazhanov for valuable feedback, Makat Tlebaliyev and Dmitriy Polynin for IT support, and Yoon Kim for providing the preprocessed datasets."}], "references": [{"title": "Compositional morphology for word representations and language modelling", "author": ["Jan Botha", "Phil Blunsom."], "venue": "Proceedings of ICML.", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "Strategies for training large vocabulary neural language models", "author": ["Wenlin Chen", "David Grangier", "Michael Auli."], "venue": "Proceedings of ACL.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of HLTNAACL.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus."], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 4(1):3.", "citeRegEx": "Creutz and Lagus.,? 2007", "shortCiteRegEx": "Creutz and Lagus.", "year": 2007}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani."], "venue": "Proceedings of NIPS.", "citeRegEx": "Gal and Ghahramani.,? 2016", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "Efficient softmax approximation for gpus", "author": ["Edouard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou."], "venue": "arXiv preprint arXiv:1609.04309.", "citeRegEx": "Grave et al\\.,? 2016", "shortCiteRegEx": "Grave et al\\.", "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Proceedings of AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Word Hy-phen-a-tion by Com-put-er", "author": ["Franklin Mark Liang."], "venue": "Citeseer.", "citeRegEx": "Liang.,? 1983", "shortCiteRegEx": "Liang.", "year": 1983}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Ling et al\\.,? 2015a", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernocky."], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf).", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio."], "venue": "Proceedings of AISTATS.", "citeRegEx": "Morin and Bengio.,? 2005", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "Proceedings of COLING.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Proceedings of NIPS.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "From characters to words to in between: Do we capture morphology", "author": ["Clara Vania", "Adam Lopez"], "venue": "In Proceedings of ACL", "citeRegEx": "Vania and Lopez.,? \\Q2017\\E", "shortCiteRegEx": "Vania and Lopez.", "year": 2017}, {"title": "Character-word lstm language models", "author": ["Lyan Verwimp", "Joris Pelemans", "Patrick Wambacq"], "venue": "In Proceedings of EACL", "citeRegEx": "Verwimp et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Verwimp et al\\.", "year": 2017}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE, 78(10).", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Recurrent highway networks", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of ICML", "citeRegEx": "Zilly et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2017}, {"title": "Neural architecture search with reinforcement learning", "author": ["Barret Zoph", "Quoc V Le."], "venue": "Proceedings of ICLR.", "citeRegEx": "Zoph and Le.,? 2017", "shortCiteRegEx": "Zoph and Le.", "year": 2017}], "referenceMentions": [{"referenceID": 10, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 13, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 23, "context": "Recent advances in neural language modeling (NLM) are connected with character-aware models (Kim et al., 2016; Ling et al., 2015b; Verwimp et al., 2017).", "startOffset": 92, "endOffset": 152}, {"referenceID": 13, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 10, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 23, "context": "Much research has been done on subword-level and subword-aware1 neural language modeling when subwords are characters (Ling et al., 2015b; Kim et al., 2016; Verwimp et al., 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al.", "startOffset": 118, "endOffset": 178}, {"referenceID": 0, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 18, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 2, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015).", "startOffset": 21, "endOffset": 93}, {"referenceID": 22, "context": "We have recently come across a concurrent paper (Vania and Lopez, 2017) where the authors systematically compare different subword units (characters, character trigrams, BPE (Sennrich et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 19, "context": "We have recently come across a concurrent paper (Vania and Lopez, 2017) where the authors systematically compare different subword units (characters, character trigrams, BPE (Sennrich et al., 2016), morphemes) and different representation models (CNN, Bi-LSTM, summation) on languages with various morphological typology.", "startOffset": 174, "endOffset": 197}, {"referenceID": 0, "context": ", 2017) or morphemes (Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Sch\u00fctze, 2015). However, not much work has been done on syllable-level or syllable-aware NLM. Mikolov et al. (2012) show that subword-level language models outperform character-level ones.", "startOffset": 22, "endOffset": 195}, {"referenceID": 10, "context": "The question is: How shall we pack the sequence (1) into a single vector x \u2208 RdW to produce a better embedding of the word w?3 In our case \u201cbetter\u201d means \u201cbetter than a character-aware embedding of w via the Char-CNN model of Kim et al. (2016)\u201d.", "startOffset": 226, "endOffset": 244}, {"referenceID": 9, "context": "There is a big variety of transformations from which one can choose f in (2); however, a recent thorough evaluation (Jozefowicz et al., 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architectures on almost all tasks, and we decided to use it for our experiments.", "startOffset": 116, "endOffset": 141}, {"referenceID": 7, "context": ", 2015) shows that the LSTM (Hochreiter and Schmidhuber, 1997) with its forget bias initialized to 1 outperforms other popular architectures on almost all tasks, and we decided to use it for our experiments.", "startOffset": 28, "endOffset": 62}, {"referenceID": 10, "context": "Inspired by recent work on character-aware neural language models (Kim et al., 2016) we decided to try this approach (Char-CNN) on syllables.", "startOffset": 66, "endOffset": 84}, {"referenceID": 21, "context": "Like in Char-CNN, our syllable-aware model, which is referred to as Syl-CNN-[L], utilizes maxpooling and highway layers (Srivastava et al., 2015) to model interactions between the syllables.", "startOffset": 120, "endOffset": 145}, {"referenceID": 10, "context": "Inspired by recent work on character-aware neural language models (Kim et al., 2016) we decided to try this approach (Char-CNN) on syllables. Our case differs mainly in the following two aspects: 1. The set of syllables S is usually bigger than the set of characters C,4 and also the dimensionality dS of syllable vectors is expected to be greater than the dimensionality dC of character vectors. Both of these factors result in allocating more parameters on syllable embeddings compared to character embeddings. 2. On average a word contains fewer syllables than characters, and therefore we need narrower convolutional filters for syllables. This results in spending fewer parameters per convolution. This means that by varying dS and the maximum width of convolutional filters L we can still fit the parameter budget of Kim et al. (2016) to allow fair comparison of the models.", "startOffset": 67, "endOffset": 841}, {"referenceID": 15, "context": "This can be also called a \u201ccontinuous bag of syllables\u201d in an analogy to a CBOW model (Mikolov et al., 2013), where vectors of neighboring words are averaged to get a word embedding of the current word.", "startOffset": 86, "endOffset": 108}, {"referenceID": 0, "context": "This approach was used by Botha and Blunsom (2014) to combine a word and its morpheme embeddings into a single word vector.", "startOffset": 26, "endOffset": 51}, {"referenceID": 12, "context": "This approach is motivated by recent work on using an attention mechanism in the CBOW model (Ling et al., 2015a).", "startOffset": 92, "endOffset": 112}, {"referenceID": 26, "context": "The most advanced recurrent neural architectures, at the time of this writing, are recurrent highway networks (Zilly et al., 2017) and a novel model which was obtained through a neural architecture search with reinforcement learning (Zoph and Le, 2017).", "startOffset": 110, "endOffset": 130}, {"referenceID": 27, "context": ", 2017) and a novel model which was obtained through a neural architecture search with reinforcement learning (Zoph and Le, 2017).", "startOffset": 110, "endOffset": 129}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart.", "startOffset": 86, "endOffset": 112}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart. However, to make our results directly comparable to those of Kim et al. (2016) we select a two-layer LSTM and regularize it as in Zaremba et al.", "startOffset": 87, "endOffset": 218}, {"referenceID": 4, "context": "These models can be spiced up with the most recent regularization techniques for RNNs (Gal and Ghahramani, 2016) to reach state-of-theart. However, to make our results directly comparable to those of Kim et al. (2016) we select a two-layer LSTM and regularize it as in Zaremba et al. (2014).", "startOffset": 87, "endOffset": 291}, {"referenceID": 14, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M.", "startOffset": 185, "endOffset": 206}, {"referenceID": 13, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M. The architectural choices are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three best-performing models from the preselection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: \u2022 dS \u223c U(20, 650),5 \u2022 log(dHW) \u223c U(log(160), log(2000)), \u2022 log(dLM) \u223c U(log(300), log(2000)), with the restriction dS < dLM.", "startOffset": 186, "endOffset": 586}, {"referenceID": 13, "context": "Pre-selection: We fix dLM (hidden layer size of the word-level LSTM) at 300 units per layer and run each syllable-aware word embedding method from Section 3 on the English PTB data set (Marcus et al., 1993), keeping the total parameter budget at 5M. The architectural choices are specified in Appendix A. Hyperparameter tuning: The hyperparameters of the three best-performing models from the preselection step are then thoroughly tuned on the same English PTB data through a random search according to the marginal distributions: \u2022 dS \u223c U(20, 650),5 \u2022 log(dHW) \u223c U(log(160), log(2000)), \u2022 log(dLM) \u223c U(log(300), log(2000)), with the restriction dS < dLM.", "startOffset": 186, "endOffset": 623}, {"referenceID": 10, "context": "The total parameter budget is kept at 20M to allow for easy comparison to the results of Kim et al. (2016). Then these three best models (with their hyperparameters tuned on PTB) are trained and evaluated on small- (DATAS) and medium-sized (DATA-L) data sets in six languages.", "startOffset": 89, "endOffset": 107}, {"referenceID": 11, "context": "Since these are not always available for lessresourced languages, we decided to utilize Liang\u2019s widely-used hyphenation algorithm (Liang, 1983).", "startOffset": 130, "endOffset": 143}, {"referenceID": 24, "context": "Optimizaton is performed in almost the same way as in the work of Zaremba et al. (2014). See Appendix B for details.", "startOffset": 66, "endOffset": 88}, {"referenceID": 26, "context": "1)? To find out whether this was the case we replaced the LSTM by a Variational RHN (Zilly et al., 2017), and that resulted in a significant reduction of perplexities on PTB for both Char-CNN and Syl-Concat (Table 5).", "startOffset": 84, "endOffset": 104}, {"referenceID": 24, "context": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013).", "startOffset": 65, "endOffset": 93}, {"referenceID": 6, "context": "LSTM-based models: We perform the training (5) by truncated BPTT (Werbos, 1990; Graves, 2013).", "startOffset": 65, "endOffset": 93}, {"referenceID": 20, "context": "For regularization we use dropout (Srivastava et al., 2014) with probability 0.", "startOffset": 34, "endOffset": 59}, {"referenceID": 25, "context": "These choices were guided by previous work on wordlevel language modeling with LSTMs (Zaremba et al., 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 8, "context": "To speed up training on DATA-L we use a sampled softmax (Jean et al., 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 1, "context": ", 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al., 2016).", "startOffset": 71, "endOffset": 90}, {"referenceID": 17, "context": "(2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al.", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "(2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al., 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 117}, {"referenceID": 0, "context": ", 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014).", "startOffset": 111, "endOffset": 136}, {"referenceID": 0, "context": ", 2015) with the number of samples equal to 20% of the vocabulary size (Chen et al., 2016). Although Kim et al. (2016) used a hierarchical softmax (Morin and Bengio, 2005) for the same purpose, a recent study (Grave et al.", "startOffset": 72, "endOffset": 119}, {"referenceID": 0, "context": ", 2016) shows that it is outperformed by sampled softmax on the Europarl corpus, from which DATA-L was derived (Botha and Blunsom, 2014). RHN-based models are optimized as in Zilly et al. (2017), except that we unrolled the networks for 70 time steps in truncated BPTT, and dropout rates were chosen to be as follows: 0.", "startOffset": 112, "endOffset": 195}], "year": 2017, "abstractText": "Syllabification does not seem to improve word-level RNN language modeling quality when compared to characterbased segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%\u201333% fewer parameters and is trained 1.2\u20132.2 times faster.", "creator": "LaTeX with hyperref package"}}}