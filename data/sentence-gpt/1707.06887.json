{"id": "1707.06887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2017", "title": "A Distributional Perspective on Reinforcement Learning", "abstract": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour.\n\n\nThe main reason for the large number of problems in learning is the need to avoid recurrent training. This makes for a useful starting point in how the value distribution works. For example, the probability distribution is a useful way to estimate the value distribution for every number of problems in a random number set. The number distributions, which are generated as inputs to the reinforcement learning agent, are then transformed into random numbers in a regular manner, with the probability distribution being a normal (the probability distribution) and the probability distribution being a function of the expected probability distributions as input. For example, we can imagine a simple model of a set of values, each of which has its own probability distributions. The expected probability distributions are then transformed into random numbers in a regular manner. The probability distribution is used as input in these cases.\nWe show that, on the basis of two simple parameters, a model can be made that can be made that can represent one of the expected probability distributions. This can be made by the prediction of a random number (e.g. when the probability distribution is a simple number) with probability values. We would also use probability distribution as input.\nIn a similar way, we could create a random number as input in a regular way and give the random number the probability distribution as input in a regular way, in an infinite loop. In this case we can say that a random number can be represented by the random number at random time and then the random number to the desired value in a normal way. This way, as the probability distribution is an infinite loop, we can say that the number distribution is a random number of positive values and the random number to the desired value in a normal way.\nOur model is not a natural function of the expected probability distribution. It is very easy to calculate the probability distribution in a random number (e.g. if a probability distribution has an expected probability distribution then the probability distribution will be the same) and we can define the probability distribution for the set of random number variables in this model. However, the probability distribution can also be made that is not directly relevant to the problem", "histories": [["v1", "Fri, 21 Jul 2017 13:21:54 GMT  (721kb,D)", "http://arxiv.org/abs/1707.06887v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["marc g bellemare", "will dabney", "r\u00e9mi munos"], "accepted": true, "id": "1707.06887"}, "pdf": {"name": "1707.06887.pdf", "metadata": {"source": "META", "title": "A Distributional Perspective on Reinforcement Learning", "authors": ["Marc G. Bellemare", "Will Dabney", "R\u00e9mi Munos"], "emails": ["<bellemare@google.com>."], "sections": [{"heading": "1. Introduction", "text": "One of the major tenets of reinforcement learning states that, when not otherwise constrained in its behaviour, an agent should aim to maximize its expected utility Q, or value (Sutton & Barto, 1998). Bellman\u2019s equation succintly describes this value in terms of the expected reward and expected outcome of the random transition (x, a)\u2192 (X \u2032, A\u2032):\nQ(x, a) = ER(x, a) + \u03b3 EQ(X \u2032, A\u2032).\nIn this paper, we aim to go beyond the notion of value and argue in favour of a distributional perspective on reinforce-\n*Equal contribution 1DeepMind, London, UK. Correspondence to: Marc G. Bellemare <bellemare@google.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nment learning. Specifically, the main object of our study is the random returnZ whose expectation is the valueQ. This random return is also described by a recursive equation, but one of a distributional nature:\nZ(x, a) D = R(x, a) + \u03b3Z(X \u2032, A\u2032).\nThe distributional Bellman equation states that the distribution ofZ is characterized by the interaction of three random variables: the reward R, the next state-action (X \u2032, A\u2032), and its random return Z(X \u2032, A\u2032). By analogy with the wellknown case, we call this quantity the value distribution.\nAlthough the distributional perspective is almost as old as Bellman\u2019s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric uncertainty (Dearden et al., 1998), to design risk-sensitive algorithms (Morimura et al., 2010b;a), or for theoretical analysis (Azar et al., 2012; Lattimore & Hutter, 2012). By contrast, we believe the value distribution has a central role to play in reinforcement learning.\nContraction of the policy evaluation Bellman operator. Basing ourselves on results by Ro\u0308sler (1992) we show that, for a fixed policy, the Bellman operator over value distributions is a contraction in a maximal form of the Wasserstein (also called Kantorovich or Mallows) metric. Our particular choice of metric matters: the same operator is not a contraction in total variation, Kullback-Leibler divergence, or Kolmogorov distance.\nInstability in the control setting. We will demonstrate an instability in the distributional version of Bellman\u2019s optimality equation, in contrast to the policy evaluation case. Specifically, although the optimality operator is a contraction in expected value (matching the usual optimality result), it is not a contraction in any metric over distributions. These results provide evidence in favour of learning algorithms that model the effects of nonstationary policies.\nBetter approximations. From an algorithmic standpoint, there are many benefits to learning an approximate distribution rather than its approximate expectation. The distributional Bellman operator preserves multimodality in value distributions, which we believe leads to more stable learning. Approximating the full distribution also mitigates the effects of learning from a nonstationary policy. As a whole,\nar X\niv :1\n70 7.\n06 88\n7v 1\n[ cs\n.L G\n] 2\n1 Ju\nl 2 01\n7\nwe argue that this approach makes approximate reinforcement learning significantly better behaved.\nWe will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment (Bellemare et al., 2013). By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games. Our results echo those of Veness et al. (2015), who obtained extremely fast learning by predicting Monte Carlo returns.\nFrom a supervised learning perspective, learning the full value distribution might seem obvious: why restrict ourselves to the mean? The main distinction, of course, is that in our setting there are no given targets. Instead, we use Bellman\u2019s equation to make the learning process tractable; we must, as Sutton & Barto (1998) put it, \u201clearn a guess from a guess\u201d. It is our belief that this guesswork ultimately carries more benefits than costs."}, {"heading": "2. Setting", "text": "We consider an agent interacting with an environment in the standard fashion: at each step, the agent selects an action based on its current state, to which the environment responds with a reward and the next state. We model this interaction as a time-homogeneous Markov Decision Process (X ,A, R, P, \u03b3). As usual, X and A are respectively the state and action spaces, P is the transition kernelP (\u00b7 |x, a), \u03b3 \u2208 [0, 1] is the discount factor, and R is the reward function, which in this work we explicitly treat as a random variable. A stationary policy \u03c0 maps each state x \u2208 X to a probability distribution over the action space A."}, {"heading": "2.1. Bellman\u2019s Equations", "text": "The return Z\u03c0 is the sum of discounted rewards along the agent\u2019s trajectory of interactions with the environment. The value function Q\u03c0 of a policy \u03c0 describes the expected return from taking action a \u2208 A from state x \u2208 X , then acting according to \u03c0:\nQ\u03c0(x, a) := EZ\u03c0(x, a) = E [ \u221e\u2211 t=0 \u03b3tR(xt, at) ] , (1) xt \u223c P (\u00b7 |xt\u22121, at\u22121), at \u223c \u03c0(\u00b7 |xt), x0 = x, a0 = a.\nFundamental to reinforcement learning is the use of Bellman\u2019s equation (Bellman, 1957) to describe the value function:\nQ\u03c0(x, a) = ER(x, a) + \u03b3 E P,\u03c0 Q\u03c0(x\u2032, a\u2032).\nIn reinforcement learning we are typically interested in acting so as to maximize the return. The most common ap-\nproach for doing so involves the optimality equation\nQ\u2217(x, a) = ER(x, a) + \u03b3 EP max a\u2032\u2208A Q\u2217(x\u2032, a\u2032).\nThis equation has a unique fixed point Q\u2217, the optimal value function, corresponding to the set of optimal policies \u03a0\u2217 (\u03c0\u2217 is optimal if Ea\u223c\u03c0\u2217 Q\u2217(x, a) = maxaQ\u2217(x, a)).\nWe view value functions as vectors in RX\u00d7A, and the expected reward function as one such vector. In this context, the Bellman operator T \u03c0 and optimality operator T are T \u03c0Q(x, a) := ER(x, a) + \u03b3 E\nP,\u03c0 Q(x\u2032, a\u2032) (2)\nT Q(x, a) := ER(x, a) + \u03b3 EP max a\u2032\u2208A Q(x\u2032, a\u2032). (3)\nThese operators are useful as they describe the expected behaviour of popular learning algorithms such as SARSA and Q-Learning. In particular they are both contraction mappings, and their repeated application to some initialQ0 converges exponentially to Q\u03c0 or Q\u2217, respectively (Bertsekas & Tsitsiklis, 1996)."}, {"heading": "3. The Distributional Bellman Operators", "text": "In this paper we take away the expectations inside Bellman\u2019s equations and consider instead the full distribution of the random variable Z\u03c0 . From here on, we will view Z\u03c0 as a mapping from state-action pairs to distributions over returns, and call it the value distribution.\nOur first aim is to gain an understanding of the theoretical behaviour of the distributional analogues of the Bellman operators, in particular in the less well-understood control setting. The reader strictly interested in the algorithmic contribution may choose to skip this section."}, {"heading": "3.1. Distributional Equations", "text": "It will sometimes be convenient to make use of the probability space (\u2126,F ,Pr). The reader unfamiliar with mea-\nsure theory may think of \u2126 as the space of all possible outcomes of an experiment (Billingsley, 1995). We will write \u2016u\u2016p to denote the Lp norm of a vector u \u2208 RX for 1 \u2264 p \u2264 \u221e; the same applies to vectors in RX\u00d7A. The Lp norm of a random vector U : \u2126 \u2192 RX (or RX\u00d7A) is then \u2016U\u2016p := [ E [ \u2016U(\u03c9)\u2016pp ]]1/p , and for p = \u221e we have \u2016U\u2016\u221e = ess sup \u2016U(\u03c9)\u2016\u221e (we will omit the dependency on \u03c9 \u2208 \u2126 whenever unambiguous). We will denote the c.d.f. of a random variable U by FU (y) := Pr{U \u2264 y}, and its inverse c.d.f. by F\u22121U (q) := inf{y : FU (y) \u2265 q}.\nA distributional equation U D := V indicates that the random variable U is distributed according to the same law as V . Without loss of generality, the reader can understand the two sides of a distributional equation as relating the distributions of two independent random variables. Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al. (2010a) among others, and in operations research by White (1988)."}, {"heading": "3.2. The Wasserstein Metric", "text": "The main tool for our analysis is the Wasserstein metric dp between cumulative distribution functions (see e.g. Bickel & Freedman, 1981, where it is called the Mallows metric). For F , G two c.d.fs over the reals, it is defined as\ndp(F,G) := inf U,V \u2016U \u2212 V \u2016p,\nwhere the infimum is taken over all pairs of random variables (U, V ) with respective cumulative distributions F and G. The infimum is attained by the inverse c.d.f. transform of a random variable U uniformly distributed on [0, 1]:\ndp(F,G) = \u2016F\u22121(U)\u2212G\u22121(U)\u2016p.\nFor p <\u221e this is more explicitly written as\ndp(F,G) = (\u222b 1 0 \u2223\u2223F\u22121(u)\u2212G\u22121(u)\u2223\u2223pdu)1/p . Given two random variables U , V with c.d.fs FU , FV , we will write dp(U, V ) := dp(FU , FV ). We will find it convenient to conflate the random variables under consideration with their versions under the inf , writing\ndp(U, V ) = inf U,V \u2016U \u2212 V \u2016p.\nwhenever unambiguous; we believe the greater legibility justifies the technical inaccuracy. Finally, we extend this metric to vectors of random variables, such as value distributions, using the corresponding Lp norm.\nConsider a scalar a and a random variable A independent\nof U, V . The metric dp has the following properties:\ndp(aU, aV ) \u2264 |a|dp(U, V ) (P1) dp(A+ U,A+ V ) \u2264 dp(U, V ) (P2)\ndp(AU,AV ) \u2264 \u2016A\u2016pdp(U, V ). (P3)\nWe will need the following additional property, which makes no independence assumptions on its variables. Its proof, and that of later results, is given in the appendix.\nLemma 1 (Partition lemma). Let A1, A2, . . . be a set of random variables describing a partition of \u2126, i.e. Ai(\u03c9) \u2208 {0, 1} and for any \u03c9 there is exactly one Ai with Ai(\u03c9) = 1. Let U, V be two random variables. Then\ndp ( U, V ) \u2264 \u2211\ni dp(AiU,AiV ).\nLet Z denote the space of value distributions with bounded moments. For two value distributions Z1, Z2 \u2208 Z we will make use of a maximal form of the Wasserstein metric:\nd\u0304p(Z1, Z2) := sup x,a dp(Z1(x, a), Z2(x, a)).\nWe will use d\u0304p to establish the convergence of the distributional Bellman operators.\nLemma 2. d\u0304p is a metric over value distributions."}, {"heading": "3.3. Policy Evaluation", "text": "In the policy evaluation setting (Sutton & Barto, 1998) we are interested in the value function V \u03c0 associated with a given policy \u03c0. The analogue here is the value distribution Z\u03c0 . In this section we characterize Z\u03c0 and study the behaviour of the policy evaluation operator T \u03c0 . We emphasize that Z\u03c0 describes the intrinsic randomness of the agent\u2019s interactions with its environment, rather than some measure of uncertainty about the environment itself.\nWe view the reward function as a random vector R \u2208 Z , and define the transition operator P\u03c0 : Z \u2192 Z\nP\u03c0Z(x, a) D := Z(X \u2032, A\u2032) (4)\nX \u2032 \u223c P (\u00b7 |x, a), A\u2032 \u223c \u03c0(\u00b7 |X \u2032),\nwhere we use capital letters to emphasize the random nature of the next state-action pair (X \u2032, A\u2032). We define the distributional Bellman operator T \u03c0 : Z \u2192 Z as\nT \u03c0Z(x, a) D:= R(x, a) + \u03b3P\u03c0Z(x, a). (5)\nWhile T \u03c0 bears a surface resemblance to the usual Bellman operator (2), it is fundamentally different. In particular, three sources of randomness define the compound distribution T \u03c0Z:\na) The randomness in the reward R, b) The randomness in the transition P\u03c0 , and c) The next-state value distribution Z(X \u2032, A\u2032).\nIn particular, we make the usual assumption that these three quantities are independent. In this section we will show that (5) is a contraction mapping whose unique fixed point is the random return Z\u03c0 .\n3.3.1. CONTRACTION IN d\u0304p\nConsider the process Zk+1 := T \u03c0Zk, starting with some Z0 \u2208 Z . We may expect the limiting expectation of {Zk} to converge exponentially quickly, as usual, to Q\u03c0 . As we now show, the process converges in a stronger sense: T \u03c0 is a contraction in d\u0304p, which implies that all moments also converge exponentially quickly.\nLemma 3. T \u03c0 : Z \u2192 Z is a \u03b3-contraction in d\u0304p.\nUsing Lemma 3, we conclude using Banach\u2019s fixed point theorem that T \u03c0 has a unique fixed point. By inspection, this fixed point must be Z\u03c0 as defined in (1). As we assume all moments are bounded, this is sufficient to conclude that the sequence {Zk} converges to Z\u03c0 in d\u0304p for 1 \u2264 p \u2264 \u221e. To conclude, we remark that not all distributional metrics are equal; for example, Chung & Sobel (1987) have shown that T \u03c0 is not a contraction in total variation distance. Similar results can be derived for the Kullback-Leibler divergence and the Kolmogorov distance."}, {"heading": "3.3.2. CONTRACTION IN CENTERED MOMENTS", "text": "Observe that d2(U, V ) (and more generally, dp) relates to a coupling C(\u03c9) := U(\u03c9)\u2212 V (\u03c9), in the sense that\nd22(U, V ) \u2264 E[(U \u2212 V )2] = V ( C ) + ( EC )2 .\nAs a result, we cannot directly use d2 to bound the variance difference |V(T \u03c0Z(x, a)) \u2212 V(Z\u03c0(x, a))|. However, T \u03c0 is in fact a contraction in variance (Sobel, 1982, see also appendix). In general, T \u03c0 is not a contraction in the pth centered moment, p > 2, but the centered moments of the iterates {Zk} still converge exponentially quickly to those of Z\u03c0; the proof extends the result of Ro\u0308sler (1992)."}, {"heading": "3.4. Control", "text": "Thus far we have considered a fixed policy \u03c0, and studied the behaviour of its associated operator T \u03c0 . We now set out to understand the distributional operators of the control setting \u2013 where we seek a policy \u03c0 that maximizes value \u2013 and the corresponding notion of an optimal value distribution. As with the optimal value function, this notion is intimately tied to that of an optimal policy. However, while all optimal policies attain the same value Q\u2217, in our case\na difficulty arises: in general there are many optimal value distributions.\nIn this section we show that the distributional analogue of the Bellman optimality operator converges, in a weak sense, to the set of optimal value distributions. However, this operator is not a contraction in any metric between distributions, and is in general much more temperamental than the policy evaluation operators. We believe the convergence issues we outline here are a symptom of the inherent instability of greedy updates, as highlighted by e.g. Tsitsiklis (2002) and most recently Harutyunyan et al. (2016).\nLet \u03a0\u2217 be the set of optimal policies. We begin by characterizing what we mean by an optimal value distribution.\nDefinition 1 (Optimal value distribution). An optimal value distribution is the v.d. of an optimal policy. The set of optimal value distributions is Z\u2217 := {Z\u03c0\u2217 : \u03c0\u2217 \u2208 \u03a0\u2217}.\nWe emphasize that not all value distributions with expectation Q\u2217 are optimal: they must match the full distribution of the return under some optimal policy.\nDefinition 2. A greedy policy \u03c0 for Z \u2208 Z maximizes the expectation of Z. The set of greedy policies for Z is\nGZ := {\u03c0 : \u2211\na \u03c0(a |x)EZ(x, a) = max a\u2032\u2208A EZ(x, a\u2032)}.\nRecall that the expected Bellman optimality operator T is\nT Q(x, a) = ER(x, a) + \u03b3 EP max a\u2032\u2208A Q(x\u2032, a\u2032). (6)\nThe maximization at x\u2032 corresponds to some greedy policy. Although this policy is implicit in (6), we cannot ignore it in the distributional setting. We will call a distributional Bellman optimality operator any operator T which implements a greedy selection rule, i.e."}, {"heading": "T Z = T \u03c0Z for some \u03c0 \u2208 GZ .", "text": "As in the policy evaluation setting, we are interested in the behaviour of the iterates Zk+1 := T Zk, Z0 \u2208 Z . Our first result is to assert that EZk behaves as expected. Lemma 4. Let Z1, Z2 \u2208 Z . Then\n\u2016E T Z1 \u2212 E T Z2\u2016\u221e \u2264 \u03b3 \u2016EZ1 \u2212 EZ2\u2016\u221e ,\nand in particular EZk \u2192 Q\u2217 exponentially quickly.\nBy inspecting Lemma 4, we might expect that Zk converges quickly in d\u0304p to some fixed point in Z\u2217. Unfortunately, convergence is neither quick nor assured to reach a fixed point. In fact, the best we can hope for is pointwise convergence, not even to the set Z\u2217 but to the larger set of nonstationary optimal value distributions.\nDefinition 3. A nonstationary optimal value distribution Z\u2217\u2217 is the value distribution corresponding to a sequence of optimal policies. The set of n.o.v.d. is Z\u2217\u2217. Theorem 1 (Convergence in the control setting). Let X be measurable and suppose that A is finite. Then\nlim k\u2192\u221e inf Z\u2217\u2217\u2208Z\u2217\u2217\ndp(Zk(x, a), Z \u2217\u2217(x, a)) = 0 \u2200x, a.\nIf X is finite, then Zk converges to Z\u2217\u2217 uniformly. Furthermore, if there is a total ordering\u227a on \u03a0\u2217, such that for any Z\u2217 \u2208 Z\u2217,"}, {"heading": "T Z\u2217 = T \u03c0Z\u2217 with \u03c0 \u2208 GZ\u2217 , \u03c0 \u227a \u03c0\u2032 \u2200\u03c0\u2032 \u2208 GZ\u2217 \\ {\u03c0}.", "text": "Then T has a unique fixed point Z\u2217 \u2208 Z\u2217.\nComparing Theorem 1 to Lemma 4 reveals a significant difference between the distributional framework and the usual setting of expected return. While the mean of Zk converges exponentially quickly toQ\u2217, its distribution need not be as well-behaved! To emphasize this difference, we now provide a number of negative results concerning T . Proposition 1. The operator T is not a contraction.\nConsider the following example (Figure 2, left). There are two states, x1 and x2; a unique transition from x1 to x2; from x2, action a1 yields no reward, while the optimal action a2 yields 1 + or \u22121 + with equal probability. Both actions are terminal. There is a unique optimal policy and therefore a unique fixed pointZ\u2217. Now considerZ as given in Figure 2 (right), and its distance to Z\u2217:\nd\u03041(Z,Z \u2217) = d1(Z(x2, a2), Z \u2217(x2, a2)) = 2 ,\nwhere we made use of the fact that Z = Z\u2217 everywhere except at (x2, a2). When we apply T to Z, however, the greedy action a1 is selected and T Z(x1) = Z(x2, a1). But\nd1(T Z, T Z\u2217) = d1(T Z(x1), Z\u2217(x1)) = 12 |1\u2212 |+ 12 |1 + | > 2\nfor a sufficiently small . This shows that the undiscounted update is not a nonexpansion: d\u03041(T Z, T Z\u2217) > d\u03041(Z,Z\u2217). With \u03b3 < 1, the same proof shows it is not a contraction. Using a more technically involved argument, we can extend this result to any metric which separates Z and T Z. Proposition 2. Not all optimality operators have a fixed point Z\u2217 = T Z\u2217.\nTo see this, consider the same example, now with = 0, and a greedy operator T which breaks ties by picking a2 if Z(x1) = 0, and a1 otherwise. Then the sequence T Z\u2217(x1), (T )2Z\u2217(x1), . . . alternates between Z\u2217(x2, a1) and Z\u2217(x2, a2).\nProposition 3. That T has a fixed point Z\u2217 = T Z\u2217 is insufficient to guarantee the convergence of {Zk} to Z\u2217.\nTheorem 1 paints a rather bleak picture of the control setting. It remains to be seen whether the dynamical eccentricies highlighted here actually arise in practice. One open question is whether theoretically more stable behaviour can be derived using stochastic policies, for example from conservative policy iteration (Kakade & Langford, 2002)."}, {"heading": "4. Approximate Distributional Learning", "text": "In this section we propose an algorithm based on the distributional Bellman optimality operator. In particular, this will require choosing an approximating distribution. Although the Gaussian case has previously been considered (Morimura et al., 2010a; Tamar et al., 2016), to the best of our knowledge we are the first to use a rich class of parametric distributions."}, {"heading": "4.1. Parametric Distribution", "text": "We will model the value distribution using a discrete distribution parametrized by N \u2208 N and VMIN, VMAX \u2208 R, and whose support is the set of atoms {zi = VMIN + i4z : 0 \u2264 i < N}, 4z := VMAX\u2212VMINN\u22121 . In a sense, these atoms are the \u201ccanonical returns\u201d of our distribution. The atom probabilities are given by a parametric model \u03b8 : X \u00d7A \u2192 RN\nZ\u03b8(x, a) = zi w.p. pi(x, a) := e\u03b8i(x,a)\u2211 j e \u03b8j(x,a) .\nThe discrete distribution has the advantages of being highly expressive and computationally friendly (see e.g. Van den Oord et al., 2016)."}, {"heading": "4.2. Projected Bellman Update", "text": "Using a discrete distribution poses a problem: the Bellman update T Z\u03b8 and our parametrization Z\u03b8 almost always have disjoint supports. From the analysis of Section 3 it would seem natural to minimize the Wasserstein metric (viewed as a loss) between T Z\u03b8 and Z\u03b8, which is also\nconveniently robust to discrepancies in support. However, a second issue prevents this: in practice we are typically restricted to learning from sample transitions, which is not possible under the Wasserstein loss (see Prop. 5 and toy results in the appendix).\nInstead, we project the sample Bellman update T\u0302 Z\u03b8 onto the support of Z\u03b8 (Figure 1, Algorithm 1), effectively reducing the Bellman update to multiclass classification. Let \u03c0 be the greedy policy w.r.t. EZ\u03b8. Given a sample transition (x, a, r, x\u2032), we compute the Bellman update T\u0302 zj := r + \u03b3zj for each atom zj , then distribute its probability pj(x\n\u2032, \u03c0(x\u2032)) to the immediate neighbours of T\u0302 zj . The ith component of the projected update \u03a6T\u0302 Z\u03b8(x, a) is (\u03a6T\u0302 Z\u03b8(x, a))i = N\u22121\u2211 j=0 [ 1\u2212 |[T\u0302 zj ]VMAXVMIN \u2212 zi| 4z ]1 0 pj(x \u2032, \u03c0(x\u2032)),\n(7) where [\u00b7]ba bounds its argument in the range [a, b].1 As is usual, we view the next-state distribution as parametrized by a fixed parameter \u03b8\u0303. The sample loss Lx,a(\u03b8) is the cross-entropy term of the KL divergence\nDKL(\u03a6T\u0302 Z\u03b8\u0303(x, a) \u2016Z\u03b8(x, a)), which is readily minimized e.g. using gradient descent. We call this choice of distribution and loss the categorical algorithm. When N = 2, a simple one-parameter alternative is \u03a6T\u0302 Z\u03b8(x, a) := [E[T\u0302 Z\u03b8(x, a)] \u2212 VMIN)/4z]10; we call this the Bernoulli algorithm. We note that, while these algorithms appear unrelated to the Wasserstein metric, recent work (Bellemare et al., 2017) hints at a deeper connection.\nAlgorithm 1 Categorical Algorithm input A transition xt, at, rt, xt+1, \u03b3t \u2208 [0, 1] Q(xt+1, a) := \u2211 i zipi(xt+1, a)\na\u2217 \u2190 arg maxaQ(xt+1, a) mi = 0, i \u2208 0, . . . , N \u2212 1 for j \u2208 0, . . . , N \u2212 1 do\n# Compute the projection of T\u0302 zj onto the support {zi} T\u0302 zj \u2190 [rt + \u03b3tzj ]VMAXVMIN bj \u2190 (T\u0302 zj \u2212 VMIN)/\u2206z # bj \u2208 [0, N \u2212 1] l\u2190 bbjc, u\u2190 dbje # Distribute probability of T\u0302 zj ml \u2190 ml + pj(xt+1, a\u2217)(u\u2212 bj) mu \u2190 mu + pj(xt+1, a\u2217)(bj \u2212 l) end for output \u2212\u2211imi log pi(xt, at) # Cross-entropy loss"}, {"heading": "5. Evaluation on Atari 2600 Games", "text": "To understand the approach in a complex setting, we applied the categorical algorithm to games from the Ar-\n1Algorithm 1 computes this projection in time linear in N .\ncade Learning Environment (ALE; Bellemare et al., 2013). While the ALE is deterministic, stochasticity does occur in a number of guises: 1) from state aliasing, 2) learning from a nonstationary policy, and 3) from approximation errors. We used five training games (Fig 3) and 52 testing games.\nFor our study, we use the DQN architecture (Mnih et al., 2015), but output the atom probabilities pi(x, a) instead of action-values, and chose VMAX = \u2212VMIN = 10 from preliminary experiments over the training games. We call the resulting architecture Categorical DQN. We replace the squared loss (r + \u03b3Q(x\u2032, \u03c0(x\u2032)) \u2212 Q(x, a))2 by Lx,a(\u03b8) and train the network to minimize this loss.2 As in DQN, we use a simple -greedy policy over the expected actionvalues; we leave as future work the many ways in which an agent could select actions on the basis of the full distribution. The rest of our training regime matches Mnih et al.\u2019s, including the use of a target network for \u03b8\u0303.\nFigure 4 illustrates the typical value distributions we observed in our experiments. In this example, three actions (those including the button press) lead to the agent releasing its laser too early and eventually losing the game. The corresponding distributions reflect this: they assign a significant probability to 0 (the terminal value). The safe actions have similar distributions (LEFT, which tracks the invaders\u2019 movement, is slightly favoured). This example helps explain why our approach is so successful: the distributional update keeps separated the low-value, \u201closing\u201d event from the high-value, \u201csurvival\u201d event, rather than average them into one (unrealizable) expectation.3\nOne surprising fact is that the distributions are not concentrated on one or two values, in spite of the ALE\u2019s determinism, but are often close to Gaussians. We believe this is due to our discretizing the diffusion process induced by \u03b3."}, {"heading": "5.1. Varying the Number of Atoms", "text": "We began by studying our algorithm\u2019s performance on the training games in relation to the number of atoms (Figure 3). For this experiment, we set = 0.05. From the data, it is clear that using too few atoms can lead to poor behaviour, and that more always increases performance; this is not immediately obvious as we may have expected to saturate the network capacity. The difference in performance between the 51-atom version and DQN is particularly striking: the latter is outperformed in all five games, and in SEAQUEST we attain state-of-the-art performance. As an additional point of the comparison, the single-parameter Bernoulli algorithm performs better than DQN in 3 games out of 5, and is most notably more robust in ASTERIX.\n2For N = 51, our TensorFlow implementation trains at roughly 75% of DQN\u2019s speed.\n3Video: http://youtu.be/yFBwyPuO2Vg.\nOne interesting outcome of this experiment was to find out that our method does pick up on stochasticity. PONG exhibits intrinsic randomness: the exact timing of the reward depends on internal registers and is truly unobservable. We see this clearly reflected in the agent\u2019s prediction (Figure 5): over five consecutive frames, the value distribution shows two modes indicating the agent\u2019s belief that it has yet to receive a reward. Interestingly, since the agent\u2019s state does not include past rewards, it cannot even extinguish the prediction after receiving the reward, explaining the relative proportions of the modes."}, {"heading": "5.2. State-of-the-Art Results", "text": "The performance of the 51-atom agent (from here onwards, C51) on the training games, presented in the last section, is particularly remarkable given that it involved none of the other algorithmic ideas present in state-of-the-art agents. We next asked whether incorporating the most common hyperparameter choice, namely a smaller training , could lead to even better results. Specifically, we set = 0.01 (instead of 0.05); furthermore, every 1 million frames, we\nevaluate our agent\u2019s performance with = 0.001.\nWe compare our algorithm to DQN ( = 0.01), Double DQN (van Hasselt et al., 2016), the Dueling architecture (Wang et al., 2016), and Prioritized Replay (Schaul et al., 2016), comparing the best evaluation score achieved during training. We see that C51 significantly outperforms these other algorithms (Figures 6 and 7). In fact, C51 surpasses the current state-of-the-art by a large margin in a number of games, most notably SEAQUEST. One particularly striking fact is the algorithm\u2019s good performance on sparse reward games, for example VENTURE and PRIVATE EYE. This suggests that value distributions are better able to propagate rarely occurring events. Full results are provided in the appendix.\nWe also include in the appendix (Figure 12) a comparison, averaged over 3 seeds, showing the number of games in which C51\u2019s training performance outperforms fullytrained DQN and human players. These results continue to show dramatic improvements, and are more representative of an agent\u2019s average performance. Within 50 million frames, C51 has outperformed a fully trained DQN agent on 45 out of 57 games. This suggests that the full 200 million training frames, and its ensuing computational cost, are unnecessary for evaluating reinforcement learning algorithms within the ALE.\nThe most recent version of the ALE contains a stochastic execution mechanism designed to ward against trajectory overfitting.Specifically, on each frame the environment rejects the agent\u2019s selected action with probability p = 0.25. Although DQN is mostly robust to stochastic execution, there are a few games in which its performance is reduced. On a score scale normalized with respect to the random and DQN agents, C51 obtains mean and median score improvements of 126% and 21.5% respectively, confirming the benefits of C51 beyond the deterministic setting."}, {"heading": "6. Discussion", "text": "In this work we sought a more complete picture of reinforcement learning, one that involves value distributions. We found that learning value distributions is a powerful notion that allows us to surpass most gains previously made on Atari 2600, without further algorithmic adjustments."}, {"heading": "6.1. Why does learning a distribution matter?", "text": "It is surprising that, when we use a policy which aims to maximize expected return, we should see any difference in performance. The distinction we wish to make is that learning distributions matters in the presence of approximation. We now outline some possible reasons.\nReduced chattering. Our results from Section 3.4 highlighted a significant instability in the Bellman optimality operator. When combined with function approximation, this instability may prevent the policy from converging, what Gordon (1995) called chattering. We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri-\n\u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning (Jaderberg et al., 2017).\nbutions, similar to conservative policy iteration (Kakade & Langford, 2002). While the chattering persists, it is integrated to the approximate solution.\nState aliasing. Even in a deterministic environment, state aliasing may result in effective stochasticity. McCallum (1995), for example, showed the importance of coupling representation learning with policy learning in partially observable domains. We saw an example of state aliasing in PONG, where the agent could not exactly predict the reward timing. Again, by explicitly modelling the resulting distribution we provide a more stable learning target.\nA richer set of predictions. A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions (Caruana 1997; Utgoff & Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017). The distributional approach naturally provides us with a rich set of auxiliary predictions, namely: the probability that the return will take on a particular value. Unlike previously proposed approaches, however, the accuracy of these predictions is tightly coupled with the agent\u2019s performance.\nFramework for inductive bias. The distributional perspective on reinforcement learning allows a more natural framework within which we can impose assumptions about the domain or the learning problem itself. In this work we used distributions with support bounded in [VMIN, VMAX]. Treating this support as a hyperparameter allows us to change the optimization problem by treating all extremal returns (e.g. greater than VMAX) as equivalent. Surprisingly, a similar value clipping in DQN significantly degrades performance in most games. To take another example: interpreting the discount factor \u03b3 as a proper probability, as some authors have argued, leads to a different algorithm.\nWell-behaved optimization. It is well-accepted that the KL divergence between categorical distributions is a reasonably easy loss to minimize. This may explain some of our empirical performance. Yet early experiments with alternative losses, such as KL divergence between continuous densities, were not fruitful, in part because the KL divergence is insensitive to the values of its outcomes. A closer minimization of the Wasserstein metric should yield even better results than what we presented here.\nIn closing, we believe our results highlight the need to account for distribution in the design, theoretical or otherwise, of algorithms."}, {"heading": "Acknowledgements", "text": "The authors acknowledge the important role played by their colleagues at DeepMind throughout the development of this work. Special thanks to Yee Whye Teh, Alex Graves, Joel Veness, Guillaume Desjardins, Tom Schaul, David Silver, Andre Barreto, Max Jaderberg, Mohammad Azar, Georg Ostrovski, Bernardo Avila Pires, Olivier Pietquin, Audrunas Gruslys, Tom Stepleton, Aaron van den Oord; and particularly Chris Maddison for his comprehensive review of an earlier draft. Thanks also to Marek Petrik for pointers to the relevant literature, and Mark Rowland for fine-tuning details in the final version.\nErratum The camera-ready copy of this paper incorrectly reported a mean score of 1010% for C51. The corrected figure stands at 701%, which remains higher than the other comparable baselines. The median score remains unchanged at 178%.\nThe error was due to evaluation episodes in one game (Atlantis) lasting over 30 minutes; in comparison, the other results presented here cap episodes at 30 minutes, as is standard. The previously reported score on Atlantis was 3.7 million; our 30-minute score is 841,075, which we believe is close to the achievable maximum in this time frame. Capping at 30 minutes brings our human-normalized score on Atlantis from 22824% to a mere (!) 5199%, unfortunately enough to noticeably affect the mean score, whose sensitivity to outliers is well-documented."}, {"heading": "A. Related Work", "text": "To the best of our knowledge, the work closest to ours are two papers (Morimura et al., 2010b;a) studying the distributional Bellman equation from the perspective of its cumulative distribution functions. The authors propose both parametric and nonparametric solutions to learn distributions for risk-sensitive reinforcement learning. They also provide some theoretical analysis for the policy evaluation setting, including a consistency result in the nonparametric case. By contrast, we also analyze the control setting, and emphasize the use of the distributional equations to improve approximate reinforcement learning.\nThe variance of the return has been extensively studied in the risk-sensitive setting. Of note, Tamar et al. (2016) analyze the use of linear function approximation to learn this variance for policy evaluation, and Prashanth & Ghavamzadeh (2013) estimate the return variance in the design of a risk-sensitive actor-critic algorithm. Mannor & Tsitsiklis (2011) provides negative results regarding the computation of a variance-constrained solution to the optimal control problem.\nThe distributional formulation also arises when modelling uncertainty. Dearden et al. (1998) considered a Gaussian approximation to the value distribution, and modelled the uncertainty over the parameters of this approximation using a Normal-Gamma prior. Engel et al. (2005) leveraged the distributional Bellman equation to define a Gaussian process over the unknown value function. More recently, Geist & Pietquin (2010) proposed an alternative solution to the same problem based on unscented Kalman filters. We believe much of the analysis we provide here, which deals with the intrinsic randomness of the environment, can also be applied to modelling uncertainty.\nOur work here is based on a number of foundational results, in particular concerning alternative optimality criteria. Early on, Jaquette (1973) showed that a moment optimality criterion, which imposes a total ordering on distributions, is achievable and defines a stationary optimal policy, echoing the second part of Theorem 1. Sobel (1982) is usually cited as the first reference to Bellman equations for the higher moments (but not the distribution) of the return. Chung & Sobel (1987) provides results concerning the convergence of the distributional Bellman operator in total variation distance. White (1988) studies \u201cnonstandard MDP criteria\u201d from the perspective of optimizing the stateaction pair occupancy.\nA number of probabilistic frameworks for reinforcement learning have been proposed in recent years. The planning as inference approach (Toussaint & Storkey, 2006; Hoffman et al., 2009) embeds the return into a graphical model, and applies probabilistic inference to determine the\nsequence of actions leading to maximal expected reward. Wang et al. (2008) considered the dual formulation of reinforcement learning, where one optimizes the stationary distribution subject to constraints given by the transition function (Puterman, 1994), in particular its relationship to linear approximation. Related to this dual is the Compress and Control algorithm Veness et al. (2015), which describes a value function by learning a return distribution using density models. One of the aims of this work was to address the question left open by their work of whether one could be design a practical distributional algorithm based on the Bellman equation, rather than Monte Carlo estimation."}, {"heading": "B. Proofs", "text": "Lemma 1 (Partition lemma). Let A1, A2, . . . be a set of random variables describing a partition of \u2126, i.e. Ai(\u03c9) \u2208 {0, 1} and for any \u03c9 there is exactly one Ai with Ai(\u03c9) = 1. Let U, V be two random variables. Then\ndp ( U, V ) \u2264 \u2211\ni dp(AiU,AiV ).\nProof. We will give the proof for p < \u221e, noting that the same applies to p = \u221e. Let Yi D := AiU and Zi D := AiV , respectively. First note that\ndpp(AiU,AiV ) = inf Yi,Zi\nE [ |Yi \u2212 Zi|p ] = inf Yi,Zi E [ E [ |Yi \u2212 Zi|p |Ai ]] .\nNow, |AiU \u2212AiV |p = 0 whenever Ai = 0. It follows that we can choose Yi, Zi so that also |Yi \u2212 Zi|p = 0 whenever Ai = 0, without increasing the expected norm. Hence\ndpp(AiU,AiV ) =\ninf Yi,Zi\nPr{Ai = 1}E [ |Yi \u2212 Zi|p |Ai = 1 ] . (8)\nNext, we claim that\ninf U,V \u2211 i Pr{Ai = 1}E [\u2223\u2223AiU \u2212AiV \u2223\u2223p |Ai = 1] (9) \u2264 inf\nY1,Y2,... Z1,Z2,...\n\u2211 i Pr{Ai = 1}E [ |Yi \u2212 Zi \u2223\u2223p |Ai = 1]. Specifically, the left-hand side of the equation is an infimum over all r.v.\u2019s whose cumulative distributions are FU and FV , respectively, while the right-hand side is an infimum over sequences of r.v\u2019s Y1, Y2, . . . and Z1, Z2, . . . whose cumulative distributions are FAiU , FAiV , respectively. To prove this upper bound, consider the c.d.f. of U :\nFU (y) = Pr{U \u2264 y} = \u2211\ni Pr{Ai = 1}Pr{U \u2264 y |Ai = 1} = \u2211\ni Pr{Ai = 1}Pr{AiU \u2264 y |Ai = 1}.\nHence the distribution FU is equivalent, in an almost sure sense, to one that first picks an element Ai of the partition, then picks a value for U conditional on the choice Ai. On the other hand, the c.d.f. of Yi D = AiU is\nFAiU (y) = Pr{Ai = 1}Pr{AiU \u2264 y |Ai = 1} + Pr{Ai = 0}Pr{AiU \u2264 y |Ai = 0}\n= Pr{Ai = 1}Pr{AiU \u2264 y |Ai = 1} + Pr{Ai = 0}I [y \u2265 0] .\nThus the right-hand side infimum in (9) has the additional constraint that it must preserve the conditional c.d.fs, in particular when y \u2265 0. Put another way, instead of having the freedom to completely reorder the mapping U : \u2126 \u2192 R, we can only reorder it within each element of the partition. We now write\ndpp(U, V ) = inf U,V \u2016U \u2212 V \u2016p\n= inf U,V\nE [ |U \u2212 V |p ] (a) = inf\nU,V \u2211 i Pr{Ai = 1}E [ |U \u2212 V |p |Ai = 1 ] = inf U,V \u2211 i Pr{Ai = 1}E [ |AiU \u2212AiV |p |Ai = 1 ] ,\nwhere (a) follows because A1, A2, . . . is a partition. Using (9), this implies\ndpp(U, V )\n= inf U,V \u2211 i Pr{Ai = 1}E [\u2223\u2223AiU \u2212AiV \u2223\u2223p |Ai = 1] \u2264 inf\nY1,Y2,... Z1,Z2,...\n\u2211 i Pr{Ai = 1}E [\u2223\u2223Yi \u2212 Zi\u2223\u2223p |Ai = 1] (b) = \u2211\ni inf Yi,Zi\nPr{Ai = 1}E [\u2223\u2223Yi \u2212 Zi\u2223\u2223p |Ai = 1]\n(c) = \u2211\ni dp(AiU,AiV ),\nbecause in (b) the individual components of the sum are independently minimized; and (c) from (8).\nLemma 2. d\u0304p is a metric over value distributions.\nProof. The only nontrivial property is the triangle inequality. For any value distribution Y \u2208 Z , write\nd\u0304p(Z1, Z2) = sup x,a dp(Z1(x, a), Z2(x, a))\n(a) \u2264 sup x,a [dp(Z1(x, a), Y (x, a)) + dp(Y (x, a), Z2(x, a))]\n\u2264 sup x,a dp(Z1(x, a), Y (x, a)) + sup x,a dp(Y (x, a), Z2(x, a))\n= d\u0304p(Z1, Y ) + d\u0304p(Y,Z2),\nwhere in (a) we used the triangle inequality for dp.\nLemma 3. T \u03c0 : Z \u2192 Z is a \u03b3-contraction in d\u0304p.\nProof. Consider Z1, Z2 \u2208 Z . By definition,\nd\u0304p(T \u03c0Z1, T \u03c0Z2) = sup x,a dp(T \u03c0Z1(x, a), T \u03c0Z2(x, a)). (10)\nBy the properties of dp, we have\ndp(T \u03c0Z1(x, a), T \u03c0Z2(x, a)) = dp(R(x, a) + \u03b3P \u03c0Z1(x, a), R(x, a) + \u03b3P \u03c0Z2(x, a))\n\u2264 \u03b3dp(P\u03c0Z1(x, a), P\u03c0Z2(x, a)) \u2264 \u03b3 sup\nx\u2032,a\u2032 dp(Z1(x\n\u2032, a\u2032), Z2(x \u2032, a\u2032)),\nwhere the last line follows from the definition of P\u03c0 (see (4)). Combining with (10) we obtain\nd\u0304p(T \u03c0Z1, T \u03c0Z2) = sup x,a dp(T \u03c0Z1(x, a), T \u03c0Z2(x, a))\n\u2264 \u03b3 sup x\u2032,a\u2032 dp(Z1(x \u2032, a\u2032), Z2(x \u2032, a\u2032))\n= \u03b3d\u0304p(Z1, Z2).\nProposition 1 (Sobel, 1982). Consider two value distributions Z1, Z2 \u2208 Z , and write V(Zi) to be the vector of variances of Zi. Then\n\u2016E T \u03c0Z1 \u2212 E T \u03c0Z2\u2016\u221e \u2264 \u03b3 \u2016EZ1 \u2212 EZ2\u2016\u221e , and \u2016V(T \u03c0Z1)\u2212 V(T \u03c0Z2)\u2016\u221e \u2264 \u03b32 \u2016VZ1 \u2212 VZ2\u2016\u221e .\nProof. The first statement is standard, and its proof follows from E T \u03c0Z = T \u03c0 EZ, where the second T \u03c0 denotes the usual operator over value functions. Now, by independence of R and P\u03c0Zi:\nV(T \u03c0Zi(x, a)) = V ( R(x, a) + \u03b3P\u03c0Zi(x, a) ) = V(R(x, a)) + \u03b32V(P\u03c0Zi(x, a)).\nAnd now\n\u2016V(T \u03c0Z1)\u2212 V(T \u03c0Z2)\u2016\u221e = sup\nx,a \u2223\u2223V(T \u03c0Z1(x, a))\u2212 V(T \u03c0Z2(x, a))\u2223\u2223 = sup\nx,a \u03b32 \u2223\u2223 [V(P\u03c0Z1(x, a))\u2212 V(P\u03c0Z2(x, a))] \u2223\u2223\n= sup x,a\n\u03b32 \u2223\u2223E [V(Z1(X \u2032, A\u2032))\u2212 V(Z2(X \u2032, A\u2032))] \u2223\u2223\n\u2264 sup x\u2032,a\u2032\n\u03b32 \u2223\u2223V(Z1(x\u2032, a\u2032))\u2212 V(Z2(x\u2032, a\u2032))\u2223\u2223\n\u2264 \u03b32 \u2016VZ1 \u2212 VZ2\u2016\u221e .\nLemma 4. Let Z1, Z2 \u2208 Z . Then\n\u2016E T Z1 \u2212 E T Z2\u2016\u221e \u2264 \u03b3 \u2016EZ1 \u2212 EZ2\u2016\u221e ,\nand in particular EZk \u2192 Q\u2217 exponentially quickly.\nProof. The proof follows by linearity of expectation. Write TD for the distributional operator and TE for the usual operator. Then\n\u2016E TDZ1 \u2212 E TDZ2\u2016\u221e = \u2016TE EZ1 \u2212 TE EZ2\u2016\u221e \u2264 \u03b3 \u2016Z1 \u2212 Z2\u2016\u221e .\nTheorem 1 (Convergence in the control setting). Let Zk := T Zk\u22121 with Z0 \u2208 Z . Let X be measurable and suppose that A is finite. Then\nlim k\u2192\u221e inf Z\u2217\u2217\u2208Z\u2217\u2217\ndp(Zk(x, a), Z \u2217\u2217(x, a)) = 0 \u2200x, a.\nIf X is finite, then Zk converges to Z\u2217\u2217 uniformly. Furthermore, if there is a total ordering\u227a on \u03a0\u2217, such that for any Z\u2217 \u2208 Z\u2217,"}, {"heading": "T Z\u2217 = T \u03c0Z\u2217 with \u03c0 \u2208 GZ\u2217 , \u03c0 \u227a \u03c0\u2032 \u2200\u03c0\u2032 \u2208 GZ\u2217 \\ {\u03c0},", "text": "then T has a unique fixed point Z\u2217 \u2208 Z\u2217.\nThe gist of the proof of Theorem 1 consists in showing that for every state x, there is a time k after which the greedy policy w.r.t. Qk is mostly optimal. To clearly expose the steps involved, we will first assume a unique (and therefore deterministic) optimal policy \u03c0\u2217, and later return to the general case; we will denote the optimal action at x by \u03c0\u2217(x). For notational convenience, we will write Qk := EZk and Gk := GZk . Let B := 2 supZ\u2208Z \u2016Z\u2016\u221e < \u221e and let k := \u03b3kB. We first define the set of states Xk \u2286 X whose values must be sufficiently close to Q\u2217 at time k:\nXk := { x : Q\u2217(x, \u03c0\u2217(x))\u2212 max\na 6=\u03c0\u2217(x) Q\u2217(x, a) > 2 k\n} .\n(11) Indeed, by Lemma 4, we know that after k iterations\n|Qk(x, a)\u2212Q\u2217(x, a)| \u2264 \u03b3k|Q0(x, a)\u2212Q\u2217(x, a)| \u2264 k.\nFor x \u2208 X , write a\u2217 := \u03c0\u2217(x). For any a \u2208 A, we deduce that\nQk(x, a \u2217)\u2212Qk(x, a) \u2265 Q\u2217(x, a\u2217)\u2212Q\u2217(x, a)\u2212 2 k.\nIt follows that if x \u2208 Xk, then also Qk(x, a\u2217) > Qk(x, a\u2032) for all a\u2032 6= \u03c0\u2217(x): for these states, the greedy policy \u03c0k(x) := arg maxaQk(x, a) corresponds to the optimal policy \u03c0\u2217.\nLemma 5. For each x \u2208 X there exists a k such that, for all k\u2032 \u2265 k, x \u2208 Xk\u2032 , and in particular arg maxaQk(x, a) = \u03c0 \u2217(x).\nProof. Because A is finite, the gap\n\u2206(x) := Q\u2217(x, \u03c0\u2217(x))\u2212 max a 6=\u03c0\u2217(x) Q\u2217(x, a)\nis attained for some strictly positive \u2206(x) > 0. By definition, there exists a k such that\nk = \u03b3 kB <\n\u2206(x)\n2 ,\nand hence every x \u2208 X must eventually be in Xk.\nThis lemma allows us to guarantee the existence of an iteration k after which sufficiently many states are wellbehaved, in the sense that the greedy policy at those states chooses the optimal action. We will call these states \u201csolved\u201d. We in fact require not only these states to be solved, but also most of their successors, and most of the successors of those, and so on. We formalize this notion as follows: fix some \u03b4 > 0, let Xk,0 := Xk, and define for i > 0 the set\nXk,i := { x : x \u2208 Xk, P (Xk\u22121,i\u22121 |x, \u03c0\u2217(x)) \u2265 1\u2212 \u03b4 } ,\nAs the following lemma shows, any x is eventually contained in the recursively-defined sets Xk,i, for any i. Lemma 6. For any i \u2208 N and any x \u2208 X , there exists a k such that for all k\u2032 \u2265 k, x \u2208 Xk\u2032,i.\nProof. Fix i and let us suppose that Xk,i \u2191 X . By Lemma 5, this is true for i = 0. We infer that for any probability measure P on X , P (Xk,i)\u2192 P (X ) = 1. In particular, for a given x \u2208 Xk, this implies that\nP (Xk,i |x, \u03c0\u2217(x))\u2192 P (X |x, \u03c0\u2217(x)) = 1.\nTherefore, for any x, there exists a time after which it is and remains a member ofXk,i+1, the set of states for which P (Xk\u22121,i |x, \u03c0\u2217(x)) \u2265 1 \u2212 \u03b4. We conclude that Xk,i+1 \u2191 X also. The statement follows by induction.\nProof of Theorem 1. The proof is similar to policy iteration-type results, but requires more care in dealing with the metric and the possibly infinite state space. We will write Wk(x) := Zk(x, \u03c0k(x)), define W \u2217 similarly and with some overload of notation write TWk(x) := Wk+1(x) = T Zk(x, \u03c0k+1(x)). Finally, let Ski (x) := I [x \u2208 Xk,i] and S\u0304ki (x) = 1\u2212 Ski (x). Fix i > 0 and x \u2208 Xk+1,i+1 \u2286 Xk. We begin by using Lemma 1 to separate the transition from x into a solved term and an unsolved term:\nP\u03c0kWk(x) = S k iWk(X \u2032) + S\u0304kiWk(X \u2032),\nwhere X \u2032 is the random successor from taking action \u03c0k(x) := \u03c0 \u2217(x), and we write Ski = S k i (X\n\u2032), S\u0304ki = S\u0304ki (X \u2032) to ease the notation. Similarly,\nP\u03c0kW \u2217(x) = SkiW \u2217(X \u2032) + S\u0304kiW \u2217(X \u2032).\nNow\ndp(Wk+1(x),W \u2217(x)) = dp(TWk(x), TW \u2217(x))\n(a) \u2264 \u03b3dp(P\u03c0kWk(x), P\u03c0 \u2217 W \u2217(x))\n(b) \u2264 \u03b3dp(SkiWk(X \u2032), SkiW \u2217(X \u2032)) + \u03b3dp(S\u0304 k iWk(X \u2032), S\u0304kiW \u2217(X \u2032)), (12)\nwhere in (a) we used Properties P1 and P2 of the Wasserstein metric, and in (b) we separate states for which \u03c0k = \u03c0\u2217 from the rest using Lemma 1 ({Ski , S\u0304ki } form a partition of \u2126). Let \u03b4i := Pr{X \u2032 /\u2208 Xk,i} = E{S\u0304ki (X \u2032)} = \u2016S\u0304ki (X \u2032)\u2016p. From property P3 of the Wasserstein metric, we have\ndp(S\u0304 k iWk(X \u2032), S\u0304kiW \u2217(X \u2032))\n\u2264 sup x\u2032 dp(S\u0304 k i (X \u2032)Wk(x \u2032), S\u0304ki (X \u2032)W \u2217(x\u2032)) \u2264 \u2016S\u0304ki (X \u2032)\u2016p sup x\u2032 dp(Wk(x \u2032),W \u2217(x\u2032)) \u2264 \u03b4i sup x\u2032 dp(Wk(x \u2032),W \u2217(x\u2032)) \u2264 \u03b4iB.\nRecall that B < \u221e is the largest attainable \u2016Z\u2016\u221e. Since also \u03b4i < \u03b4 by our choice of x \u2208 Xk+1,i+1, we can upper bound the second term in (12) by \u03b3\u03b4B. This yields\ndp(Wk+1(x),W \u2217(x)) \u2264\n\u03b3dp(S k iWk(X \u2032), SkiW \u2217(X \u2032)) + \u03b3\u03b4B.\nBy induction on i > 0, we conclude that for x \u2208 Xk+i,i and some random state X \u2032\u2032 i steps forward,\ndp(Wk+i(x),W \u2217(x)) \u2264\n\u03b3idp(S k 0Wk(X \u2032\u2032), Sk0W \u2217(X \u2032\u2032)) +\n\u03b4B\n1\u2212 \u03b3\n\u2264 \u03b3iB + \u03b4B 1\u2212 \u03b3 .\nHence for any x \u2208 X , > 0, we can take \u03b4, i, and finally k large enough to make dp(Wk(x),W \u2217(x)) < . The proof then extends to Zk(x, a) by considering one additional application of T . We now consider the more general case where there are multiple optimal policies. We expand the definition of Xk,i as follows:\nXk,i := { x \u2208 Xk : \u2200\u03c0\u2217 \u2208 \u03a0\u2217, E\na\u2217\u223c\u03c0\u2217(x) P (Xk\u22121,i\u22121 |x, a\u2217) \u2265 1\u2212\u03b4\n} ,\nBecause there are finitely many actions, Lemma 6 also holds for this new definition. As before, take x \u2208 Xk,i, but now consider the sequence of greedy policies \u03c0k, \u03c0k\u22121, . . . selected by successive applications of T , and write\nT \u03c0\u0304k := T \u03c0kT \u03c0k\u22121 \u00b7 \u00b7 \u00b7 T \u03c0k\u2212i+1 ,\nsuch that Zk+1 = T \u03c0\u0304kZk\u2212i+1.\nNow denote by Z\u2217\u2217 the set of nonstationary optimal policies. If we take any Z\u2217 \u2208 Z\u2217, we deduce that\ninf Z\u2217\u2217\u2208Z\u2217\u2217\ndp(T \u03c0\u0304kZ\u2217(x, a), Z\u2217\u2217(x, a)) \u2264 \u03b4B\n1\u2212 \u03b3 ,\nsince Z\u2217 corresponds to some optimal policy \u03c0\u2217 and \u03c0\u0304k is optimal along most of the trajectories from (x, a). In effect, T \u03c0\u0304kZ\u2217 is close to the value distribution of the nonstationary optimal policy \u03c0\u0304k\u03c0\u2217. Now for this Z\u2217,\ninf Z\u2217\u2217\ndp(Zk(x, a), Z \u2217\u2217(x, a))\n\u2264 dp(Zk(x, a), T \u03c0\u0304kZ\u2217(x, a)) + inf Z\u2217\u2217 dp(T \u03c0\u0304kZ\u2217(x, a), Z\u2217\u2217(x, a))\n\u2264 dp(T \u03c0\u0304kZk\u2212i+1(x, a), T \u03c0\u0304kZ\u2217(x, a)) + \u03b4B\n1\u2212 \u03b3\n\u2264 \u03b3iB + 2\u03b4B 1\u2212 \u03b3 ,\nusing the same argument as before with the newly-defined Xk,i. It follows that\ninf Z\u2217\u2217\u2208Z\u2217\u2217\ndp(Zk(x, a), Z \u2217\u2217(x, a))\u2192 0.\nWhen X is finite, there exists a fixed k after which Xk = X . The uniform convergence result then follows. To prove the uniqueness of the fixed point Z\u2217 when T selects its actions according to the ordering \u227a, we note that for any optimal value distribution Z\u2217, its set of greedy policies is \u03a0\u2217. Denote by \u03c0\u2217 the policy coming first in the ordering over \u03a0\u2217. Then T = T \u03c0\u2217 , which has a unique fixed point (Section 3.3).\nProposition 4. That T has a fixed point Z\u2217 = T Z\u2217 is insufficient to guarantee the convergence of {Zk} to Z\u2217.\nWe provide here a sketch of the result. Consider a single state x1 with two actions, a1 and a2 (Figure 8). The first action yields a reward of 1/2, while the other either yields 0 or 1 with equal probability, and both actions are optimal. Now take \u03b3 = 1/2 and write R0, R1, . . . for the received rewards. Consider a stochastic policy that takes action a2 with probability p. For p = 0, the return is\nZp=0 = 1 1\u2212 \u03b3 1 2 = 1.\nFor p = 1, on the other hand, the return is random and is given by the following fractional number (in binary):\nZp=1 = R0.R1R2R3 \u00b7 \u00b7 \u00b7 .\nAs a result, Zp=1 is uniformly distributed between 0 and 2! In fact, note that\nZp=0 = 0.11111 \u00b7 \u00b7 \u00b7 = 1.\nFor some intermediary value of p, we obtain a different probability of the different digits, but always putting some probability mass on all returns in [0, 2].\nNow suppose we follow the nonstationary policy that takes a1 on the first step, then a2 from there on. By inspection, the return will be uniformly distributed on the interval [1/2, 3/2], which does not correspond to the return under any value of p. But now we may imagine an operator T which alternates between a1 and a2 depending on the exact value distribution it is applied to, which would in turn converge to a nonstationary optimal value distribution.\nLemma 7 (Sample Wasserstein distance). Let {Pi} be a collection of random variables, I \u2208 N a random index independent from {Pi}, and consider the mixture random variable P = PI . For any random variable Q independent of I ,\ndp(P,Q) \u2264 E i\u223cI dp(Pi, Q),\nand in general the inequality is strict and\n\u2207Qdp(PI , Q) 6= E i\u223cI \u2207Qdp(Pi, Q).\nProof. We prove this using Lemma 1. Let Ai := I [I = i]. We write\ndp(P,Q) = dp(PI , Q)\n= dp (\u2211 i AiPi, \u2211 i AiQ ) \u2264 \u2211\ni dp(AiPi, AiQ) \u2264 \u2211\ni Pr{I = i}dp(Pi, Q)\n= EI dP (Pi, Q).\nwhere in the penultimate line we used the independence of I from Pi andQ to appeal to property P3 of the Wasserstein metric.\nTo show that the bound is in general strict, consider the mixture distribution depicted in Figure 9. We will simply\nconsider the d1 metric between this distribution P and another distribution Q. The first distribution is\nP = { 0 w.p. 1/2 1 w.p. 1/2.\nIn this example, i \u2208 {1, 2}, P1 = 0, and P2 = 1. Now consider the distribution with the same support but that puts probability p on 0:\nQ = { 0 w.p. p 1 w.p. 1\u2212 p.\nThe distance between P and Q is\nd1(P,Q) = |p\u2212 12 |.\nThis is d1(P,Q) = 12 for p \u2208 {0, 1}, and strictly less than 1 2 for any other values of p. On the other hand, the corresponding expected distance (after sampling an outcome x1 or x2 with equal probability) is\nEI d1(Pi, Q) = 12p+ 1 2 (1\u2212 p) = 12 .\nHence d1(P,Q) < EI d1(Pi, Q) for p \u2208 (0, 1). This shows that the bound is in general strict. By inspection, it is clear that the two gradients are different.\nProposition 5. Fix some next-state distribution Z and policy \u03c0. Consider a parametric value distribution Z\u03b8, and and define the Wasserstein loss\nLW (\u03b8) := dp(Z\u03b8(x, a), R(x, a) + \u03b3Z(X \u2032, \u03c0(X \u2032))).\nLet r \u223c R(x, a) and x\u2032 \u223c P (\u00b7 |x, a) and consider the sample loss\nLW (\u03b8, r, x \u2032) := dp(Z\u03b8(x, a), r + \u03b3Z(x \u2032, \u03c0(x\u2032)).\nIts expectation is an upper bound on the loss LW :\nLW (\u03b8) \u2264 E R,P LW (\u03b8, r, x \u2032),\nin general with strict inequality.\nThe result follows directly from the previous lemma."}, {"heading": "C. Algorithmic Details", "text": "While our training regime closely follows that of DQN (Mnih et al., 2015), we use Adam (Kingma & Ba, 2015) instead of RMSProp (Tieleman & Hinton, 2012) for gradient rescaling. We also performed some hyperparameter tuning for our final results. Specifically, we evaluated two hyperparameters over our five training games and choose the values that performed best. The hyperparameter values we considered were VMAX \u2208 {3, 10, 100} and adam \u2208 {1/L, 0.1/L, 0.01/L, 0.001/L, 0.0001/L}, where L = 32 is the minibatch size. We found VMAX = 10 and adam = 0.01/L performed best. We used the same step-size value as DQN (\u03b1 = 0.00025).\nPseudo-code for the categorical algorithm is given in Algorithm 1. We apply the Bellman update to each atom separately, and then project it into the two nearest atoms in the original support. Transitions to a terminal state are handled with \u03b3t = 0."}, {"heading": "D. Comparison of Sampled Wasserstein Loss and Categorical Projection", "text": "Lemma 3 proves that for a fixed policy \u03c0 the distributional Bellman operator is a \u03b3-contraction in d\u0304p, and therefore that T \u03c0 will converge in distribution to the true distribution of returns Z\u03c0 . In this section, we empirically validate these results on the CliffWalk domain shown in Figure 11. The dynamics of the problem match those given by Sutton & Barto (1998). We also study the convergence of the distributional Bellman operator under the sampled Wasserstein loss and the categorical projection (Equation 7) while fol-\nlowing a policy that tries to take the safe path but has a 10% chance of taking another action uniformly at random.\nWe compute a ground-truth distribution of returnsZ\u03c0 using 10000 Monte-Carlo (MC) rollouts from each state. We then perform two experiments, approximating the value distribution at each state with our discrete distributions.\nIn the first experiment, we perform supervised learning using either the Wasserstein loss or categorical projection (Equation 7) with cross-entropy loss. We use Z\u03c0 as the supervised target and perform 5000 sweeps over all states to ensure both approaches have converged. In the second experiment, we use the same loss functions, but the training target comes from the one-step distributional Bellman operator with sampled transitions. We use VMIN = \u2212100 and VMAX = \u22121.4 For the sample updates we perform 10 times as many sweeps over the state space. Fundamentally, these experiments investigate how well the two training regimes\n4Because there is a small probability of larger negative returns, some approximation error is unavoidable. However, this effect is relatively negligible in our experiments.\n(minimizing the Wasserstein or categorical loss) minimize the Wasserstein metric under both ideal (supervised target) and practical (sampled one-step Bellman target) conditions.\nIn Figure 10a we show the final Wasserstein distance d1(Z\n\u03c0, Z\u03b8) between the learned distributions and the ground-truth distribution as we vary the number of atoms. The graph shows that the categorical algorithm does indeed minimize the Wasserstein metric in both the supervised and sample Bellman setting. It also highlights that minimizing the Wasserstein loss with stochastic gradient descent is in general flawed, confirming the intuition given by Proposition 5. In repeat experiments the process converged to different values of d1(Z\u03c0, Z\u03b8), suggesting the presence of local minima (more prevalent with fewer atoms).\nFigure 10 provides additional insight into why the sampled Wasserstein distance may perform poorly. Here, we see the cumulative densities for the approximations learned under these two losses for five different states along the safe path in CliffWalk. The Wasserstein has converged to a fixedpoint distribution, but not one that captures the true (Monte Carlo) distribution very well. By comparison, the categorical algorithm captures the variance of the true distribution much more accurately."}, {"heading": "E. Supplemental Videos and Results", "text": "In Figure 13 we provide links to supplemental videos showing the C51 agent during training on various Atari 2600 games. Figure 12 shows the relative performance of C51 over the course of training. Figure 14 provides a table of evaluation results, comparing C51 to other state-of-theart agents. Figures 15\u201318 depict particularly interesting frames."}], "references": [{"title": "On the sample complexity of reinforcement learning with a generative model", "author": ["Azar", "Mohammad Gheshlaghi", "Munos", "R\u00e9mi", "Kappen", "Hilbert"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Azar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "The cramer distance as a solution to biased wasserstein gradients", "author": ["Bellemare", "Marc G", "Danihelka", "Ivo", "Dabney", "Will", "Mohamed", "Shakir", "Lakshminarayanan", "Balaji", "Hoyer", "Stephan", "Munos", "R\u00e9mi"], "venue": null, "citeRegEx": "Bellemare et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2017}, {"title": "Dynamic programming", "author": ["Bellman", "Richard E"], "venue": null, "citeRegEx": "Bellman and E.,? \\Q1957\\E", "shortCiteRegEx": "Bellman and E.", "year": 1957}, {"title": "Some asymptotic theory for the bootstrap", "author": ["Bickel", "Peter J", "Freedman", "David A"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 1981}, {"title": "Probability and measure", "author": ["Billingsley", "Patrick"], "venue": null, "citeRegEx": "Billingsley and Patrick.,? \\Q1995\\E", "shortCiteRegEx": "Billingsley and Patrick.", "year": 1995}, {"title": "Multitask learning", "author": ["Caruana", "Rich"], "venue": "Machine Learning,", "citeRegEx": "Caruana and Rich.,? \\Q1997\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1997}, {"title": "Discounted mdps: Distribution functions and exponential utility maximization", "author": ["Chung", "Kun-Jen", "Sobel", "Matthew J"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Chung et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Chung et al\\.", "year": 1987}, {"title": "Bayesian Q-learning", "author": ["Dearden", "Richard", "Friedman", "Nir", "Russell", "Stuart"], "venue": "In Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Reinforcement learning with gaussian processes", "author": ["Engel", "Yaakov", "Mannor", "Shie", "Meir", "Ron"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Engel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Engel et al\\.", "year": 2005}, {"title": "Kalman temporal differences", "author": ["Geist", "Matthieu", "Pietquin", "Olivier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Geist et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Geist et al\\.", "year": 2010}, {"title": "Stable function approximation in dynamic programming", "author": ["Gordon", "Geoffrey"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Gordon and Geoffrey.,? \\Q1995\\E", "shortCiteRegEx": "Gordon and Geoffrey.", "year": 1995}, {"title": "Q(\u03bb) with off-policy corrections", "author": ["Harutyunyan", "Anna", "Bellemare", "Marc G", "Stepleton", "Tom", "Munos", "R\u00e9mi"], "venue": "In Proceedings of the Conference on Algorithmic Learning", "citeRegEx": "Harutyunyan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "An expectation maximization algorithm for continuous markov decision processes with arbitrary reward", "author": ["Hoffman", "Matthew D", "de Freitas", "Nando", "Doucet", "Arnaud", "Peters", "Jan"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Hoffman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2009}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Markov decision processes with a new optimality criterion: Discrete time", "author": ["Jaquette", "Stratton C"], "venue": "The Annals of Statistics,", "citeRegEx": "Jaquette and C.,? \\Q1973\\E", "shortCiteRegEx": "Jaquette and C.", "year": 1973}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "PAC bounds for discounted MDPs", "author": ["Lattimore", "Tor", "Hutter", "Marcus"], "venue": "In Proceedings of the Conference on Algorithmic Learning Theory,", "citeRegEx": "Lattimore et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2012}, {"title": "Mean-variance optimization in markov decision processes", "author": ["Mannor", "Shie", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Mannor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2011}, {"title": "Reinforcement learning with selective perception and hidden state", "author": ["McCallum", "Andrew K"], "venue": "PhD thesis, University of Rochester,", "citeRegEx": "McCallum and K.,? \\Q1995\\E", "shortCiteRegEx": "McCallum and K.", "year": 1995}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Parametric return density estimation for reinforcement learning", "author": ["Morimura", "Tetsuro", "Hachiya", "Hirotaka", "Sugiyama", "Masashi", "Tanaka", "Toshiyuki", "Kashima", "Hisashi"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Morimura et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Morimura et al\\.", "year": 2010}, {"title": "Nonparametric return distribution approximation for reinforcement learning", "author": ["Morimura", "Tetsuro", "Sugiyama", "Masashi", "Kashima", "Hisashi", "Hachiya", "Hirotaka", "Tanaka", "Toshiyuki"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Morimura et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Morimura et al\\.", "year": 2010}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Nair", "Arun", "Srinivasan", "Praveen", "Blackwell", "Sam", "Alcicek", "Cagdas", "Fearon", "Rory", "De Maria", "Alessandro", "Panneershelvam", "Vedavyas", "Suleyman", "Mustafa", "Beattie", "Charles", "Petersen", "Stig"], "venue": "In ICML Workshop on Deep Learning,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Actor-critic algorithms for risk-sensitive mdps", "author": ["LA Prashanth", "Ghavamzadeh", "Mohammad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Prashanth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prashanth et al\\.", "year": 2013}, {"title": "A fixed point theorem for distributions", "author": ["R\u00f6sler", "Uwe"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "R\u00f6sler and Uwe.,? \\Q1992\\E", "shortCiteRegEx": "R\u00f6sler and Uwe.", "year": 1992}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "The variance of discounted markov decision processes", "author": ["Sobel", "Matthew J"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel and J.,? \\Q1982\\E", "shortCiteRegEx": "Sobel and J.", "year": 1982}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "In Proceedings of the International Conference on Autonomous Agents and Multiagents Systems,", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Learning the variance of the reward-to-go", "author": ["Tamar", "Aviv", "Di Castro", "Dotan", "Mannor", "Shie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural networks for machine learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Toussaint", "Marc", "Storkey", "Amos"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Toussaint et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toussaint et al\\.", "year": 2006}, {"title": "On the convergence of optimistic policy iteration", "author": ["Tsitsiklis", "John N"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsitsiklis and N.,? \\Q2002\\E", "shortCiteRegEx": "Tsitsiklis and N.", "year": 2002}, {"title": "Pixel recurrent neural networks", "author": ["Van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Compress and control", "author": ["Veness", "Joel", "Bellemare", "Marc G", "Hutter", "Marcus", "Chua", "Alvin", "Desjardins", "Guillaume"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "citeRegEx": "Veness et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "Dual representations for dynamic programming", "author": ["Wang", "Tao", "Lizotte", "Daniel", "Bowling", "Michael", "Schuurmans", "Dale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Wang", "Ziyu", "Schaul", "Tom", "Hessel", "Matteo", "Hasselt", "Hado van", "Lanctot", "Marc", "de Freitas", "Nando"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Mean, variance, and probabilistic criteria in finite markov decision processes: a review", "author": ["D.J. White"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "White,? \\Q1988\\E", "shortCiteRegEx": "White", "year": 1988}, {"title": "embeds the return into a graphical model, and applies probabilistic inference", "author": ["Hoffman"], "venue": null, "citeRegEx": "Hoffman,? \\Q2009\\E", "shortCiteRegEx": "Hoffman", "year": 2009}, {"title": "2015) instead of RMSProp (Tieleman & Hinton, 2012) for gradient rescaling", "author": ["Adam (Kingma", "Ba"], "venue": "(Mnih et al.,", "citeRegEx": ".Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": ".Kingma and Ba", "year": 2015}], "referenceMentions": [{"referenceID": 40, "context": "Although the distributional perspective is almost as old as Bellman\u2019s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric uncertainty (Dearden et al.", "startOffset": 86, "endOffset": 128}, {"referenceID": 8, "context": "Although the distributional perspective is almost as old as Bellman\u2019s equation itself (Jaquette, 1973; Sobel, 1982; White, 1988), in reinforcement learning it has thus far been subordinated to specific purposes: to model parametric uncertainty (Dearden et al., 1998), to design risk-sensitive algorithms (Morimura et al.", "startOffset": 244, "endOffset": 266}, {"referenceID": 0, "context": ", 2010b;a), or for theoretical analysis (Azar et al., 2012; Lattimore & Hutter, 2012).", "startOffset": 40, "endOffset": 85}, {"referenceID": 0, "context": ", 2010b;a), or for theoretical analysis (Azar et al., 2012; Lattimore & Hutter, 2012). By contrast, we believe the value distribution has a central role to play in reinforcement learning. Contraction of the policy evaluation Bellman operator. Basing ourselves on results by R\u00f6sler (1992) we show that, for a fixed policy, the Bellman operator over value distributions is a contraction in a maximal form of the Wasserstein (also called Kantorovich or Mallows) metric.", "startOffset": 41, "endOffset": 288}, {"referenceID": 1, "context": "We will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 126, "endOffset": 150}, {"referenceID": 21, "context": "By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games.", "startOffset": 55, "endOffset": 74}, {"referenceID": 1, "context": "We will illustrate the practical benefits of the distributional perspective in the context of the Arcade Learning Environment (Bellemare et al., 2013). By modelling the value distribution within a DQN agent (Mnih et al., 2015), we obtain considerably increased performance across the gamut of benchmark Atari 2600 games, and in fact achieve stateof-the-art performance on a number of games. Our results echo those of Veness et al. (2015), who obtained extremely fast learning by predicting Monte Carlo returns.", "startOffset": 127, "endOffset": 438}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al. (2010a) among others, and in operations research by White (1988).", "startOffset": 69, "endOffset": 114}, {"referenceID": 9, "context": "Distributional equations have been used in reinforcement learning by Engel et al. (2005); Morimura et al. (2010a) among others, and in operations research by White (1988).", "startOffset": 69, "endOffset": 171}, {"referenceID": 12, "context": "Tsitsiklis (2002) and most recently Harutyunyan et al. (2016). Let \u03a0\u2217 be the set of optimal policies.", "startOffset": 36, "endOffset": 62}, {"referenceID": 31, "context": "Although the Gaussian case has previously been considered (Morimura et al., 2010a; Tamar et al., 2016), to the best of our knowledge we are the first to use a rich class of parametric distributions.", "startOffset": 58, "endOffset": 102}, {"referenceID": 2, "context": "We note that, while these algorithms appear unrelated to the Wasserstein metric, recent work (Bellemare et al., 2017) hints at a deeper connection.", "startOffset": 93, "endOffset": 117}, {"referenceID": 1, "context": "cade Learning Environment (ALE; Bellemare et al., 2013).", "startOffset": 26, "endOffset": 55}, {"referenceID": 21, "context": "For our study, we use the DQN architecture (Mnih et al., 2015), but output the atom probabilities pi(x, a) instead of action-values, and chose VMAX = \u2212VMIN = 10 from preliminary experiments over the training games.", "startOffset": 43, "endOffset": 62}, {"referenceID": 39, "context": ", 2016), the Dueling architecture (Wang et al., 2016), and Prioritized Replay (Schaul et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 27, "context": ", 2016), and Prioritized Replay (Schaul et al., 2016), comparing the best evaluation score achieved during training.", "startOffset": 32, "endOffset": 53}, {"referenceID": 14, "context": "We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri\u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning (Jaderberg et al., 2017).", "startOffset": 270, "endOffset": 294}, {"referenceID": 30, "context": "A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions (Caruana 1997; Utgoff & Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017).", "startOffset": 110, "endOffset": 192}, {"referenceID": 14, "context": "A recurring theme in artificial intelligence is the idea of an agent learning from a multitude of predictions (Caruana 1997; Utgoff & Stracuzzi 2002; Sutton et al. 2011; Jaderberg et al. 2017).", "startOffset": 110, "endOffset": 192}, {"referenceID": 14, "context": "We believe the gradient-based categorical algorithm is able to mitigate these effects by effectively averaging the different distri\u2020 The UNREAL results are not altogether comparable, as they were generated in the asynchronous setting with per-game hyperparameter tuning (Jaderberg et al., 2017). butions, similar to conservative policy iteration (Kakade & Langford, 2002). While the chattering persists, it is integrated to the approximate solution. State aliasing. Even in a deterministic environment, state aliasing may result in effective stochasticity. McCallum (1995), for example, showed the importance of coupling representation learning with policy learning in partially observable domains.", "startOffset": 271, "endOffset": 573}], "year": 2017, "abstractText": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman\u2019s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "creator": "LaTeX with hyperref package"}}}