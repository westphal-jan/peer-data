{"id": "1707.08052", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2017", "title": "Challenges in Data-to-Document Generation", "abstract": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements. In this paper, we focus on the potential for new approaches that integrate new algorithms to enhance accuracy, while also highlighting the potential for improved processing times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 25 Jul 2017 15:42:25 GMT  (42kb)", "http://arxiv.org/abs/1707.08052v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sam wiseman", "stuart m shieber", "alexander m rush"], "accepted": true, "id": "1707.08052"}, "pdf": {"name": "1707.08052.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["swiseman@seas.harvard.edu", "shieber@seas.harvard.edu", "srush@seas.harvard.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 7.\n08 05\n2v 1\n[ cs\n.C L\n] 2\n5 Ju\nl 2 01\n7\nicant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate humangenerated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstructionbased extensions lead to noticeable improvements."}, {"heading": "1 Introduction", "text": "Over the past several years, neural text generation systems have shown impressive performance on tasks such as machine translation and summarization. As neural systems begin to move toward generating longer outputs in response to longer and more complicated inputs, however, the generated texts begin to display reference errors, intersentence incoherence, and a lack of fidelity to the source material. The goal of this paper is to suggest a particular, long-form generation task in which these challenges may be fruitfully explored, to provide a publically available dataset for this task, to suggest some automatic evaluation metrics, and finally to establish how current, neural\ntext generation methods perform on this task.\nA classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output. Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014). Traditionally, these two challenges have been modularized and handled separately by generation systems. However, neural generation systems, which are typically trained end-to-end as conditional language models (Mikolov et al., 2010; Sutskever et al., 2011, 2014), blur this distinction.\nIn this context, we believe the problem of generating multi-sentence summaries of tables or database records to be a reasonable next-problem for neural techniques to tackle as they begin to consider more difficult NLG tasks. In particular, we would like this generation task to have the following two properties: (1) it is relatively easy to obtain fairly clean summaries and their corresponding databases for dataset construction, and (2) the summaries should be primarily focused on conveying the information in the database. This latter property ensures that the task is somewhat congenial to a standard encoder-decoder approach, and, more importantly, that it is reasonable to evaluate generations in terms of their fidelity to the database.\nOne task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a\nlong history of NLG work that generates sports game summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005). To this end, we make the following contributions:\n\u2022 We introduce a new large-scale corpus consisting of textual descriptions of basketball\ngames paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient.\n\u2022 We introduce a series of extractive evaluation models to automatically evaluate output\ngeneration performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself.\n\u2022 We apply a series of state-of-the-art neural methods, as well as a simple templated gener-\nation system, to our data-to-document generation task in order to establish baselines and study their generations.\nOur experiments indicate that neural systems are quite good at producing fluent outputs and generally score well on standard word-match metrics, but perform quite poorly at content selection and at capturing long-term structure. While the use of copy-based models and additional reconstruction terms in the training loss can lead to improvements in BLEU and in our proposed extractive evaluations, current models are still quite far from producing human-level output, and are significantly worse than templated systems in terms of content selection and realization. Overall, we believe this problem of data-to-document generation highlights important remaining challenges in neural generation systems, and the use of extractive evaluation reveals significant issues hidden by standard automatic metrics."}, {"heading": "2 Data-to-Text Datasets", "text": "We consider the problem of generating descriptive text from database records. Following the notation in Liang et al. (2009), let s = {rj} J j=1 be a set of records, where for each r\u2208 s we define r.t\u2208T to be the type of r, and we assume each r to be a binarized relation, where r.e and r.m are a record\u2019s entity and value, respectively. For example, a database recording statistics for a basketball game might have a record r such that r.t = POINTS, r.e = RUSSELL WESTBROOK, and r.m = 50. In this case, r.e gives the player in question, and r.m\ngives the number of points the player scored. From these records, we are interested in generating descriptive text, y\u03021:T = y\u03021, . . . , y\u0302T of T words such that y\u03021:T is an adequate and fluent summary of s. A dataset for training data-to-document systems typically consists of (s, y1:T ) pairs, where y1:T is a document consisting of a gold (i.e., human generated) summary for database s.\nSeveral benchmark datasets have been used in recent years for the text generation task, the most popular of these being WEATHERGOV (Liang et al., 2009) and ROBOCUP (Chen and Mooney, 2008). Recently, neural generation systems have show strong results on these datasets, with the system of Mei et al. (2016) achieving BLEU scores in the 60s and 70s on WEATHERGOV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset. These results are quite promising, and suggest that neural models are a good fit for text generation. However, the statistics of these datasets, shown in Table 1, indicate that these datasets use relatively simple language and record structure. Furthermore, there is reason to believe that WEATHERGOV is at least partially machine-generated (Reiter, 2017). More recently, Lebret et al. (2016) introduced the WIKIBIO dataset, which is at least an order of magnitude larger in terms of number of tokens and record types. However, as shown in Table 1, this dataset too only contains short (single-sentence) generations, and relatively few records per generation. As such, we believe that early success on these datasets is not yet sufficient for testing the desired linguistic capabilities of text generation at a document-scale.\nWith this challenge in mind, we introduce a new dataset for data-to-document text generation, available at https://github.com/harvar dnlp/boxscore-data. The dataset is intended to be comparable to WEATHERGOV in terms of token count, but to have significantly longer target texts, a larger vocabulary space, and to require more difficult content selection.\nThe dataset consists of two sources of articles summarizing NBA basketball games, paired with their corresponding box- and line-score tables. The data statistics of these two sources, ROTOWIRE and SBNATION, are also shown in Table 1. The first dataset, ROTOWIRE, uses professionally written, medium length game summaries\ntargeted at fantasy basketball fans. The writing is colloquial, but relatively well structured, and targets an audience primarily interested in game statistics. The second dataset, SBNATION, uses fan-written summaries targeted at other fans. This dataset is significantly larger, but also much more challenging, as the language is very informal, and often tangential to the statistics themselves. We show some sample text from ROTOWIRE in Figure 1. Our primary focus will be on the ROTOWIRE data."}, {"heading": "3 Evaluating Document Generation", "text": "We begin by discussing the evaluation of generated documents, since both the task we introduce and the evaluation methods we propose are motivated by some of the shortcomings of current approaches to evaluation. Text generation systems\nare typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation. While BLEU is perhaps a reasonably effective way of evaluating short-form text generation, we found it to be unsatisfactory for document generation. In particular, we note that it primarily rewards fluent text generation, rather than generations that capture the most important information in the database, or that report the information in a particularly coherent way. While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics. Furthermore, we believe that current text generations are sufficiently bad in sufficiently obvious ways that automatic metrics can still be of use in evaluation, and we are not yet at the point of needing to rely solely on human evaluators."}, {"heading": "3.1 Extractive Evaluation", "text": "To address this evaluation challenge, we begin with the intuition that assessing document quality is easier than document generation. In particular, it is much easier to automatically extract information from documents than to generate documents that accurately convey desired information. As such, simple, high-precision information extraction models can serve as the basis for assessing and better understanding the quality of automatic\ngenerations. We emphasize that such an evaluation scheme is most appropriate when evaluating generations (such as basketball game summaries) that are primarily intended to summarize information. While many generation problems do not fall into this category, we believe this to be an interesting category, and one worth focusing on because it is amenable to this sort of evaluation.\nTo see how a simple information extraction system might work, consider the document in Figure 1. We may first extract candidate entity (player, team, and city) and value (number and certain string) pairs r.e, r.m that appear in the text, and then predict the type r.t (or none) of each candidate pair. For example, we might extract the entity-value pair (\u201cMiami Heat\u201d, \u201c95\u201d) from the first sentence in Figure 1, and then predict that the type of this pair is POINTS, giving us an extracted record r such that (r.e, r.m, r.t) = (MIAMI HEAT, 95, POINTS). Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015).\nMore concretely, given a document y\u03021:T , we consider all pairs of word-spans in each sentence that represent possible entities e and values m. We then model p(r.t | e,m;\u03b8) for each pair, using r.t = \u01eb to indicate unrelated pairs. We use architectures similar to those discussed in Collobert et al. (2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix.\nImportantly, we note that the (s, y1:T ) pairs typically used for training data-to-document systems are also sufficient for training the information extraction model presented above, since we can obtain (partial) supervision by simply checking whether a candidate record lexically matches a record in s.1 However, since there may be multiple records r\u2208 s with the same e andm but with different types r.t, we will not always be able to determine the type of a given entity-value pair found in the text. We therefore train our classifier to minimize a latent-variable loss: for all document spans e and m, with observed types t(e,m) = {r.t : r\u2208 s, r.e= e, r.m=m} (possi-\n1Alternative approaches explicitly align the document with the table for this task (Liang et al., 2009).\nbly {\u01eb}), we minimize\nL(\u03b8) = \u2212 \u2211\ne,m\nlog \u2211\nt\u2032\u2208t(e,m)\np(r.t = t\u2032 | e,m;\u03b8).\nWe find that this simple system trained in this way is quite accurate at predicting relations. On the ROTOWIRE data it achieves over 90% accuracy on held-out data, and recalls approximately 60% of the relations licensed by the records."}, {"heading": "3.2 Comparing Generations", "text": "With a sufficiently precise relation extraction system, we can begin to evaluate how well an automatic generation y\u03021:T has captured the information in a set of records s. In particular, since the predictions of a precise information extraction system serve to align entity-mention pairs in the text with database records, this alignment can be used both to evaluate a generation\u2019s content selection (\u201cwhat the generation says\u201d), as well as content placement (\u201chow the generation says it\u201d).\nWe consider in particular three induced metrics:\n\u2022 Content Selection (CS): precision and recall of unique relations r extracted from\ny\u03021:T that are also extracted from y1:T . This measures how well the generated document matches the gold document in terms of selecting which records to generate.\n\u2022 Relation Generation (RG): precision and number of unique relations r extracted from\ny\u03021:T that also appear in s. This measures how well the system is able to generate text containing factual (i.e., correct) records.\n\u2022 Content Ordering (CO): normalized Damerau-Levenshtein Dis-\ntance (Brill and Moore, 2000)2 between the sequences of records extracted from y1:T and that extracted from y\u03021:T . This measures how well the system orders the records it chooses to discuss.\nWe note that CS primarily targets the \u201cwhat to say\u201d aspect of evaluation, CO targets the \u201chow to say it\u201d aspect, and RG targets both.\nWe conclude this section by contrasting the automatic evaluation we have proposed with\n2DLD is a variant of Levenshtein distance that allows transpositions of elements; it is useful in comparing the ordering of sequences that may not be permutations of the same set (which is a requirement for measures like Kendall\u2019s Tau).\nrecently proposed adversarial evaluation approaches, which also advocate automatic metrics backed by classification (Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017). Unlike adversarial evaluation, which uses a blackbox classifier to determine the quality of a generation, our metrics are defined with respect to the predictions of an information extraction system. Accordingly, our metrics are quite interpretable, since by construction it is always possible to determine which fact (i.e., entity-value pair) in the generation is determined by the extractor to not match the database or the gold generation."}, {"heading": "4 Neural Data-to-Document Models", "text": "In this section we briefly describe the neural generation methods we apply to the proposed task. As a base model we utilize the now standard attentionbased encoder-decoder model (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). We also experiment with several recent extensions to this model, including copy-based generation, and training with a source reconstruction term in the loss (in addition to the standard per-targetword loss).\nBase Model For our base model, we map each record r\u2208 s into a vector r\u0303 by first embedding r.t (e.g., POINTS), r.e (e.g., RUSSELL WESTBROOK), and r.m (e.g., 50), and then applying a 1-layer MLP (similar to Yang et al. (2016)).3 Our source data-records are then represented as s\u0303 = {r\u0303j} J j=1. Given s\u0303, we use an LSTM decoder with attention and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s. The model is trained end-to-end to minimize the negative log-likelihood of the words in the gold text y1:T given corresponding source material s.\nCopying There has been a surge of recent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; Gu\u0308lc\u0327ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016). These models typically introduce an additional binary variable zt into the per-timestep target word distribution, which indicates whether the target word\n3We also include an additional feature for whether the player is on the home- or away-team.\ny\u0302t is copied from the source or generated:\np(y\u0302t | y\u03021:t\u22121, s) = \u2211\nz\u2208{0,1}\np(y\u0302t, zt = z | y\u03021:t\u22121, s).\nIn our case, we assume that target words are copied from the value portion of a record r; that is, a copy implies y\u0302t= r.m for some r and t.\nJoint Copy Model The models of Gu et al. (2016) and Yang et al. (2016) parameterize the joint distribution table over y\u0302t and zt directly:\np(y\u0302t, zt | y\u03021:t\u22121, s) \u221d \n \n \ncopy(y\u0302t, y\u03021:t\u22121, s) zt = 1, y\u0302t \u2208 s\n0 zt = 1, y\u0302t 6\u2208 s gen(y\u0302t, y\u03021:t\u22121, s) zt = 0,\nwhere copy and gen are functions parameterized in terms of the decoder RNN\u2019s hidden state that assign scores to words, and where the notation y\u0302t \u2208 s indicates that y\u0302t is equal to r.m for some r\u2208 s.\nConditional Copy Model Gu\u0308lc\u0327ehre et al. (2016), on the other hand, decompose the joint probability as:\np(y\u0302t, zt | y\u03021:t\u22121, s) = {\npcopy(y\u0302t | zt, y\u03021:t\u22121, s) p(zt | y\u03021:t\u22121, s) zt=1\npgen(y\u0302t | zt, y\u03021:t\u22121, s) p(zt | y\u03021:t\u22121, s) zt=0,\nwhere an MLP is used to model p(zt | y\u03021:t\u22121, s).\nModels with copy-decoders may be trained to minimize the negative log marginal probability, marginalizing out the latent-variable zt (Gu et al., 2016; Yang et al., 2016; Merity et al., 2016). However, if it is known which target words yt are copied, it is possible to train with a loss that does not marginalize out the latent zt. Gu\u0308lc\u0327ehre et al. (2016), for instance, assume that any target word yt that also appears in the source is copied, and train to minimize the negative joint log-likelihood of the yt and zt.\nIn applying such a loss in our case, we again note that there may be multiple records r such that r.m appears in y\u03021:T . Accordingly, we slightly modify the pcopy portion of the loss of Gu\u0308lc\u0327ehre et al. (2016) to sum over all matched records. In particular, we model the probability of relations r \u2208 s such that r.m = yt and r.e is in the same sentence as r.m. Letting r(yt) =\n{r \u2208 s : r.m = yt, same\u2212sentence(r.e, r.m)}, we have:\npcopy(yt | zt, y1:t\u22121, s) = \u2211\nr\u2208r(yt)\np(r | zt, y1:t\u22121, s).\nWe note here that the key distinction for our purposes between the Joint Copy model and the Conditional Copy model is that the latter conditions on whether there is a copy or not, and so in pcopy the source records compete only with each other. In the Joint Copy model, however, the source records also compete with words that cannot be copied. As a result, training the Conditional Copy model with the supervised loss of Gu\u0308lc\u0327ehre et al. (2016) can be seen as training with a word-level reconstruction loss, where the decoder is trained to choose the record in s that gives rise to yt.\nReconstruction Losses Reconstruction-based techniques can also be applied at the documentor sentence-level during training. One simple approach to this problem is to utilize the hidden states of the decoder to try to reconstruct the database. A fully differentiable approach using the decoder hidden states has recently been successfully applied to neural machine translation by Tu et al. (2017). Unlike copying, this method is applied only at training, and attempts to learn decoder hidden states with broader coverage of the input data.\nIn adopting this reconstruction approach we segment the decoder hidden states ht into \u2308 T B \u2309 contiguous blocks of size at most B. Denoting a single one of these hidden state blocks as bi, we attempt to predict each field value in some record r \u2208 s from bi. We define p(r.e, r.m | bi), the probability of the entity and value in record r given bi, to be softmax(f(bi)), where f is a parameterized function of bi, which in our experiments utilize a convolutional layer followed by an MLP; full details are given in the Appendix. We further extend this idea and predictK records in s from bi, rather than one. We can train with the following reconstruction loss for a particular bi:\nL(\u03b8) = \u2212\nK \u2211\nk=1\nmin r\u2208s log pk(r | bi;\u03b8)\n= \u2212 K \u2211\nk=1\nmin r\u2208s\n\u2211\nx\u2208{e,m,t}\nlog pk(r.x | bi;\u03b8),\nwhere pk is the k\u2019th predicted distribution over records, and where we have modeled each com-\nponent of r independently. This loss attempts to make the most probable record in s given bi more probable. We found that augmenting the above loss with a term that penalizes the total variation distance (TVD) between the pk to be helpful. 4 Both L(\u03b8) and the TVD term are simply added to the standard negative log-likelihood objective at training time."}, {"heading": "5 Experimental Methods", "text": "In this section we highlight a few important details of our models and methods; full details are in the Appendix. For our ROTOWIRE models, the record encoder produces r\u0303j in R 600, and we use a 2-layer LSTM decoder with hidden states of the same size as the r\u0303j , and dot-product attention and input-feeding in the style of Luong et al. (2015). Unlike past work, we use two identically structured attention layers, one to compute the standard generation probabilities (gen or pgen), and one to produce the scores used in copy or pcopy.\nWe train the generation models using SGD and truncated BPTT (Elman, 1990; Mikolov et al., 2010), as in language modeling. That is, we split each y1:T into contiguous blocks of length 100, and backprop both the gradients with respect to the current block as well as with respect to the encoder parameters for each block.\nOur extractive evaluator consists of an ensemble of 3 single-layer convolutional and 3 singlelayer bidirectional LSTM models. The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the style of (Kim, 2014). Both models are trained with SGD.\nTemplatized Generator In addition to neural baselines, we also use a problem-specific, template-based generator. The template-based generator first emits a sentence about the teams playing in the game, using a templatized sentence taken from the training set:\nThe <team1> (<wins1>-<losses1>) de-\nfeated the <team2> (<wins2>-<losses2>)\n<pts1>-<pts2>.\n4Penalizing the TVD between the pk might be useful if, for instance, K is too large, and only a smaller number of records can be predicted from bi. We also experimented with encouraging, rather than penalizing the TVD between the pk, which might make sense if we were worried about ensuring the pk captured different records.\nThen, 6 player-specific sentences of the following form are emitted (again adapting a simple sentence from the training set):\n<player> scored <pts> points (<fgm>-\n<fga> FG, <tpm>-<tpa> 3PT, <ftm>-\n<fta> FT) to go with <reb> rebounds.\nThe 6 highest-scoring players in the game are used to fill in the above template. Finally, a typical end sentence is emitted:\nThe <team1>\u2019 next game will be at home\nagainst the Dallas Mavericks, while the\n<team2> will travel to play the Bulls.\nCode implementing all models can be found at https://github.com/harvardnlp/d ata2text. Our encoder-decoder models are based on OpenNMT (Klein et al., 2017)."}, {"heading": "6 Results", "text": "We found that all models performed quite poorly on the SBNATION data, with the best model achieving a validation perplexity of 33.34 and a BLEU score of 1.78. This poor performance is presumably attributable to the noisy quality of the SBNATION data, and the fact that many documents in the dataset focus on information not in the box- and line-scores. Accordingly, we focus on ROTOWIRE in what follows.\nThe main results for the ROTOWIRE dataset are shown in Table 2, which shows the performance of the models in Section 4 in terms of the metrics defined in Section 3.2, as well as in terms of perplexity and BLEU."}, {"heading": "6.1 Discussion", "text": "There are several interesting relationships in the development portion of Table 2. First we note that the Template model scores very poorly on BLEU, but does quite well on the extractive metrics, providing an upper-bound for how domain knowledge could help content selection and generation. All the neural models make significant improvements in terms of BLEU score, with the conditional copying with beam search performing the best, even though all the neural models achieve roughly the same perplexity.\nThe extractive metrics provide further insight into the behavior of the models. We first note that on the gold documents y1:T , the extractive model reaches 92% precision. Using the Joint\nCopy model, generation only has a record generation (RG) precision of 47% indicating that relationships are often generated incorrectly. The best Conditional Copy system improves this value to 71%, a significant improvement and potentially the cause of the improved BLEU score, but still far below gold.\nNotably, content selection (CS) and content ordering (CO) seem to have no correlation at all with BLEU. There is some improvement with CS for the conditional model or reconstruction loss, but not much change as we move to beam search. CO actually gets worse as beam search is utilized, possibly a side effect of generating more records (RG#). The fact that these scores are much worse than the simple templated model indicates that further research is needed into better copying alone for content selection and better long term content ordering models.\nTest results are consistent with development results, indicating that the Conditional Copy model is most effective at BLEU, RG, and CS, and that reconstruction is quite helpful for improving the joint model."}, {"heading": "6.2 Human Evaluation", "text": "We also undertook two human evaluation studies, using Amazon Mechanical Turk. The first study attempted to determine whether generations considered to be more precise by our metrics were also considered more precise by human raters. To accomplish this, raters were presented with a particular NBA game\u2019s box score and line score, as well as with (randomly selected) sentences from summaries generated by our different models for\nthose games. Raters were then asked to count how many facts in each sentence were supported by records in the box or line scores, and how many were contradicted. We randomly selected 20 distinct games to present to raters, and a total of 20 generated sentences per game were evaluated by raters. The left two columns of Table 3 contain the average numbers of supporting and contradicting facts per sentence as determined by the raters, for each model. We see that these results are generally in line with the RG and CS metrics, with the Conditional Copy model having the highest number of supporting facts, and the reconstruction terms significantly improving the Joint Copy models.\nUsing a Tukey HSD post-hoc analysis of an ANOVA with the number of contradicting facts as the dependent variable and the generating model and rater id as independent variables, we found significant (p < 0.01) pairwise differences in contradictory facts between the gold generations and all models except \u201cCopy+Rec+TVD,\u201d as well as a significant difference between \u201cCopy+Rec+TVD\u201d and \u201cCopy\u201d. We similarly found a significant pairwise difference between \u201cCopy+Rec+TVD\u201d and \u201cCopy\u201d for number of supporting facts.\nOur second study attempted to determine whether generated summaries differed in terms of how natural their ordering of records (as captured, for instance, by the DLD metric) is. To test this,\nwe presented raters with random summaries generated by our models and asked them to rate the naturalness of the ordering of facts in the summaries on a 1-7 Likert scale. 30 random summaries were used in this experiment, each rated 3 times by distinct raters. The average Likert ratings are shown in the rightmost column of Table 3. While it is encouraging that the gold summaries received a higher average score than the generated summaries (and that the reconstruction term again improved the Joint Copy model), a Tukey HSD analysis similar to the one presented above revealed no significant pairwise differences."}, {"heading": "6.3 Qualitative Example", "text": "Figure 2 shows a document generated by the Conditional Copy model, using a beam of size 5. This particular generation evidently has several nice\nproperties: it nicely learns the colloquial style of the text, correctly using idioms such as \u201c19 percent from deep.\u201d It is also partially accurate in its use of the records; we highlight in blue when it generates text that is licensed by a record in the associated box- and line-scores.\nAt the same time, the generation also contains major logical errors. First, there are basic copying mistakes, such as flipping the teams\u2019 win/loss records. The system also makes obvious semantic errors; for instance, it generates the phrase \u201cthe Rockets were able to out-rebound the Rockets.\u201d Finally, we see the model hallucinates factual statements, such as \u201cin front of their home crowd,\u201d which is presumably likely according to the language model, but ultimately incorrect (and not supported by anything in the box- or linescores). In practice, our proposed extractive evaluation will pick up on many errors in this passage. For instance, \u201cfour assists\u201d is an RG error, repeating the Rockets\u2019 rebounds could manifest in a lower CO score, and incorrectly indicating the win/loss records is a CS error."}, {"heading": "7 Related Work", "text": "In this section we note additional related work not noted throughout. Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).\nHistorically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007). In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).\nWithin the world of neural text generation, some recent work has focused on conditioning lan-\nguage models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoder-decoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems."}, {"heading": "8 Conclusion and Future Work", "text": "This work explores the challenges facing neural data-to-document generation by introducing a new dataset, and proposing various metrics for automatically evaluating content selection, generation, and ordering. We see that recent ideas in copying and reconstruction lead to improvements on this task, but that there is a significant gap even between these neural models and templated systems. We hope to motivate researchers to focus further on generation problems that are relevant both to content selection and surface realization, but may not be reflected clearly in the model\u2019s perplexity.\nFuture work on this task might include approaches that process or attend to the source records in a more sophisticated way, generation models that attempt to incorporate semantic or reference-related constraints, and approaches to conditioning on facts or records that are not as explicit in the box- and line-scores."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of a Google Research Award."}, {"heading": "A. Additional Dataset Details", "text": "The ROTOWIRE data covers NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively.\nThe SBNATION data covers NBA games played between 11/3/2006 and 3/26/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 7633, 1635, and 1635 summaries, respectively.\nAll numbers in the box- and line-scores (but not the summaries) are converted to integers; fractional numbers corresponding to percents are multiplied by 100 to obtain integers in [0, 100]. We show the types of records in the data in Table 4."}, {"heading": "B. Generation Model Details", "text": "Encoder For the ROTOWIRE data, a relation r is encoded into r\u0303 by embedding each of r.e, r.t, r.m and a \u201chome-or-away\u201d indicator feature in R 600, and applying a 1-layer MLP (with ReLU nonlinearity) to map the concatenation of these vectors back into R600. To initialize the decoder LSTMs, we first mean-pool over the r\u0303j by entity (giving one vector per entity), and then linearly transform the concatenation of these pooled entity-representations so that they can initialize the cells and hidden states of a 2-layer LSTMwith states also in R600. The SBNATION setup is identical, except all vectors are in R700.\nDecoder As mentioned in the body of the paper, we compute two different attention distributions (i.e., using different parameters) at each decoding step. For the Joint Copy model, one attention distribution is not normalized, and is normalized along with all the output-word probabilities.\nWithin the Conditional Copy model we compute p(zt|y\u03021:t\u22121, s) by mean-pooling the r\u0303j , concatenating them with the current (topmost) hidden state of the LSTM, and then feeding this concatenation via a 1-layer ReLU MLP with hidden dimension 600, and with a Sigmoid output layer.\nFor the reconstruction-loss, we feed blocks (of size at most 100) of the decoder\u2019s LSTM hidden states through a (Kim, 2014)-style convolutional model. We use kernels of width 3 and\n5, 200 filters, a ReLU nonlinearity, and maxover-time pooling. To create the pk, these now 400-dimensional features are then mapped via an MLP with a ReLU nonlinearity into 3 separate 200 dimensional vectors corresponding to the predicted relation\u2019s entity, value, and type, respectively. These 200 dimensional vectors are then fed through (separate) linear decoders and softmax layers in order to obtain distributions over entities, values, and types. We useK = 3 distinct pk.\nModels are trained with SGD, a learning rate of 1 (which is divided by 2 every time validation perplexity fails to decrease), and a batch size of 16. We use dropout (at a rate of 0.5) between LSTM layers and before the linear decoder.\nC. Information Extraction Details\nData To form an information extraction dataset, we first sentence-tokenize the gold summary documents y1:T using NLTK (Bird, 2006). We then determine which word-spans yi:j could represent entities (by matching against players, teams, or cities in the database), and which word-spans yk:l could represent numbers (using the open source text2num library5 to convert (strings of) number-words into numbers).6 We then consider each yi:j, yk:l pair in the same sentence, and if there is a record r in the database such that r.e= yi:j and r.m= text2num(yk:l) we annotate the yi:j, yk:l pair with the label r.t; otherwise, we give it a label of \u01eb.\nModel We predict relations by ensembling 3 convolutional models and 3 bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) models. Each model consumes the words in the sentence, which are embedded in R200, as well as the distances of each word in the sentence from both the entity-word-span and the number-word-spans (as described above), which are each embedded in R100. These vectors are concatenated (into a vector in R500) and fed into either a convolutional model or a bidirectional LSTM model.\nThe convolutional model uses 600 total filters, with 200 filters for kernels of width 2, 3, and 5, respectively, a ReLU nonlinearity, and maxpooling. These features are then mapped via a 1-\n5https://github.com/exogen/text2num 6We ignore certain particularly misleading numberwords, such as \u201dthree-point,\u201d where we should not expect a corresponding value of 3 among the records.\nlayer (ReLU) MLP into R500, which predicts one of the 39 relation types (or \u01eb) with a linear decoder layer and softmax.\nThe bidirectional LSTM model uses a single layer with 500 units in each direction, which are concatenated. The hidden states are max-pooled, and then mapped via a 1-layer (ReLU) MLP into R 700, which predicts one of the 39 relation types (or \u01eb) with a linear decoder layer and softmax."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Gabor Angeli", "Percy Liang", "Dan Klein."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512. Association for Com-", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Collective content selection for concept-to-text generation", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331\u2013338. Association for", "citeRegEx": "Barzilay and Lapata.,? 2005", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2005}, {"title": "Automatic generation of weather", "author": ["Anja Belz"], "venue": null, "citeRegEx": "Belz.,? \\Q2008\\E", "shortCiteRegEx": "Belz.", "year": 2008}, {"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": null, "citeRegEx": "Bird.,? \\Q2006\\E", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "On the properties", "author": ["danau", "Yoshua Bengio"], "venue": null, "citeRegEx": "danau and Bengio.,? \\Q2014\\E", "shortCiteRegEx": "danau and Bengio.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput., 9:1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "ACL Volume 1: Long Papers.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Speech and language processing, volume 3", "author": ["Dan Jurafsky", "James H Martin."], "venue": "Pearson London.", "citeRegEx": "Jurafsky and Martin.,? 2014", "shortCiteRegEx": "Jurafsky and Martin.", "year": 2014}, {"title": "Adversarial evaluation of dialogue models", "author": ["Anjuli Kannan", "Oriol Vinyals."], "venue": "NIPS 2016 Workshop on Adversarial Training.", "citeRegEx": "Kannan and Vinyals.,? 2016", "shortCiteRegEx": "Kannan and Vinyals.", "year": 2016}, {"title": "Generative alignment and semantic parsing for learning from ambiguous supervision", "author": ["Joohyun Kim", "Raymond J Mooney."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543\u2013551. Associ-", "citeRegEx": "Kim and Mooney.,? 2010", "shortCiteRegEx": "Kim and Mooney.", "year": 2010}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "EMNLP, pages 1746\u2013 1751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Opennmt: Open-source toolkit for neural machine translation", "author": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush."], "venue": "CoRR, abs/1701.02810.", "citeRegEx": "Klein et al\\.,? 2017", "shortCiteRegEx": "Klein et al\\.", "year": 2017}, {"title": "A global model for concept-to-text generation", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "J. Artif. Intell. Res.(JAIR), 48:305\u2013346.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Design of a knowledge-based report generator", "author": ["Karen Kukich."], "venue": "ACL, pages 145\u2013150.", "citeRegEx": "Kukich.,? 1983", "shortCiteRegEx": "Kukich.", "year": 1983}, {"title": "Neural text generation from structured data with application to the biography domain", "author": ["R\u00e9mi Lebret", "David Grangier", "Michael Auli."], "venue": "EMNLP, pages 1203\u20131213.", "citeRegEx": "Lebret et al\\.,? 2016", "shortCiteRegEx": "Lebret et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky."], "venue": "CoRR, abs/1701.06547.", "citeRegEx": "Li et al\\.,? 2017", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Learning semantic correspondences with less supervision", "author": ["Percy Liang", "Michael I Jordan", "Dan Klein."], "venue": "ACL, pages 91\u201399. Association for Computational Linguistics.", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "EMNLP,", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "A probabilistic forest-to-string model for language generation from typed lambda calculus expressions", "author": ["Wei Lu", "Hwee Tou Ng."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1611\u20131622. Asso-", "citeRegEx": "Lu and Ng.,? 2011", "shortCiteRegEx": "Lu and Ng.", "year": 2011}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 1412\u2013", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Text generation - using discourse strategies and focus constraints to generate natural language text", "author": ["Kathleen McKeown."], "venue": "Studies in natural language processing. Cambridge University Press.", "citeRegEx": "McKeown.,? 1992", "shortCiteRegEx": "McKeown.", "year": 1992}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "NAACL HLT, pages 720\u2013730.", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher."], "venue": "CoRR, abs/1609.07843.", "citeRegEx": "Merity et al\\.,? 2016", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "You need to understand your corpora! the weathergov example", "author": ["Ehud Reiter."], "venue": "https://ehudr eiter.com/2017/05/09/weathergov/.", "citeRegEx": "Reiter.,? 2017", "shortCiteRegEx": "Reiter.", "year": 2017}, {"title": "Building applied natural language generation systems", "author": ["Ehud Reiter", "Robert Dale."], "venue": "Natural Language Engineering, 3(1):57\u201387.", "citeRegEx": "Reiter and Dale.,? 1997", "shortCiteRegEx": "Reiter and Dale.", "year": 1997}, {"title": "Choosing words in computergenerated weather forecasts", "author": ["Ehud Reiter", "Somayajulu Sripada", "Jim Hunter", "Jin Yu", "Ian Davy."], "venue": "Artificial Intelligence, 167(1-2):137\u2013169.", "citeRegEx": "Reiter et al\\.,? 2005", "shortCiteRegEx": "Reiter et al\\.", "year": 2005}, {"title": "Revision-based generation of Natural Language Summaries providing historical Background", "author": ["Jacques Robin."], "venue": "Ph.D. thesis, Citeseer.", "citeRegEx": "Robin.,? 1994", "shortCiteRegEx": "Robin.", "year": 1994}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bing Xiang", "Bowen Zhou"], "venue": "In ACL,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Stochastic language generation using widl-expressions and its application in machine translation and summarization", "author": ["Radu Soricut", "Daniel Marcu."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual", "citeRegEx": "Soricut and Marcu.,? 2006", "shortCiteRegEx": "Soricut and Marcu.", "year": 2006}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reactive content selection in the generation of real-time soccer commentary", "author": ["Kumiko Tanaka-Ishii", "K\u00f4iti Hasida", "Itsuki Noda."], "venue": "COLING-ACL, pages 1282\u20131288.", "citeRegEx": "Tanaka.Ishii et al\\.,? 1998", "shortCiteRegEx": "Tanaka.Ishii et al\\.", "year": 1998}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu", "Hang Li."], "venue": "AAAI, pages 3097\u20133103.", "citeRegEx": "Tu et al\\.,? 2017", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "Towards broad coverage surface realization with ccg", "author": ["Michael White", "Rajakrishnan Rajkumar", "Scott Martin."], "venue": "Proceedings of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+ MT), pages 267\u2013", "citeRegEx": "White et al\\.,? 2007", "shortCiteRegEx": "White et al\\.", "year": 2007}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Yuk Wah Wong", "Raymond J Mooney."], "venue": "HLT-NAACL, pages 172\u2013179.", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Reference-aware language models", "author": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling."], "venue": "CoRR, abs/1611.01628.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Relation classification via convolutional deep neural network", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Weakly-supervised relation classification for information extraction", "author": ["Zhu Zhang."], "venue": "Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 581\u2013 588. ACM.", "citeRegEx": "Zhang.,? 2004", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Semi-supervised learning for relation extraction", "author": ["GuoDong Zhou", "JunHui Li", "LongHua Qian", "Qiaoming Zhu."], "venue": "Third International Joint Conference on Natural Language Processing, page 32.", "citeRegEx": "Zhou et al\\.,? 2008", "shortCiteRegEx": "Zhou et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output.", "startOffset": 55, "endOffset": 107}, {"referenceID": 21, "context": "A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output.", "startOffset": 55, "endOffset": 107}, {"referenceID": 27, "context": "A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output.", "startOffset": 55, "endOffset": 107}, {"referenceID": 27, "context": "Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014).", "startOffset": 336, "endOffset": 386}, {"referenceID": 8, "context": "Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: what to say, the selection of an appropriate subset of the input data to discuss, and how to say it, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014).", "startOffset": 336, "endOffset": 386}, {"referenceID": 24, "context": "However, neural generation systems, which are typically trained end-to-end as conditional language models (Mikolov et al., 2010; Sutskever et al., 2011, 2014), blur this distinction.", "startOffset": 106, "endOffset": 158}, {"referenceID": 29, "context": "long history of NLG work that generates sports game summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 62, "endOffset": 129}, {"referenceID": 34, "context": "long history of NLG work that generates sports game summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 62, "endOffset": 129}, {"referenceID": 2, "context": "long history of NLG work that generates sports game summaries (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 62, "endOffset": 129}, {"referenceID": 17, "context": "Following the notation in Liang et al. (2009), let s = {rj} J j=1 be a set of records, where for each r\u2208 s we define r.", "startOffset": 26, "endOffset": 46}, {"referenceID": 17, "context": "Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being WEATHERGOV (Liang et al., 2009) and ROBOCUP (Chen and Mooney, 2008).", "startOffset": 131, "endOffset": 151}, {"referenceID": 26, "context": "Furthermore, there is reason to believe that WEATHERGOV is at least partially machine-generated (Reiter, 2017).", "startOffset": 96, "endOffset": 110}, {"referenceID": 16, "context": "Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being WEATHERGOV (Liang et al., 2009) and ROBOCUP (Chen and Mooney, 2008). Recently, neural generation systems have show strong results on these datasets, with the system of Mei et al. (2016) achieving BLEU scores in the 60s and 70s on WEATHERGOV, and BLEU scores of almost 30 even on the smaller ROBOCUP dataset.", "startOffset": 132, "endOffset": 306}, {"referenceID": 15, "context": "More recently, Lebret et al. (2016) introduced the WIKIBIO dataset, which is at least an order of magnitude larger in terms of number of tokens and record types.", "startOffset": 15, "endOffset": 36}, {"referenceID": 25, "context": "Text generation systems are typically evaluated using a combination of automatic measures, such as BLEU (Papineni et al., 2002), and human evaluation.", "startOffset": 104, "endOffset": 127}, {"referenceID": 18, "context": "While human evaluation, on the other hand, is likely ultimately necessary for evaluating generations (Liu et al., 2016; Wu et al., 2016), it is much less convenient than using automatic metrics.", "startOffset": 101, "endOffset": 136}, {"referenceID": 40, "context": "Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 41, "context": "Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 39, "context": "Indeed, many relation extraction systems reduce relation extraction to multi-class classification precisely in this way (Zhang, 2004; Zhou et al., 2008; Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 120, "endOffset": 196}, {"referenceID": 30, "context": "(2011) and dos Santos et al. (2015) to parameterize this probability; full details are given in the Appendix.", "startOffset": 15, "endOffset": 36}, {"referenceID": 17, "context": "Alternative approaches explicitly align the document with the table for this task (Liang et al., 2009).", "startOffset": 82, "endOffset": 102}, {"referenceID": 9, "context": "recently proposed adversarial evaluation approaches, which also advocate automatic metrics backed by classification (Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017).", "startOffset": 116, "endOffset": 180}, {"referenceID": 16, "context": "recently proposed adversarial evaluation approaches, which also advocate automatic metrics backed by classification (Bowman et al., 2016; Kannan and Vinyals, 2016; Li et al., 2017).", "startOffset": 116, "endOffset": 180}, {"referenceID": 33, "context": "As a base model we utilize the now standard attentionbased encoder-decoder model (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 81, "endOffset": 146}, {"referenceID": 1, "context": "As a base model we utilize the now standard attentionbased encoder-decoder model (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).", "startOffset": 81, "endOffset": 146}, {"referenceID": 37, "context": ", 50), and then applying a 1-layer MLP (similar to Yang et al. (2016)).", "startOffset": 51, "endOffset": 70}, {"referenceID": 20, "context": "Given s\u0303, we use an LSTM decoder with attention and input-feeding, in the style of Luong et al. (2015), to compute the probability of each target word, conditioned on the previous words and on s.", "startOffset": 83, "endOffset": 103}, {"referenceID": 23, "context": "cent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016).", "startOffset": 126, "endOffset": 227}, {"referenceID": 7, "context": "cent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016).", "startOffset": 126, "endOffset": 227}, {"referenceID": 38, "context": "cent work involving augmenting encoder-decoder models to copy words directly from the source material on which they condition (Gu et al., 2016; G\u00fcl\u00e7ehre et al., 2016; Merity et al., 2016; Jia and Liang, 2016; Yang et al., 2016).", "startOffset": 126, "endOffset": 227}, {"referenceID": 38, "context": "(2016) and Yang et al. (2016) parameterize the joint distribution table over \u0177t and zt directly:", "startOffset": 11, "endOffset": 30}, {"referenceID": 38, "context": "Models with copy-decoders may be trained to minimize the negative log marginal probability, marginalizing out the latent-variable zt (Gu et al., 2016; Yang et al., 2016; Merity et al., 2016).", "startOffset": 133, "endOffset": 190}, {"referenceID": 23, "context": "Models with copy-decoders may be trained to minimize the negative log marginal probability, marginalizing out the latent-variable zt (Gu et al., 2016; Yang et al., 2016; Merity et al., 2016).", "startOffset": 133, "endOffset": 190}, {"referenceID": 35, "context": "A fully differentiable approach using the decoder hidden states has recently been successfully applied to neural machine translation by Tu et al. (2017). Unlike copying, this method is applied only at training, and attempts to learn decoder hidden states with broader coverage of the input data.", "startOffset": 136, "endOffset": 153}, {"referenceID": 24, "context": "We train the generation models using SGD and truncated BPTT (Elman, 1990; Mikolov et al., 2010), as in language modeling.", "startOffset": 60, "endOffset": 95}, {"referenceID": 20, "context": "For our ROTOWIRE models, the record encoder produces r\u0303j in R 600, and we use a 2-layer LSTM decoder with hidden states of the same size as the r\u0303j , and dot-product attention and input-feeding in the style of Luong et al. (2015). Unlike past work, we use two identically structured attention layers, one to compute the standard generation probabilities (gen or pgen), and one to produce the scores used in copy or pcopy.", "startOffset": 210, "endOffset": 230}, {"referenceID": 11, "context": "The convolutional models concatenate convolutions with kernel widths 2, 3, and 5, and 200 feature maps in the style of (Kim, 2014).", "startOffset": 119, "endOffset": 130}, {"referenceID": 12, "context": "Our encoder-decoder models are based on OpenNMT (Klein et al., 2017).", "startOffset": 48, "endOffset": 68}, {"referenceID": 14, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al.", "startOffset": 57, "endOffset": 109}, {"referenceID": 21, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al.", "startOffset": 57, "endOffset": 109}, {"referenceID": 27, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al.", "startOffset": 57, "endOffset": 109}, {"referenceID": 29, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 200, "endOffset": 267}, {"referenceID": 34, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 200, "endOffset": 267}, {"referenceID": 2, "context": "Natural language generation has been studied for decades (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997), and generating summaries of sports games has been a topic of interest for almost as long (Robin, 1994; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005).", "startOffset": 200, "endOffset": 267}, {"referenceID": 14, "context": "Historically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al.", "startOffset": 77, "endOffset": 182}, {"referenceID": 21, "context": "Historically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al.", "startOffset": 77, "endOffset": 182}, {"referenceID": 27, "context": "Historically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al.", "startOffset": 77, "endOffset": 182}, {"referenceID": 2, "context": "Historically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al.", "startOffset": 77, "endOffset": 182}, {"referenceID": 28, "context": "Historically, research has focused on both content selection (\u201cwhat to say\u201d) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005), and surface realization (\u201chow to say it\u201d) (Goldberg et al., 1994; Reiter et al., 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al.", "startOffset": 226, "endOffset": 270}, {"referenceID": 37, "context": ", 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 3, "context": ", 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al.", "startOffset": 145, "endOffset": 157}, {"referenceID": 31, "context": ", 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007).", "startOffset": 178, "endOffset": 223}, {"referenceID": 36, "context": ", 2005) with earlier work using (hand-built) grammars, and later work using SMT-like approaches (Wong and Mooney, 2007) or generating from PCFGs (Belz, 2008) or other formalisms (Soricut and Marcu, 2006; White et al., 2007).", "startOffset": 178, "endOffset": 223}, {"referenceID": 17, "context": "In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).", "startOffset": 83, "endOffset": 189}, {"referenceID": 0, "context": "In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).", "startOffset": 83, "endOffset": 189}, {"referenceID": 10, "context": "In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).", "startOffset": 83, "endOffset": 189}, {"referenceID": 19, "context": "In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).", "startOffset": 83, "endOffset": 189}, {"referenceID": 13, "context": "In the late 2000s and early 2010s, a number of systems were proposed that did both (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013).", "startOffset": 83, "endOffset": 189}, {"referenceID": 38, "context": "Within the world of neural text generation, some recent work has focused on conditioning language models on tables (Yang et al., 2016), and generating short biographies from Wikipedia Tables (Lebret et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 15, "context": ", 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017).", "startOffset": 64, "endOffset": 108}, {"referenceID": 15, "context": ", 2016), and generating short biographies from Wikipedia Tables (Lebret et al., 2016; Chisholm et al., 2017). Mei et al. (2016) use a neural encoder-decoder approach on standard record-based generation datasets, obtaining impressive results, and motivating the need for more challenging NLG problems.", "startOffset": 65, "endOffset": 128}], "year": 2017, "abstractText": "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate humangenerated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copyand reconstructionbased extensions lead to noticeable improvements.", "creator": "LaTeX with hyperref package"}}}