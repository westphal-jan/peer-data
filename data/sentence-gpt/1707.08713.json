{"id": "1707.08713", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jul-2017", "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "abstract": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logic-based systems and that features derived from the proofs are effective for learning textual similarity. Although these tests have been performed successfully, our approach for natural deduction proofs is to compare a collection of features extracted by the source tree from a single source tree to the full-fledged semantic representation using both a sparse and sparse-level set of data. Thus we construct a model for quantitatively matching semantic textual similarity across semantic representations of the natural deduction proofs. In the current work, we define a single (previously-developed) feature from a single source tree. A further example is a tree containing the result from a simple sentence to the full-fledged semantic representation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 27 Jul 2017 05:49:51 GMT  (94kb,D)", "http://arxiv.org/abs/1707.08713v1", "11 pages, 5 figures, accepted as long paper of EMNLP2017"]], "COMMENTS": "11 pages, 5 figures, accepted as long paper of EMNLP2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hitomi yanaka", "koji mineshima", "pascual mart\u00ednez-g\u00f3mez", "daisuke bekki"], "accepted": true, "id": "1707.08713"}, "pdf": {"name": "1707.08713.pdf", "metadata": {"source": "CRF", "title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "authors": ["Hitomi Yanaka", "Koji Mineshima", "Daisuke Bekki"], "emails": ["hitomiyanaka@g.ecc.u-tokyo.ac.jp", "mineshima.koji@ocha.ac.jp", "pascual.mg@aist.go.jp", "bekki@is.ocha.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Determining semantic textual similarity (STS) is one of the most critical tasks in information retrieval and natural language processing. Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014). Re-\ncently, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed for learning textual similarity. However, these vector-based models often use shallow information, such as words and characters, and whether they can account for phenomena such as negation and quantification is not clear. Consider the sentences: Tom did not meet some of the players and Tom did not meet any of the players. If functional words such as some or any are ignored or represented as the same vector, then these sentences are to be represented by identical vectors. However, the first sentence implies that there is a player who Tom did not meet, whereas the second sentence means that Tom did not meet anyone, so the sentences have different meanings.\nConversely, logic-based approaches have been successful in representing the meanings of complex sentences, having had a positive impact for applications such as recognizing textual entailment (Mineshima et al., 2015, 2016; Abzianidze, 2015, 2016). However, purely logic-based approaches only assess entailment or contradiction relations between sentences and do not offer graded notions of semantic similarity.\nIn this paper, we propose to leverage logic cues to learn textual similarity. Our hypothesis is that observing proof processes when testing the semantic relations is predictive of textual similarity. We show that our approach can be more effective than systems that ignore these logic cues."}, {"heading": "2 Related Work", "text": "Vector-based models of semantic composition have been widely studied with regards to calculating STS. Mitchell and Lapata (2008, 2010)\nar X\niv :1\n70 7.\n08 71\n3v 1\n[ cs\n.C L\n] 2\n7 Ju\nl 2 01\n7\nproposed a sentence vector model involving word vector addition or component-wise multiplication. Addition and multiplication are commutative and associative and thus ignore word order. Polajnar et al. (2015) proposed a discourse-based sentence vector model considering extra-intra sentential context. Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016). However, these previous studies are mostly concerned with the structures of basic phrases or sentences and do not address logical and functional words such as negations and connectives. Neural network-based models of semantic composition (Mueller and Thyagarajan, 2016; Hill et al., 2016) have also been proposed. Although these models achieve higher accuracy, their end-to-end nature introduces challenges in the diagnosis of the reasons that make two sentences to be similar or dissimilar to each other. These diagnosis capabilities may play an important role in making the system explainable and also to guide future system improvements in a more precise manner. Our approach presented in this paper is partially inspired by the latter two objectives.\nMeanwhile, some previous studies have proposed logic systems for capturing the semantic relatedness of sentences. The Meaning Factory (Bjerva et al., 2014) uses both shallow and logic-based features for learning textual similarity. In this system, the overlap of predicates and entailment judgments are extracted as logic-based features. UTexas (Beltagy et al., 2014b) uses Probabilistic Soft Logic for learning textual similarity. In this system, each ground atom in the logical formulas has a probability based on distributional semantics of a word. The weights of the logical formulas are calculated from the probabilities of their ground atoms and are extracted as features. These previous studies improved the accuracy by using logic-based features derived from the entailment results of first-order theorem proving in addition to using shallow features such as sentence lengths.\nIn our study, we determine the semantic similarity of sentences based on the conception of prooftheoretic semantics (Bekki and Mineshima, 2017). The key idea is that not only the entailment results but also the theorem proving process can be considered as features for learning textual similarity.\nThat is, by taking into account not only whether a theorem is proved but also how it is proved, we can capture the semantic relationships between sentence pairs in more depth.\nAnother difference between our study and previous logic systems is that we use higher-order predicate logic. Higher-order predicate logic is able to represent complex sentence semantics such as generalized quantifiers more precisely than first-order predicate logic. In addition, higherorder predicate logic makes the logical structure of a sentence more explicit than first-order predicate logic does, so it can simplify the process of proof search (Miller and Nadathur, 1986)."}, {"heading": "3 System Overview", "text": "Figure 1 shows an overview of the system which extracts features for learning textual similarity from logical proofs. To produce semantic representations of sentences and prove them automatically, we use ccg2lambda (Mart\u0131\u0301nez-Go\u0301mez et al., 2016), which is a semantic parser combined with an inference system based on natural deduction.\nFirst, sentences are parsed into syntactic trees based on Combinatory Categorial Grammar (CCG) (Steedman, 2000). CCG is a syntactic theory suitable for semantic composition from syntactic structures. Meaning representations are obtained based on semantic templates and combinatory rules for the CCG trees. Semantic templates are defined manually based on formal semantics. Combinatory rules specify the syntactic behaviors of words and compositional rules for the CCG trees. In ccg2lambda, two wide-coverage CCG parsers, C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014), are used for converting tokenized sentences into CCG trees robustly. According to a previous study (Mart\u0131\u0301nezGo\u0301mez et al., 2017), EasyCCG achieves higher accuracy. Thus, when the output of both C&C and EasyCCG can be proved, we use EasyCCG\u2019s output for creating features.\nSecond, the meanings of words are described using lambda terms. Semantic representations are obtained by combining lambda terms in accordance with the meaning composition rules specified in the CCG tree. The semantic representations are based on Neo-Davidsonian event semantics (Parsons, 1990; Mineshima et al., 2015), in which every verb is decomposed into a predicate over events and a set of functional expressions re-\nlating the events. Adverbs and prepositions are also represented as predicates over events.\nThird, we attempt to prove entailment relations between sentence pairs. For this purpose, we use Coq (Bertot and Castran, 2010), which can be used for efficient theorem-proving for natural language inference using both first-order and higherorder logic (Mineshima et al., 2015). Coq\u2019s proof calculus is based on natural deduction (Prawitz, 1965), a proof system based on inference rules called introduction and elimination rules for logical connectives. The inference system implemented in ccg2lambda using Coq achieves efficient automatic inference by feeding a set of predefined tactics and user-defined proof-search tactics to its interactive mode. The natural deduction system is particularly suitable for injecting external axioms during the theorem-proving process (Mart\u0131\u0301nez-Go\u0301mez et al., 2017).\nFinally, features for learning textual similarity are extracted from the proofs produced by ccg2lambda during the theorem-proving process. In this study, we experimented with logistic regression, support vector regression and random forest regression, finding that random forest regression was the most effective. We therefore chose random forest regression for learning textual similarity, with its hyperparameters being optimized by grid search. The mean squared error (MSE) was used to measure the prediction performance of our system."}, {"heading": "4 Proof Strategy for Learning Textual Similarity", "text": ""}, {"heading": "4.1 Overview of the proof strategy", "text": "Sentence similarity depends on complex elements, such as word overlaps and semantic relations. We capture the similarity between the sentence pair (A,B) as a function of the provability of bidirectional entailment relations for (A,B) and combine it with shallow features. After obtaining logical formulas A\u2032 and B\u2032 from A and B, we attempt to\nprove the bidirectional entailment relations, A\u2032 \u21d2 B\u2032 and B\u2032 \u21d2 A\u2032. If the initial natural deduction proofs fail, we re-run the proof, adding relevant external axioms or skipping unproved subgoals until the proof is completed. After that, features for learning textual similarity are extracted by quantifying the provability of the bidirectional entailment relations.\nThe details of the procedure are as follows. First, we attempt a natural deduction proof without using external axioms, aiming to prove entailment relations, A\u2032 \u21d2 B\u2032 and B\u2032 \u21d2 A\u2032. If both fail, then we check whether A\u2032 contradicts B\u2032, which amounts to proving the negation of the original conclusion, namely A\u2032 \u21d2 \u00acB\u2032 and B\u2032 \u21d2 \u00acA\u2032.\nThe similarity of a sentence pair tends to be higher when the negation of the conclusion can be proved, compared with the case where neither the conclusion nor its negation can be proved. In the SICK (Sentences Involving Compositional Knowledge) dataset (Marelli et al., 2014) (see Section 6.1 for details), 70% of the sentence pairs annotated as contradictory are assigned a relatedness score in [3, 5).\nNext, if we fail to prove entailment or contradiction, that is, we cannot prove the conclusion or its negation, we identify an unproved sub-goal which is not matched by any predicate in the premise. We then attempt to prove A\u2032 \u21d2 B\u2032 and B\u2032 \u21d2 A\u2032 using axiom injection, following the method introduced in Mart\u0131\u0301nez-Go\u0301mez et al. (2017). In axiom injection, unproved sub-goals are candidates to form axioms. We focus only on predicates that share at least one argument with both the premise and the conclusion. This means that an axiom can be generated only if there is a predicate p in the pool of premises and a predicate q in a sub-goal and p and q share a variable in an argument position, possibly with the same case (e.g., Subject or Object).\nIn generating axioms, the semantic relationships between the predicates in the premise and those in the conclusion are checked using lexical knowledge. In this study, we use WordNet (Miller, 1995) as the source of lexical knowledge. Linguistic relations between predicates are checked in the following order: inflections, derivationally related forms, synonyms, antonyms, hypernyms, similarities, and hyponyms. If any one of these relations is found in the lexical knowledge, an axiom can be generated. Again, if the proof fails, we attempt\nto prove the negation of the conclusion using the axiom injection mechanism.\nIf the proof by axiom injection fails because of a lack of lexical knowledge, we obtain sentence similarity information from partial proofs by simply accepting the unproved sub-goals and forcibly completing the proof. After the proof is completed, information about the generated axioms and skipped sub-goals is used to create features."}, {"heading": "4.2 Proving entailment relations", "text": "As an illustration of how our natural deduction proof works, consider the case of proving entailment for the following sentence pair:\nA: A man is singing in a bar. B: A man is singing.\nThe sentencesA andB are mapped onto logical formulas A\u2032 and B\u2032 based on event semantics via CCG-based semantic composition, as follows.\nA\u2032 : \u2203e1x1x2(man(x1) \u2227 sing(e1) \u2227 (subj(e1) = x1) \u2227 bar(x2) \u2227 in(e1, x2)) B\u2032 : \u2203e1x1(man(x1) \u2227 sing(e1) \u2227 (subj(e1) = x1))\nFirst, we attempt a natural deduction proof of A\u2032 \u21d2 B\u2032, setting A\u2032 as the premise and B\u2032 as the goal of the proof. ThenA\u2032 andB\u2032 are decomposed according to the inference rules.\nFigure 2 shows the major inference rules we use in the proofs. Inference rules in natural deduction are divided into two types: introduction rules and\nelimination rules. Introduction rules specify how to prove a formula in the goal, decomposing a goal formula into smaller sub-goals. Elimination rules specify how to use a premise, decomposing a formula in the pool of premises into smaller ones.\nThe proof process forA\u2032 \u21d2 B\u2032 is shown in Figure 3. HereA\u2032 is initially set to the premise P0 and B\u2032 to the goalG0. P0 andG0 are then decomposed using elimination rules (\u2227-ELIM, \u2203-ELIM) and introduction rules (\u2227-INTRO, \u2203-INTRO). Then we obtain a set of premise formulasP = {P2, P3, P4, P5, P6}, and a set of sub-goals G = {G2, G3, G4}. The proof is performed by searching for a premise Pi whose predicate and arguments match those of a given sub-goal Gj . If such a logical premise is found, the sub-goal is removed. In this example, the sub-goals G2, G3, and G4 match the premises P2, P3, and P4, respectively. Thus, A\u2032 \u21d2 B\u2032 can be proved without introducing axioms.\nSecond, we attempt the proof in the opposite direction, B\u2032 \u21d2 A\u2032, by switching P0 and G0 in Figure 3. Again, by applying inference rules, we obtain the following sets of premises P and subgoals G:\nP = {P2 : man(x1), P3 : sing(e1), P4 : subj(e1) = x1} G = {G2 : man(x1), G3 : sing(e1), G4 : subj(e1) = x1, G5 : bar(x2), G6 : in(e1, x2))}\nHere, the two sub-goals G5 and G6 do not match any of the premises, so the attempted proof of B\u2032 \u21d2 A\u2032 fails. We therefore attempt to inject additional axioms, but in this case no predicate in P shares the argument x2 of the predicates bar(x2) and in(e1, x2) in G. Thus, no axiom can be generated. To obtain information from a partial proof, we forcibly complete the proof of B\u2032 \u21d2 A\u2032 by skipping the unproved sub-goals bar(x) and\nin(e1, x2)."}, {"heading": "4.3 Proving the contradiction", "text": "The proof strategy illustrated here can be straightforwardly applied to proving the contradiction. In natural deduction, a negative formula of the form \u00acA can be defined as A \u2192 False (\u201cthe formula A implies the contradiction\u201d), by using a propositional constant False to encode the contradiction. Thus, the inference rules for negation can be taken as special cases of implication rules, as shown in Figure 4.\nAs an illustration, let us consider the following sentence pair: A: No man is singing. B: There is a man singing loudly.\nFigure 5 shows the proof process. The sentences A and B are mapped to P0 and P1, respectively, via compositional semantics and the goalG0 is set to False. By decomposing P1 using elimination rules and then by combining P2, P3, and P4, we can obtain P6. From P0 and P6 we can then derive the contradiction.\nThese proofs are performed by an automated prover implemented on Coq, using tactics for firstorder theorem proving. When a proof is successful, Coq outputs the resulting proof (a proof term), from which we can extract detailed information such as the number of proof steps and the types of inference rules used. In addition to the entailment/contradiction result, information about the proof process is used to create features."}, {"heading": "5 Description of the Features", "text": "To maximize accuracy when learning textual similarity, we adopt a hybrid approach that uses both logic-based features extracted from the natural deduction proof and other, non-logic-based features. All features are scaled to the [0, 1] range."}, {"heading": "5.1 Logic-based Features", "text": "We propose 15 features consisting of nine different types of logic-based features. Six of these feature types are derived from the bidirectional natural deduction proofs: six features are extracted from the direct proof (A\u2032 \u21d2 B\u2032) and another six from the reverse proof (B\u2032 \u21d2 A\u2032). The remaining three feature types are derived from semantic representations of the sentence pairs. The feature types are as follows.\nP0 : \u00ac\u2203e1\u2203x1(man(x1) \u2227 sing(e1) \u2227 (subj(e1) = x1)) P1 : \u2203e1\u2203x1(man(x1) \u2227 sing(e1) \u2227 (subj(e1) = x1) \u2227 loudly(e1)) G0 : False\nLogical inference result. As stated in Section 4, we include features to distinguish the case where either the conclusion or its negation can be proved from the one where neither can be proved. If the conclusion can be proved, the feature is set to 1.0. If the negation of the conclusion can be proved, the feature is set to 0.5. If neither can be proved, the feature is set to 0.0. Axiom probabilities. The probability of an axiom and the number of axioms appearing in the proof are used to create features. The probability of an axiom is defined as the inverse of the length of the shortest path that connects the senses in the is-a (hypernym/hyponym) taxonomy in WordNet. When multiple axioms are used in the proof, the average of the probabilities of the axioms is extracted as a feature. If the proof can be completed without using axioms, the feature is set to 1.0. Proved sub-goals. Given that proofs can be obtained either by proving all the sub-goals or skipping unproved sub-goals, we use the proportion of proved sub-goals as a feature. Our assumption is that if there are more unproved sub-goals then the sentence pair is less similar. When there are m logical formulas in the premise pool and n proved sub-goals, we set the feature to n/m. If the theorem can be proved without skipping any sub-goals, the feature is set to 1.0. It may be the case that the number of sub-goals is so large that some subgoals remain unproved even after axiom injection.\nSince the proportion of unproved sub-goals is decreased by axiom injection, we use the proportion of unproved sub-goals both with and without axiom injection as features. Cases in unproved sub-goals. Subject or object words can affect the similarity of sentence pairs. Therefore, the number of each case in unproved sub-goals, like subj(e1) in Figures 3 and 5, is used as a feature. Here, we count subjective, objective, and dative cases. Proof steps. In general, complex theorems are difficult to prove and in such cases the sentence pairs are considered to be less similar. We therefore use the number of Coq\u2019s proof steps, namely the number of inference rule applications in a given proof, as a feature. Inference rules. The complexity of a natural deduction proof can be measured in terms of the inference rules used for each proof step. We therefore extract the relative frequency with which each inference rule is used in the proof as a feature. We check seven inference rules for natural deduction using Coq (cf. Figure 2): introduction and elimination rules for conjunction (\u2227-INTRO, \u2227-ELIM), implication (\u2192-INTRO, \u2192-ELIM), and existential quantification (\u2203-INTRO, \u2203-ELIM), and the elimination rule for equality (=-ELIM). Predicate overlap. Intuitively, the more predicates overlap between the premise and the conclusion, the more likely it is that the inference can be proved. We therefore use the proportion of predicates that overlap between the premise and the conclusion as a feature. Semantic type overlap. Each semantic representation in higher-order logic has a semantic type, such as Entity for entities and Prop for propositions. As with predicates, we use the degree of semantic type overlap between the premise and the conclusion as a feature. Existence of negative clauses. Whether or not the premise or conclusion contain negative clauses is an effective measure of similarity. In semantic representations, negative clauses are represented by the negation operator \u00ac, so we check for negation operators in the premise and the conclusion and set this feature to 1.0 if either contains one."}, {"heading": "5.2 Non-logic-based Features", "text": "We also use the following eight non-logic-based features. Noun/verb overlap. We extract and lemmatize all\nnouns and verbs from the sentence pairs and use the degrees of overlap of the noun and verb lemmas as features. Part-of-speech overlap. We obtain part-ofspeech (POS) tags for all words in the sentence pairs by first tokenizing them with the Penn Treebank Project tokenizer1 and then POS tagging them with C&C POS tagger (Curran and Clark, 2003). The degree of overlap between the sentences\u2019 POS tags is used as a feature. Synset overlap. For each sentence in the pair, we obtain the set containing all the synonym lemmas (the synset) for the words in the sentence. The degree of overlap between the sentences\u2019 synsets is used as a feature. Synset distance. For each word in the first sentence, we compute the maximum path similarity between its synset and the synset of any other word in the second sentence. Then, we use the average of maximum path similarities as a feature. Sentence length. If the conclusion sentence is long, there will possibly be many sub-goals in the proof. We therefore use the average of the sentence lengths and the difference in length between the premise and the conclusion sentences as features. String similarity. We use the similarity of the sequence of characters within the sentence pairs as a feature. The Python Difflib2 function returns the similarity between two sequences as a floatingpoint value in [0, 1]. This measure is given by 2.0 \u2217 M/T , where T is the total number of elements in both sequences and M is the number of matches. This feature is 1.0 if the sequences are identical and 0.0 if they have nothing in common. Sentence similarity from vector space models. We calculate sentence similarity by using three major vector space models, TF-IDF, latent semantic analysis (LSA) (Deerwester et al., 1990), and latent Dirichlet allocation (LDA) (Blei et al., 2003). We use these cosine similarities as features. Existence of passive clauses. Passive clauses have an influence on similarity. In CCG trees, passive clauses are represented using the syntactic category Spss\\NP . We check for the occurrence of passive clauses in the premise and conclusion, and if either of them contains a passive clause then the feature is set to 1.0.\n1ftp://ftp.cis.upenn.edu/pub/treebank/public html/ tokenization.html\n2https://docs.python.org/3.5/library/difflib.html"}, {"heading": "6 Experiments and Evaluation", "text": ""}, {"heading": "6.1 Experimental Conditions", "text": "We evaluated our system3 using two datasets: the SemEval-2014 version of the SICK dataset (Marelli et al., 2014) and the SemEval2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) (Agirre et al., 2012). The experimental conditions were as follows."}, {"heading": "6.1.1 The SICK dataset", "text": "The SICK dataset is a dataset for studying STS as well as for recognizing textual entailment (RTE). It was originally developed for evaluating compositional distributional semantics, so it contains logically challenging expressions such as quantifiers, negations, conjunctions and disjunctions. The dataset contains 9927 sentence pairs with a 5000/4927 training/test split. These sentence pairs are manually annotated with three types of labels yes (entailment), no (contradiction), or unknown (neutral) as well as a semantic relatedness scores in [1, 5] (see Table 1 for a sample).\nIn this dataset, sentence pairs whose gold entailment labels are no tend to be scored a little more highly than the average, whereas those whose labels are unknown have a wide range of scores. Thus, we set the baseline of the relatedness score to 5 when the gold entailment label was yes and to 3 when the label was no or unknown.\nWe compared our system with the following systems: the state-of-the-art neural network-based system (Mueller and Thyagarajan, 2016); the best system (Zhao et al., 2014) from SemEval-2014; and two of the logic-based systems stated in Sec-\n3Available at https://github.com/mynlp/ccg2lambda.\ntion 2: namely The Meaning Factory (Bjerva et al., 2014) and UTexas (Beltagy et al., 2014b). The Pearson correlation coefficient \u03b3, Spearman\u2019s rank correlation coefficient \u03c1, and the MSE were used as the evaluation metrics."}, {"heading": "6.1.2 The MSR-vid dataset", "text": "The MSR-vid dataset is our second dataset for the STS task and contains 1500 sentence pairs with a 750/750 training/test split. All sentence pairs are annotated with semantic relatedness scores in the range [0, 5]. We used this dataset to compare our system with the best system from SemEval2012 (Ba\u0308r et al., 2012) and the logic-based UTexas system (Beltagy et al., 2014a). We used the Pearson correlation coefficient \u03b3 as the evaluation metric."}, {"heading": "6.2 Results", "text": "Table 2 shows the results of our experiments with the SICK dataset. Although the state-of-the-art neural network-based system yielded the best results overall, our system achieved higher scores than SemEval-2014 submissions, including the two logic-based systems (The Meaning Factory and UTexas), in terms of Pearson correlation and Spearman\u2019s correlation.\nThe main reason for our system\u2019s lower performance in terms of MSE is that some theorems could not be proved because of a lack of lexical knowledge. In the current work, we only consider word-level knowledge (word-for-word paraphrasing); we may expand the knowledge base in the future by using more external resources.\nAs we mentioned above, the sentence pairs annotated as unknown produced a wide range of scores. The Pearson correlation of the unknown portion of the SICK dataset was 0.766, which suggests that our logic-based system can also be applied to neutral sentence pairs.\nTable 3 shows the results of our experiments with the MSR-vid dataset. These results also indicate that our logic-based system achieved higher accuracy than the other logic-based systems.\nTable 4 shows evaluation results for each feature\ngroup in isolation, showing that inference rules and predicate overlaps are the most effective features. Compared with the non-logic-based features, the logic-based features achieved a slightly higher accuracy, a point that will be analyzed in more detail in the next section. Overall, our results show that combining logic-based features with non logic-based ones is an effective method for determining textual similarity."}, {"heading": "6.3 Positive examples and error analysis", "text": "Table 5 shows some examples for which the prediction score was better when using logic-based features than when using non-logic-based ones.\nFor IDs 642 and 1360, one sentence contains a passive clause while the other sentence does not. In such cases, the sentence pairs are not superficially similar. By using logical formulas based on event semantics we were able to interpret the sentence containing the passive clause correctly and judge that the passive and non-passive sentences\nare similar to each other. In ID 891, one sentence contains a negative clause while the other does not. Using shallow features, the word overlap is small and the prediction score was much lower than the correct score. Our logic-based method, however, interpreted the first sentence as a negative existential formula of the form \u00ac\u2203xP(x) and the second sentence as an existential formula \u2203xP \u2032(x). Thus, it could easily handle the semantic difference between the positive and negative sentences.\nIn ID 1158, by contrast, the proportion of word overlap is so high that the prediction score with non-logic-based features was much higher than the correct score. Our method, however, was able to prove the contradiction using an antonym axiom of the form \u2200x(remove(x) \u2192 \u00acadd(x)) from WordNet and thus predict the score correctly.\nIn ID 59, the proportion of word overlap is low, so the prediction score with non-logic-based features was lower than the correct score. Our method, however, was able to prove the partial entailment relations for the sentence pair and thus predict the score correctly. Here the logic-based method captured the common meaning of the sentence pair: both sentences talk about the kids playing in the leaves.\nFinally, in ID 71, the prediction score with nonlogic-based features was much higher than the correct score. There are two reasons for this phenomenon: negations tend to be omitted in nonlogic-based features such as TF-IDF and the proportion of word overlap is high. However, as logical formulas and proofs can handle negative clauses correctly, our method was able to predict the score correctly.\nTable 6 shows examples where using only logicbased features produced erroneous results. In ID 3974, the probability of axiom \u2200x(awaken(x)\u2192 up(x)) was low (0.25) and thus the prediction score was lower than the correct score. Likewise, in ID 4833, the probability of axiom \u2200x(file(x)\u2192 do(x)) was very low (0.09) and thus the prediction score was negatively affected. In these cases, we need to consider phrase-level axioms such as \u2200x(awaken(x) \u2192 wake up(x)) and \u2200x(file nail(x) \u2192 do manicure(x)) using a paraphrase database. This, however, is an issue for future study. In ID 1941, the system wrongly proved the bidirectional entailment relations by adding external axioms, so the prediction score\nwas much higher than the correct score. Setting the threshold for the probability of an axiom may be an effective way of improving our axiominjection method."}, {"heading": "7 Conclusion", "text": "We have developed a hybrid method for learning textual similarity by combining features based on logical proofs of bidirectional entailment relations with non-logic-based features. The results of our experiments on two datasets show that our system was able to outperform other logic-based systems. In addition, the results show that information about the natural deduction proof process can be used to create effective features for learning textual similarity. Since these logic-based features provide accuracy improvements that are largely additive with those provided by non-logic-based features, neural network-based systems may also benefit from using them.\nIn future work, we will refine our system so that it can be applied to other tasks such as question answering. Compared with neural networkbased systems, our natural deduction-based system can not only assess how similar sentence pairs are, but also explain what the sources of simi-\nlarity/dissimilarity are by referring to information about sub-goals in the proof. Given this interpretative ability, we believe that our logic-based system may also be of benefit to other natural language processing tasks, such as question answering and text summarization."}, {"heading": "Acknowledgments", "text": "We thank the three anonymous reviewers for their detailed comments. This work was supported by JST CREST Grant Number JPMJCR1301, Japan."}], "references": [{"title": "A tableau prover for natural logic and language", "author": ["Lasha Abzianidze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15), pages 2492\u20132502, Lisbon, Portugal. Association for Computational Linguis-", "citeRegEx": "Abzianidze.,? 2015", "shortCiteRegEx": "Abzianidze.", "year": 2015}, {"title": "Natural solution to FraCaS entailment problems", "author": ["Lasha Abzianidze."], "venue": "Proceedings of the 5th Joint Conference on Lexical and Computational Semantics, pages 64\u201374, Berlin, Germany. Association for Computational Linguistics.", "citeRegEx": "Abzianidze.,? 2016", "shortCiteRegEx": "Abzianidze.", "year": 2016}, {"title": "SemEval-2012 Task 6: A", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "UKP: Computing semantic textual similarity by combining multiple content similarity measures", "author": ["Daniel B\u00e4r", "Chris Biemann", "Iryna Gurevych", "Torsten Zesch."], "venue": "Proceedings of the Sixth International Workshop on Semantic Evalu-", "citeRegEx": "B\u00e4r et al\\.,? 2012", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2012}, {"title": "Contextpassing and underspecification in dependent type semantics", "author": ["Daisuke Bekki", "Koji Mineshima."], "venue": "Stergios Chatzikyriakidis and Zhaohui Luo, editors, Modern Perspectives in Type Theoretical Semantics, Studies of Linguistics and Philoso-", "citeRegEx": "Bekki and Mineshima.,? 2017", "shortCiteRegEx": "Bekki and Mineshima.", "year": 2017}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["Islam Beltagy", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014), pages 1210\u20131219, Baltimore,", "citeRegEx": "Beltagy et al\\.,? 2014a", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "UTexas: Natural language semantics using distributional semantics and probabilistic logic", "author": ["Islam Beltagy", "Stephen Roller", "Gemma Boleda", "Katrin Erk", "Raymond Mooney."], "venue": "Proceedings of the 8th International Workshop on Semantic Evalu-", "citeRegEx": "Beltagy et al\\.,? 2014b", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics and Dublin City University", "author": ["Dublin", "Ireland"], "venue": null, "citeRegEx": "Dublin and Ireland.,? \\Q2014\\E", "shortCiteRegEx": "Dublin and Ireland.", "year": 2014}, {"title": "Interactive Theorem Proving and Program Development: Coq\u2019Art The Calculus of Inductive Constructions", "author": ["Yves Bertot", "Pierre Castran."], "venue": "Springer Publishing Company, Incorporated, New York, USA.", "citeRegEx": "Bertot and Castran.,? 2010", "shortCiteRegEx": "Bertot and Castran.", "year": 2010}, {"title": "The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim."], "venue": "Proceedings of the 8th International Workshop on Semantic Eval-", "citeRegEx": "Bjerva et al\\.,? 2014", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics and Dublin City University", "author": ["Dublin", "Ireland"], "venue": null, "citeRegEx": "Dublin and Ireland.,? \\Q2014\\E", "shortCiteRegEx": "Dublin and Ireland.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Journal of Machine Learning, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Widecoverage efficient statistical parsing with CCG and log-linear models", "author": ["Stephen Clark", "James R. Curran."], "venue": "Computational Linguistics, 33(4):493\u2013552.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Investigating GIS and smoothing for maximum entropy taggers", "author": ["James R Curran", "Stephen Clark."], "venue": "Proceedings of the tenth conference on European chapter of the Association for Computational", "citeRegEx": "Curran and Clark.,? 2003", "shortCiteRegEx": "Curran and Clark.", "year": 2003}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "Richard Harshman."], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-2011),", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Resolving lexical ambiguity in tensor regression models of meaning", "author": ["Dimitri Kartsaklis", "Nal Kalchbrenner", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014),", "citeRegEx": "Kartsaklis et al\\.,? 2014", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Distributional inclusion hypothesis for tensor-based composition", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers (COLING-2016), pages 2849\u2013", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2016", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V. Le", "Tomas Mikolov."], "venue": "Proceedings of the 31th International Conference on Machine Learning, (ICML-2014), pages 1188\u2013 1196, Beijing, China.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "A* CCG parsing with a supertag-factored model", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP-2014), pages 990\u20131000, Doha, Qatar. Association for Com-", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "A SICK cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Proceedings of the 9th International Conference on", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "On-demand injection of lexical knowledge for recognising textual entailment", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Koji Mineshima", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Compu-", "citeRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.", "year": 2017}, {"title": "Association for Computational Linguistics", "author": ["Valencia", "Spain"], "venue": null, "citeRegEx": "Valencia and Spain.,? \\Q2017\\E", "shortCiteRegEx": "Valencia and Spain.", "year": 2017}, {"title": "Some uses of higher-order logic in computational linguistics", "author": ["Dale A. Miller", "Gopalan Nadathur."], "venue": "Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 247\u2013256, New York, New York, USA. Asso-", "citeRegEx": "Miller and Nadathur.,? 1986", "shortCiteRegEx": "Miller and Nadathur.", "year": 1986}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller."], "venue": "Communications of the ACM, 38(11):39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Higher-order logical inference with compositional semantics", "author": ["Koji Mineshima", "Pascual Mart\u0131\u0301nez-G\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-", "citeRegEx": "Mineshima et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2015}, {"title": "Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser", "author": ["Koji Mineshima", "Ribeka Tanaka", "Pascual Mart\u0131\u0301nezG\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of the 2016 Conference", "citeRegEx": "Mineshima et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2016}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 236\u2013 244, Columbus, Ohio. Association for Computa-", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131429.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Siamese recurrent architectures for learning sentence similarity", "author": ["Jonas Mueller", "Aditya Thyagarajan."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-2016), pages 2786\u2013 2792, Arizona, USA. Association for the Advance-", "citeRegEx": "Mueller and Thyagarajan.,? 2016", "shortCiteRegEx": "Mueller and Thyagarajan.", "year": 2016}, {"title": "Events in The Semantics of English: a Study in Subatomic Semantics", "author": ["Terence Parsons."], "venue": "MIT Press, Cambridge, USA.", "citeRegEx": "Parsons.,? 1990", "shortCiteRegEx": "Parsons.", "year": 1990}, {"title": "An exploration of discourse-based sentence spaces for compositional distributional semantics", "author": ["Tamara Polajnar", "Laura Rimell", "Stephen Clark."], "venue": "Proceedings of the 1st Workshop on Linking Computational Models of Lexical, Sentential and", "citeRegEx": "Polajnar et al\\.,? 2015", "shortCiteRegEx": "Polajnar et al\\.", "year": 2015}, {"title": "Natural Deduction \u2013 A ProofTheoretical Study", "author": ["Dag Prawitz."], "venue": "Almqvist & Wiksell, Stockholm, Sweden.", "citeRegEx": "Prawitz.,? 1965", "shortCiteRegEx": "Prawitz.", "year": 1965}, {"title": "The Syntactic Process", "author": ["Mark Steedman."], "venue": "MIT Press, Cambridge, USA.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Vector space model of information retrieval: A reevaluation", "author": ["S.K.M. Wong", "Vijay V. Raghavan."], "venue": "Proceedings of the 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 167\u2013185.", "citeRegEx": "Wong and Raghavan.,? 1984", "shortCiteRegEx": "Wong and Raghavan.", "year": 1984}, {"title": "ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Jiang Zhao", "Tiantian Zhu", "Man Lan."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 35, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 29, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 19, "context": "Vectorbased sentence representation models have been widely used to compare and rank words, phrases or sentences using various similarity and relatedness scores (Wong and Raghavan, 1984; Mitchell and Lapata, 2010; Le and Mikolov, 2014).", "startOffset": 161, "endOffset": 235}, {"referenceID": 30, "context": "Recently, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed for learning textual similarity.", "startOffset": 62, "endOffset": 112}, {"referenceID": 16, "context": "Recently, neural network-based sentence representation models (Mueller and Thyagarajan, 2016; Hill et al., 2016) have been proposed for learning textual similarity.", "startOffset": 62, "endOffset": 112}, {"referenceID": 15, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 17, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 18, "context": "Also, a categorical compositional distributional semantic model has been developed for recognizing textual entailment and for calculating STS (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2014; Kartsaklis and Sadrzadeh, 2016).", "startOffset": 142, "endOffset": 233}, {"referenceID": 30, "context": "Neural network-based models of semantic composition (Mueller and Thyagarajan, 2016; Hill et al., 2016) have also been proposed.", "startOffset": 52, "endOffset": 102}, {"referenceID": 16, "context": "Neural network-based models of semantic composition (Mueller and Thyagarajan, 2016; Hill et al., 2016) have also been proposed.", "startOffset": 52, "endOffset": 102}, {"referenceID": 27, "context": "Polajnar et al. (2015) proposed a discourse-based sentence vector model considering extra-intra sentential context.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "The Meaning Factory (Bjerva et al., 2014) uses both shallow and logic-based features for learning textual similarity.", "startOffset": 20, "endOffset": 41}, {"referenceID": 6, "context": "UTexas (Beltagy et al., 2014b) uses Probabilistic Soft Logic for learning textual similarity.", "startOffset": 7, "endOffset": 30}, {"referenceID": 4, "context": "In our study, we determine the semantic similarity of sentences based on the conception of prooftheoretic semantics (Bekki and Mineshima, 2017).", "startOffset": 116, "endOffset": 143}, {"referenceID": 24, "context": "In addition, higherorder predicate logic makes the logical structure of a sentence more explicit than first-order predicate logic does, so it can simplify the process of proof search (Miller and Nadathur, 1986).", "startOffset": 183, "endOffset": 210}, {"referenceID": 34, "context": "First, sentences are parsed into syntactic trees based on Combinatory Categorial Grammar (CCG) (Steedman, 2000).", "startOffset": 95, "endOffset": 111}, {"referenceID": 12, "context": "In ccg2lambda, two wide-coverage CCG parsers, C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014), are used for converting tokenized sentences into CCG trees robustly.", "startOffset": 50, "endOffset": 74}, {"referenceID": 20, "context": "In ccg2lambda, two wide-coverage CCG parsers, C&C (Clark and Curran, 2007) and EasyCCG (Lewis and Steedman, 2014), are used for converting tokenized sentences into CCG trees robustly.", "startOffset": 87, "endOffset": 113}, {"referenceID": 31, "context": "The semantic representations are based on Neo-Davidsonian event semantics (Parsons, 1990; Mineshima et al., 2015), in which every verb is decomposed into a predicate over events and a set of functional expressions re-", "startOffset": 74, "endOffset": 113}, {"referenceID": 26, "context": "The semantic representations are based on Neo-Davidsonian event semantics (Parsons, 1990; Mineshima et al., 2015), in which every verb is decomposed into a predicate over events and a set of functional expressions re-", "startOffset": 74, "endOffset": 113}, {"referenceID": 8, "context": "For this purpose, we use Coq (Bertot and Castran, 2010), which can be used for efficient theorem-proving for natural language inference using both first-order and higherorder logic (Mineshima et al.", "startOffset": 29, "endOffset": 55}, {"referenceID": 26, "context": "For this purpose, we use Coq (Bertot and Castran, 2010), which can be used for efficient theorem-proving for natural language inference using both first-order and higherorder logic (Mineshima et al., 2015).", "startOffset": 181, "endOffset": 205}, {"referenceID": 33, "context": "Coq\u2019s proof calculus is based on natural deduction (Prawitz, 1965), a proof system based on inference rules called introduction and elimination rules for logical connectives.", "startOffset": 51, "endOffset": 66}, {"referenceID": 22, "context": "The natural deduction system is particularly suitable for injecting external axioms during the theorem-proving process (Mart\u0131\u0301nez-G\u00f3mez et al., 2017).", "startOffset": 119, "endOffset": 149}, {"referenceID": 21, "context": "In the SICK (Sentences Involving Compositional Knowledge) dataset (Marelli et al., 2014) (see Section 6.", "startOffset": 66, "endOffset": 88}, {"referenceID": 22, "context": "We then attempt to prove A\u2032 \u21d2 B\u2032 and B\u2032 \u21d2 A\u2032 using axiom injection, following the method introduced in Mart\u0131\u0301nez-G\u00f3mez et al. (2017). In axiom injection, unproved sub-goals are candidates to form axioms.", "startOffset": 103, "endOffset": 133}, {"referenceID": 25, "context": "In this study, we use WordNet (Miller, 1995) as the source of lexical knowledge.", "startOffset": 30, "endOffset": 44}, {"referenceID": 13, "context": "We obtain part-ofspeech (POS) tags for all words in the sentence pairs by first tokenizing them with the Penn Treebank Project tokenizer1 and then POS tagging them with C&C POS tagger (Curran and Clark, 2003).", "startOffset": 184, "endOffset": 208}, {"referenceID": 14, "context": "We calculate sentence similarity by using three major vector space models, TF-IDF, latent semantic analysis (LSA) (Deerwester et al., 1990), and latent Dirichlet allocation (LDA) (Blei et al.", "startOffset": 114, "endOffset": 139}, {"referenceID": 11, "context": ", 1990), and latent Dirichlet allocation (LDA) (Blei et al., 2003).", "startOffset": 47, "endOffset": 66}, {"referenceID": 21, "context": "We evaluated our system3 using two datasets: the SemEval-2014 version of the SICK dataset (Marelli et al., 2014) and the SemEval2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) (Agirre et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": ", 2014) and the SemEval2012 version of the MSR-paraphrase video corpus dataset (MSR-vid) (Agirre et al., 2012).", "startOffset": 89, "endOffset": 110}, {"referenceID": 30, "context": "We compared our system with the following systems: the state-of-the-art neural network-based system (Mueller and Thyagarajan, 2016); the best system (Zhao et al.", "startOffset": 100, "endOffset": 131}, {"referenceID": 36, "context": "We compared our system with the following systems: the state-of-the-art neural network-based system (Mueller and Thyagarajan, 2016); the best system (Zhao et al., 2014) from SemEval-2014; and two of the logic-based systems stated in Sec-", "startOffset": 149, "endOffset": 168}, {"referenceID": 9, "context": "tion 2: namely The Meaning Factory (Bjerva et al., 2014) and UTexas (Beltagy et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 6, "context": ", 2014) and UTexas (Beltagy et al., 2014b).", "startOffset": 19, "endOffset": 42}, {"referenceID": 3, "context": "We used this dataset to compare our system with the best system from SemEval2012 (B\u00e4r et al., 2012) and the logic-based UTexas system (Beltagy et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 5, "context": ", 2012) and the logic-based UTexas system (Beltagy et al., 2014a).", "startOffset": 42, "endOffset": 65}], "year": 2017, "abstractText": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higherorder automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logicbased systems and that features derived from the proofs are effective for learning textual similarity.", "creator": "LaTeX with hyperref package"}}}