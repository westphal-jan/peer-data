{"id": "1708.00214", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "Natural Language Processing with Small Feed-Forward Networks", "abstract": "We show that small and shallow feed-forward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.\n\n\n\n\nThe large data sets, as well as the underlying data, are a first step toward designing a model for large data processing tasks. We have detailed the model for these small data sets for this post, which includes information on the processing of these low-cost, low-cost, high-complexity memory networks, and the potential for complex computational challenges in memory management.\nThe model is composed of 16 high-end, high-end, high-end, high-complexity memory networks, and includes a database of high-end, high-end, high-end, high-complexity memory networks, and a database of high-end, high-end, high-complexity memory networks. It is based on the data, a collection of a single large-scale data set, in two steps, and uses multiple types of data in a specific type of data set. The results are presented here.\nThe large data sets are based on a single large-scale data set, in two steps, and uses multiple types of data in a specific type of data set. The results are presented here. Image copyright Reuters Image caption The data is used to visualize the shape of a computer screen\nThe data is used to visualize the shape of a computer screen, and the spatial context of the screen. The spatial context of the screen, and the spatial context of the screen. The spatial context of the screen, and the spatial context of the screen.\nThe spatial context of the screen, and the spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of the screen.\nThe spatial context of", "histories": [["v1", "Tue, 1 Aug 2017 09:13:44 GMT  (107kb,D)", "http://arxiv.org/abs/1708.00214v1", "EMNLP 2017 short paper"]], "COMMENTS": "EMNLP 2017 short paper", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jan a botha", "emily pitler", "ji ma", "anton bakalov", "alex salcianu", "david weiss", "ryan t mcdonald", "slav petrov"], "accepted": true, "id": "1708.00214"}, "pdf": {"name": "1708.00214.pdf", "metadata": {"source": "CRF", "title": "Natural Language Processing with Small Feed-Forward Networks", "authors": ["Jan A. Botha", "Emily Pitler", "Ji Ma", "Anton Bakalov", "Alex Salcianu", "David Weiss", "Ryan McDonald", "Slav Petrov"], "emails": ["jabot@google.com", "epitler@google.com", "maji@google.com", "abakalov@google.com", "salcianu@google.com", "djweiss@google.com", "ryanmcd@google.com", "slav@google.com"], "sections": [{"heading": "1 Introduction", "text": "Deep and recurrent neural networks with large network capacity have become increasingly accurate for challenging language processing tasks. For example, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al., 2017) of parameters. These models, however, may not be feasible in all computational settings. In particular, models running on mobile devices are often constrained in terms of memory and computation.\nLong Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone.\nFeed-forward neural networks have the potential to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach.\nWe begin by introducing the network model structure and the character-based representations we use throughout all tasks (\u00a72). The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation. In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks. We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget:\n1. Quantization: Using more dimensions and less precision (Lang-ID: \u00a73.1).\n2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: \u00a73.2).\n3. Selected features: Adding explicit feature conjunctions (segmentation: \u00a73.3).\n4. Pipelines: Introducing another task in a pipeline and allocating parameters to the auxiliary task instead (preordering: \u00a73.4).\nWe achieve results at or near state-of-the-art with small (< 3 MB) models on all four tasks."}, {"heading": "2 Small Feed-Forward Network Models", "text": "The network architectures are designed to limit the memory and runtime of the model. Figure 1 illustrates the model architecture:\nar X\niv :1\n70 8.\n00 21\n4v 1\n[ cs\n.C L\n] 1\nA ug\n2 01\n7\n1. Discrete features are organized into groups (e.g., Ebigrams), with one embedding matrix Eg \u2208 RVg\u00d7Dg per group.\n2. Embeddings of features extracted for each group are reshaped into a single vector and concatenated to define the output of the embedding layer as h0 = [XgEg | \u2200g].\n3. A single hidden layer, h1, with M rectified linear units (Nair and Hinton, 2010) is fully connected to h0.\n4. A softmax function models the probability of an output class y: P (y) \u221d exp(\u03b2Ty h1 + by), where \u03b2y \u2208 RM and by are the weight vector and bias, respectively.\nMemory needs are dominated by the embedding matrix sizes ( \u2211 g VgDg, where Vg and Dg are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimensions.\nHashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015). However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch.\nWe use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008). That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg, where H is a well-behaved hash function. Typical values for Vg are in the 100-5000 range, which is far smaller than the exponential number of unique raw n-grams. A consequence of these small feature vocabularies is that we can also use small feature embeddings, typically Dg=16.\nQuantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015). We compress the embedding weights (the vast majority of the parameters for these shallow models) by storing scale factors for each embedding (details in the supplementary material). In \u00a73.1, we contrast devoting model size to higher\nprecision and lower dimensionality versus lower precision and more network dimensions.\nTraining Our objective function combines the cross-entropy loss for model predictions relative to the ground truth with L2 regularization of the biases and hidden layer weights. For optimization, we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates. The mini-batch size is fixed to 32 and we perform a grid search for the other hyperparameters, tuning against the task-specific evaluation metric on held-out data, with early stopping. Full feature templates and optimal hyperparameter settings are given in the supplementary material."}, {"heading": "3 Experiments", "text": "We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation.\nEvaluation Metrics In addition to standard task-specific quality metrics, our evaluations also consider model size and computational cost. We skirt implementation details by calculating size as the number of kilobytes (1KB=1024 bytes) needed to represent all model parameters and resources. We approximate the computational cost as the number of floating-point operations (FLOPs) performed for one forward pass through the network given an embedding vector h0. This cost is dominated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal-\nization, but still accounts for the final layer logits. To ground this metric, we also provide indicative absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3)."}, {"heading": "3.1 Language Identification", "text": "Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification. We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages. For this task, we obtain the input h0 by separately averaging the embeddings for each ngram length (N = [1, 4]), as summation did not produce good results.\nTable 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010). Their nearest neighbor model is the most accurate but its memory scales linearly with the size of the training data.\nMoreover, we can apply quantization to the embedding matrix without hurting prediction accuracy: it is better to use less precision for each dimension, but to use more dimensions. Our subsequent models all use quantization. There is no noticeable variation in processing speed when performing dequantization on-the-fly at inference time. Our 16-dim Lang-ID model runs at 4450 documents/second (5.6 MB of text per second) on the preprocessed Wikipedia dataset.\nRelationship to Compact Language Detector These techniques back the open-source Compact Language Detector v3 (CLD3)1 that runs in Google Chrome browsers.2 Our experimental Lang-ID model uses the same overall architecture as CLD3, but uses a simpler feature set, less involved preprocessing, and covers fewer languages."}, {"heading": "3.2 POS Tagging", "text": "We apply our model as an unstructured classifier to predict a POS tag for each token independently, and compare its performance to that of the byteto-span (BTS) model (Gillick et al., 2016). BTS is a 4-layer LSTM network that maps a sequence of bytes to a sequence of labeled spans, such as tokens and their POS tags. Both approaches limit\n1github.com/google/cld3 2As of the date of this writing in 2017.\nmodel size by using small input vocabularies: byte values in the case of BTS, and hashed character ngrams and (optionally) cluster ids in our case.\nBloom Mapped Word Clusters It is well known that word clusters can be powerful features in linear models for a variety of tasks (Koo et al., 2008; Turian et al., 2010). Here, we show that they can also be useful in neural network models. However, naively introducing word cluster features drastically increases the amount of memory required, as a word-to-cluster mapping file with hundreds of thousands of entries can be several megabytes on its own.3 By representing word clusters with a Bloom map (Talbot and Talbot, 2008), a key-value based generalization of Bloom filters, we can reduce the space required by a factor of\u223c15 and use 300KB to (approximately) represent the clusters for 250,000 word types.\nIn order to compare against the monolingual setting of Gillick et al. (2016), we train models for the same set of 13 languages from the Universal Dependency treebanks v1.1 (Nivre et al., 2016) corpus, using the standard predefined splits.\nAs shown in Table 2, our best models are 0.3% more accuate on average across all languages than the BTS monolingual models, while using 6x fewer parameters and 36x fewer FLOPs. The cluster features play an important role, providing a 15% relative reduction in error over our vanilla model, but also increase the overall size. Halv-\n3For example, the commonly used English clusters from the BLLIP corpus is over 7 MB \u2013 people.csail.mit. edu/maestro/papers/bllip-clusters.gz\ning all feature embedding dimensions (except for the cluster features) still gives a 12% reduction in error and trims the overall size back to 1.1x the vanilla model, staying well under 1MB in total. This halved model configuration has a throughput of 46k tokens/second, on average.\nTwo potential advantages of BTS are that it does not require tokenized input and has a more accurate multilingual version, achieving 95.85% accuracy. From a memory perspective, one multilingual BTS model will take less space than separate FF models. However, from a runtime perspective, a pipeline of our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4"}, {"heading": "3.3 Segmentation", "text": "Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces. Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No external resources or pretrained embeddings are used. Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task. To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol.\nSelected Features Because we are not using hashing here, we need to be careful about the size of the input vocabulary. The neural network with its non-linearity is in theory able to learn bigrams by conjoining unigrams, but it has been\n4Our calculation of BTS FLOPs is very conservative and favorable to BTS, as detailed in the supplementary material.\nshown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (\u2018Zhang et al. (2016)-combo\u2019 in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work.\nThe results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second."}, {"heading": "3.4 Preordering", "text": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n \u2212 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).\nPipelines For preordering, we experiment with either spending all of our memory budget on reordering, or spending some of the memory budget on features over predicted POS tags, which also requires an additional neural network to predict these tags. Full feature templates are in the supplementary material. As the POS tagger network uses features based on a three word window around the token, another possibility is to add all of the features that would have affected the POS tag of a token to the reorderer directly.\nTable 6 shows results with or without using the predicted POS tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the downstream task. The preorderer that includes a separate network for POS tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a POS tag in the reorderer directly. This pipeline processes 7k tokens/second when taking pretokenized text as input, with the POS tagger accounting for 23% of the computation time."}, {"heading": "4 Conclusions", "text": "This paper shows that small feed-forward networks are sufficient to achieve useful accuracies on a variety of tasks. In resource-constrained environments, speed and memory are important metrics to optimize as well as accuracies. While large and deep recurrent models are likely to be the most accurate whenever they can be afforded, feed-foward networks can provide better value in terms of runtime and memory, and should be considered a strong baseline."}, {"heading": "Acknowledgments", "text": "We thank Kuzman Ganchev, Fernando Pereira, and the anonymous reviewers for their useful comments."}, {"heading": "A Quantization Details", "text": "The values comprising a generic embedding matrix E \u2208 RV\u00d7D are ordinarily stored with 32-bit floating-point precision in our implementation. For quantization, we first calculate a scale factor si for each embedding vector ei as\nsi = 1\nb\u2212 1 max j |eij | .\nEach weight eij is then quantized into an 8-bit integer as\nqij = b 1 2 + eij si + bc,\nwhere the bias b = 128. Hence, the number of bits required to store the embedding matrix is reduced by a factor of 4, in exchange for storing the V additional scale values. At inference time, the embeddings are dequantized on-the-fly."}, {"heading": "B FLOPs Calculation", "text": "The product of A \u2208 RP\u00d7Q and b \u2208 RQ involves P (2Q\u2212 1) FLOPs, and our single ReLu hidden layer requires performing this operation once per timestep (P=M , Q=H0). Here, H0 denotes the size of the embedding vector h0, which equals 408, 464 and 260 for our respective POS models as ordered in Table 2.\nIn contrast, each LSTM layer requires eight products per timestep, and the BTS model has four layers (P=Q=320). The particular sequence-tosequence representation scheme of Gillick et al. (2016) requires at least four timesteps to produce a meaningful output: the individual input byte(s), and a start, length and label of the predicted span. A single timestep is therefore a relaxed lower bound on the number of FLOPs needed for BTS inference."}, {"heading": "C Word Clusters", "text": "The word clusters we use are for the 250k most frequent words from a large unannotated corpus that was clustered into 256 classes using the distributed Exchange algorithm (Uszkoreit and Brants, 2008) and the procedure described in Appendix A of Ta\u0308ckstro\u0308m et al. (2012).\nThe space required to store them in a Bloom map is calculated using the formula derived by\nTalbot and Talbot (2008): each entry requires 1.23 \u2217 (log 1 +H) bits, where H is the entropy of the distribution on the set of values, and = 2\u2212E , with E the number of error bits employed. We use 0 error bits and assume a uniform distribution for the 256 values, i.e. H = 8, hence we need 9.84 bits per entry, or 300KB for the 250k entries."}, {"heading": "D Lang-ID Details", "text": "In our language identification evaluation, the 1,2,3,4-gram embedding vectors each have 6 or 16 dimensions, depending on the experimental setting. Their hashed vocabulary sizes (Vg) are 100, 1000, 5000, and 5000, respectively. The hidden layer size is fixed at M=208.\nWe preprocess data by removing non-alphabetic characters and pieces of markup text (i.e., anything located between < and >, including the brackets). At test time, if this results in an empty string, we skip the markup removal, and if that still results in an empty string, we process the original string. This procedure is an artefact of the Wikipedia dataset, where some documents contain only punctuation or trivial HTML code, yet we must make predictions for them to render the results directly comparable to the literature."}, {"heading": "E POS Details", "text": "The Small FF model in the comparison to BTS uses 2,3,4-grams and some byte unigrams (see feature templates in Table vii). The n-grams have embedding sizes of 16 and the byte unigrams get 4 dimensions. In our 12 -dimension setting, the aforementioned dimensions are halved to 8 and 2.\nCluster features get embedding vectors of size 8. The hashed feature vocabularies for ngrams are 500, 200, and 4000, respectively. The hidden layer size is fixed at M=320.\nbytes \u2200i \u2208 [0, 1], \u2200j \u2208 [0, 3] : l\u00b1j\u00b1i char n-grams \u2200i \u2208 [0, 3], \u2200N \u2208 [2, 4] : {u(N)\u00b1i } clusters \u2200i \u2208 [0, 3] : c\u00b1i\nTable vii: POS tagging feature templates. i is a position relative to the focus token. lj is the value of the j-th UTF8 byte from the start/end of a word. {u(N)} designates the set of Unicode character n-grams in a word. c is the cluster id of a word.\nchar \u2200i \u2208 [0, 1] : \u03c3\u00b1i.c; \u2200i \u2208 [0, 2] \u03b2\u00b1i.c bigram \u2200i \u2208 [0, 1] : \u03c3\u00b1i.b; \u03b2\u00b1i.b\nTable viii: Word segmentation feature templates. \u2018\u03b2\u00b1i\u2019 denotes starting at the i-th character to the left/right of the front of the buffer. \u2018c\u2019 and \u2018b\u2019 denote character and characterbigram, respectively.\nFeatures Positions char bigrams for i \u2208 [0, 1]\u03c3(i)1\nfor i \u2208 [0, 2]\u03c3(i)l\u03c3(i) \u03b2(0)1\nbytes for i \u2208 [0, 1]\u03c3(i)1 for i \u2208 [0, 2]\u03c3(i)l\u03c3(i) \u03b2(0)1 has-swapped for i \u2208 [0, 1]\u03c3(i) tags-main for i \u2208 [0, 1]\u03c3(i)1\nfor i \u2208 [0, 2]\u03c3(i)l\u03c3(i) \u03b2(0)1\ntags-aux for i \u2208 [0, 1]\u03c3(i)2 \u03c3(i)l\u03c3(i)\u22121 for i \u2208 [2, 3]\u03c3(i)1 \u03c3(3)l\u03c3(3) \u03b2(0)2 \u03b2(0)l\u03b2(0)\u22121 \u03b2l\u03b2(0) for j \u2208 [1, 3]\u03b2(j)1 \u03b2(j)l\u03b2(j)\nTable ix: Preordering feature templates. Each feature group applies to the set of positions given. \u03c3(i) denotes the i-th span from the top of the stack, and \u03b2(j) the j-th span from the front of the buffer. Within a span s, the ls tokens are s1...sls , so s1 is the leftmost token in s and sls the rightmost.\nModel. L.R. Mom. \u03b3 Steps D.P. C-64 0.03 0.8 32K 3.8M 0.2 C-256 0.03 0.8 32K 3.6M 0.4 C-64+B-04 0.03 0.8 64K 7.6M 0.3\nTable x: Segmentation: Optimal hyperparameter settings per model for our segmentation experiments reported in Table 4. The columns show learning rate (L.R.), momentum factor (Mom.), the step-frequency at which the learning rate is scaled by 0.96 (\u03b3), and the number of steps at which training was stopped because accuracy peaked on the held-out tuning data. The column D.P. shows the optimal dropout probability.\nL.R. Mom. \u03b3 Steps No POS tags 0.05 0.9 2k 38k w/ POS tags 0.05 0.9 8k 46k POS model 0.05 0.9 8k 500k w/ tagger input fts. 0.1 0.8 4k 76k\nTable xi: Preordering: Optimal hyperparameter settings obtained for our preordering experiments reported in Table 6. Columns have the same meanings as in Table x."}, {"heading": "F Segmentation Details", "text": "Feature templates used in segmentation experiments are listed in Table viii. Besides, we define length feature to be the number of characters between top of \u03c3 and the front of \u03b2, this maximum feature value is clipped to 100. The length feature is used in all segmentation models, and the embedding dimension is set to 6. We set the cutoff for both character and character-bigrams to 2 in order to learn unknown character/bigram embeddings. The hidden layer size is fixed at M=256."}, {"heading": "G Preordering Details", "text": "The feature templates for the preorderer look at the top four spans on the stack and the first four spans in the buffer; for each span, the feature templates look at up to the first two words and last two words within the span. The \u201cvanilla\u201d variant of the preorderer includes character n-grams, word bytes, and whether the span has ever participated in a SWAP transition. The POS features are the predicted tags for the words in these positions. Table ix shows the full feature templates for the preorderer.\nSmall FF 6 dim Small FF 16 dim # L.R. Mom. \u03b3 L.R. Mom. \u03b3 0 0.4 0.9 8k 0.4 0.9 16k 1 0.4 0.9 32k 0.4 0.9 32k 2 0.4 0.9 32k 0.4 0.9 8k 3 0.3 0.9 64k 0.5 0.9 16k 4 0.4 0.8 100k 0.4 0.9 32k 5 0.5 0.8 100k 0.4 0.9 64k 6 0.3 0.9 32k 0.3 0.9 32k 7 0.3 0.9 100k 0.5 0.9 16k 8 0.4 0.9 32k 0.5 0.9 8k 9 0.4 0.9 32k 0.3 0.9 16k\nTable xii: Lang-ID: Optimal hyperparameter settings obtained with the results reported in Table 1. The first column is the cross-validation fold, while the other columns have the same meanings as in Table x.\nLang. L.R. Mom. \u03b3 Steps Acc. Small FF bg 0.1 0.8 32k 90k 97.12 cs 0.05 0.9 32k 480k 97.97 da 0.05 0.9 32k 480k 94.17 en 0.01 0.9 128k 660k 92.50 fi 0.05 0.9 8k 210k 93.84 fr 0.1 0.8 64k 60k 95.10 de 0.1 0.8 8k 120k 91.23 el 0.08 0.9 64k 60k 96.88 id 0.08 0.8 32k 180k 91.60 it 0.08 0.8 128k 330k 96.79 fa 0.08 0.9 128k 60k 95.80 es 0.1 0.8 32k 60k 94.37 sv 0.1 0.9 8k 210k 94.54 Small FF + Clusters bg 0.08 0.8 64k 120k 97.72 cs 0.1 0.8 16k 420k 98.12 da 0.1 0.8 32k 360k 95.49 en 0.05 0.8 8k 510k 93.88 fi 0.1 0.8 8k 300k 94.97 fr 0.05 0.9 8k 630k 95.65 de 0.05 0.9 8k 480k 92.40 el 0.1 0.9 8k 60k 97.60 id 0.1 0.8 64k 150k 91.94 it 0.1 0.8 32k 270k 97.36 fa 0.08 0.9 64k 90k 96.24 es 0.05 0.9 128k 30k 95.01 sv 0.08 0.9 16k 150k 95.90 Small FF (12 Dim.) + Clusters bg 0.1 0.8 128k 210k 97.76 cs 0.05 0.9 32k 420k 98.06 da 0.05 0.9 16k 240k 95.33 en 0.05 0.8 8k 300k 93.06 fi 0.05 0.9 16k 390k 94.66 fr 0.08 0.9 128k 120k 95.28 de 0.08 0.9 16k 90k 92.13 el 0.08 0.9 16k 60k 97.42 id 0.08 0.9 8k 690k 92.15 it 0.05 0.9 64k 210k 97.42 fa 0.1 0.8 8k 510k 96.19 es 0.08 0.9 8k 60k 94.79 sv 0.1 0.8 16k 300k 95.76\nTable xiii: POS: Optimal hyperparameter settings per language obtained for our POS experiments. Columns have the same meanings as in Table x. The final column shows the test set accuracies that back the averages shown in Table 2."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Language identification: The long and the short of the matter", "author": ["Timothy Baldwin", "Marco Lui."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baldwin and Lui.,? 2010", "shortCiteRegEx": "Baldwin and Lui.", "year": 2010}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou."], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer.", "citeRegEx": "Bottou.,? 2010", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Neural word segmentation learning for Chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 409\u2013420, Berlin, Germany. Association for Compu-", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1."], "venue": "Proceedings of ACL, pages 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Small statistical models by random feature mixing", "author": ["Kuzman Ganchev", "Mark Dredze."], "venue": "Proceedings of the ACL-08- HLT Workshop on Mobile Language Processing, pages 18\u201319. Association for Computational Linguistics.", "citeRegEx": "Ganchev and Dredze.,? 2008", "shortCiteRegEx": "Ganchev and Dredze.", "year": 2008}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Proceedings of NAACL-HLT, pages 1296\u20131306, San Diego, USA. Association for Computational Linguistics.", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Fast and accurate preordering for SMT using neural networks", "author": ["Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Bill Byrne."], "venue": "Proceedings of NAACL, pages 1012\u20131017.", "citeRegEx": "Gispert et al\\.,? 2015", "shortCiteRegEx": "Gispert et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally."], "venue": "arXiv preprint arXiv:1510.00149.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["Geoffrey E. Hinton."], "venue": "Neural Networks: Tricks of the Trade (2nd ed.), Lecture Notes in Computer Science, pages 599\u2013619. Springer.", "citeRegEx": "Hinton.,? 2012", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Sequencelevel knowledge distillation", "author": ["Yoon Kim", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Segmental recurrent neural networks", "author": ["Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "CoRR, abs/1511.06018.", "citeRegEx": "Kong et al\\.,? 2015", "shortCiteRegEx": "Kong et al\\.", "year": 2015}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of ACL-08: HLT, pages 595\u2013603.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Exploring segment representations for neural segmentation models", "author": ["Yijia Liu", "Wanxiang Che", "Jiang Guo", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY,", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Discriminating between similar languages and Arabic dialect identification: A report on the third DSL shared task", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov", "Ahmed Ali", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the Third", "citeRegEx": "Malmasi et al\\.,? 2016", "shortCiteRegEx": "Malmasi et al\\.", "year": 2016}, {"title": "Overview for the second shared task on language identification in code-switched data", "author": ["Giovanni Molina", "Fahad AlGhamdi", "Mahmoud Ghoneim", "Abdelati Hawwari", "Nicolas ReyVillamizar", "Mona Diab", "Thamar Solorio."], "venue": "Proceedings", "citeRegEx": "Molina et al\\.,? 2016", "shortCiteRegEx": "Molina et al\\.", "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning, pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Efficient top-down BTG parsing for machine translation preordering", "author": ["Tetsuji Nakagawa."], "venue": "Proceedings of ACL, pages 208\u2013218.", "citeRegEx": "Nakagawa.,? 2015", "shortCiteRegEx": "Nakagawa.", "year": 2015}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Distributed Word Clustering for Large Scale ClassBased Language Modeling in Machine Translation", "author": ["Jakob Uszkoreit", "Thorsten Brants."], "venue": "ACL, pages 755\u2013762.", "citeRegEx": "Uszkoreit and Brants.,? 2008", "shortCiteRegEx": "Uszkoreit and Brants.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Improving a statistical MT system with automatically learned rewrite patterns", "author": ["Fei Xia", "Michael McCord."], "venue": "Proceedings of COLING, page 508.", "citeRegEx": "Xia and McCord.,? 2004", "shortCiteRegEx": "Xia and McCord.", "year": 2004}, {"title": "Neural word segmentation with rich pretraining", "author": ["Jie Yang", "Yue Zhang", "Fei Dong."], "venue": "CoRR, abs/1704.08960.", "citeRegEx": "Yang et al\\.,? 2017", "shortCiteRegEx": "Yang et al\\.", "year": 2017}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 421\u2013431, Berlin, Germany. Associa-", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840\u2013 847, Prague, Czech Republic. Association for Com-", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al.", "startOffset": 130, "endOffset": 170}, {"referenceID": 11, "context": "Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.", "startOffset": 37, "endOffset": 71}, {"referenceID": 7, "context": ", the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters.", "startOffset": 39, "endOffset": 61}, {"referenceID": 7, "context": ", the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.", "startOffset": 39, "endOffset": 257}, {"referenceID": 4, "context": "diction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks.", "startOffset": 114, "endOffset": 138}, {"referenceID": 19, "context": "A single hidden layer, h1, with M rectified linear units (Nair and Hinton, 2010) is fully connected to h0.", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 126, "endOffset": 170}, {"referenceID": 23, "context": "Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 126, "endOffset": 170}, {"referenceID": 15, "context": "Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch.", "startOffset": 59, "endOffset": 78}, {"referenceID": 6, "context": "We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008).", "startOffset": 127, "endOffset": 153}, {"referenceID": 9, "context": "Quantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015).", "startOffset": 128, "endOffset": 146}, {"referenceID": 2, "context": "we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates.", "startOffset": 71, "endOffset": 99}, {"referenceID": 10, "context": "we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates.", "startOffset": 71, "endOffset": 99}, {"referenceID": 18, "context": "Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": ", 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification.", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages.", "startOffset": 109, "endOffset": 132}, {"referenceID": 1, "context": "Table 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010). Their nearest neighbor model is the most accurate but its memory scales linearly with the size of the training data.", "startOffset": 74, "endOffset": 97}, {"referenceID": 7, "context": "We apply our model as an unstructured classifier to predict a POS tag for each token independently, and compare its performance to that of the byteto-span (BTS) model (Gillick et al., 2016).", "startOffset": 167, "endOffset": 189}, {"referenceID": 1, "context": "The two models from Baldwin and Lui (2010) are the nearest neighbor (NN) and nearest prototype (NP) approaches.", "startOffset": 20, "endOffset": 43}, {"referenceID": 14, "context": "known that word clusters can be powerful features in linear models for a variety of tasks (Koo et al., 2008; Turian et al., 2010).", "startOffset": 90, "endOffset": 129}, {"referenceID": 7, "context": "In order to compare against the monolingual setting of Gillick et al. (2016), we train models for the same set of 13 languages from the Universal Dependency treebanks v1.", "startOffset": 55, "endOffset": 77}, {"referenceID": 26, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 3, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 16, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 25, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 13, "context": "Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).", "startOffset": 76, "endOffset": 172}, {"referenceID": 3, "context": ", 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.", "startOffset": 8, "endOffset": 212}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014).", "startOffset": 79, "endOffset": 117}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (\u2018Zhang et al.", "startOffset": 80, "endOffset": 139}, {"referenceID": 26, "context": "shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (\u2018Zhang et al. (2016)-combo\u2019 in Table 4).", "startOffset": 80, "endOffset": 251}, {"referenceID": 24, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 5, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 20, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).", "startOffset": 129, "endOffset": 214}, {"referenceID": 21, "context": "Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans.", "startOffset": 55, "endOffset": 68}, {"referenceID": 5, "context": "Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n \u2212 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).", "startOffset": 152, "endOffset": 837}], "year": 2017, "abstractText": "We show that small and shallow feedforward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.", "creator": "LaTeX with hyperref package"}}}