{"id": "1708.00339", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin", "abstract": "The past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together? Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; we call AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long short-term memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map. For example, when a group of individuals were asked to perform the task, the task did not respond at all, although they did do not respond at all. These findings suggest that having a complete and efficient LSTM in place at the LSTM allows us to predict the distribution of significant patterns across the long-term (Fig. 1B). The LSTM is a unique feature of our knowledge-based deep learning, and the learning model is able to efficiently analyze several features of the LSTM. AttentiveChrome provides the following in-depth analysis:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 1 Aug 2017 14:06:12 GMT  (661kb,D)", "http://arxiv.org/abs/1708.00339v1", "12 pages"], ["v2", "Wed, 2 Aug 2017 17:20:13 GMT  (701kb,D)", "http://arxiv.org/abs/1708.00339v2", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["ritambhara singh", "jack lanchantin", "arshdeep sekhon", "yanjun qi"], "accepted": true, "id": "1708.00339"}, "pdf": {"name": "1708.00339.pdf", "metadata": {"source": "CRF", "title": "Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin", "authors": ["Ritambhara Singh", "Jack Lanchantin", "Arshdeep Sekhon", "Yanjun Qi"], "emails": ["yanjun@virginia.edu"], "sections": [{"heading": "1 Introduction", "text": "Gene regulation is the process of how a cell controls which genes in its genome are turned \u201con\u201d (expressed) or \u201coff\u201d (not-expressed). The human body contains hundreds of different cell types, from liver cells to blood cells to neurons. Although these cells include the same set of DNA information, they function differently 2. The regulation of different genes controls the destiny and function of each cell. In addition to DNA sequence information, many factors, especially those in its environment (i.e., chromatin) can affect which genes the cell expresses. This paper proposes an attention-based deep learning architecture to learn from data how different chromatin factors influence gene expression in a cell. Such understanding of gene regulation can enable new insights into principles of life, the study of disease, and drug development.\n\u201cChromatin\u201d denotes DNA and its organizing proteins 3. A cell uses specialized proteins to organize DNA in a condensed structure. These proteins include histones, which form \u201cbead\u201c-like structures that DNA wraps around, in turn organizing and making the DNA more compact. An important aspect of histone proteins is that they are prone to chemical modifications that can change\n1Code and data are shared at www.deepchrome.org. 2DNA is a long string of paired chemical units that fall into four different types (ATCG). DNA carries information organized into units\nsuch as genes. The set of DNA in a cell is called its genome. 3The complex of DNA, histones, and other structural proteins is called chromatin.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 8.\n00 33\n9v 1\n[ cs\n.L G\n] 1\nA ug\nthe spatial arrangement of DNA, resulting in certain DNA regions becoming accessible or restricted and therefore affecting expressions of genes in the neighborhood region. Researchers have established the \u201cHistone Code Hypothesis\u201d that explores the role of histone modifications in controlling gene regulation. Unlike genetic mutations, chromatin changes such as histone modifications are potentially reversible ([5]). This crucial difference makes the understanding of how chromatin factors determine gene regulation even more impactful because the knowledge can help developing drugs targeting genetic diseases.\nAt the whole genome level, researchers are trying to chart the locations and intensities of all the chemical modifications, referred to as marks, over the chromatin 4. Recent advances in nextgeneration sequencing have allowed biologists to profile a significant amount of gene expression and chromatin patterns as signals (or read counts) across many cell types covering the full human genome. These datasets have been made available through large-scale repositories, the latest being the Roadmap Epigenome Project (REMC, publicly available) ([18]). REMC recently released 2,804 genome-wide datasets, among which 166 datasets are gene expression reads (RNA-Seq datasets) and the rest are signal reads of various chromatin marks across 100 different \u201cnormal\u201d human cells/tissues (1,821 datasets for histone modification marks) [18].\nThe fundamental aim of processing and understanding this repository of \u201cbig\u201d data is to understand gene regulation. For each cell type, we want to know which chromatin marks are the most important and how they work together in controlling gene expression. However, previous machine learning studies on this task either failed to model spatial dependencies among mark signals or required additional feature analysis to explain the predictions (Section 3). Computational tools should consider two important properties when modeling such data.\n\u2022 First, signal reads for each mark are spatially structured and high-dimensional. For instance, to quantify the influence of a histone modification mark, learning methods typically need to use all of its signals covering a DNA region of length 10, 000 base pair (bp)5 centered at the transcription start site (TSS) of each gene (as input features). These signals are sequentially ordered along the genome direction. To develop \u201cepigenetic\u201d drugs, it is important to recognize how a chromatin mark\u2019s effect on regulation varies across different genome locations. \u2022 Second, various types of marks exist over human chromatin that can influence gene regulation. For example, each of the five standard histone proteins can be simultaneously modified at multiple different sites with various kinds of chemical modifications, resulting in many different kinds of histone modification marks. For each mark, we build a feature vector representing its signals surrounding a gene\u2019s TSS position. When modeling genome-wide signal reads from multiple marks, learning algorithms should take into account the modular nature in such feature inputs, where each mark functions as a module. We want to understand how the interactions among these modules influence the prediction.\nIn this paper we propose an attention-based deep learning model, AttentiveChrome, that learns to predict the expression of a gene from an input of histone modification signals covering the gene\u2019s neighboring DNA region. By using a hierarchy of multiple LSTM modules, AttentiveChrome can discover interactions among signals of each chromatin mark, and simultaneously learn complex dependencies among different marks. Two levels of \u201csoft\u201d attention mechanisms are trained, (1) to attend to the most relevant regions of a chromatin mark, and (2) to recognize and attend to the important marks. Through predicting and attending in one unified architecture, AttentiveChrome allows users to understand how chromatin marks control gene regulation in a cell. In summary, this work makes the following contributions:\n\u2022 AttentiveChrome provides more accurate predictions than state-of-the-art baselines. Using datasets from REMC, We evaluate AttentiveChrome on 56 different cell types (tasks). \u2022 We validate and compare interpretation scores using correlation to new mark signals from REMC (not used in modeling). AttentiveChrome\u2019s attention scores provide a better interpretation than state-of-the-art methods for visualizing deep learning models. \u2022 AttentiveChrome can model highly modular inputs where each module is highly structured. AttentiveChrome can explain its decisions by providing \u201cwhat\u201d and \u201cwhere\u201d the model has\n4In biology this field is called epigenetics. \u201cEpi\u201d in Greek means over. The epigenome in a cell is the set of chemical modifications over the chromatin that alter gene expression.\n5A base pair refers to one of the double-stranded DNA sequence characters (ACGT)\nfocused. The flexible and interpretability make this model an ideal approach for many real-world applications. \u2022 To the authors\u2019 best knowledge, AttentiveChrome is the first attention-based deep learning method for modeling data from molecular biology.\nIn the following sections, we denote vectors with bold font and matrices using capital letters. To simplify notation, we use \u201cHM\u201d as a short form for the term \u201chistone modification\u201d."}, {"heading": "1.1 Background: Long Short-Term Memory (LSTM) Networks", "text": "Recurrent neural networks (RNNs) have been widely used in applications like natural language processing 6. Given an input matrix X of size n in\n\u21e5 T , an RNN produces a matrix H of size d\u21e5 T , where n\nin is the input feature size, T is the length of input feature , and d is the RNN embedding size. At each timestep t 2 {1, \u00b7 \u00b7 \u00b7 , T}, an RNN takes an input column vector x\nt 2 Rnin and the previous hidden state vector ht 1 2 Rd and produces the next hidden state ht by applying the following recursive operation:\nh t = (Wx t + Uh t 1 + b) =\n! LSTM(x\nt\n), (1)\nwhere W,U,b are the trainable parameters of the model, and is an element-wise nonlinearity function. Due to the recursive nature, RNNs can capture the complete set of dependencies among all timesteps being modeled, like all spatial positions in a sequential sample. To handle the \u201cvanishing gradient\u201d issue of training basic RNNs, [13] proposed an RNN variant called the Long Short-term Memory (LSTM) network.\nAn LSTM layer has an input-to-state component and a recurrent state-to-state component like those in Eq. (1). Additionally, it has gating functions that control when information is written to, read from, and forgotten. Though LSTM formulation results in a complex form of Eq. (1), when given input vector x\nt and the state h t 1 from previous time step t 1, an LSTM module also produces\na new state h t . The embedding vector h t encodes the learned representation summarizing feature dependencies from the time step 0 to the time step t. For our task, we call each bin position on the genome coordinate a \u201ctime step\u201d."}, {"heading": "2 AttentiveChrome: A Deep Model for Joint Classification and Visualization", "text": "Input and output formulation for the task: We use the same feature inputs and outputs as done previously in DeepChrome ([28]). Following [7], the gene expression prediction is formulated as a binary classification task whose output represents gene expression of a gene being high(+1) or low(-1). As shown in Figure 1, the input feature of a sample (a particular gene) is denoted as a matrix X of size M \u21e5 T . Here M denotes the number of HM marks we consider in input. T is the total number of bin positions we take into account from the neighboring region of a gene\u2019s TSS site on the genome. We refer to this region as the \u2018gene region\u2019 in the rest of the paper. xj denotes the j-th row vector of X whose elements are sequentially structured (signals from the j-th HM mark) j 2 {1, ..., M}. xj\nt in matrix X represents the signal from the t-th bin of the j-th HM mark. t 2 {1, ..., T}. We assume our\n6 In general, CNNs train much faster than RNNs though the issue of speed is not critical to our task.\ntraining set D contains N tr labeled pairs. We denote the n-th pair as (X(n), y(n)), X(n) is a matrix of size M \u21e5 T and y(n) 2 { 1, +1}. n 2 {1, ..., N\ntr }. An end-to-end deep architecture for predicting and attending jointly: AttentiveChrome learns to predict the expression of a gene from an input of HM signals covering its gene region. First, each HM mark is fed into a separate LSTM network to encode the spatial dependencies among its bin signals, and then another LSTM is used to model how multiple factors work together for predicting gene expression. Two levels of \"soft\" attention mechanisms are trained and dynamically predicted for each gene: (1) to attend to the most relevant positions of an HM mark, and (2) to recognize and attend more to the relevant marks. In summary, AttentiveChrome consists of five main modules: (1) Bin-level LSTM encoder for each HM mark; (2) Bin-level Attention on each HM mark; (3) HM-level LSTM encoder encoding all HM marks; (4) HM-level Attention over all HM marks; (5) the final classification module. We describe the details of each component as follows.\nBin-Level Encoder Using LSTMs: For a gene of interest, the j-th row vector, xj , from X includes a total of T elements that are sequentially ordered along the genome coordinate. Considering the sequential nature of such signal reads, we treat each element (essentially a bin position) as a \u2018time step\u2019 and use a bidirectional LSTM to model the complete dependencies among elements in xj . A bidirectional LSTM contains two LSTMs, one in each direction (see supplementary Figure 3(c)). It includes a forward ! LSTM j that models xj from xj1 to x j\nT\nand a backward LSTM j that models from\nxj T to xj1. For each position t, the two LSTMs output ! hj t and hj t , each of size d. ! hj t = ! LSTM j(xj t )\nand hj t = LSTM j(xj t ). The final embedding vector at the t-th position is the concatenation\nhj t = [ ! hj t , hj t\n]. By coupling these LSTM-based HM encoders with the final classification, they can learn to embed each HM mark by extracting the dependencies among bins that are essential for the prediction task.\nBin-Level Attention, \u21b5-attention: Although the LSTM can encode dependencies among bins, it is difficult to determine which bins are more necessary for prediction from the LSTM. To automatically and adaptively, highlight the most relevant bins for each sample, we use a \"soft\" attention to learn the importance weights of bins. This means when representing j-th HM mark, AttentiveChrome follows a basic concept that not all bins contribute equally to the encoding of the entire j-th HM mark. The attention mechanism can help locate and recognize those bins that are important for the current gene sample of interest from j-th HM mark and can aggregate those important bins to form an embedding vector. This extraction is implemented through learning a weight vector \u21b5j of size T for the j-th HM mark. For t 2 {1, ..., T}, \u21b5j\nt\nrepresents the importance of t-th bin in the j-th HM. It is computed as:\n\u21b5j t\n= exp(Wbh j t)PT\ni=1 exp(Wbh j i )\n.\n\u21b5j t is a scalar and is computed by considering all bins\u2019 embedding vectors {hj1, \u00b7 \u00b7 \u00b7 ,hj T }. The context parameter W\nb is randomly initialized and jointly learned with the other model parameters during training. Our intuition is that, through W\nb the model will automatically learn the context of the task (e.g. type of a cell) as well as the positional relevance to the context simultaneously. Once we have the importance weight of each bin position, we can represent the entire j-th HM mark as a weighted sum of all its bin embeddings: mj = P T\nt=1 \u21b5 j t \u21e5 hj t . Essentially the attention weights \u21b5j t\ntell us the relative importance of the t-th bin in the representation mj for the current input X (both hj t and \u21b5j t depend on X).\nEncode Multiple HMs Using another LSTM: We then need to model the joint dependencies among multiple HM marks (essentially the issue of learning to represent a set). Even though there exists no clear order among HMs, we assume an imagined sequence as {HM1, HM2, HM3, ..., HMM}. We implement another bi-directional LSTM encoder, this time on the imagined sequence of HMs using the representations mj of the j-th HMs as LSTM inputs (Figure 3). We aim to capture the dependencies among HMs as some HMs are known to work together to repress or activate gene expression [6]. Setting the embedding size as d0, this set-based encoder, we denote as LSTM\ns , encodes the j-th HM as: sj = [ ! LSTM\ns\n(mj), LSTM\ns (mj)]. Differently from mj , sj encodes the dependencies between the j-th HM and other HM marks.\nHM-Level Attention, -attention: Now we want to focus on the important HM markers for classifying a gene\u2019s expression as high or low. We do this by learning a second level of attention among\nHMs. Similar to learning \u21b5j t , we learn another set of weights j for j 2 {1, \u00b7 \u00b7 \u00b7 , M} representing the importance of HMj . i is calculated similarly as: j = exp(Wss\nj)PM i=1 exp(Wss i) . The HM-level context\nparameter W s learns the context of the task and learns how HMs are relevant to that context. W s is randomly initialized and jointly trained. Then, we encode the entire \"gene region\" into a hidden representation v as a weighted sum of embeddings from all HM marks: v = P M\nj=1 jsj . We can\ninterpret the learned attention weight i as the relative importance of HMi when blending all HM marks to represent the entire gene region for the current gene sample X.\nTraining AttentiveChrome End-to-End: The vector v summarizes the information of all HMs for a gene sample. We feed it to a simple classification module f (Figure 3(f)) that computes the probability of the current gene being expressed high or low: f(v) = softmax(W\nc v + b c ). W c\nand b c are trainable parameters. Since the entire model, including the attention mechanisms, is differentiable, learning end-to-end is trivial by using backpropagation ([21]). All parameters are learned together to minimize a negative log-likelihood loss function that captures the difference between true labels y and predicted scores from f(.)."}, {"heading": "3 Connecting to Previous Studies", "text": "In recent years, there has been an explosion of deep learning models that have led to groundbreaking performance in many fields such as computer vision[17], natural language processing[29], and computational biology [1? , 37, 16, 19, 28]. Attention-based deep models: The idea of attention in deep learning arises from the properties of the human visual system. When perceiving a scene, the human vision gives more importance to some areas over others ([9]). This adaptation of \u201cattention\u201d allows deep learning models to focus selectively on only the important features. Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30]. Attention brings in two benefits: (1) By selectively focusing on parts of the input during prediction the attention mechanisms can reduce the amount of computation and the number of parameters associated with deep learning model ([2, 25]). (2). Attention-based modeling allows for learning salient features dynamically as needed ([33]).\nDifferent attention mechanisms have been proposed in the literature, including \u2018soft\u2019 attention [4], \u2018hard attention\u2019 [32, 23], or \u2018location-aware\u2019 [8]. Soft attention [4] calculates a \u2018soft\u2019 weighting scheme over all the component feature vectors of input. These weights are then used to compute a weighted combination of the candidate feature vectors. The magnitude of an attention weight correlates highly with the degree of significance of the corresponding component feature vector to the prediction. Inspired by [33], AttentiveChrome uses two levels of soft attention for predicting gene expression from HM marks.\nVisualizing and understanding deep models: Although deep learning models have proven to be very accurate, they have widely been viewed as \u201cblack boxes\u201d. Researchers have attempted to develop separate visualisation techniques that explain a deep classifier\u2019s decisions. Most prior studies have focused on understanding convolutional neural networks (CNN) for image classifications, including techniques such as \u201cdeconvolution\u201d [35], \u201csaliency maps\u201d [3, 27] and \u201cclass optimization\u201d based\nvisualisation [27]. The \u201cdeconvolution\u2019 approach [35] maps hidden layer representations back to the input space for a specific example, showing those features of a image that are important for classification. \u201cSaliency maps\" [27] use a first-order Taylor expansion to linearly approximate the deep network and seek most relevant input features. The \u201cclass optimization\u201d based visualisation [27] tries to find the best example (through optimization) that maximizes the probability of the class of interest. Recent studies from [15, 22] explored the interpretability of recurrent neural networks (RNN) for text-based tasks. Moreover, since attention in models allow for automatically extracting salient features, attention-coupled neural networks impart a degree of interpretability. By visualizing what the model attends to in [33], attention can help gauge the predictive importance of a feature and hence interpret the output of a deep neural network.\nDeep learning in bioinformatics: Deep learning is steadily gaining popularity in the bioinformatics community. This trend is credited to their ability to extract meaningful representations from large datasets. For instance, multiple recent studies have successfully used deep learning for modeling protein sequences [? 36], modeling DNA sequences [1, 20], predicting gene expressions [28], as well as understanding the effects of non-coding variants ([37], [26]).\nPrevious machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28]. These studies designed different feature selection strategies to accommodate a large amount of histone modification signals as input. The strategies vary from using signal averaging across all relevant positions, to a \u2018best position\u2019 strategy that selected the input signals at the position with the highest correlation to the target gene expression and automatically learning combinatorial interactions among histone modification marks using CNN (in a so-called DeepChrome study [28]). DeepChrome outperformed all previous methods on this task and used a class optimization-based technique for visualizing the learned model. However, this class-level visualization lacks the necessary granularity to understand the signals from multiple chromatin marks at the individual gene level.\nSupplementary Table 3 compares previous learning studies on the same task with AttentiveChrome across seven desirable model properties. AttentiveChrome is the only model that exhibits all seven properties. In addition, Section 4 compares the attnetion weights from AttentiveChrome with the visualization from \"saliency map\" and \"class optimization.\" Using the correlation to one additional HM mark from REMC, AttentiveChrome shows to provide a better interpretation and validation."}, {"heading": "4 Experiments and Results", "text": "Dataset: Following DeepChrome ([28]), we downloaded gene expression levels and signal data of five core HM marks for 56 different cell types archived by the REMC database ([18]). Each dataset contains information about both the location and the signal intensity for a mark measured across the whole genome. The selected five core HM marks have been uniformly profiled across all 56 cell types in the REMC study ([18]). These five HM marks include (we rename these HMs in our analysis for easy reading): H3K27me3 as H\nreprA , H3K36me3 as H struct , H3K4me1 as H enhc , H3K4me3 as H\nprom , and H3K9me3 as H reprB . HMs H reprA and H reprB are known to repress the gene expression,H\nprom activates gene expression, H struct is found over the gene body, and H enhc\nsometimes helps in activating gene expression.\nModel Variations and Two Baselines: In Section 2, we introduced three main components of AttentiveChrome to handle the task of predicting gene expression from HM marks: LSTMs, attention mechanisms, and hierarchical attention. To investigate the performance of these components, our experiments compare multiple AttentiveChrome model variations plus two standard baselines.\n\u2022 LSTM: We directly apply an LSTM on the input matrix X without adding any attention. This does not consider the modular property of each HM mark, that is, we treat the signals of all HMs at t-th bin position as the t-th input to LSTM. \u2022 CNN: The temporal (1-D) CNN model used by DeepChrome [28] for the same classification task. This study did not consider the modular property of HM marks. \u2022 LSTM-Attn: We add one attention layer on the baseline LSTM model over input X. This does not consider the modular property of HM marks. \u2022 CNN-Attn: We apply one attention layer over the CNN model from DeepChrome [28], but removing the max-pooling layer. This does not consider the modular property of HM marks.\n\u2022 LSTM-\u21b5, : As introduced in Section 2, this model uses one LSTM per HM mark and add one \u21b5-attention per mark. Then it uses another level of LSTM and -attention to combine HMs. \u2022 CNN-\u21b5, : This considers the modular property among HM marks. We apply one CNN per HM mark and add one \u21b5-attention per mark. Then it uses another level of CNN and -attention to combine HMs. \u2022 LSTM-\u21b5: This considers the modular property of HM marks. We apply one LSTM per HM mark and add one \u21b5-attention per mark. Then, the embedding of HM marks are concatenated as one long vector, and then fed to a 2-layer MLP module.\nSince we use datasets across 56 cell types, the above methods are essentially compared across 56 different tasks. More details of hyperparameter tuning, data generation, evaluation metrics and more visualization analysis are in Supplementary.\nDataset Details: Following DeepChrome ([28]), we downloaded gene expression levels and signal data for five core HM marks for 56 different cell types archived by the REMC database ([18]). Each dataset contains information about both the location and the signal intensity for a mark measured across the whole genome. We divided the 10, 000 base pair DNA region (+/ 5000 bp) around the transcription start site (TSS) of each gene into bins (each with 100 continuous bp). For each gene in a certain celltype, the feature generation process generates a 5 \u21e5 100 matrix, X, where columns represent T (= 100) different bins and rows represent M(= 5) HMs. For each celll type,the gene expression has been quantified for all annotated genes in the human genome and has been normalized. As previously mentioned, we formulate the task of gene expression prediction as a binary classification task. Following [7], we use the median gene expression across all genes for a particular cell type as the threshold to discretize expression values. For each cell type, we divided our set of 19,802 gene samples into three separate, but equal-size folds for training (6601 genes), validation (6601 genes), and testing (6600 genes) respectively. The five core HM marks include (1) H3K27me3, a repressor mark; (2) H3K36me3, a structural promoter mark; (3) H3K4me1, an enhancer mark; (4) H3K4me3, a promoter mark; and (5) H3K9me3, a repressor mark.\nModel Hyperparameters: For AttentiveChrome variations, we set the bin-level LSTM embedding size d to 32 and the HM-level LSTM embedding size as 16. Since we implement a bi-directional LSTM, this results in each embedding vector h\nt as size 64 and embedding vector m j as size 32. Therefore, we set the context vectors, Wb and Ws, to size 64 and 32 respectively. 7\nEvaluation Metric for Classification: We use the area under the receiver operating characteristic curve (AUC) as our evaluation metric. AUC represents the probability that a randomly selected \u2018event\u2019 will be regarded with greater suspicion than a randomly selected \u2018non-event\u2019. AUC scores range between 0 and 1, with values closer to 1 indicating successful predictions.\nPerformance Evaluation: Table 1 compares different variations of AttentiveChrome using summarized AUC scores across all 56 cell types on the test set. We find that overall the attention based models outperform the baseline CNN and LSTM. LSTM-\u21b5 has better empirical performance than the LSTM-\u21b5, model. We recommend the use of the proposed AttentiveChrome LSTM-\u21b5, for hypothesis generation because it provides a good trade-off between AUC and interpretability.\nUsing Attention Scores for Interpretation: Unlike images and text, the results for biology are hard to interpret by just looking at them. Therefore, we use additional evidence from REMC as well as introducing a new strategy to qualitatively and quantitatively evaluate the attention \u21b5-map from AttentiveChrome. To specifically validate that the model is focusing its attention at the right bins, we\n7We can view Wb as 1\u21e5 64 matrices.\nuse the read counts of a new HM signal - H3K27ac from REMC database. We represent this HM as H\nactive because this HM marks the region that is active when the gene is \"ON\". H3K27ac is an important indication of activity in the DNA regions and is a good source to validate the results. We did not include H3K27ac Mark as input because it has not been profiled for all 56 cell types we used for prediction. However, the genome-wide reads of this HM mark are available for three important cell types in the blood lineage: H1-hESC (stem cell), GM12878 (blood cell), and K562 (leukemia cell). We therefore chose to compare and validate interpretation in these three cell types. This HM signal has not be used at any stage of the model training or testing. We use it solely for the purpose of analyzing the visualization results.\nWe use the average read counts of H active across all 100 bins and for all the active genes (gene=ON) inthe three selected cell types to compare different visualization methods. We compare the attention \u21b5- maps of the best performing LSTM-\u21b5 and LSTM-\u21b5, models with the other two popular visualization techniques: (1) the Class-based optimization method and (2) the Saliency map applied on the baseline DeepChrome-CNN model. We take the importance weights calculated by all visualization methods for mark H\nprom across 100 bins and then calculate their pearson correlation to H active counts across the same 100 bins. H\nactive counts indicate the actual active regions. Table 2 reports the correlation coefficients between H\nprom weights and read coverage of H active . We observe that attention weights from AttentiveChrome consistently achieve the highest correlation with the actual active regions near the gene, indicating that this method is able to capture the important signals for predicting gene activity. Interestingly, we observe that the saliency map on the DeepChrome-CNN achieves a higher correlation with H\nactive than the Class-based optimization method for two cell types: H1-hESC (stem cell) and K562 (leukemia cell).\nThen we obtain the visualization outputs, representing the important features or bins for each prediction of a certain gene as ON or OFF, for all four methods covered in Table 2. For each particular gene sample, we can visualize and inspect the bin-level attention vectors \u21b5j\nt generated by LSTM-\u21b5 or LSTM-\u21b5, . These vectors are normalized and represent the cell-type-specific importance of bins for each gene.\nIn Figure 2(a), we plot the average bin-level attention weights for each HM for a well-studied cell type GM12878 (blood cell),by averaging \u21b5-maps of all predicted \u201cON\" genes(top) and \u201cOFF\" genes (bottom subfigure). We see that on average for \u201cON\" genes, the attention profiles near the TSS region are well defined for H\nprom , H enhc , and H struct , among which H prom provides the highest average importance. On the contrary, the weights are low and close to uniform for H\nreprA and H reprB . This average trend reverses for \u201cOFF\" genes in which H\nreprA and H reprB seem to gain more importance over H\nprom , H enhc , and H struct . These observations make sense biologically as H prom , H enhc , and H\nstruct are know to encourage gene activation while H reprA and H reprB are known to repress the genes 8.\nThen In Figure 2(b),at the top we plot the average read coverage of H active for the same 100 bins summarizing as our input across all the active genes (gene=ON) for GM12878 cell type. At the bottom using a heatmap, we show the averaged alpha-map from the LSTM-, model over all individual genes predicted as ON for GM12878. Visually, we can tell that the averaged H\nprom profile is quite similar to H\nactive . This makes sense because H prom is related to active regions for \u201cON\" genes.\nFinally in Figure 2(c) we demonstrate the advantage of LSTM-\u21b5, model by printing out the the j weights for genes with differential expressions across the three cell types. That is, we select genes with varying ON(+1)/OFF( 1) states across the three chosen cell types using heatmap. Figure 2(c) visualizes the j weights for one of such differentially regulated gene, PAX5. PAX5 is critical for the gene regulation when stem cells convert to blood cells ([24]).This gene is OFF in the H1-hESC cell stage(left column) but turns ON when the cell develops into GM12878 cell(middle column). The j weight of repressor mark H\nreprA is high when gene=OFF in H1-hESC(left). This same weight decreases when gene=ON in GM12878(middle). In contrast, the j weight of the promoter mark H\nprom increases (darker color) from H1-hESC(left) to GM12878(middle). These trends have been observed in [24] showing that PAX5 relates to the conversion of chromatin states: from a repressive state (H\nprom (H3K4me3): , H reprA (H3K27me3):+) to an active state\n8The small dips at the TSS in both subfigures of Figure 2 (a) are caused by missing signals at the TSS due to the inherent nature of the sequencing experiments.\n(H prom (H3K4me3):+, H reprA (H3K27me3): ). This example shows that the -map visualizes how different HMs work together to influence a gene\u2019s ON/OFF."}, {"heading": "5 Conclusion", "text": "We have presented AttentiveChrome, an attention based deep-learning approach that handles prediction and understanding in one architecture. The advantages of this work include:\n\u2022 AttentiveChrome provides more accurate predictions than state-of-the-art baselines (Table 1). \u2022 The attention scores of AttentiveChrome provide a better interpretation than saliency map and\nclass optimization (Table 2). \u2022 AttentiveChrome can model highly modular feature inputs in which each is sequentially structured. \u2022 To the authors\u2019 best knowledge, AttentiveChrome is the first implementation of deep attention\nmechanism for understanding data about gene regulation. We can gain insights and understand the predictions by locating \u2018what\u2019 and \u2018where\u2019 AttentiveChrome has focused (Figure 2). Many real-world applications are seeking such knowledge from data."}, {"heading": "6.1 More about Method", "text": ""}, {"heading": "6 Supplementary Information", "text": ""}], "references": [{"title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning", "author": ["Babak Alipanahi", "Andrew Delong", "Matthew T Weirauch", "Brendan J Frey"], "venue": "Nature Publishing Group,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "M\u00c3\u017eller. How to explain individual classification decisions", "author": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Regulation of chromatin by histone modifications", "author": ["Andrew J Bannister", "Tony Kouzarides"], "venue": "Cell research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Polycomb repressive complex 2 and h3k27me3 cooperate with h3k9 methylation to maintain heterochromatin protein 1\u21b5 at chromatin", "author": ["Joanna Boros", "Nausica Arnoult", "Vincent Stroobant", "Jean-Fran\u00e7ois Collet", "Anabelle Decottignies"], "venue": "Molecular and cellular biology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A statistical framework for modeling gene expression using chromatin features and application to modencode datasets", "author": ["Chao Cheng", "Koon-Kiu Yan", "Kevin Y Yip", "Joel Rozowsky", "Roger Alexander", "Chong Shou", "Mark Gerstein"], "venue": "Genome Biol,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Attentionbased models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Control of goal-directed and stimulus-driven attention in the brain", "author": ["Maurizio Corbetta", "Gordon L Shulman"], "venue": "Nature reviews neuroscience,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Modeling gene expression using chromatin features in various cellular contexts", "author": ["Xianjun Dong", "Melissa C Greven", "Anshul Kundaje", "Sarah Djebali", "James B Brown", "Chao Cheng", "Thomas R Gingeras", "Mark Gerstein", "Roderic Guig\u00f3", "Ewan Birney"], "venue": "Genome Biol,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The correlation between histone modifications and gene expression", "author": ["Xianjun Dong", "Zhiping Weng"], "venue": "Epigenomics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Combinatorial roles of dna methylation and histone modifications on gene expression", "author": ["Bich Hai Ho", "Rania Mohammed Kotb Hassen", "Ngoc Tu Le"], "venue": "In Some Current Advanced Researches on Information and Computer Science in Vietnam,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Long short-term memory. volume 9, pages 1735\u20131780", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Histone modification levels are predictive for gene expression", "author": ["Rosa Karli\u0107", "Ho-Ryun Chung", "Julia Lasserre", "Kristian Vlahovi\u010dek", "Martin Vingron"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Basset: Learning the regulatory code of the accessible genome with deep convolutional neural networks", "author": ["David R Kelley", "Jasper Snoek", "John L Rinn"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Integrative analysis of 111 reference human epigenomes", "author": ["Anshul Kundaje", "Wouter Meuleman", "Jason Ernst", "Misha Bilenky", "Angela Yen", "Alireza Heravi-Moussavi", "Pouya Kheradpour", "Zhizhuo Zhang", "Jianrong Wang", "Michael J Ziller"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep motif: Visualizing genomic sequence classifications", "author": ["Jack Lanchantin", "Ritambhara Singh", "Zeming Lin", "Yanjun Qi"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks", "author": ["Jack Lanchantin", "Ritambhara Singh", "Beilun Wang", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1608.03644,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "The transcription factor pax5 regulates its target genes by recruiting chromatin-modifying proteins in committed b cells", "author": ["Shane McManus", "Anja Ebert", "Giorgia Salvagiotto", "Jasna Medvedovic", "Qiong Sun", "Ido Tamir", "Markus Jaritz", "Hiromi Tagoh", "Meinrad Busslinger"], "venue": "The EMBO journal,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Danq: a hybrid convolutional and recurrent deep neural network for quantifying the function of dna sequences", "author": ["Daniel Quang", "Xiaohui Xie"], "venue": "Nucleic acids research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Deepchrome: deep-learning for predicting gene expression from histone", "author": ["Ritambhara Singh", "Jack Lanchantin", "Gabriel Robins", "Yanjun Qi"], "venue": "modifications. Bioinformatics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In ECCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deep supervised and convolutional generative stochastic network for protein secondary structure prediction", "author": ["Jian Zhou", "Olga G Troyanskaya"], "venue": "arXiv preprint arXiv:1403.1347,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Unlike genetic mutations, chromatin changes such as histone modifications are potentially reversible ([5]).", "startOffset": 102, "endOffset": 105}, {"referenceID": 16, "context": "These datasets have been made available through large-scale repositories, the latest being the Roadmap Epigenome Project (REMC, publicly available) ([18]).", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "REMC recently released 2,804 genome-wide datasets, among which 166 datasets are gene expression reads (RNA-Seq datasets) and the rest are signal reads of various chromatin marks across 100 different \u201cnormal\u201d human cells/tissues (1,821 datasets for histone modification marks) [18].", "startOffset": 276, "endOffset": 280}, {"referenceID": 11, "context": "To handle the \u201cvanishing gradient\u201d issue of training basic RNNs, [13] proposed an RNN variant called the Long Short-term Memory (LSTM) network.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "2 AttentiveChrome: A Deep Model for Joint Classification and Visualization Input and output formulation for the task: We use the same feature inputs and outputs as done previously in DeepChrome ([28]).", "startOffset": 195, "endOffset": 199}, {"referenceID": 5, "context": "Following [7], the gene expression prediction is formulated as a binary classification task whose output represents gene expression of a gene being high(+1) or low(-1).", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "We aim to capture the dependencies among HMs as some HMs are known to work together to repress or activate gene expression [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 25, "context": "Table 1: AUC score performances for different variations of AttentiveChrome and baselines Baselines AttentiveChrome Variations Model CNN[28] LSTM CNN-Attn CNN-\u21b5, LSTM-Attn LSTM-\u21b5 LSTM-\u21b5, Mean 0.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "Since the entire model, including the attention mechanisms, is differentiable, learning end-to-end is trivial by using backpropagation ([21]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "3 Connecting to Previous Studies In recent years, there has been an explosion of deep learning models that have led to groundbreaking performance in many fields such as computer vision[17], natural language processing[29], and computational biology [1? , 37, 16, 19, 28].", "startOffset": 184, "endOffset": 188}, {"referenceID": 26, "context": "3 Connecting to Previous Studies In recent years, there has been an explosion of deep learning models that have led to groundbreaking performance in many fields such as computer vision[17], natural language processing[29], and computational biology [1? , 37, 16, 19, 28].", "startOffset": 217, "endOffset": 221}, {"referenceID": 7, "context": "When perceiving a scene, the human vision gives more importance to some areas over others ([9]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 142, "endOffset": 145}, {"referenceID": 28, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 204, "endOffset": 208}, {"referenceID": 26, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 231, "endOffset": 235}, {"referenceID": 29, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 268, "endOffset": 272}, {"referenceID": 30, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 303, "endOffset": 307}, {"referenceID": 27, "context": "Deep neural networks augmented with attention mechanisms have obtained great success on multiple research topics such as machine translation ([4]), object recognition ([2, 25]), image caption generation ([32]), question answering ([29]), text document classification ([33]), video description generation[34], visual question answering[31], or solving discrete optimization [30].", "startOffset": 334, "endOffset": 338}, {"referenceID": 29, "context": "Attention-based modeling allows for learning salient features dynamically as needed ([33]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "Different attention mechanisms have been proposed in the literature, including \u2018soft\u2019 attention [4], \u2018hard attention\u2019 [32, 23], or \u2018location-aware\u2019 [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 28, "context": "Different attention mechanisms have been proposed in the literature, including \u2018soft\u2019 attention [4], \u2018hard attention\u2019 [32, 23], or \u2018location-aware\u2019 [8].", "startOffset": 118, "endOffset": 126}, {"referenceID": 21, "context": "Different attention mechanisms have been proposed in the literature, including \u2018soft\u2019 attention [4], \u2018hard attention\u2019 [32, 23], or \u2018location-aware\u2019 [8].", "startOffset": 118, "endOffset": 126}, {"referenceID": 6, "context": "Different attention mechanisms have been proposed in the literature, including \u2018soft\u2019 attention [4], \u2018hard attention\u2019 [32, 23], or \u2018location-aware\u2019 [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "Soft attention [4] calculates a \u2018soft\u2019 weighting scheme over all the component feature vectors of input.", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "Inspired by [33], AttentiveChrome uses two levels of soft attention for predicting gene expression from HM marks.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "Most prior studies have focused on understanding convolutional neural networks (CNN) for image classifications, including techniques such as \u201cdeconvolution\u201d [35], \u201csaliency maps\u201d [3, 27] and \u201cclass optimization\u201d based", "startOffset": 157, "endOffset": 161}, {"referenceID": 1, "context": "Most prior studies have focused on understanding convolutional neural networks (CNN) for image classifications, including techniques such as \u201cdeconvolution\u201d [35], \u201csaliency maps\u201d [3, 27] and \u201cclass optimization\u201d based", "startOffset": 179, "endOffset": 186}, {"referenceID": 24, "context": "Most prior studies have focused on understanding convolutional neural networks (CNN) for image classifications, including techniques such as \u201cdeconvolution\u201d [35], \u201csaliency maps\u201d [3, 27] and \u201cclass optimization\u201d based", "startOffset": 179, "endOffset": 186}, {"referenceID": 24, "context": "visualisation [27].", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "The \u201cdeconvolution\u2019 approach [35] maps hidden layer representations back to the input space for a specific example, showing those features of a image that are important for classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "\u201cSaliency maps\" [27] use a first-order Taylor expansion to linearly approximate the deep network and seek most relevant input features.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "The \u201cclass optimization\u201d based visualisation [27] tries to find the best example (through optimization) that maximizes the probability of the class of interest.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "Recent studies from [15, 22] explored the interpretability of recurrent neural networks (RNN) for text-based tasks.", "startOffset": 20, "endOffset": 28}, {"referenceID": 20, "context": "Recent studies from [15, 22] explored the interpretability of recurrent neural networks (RNN) for text-based tasks.", "startOffset": 20, "endOffset": 28}, {"referenceID": 29, "context": "By visualizing what the model attends to in [33], attention can help gauge the predictive importance of a feature and hence interpret the output of a deep neural network.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "For instance, multiple recent studies have successfully used deep learning for modeling protein sequences [? 36], modeling DNA sequences [1, 20], predicting gene expressions [28], as well as understanding the effects of non-coding variants ([37], [26]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 18, "context": "For instance, multiple recent studies have successfully used deep learning for modeling protein sequences [? 36], modeling DNA sequences [1, 20], predicting gene expressions [28], as well as understanding the effects of non-coding variants ([37], [26]).", "startOffset": 137, "endOffset": 144}, {"referenceID": 25, "context": "For instance, multiple recent studies have successfully used deep learning for modeling protein sequences [? 36], modeling DNA sequences [1, 20], predicting gene expressions [28], as well as understanding the effects of non-coding variants ([37], [26]).", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "For instance, multiple recent studies have successfully used deep learning for modeling protein sequences [? 36], modeling DNA sequences [1, 20], predicting gene expressions [28], as well as understanding the effects of non-coding variants ([37], [26]).", "startOffset": 247, "endOffset": 251}, {"referenceID": 9, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 221, "endOffset": 225}, {"referenceID": 12, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 255, "endOffset": 259}, {"referenceID": 5, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 285, "endOffset": 288}, {"referenceID": 8, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 305, "endOffset": 309}, {"referenceID": 10, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 331, "endOffset": 335}, {"referenceID": 25, "context": "Previous machine learning models for predicting gene expression from histone modification marks: Multiple machine learning methods have been proposed to predict gene expression from histone modification data (surveyed by [11]) including linear regression [14], support vector machines [7], random forests [10], rule-based learning [12] and CNNs [28].", "startOffset": 345, "endOffset": 349}, {"referenceID": 25, "context": "The strategies vary from using signal averaging across all relevant positions, to a \u2018best position\u2019 strategy that selected the input signals at the position with the highest correlation to the target gene expression and automatically learning combinatorial interactions among histone modification marks using CNN (in a so-called DeepChrome study [28]).", "startOffset": 346, "endOffset": 350}, {"referenceID": 25, "context": "4 Experiments and Results Dataset: Following DeepChrome ([28]), we downloaded gene expression levels and signal data of five core HM marks for 56 different cell types archived by the REMC database ([18]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "4 Experiments and Results Dataset: Following DeepChrome ([28]), we downloaded gene expression levels and signal data of five core HM marks for 56 different cell types archived by the REMC database ([18]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 16, "context": "The selected five core HM marks have been uniformly profiled across all 56 cell types in the REMC study ([18]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "\u2022 CNN: The temporal (1-D) CNN model used by DeepChrome [28] for the same classification task.", "startOffset": 55, "endOffset": 59}, {"referenceID": 25, "context": "\u2022 CNN-Attn: We apply one attention layer over the CNN model from DeepChrome [28], but removing the max-pooling layer.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Dataset Details: Following DeepChrome ([28]), we downloaded gene expression levels and signal data for five core HM marks for 56 different cell types archived by the REMC database ([18]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Dataset Details: Following DeepChrome ([28]), we downloaded gene expression levels and signal data for five core HM marks for 56 different cell types archived by the REMC database ([18]).", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "Following [7], we use the median gene expression across all genes for a particular cell type as the threshold to discretize expression values.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "PAX5 is critical for the gene regulation when stem cells convert to blood cells ([24]).", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "These trends have been observed in [24] showing that PAX5 relates to the conversion of chromatin states: from a repressive state (H", "startOffset": 35, "endOffset": 39}], "year": 2017, "abstractText": "The past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what are the relevant factors and how they work together? Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach; we call AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long short-term memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in human. Not only is the proposed architecture more accurate, but its attention scores also provide a better interpretation than state-of-the-art feature visualization methods such as saliency map.1", "creator": "LaTeX with hyperref package"}}}