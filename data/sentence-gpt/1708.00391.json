{"id": "1708.00391", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Aug-2017", "title": "A Continuously Growing Dataset of Sentential Paraphrases", "abstract": "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. A more general approach to identifying which tweets represent the most accurately represent the information, and how this would fit with the broader approach.\n\n\nThe first approach is that it will automatically detect the specific tweets that we see coming from the account. We will also identify which Twitter users have shared their tweets. We'll also identify which tweets represent the most accurately represent the information, and how this would fit with the broader approach.", "histories": [["v1", "Tue, 1 Aug 2017 15:41:51 GMT  (808kb,D)", "http://arxiv.org/abs/1708.00391v1", "11 pages, accepted to EMNLP 2017"]], "COMMENTS": "11 pages, accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wuwei lan", "siyu qiu", "hua he", "wei xu"], "accepted": true, "id": "1708.00391"}, "pdf": {"name": "1708.00391.pdf", "metadata": {"source": "CRF", "title": "A Continuously Growing Dataset of Sentential Paraphrases", "authors": ["Wuwei Lan", "Siyu Qiu", "Hua He", "Wei Xu"], "emails": ["xu.1265}@osu.edu", "siqiu@seas.upenn.edu", "huah@umd.edu"], "sections": [{"heading": "1 Introduction", "text": "A paraphrase is a restatement of meaning using different expressions (Bhagat and Hovy, 2013). It is a fundamental semantic relation in human language, as formalized in the Meaning-Text linguistic theory which defines meaning as \u2018invariant of paraphrases\u2019 (Milic\u0301evic\u0301, 2006). Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al.,\n1The code and data can be obtained from the first and last author\u2019s websites.\n2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks.\nIn this paper, we address a major challenge in paraphrase research \u2014 the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases:2 the MSRP corpus derived from clustered news articles (Dolan and Brockett, 2005) and the PIT-2015 corpus from Twitter trending topics (Xu et al., 2014, 2015). Our goal is not only to create a new annotated paraphrase corpus, but to identify a new data source and method that can narrow down the search space of paraphrases without using the classifier-biased or human-in-the-loop data selection as in MSRP and PIT-2015. This is so that sentential paraphrases can be conveniently and continuously harvested in large quantities to benefit downstream applications.\nWe present an effective method to collect sentential paraphrases from tweets that refer to the same URL and contribute a new gold-standard annotated corpus of 51,524 sentence pairs, which is the largest to date (Table 1). We show the different characteristics of this new dataset contrasting the two existing corpora through the first system-\n2Meaningful non-paraphrases (pairs of sentences that have similar wordings or topics but different meanings, and that are not randomly or artificially generated) have been very difficult to obtain but are very important, because they serve as necessary distractors in training and evaluation.\nar X\niv :1\n70 8.\n00 39\n1v 1\n[ cs\n.C L\n] 1\nA ug\n2 01\n7\natic study of paraphrase identification across multiple datasets. Our new corpus is complementary to previous work, as the corpus contains multiple references of both formal well-edited and informal user-generated texts. This is also the first work that provides a continuously growing collection, with more than 30,000 new sentential paraphrases per month automatically labeled at \u223c70% precision. We demonstrate that up-to-date phrasal paraphrases can then be extracted via word alignment (see examples in Table 2). We plan to continue collecting paraphrases using our method and release a constantly updating paraphrase resource."}, {"heading": "2 Existing Paraphrase Corpora and Their Limitations", "text": "To date, there exist only two publicly available corpora of both sentential paraphrases and nonparaphrases:\nMSR Paraphrase Corpus [MSRP] (Dolan et al., 2004; Dolan and Brockett, 2005) This corpus contains 5,801 pairs of sentences from news articles, with 4,076 for training and the remaining 1,725 for testing. It was created from clustered news articles by using an SVM classifier (using\n3Another 12 name variations are omitted in the paper due to their offensive nature.\nfeatures including string similarity and WordNet synonyms) to gather likely paraphrases, then annotated by human on semantic equivalence. The MSRP corpus has a known deficiency skewed toward over-identification (Das and Smith, 2009), because the \u201cpurpose was not to evaluate the potential effectiveness of the classifier itself, but to identify a reasonably large set of both positive and plausible \u2018near-miss\u2019 negative examples\u201d (Dolan and Brockett, 2005). It contains a large portion of sentence pairs with many ngrams shared in common.\nTwitter Paraphrase Corpus [PIT-2015] (Xu et al., 2014, 2015) This corpus was derived from Twitter\u2019s trending topic data. The training set contains 13,063 sentence pairs on 400 distinct topics, and the test set contains 972 sentence pairs on 20 topics. As numerous Twitter users spontaneously talk about varied topics, this dataset contains many lexically divergent paraphrases. However, this method requires a manual step of selecting topics to ensure the quality of collected paraphrases, because many topics detected automatically are either incorrect or too broad. For example, the topic \u201cNew York\u201d relates to tweets with a wide range of information and cannot narrow the search space down enough for human annotation and the subsequent application of classification algorithms."}, {"heading": "3 Constructing the Twitter URL Paraphrase Corpus", "text": "For paraphrase acquisition, it has been crucial to find a simple and effective way to locate paraphrase candidates (see related work in Section 6). We show the efficacy of tracking URLs in Twitter. This method does not rely on automatic news clustering as in MSRP or topic detection as in PIT2015, but it keeps collecting good candidate paraphrase pairs in large quantities."}, {"heading": "3.1 Data Source: News Tweets vs. Streaming", "text": "We extracted the embedded URL in each tweet and used Twitter\u2019s Search API to retrieve all tweets that contain the same URL. Some tweets use shortened URLs, which we resolve as full URLs. We tracked 22 English news accounts in Twitter to create the paraphrase corpus in this paper (see examples in Table 3). We will extend the corpus to include other languages and domains in future work.\nAs shown in Table 5, nearly all the tweets posted by news agencies have embedded URLs. About 51.17% of posts contain two URLs, usually one pointing to a news article and the other to media such as a photo or video. Although close to half of the tweets in Twitter streaming data4 contain at least one URL, most of them are very hard to read (see examples in Table 4).\n4We used Twitters Streaming API which provided a realtime stream of public tweets posted on Twitter."}, {"heading": "3.2 Filtering of Retweets", "text": "Retweeting is an important feature in Twitter. There are two types: automatic and manual retweets. An automatic retweet is done by clicking the retweet button on Twitter and is easy to remove using the Twitter API. A manual retweet occurs when the user creates a new tweet by copying and pasting the original tweet and possibly adding some extras, such as hashtags, usernames or comments. It is crucial to remove these redundant tweets with minor variations, which otherwise represent a significant portion of the data (Table 6). We preprocessed the tweets using a tokenizer5 (Gimpel et al., 2011) and an in-house sentence splitter. We then filtered out manual retweets using a set of rules, checking if one tweet was a sub- string of the other, or if it only differed in punctuation, or the contents of the \u201ctwitter:title\u201d or \u201ctwitter:description\u201d tag in the linked HTML file of the news article.\nTable 6 shows the effectiveness of the filtering. We used PINC, a standard paraphrase metric, to measure ngram-based dissimilarity (Chen and Dolan, 2011), and Jaccard metric to measure token-based string similarity (Jaccard, 1912). After filtering, the dataset contains tweets with more significant rephrasing as indicated by higher PINC and lower Jaccard scores.\n5http://www.cs.cmu.edu/\u02dcark/TweetNLP/"}, {"heading": "3.3 Gold Standard Corpus", "text": "To get the gold-standard paraphrase corpus, we obtained human labels on Amazon Mechanical Turk. We showed annotators an original sentence, and asked them to select sentences with the same meaning from 10 candidate sentences. For each question, we recruited 6 annotators and paid $0.03 to each worker.6 On average, each question took about 53 seconds to finish. For each sentence pair, we aggregated the paraphrase and non-paraphrase labels using the majority vote.\nWe constructed the largest gold standard paraphrase corpus to date, with 42,200 tweets of 4,272 distinct URLs annotated in the training set and 9,324 tweets of 915 distinct URLs in the test set. The training data was collected between 10/10/2016 and 11/22/2016, and testing data between 01/09/2017 and 01/19/2017. In Section 4, we contrast the characteristics of our data against existing paraphrase corpora.\nQuality Control We evaluated the annotation quality of each worker using Cohen\u2019s kappa agreement (Artstein and Poesio, 2008) against the majority vote of other workers. We asked the best workers (the top 528 out of 876) to label more data by republishing the questions done by workers with low reliability (Cohen\u2019s kappa <0.4).\nInter-Annotator Agreement In addition, we had 300 sampled sentence pairs independently annotated by an expert. The annotated agreement is 0.739 by Cohen\u2019s kappa between the expert and the majority vote of 6 crowdsourcing workers. If we assume the expert annotation is gold, the precision of worker vote is 0.871, the recall is 0.787, and F1 is 0.827, similar to those of PIT-2015."}, {"heading": "3.4 Continuous Harvesting of Sentential Paraphrases", "text": "Since our method directly applies to raw tweets, it can continuously extract sentential paraphrases from Twitter. In Section 4, we show that this approach can produce a silver-standard paraphrase corpus at about 70% precision that grows by more than 30,000 new sentential paraphrases per month. Section 5 presents experiments demonstrating the utility of these automatically identified sentential paraphrases.\n6The low pricing helps to not attract spammers to this easy-to-finish task. We gave bonus to workers based on quality and the average hourly pay for each worker is about $7."}, {"heading": "4 Comparison of Paraphrase Corpora", "text": "Though paraphrasing has been widely studied, supporting analyses and experiments have thus far often only been conducted on a single dataset. In this section, we present a comparative analysis of our newly constructed gold-standard corpus with two existing corpora by 1) individually examining the instances of paraphrase phenomena and 2) benchmarking a range of automatic paraphrase identification approaches."}, {"heading": "4.1 Paraphrase Phenomena", "text": "In order to show the differences across these three datasets, we sampled 100 sentential paraphrases from each training set and counted occurrences of each phenomenon in the following categories: Elaboration (textual pairs can differ in total information content, such as Trump\u2019s ex-wife Ivana and Ivana Trump), Phrasal (alternates of phrases, such as taking over and replaces), Spelling (spelling variants, such as Trump and Trumpf ), Synonym (such as said and told), Anaphora (a full noun phrase in one sentence that corresponds to the counterpart, such as @MarkKirk and Kirk) and Reordering (when a word, phrase or the whole sentence reorders, or even logically reordered, such as Matthew Fishbein questioned him and under questioning by Matthew Fishbein). We report the average number of occurrences of each paraphrase type per sentence pair for each corpus in Table 7. As sentences tend to be longer in MSRP and shorter in PIT-2015, we also normalized the numbers by the length of sentences to be more comparable to the URL dataset.\nThese three datasets exhibit distinct and complementary compositions of paraphrase phenom-\nena. MSRP has more synonyms, because authors of different news articles may use different and rather sophisticated words. PIT-2015 contains many phrasal paraphrases, probably due to the fact that most tweets under the same trending topic are written spontaneously and independently. Our URL dataset shows more elaboration, spelling and anaphora paraphrase phenomena, showing that many URL-embedded tweets are created by users with a conscious intention to rephrase the original news headline."}, {"heading": "4.2 Automatic Paraphrase Identification", "text": "We provide a benchmark on paraphrase identification to better understand various models, as well as the characteristics of our new corpus compared to the existing ones. We focus on binary classification of paraphrase/non-paraphrase, and report the maximum F1 measure of any point on the precision-recall curve."}, {"heading": "4.2.1 Models", "text": "We chose several representative technical approaches for automatic paraphrase identification:\nGloVe (Pennington et al., 2014) This is a word representation model trained on aggregated global word-word co-occurrence statistics from a corpus. We used 300-dimensional word vectors trained on Common Crawl and Twitter, summed the vectors for each sentence, and computed the cosine similarity.\nLR The logistic regression (LR) model incorporates 18 features based on 1-3 gram overlaps between two sentences (s1 and s2) (Das and Smith, 2009). The features are of the form precisionn (number of n-gram matches divided by the number of n-grams in s1), recalln (number of n-gram matches divided by the number of n-grams in s2), and Fn (harmonic mean of recall and precision). The model also includes lemmatized versions of these features.\nWMF/OrMF Weighted Matrix Factorization (WMF) (Guo and Diab, 2012) is an unsupervised latent space model. The unobserved words are carefully handled, which results in more robust embeddings for short texts. Orthogonal Matrix Factorization (OrMF) (Guo et al., 2014) is the extension of WMF, with an additional objective to obtain nearly orthogonal dimensions in matrix factorization to discount redundant information.\nSpecifically, for the (vec) version, vectors of a pair of sentences ~v1 and ~v2 are converted into one feature vector, [~v1 + ~v2, |~v1 \u2212 ~v2|], by concatenating the element-wise sum ~v1 + ~v2 and absolute difference |~v1 \u2212 ~v2|. We also provide the (sim) variation, which directly uses the single cosine similarity score between two sentence vectors.\nLEX-WMF/LEX-OrMF This is an opensourced adaptation (Xu et al., 2014) of LEXDISCRIM (Ji and Eisenstein, 2013) that have shown comparable performance. It combines WMF/OrMF with n-gram overlapping features to train a LR classifier.\nMultiP MultiP (Xu et al., 2014) is a multiinstance learning model suited for short messages on Twitter. The at-least-one-anchor assumption in this model looks for two sentences that have a topical phrase in common, plus at least one pair of anchor words that carry a similar key meaning. This model achieved the best performance in the PIT2015 (Xu et al., 2014) dataset.\nDeepPairwiseWord He et al. (2016) developed a deep neural network model that focuses on important pairwise word interactions across input sentences. This model innovates in proposing a similarity focus layer and a 19-layer very deep convolutional neural network to guide model attention to important word pairs. It has shown stateof-the-art performance on several textual similarity measurement datasets."}, {"heading": "4.2.2 Model Performance and Dataset Difference", "text": "The results on three benchmark paraphrase corpora are shown in Table 8, 9 and 10. The random baseline reflects that close to 80% sentence\npairs are paraphrases in the MSPR corpus. This is atypical in the real-world text data and may cause falsely positive predictions.\nBoth the edit distance and the LR models exploit surface word features. In particular, the LR model that uses lemmatization and ngram overlap features achieves very competitive performance on all datasets. Figure 1 shows a closer look at ngram differences across datasets measured by the PINC metric (Chen and Dolan, 2011), which is the opposite of BLEU (Papineni et al., 2002). MSRP consists of paraphrases with more ngram overlap (lower PINC), while PIT-2015 contains shorter and more lexically dissimilar sentences. Our new URL corpus is in between the two, and is more similar to PIT-2015. It includes user\u2019s intentional rephrasing of an original tweet from a news agency with some words untouched, as well as some dramatic paraphrases that are challenging for any automatic identification methods, such as CO2 levels mark \u2018new era\u2019 in the world\u2019s changing climate and CO2 levels haven\u2019t been this high for 3 to 5 million years.\nMultiP exploits a restrictive constraint that the candidate sentence pairs share a same topical phrase. It achieves the best performance on PIT2015, which naturally contains such phrases. For MSRP and URL datasets, we uses the named entity tagged with the longest span as an approximation of a shared topic phrase and thus suffered a performance drop.\nBoth Glove and WMT/OrMF utilize the underlying co-occurrence statistics of the text corpus. WMT/OrMF use global matrix factorization to project sentences into lower dimension and show great advantages on measuring sentence-level semantic similarities over Glove, which focuses on\nword representations. Figure 2 shows that the finegrained distribution of the OrMF-based cosine similarities and that the URL-linked Twitter data works well with OrMF to yield sentential paraphrases. Once combined with ngram overlap features, LEX-WMF and LEX-OrMF show consistently high performance across different datasets, close to the more complicated DeepPairwiseWord. The similarity focus mechanism on important pairwise word interactions in DeepPairwiseWord is more helpful for the two Twitter datasets, due to the fact that they contain lexically divergent paraphrases while MSRP has an artificial bias toward sentences with high n-gram overlap."}, {"heading": "5 Extracting Phrasal Paraphrases", "text": "We can apply paraphrase identification models trained on our gold standard corpus to unlabeled Twitter data and continuously harvest sentential paraphrases in large quantities. We used the opensourced LEX-OrMF model and obtained 114,025 sentential paraphrases (system predicted probability \u2265 0.5 and average precision = 69.08%) from raw 1% free Twitter data between 10/10/2016 and 01/10/2017. To demonstrate the utility, we show that we can extract up-to-date lexical and phrasal paraphrases from this data."}, {"heading": "5.1 Phrase Extraction and Ranking", "text": "One of the most successful ideas to obtain lexical and phrasal paraphrases in large quantities is through word alignment, then ranking for better quality. This approach was proposed by Bannard (Bannard and Callison-Burch, 2005) and previously applied to bilingual parallel data to create PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015). There has been little previous work utilizing monolingual parallel data to learn paraphrases since it is not as naturally available as bitexts.\nWe used the GIZA++ word aligner in the Moses machine translation toolkit (Koehn et al., 2007) and extracted 245,686 phrasal paraphrases. Some examples are shown in Table 2. We additionally explored two supervised monolingual aligners: Jacana aligner (Yao et al., 2013) and Md Sultan\u2019s aligner (Sultan et al., 2014). We ranked the phrase pairs using four different scores:\n\u2022 Language Model Score Let w\u22122w\u22121pw1w2 be the context of the phrase p. We considered a phrase p\u2032 to be a good substitute for p if w\u22122w\u22121p\u2032w1w2 is a likely sequence according to a language model (Heafield, 2011) trained on Twitter data.\n\u2022 Translation Score Moses provides translation probabilities \u03d5(p|p\u2032). \u2022 Glove Score We used Glove (Penning-\nton et al., 2014) pretrained 100-dimensional Twitter word vectors and cosine similarity.\n\u2022 Our Score We trained a supervised SVM regression model using 500 phrase pairs with human ratings. We used the language model, translation, and glove scores as features, and additionally used the inverse phrase translation probability \u03d5(p\u2032|p), lexical weighting lex(p|p\u2032), and lex(p\u2032|p) from Moses.\nFigure 3 compares the different ranking methods against the human judgments on 200 phrase pairs randomly sampled from GIZA++."}, {"heading": "5.2 Paraphrase Quality Evaluation", "text": "We compared the quality of paraphrases extracted by our method with the closest previous work (BUCC-2013) (Xu et al., 2013), in which a similar phrase table was created using Moses from monolingual parallel tweets that contain the same named entity and calendar date. We randomly\nsampled 500 phrase pairs from each phrase table and collected human judgements on a 5- point Likert scale, as described in Callison-Burch (Callison-Burch, 2008). Table 11 shows the evaluation results. We focused on the highest-quality paraphrases that rated as 5 (\u201call of the meaning of the original phrase is retained, and nothing is added\u201d) and their presence among all extracted paraphrases sorted by ranking scores.\nWe were also interested in how these phrasal paraphrases compared with those in PPDB. We sampled an equal amount of 420 paraphrase pairs from our phrase tables and PPDB, and then checked what percentage out of the total 840 could be found in our phrase tables and PPDB, respectively. As shown in Table 12, there is little overlap between URL data and PPDB, only 1.3% (51.3- 50%) plus 0.8% (50.8-50%). Our Twitter URL data complements well with the existing paraphrase resources, such as PPDB, which are primarily derived from well-edited texts."}, {"heading": "6 Related Work", "text": "Sentential Paraphrase Data Researchers have found several data sources from which to collect sentential paraphrases: multiple news agencies reporting the same event (MSRP) (Dolan et al., 2004; Dolan and Brockett, 2005), multiple trans-\nlated versions of a foreign novel (Barzilay and Elhadad, 2003; Barzilay and Lee, 2003) or other texts (Cohn et al., 2008), multiple definitions of the same concept (Hashimoto et al., 2011), descriptions of the same video clip from multiple workers (Chen and Dolan, 2011) or rephrased sentences (Burrows et al., 2013; Toutanova et al., 2016). However, all these data collection methods are incapable of obtaining sentential paraphrases on a large scale (i.e. limited number of news agencies or books with multiple translated versions), and/or lack meaningful negative examples. Both of these properties are crucial for developing machine learning models that identify paraphrases and measure semantic similarities.\nNon-sentential Paraphrase Data There are other phrasal and syntactic paraphrase data, such as DIRT (Lin and Pantel, 2001), POLY (Grycner et al., 2016), PATTY (Nakashole et al., 2012), DEFIE (Bovi et al., 2015), and PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015). Most of these works focus on news or web data. Other earlier works on Twitter paraphrase extraction used unsupervised approaches (Xu et al., 2013; Wang et al., 2013) or small datasets (Zanzotto et al., 2011; Antoniak et al., 2015)."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we show how a simple method can effectively and continuously collect large-scale\nsentential paraphrases from Twitter. We rigorously evaluated our data with automatic identification classification models and various measurements. We will share our new dataset with the research community; this dataset includes 51,524 sentence pairs manually labeled and a monthly growth of 30,000 sentential paraphrases automatically labeled. Future work could include expanding into many different languages present in social media and developing language-independent automatic paraphrase identification models."}, {"heading": "Acknowledgments", "text": "We would like to thank Chris Callison-Burch, Weiwei Guo and Mike White for valuable discussions, as well as the anonymous reviewers for helpful feedback."}], "references": [{"title": "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2015}, {"title": "Leveraging paraphrase labels to extract synonyms from twitter", "author": ["Maria Antoniak", "Eric Bell", "Fei Xia."], "venue": "Proceedings of the 28th Florida Artificial Intelligence Research Society Conference (FLAIRS).", "citeRegEx": "Antoniak et al\\.,? 2015", "shortCiteRegEx": "Antoniak et al\\.", "year": 2015}, {"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio."], "venue": "Computational Linguistics 34(4):555\u2013596.", "citeRegEx": "Artstein and Poesio.,? 2008", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "Everyone\u2019s an influencer: quantifying influence on twitter", "author": ["Eytan Bakshy", "Jake M Hofman", "Winter A Mason", "Duncan J Watts."], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining (WSDM).", "citeRegEx": "Bakshy et al\\.,? 2011", "shortCiteRegEx": "Bakshy et al\\.", "year": 2011}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Colin Bannard", "Chris Callison-Burch."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL).", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Sentence alignment for monolingual comparable corpora", "author": ["Regina Barzilay", "Noemie Elhadad."], "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Barzilay and Elhadad.,? 2003", "shortCiteRegEx": "Barzilay and Elhadad.", "year": 2003}, {"title": "Learning to paraphrase: an unsupervised approach using multiple-sequence alignment", "author": ["Regina Barzilay", "Lillian Lee."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on", "citeRegEx": "Barzilay and Lee.,? 2003", "shortCiteRegEx": "Barzilay and Lee.", "year": 2003}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "What is a paraphrase? Computational Linguistics 39(3)", "author": ["Rahul Bhagat", "Eduard Hovy"], "venue": null, "citeRegEx": "Bhagat and Hovy.,? \\Q2013\\E", "shortCiteRegEx": "Bhagat and Hovy.", "year": 2013}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["Johannes Bjerva", "Johan Bos", "Rob van der Goot", "Malvina Nissim."], "venue": "SemEval 2014 page 642.", "citeRegEx": "Bjerva et al\\.,? 2014", "shortCiteRegEx": "Bjerva et al\\.", "year": 2014}, {"title": "Large-scale information extraction from textual definitions through deep syntactic and semantic analysis", "author": ["Claudio Delli Bovi", "Luca Telesca", "Roberto Navigli."], "venue": "Transactions of the Association for Computational Linguistics (TACL) pages 529\u2013543.", "citeRegEx": "Bovi et al\\.,? 2015", "shortCiteRegEx": "Bovi et al\\.", "year": 2015}, {"title": "Paraphrase acquisition via crowdsourcing and machine learning", "author": ["Steven Burrows", "Martin Potthast", "Benno Stein."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 4(3):43.", "citeRegEx": "Burrows et al\\.,? 2013", "shortCiteRegEx": "Burrows et al\\.", "year": 2013}, {"title": "Syntactic constraints on paraphrases extracted from parallel corpora", "author": ["Chris Callison-Burch."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Callison.Burch.,? 2008", "shortCiteRegEx": "Callison.Burch.", "year": 2008}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L. Chen", "William B. Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Constructing corpora for the development and evaluation of paraphrase systems", "author": ["Trevor Cohn", "Chris Callison-Burch", "Mirella Lapata."], "venue": "Computational Linguistics 34(4):597\u2013614.", "citeRegEx": "Cohn et al\\.,? 2008", "shortCiteRegEx": "Cohn et al\\.", "year": 2008}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Proceedings of the First international conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Paraphrase identification as probabilistic quasi-synchronous recognition", "author": ["Dipanjan Das", "Noah A Smith."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Das and Smith.,? 2009", "shortCiteRegEx": "Das and Smith.", "year": 2009}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett."], "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP).", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["William B. Dolan", "Chris Quirk", "Chris Brockett."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics (COL-", "citeRegEx": "Dolan et al\\.,? 2004", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke S Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "POLY: Mining relational paraphrases from multilingual sentences", "author": ["Adam Grycner", "Saarland Informatics Campus", "Gerhard Weikum."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Grycner et al\\.,? 2016", "shortCiteRegEx": "Grycner et al\\.", "year": 2016}, {"title": "Modeling sentences in the latent space", "author": ["Weiwei Guo", "Mona Diab."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Guo and Diab.,? 2012", "shortCiteRegEx": "Guo and Diab.", "year": 2012}, {"title": "Fast tweet retrieval with compact binary codes", "author": ["Weiwei Guo", "Wei Liu", "Mona Diab."], "venue": "Proceedings of the 30th International Conference on Computational Linguistics (COLING).", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Extracting paraphrases from definition sentences on the web", "author": ["Chikara Hashimoto", "Kentaro Torisawa", "Stijn De Saeger", "Jun\u2019ichi Kazama", "Sadao Kurohashi"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Hashimoto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2011}, {"title": "Pairwise word interaction modeling with deep neural networks for semantic similarity measurement", "author": ["Hua He", "Jimmy Lin."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "He and Lin.,? 2016", "shortCiteRegEx": "He and Lin.", "year": 2016}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT).", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Segment-phrase table for semantic segmentation, visual entailment and paraphrasing", "author": ["Hamid Izadinia", "Fereshteh Sadeghi", "Santosh K Divvala", "Hannaneh Hajishirzi", "Yejin Choi", "Ali Farhadi."], "venue": "Proceedings of the IEEE International Confer-", "citeRegEx": "Izadinia et al\\.,? 2015", "shortCiteRegEx": "Izadinia et al\\.", "year": 2015}, {"title": "The distribution of the flora in the alpine zone", "author": ["P. Jaccard."], "venue": "New Phytologist 11(2):37\u201350.", "citeRegEx": "Jaccard.,? 1912", "shortCiteRegEx": "Jaccard.", "year": 1912}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Exploiting sentence similarities for better alignments", "author": ["Tao Li", "Vivek Srikumar."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Li and Srikumar.,? 2016", "shortCiteRegEx": "Li and Srikumar.", "year": 2016}, {"title": "DIRT \u2013 Discovery of inference rules from text", "author": ["Dekang Lin", "Patrick Pantel."], "venue": "Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).", "citeRegEx": "Lin and Pantel.,? 2001", "shortCiteRegEx": "Lin and Pantel.", "year": 2001}, {"title": "Generating phrasal and sentential paraphrases: A survey of datadriven methods", "author": ["Nitin Madnani", "Bonnie J. Dorr."], "venue": "Computational Linguistics 36(3).", "citeRegEx": "Madnani and Dorr.,? 2010", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Improving statistical machine translation with a multilingual paraphrase database", "author": ["Ramtin Mehdizadeh Seraj", "Maryam Siahbani", "Anoop Sarkar."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Seraj et al\\.,? 2015", "shortCiteRegEx": "Seraj et al\\.", "year": 2015}, {"title": "A short guide to the meaning-text linguistic theory", "author": ["Jasmina Mili\u0107evi\u0107."], "venue": "Journal of Koralex 8:187\u2013233.", "citeRegEx": "Mili\u0107evi\u0107.,? 2006", "shortCiteRegEx": "Mili\u0107evi\u0107.", "year": 2006}, {"title": "PATTY: a taxonomy of relational patterns with semantic types", "author": ["Ndapandula Nakashole", "Gerhard Weikum", "Fabian Suchanek."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Nakashole et al\\.,? 2012", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevich"], "venue": "In Proceedings of the 53rd Annual", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "On-demand information extraction", "author": ["Satoshi Sekine."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (COLING).", "citeRegEx": "Sekine.,? 2006", "shortCiteRegEx": "Sekine.", "year": 2006}, {"title": "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence", "author": ["Md Arafat Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2:219\u2013230.", "citeRegEx": "Sultan et al\\.,? 2014", "shortCiteRegEx": "Sultan et al\\.", "year": 2014}, {"title": "A dataset and evaluation metrics for abstractive compression of sentences and short paragraphs", "author": ["Kristina Toutanova", "Chris Brockett", "Ke M. Tran", "Saleema Amershi."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language", "citeRegEx": "Toutanova et al\\.,? 2016", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "A semiautomatic method for efficient detection of stories on social media", "author": ["Soroush Vosoughi", "Deb Roy."], "venue": "Tenth International AAAI Conference on Web and Social Media (ICWSM).", "citeRegEx": "Vosoughi and Roy.,? 2016", "shortCiteRegEx": "Vosoughi and Roy.", "year": 2016}, {"title": "Paraphrasing 4 microblog normalization", "author": ["Ling Wang", "Chris Dyer", "Alan W. Black", "Isabel Trancoso."], "venue": "Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP).", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu", "D. Roth."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:345\u2013358.", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Wei Xu", "Chris Callison-Burch", "William B. Dolan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Extracting lexically divergent paraphrases from Twitter", "author": ["Wei Xu", "Alan Ritter", "Chris Callison-Burch", "William B. Dolan", "Yangfeng Ji."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 2:435\u2013448.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Gathering and generating paraphrases from twitter with application to normalization", "author": ["Wei Xu", "Alan Ritter", "Ralph Grishman."], "venue": "Proceedings of the Sixth Workshop on Building and Using Comparable Corpora (BUCC).", "citeRegEx": "Xu et al\\.,? 2013", "shortCiteRegEx": "Xu et al\\.", "year": 2013}, {"title": "A lightweight and high performance monolingual word aligner", "author": ["Xuchen Yao", "Benjamin Van Durme", "Chris CallisonBurch", "Peter Clark."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Linguistic redundancy in Twitter", "author": ["Fabio Massimo Zanzotto", "Marco Pennacchiotti", "Kostas Tsioutsiouliklis."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Zanzotto et al\\.,? 2011", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2011}, {"title": "Exploiting parallel news streams for unsupervised event extraction", "author": ["Congle Zhang", "Stephen Soderland", "Daniel S Weld."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3:117\u2013 129.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "A paraphrase is a restatement of meaning using different expressions (Bhagat and Hovy, 2013).", "startOffset": 69, "endOffset": 92}, {"referenceID": 38, "context": "It is a fundamental semantic relation in human language, as formalized in the Meaning-Text linguistic theory which defines meaning as \u2018invariant of paraphrases\u2019 (Mili\u0107evi\u0107, 2006).", "startOffset": 161, "endOffset": 178}, {"referenceID": 35, "context": "Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 19, "context": "Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al.", "startOffset": 140, "endOffset": 160}, {"referenceID": 7, "context": ", 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 15, "context": ", 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al.", "startOffset": 28, "endOffset": 114}, {"referenceID": 9, "context": ", 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al.", "startOffset": 28, "endOffset": 114}, {"referenceID": 36, "context": ", 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al.", "startOffset": 28, "endOffset": 114}, {"referenceID": 29, "context": ", 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al.", "startOffset": 28, "endOffset": 114}, {"referenceID": 0, "context": ", 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016).", "startOffset": 41, "endOffset": 85}, {"referenceID": 33, "context": ", 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016).", "startOffset": 41, "endOffset": 85}, {"referenceID": 46, "context": "Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 17, "context": "derived from clustered news articles (Dolan and Brockett, 2005) and the PIT-2015 corpus from Twitter trending topics (Xu et al.", "startOffset": 37, "endOffset": 63}, {"referenceID": 18, "context": "MSR Paraphrase Corpus [MSRP] (Dolan et al., 2004; Dolan and Brockett, 2005) This corpus contains 5,801 pairs of sentences from news articles, with 4,076 for training and the remaining 1,725 for testing.", "startOffset": 29, "endOffset": 75}, {"referenceID": 17, "context": "MSR Paraphrase Corpus [MSRP] (Dolan et al., 2004; Dolan and Brockett, 2005) This corpus contains 5,801 pairs of sentences from news articles, with 4,076 for training and the remaining 1,725 for testing.", "startOffset": 29, "endOffset": 75}, {"referenceID": 16, "context": "The MSRP corpus has a known deficiency skewed toward over-identification (Das and Smith, 2009), because the \u201cpurpose was not to evaluate the po-", "startOffset": 73, "endOffset": 94}, {"referenceID": 17, "context": "tential effectiveness of the classifier itself, but to identify a reasonably large set of both positive and plausible \u2018near-miss\u2019 negative examples\u201d (Dolan and Brockett, 2005).", "startOffset": 149, "endOffset": 175}, {"referenceID": 22, "context": "We preprocessed the tweets using a tokenizer5 (Gimpel et al., 2011) and an in-house sen-", "startOffset": 46, "endOffset": 67}, {"referenceID": 13, "context": "ric, to measure ngram-based dissimilarity (Chen and Dolan, 2011), and Jaccard metric to measure token-based string similarity (Jaccard, 1912).", "startOffset": 42, "endOffset": 64}, {"referenceID": 30, "context": "ric, to measure ngram-based dissimilarity (Chen and Dolan, 2011), and Jaccard metric to measure token-based string similarity (Jaccard, 1912).", "startOffset": 126, "endOffset": 141}, {"referenceID": 2, "context": "Quality Control We evaluated the annotation quality of each worker using Cohen\u2019s kappa agreement (Artstein and Poesio, 2008) against the majority vote of other workers.", "startOffset": 97, "endOffset": 124}, {"referenceID": 42, "context": "GloVe (Pennington et al., 2014) This is a word representation model trained on aggregated global word-word co-occurrence statistics from a corpus.", "startOffset": 6, "endOffset": 31}, {"referenceID": 16, "context": "LR The logistic regression (LR) model incorporates 18 features based on 1-3 gram overlaps between two sentences (s1 and s2) (Das and Smith, 2009).", "startOffset": 124, "endOffset": 145}, {"referenceID": 24, "context": "WMF/OrMF Weighted Matrix Factorization (WMF) (Guo and Diab, 2012) is an unsupervised latent space model.", "startOffset": 45, "endOffset": 65}, {"referenceID": 25, "context": "Orthogonal Matrix Factorization (OrMF) (Guo et al., 2014) is the extension of WMF, with an additional objective to obtain nearly orthogonal dimensions in matrix factorization to discount redundant information.", "startOffset": 39, "endOffset": 57}, {"referenceID": 50, "context": "LEX-WMF/LEX-OrMF This is an opensourced adaptation (Xu et al., 2014) of LEXDISCRIM (Ji and Eisenstein, 2013) that have shown comparable performance.", "startOffset": 51, "endOffset": 68}, {"referenceID": 31, "context": ", 2014) of LEXDISCRIM (Ji and Eisenstein, 2013) that have shown comparable performance.", "startOffset": 22, "endOffset": 47}, {"referenceID": 50, "context": "MultiP MultiP (Xu et al., 2014) is a multiinstance learning model suited for short messages on Twitter.", "startOffset": 14, "endOffset": 31}, {"referenceID": 50, "context": "This model achieved the best performance in the PIT2015 (Xu et al., 2014) dataset.", "startOffset": 56, "endOffset": 73}, {"referenceID": 13, "context": "Figure 1 shows a closer look at ngram differences across datasets measured by the PINC metric (Chen and Dolan, 2011), which", "startOffset": 94, "endOffset": 116}, {"referenceID": 40, "context": "is the opposite of BLEU (Papineni et al., 2002).", "startOffset": 24, "endOffset": 47}, {"referenceID": 4, "context": "This approach was proposed by Bannard (Bannard and Callison-Burch, 2005) and previously applied to bilingual parallel data to create PPDB (Ganitkevitch et al.", "startOffset": 38, "endOffset": 72}, {"referenceID": 21, "context": "This approach was proposed by Bannard (Bannard and Callison-Burch, 2005) and previously applied to bilingual parallel data to create PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015).", "startOffset": 138, "endOffset": 187}, {"referenceID": 41, "context": "This approach was proposed by Bannard (Bannard and Callison-Burch, 2005) and previously applied to bilingual parallel data to create PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015).", "startOffset": 138, "endOffset": 187}, {"referenceID": 32, "context": "We used the GIZA++ word aligner in the Moses machine translation toolkit (Koehn et al., 2007) and extracted 245,686 phrasal paraphrases.", "startOffset": 73, "endOffset": 93}, {"referenceID": 52, "context": "cana aligner (Yao et al., 2013) and Md Sultan\u2019s aligner (Sultan et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 44, "context": ", 2013) and Md Sultan\u2019s aligner (Sultan et al., 2014).", "startOffset": 32, "endOffset": 53}, {"referenceID": 28, "context": "p if w\u22122w\u22121pw1w2 is a likely sequence according to a language model (Heafield, 2011) trained on Twitter data.", "startOffset": 68, "endOffset": 84}, {"referenceID": 42, "context": "\u2022 Glove Score We used Glove (Pennington et al., 2014) pretrained 100-dimensional Twitter word vectors and cosine similarity.", "startOffset": 28, "endOffset": 53}, {"referenceID": 51, "context": "We compared the quality of paraphrases extracted by our method with the closest previous work (BUCC-2013) (Xu et al., 2013), in which a similar phrase table was created using Moses from monolingual parallel tweets that contain the same named entity and calendar date.", "startOffset": 106, "endOffset": 123}, {"referenceID": 12, "context": "sampled 500 phrase pairs from each phrase table and collected human judgements on a 5point Likert scale, as described in Callison-Burch (Callison-Burch, 2008).", "startOffset": 136, "endOffset": 158}, {"referenceID": 18, "context": "Sentential Paraphrase Data Researchers have found several data sources from which to collect sentential paraphrases: multiple news agencies reporting the same event (MSRP) (Dolan et al., 2004; Dolan and Brockett, 2005), multiple transPPDB URL GIZA++ Jacana Sultan Sample Size 50% 50% 16.", "startOffset": 172, "endOffset": 218}, {"referenceID": 17, "context": "Sentential Paraphrase Data Researchers have found several data sources from which to collect sentential paraphrases: multiple news agencies reporting the same event (MSRP) (Dolan et al., 2004; Dolan and Brockett, 2005), multiple transPPDB URL GIZA++ Jacana Sultan Sample Size 50% 50% 16.", "startOffset": 172, "endOffset": 218}, {"referenceID": 21, "context": "outputs) and the PPDB (Ganitkevitch et al., 2013).", "startOffset": 22, "endOffset": 49}, {"referenceID": 5, "context": "lated versions of a foreign novel (Barzilay and Elhadad, 2003; Barzilay and Lee, 2003) or other texts (Cohn et al.", "startOffset": 34, "endOffset": 86}, {"referenceID": 6, "context": "lated versions of a foreign novel (Barzilay and Elhadad, 2003; Barzilay and Lee, 2003) or other texts (Cohn et al.", "startOffset": 34, "endOffset": 86}, {"referenceID": 14, "context": "lated versions of a foreign novel (Barzilay and Elhadad, 2003; Barzilay and Lee, 2003) or other texts (Cohn et al., 2008), multiple definitions of the same concept (Hashimoto et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 26, "context": ", 2008), multiple definitions of the same concept (Hashimoto et al., 2011), descriptions of the same video clip from multiple", "startOffset": 50, "endOffset": 74}, {"referenceID": 13, "context": "workers (Chen and Dolan, 2011) or rephrased sentences (Burrows et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 11, "context": "workers (Chen and Dolan, 2011) or rephrased sentences (Burrows et al., 2013; Toutanova et al., 2016).", "startOffset": 54, "endOffset": 100}, {"referenceID": 45, "context": "workers (Chen and Dolan, 2011) or rephrased sentences (Burrows et al., 2013; Toutanova et al., 2016).", "startOffset": 54, "endOffset": 100}, {"referenceID": 34, "context": "Non-sentential Paraphrase Data There are other phrasal and syntactic paraphrase data, such as DIRT (Lin and Pantel, 2001), POLY (Grycner", "startOffset": 99, "endOffset": 121}, {"referenceID": 39, "context": ", 2016), PATTY (Nakashole et al., 2012), DEFIE (Bovi et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 10, "context": ", 2012), DEFIE (Bovi et al., 2015), and PPDB (Ganitkevitch et al.", "startOffset": 15, "endOffset": 34}, {"referenceID": 21, "context": ", 2015), and PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015).", "startOffset": 18, "endOffset": 67}, {"referenceID": 41, "context": ", 2015), and PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015).", "startOffset": 18, "endOffset": 67}, {"referenceID": 51, "context": "pervised approaches (Xu et al., 2013; Wang et al., 2013) or small datasets (Zanzotto et al.", "startOffset": 20, "endOffset": 56}, {"referenceID": 47, "context": "pervised approaches (Xu et al., 2013; Wang et al., 2013) or small datasets (Zanzotto et al.", "startOffset": 20, "endOffset": 56}, {"referenceID": 53, "context": ", 2013) or small datasets (Zanzotto et al., 2011; Antoniak et al., 2015).", "startOffset": 26, "endOffset": 72}, {"referenceID": 1, "context": ", 2013) or small datasets (Zanzotto et al., 2011; Antoniak et al., 2015).", "startOffset": 26, "endOffset": 72}], "year": 2017, "abstractText": "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at \u223c70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.1", "creator": "TeX"}}}