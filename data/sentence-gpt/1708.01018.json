{"id": "1708.01018", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2017", "title": "CRF Autoencoder for Unsupervised Dependency Parsing", "abstract": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.\n\n\n\n\nFor the purpose of generating model, we first looked at the probability distribution. This is how we extract data into a specific, non-uniform tree. This model can be generated by applying simple model optimization to every variable, by simply adding a number in a logarithmic scale. Since a single variable is in an instance of a set of variable-size trees, the probability distribution is normalized in every variable as much as the probability distribution. For a tree with a set of values, we compute the probability distribution in the root tree, and compute the likelihood distribution for each tree in an order of the probability distribution. Since we use the term \"least robust\", the probability distribution is the distribution of the root tree, or the probability distribution of each tree in an order of the probability distribution.\nWe first performed an unsupervised classification and a non-uniform tree classification algorithm. The output is as follows:\nThe model is given as follows:\nThe logarithmic model is given as follows:\nThe algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model.\nThe algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability distribution using the unsupervised model. The algorithm produces the probability", "histories": [["v1", "Thu, 3 Aug 2017 06:45:31 GMT  (135kb,D)", "http://arxiv.org/abs/1708.01018v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiong cai", "yong jiang", "kewei tu"], "accepted": true, "id": "1708.01018"}, "pdf": {"name": "1708.01018.pdf", "metadata": {"source": "CRF", "title": "CRF Autoencoder for Unsupervised Dependency Parsing\u2217", "authors": ["Jiong Cai", "Yong Jiang", "Kewei Tu"], "emails": ["tukw}@shanghaitech.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised dependency parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016).\nBesides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim-\n\u2217This work was supported by the National Natural Science Foundation of China (61503248).\ninative parser. They designed a convex quadratic objective function under the discriminative clustering framework. By utilizing global features and linguistic priors, their approach achieves stateof-the-art performance. However, their approach uses an approximate parsing algorithm, which has no theoretical guarantee. In addition, the performance of the approach depends on a set of manually specified linguistic priors.\nConditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the structured input from the latent structure. Ammar et al. (2014) applied the model to two sequential structured prediction tasks, part-of-speech induction and word alignment and showed that by utilizing context information the model can achieve better performance than previous generative models and locally normalized models. However, to the best of our knowledge, there is no previous work applying the CRF autoencoder to tasks with more complicated outputs such as tree structures.\nIn this paper, we propose an unsupervised discriminative dependency parser based on the CRF autoencoder framework and provide tractable algorithms for learning and parsing. We performed experiments in eight languages and show that our approach achieves comparable results with previous state-of-the-art models.\nar X\niv :1\n70 8.\n01 01\n8v 1\n[ cs\n.C L\n] 3\nA ug\n2 01\n7\nThese stocks eventually reopenedROOT\nx\ny1\ny2\ny3\ny4\nThese\nstocks\neventually\nreopenedx\u03024\nx\u03023\nx\u03022\nx\u03021\nEncoder Decoder\nFigure 1: The CRF Autoencoder for the input sentence \u201cThese stocks eventually reopened\u201d and its corresponding parse tree (shown at the top). x and x\u0302 are the original and reconstructed sentence. y is the dependency parse tree represented by a sequence where yi contains the token and index of the parent of token xi in the parse tree, e.g., y1 = \u3008stocks, 2\u3009 and y2 = \u3008reopened, 4\u3009. The encoder is represented by a factor graph (with a global factor specifying valid parse trees) and the decoder is represented by a Bayesian net."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 Model", "text": "Figure 1 shows our model with an example input sentence. Given an input sentence x = (x1, x2, . . . , xn), we regard its parse tree as the latent structure represented by a sequence y = (y1, y2, . . . , yn) where yi is a pair \u3008ti, hi\u3009, ti is the head token of the dependency connecting to token xi in the parse tree, and hi is the index of this head token in the sentence. The model also contains a reconstruction output, which is a token sequence x\u0302 = (x\u03021, x\u03022, . . . , x\u0302n). Throughout this paper, we set x\u0302 = x.\nThe encoder in our model is a log-linear model represented by a first-order dependency parser. The score of a dependency tree can be factorized as the sum of scores of its dependencies. For each dependency arc (x, i, j), where i and j are the indices of the head and child of the dependency, a feature vector f(x, i, j) is specified. The score of a dependency is defined as the inner product of the feature vector and a weight vector w,\n\u03c6(x, i, j) = wT f(x, i, j)\nThe score of a dependency tree y of sentence x is\n\u03c6(x,y) = n\u2211 i=1 \u03c6(x, hi, i)\nWe define the probability of parse tree y given sentence x as\nP (y|x) = exp(\u03c6(x,y)) Z(x)\nZ(x) is the partition function,\nZ(x) = \u2211\ny\u2032\u2208Y(x)\nexp(\u03c6(x,y\u2032))\nwhere Y(x) is the set of all valid parse trees of x. The partition function can be efficiently computed in O(n3) time using a variant of the inside-outside algorithm (Paskin, 2001) for projective tree structures, or using the Matrix-Tree Theorem for nonprojective tree structures (Koo et al., 2007).\nThe decoder of our model consists of a set of categorical conditional distributions \u03b8x|t, which represents the probability of generating token x conditioned on token t. So the probability of the reconstruction output x\u0302 given the parse tree y is\nP (x\u0302|y) = n\u220f i=1 \u03b8x\u0302i|ti\nThe conditional distribution of x\u0302,y given x is\nP (y, x\u0302|x) = P (y|x)P (x\u0302|y)"}, {"heading": "2.1.1 Features", "text": "Following McDonald et al. (2005) and Grave et al. (2015), we define the feature vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction, and the distance between the head and child of the dependency. The feature template used in our parser is shown in Table 1."}, {"heading": "2.1.2 Parsing", "text": "Given parameters w and \u03b8, we can parse a sentence x by searching for a dependency tree y which has the highest probability P (x\u0302,y|x).\ny\u2217 = arg max y\u2208Y(x) logP (x\u0302,y|x)\n= arg max y\u2208Y(x) n\u2211 i=1 ( \u03c6(x, hi, i) + log \u03b8x\u0302i|ti )\nFor projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n3) time. For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time."}, {"heading": "2.2 Parameter Learning", "text": ""}, {"heading": "2.2.1 Objective Function", "text": "Spitkovsky et al. (2010) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM. Therefore, instead of using negative conditional log likelihood as our objective function, we choose to use negative conditional Viterbi log likelihood,\n\u2212 N\u2211 i=1 log ( max y\u2208Y(xi) P (x\u0302i,y|xi) ) + \u03bb\u2126(w) (1)\nwhere \u2126(w) is a L1 regularization term of the encoder parameter w and \u03bb is a hyper-parameter controlling the strength of regularization.\nTo encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al. (2015). Hence our objective function becomes \u2212 N\u2211 i=1 log ( max y\u2208Y(xi) P (x\u0302i,y|xi)Q\u03b1(xi,y) ) +\u03bb\u2126(w)\nwhere Q(x,y) is a soft constraint factor over the parse tree, and \u03b1 is a hyper-parameter controlling the strength of the constraint factor. The factor Q is also decomposable by edges in the same way as the encoder and the decoder, and therefore our parsing algorithm can still be used with this factor\ntaken into account.\nQ(x,y) = exp (\u2211 i 1[(ti \u2192 xi) \u2208 R] )\nwhere 1[(ti \u2192 xi) \u2208 R] is an indicator function of whether dependency ti \u2192 xi satisfies one of the universal linguistic rules in R. The universal linguistic rules that we use are shown in Table 2 (Naseem et al., 2010)."}, {"heading": "2.2.2 Algorithm", "text": "We apply coordinate descent to minimize the objective function, which alternately updates w and \u03b8. In each optimization step of w, we run two epochs of stochastic gradient descent, and in each optimization step of \u03b8, we run two iterations of the Viterbi EM algorithm.\nTo update w using stochastic gradient descent, for each sentence x, we first run the parsing algorithm to find the best parse tree y\u2217 = arg maxy\u2208Y(x)(P (x\u0302,y|x)Q\u03b1(x,y)); then we can calculate the gradient of the objective function based on the following derivation,\n\u2202logP (x\u0302,y\u2217|x) \u2202w = \u2202logP (y\u2217|x)\n\u2202w + \u2202logP (x\u0302|y\u2217) \u2202w\n= \u2202logP (y\u2217|x)\n\u2202w = \u2202 (\u2211n i=1w T f(x, hi, i)\u2212 logZ(x) ) \u2202w\n= \u2211\n(i,j)\u2208D(x)\nf(x, i, j) ( 1[y\u2217j = \u3008i, xi\u3009]\u2212 \u00b5(x, i, j) )\nwhere D(x) is the set of all possible dependency arcs of sentence x, 1[\u00b7] is the indicator function, and \u00b5(x, i, j) is the expected count defined as follows,\n\u00b5(x, i, j) = \u2211\ny\u2208Y(x)\n( 1[yj = \u3008i, xi\u3009]P (y|x) )\nThe expected count can be efficiently computed using the Matrix-Tree Theorem (Koo et al., 2007) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures (Paskin, 2001).\nTo update \u03b8 using Viterbi EM, in the E-step we again use the parsing algorithm to find the best parse tree y\u2217 for each sentence x; then in the Mstep we update \u03b8 by maximum likelihood estimation.\n\u03b8c|t =\n\u2211 x \u2211 i 1[xi = c, y \u2217 i = \u3008\u00b7, t\u3009]\u2211\nc\u2032 \u2211 x \u2211 i 1[xi = c \u2032, y\u2217i = \u3008\u00b7, t\u3009]"}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Setup", "text": "We experimented with projective parsing and used the informed initialization method proposed by Klein and Manning (2004) to initialize our model before learning. We tested our model both with and without using the universal linguistic rules. We used AdaGrad to optimize w. We used POS tags of the input sentence as the tokens in our model. We learned our model on training sentences of length \u2264 10 and reported the directed dependency accuracy on test sentences of length \u2264 10 and on all test sentences."}, {"heading": "3.2 Results on English", "text": "We evaluated our model on the Wall Street Journal corpus. We trained our model on section 2- 21, tuned the hyperparameters on section 22, and tested our model on section 23. Table 3 shows the directed dependency accuracy of our model (CRFAE) compared with recently published results. It can be seen that our method achieves a comparable performance with state-of-the-art systems.\nWe also compared the performances of CRF autoencoder using an objective function with negative log likelihood vs. using our Viterbi version of the objective function (Eq.1). We find that the Viterbi version leads to much better performance (55.7 vs. 41.8 in parsing accuracy of WSJ), which echoes Spitkovsky et al. \u2019s findings on Viterbi EM (2010)."}, {"heading": "3.3 Multilingual Results", "text": "We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). We did not use the Arabic corpus because the number of training sentences with length \u2264 10 is less than 1000. The result is shown in Table 4. The accuracies of DMV and Neural DMV are from Jiang et.al (2016). Both our model (CRFAE) and Convex-MST were tuned on the validation set of each corpus. It can be seen that our method achieves the best results on average. Besides, our method outperforms ConvexMST both with and without linguistic prior. From the results we can also see that utilizing universal linguistic prior greatly improves the performance of Convex-MST and our model."}, {"heading": "4 Conclusion", "text": "In this paper, we propose a new discriminative model for unsupervised dependency parsing based on CRF autoencoder. Both learning and inference of our model are tractable. We tested our method on eight languages and show that our model is competitive to the state-of-the-art systems.\nThe code is available at https://github. com/caijiong/CRFAE-Dep-Parser"}], "references": [{"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Waleed Ammar", "Chris Dyer", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 3311\u20133319.", "citeRegEx": "Ammar et al\\.,? 2014", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Unsupervised induction of tree substitution grammars for dependency parsing", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204\u20131213. Association for Com-", "citeRegEx": "Blunsom and Cohn.,? 2010", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2010}, {"title": "On shortest arborescence of a directed graph", "author": ["Yoeng-Jin Chu", "Tseng-Hong Liu."], "venue": "Scientia Sinica, 14(10):1396.", "citeRegEx": "Chu and Liu.,? 1965", "shortCiteRegEx": "Chu and Liu.", "year": 1965}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B Cohen", "Kevin Gimpel", "Noah A Smith."], "venue": "Advances in Neural Information Processing Systems, pages 321\u2013328.", "citeRegEx": "Cohen et al\\.,? 2008", "shortCiteRegEx": "Cohen et al\\.", "year": 2008}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Journal of Research of the National Bureau of Standards B, 71(4):233\u2013240.", "citeRegEx": "Edmonds.,? 1967", "shortCiteRegEx": "Edmonds.", "year": 1967}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason M Eisner."], "venue": "Proceedings of the 16th conference on Computational linguistics-Volume 1, pages 340\u2013345. Association for Computational Linguistics.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "The pascal challenge on grammar induction", "author": ["Douwe Gelling", "Trevor Cohn", "Phil Blunsom", "Joao Gra\u00e7a."], "venue": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64\u201380. Association for Computational Linguistics.", "citeRegEx": "Gelling et al\\.,? 2012", "shortCiteRegEx": "Gelling et al\\.", "year": 2012}, {"title": "A convex and feature-rich discriminative approach to dependency grammar induction", "author": ["Edouard Grave", "No\u00e9mie Elhadad."], "venue": "ACL (1), pages 1375\u2013 1384.", "citeRegEx": "Grave and Elhadad.,? 2015", "shortCiteRegEx": "Grave and Elhadad.", "year": 2015}, {"title": "Unsupervised neural dependency parsing", "author": ["Yong Jiang", "Wenjuan Han", "Kewei Tu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763\u2013771, Austin, Texas. Association for Computational Lin-", "citeRegEx": "Jiang et al\\.,? 2016", "shortCiteRegEx": "Jiang et al\\.", "year": 2016}, {"title": "Corpusbased induction of syntactic structure: Models of dependency and constituency", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Com-", "citeRegEx": "Klein and Manning.,? 2004", "shortCiteRegEx": "Klein and Manning.", "year": 2004}, {"title": "Structured prediction models via the matrix-tree theorem", "author": ["Terry Koo", "Amir Globerson", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Koo et al\\.,? 2007", "shortCiteRegEx": "Koo et al\\.", "year": 2007}, {"title": "Unsupervised dependency parsing: Let\u2019s use supervised parsers", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Le and Zuidema.,? 2015", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 91\u201398. Association for Computa-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234\u20131244. Asso-", "citeRegEx": "Naseem et al\\.,? 2010", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Cubic-time parsing and learning algorithms for grammatical bigram models", "author": ["Mark A Paskin."], "venue": "Citeseer.", "citeRegEx": "Paskin.,? 2001", "shortCiteRegEx": "Paskin.", "year": 2001}, {"title": "Breaking out of local optima with count transforms and model recombination: A study in grammar induction", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky."], "venue": "EMNLP, pages 1983\u2013 1995.", "citeRegEx": "Spitkovsky et al\\.,? 2013", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2013}, {"title": "Viterbi training improves unsupervised dependency parsing", "author": ["Valentin I Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher D Manning."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9\u201317. As-", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Finding optimum branchings", "author": ["Robert Endre Tarjan."], "venue": "Networks, 7(1):25\u201335.", "citeRegEx": "Tarjan.,? 1977", "shortCiteRegEx": "Tarjan.", "year": 1977}, {"title": "Unambiguity regularization for unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Tu and Honavar.,? 2012", "shortCiteRegEx": "Tu and Honavar.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al.", "startOffset": 49, "endOffset": 80}, {"referenceID": 9, "context": ", 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016).", "startOffset": 66, "endOffset": 86}, {"referenceID": 6, "context": "Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al.", "startOffset": 153, "endOffset": 178}, {"referenceID": 1, "context": ", 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim\u2217This work was supported by the National Natural Science Foundation of China (61503248).", "startOffset": 50, "endOffset": 217}, {"referenceID": 0, "context": "Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the structured input from the latent structure. Ammar et al. (2014) applied the model to two sequential structured prediction tasks, part-of-speech induction and word alignment and showed that by utilizing context information the model can achieve better performance than previous generative models and locally normalized models.", "startOffset": 38, "endOffset": 490}, {"referenceID": 15, "context": "The partition function can be efficiently computed in O(n3) time using a variant of the inside-outside algorithm (Paskin, 2001) for projective tree structures, or using the Matrix-Tree Theorem for nonprojective tree structures (Koo et al.", "startOffset": 113, "endOffset": 127}, {"referenceID": 11, "context": "The partition function can be efficiently computed in O(n3) time using a variant of the inside-outside algorithm (Paskin, 2001) for projective tree structures, or using the Matrix-Tree Theorem for nonprojective tree structures (Koo et al., 2007).", "startOffset": 227, "endOffset": 245}, {"referenceID": 13, "context": "1 Features Following McDonald et al. (2005) and Grave et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 13, "context": "1 Features Following McDonald et al. (2005) and Grave et al. (2015), we define the feature vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction, and the distance between the head and child of the dependency.", "startOffset": 21, "endOffset": 68}, {"referenceID": 3, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 5, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 18, "context": "For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2) time.", "startOffset": 79, "endOffset": 127}, {"referenceID": 4, "context": "For projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n3) time.", "startOffset": 46, "endOffset": 71}, {"referenceID": 16, "context": "1 Objective Function Spitkovsky et al. (2010) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM.", "startOffset": 21, "endOffset": 46}, {"referenceID": 14, "context": "To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al.", "startOffset": 183, "endOffset": 204}, {"referenceID": 14, "context": "To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al. (2015). Hence our objective function becomes", "startOffset": 183, "endOffset": 228}, {"referenceID": 14, "context": "The universal linguistic rules that we use are shown in Table 2 (Naseem et al., 2010).", "startOffset": 64, "endOffset": 85}, {"referenceID": 10, "context": "Our model is compared with DMV (Klein and Manning, 2004), Neural DMV (Jiang et al.", "startOffset": 31, "endOffset": 56}, {"referenceID": 9, "context": "Our model is compared with DMV (Klein and Manning, 2004), Neural DMV (Jiang et al., 2016), and Convex-MST (Grave and Elhadad, 2015)", "startOffset": 69, "endOffset": 89}, {"referenceID": 8, "context": ", 2016), and Convex-MST (Grave and Elhadad, 2015)", "startOffset": 24, "endOffset": 49}, {"referenceID": 1, "context": "Methods WSJ10 WSJ Basic Setup Feature DMV (Berg-Kirkpatrick et al., 2010) 63.", "startOffset": 42, "endOffset": 73}, {"referenceID": 19, "context": "0 UR-A E-DMV (Tu and Honavar, 2012) 71.", "startOffset": 13, "endOffset": 35}, {"referenceID": 9, "context": "0 Neural E-DMV (Jiang et al., 2016) 69.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "5 Neural E-DMV (Good Init) (Jiang et al., 2016) 72.", "startOffset": 27, "endOffset": 47}, {"referenceID": 8, "context": "6 Basic Setup + Universal Linguistic Prior Convex-MST (Grave and Elhadad, 2015) 60.", "startOffset": 54, "endOffset": 79}, {"referenceID": 14, "context": "6 HDP-DEP (Naseem et al., 2010) 71.", "startOffset": 10, "endOffset": 31}, {"referenceID": 2, "context": "7 Systems Using Extra Info LexTSG-DMV (Blunsom and Cohn, 2010) 67.", "startOffset": 38, "endOffset": 62}, {"referenceID": 16, "context": "7 CS (Spitkovsky et al., 2013) 72.", "startOffset": 5, "endOffset": 30}, {"referenceID": 12, "context": "4 MaxEnc (Le and Zuidema, 2015) 73.", "startOffset": 9, "endOffset": 31}, {"referenceID": 11, "context": "The expected count can be efficiently computed using the Matrix-Tree Theorem (Koo et al., 2007) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures (Paskin, 2001).", "startOffset": 77, "endOffset": 95}, {"referenceID": 15, "context": ", 2007) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures (Paskin, 2001).", "startOffset": 125, "endOffset": 139}, {"referenceID": 10, "context": "We experimented with projective parsing and used the informed initialization method proposed by Klein and Manning (2004) to initialize our model before learning.", "startOffset": 96, "endOffset": 121}, {"referenceID": 16, "context": "8 in parsing accuracy of WSJ), which echoes Spitkovsky et al. \u2019s findings on Viterbi EM (2010).", "startOffset": 44, "endOffset": 95}, {"referenceID": 7, "context": "3 Multilingual Results We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012).", "startOffset": 112, "endOffset": 134}, {"referenceID": 7, "context": "3 Multilingual Results We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). We did not use the Arabic corpus because the number of training sentences with length \u2264 10 is less than 1000. The result is shown in Table 4. The accuracies of DMV and Neural DMV are from Jiang et.al (2016). Both our model (CRFAE) and Convex-MST were tuned on the validation set of each corpus.", "startOffset": 113, "endOffset": 343}], "year": 2017, "abstractText": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}