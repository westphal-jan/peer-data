{"id": "1708.02300", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Aug-2017", "title": "Reinforced Video Captioning with Entailment Rewards", "abstract": "Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 7 Aug 2017 20:50:24 GMT  (1956kb)", "http://arxiv.org/abs/1708.02300v1", "EMNLP 2017 (9 pages)"]], "COMMENTS": "EMNLP 2017 (9 pages)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["ramakanth pasunuru", "mohit bansal"], "accepted": true, "id": "1708.02300"}, "pdf": {"name": "1708.02300.pdf", "metadata": {"source": "CRF", "title": "Reinforced Video Captioning with Entailment Rewards", "authors": ["Ramakanth Pasunuru"], "emails": ["mbansal}@cs.unc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n02 30\n0v 1\n[ cs\n.C L\n] 7\nA ug\n2 01\n7\npromising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset."}, {"heading": "1 Introduction", "text": "The task of video captioning (Fig. 1) is an important next step to image captioning, with additional modeling of temporal knowledge and action sequences, and has several applications in online content search, assisting the visuallyimpaired, etc. Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a). However, these models are still trained using a wordlevel cross-entropy loss, which does not correlate well with the sentence-level metrics that the task is finally evaluated on (e.g., CIDEr, BLEU). Moreover, these models suffer from exposure bias (Ran-\nzato et al., 2016), which occurs when a model is only exposed to the training data distribution, instead of its own predictions. First, using a sequence-level training, policy gradient approach (Ranzato et al., 2016), we allow video captioning models to directly optimize these nondifferentiable metrics, as rewards in a reinforcement learning paradigm. We also address the exposure bias issue by using a mixed-loss (Paulus et al., 2017; Wu et al., 2016), i.e., combining the cross-entropy and reward-based losses, which also helps maintain output fluency.\nNext, we introduce a novel entailment-corrected reward that checks for logically-directed partial matches. Current reinforcement-based text generation works use traditional phrase-matching metrics (e.g., CIDEr, BLEU) as their reward function. However, these metrics use undirected ngram matching of the machine-generated caption with the ground-truth caption, and hence fail to capture its directed logical correctness. Therefore, they still give high scores to even those generated captions that contain a single but critical wrong word (e.g., negation, unrelated action or object), because all the other words still match with the ground truth. We introduce CIDEnt, which penalizes the phrase-matching metric (CIDEr) based reward, when the entailment score is low. This ensures that a generated caption gets a high re-\nward only when it is a directed match with (i.e., it is logically implied by) the ground truth caption, hence avoiding contradictory or unrelated information (e.g., see Fig. 1). Empirically, we show that first the CIDEr-reward model achieves significant improvements over the cross-entropy baseline (on multiple datasets, and automatic and human evaluation); next, the CIDEnt-reward model further achieves significant improvements over the CIDEr-based reward. Overall, we achieve the new state-of-the-art on the MSR-VTT dataset."}, {"heading": "2 Related Work", "text": "Past work has presented several sequence-tosequence models for video captioning, using attention, hierarchical RNNs, 3D-CNN video features, joint embedding spaces, language fusion, etc., but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).\nPolicy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.1 Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup.\nRecognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockta\u0308schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation.\n1Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014)."}, {"heading": "3 Models", "text": "Attention Baseline (Cross-Entropy) Our attention-based seq-to-seq baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features {f1:n} via a bi-directional LSTM-RNN and then generate the caption w1:m using an LSTM-RNN with an attention mechanism. Let \u03b8 be the model parameters and w\u2217 1:m be the ground-truth caption, then the cross entropy loss function is:\nL(\u03b8) = \u2212 m \u2211\nt=1\nlog p(w\u2217t |w \u2217 1:t\u22121, f1:n) (1)\nwhere p(wt|w1:t\u22121, f1:n) = softmax(W Thdt ), W T is the projection matrix, and wt and h d t are the generated word and the RNN decoder hidden state at time step t, computed using the standard RNN recursion and attention-based context vector ct. Details of the attention model are in the supplementary (due to space constraints).\nReinforcement Learning (Policy Gradient) In order to directly optimize the sentence-level test metrics (as opposed to the cross-entropy loss above), we use a policy gradient p\u03b8, where \u03b8 represent the model parameters. Here, our baseline model acts as an agent and interacts with its environment (video and caption). At each time step, the agent generates a word (action), and the generation of the end-of-sequence token results in a reward r to the agent. Our training objective is to minimize the negative expected reward function:\nL(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w s)] (2)\nwhere ws is the word sequence sampled from the model. Based on the REINFORCE algorithm (Williams, 1992), the gradients of this nondifferentiable, reward-based loss function are:\n\u2207\u03b8L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w s) \u00b7 \u2207\u03b8 log p\u03b8(w s)] (3)\nWe follow Ranzato et al. (2016) approximating the above gradients via a single sampled word\nsequence. We also use a variance-reducing bias (baseline) estimator in the reward function. Their details and the partial derivatives using the chain rule are described in the supplementary.\nMixed Loss During reinforcement learning, optimizing for only the reinforcement loss (with automatic metrics as rewards) doesn\u2019t ensure the readability and fluency of the generated caption, and there is also a chance of gaming the metrics without actually improving the quality of the output (Liu et al., 2016a). Hence, for training our reinforcement based policy gradients, we use a mixed loss function, which is a weighted combination of the cross-entropy loss (XE) and the reinforcement learning loss (RL), similar to the previous work (Paulus et al., 2017; Wu et al., 2016). This mixed loss improves results on the metric used as reward through the reinforcement loss (and improves relevance based on our entailmentenhanced rewards) but also ensures better readability and fluency due to the cross-entropy loss (in which the training objective is a conditioned language model, learning to produce fluent captions). Our mixed loss is defined as:\nLMIXED = (1\u2212 \u03b3)LXE + \u03b3LRL (4)\nwhere \u03b3 is a tuning parameter used to balance the two losses. For annealing and faster convergence, we start with the optimized cross-entropy loss baseline model, and then move to optimizing the above mixed loss function.2"}, {"heading": "4 Reward Functions", "text": "Caption Metric Reward Previous image captioning papers have used traditional captioning metrics such as CIDEr, BLEU, or METEOR as reward functions, based on the match between the generated caption sample and the ground-truth reference(s). First, it has been shown by Vedantam\n2We also experimented with the curriculum learning \u2018MIXER\u2019 strategy of Ranzato et al. (2016), where the XE+RL annealing is based on the decoder time-steps; however, the mixed loss function strategy (described above) performed better in terms of maintaining output caption fluency.\net al. (2015) that CIDEr, based on a consensus measure across several human reference captions, has a higher correlation with human evaluation than other metrics such as METEOR, ROUGE, and BLEU. They further showed that CIDEr gets better with more number of human references (and this is a good fit for our video captioning datasets, which have 20-40 human references per video).\nMore recently, Rennie et al. (2016) further showed that CIDEr as a reward in image captioning outperforms all other metrics as a reward, not just in terms of improvements on CIDEr metric, but also on all other metrics. In line with these above previous works, we also found that CIDEr as a reward (\u2018CIDEr-RL\u2019 model) achieves the best metric improvements in our video captioning task, and also has the best human evaluation improvements (see Sec. 6.3 for result details, incl. those about other rewards based on BLEU, SPICE).\nEntailment Corrected Reward Although CIDEr performs better than other metrics as a reward, all these metrics (including CIDEr) are still based on an undirected n-gram matching score between the generated and ground truth captions. For example, the wrong caption \u201ca man is playing football\u201d w.r.t. the correct caption \u201ca man is playing basketball\u201d still gets a high score, even though these two captions belong to two completely different events. Similar issues hold in case of a negation or a wrong action/object in the generated caption (see examples in Table 1).\nWe address the above issue by using an entailment score to correct the phrase-matching metric (CIDEr or others) when used as a reward, ensuring that the generated caption is logically implied by (i.e., is a paraphrase or directed partial match with) the ground-truth caption. To achieve an accurate entailment score, we adapt the state-of-theart decomposable-attention model of Parikh et al. (2016) trained on the SNLI corpus (image caption domain). This model gives us a probability for whether the sampled video caption (generated by our model) is entailed by the ground truth caption as premise (as opposed to a contradiction or neu-\ntral case).3 Similar to the traditional metrics, the overall \u2018Ent\u2019 score is the maximum over the entailment scores for a generated caption w.r.t. each reference human caption (around 20/40 per MSRVTT/YouTube2Text video). CIDEnt is defined as:\nCIDEnt =\n{\nCIDEr\u2212 \u03bb, if Ent < \u03b2 CIDEr, otherwise (5)\nwhich means that if the entailment score is very low, we penalize the metric reward score by decreasing it by a penalty \u03bb. This agreement-based formulation ensures that we only trust the CIDErbased reward in cases when the entailment score is also high. Using CIDEr\u2212\u03bb also ensures the smoothness of the reward w.r.t. the original CIDEr function (as opposed to clipping the reward to a constant). Here, \u03bb and \u03b2 are hyperparameters that can be tuned on the dev-set; on light tuning, we found the best values to be intuitive: \u03bb = roughly the baseline (cross-entropy) model\u2019s score on that metric (e.g., 0.45 for CIDEr on MSR-VTT dataset); and \u03b2 = 0.33 (i.e., the 3-class entailment classifier chose contradiction or neutral label for this pair). Table 1 shows some examples of sampled generated captions during our model training, where CIDEr was misleadingly high for incorrect captions, but the low entailment score (probability) helps us successfully identify these cases and penalize the reward."}, {"heading": "5 Experimental Setup", "text": "Datasets We use 2 datasets: MSR-VTT (Xu et al., 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video. Standard splits and other details in supp. Automatic Evaluation We use several standard automated evaluation metrics: METEOR, BLEU4, CIDEr-D, and ROUGE-L (from MS-COCO evaluation server (Chen et al., 2015)). Human Evaluation We also present human evaluation for comparison of baseline-XE, CIDEr-RL, and CIDEnt-RL models, esp. because the automatic metrics cannot be trusted solely. Relevance measures how related is the generated caption w.r.t, to the video content, whereas coherence measures readability of the generated caption.\n3Our entailment classifier based on Parikh et al. (2016) is 92% accurate on entailment in the caption domain, hence serving as a highly accurate reward score. For other domains in future tasks such as new summarization, we plan to use the new multi-domain dataset by Williams et al. (2017).\nTraining Details All the hyperparameters are tuned on the validation set. All our results (including baseline) are based on a 5-avg-ensemble. See supplementary for extra training details, e.g., about the optimizer, learning rate, RNN size, Mixed-loss, and CIDEnt hyperparameters."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Primary Results", "text": "Table 2 shows our primary results on the popular MSR-VTT dataset. First, our baseline attention model trained on cross entropy loss (\u2018BaselineXE\u2019) achieves strong results w.r.t. the previous state-of-the-art methods.4 Next, our policy gradient based mixed-loss RL model with reward as CIDEr (\u2018CIDEr-RL\u2019) improves significantly5 over the baseline on all metrics, and not just the CIDEr metric. It also achieves statistically significant improvements in terms of human relevance evaluation (see below). Finally, the last row in Table 2 shows results for our novel CIDEnt-reward RL model (\u2018CIDEnt-RL\u2019). This model achieves statistically significant6 improvements on top of the strong CIDEr-RL model, on all automatic metrics (as well as human evaluation). Note that in Table 2, we also report the CIDEnt reward scores, and the CIDEnt-RL model strongly outperforms CIDEr and baseline models on this entailmentcorrected measure. Overall, we are also the new Rank1 on the MSR-VTT leaderboard, based on their ranking criteria.\nHuman Evaluation We also perform small human evaluation studies (250 samples from the MSR-VTT test set output) to compare our 3 models pairwise.7 As shown in Table 3 and Table 4, in terms of relevance, first our CIDEr-RL model stat. significantly outperforms the baseline XE model (p < 0.02); next, our CIDEnt-RL model significantly outperforms the CIDEr-RL model (p <\n4We list previous works\u2019 results as reported by the MSR-VTT dataset paper itself, as well as their 3 leaderboard winners (http://ms-multimedia-challenge. com/leaderboard), plus the 10-ensemble video+entailment generation multi-task model of Pasunuru and Bansal (2017).\n5Statistical significance of p < 0.01 for CIDEr, METEOR, and ROUGE, and p < 0.05 for BLEU, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994).\n6Statistical significance of p < 0.01 for CIDEr, BLEU, ROUGE, and CIDEnt, and p < 0.05 for METEOR.\n7We randomly shuffle pairs to anonymize model identity and the human evaluator then chooses the better caption based on relevance and coherence (see Sec. 5). \u2018Not Distinguishable\u2019 are cases where the annotator found both captions to be equally good or equally bad).\n0.03). The models are statistically equal on coherence in both comparisons."}, {"heading": "6.2 Other Datasets", "text": "We also tried our CIDEr and CIDEnt reward models on the YouTube2Text dataset. In Table 5, we first see strong improvements from our CIDEr-RL model on top of the cross-entropy baseline. Next, the CIDEnt-RL model also shows some improvements over the CIDEr-RL model, e.g., on BLEU and the new entailment-corrected CIDEnt score. It also achieves significant improvements on human relevance evaluation (250 samples).8"}, {"heading": "6.3 Other Metrics as Reward", "text": "As discussed in Sec. 4, CIDEr is the most promising metric to use as a reward for captioning, based on both previous work\u2019s findings as well as ours. We did investigate the use of other metrics as the reward. When using BLEU as a reward (on MSR-VTT), we found that this BLEU-RL model achieves BLEU-metric improvements, but was worse than the cross-entropy baseline on human evaluation. Similarly, a BLEUEnt-RL model achieves BLEU and BLEUEnt metric improvements, but is again worse on human evaluation.\n8This dataset has a very small dev-set, causing tuning issues \u2013 we plan to use a better train/dev re-split in future work.\nWe also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this produced long repetitive phrases (as also discussed in Liu et al. (2016b))."}, {"heading": "6.4 Analysis", "text": "Fig. 1 shows an example where our CIDEntreward model correctly generates a ground-truth style caption, whereas the CIDEr-reward model produces a non-entailed caption because this caption will still get a high phrase-matching score. Several more such examples are in the supp."}, {"heading": "7 Conclusion", "text": "We first presented a mixed-loss policy gradient approach for video captioning, allowing for metric-based optimization. We next presented an entailment-corrected CIDEnt reward that further improves results, achieving the new state-of-theart on MSR-VTT. In future work, we are applying our entailment-corrected rewards to other directed generation tasks such as image captioning and document summarization (using the new multi-domain NLI corpus (Williams et al., 2017))."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their helpful comments. This work was supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Research Grant, and NVidia GPU awards.\nSupplementary Material"}, {"heading": "A Attention-based Baseline Model (Cross-Entropy)", "text": "Our attention baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features to a bi-directional LSTM-RNN and then generate the caption using a single layer LSTM-RNN, with an attention mechanism. Let {f1, f2, ..., fn} be the frame-level features of a video clip and {w1, w2, ..., wm} be the sequence of words forming a caption. The distribution of words at time step t given the previously generated words and input video frame-level features is given as follows:\np(wt|w1:t\u22121, f1:n) = softmax(W Thdt ) (6)\nwhere wt and h d t are the generated word and hidden state of the LSTM decoder at time step t. W T is the projection matrix. hdt is given as follows:\nhdt = S(h d t\u22121, wt\u22121, ct) (7)\nwhere S is a non-linear function. hdt\u22121 and wt\u22121 are previous time step\u2019s hidden state and generated word. ct is a context vector which is a linear weighted combination of the encoder hidden states hei , given by ct = \u2211 \u03b1t,ih e i . These weights \u03b1t,i act as an attention mechanism, and are defined as follows:\n\u03b1t,i = exp(et,i)\n\u2211n k=1 exp(et,k)\n(8)\nwhere the attention function et,i is defined as:\net,i = w T tanh(Wah e i + Uah d t\u22121 + ba) (9)\nwhere w, Wa, Ua, and ba are trained attention parameters. Let \u03b8 be the model parameters and {w\u2217 1 , w\u2217 2 , ..., w\u2217m} be the ground-truth word sequence, then the cross entropy loss optimization function is defined as follows:\nL(\u03b8) = \u2212\nm \u2211\ni=1\nlog(p(w\u2217t |w \u2217 1:t\u22121, f1:n)) (10)"}, {"heading": "B Reinforcement Learning (Policy Gradient)", "text": "Traditional video captioning systems minimize the cross entropy loss during training, but typically evaluated using phrase-matching metrics: BLEU, METEOR, CIDEr, and ROUGE-L. This discrepancy can be addressed by directly optimizing the\nnon-differentiable metric scores using policy gradients p\u03b8, where \u03b8 represents the model parameters. In our captioning system, our baseline attention model acts as an agent and interacts with its environment (video and caption). At each time step, the agent generates a word (action), and the generation of the end-of-sequence token results in a reward r to the agent. Our training objective is to minimize the negative expected reward function given by:\nL(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w s)] (11)\nwhere ws = {ws 1 , ws 2 , ..., wsm}, and w s t is the word sampled from the model at time step t. Based on the REINFORCE algorithm (Williams, 1992), the gradients of the non-differentiable, reward-based loss function can be computed as follows:\n\u2207\u03b8L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(w s)\u2207\u03b8 log p\u03b8(w s)] (12)\nThe above gradients can be approximated from a single sampled word sequence ws from p\u03b8 as follows:\n\u2207\u03b8L(\u03b8) \u2248 \u2212r(w s)\u2207\u03b8 log p\u03b8(w s) (13)\nHowever, the above approximation has high variance because of estimating the gradient with a single sample. Adding a baseline estimator reduces this variance (Williams, 1992) without changing the expected gradient. Hence, Eqn: 13 can be rewritten as follows:\n\u2207\u03b8L(\u03b8) \u2248 \u2212(r(w s)\u2212 bt)\u2207\u03b8 log p\u03b8(w s) (14)\nwhere bt is the baseline estimator, where bt can be a function of \u03b8 or time step t, but not a function of ws. In our model, baseline estimator is a simple linear regressor with hidden state of the decoder hdt at time step t as the input. We stop the back propagation of gradients before the hidden states for the baseline bias estimator. Using the chain rule, loss function can be written as:\n\u2207\u03b8L(\u03b8) =\nm \u2211\nt=1\n\u2202L\n\u2202st\n\u2202st \u2202\u03b8 (15)\nwhere st is the input to the softmax layer, where st = W Thdt . \u2202L \u2202st is given by (Zaremba and Sutskever, 2015) as follows:\n\u2202L \u2202st \u2248 (r(ws)\u2212 bt)(p\u03b8(wt|h d t )\u2212 1wst ) (16)\nThe overall intuition behind this gradient formulation is: if the reward r(ws) for the sampled word sequence ws is greater than the baseline estimator bt, the gradient of the loss function becomes negative, then model encourages the sampled distribution by increasing their word probabilities, otherwise the model discourages the sampled distribution by decreasing their word probabilities."}, {"heading": "C Experimental Setup", "text": "C.1 MSR-VTT Dataset\nMSR-VTT is a diverse collection of 10, 000 video clips (41.2 hours of duration) from a commercial video search engine. Each video has 20 human annotated reference captions collected through Amazon Mechanical Turk (AMT). We use the standard split as provided in (Xu et al., 2016), i.e., 6513 for training, 497 for testing , and remaining for testing. For each video, we sample at 3fps and we extract Inception-v4 (Szegedy et al., 2016) features from these sampled frames and we also remove all the punctuations from the text data.\nC.2 YouTube2Text Dataset\nWe also evaluate our models on YouTube2Text dataset (Chen and Dolan, 2011). This dataset has 1970 video clips and each clip is annotated with an average of 40 captions by humans. We use the standard split as given in (Venugopalan et al., 2015a), i.e., 1200 clips for training, 100 for validation and 670 for testing. We do similar preprocessing as the MSR-VTT dataset.\nC.3 Automatic Evaluation Metrics\nWe use several standard automated evaluation metrics: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004). We use the standard Microsoft-COCO evaluation server (Chen et al., 2015).\nC.4 Human Evaluation\nApart from the automatic metrics, we also present human evaluation comparing the CIDEnt-reward model with the CIDEr-reward model, esp. because the automatic metrics cannot be trusted solely. Human evaluation uses Relevance and Coherence as the comparison metrics. Relevance is about how related is the generated caption w.r.t. the content\nof the video, whereas coherence is about the logic, fluency, and readability of the generated caption."}, {"heading": "D Training Details", "text": "All the hyperparameters are tuned on the validation set. For each of our main models (baseline, CIDEr and CIDEnt), we report the results on a 5-avg-ensemble, where we run the model 5 times with different initialization random seeds and take the average probabilities at each time step of the decoder during inference time. We use a fixed size step LSTM-RNN encoder-decoder, with encoder step size of 50 and decoder step size of 16. Each LSTM has a hidden size of 1024. We use Inception-v4 features as video frame-level features. We use word embedding size of 512. Also, we project down the 1536-dim image features (Inception-v4) to 512-dim.\nWe apply dropout to vertical connections as proposed in Zaremba et al. (2014), with a value 0.5 and a gradient clip size of 10. We use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.0001 for baseline cross-entropy loss. All the trainable weights are initialized with a uniform distribution in the range [\u22120.08, 0.08]. During the test time inference, we use beam search of size 5. All our reward-based models use mixed loss optimization (Paulus et al., 2017; Wu et al., 2016), where we train the model based on weighted (\u03b3) combination of crossentropy loss and reinforcement loss. For MSRVTT dataset, we use \u03b3 = 0.9995 for our CIDErRL model and \u03b3 = 0.9990 for our CIDEntRL model. For YouTube2Text/MSVD dataset, we use \u03b3 = 0.9985 for our CIDEr-RL model and \u03b3 = 0.9990 and for our CIDEnt-RL model. The learning rate for the mixed-loss optimization is 1 \u00d7 10\u22125 for MSR-VTT, and 1 \u00d7 10\u22126 for YouTube2Text/MSVD. The \u03bb hyperparameter in our CIDEnt reward formulation (see Sec. 4 in main paper) is roughly equal to the baseline cross-entropy model\u2019s score on that metric, i.e., \u03bb = 0.45 for MSR-VTT CIDEnt-RL model and \u03bb = 0.75 for YouTube2Text/MSVD CIDEnt-RL model."}, {"heading": "E Analysis", "text": "Figure 3 shows several examples where our CIDEnt-reward model produces better entailed captions than the ones generated by the CIDErreward model. This is because the CIDEr-style\ncaptioning metrics achieve a high score even when the generation does not exactly entail the ground truth but is just a high phrase overlap. This can obviously cause issues by inserting a single wrong word such as a negation, contradiction, or wrong action/object. On the other hand, our entailment-enhanced CIDEnt score is only high when both CIDEr and the entailment classifier achieve high scores. The CIDEr-RL model, in turn, produces better captions than the baseline cross-entropy model, which is not aware of sentence-level matching at all."}], "references": [{"title": "SPICE: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."], "venue": "ECCV, pages 382\u2013398.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190\u2013200.", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "The PASCAL recognising textual entailment challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177\u2013", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "EACL.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Comparing automatic evaluation measures for image description", "author": ["Desmond Elliott", "Frank Keller."], "venue": "ACL, pages 452\u2013457.", "citeRegEx": "Elliott and Keller.,? 2014", "shortCiteRegEx": "Elliott and Keller.", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "Journal of Artificial Intelligence Research, 47:853\u2013899.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment", "author": ["Sergio Jimenez", "George Duenas", "Julia Baquero", "Alexander Gelbukh", "Av Juan Dios B\u00e1tiz", "AvMendiz\u00e1bal."], "venue": "In SemEval, pages", "citeRegEx": "Jimenez et al\\.,? 2014", "shortCiteRegEx": "Jimenez et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Illinois-LH: A denotational and distributional approach to semantics", "author": ["Alice Lai", "Julia Hockenmaier."], "venue": "Proc. SemEval, 2:5.", "citeRegEx": "Lai and Hockenmaier.,? 2014", "shortCiteRegEx": "Lai and Hockenmaier.", "year": 2014}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 workshop, volume 8.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "EMNLP.", "citeRegEx": "Liu et al\\.,? 2016a", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Improved image captioning via policy gradient optimization of SPIDEr", "author": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy."], "venue": "arXiv preprint arXiv:1612.00370.", "citeRegEx": "Liu et al\\.,? 2016b", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Computer-intensive methods for testing hypotheses", "author": ["Eric W Noreen."], "venue": "Wiley New York.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "ZhongwenXu", "Yi Yang", "FeiWu", "Yueting Zhuang."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages", "citeRegEx": "Pan et al\\.,? 2016a", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4594\u20134602.", "citeRegEx": "Pan et al\\.,? 2016b", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "ToddWard", "WeiJing Zhu."], "venue": "ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit."], "venue": "EMNLP.", "citeRegEx": "Parikh et al\\.,? 2016", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Multitask video captioning with video and entailment generation", "author": ["Ramakanth Pasunuru", "Mohit Bansal."], "venue": "Proceedings of ACL.", "citeRegEx": "Pasunuru and Bansal.,? 2017", "shortCiteRegEx": "Pasunuru and Bansal.", "year": 2017}, {"title": "A deep reinforced model for abstractive summarization", "author": ["Romain Paulus", "Caiming Xiong", "Richard Socher."], "venue": "arXiv preprint arXiv:1705.04304.", "citeRegEx": "Paulus et al\\.,? 2017", "shortCiteRegEx": "Paulus et al\\.", "year": 2017}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Self-critical sequence training for image captioning", "author": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel."], "venue": "arXiv preprint arXiv:1612.00563.", "citeRegEx": "Rennie et al\\.,? 2016", "shortCiteRegEx": "Rennie et al\\.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "ICLR.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke."], "venue": "CoRR.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "CVPR, pages 4566\u20134575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Improving lstm-based video description with linguistic knowledge mined from text", "author": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Raymond Mooney", "Kate Saenko."], "venue": "EMNLP.", "citeRegEx": "Venugopalan et al\\.,? 2016", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko."], "venue": "CVPR, pages 4534\u20134542.", "citeRegEx": "Venugopalan et al\\.,? 2015a", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko."], "venue": "NAACL HLT.", "citeRegEx": "Venugopalan et al\\.,? 2015b", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "author": ["Adina Williams", "Nikita Nangia", "Samuel R Bowman."], "venue": "arXiv preprint arXiv:1704.05426.", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "MSR-VTT: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui."], "venue": "InCVPR, pages 5288\u2013 5296.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville."], "venue": "CVPR, pages 4507\u20134515.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521, 362.", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 30, "context": "Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a).", "startOffset": 159, "endOffset": 205}, {"referenceID": 18, "context": "Advancements in neural sequenceto-sequence learning has shown promising improvements on this task, based on encoderdecoder, attention, and hierarchical models (Venugopalan et al., 2015a; Pan et al., 2016a).", "startOffset": 159, "endOffset": 205}, {"referenceID": 24, "context": "First, using a sequence-level training, policy gradient approach (Ranzato et al., 2016), we allow video captioning models to directly optimize these nondifferentiable metrics, as rewards in a reinforcement learning paradigm.", "startOffset": 65, "endOffset": 87}, {"referenceID": 23, "context": "We also address the exposure bias issue by using a mixed-loss (Paulus et al., 2017; Wu et al., 2016), i.", "startOffset": 62, "endOffset": 100}, {"referenceID": 30, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 35, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 29, "context": ", but using word-level cross entropy loss training (Venugopalan et al., 2015a; Yao et al., 2015; Pan et al., 2016a,b; Venugopalan et al., 2016).", "startOffset": 51, "endOffset": 143}, {"referenceID": 20, "context": "Policy gradient for image captioning was recently presented by Ranzato et al. (2016), using a mixed sequence level training paradigm to use non-differentiable evaluation metrics as rewards.", "startOffset": 63, "endOffset": 85}, {"referenceID": 14, "context": "Liu et al. (2016b) and Rennie et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Liu et al. (2016b) and Rennie et al. (2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively.", "startOffset": 0, "endOffset": 44}, {"referenceID": 14, "context": "(2016) improve upon this using Monte Carlo roll-outs and a test inference baseline, respectively. Paulus et al. (2017) presented summarization results with ROUGE rewards, in a mixed-loss setup.", "startOffset": 78, "endOffset": 119}, {"referenceID": 6, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 13, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 11, "context": "Recognizing Textual Entailment (RTE) is a traditional NLP task (Dagan et al., 2006; Lai and Hockenmaier, 2014; Jimenez et al., 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al.", "startOffset": 63, "endOffset": 132}, {"referenceID": 5, "context": "There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 26, "context": "There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al.", "startOffset": 51, "endOffset": 97}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation.", "startOffset": 66, "endOffset": 272}, {"referenceID": 2, "context": ", 2014), boosted by a large dataset (SNLI) recently introduced by Bowman et al. (2015). There have been several leaderboard models on SNLI (Cheng et al., 2016; Rockt\u00e4schel et al., 2016); we focus on the decomposable, intra-sentence attention model of Parikh et al. (2016). Recently, Pasunuru and Bansal (2017) used multi-task learning to combine video captioning with entailment and video generation.", "startOffset": 66, "endOffset": 310}, {"referenceID": 28, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 0, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 16, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 10, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 9, "context": "Several papers have presented the relative comparison of image captioning metrics, and their pros and cons (Vedantam et al., 2015; Anderson et al., 2016; Liu et al., 2016b; Hodosh et al., 2013; Elliott and Keller, 2014).", "startOffset": 107, "endOffset": 219}, {"referenceID": 1, "context": "Attention Baseline (Cross-Entropy) Our attention-based seq-to-seq baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features {f1:n} via a bi-directional LSTM-RNN and then generate the caption w1:m using an LSTM-RNN with an attention mechanism.", "startOffset": 99, "endOffset": 122}, {"referenceID": 33, "context": "Based on the REINFORCE algorithm (Williams, 1992), the gradients of this nondifferentiable, reward-based loss function are:", "startOffset": 33, "endOffset": 49}, {"referenceID": 24, "context": "We follow Ranzato et al. (2016) approximating the above gradients via a single sampled word", "startOffset": 10, "endOffset": 32}, {"referenceID": 15, "context": "Mixed Loss During reinforcement learning, optimizing for only the reinforcement loss (with automatic metrics as rewards) doesn\u2019t ensure the readability and fluency of the generated caption, and there is also a chance of gaming the metrics without actually improving the quality of the output (Liu et al., 2016a).", "startOffset": 292, "endOffset": 311}, {"referenceID": 23, "context": "Hence, for training our reinforcement based policy gradients, we use a mixed loss function, which is a weighted combination of the cross-entropy loss (XE) and the reinforcement learning loss (RL), similar to the previous work (Paulus et al., 2017; Wu et al., 2016).", "startOffset": 226, "endOffset": 264}, {"referenceID": 23, "context": "We also experimented with the curriculum learning \u2018MIXER\u2019 strategy of Ranzato et al. (2016), where the XE+RL annealing is based on the decoder time-steps; however, the mixed loss function strategy (described above) performed better in terms of maintaining output caption fluency.", "startOffset": 70, "endOffset": 92}, {"referenceID": 14, "context": "(2016), where the XE+RL annealing is based on the decoder time-steps; however, the mixed loss function strategy (described above) performed better in terms of maintaining output caption fluency. et al. (2015) that CIDEr, based on a consensus measure across several human reference captions, has a higher correlation with human evaluation than other metrics such as METEOR, ROUGE, and BLEU.", "startOffset": 29, "endOffset": 209}, {"referenceID": 24, "context": "More recently, Rennie et al. (2016) further showed that CIDEr as a reward in image captioning outperforms all other metrics as a reward, not just in terms of improvements on CIDEr metric, but also on all other metrics.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "To achieve an accurate entailment score, we adapt the state-of-theart decomposable-attention model of Parikh et al. (2016) trained on the SNLI corpus (image caption domain).", "startOffset": 102, "endOffset": 123}, {"referenceID": 34, "context": "Datasets We use 2 datasets: MSR-VTT (Xu et al., 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video.", "startOffset": 36, "endOffset": 53}, {"referenceID": 3, "context": ", 2016) has 10, 000 videos, 20 references/video; and YouTube2Text/MSVD (Chen and Dolan, 2011) has 1970 videos, 40 references/video.", "startOffset": 71, "endOffset": 93}, {"referenceID": 4, "context": "Automatic Evaluation We use several standard automated evaluation metrics: METEOR, BLEU4, CIDEr-D, and ROUGE-L (from MS-COCO evaluation server (Chen et al., 2015)).", "startOffset": 143, "endOffset": 162}, {"referenceID": 20, "context": "Our entailment classifier based on Parikh et al. (2016) is 92% accurate on entailment in the caption domain, hence serving as a highly accurate reward score.", "startOffset": 35, "endOffset": 56}, {"referenceID": 20, "context": "Our entailment classifier based on Parikh et al. (2016) is 92% accurate on entailment in the caption domain, hence serving as a highly accurate reward score. For other domains in future tasks such as new summarization, we plan to use the new multi-domain dataset by Williams et al. (2017). Training Details All the hyperparameters are tuned on the validation set.", "startOffset": 35, "endOffset": 289}, {"referenceID": 17, "context": "05 for BLEU, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994).", "startOffset": 41, "endOffset": 83}, {"referenceID": 8, "context": "05 for BLEU, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994).", "startOffset": 41, "endOffset": 83}, {"referenceID": 20, "context": "com/leaderboard), plus the 10-ensemble video+entailment generation multi-task model of Pasunuru and Bansal (2017). Statistical significance of p < 0.", "startOffset": 87, "endOffset": 114}, {"referenceID": 32, "context": "4 Yao et al. (2015) 35.", "startOffset": 2, "endOffset": 20}, {"referenceID": 32, "context": "2 Xu et al. (2016) 36.", "startOffset": 2, "endOffset": 19}, {"referenceID": 21, "context": "9 Pasunuru and Bansal (2017) 40.", "startOffset": 2, "endOffset": 29}, {"referenceID": 0, "context": "We also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this produced long repetitive phrases (as also discussed in Liu et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "We also experimented with the new SPICE metric (Anderson et al., 2016) as a reward, but this produced long repetitive phrases (as also discussed in Liu et al. (2016b)).", "startOffset": 48, "endOffset": 167}, {"referenceID": 32, "context": "In future work, we are applying our entailment-corrected rewards to other directed generation tasks such as image captioning and document summarization (using the new multi-domain NLI corpus (Williams et al., 2017)).", "startOffset": 191, "endOffset": 214}, {"referenceID": 1, "context": "Our attention baseline model is similar to the Bahdanau et al. (2015) architecture, where we encode input frame level video features to a bi-directional LSTM-RNN and then generate the caption using a single layer LSTM-RNN, with an attention mechanism.", "startOffset": 47, "endOffset": 70}, {"referenceID": 33, "context": "Based on the REINFORCE algorithm (Williams, 1992), the gradients of the non-differentiable, reward-based loss function can be computed as follows:", "startOffset": 33, "endOffset": 49}, {"referenceID": 33, "context": "Adding a baseline estimator reduces this variance (Williams, 1992) without changing the expected gradient.", "startOffset": 50, "endOffset": 66}, {"referenceID": 36, "context": "\u2202L \u2202st is given by (Zaremba and Sutskever, 2015) as follows:", "startOffset": 19, "endOffset": 48}, {"referenceID": 34, "context": "We use the standard split as provided in (Xu et al., 2016), i.", "startOffset": 41, "endOffset": 58}, {"referenceID": 27, "context": "For each video, we sample at 3fps and we extract Inception-v4 (Szegedy et al., 2016) features from these sampled frames and we also remove all the punctuations from the text data.", "startOffset": 62, "endOffset": 84}, {"referenceID": 3, "context": "We also evaluate our models on YouTube2Text dataset (Chen and Dolan, 2011).", "startOffset": 52, "endOffset": 74}, {"referenceID": 30, "context": "We use the standard split as given in (Venugopalan et al., 2015a), i.", "startOffset": 38, "endOffset": 65}, {"referenceID": 7, "context": "We use several standard automated evaluation metrics: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al.", "startOffset": 61, "endOffset": 88}, {"referenceID": 20, "context": "We use several standard automated evaluation metrics: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 28, "context": ", 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004).", "startOffset": 17, "endOffset": 40}, {"referenceID": 14, "context": ", 2015), and ROUGE-L (Lin, 2004).", "startOffset": 21, "endOffset": 32}, {"referenceID": 4, "context": "We use the standard Microsoft-COCO evaluation server (Chen et al., 2015).", "startOffset": 53, "endOffset": 72}, {"referenceID": 12, "context": "We use Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.", "startOffset": 22, "endOffset": 43}, {"referenceID": 23, "context": "All our reward-based models use mixed loss optimization (Paulus et al., 2017; Wu et al., 2016), where we train the model based on weighted (\u03b3) combination of crossentropy loss and reinforcement loss.", "startOffset": 56, "endOffset": 94}, {"referenceID": 34, "context": "We apply dropout to vertical connections as proposed in Zaremba et al. (2014), with a value 0.", "startOffset": 56, "endOffset": 78}], "year": 2017, "abstractText": "Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}