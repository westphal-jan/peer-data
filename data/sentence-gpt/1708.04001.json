{"id": "1708.04001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Group-driven Reinforcement Learning for Personalized mHealth Intervention", "abstract": "Due to the popularity of smartphones and wearable devices nowadays, mobile health (mHealth) technologies are promising to bring positive and wide impacts on people's health. State-of-the-art decision-making methods for mHealth rely on some ideal assumptions. Those methods either assume that the users are completely homogenous or completely heterogeneous. However, in reality, a user might be similar with some, but not all, users. In this paper, we propose a novel group-driven reinforcement learning method for the mHealth. We aim to understand how to share information among similar users to better convert the limited user information into sharper learned RL policies. Specifically, we employ the K-means clustering method to group users based on their trajectory information similarity and learn a shared RL policy for each group. Extensive experiment results have shown that our method can achieve clear gains over the state-of-the-art RL methods for mHealth. Specifically, we use the K-means clustering method to learn how to analyze information from both users and a single user based on the correlation between these two data points. In other words, if we have no prior data on the correlation between these two data points, it would be quite difficult to perform the analysis in an unbiased manner. We also use this technique to analyze the social networks of other people's Facebook users. To summarize, our data was generated with the help of a simple neural network and was then used to develop a neural network that can measure social interaction among the users, with a focus on the social network of other users.\n\n\n\nWe have shown that our technique can achieve clear gains over the state-of-the-art RL methods for mHealth. This technique is currently being developed by the M&M Research Center, the Universit\u00e9 de la Universit\u00e9 of Madrid. The aim of this study is to further develop the technique in the field of social networks that will enable the implementation of social networks. For example, in order to analyze social networks, we need to collect information on individuals' behaviors.\nIn order to improve the results from our study, we have used a network that uses neural networks of other users. We have been using several techniques for detecting social networks of other people, including a search term based on the search term of the subject for a person. We believe that it is possible to learn a much deeper understanding of the social network and the social network of others. We also use a method for extracting the social network of other people with similar characteristics.\nWe", "histories": [["v1", "Mon, 14 Aug 2017 03:58:29 GMT  (64kb,D)", "http://arxiv.org/abs/1708.04001v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["feiyun zhu", "jun guo", "zheng xu", "peng liao", "junzhou huang"], "accepted": false, "id": "1708.04001"}, "pdf": {"name": "1708.04001.pdf", "metadata": {"source": "META", "title": "Group-driven Reinforcement Learning for Personalized mHealth Intervention", "authors": ["Feiyun Zhu", "Jun Guo", "Zheng Xu", "Peng Liao", "Junzhou Huang"], "emails": [], "sections": null, "references": [{"title": "Semisupervised hyperspectral image classification via discriminant analysis and robust regression", "author": ["G. Cheng", "F. Zhu", "S. Xiang", "Y. Wang", "C. Pan"], "venue": "IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Randomised trials for the fitbit generation. Significance", "author": ["W. Dempsey", "P. Liao", "P. Klasnja", "I. Nahum-Shani", "S.A. Murphy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Algorithmic survey of parametric value function approximation", "author": ["M. Geist", "O. Pietquin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A survey of actorcritic reinforcement learning: Standard and natural policy gradients", "author": ["I. Grondman", "L. Busoniu", "G.A.D. Lopes", "R. Babuska"], "venue": "IEEE Trans. Systems, Man, and Cybernetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A smartphone application to support recovery from alcoholism: a randomized clinical trial", "author": ["D. Gustafson", "F. McTavish", "M. Chih", "A. Atwood", "R. Johnson", "M. B", "D. Shah"], "venue": "JAMA Psychiatry,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Learning-based fully 3d face reconstruction from a single image", "author": ["X. Hu", "Y. Wang", "F. Zhu", "C. Pan"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. of Machine Learning Research (JLMR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention", "author": ["H. Lei"], "venue": "PhD thesis, University of Michigan,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "An actor-critic contextual bandit algorithm for personalized interventions using mobile devices", "author": ["H. Lei", "A. Tewari", "S. Murphy"], "venue": "In NIPS 2014 Workshop: Personalization: Methods and Applications,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A label propagation method using spatial-spectral consistency for hyperspectral image classification", "author": ["H. Li", "Y. Wang", "S. Xiang", "J. Duan", "F. Zhu", "C. Pan"], "venue": "International Journal of Remote Sensing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Prediction models for network-linked data", "author": ["T. Li", "E. Levina", "J. Zhu"], "venue": "CoRR, abs/1602.01192,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Constructing just-in-time adaptive interventions", "author": ["P. Liao", "A. Tewari", "S. Murphy"], "venue": "Phd Section Proposal,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A batch, off-policy, actor-critic algorithm for optimizing the average reward", "author": ["S.A. Murphy", "Y. Deng", "E.B. Laber", "H.R. Maei", "R.S. Sutton", "K. Witkiewitz"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "A text message-based intervention for weight loss: randomized controlled trial", "author": ["K. Patrick", "F. Raab", "M. Adams", "L. Dillon", "M. Zabinski", "C. Rock", "W. Griswold", "G. Norman"], "venue": "Journal of Medical Internet Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Reinforcement learning for context aware segmentation", "author": ["L. Wang", "R.D. Merrifield", "G. Yang"], "venue": "In Medical Image Computing and Computer-Assisted Intervention,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Robust hyperspectral unmixing with correntropy-based metric", "author": ["Y. Wang", "C. Pan", "S. Xiang", "F. Zhu"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Development and evaluation of a mobile intervention for heavy drinking and smoking among college studen", "author": ["K. Witkiewitz", "S. Desai", "S. Bowen", "B. Leigh", "M. Kirouac", "M. Larimer"], "venue": "Psychology of Addictive Behaviors,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery", "author": ["Z. Xu", "S. Wang", "F. Zhu", "J. Huang"], "venue": "In ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Deep correlational learning for survival prediction from multi-modality datay", "author": ["J. Yao", "X. Zhu", "F. Zhu", "J. Huang"], "venue": "In International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "10,000+ times accelerated robust subset selection (ARSS)", "author": ["F. Zhu", "B. Fan", "X. Zhu", "Y. Wang", "S. Xiang", "C. Pan"], "venue": "In Proc. Assoc. Adv. Artif. Intell. (AAAI),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Effective warm start for the online actor-critic reinforcement learning based mhealth intervention", "author": ["F. Zhu", "P. Liao"], "venue": "In The Multi-disciplinary Conference on Reinforcement Learning and Decision Making,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Cohesion-based online actor-critic reinforcement learning for mhealth intervention", "author": ["F. Zhu", "P. Liao", "X. Zhu", "Y. Yao", "J. Huang"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Effective spectral unmixing via robust representation and learning-based sparsity", "author": ["F. Zhu", "Y. Wang", "B. Fan", "G. Meng", "C. Pan"], "venue": "CoRR, abs/1409.0685,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Structured sparse method for hyperspectral unmixing", "author": ["F. Zhu", "Y. Wang", "S. Xiang", "B. Fan", "C. Pan"], "venue": "ISPRS Journal of Photogrammetry and Remote Sensing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Wsisa: Making survival prediction from whole slide histopathological images", "author": ["X. Zhu", "J. Yao", "F. Zhu", "J. Huang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "Based on that, the aim is to provide in-time interventions to device users according to their ongoing status and changing needs, helping users to lead healthier lives, such as reducing the alcohol abuse [5,19] and the obesity management [15].", "startOffset": 203, "endOffset": 209}, {"referenceID": 18, "context": "Based on that, the aim is to provide in-time interventions to device users according to their ongoing status and changing needs, helping users to lead healthier lives, such as reducing the alcohol abuse [5,19] and the obesity management [15].", "startOffset": 203, "endOffset": 209}, {"referenceID": 14, "context": "Based on that, the aim is to provide in-time interventions to device users according to their ongoing status and changing needs, helping users to lead healthier lives, such as reducing the alcohol abuse [5,19] and the obesity management [15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 9, "context": "It aims to learn the optimal decision rule to decide when, where and how to deliver interventions [10,13,14,24,23] to best serve users.", "startOffset": 98, "endOffset": 114}, {"referenceID": 12, "context": "It aims to learn the optimal decision rule to decide when, where and how to deliver interventions [10,13,14,24,23] to best serve users.", "startOffset": 98, "endOffset": 114}, {"referenceID": 13, "context": "It aims to learn the optimal decision rule to decide when, where and how to deliver interventions [10,13,14,24,23] to best serve users.", "startOffset": 98, "endOffset": 114}, {"referenceID": 23, "context": "It aims to learn the optimal decision rule to decide when, where and how to deliver interventions [10,13,14,24,23] to best serve users.", "startOffset": 98, "endOffset": 114}, {"referenceID": 22, "context": "It aims to learn the optimal decision rule to decide when, where and how to deliver interventions [10,13,14,24,23] to best serve users.", "startOffset": 98, "endOffset": 114}, {"referenceID": 12, "context": "Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL [13,14] assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL [10,9,23] assumes that all users are completely different: they share no information and run a separate RL for each user.", "startOffset": 138, "endOffset": 145}, {"referenceID": 13, "context": "Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL [13,14] assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL [10,9,23] assumes that all users are completely different: they share no information and run a separate RL for each user.", "startOffset": 138, "endOffset": 145}, {"referenceID": 9, "context": "Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL [13,14] assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL [10,9,23] assumes that all users are completely different: they share no information and run a separate RL for each user.", "startOffset": 312, "endOffset": 321}, {"referenceID": 8, "context": "Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL [13,14] assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL [10,9,23] assumes that all users are completely different: they share no information and run a separate RL for each user.", "startOffset": 312, "endOffset": 321}, {"referenceID": 22, "context": "Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL [13,14] assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL [10,9,23] assumes that all users are completely different: they share no information and run a separate RL for each user.", "startOffset": 312, "endOffset": 321}, {"referenceID": 25, "context": "A more realistic assumption lies between the above two extremes: a user may be similar to some, but not all, users and similar users tend to have similar behaviors [26,11].", "startOffset": 164, "endOffset": 171}, {"referenceID": 10, "context": "A more realistic assumption lies between the above two extremes: a user may be similar to some, but not all, users and similar users tend to have similar behaviors [26,11].", "startOffset": 164, "endOffset": 171}, {"referenceID": 3, "context": "It is in an actor-critic setting [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The Markov Decision Process (MDP) provides a mathematical tool to model the dynamic system [3,4,17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 3, "context": "The Markov Decision Process (MDP) provides a mathematical tool to model the dynamic system [3,4,17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 16, "context": "The Markov Decision Process (MDP) provides a mathematical tool to model the dynamic system [3,4,17].", "startOffset": 91, "endOffset": 99}, {"referenceID": 0, "context": "The state transition model P : S \u00d7A\u00d7S 7\u2192 [0, 1] indicates the probability of transiting from one state s to another s\u2032 under a given action a.", "startOffset": 41, "endOffset": 47}, {"referenceID": 3, "context": "It is defined as follows [4]:", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "The goal of various RL methods is to learn an optimal policy \u03c0\u2217 that maximizes the Q-value for all the state-action pairs [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "Such procedure is called the actor updating [4].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Instead, the linear approximation alleviates this problem by assuming that Q is in a low dimensional space [16]: Qw = w x (s, a) \u2248 Q where x (s, a) is a feature processing step that combines the information in the state and action.", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "same MDPs); they share all information and run an identical RL for all users [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "In this setting, the critic updating, with an aim of seeking for solutions to satisfy the Linear Bellman equation [4,3], is", "startOffset": 114, "endOffset": 119}, {"referenceID": 2, "context": "In this setting, the critic updating, with an aim of seeking for solutions to satisfy the Linear Bellman equation [4,3], is", "startOffset": 114, "endOffset": 119}, {"referenceID": 7, "context": "The least-square temporal difference for Q-value (LSTDQ) [8,7] provides a closed-form solver for (3) as follows", "startOffset": 57, "endOffset": 62}, {"referenceID": 6, "context": "The least-square temporal difference for Q-value (LSTDQ) [8,7] provides a closed-form solver for (3) as follows", "startOffset": 57, "endOffset": 62}, {"referenceID": 26, "context": "The state-of-the-art deep learning methods can be a great idea to deal with this problem [27,21,20].", "startOffset": 89, "endOffset": 99}, {"referenceID": 20, "context": "The state-of-the-art deep learning methods can be a great idea to deal with this problem [27,21,20].", "startOffset": 89, "endOffset": 99}, {"referenceID": 19, "context": "The state-of-the-art deep learning methods can be a great idea to deal with this problem [27,21,20].", "startOffset": 89, "endOffset": 99}, {"referenceID": 9, "context": "The second type of RL methods (Separ-RL), such as Lei\u2019s online contextual bandit for mHealth [10,9], assume that all users are completely heterogeneous.", "startOffset": 93, "endOffset": 99}, {"referenceID": 8, "context": "The second type of RL methods (Separ-RL), such as Lei\u2019s online contextual bandit for mHealth [10,9], assume that all users are completely heterogeneous.", "startOffset": 93, "endOffset": 99}, {"referenceID": 11, "context": "We observe that users in mHealth are generally similar with some (but not all) users in the sense that they may have some similar features, such as age, gender, race, religion, education level, income and other socioeconomic status [12].", "startOffset": 232, "endOffset": 236}, {"referenceID": 12, "context": "There are three RL methods for comparison: (a) the Pooled-RL that pools the data across all users and learn an identical policy [13,14] for all users; (b) the Separ-RL, which learns a separate RL policy for each user by only using his or her data [10]; (c) The group driven RL (Gr-RL) is the proposed method.", "startOffset": 128, "endOffset": 135}, {"referenceID": 13, "context": "There are three RL methods for comparison: (a) the Pooled-RL that pools the data across all users and learn an identical policy [13,14] for all users; (b) the Separ-RL, which learns a separate RL policy for each user by only using his or her data [10]; (c) The group driven RL (Gr-RL) is the proposed method.", "startOffset": 128, "endOffset": 135}, {"referenceID": 9, "context": "There are three RL methods for comparison: (a) the Pooled-RL that pools the data across all users and learn an identical policy [13,14] for all users; (b) the Separ-RL, which learns a separate RL policy for each user by only using his or her data [10]; (c) The group driven RL (Gr-RL) is the proposed method.", "startOffset": 247, "endOffset": 251}, {"referenceID": 1, "context": "It is a 42-day mHealth intervention that aims to increase the users\u2019 steps they take everyday by providing some positive suggestions, such as going for a walk after long sitting [2].", "startOffset": 178, "endOffset": 181}, {"referenceID": 13, "context": "In our study, there are two choices for a policy {0, 1}: a = 1 indicates sending the positive intervention, while a = 0 means no intervention [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "A trajectory of T tuples DT = {(si, ai, ri)}Ti=1 are collected from each user via the micro-randomized trial [14,10].", "startOffset": 109, "endOffset": 116}, {"referenceID": 9, "context": "A trajectory of T tuples DT = {(si, ai, ri)}Ti=1 are collected from each user via the micro-randomized trial [14,10].", "startOffset": 109, "endOffset": 116}, {"referenceID": 1, "context": ", a dataset from the mobile health study, called HeartSteps [2], to approximate the generative model.", "startOffset": 60, "endOffset": 63}, {"referenceID": 9, "context": "The value of \u03b3 specifies different RL methods: (a) \u03b3 = 0 means the contextual bandit [10], (b) 0 < \u03b3 < 1 indicates the discounted reward RL.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "In the experiments, the expectation of long run average reward (ElrAR) E [\u03b7\u03b8\u0302 ] is proposed to evaluate the quality of a learned policy \u03c0\u03b8\u0302 [13,14].", "startOffset": 140, "endOffset": 147}, {"referenceID": 13, "context": "In the experiments, the expectation of long run average reward (ElrAR) E [\u03b7\u03b8\u0302 ] is proposed to evaluate the quality of a learned policy \u03c0\u03b8\u0302 [13,14].", "startOffset": 140, "endOffset": 147}, {"referenceID": 13, "context": "Specifically, there are two steps to achieve the ElrAR [14]: (a) get the \u03b7 \u03c0\u03b8\u0302 for each user by averaging the rewards over the last 4, 000 elements in the long run trajectory with a total number of 5, 000 tuples; (a) ElrAR E [\u03b7\u03b8\u0302 ] is achieved by averaging over the \u03b7\u03b8\u0302 \u2019s of all users.", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "Such results demonstrate that our method doesn\u2019t require the true value of groups and is robust to the value of K [25,18,1,22,6].", "startOffset": 114, "endOffset": 128}, {"referenceID": 17, "context": "Such results demonstrate that our method doesn\u2019t require the true value of groups and is robust to the value of K [25,18,1,22,6].", "startOffset": 114, "endOffset": 128}, {"referenceID": 0, "context": "Such results demonstrate that our method doesn\u2019t require the true value of groups and is robust to the value of K [25,18,1,22,6].", "startOffset": 114, "endOffset": 128}, {"referenceID": 21, "context": "Such results demonstrate that our method doesn\u2019t require the true value of groups and is robust to the value of K [25,18,1,22,6].", "startOffset": 114, "endOffset": 128}, {"referenceID": 5, "context": "Such results demonstrate that our method doesn\u2019t require the true value of groups and is robust to the value of K [25,18,1,22,6].", "startOffset": 114, "endOffset": 128}], "year": 2017, "abstractText": "Due to the popularity of smartphones and wearable devices nowadays, mobile health (mHealth) technologies are promising to bring positive and wide impacts on people\u2019s health. State-of-the-art decisionmaking methods for mHealth rely on some ideal assumptions. Those methods either assume that the users are completely homogenous or completely heterogeneous. However, in reality, a user might be similar with some, but not all, users. In this paper, we propose a novel group-driven reinforcement learning method for the mHealth. We aim to understand how to share information among similar users to better convert the limited user information into sharper learned RL policies. Specifically, we employ the K-means clustering method to group users based on their trajectory information similarity and learn a shared RL policy for each group. Extensive experiment results have shown that our method can achieve clear gains over the state-of-the-art RL methods for mHealth.", "creator": "LaTeX with hyperref package"}}}