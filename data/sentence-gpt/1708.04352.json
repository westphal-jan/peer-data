{"id": "1708.04352", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2017", "title": "Benchmark Environments for Multitask Learning in Continuous Domains", "abstract": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly in terms of performance. For instance, for the Atari game suite, performance is a major hurdle in terms of performance in both the two single- and double-memory architectures. For the Atari game suite, it may seem that the performance in multitask learning is generally well defined, but the performance gap between the two single- and double-memory architectures appears small enough for the game suite to offer little benefit in comparison to the two single- and double-memory architectures, and thus no evidence for such an improvement.\n\n\n\nTo address some of the limitations of the dual- and double-memory architectures, we first considered the performance of two two memory configurations in a game suite based on the Atari game suite (1) and the second game suite (2). One of the problems with this characterization is the performance of multiple configurations. We have already investigated the performance of multiple performance configurations for a game suite based on the single- and double-memory architectures. In the single- and double-memory architectures, the performance of multiple configurations is significantly higher than for the single- and double-memory architectures. A third aspect of the task is the ability to generate a variety of data types for each particular performance. This is a challenge to assess the performance of multiple multi- and double-memory architectures in the game suite. The two multi- and double-memory architectures have differing performance profiles and the different types of data types differ from each other, and when performing multiple multi- and double-memory architectures, performance is a significant barrier for performance of multiple single- and double-memory architectures in the game suite. The performance of multiple multi- and double-memory architectures is also influenced by the performance of different components, and the performance of each component determines the performance of different components, and can be measured using multiple analysis techniques. In this paper, we examine the performance of multi- and double-memory architectures in the game suite by combining two data types in a multi- and double-memory architecture. We use the same set of three data types: the one for multitasking, the one for multitasking, the one for multitasking. Both sets of data", "histories": [["v1", "Mon, 14 Aug 2017 22:55:03 GMT  (560kb,D)", "http://arxiv.org/abs/1708.04352v1", "Accepted at Lifelong Learning: A Reinforcement Learning Approach Workshop @ ICML, Sydney, Australia, 2017"]], "COMMENTS": "Accepted at Lifelong Learning: A Reinforcement Learning Approach Workshop @ ICML, Sydney, Australia, 2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peter henderson", "wei-di chang", "florian shkurti", "johanna hansen", "david meger", "gregory dudek"], "accepted": false, "id": "1708.04352"}, "pdf": {"name": "1708.04352.pdf", "metadata": {"source": "META", "title": "Benchmark Environments for Multitask Learning in Continuous Domains", "authors": ["Peter Henderson", "Wei-Di Chang", "Florian Shkurti", "Johanna Hansen", "David Meger", "Gregory Dudek"], "emails": ["<peter.henderson@mail.mcgill.ca>.", "@ICML,"], "sections": [{"heading": "1. Introduction", "text": "Multitask learning involves training a single agent in a lifelong context across a series of tasks. When tasks share similar characteristics, there is potential for the learning agent to reuse information, potentially achieving greater performance or learning more rapidly on down-stream tasks than would be possible by learning each task from scratch.\nThe promise of multitask learning has been previously demonstrated in several contexts. It has been shown that transfer learning on multiple related tasks improves the ability of the agent to learn a larger variety of domains while using less training data overall (Thrun, 1996). This naturally renders the agent more applicable to real-world scenarios. Additionally, by training on multiple tasks, the agent can exploit common traits to gain efficiency and generalize to unseen tasks (Caruana, 1998; Murugesan et al., 2016; Finn et al., 2016).\n1McGill University, Montreal, Quebec, Canada. Correspondence to: Peter Henderson <peter.henderson@mail.mcgill.ca>.\nAccepted at Lifelong Learning: A Reinforcement Learning Approach Workshop @ICML, Sydney, Australia, 2017. Copyright 2017 by the author(s).\nFigure 1. Example environments: 2D navigation task, with several sample paths (top-left). Hopper with a wall (top-right). Walker2d with big (bottom-right) and small (bottom-left) feet.\nIn discrete domains, several works have investigated the transferring of playing knowledge acquired between various Atari games. It is intuitive that there should be some knowledge transfer between Atari games (Breakout and Pong are similar in catching a ball; DemonAttack, Carnaval, Assault, and AirRaid share a goal in shooting upwards to destroy enemies). However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved. Atari videogames therefore stand as a useful benchmark which can be used to induce further progress in multitask, transfer and lifelong learning in discrete domains. On the contrary, in continuous domains, each approach for continuous domain multitask learning has utilized a unique set of environments with little mutual overlap. As such, there is a need for such open-source standard benchmarks.\nThis problem is one that has been recognized by other research groups. Particularly, OpenAI lists the need for benchmark environments and investigation of multitask learning in continuous domains in a request for research1. As they mention, the current OpenAI Gym (Brockman et al., 2016) environments do not share enough characteristics to likely pose as effective multitask or lifelong learn-\n1See: https://openai.com/requests-for-research/ #multitask-rl-with-continuous-actions\nar X\niv :1\n70 8.\n04 35\n2v 1\n[ cs\n.A I]\n1 4\nA ug\n2 01\n7\ning benchmarks. The contribution of this work is a set of benchmark environments that are suitable to evaluate continuous domain multitask learning. Our environments are constructed using an expandable software framework built on top of OpenAI Gym. Here, we show over 50 new environment variations (spread among 12 broad groups of variation types) for challenging new continuous domain tasks. We verify the utility of these environments for evaluating multitask learning by reporting the performance of a wellknown reinforcement learning algorithm on our multitask benchmark environments as a simple baseline."}, {"heading": "2. Related Work", "text": "Several works investigate multitask or transfer learning with MuJoCo tasks. These tasks include: navigating around a wall (where a wall separates an agent from its goal); the OpenAI Gym Reacher environment with an added image state space of the environment; jumping over a wall using a model similar to the OpenAI HalfCheetah environment (Finn et al., 2016); varying the gravity of various standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same set of environments (Christiano et al., 2016); simulated grasping and stacking using a Jaco arm (Rusu et al., 2016); and several custom grasping and manipulation tasks to demonstrate learning invariant feature spaces (Gupta et al., 2017).\nOther works investigate using classical control systems and robotics simulations with a set of varied hyperparameters for each environment. These include: a simple mass spring damper task, cart-pole with continuous control; a threelink inverted pendulum with continuous control; a quadrotor control task (Ammar et al., 2014); a double-linked pendulum task; a modified cartpole balancing task which can transfer to physical system (Higuera et al., 2017)."}, {"heading": "3. Environments", "text": "In our initial release of the gym-extensions framework2, we include a number of modifications of the standard gym environments as well as novel continuous domains, and provide a framework which allows easy modification of environment characteristics.\n2Found at: https://github.com/Breakend/ gym-extensions/. Pull requests and issues are welcome. More details for each environment will be provided in the opensource repository as well as a place to upload new benchmark algorithm runs."}, {"heading": "3.1. Mujoco", "text": "We base our modified environments on the existing \u201crunning\u201d (Humanoid, Hopper, Half-Cheetah, and Walker2d) and \u201carm-based\u201d (Pusher and Striker) environments in OpenAI Gym. First, we provide a high-level overview of our modifications and suggested grouping, then we show the specific environment names in our benchmarking results."}, {"heading": "3.1.1. GRAVITY MODIFICATIONS", "text": "For the running agents, we provide ready environments with various scales of simulated earth-like gravity, ranging from one half to one and a half of the normal gravity level (\u22124.91 to \u221212.26m \u00b7 s\u22122 in increments of .25gearth). We propose that a successful multitask learning algorithm will extract the underlying walking action structure and reuse the applicable knowledge without forgetting how to walk in varying gravity conditions."}, {"heading": "3.1.2. WALL AND SENSOR ENVIRONMENTS", "text": "Inspired by the wall jumping experiment in (Finn et al., 2016), we build a set of similar environments by extending the OpenAI running tasks to use a multi-beam noiseless range sensor. We emit ray-beams from the torso of the runner for the measurements (with an arc of 90 degrees, 10 beams, a maximum sensing distance of 10 meters, and readouts normalized to a range of [0, 1]). We provide the usual running tasks with the sensor perception enabled (with no readings since there is no wall), and extra environments with a wall set in the path of the agent at a location drawn from a uniform distribution from 1.8 to 3.8 meters ahead of the agent\u2019s start location."}, {"heading": "3.1.3. MORPHOLOGY MODIFICATIONS", "text": "For the running agents, we provide environments which vary the morphology of a specific body part of the agent. The modifications made to each agent are seen in Table 2. We define \u201cBig\u201d bodyparts as scaling the mass and width of the limb by 1.25 and \u201cSmall\u201d bodyparts as being scaled by 0.75. We also group categories of limbs for environments with multiple appendages (i.e. humanoid torso includes the abdomen; humanoid thigh also includes the hips; all appendages encompass both the left/right or front/back simultaneously such that a modified thigh includes both thighs)."}, {"heading": "3.1.4. ROBOT ARM MODIFICATIONS", "text": "In the OpenAI Striker and Pusher tasks, a 7 DoF arm tries to hit a ball into a hole or push a peg to a goal position respectively. We extend these tasks to randomly move the goal position for the Pusher task, and randomly move the ball start position for the Striker task. As in the original\ntasks, we bound the varied goal or start state within some restricted uniform distribution as domain appropriate."}, {"heading": "3.1.5. HUMANOID MULTITASK", "text": "We provide a humanoid multitask environment which combines the rewards for standing up and running in the same environment. The reward scale for this task is rather large, but aligns with the HumanoidStandup-v1 environment from OpenAI Gym. Additionally we provide a version of each environment with a sensor readout as in Section 3.1.2. When no wall is used, all sensors read zero. When a wall is used, each returns a distance to the wall as previously described."}, {"heading": "3.2. 2D Navigation", "text": "We also provide several novel 2D environments that focus on navigation tasks with continuous action spaces to enable benchmarking of learning tasks requiring an implicit memory. The tasks take place in a given occupancy grid map, similar to (Tamar et al., 2016). We opt to make the layout and shape of the obstacles as the only disambiguating feature for localizing within the map. Aside from that information, the environment does not have any texture mapping or other distinctive features.\nWe provide three different types of navigation tasks, increasing in level of difficulty:\n\u2022 Image-based navigation where the agent has access to the entire map, including its own position within the map and the destination in the map as part of the image data.\n\u2022 State-based navigation, where the agent has access to its own position in the map and the distance and bearing to the closest obstacle. A simpler version also contains the destination coordinates.\n\u2022 Navigation based only on local range-and-bearing data around the agent using ray-tracing. It has to perform mapping and estimate its own position within the map (i.e. perform SLAM), while at the same time exploring to find the goal location, and learning to avoid obstacles. We also modify this with a simpler version, where the goal and current position are known as well.\nWe provide a reward of -1 for every timestep, -5 for obstacle collisions, and +10 for reaching the goal (which also ends the task, similarly to the MountainCar-v0 environment in OpenAI Gym). The action space is the bounded velocity to apply in the x and y directions."}, {"heading": "4. Multitask Sets", "text": "We develop several sets of intuitive task groups which can serve as simple benchmarks which increase in complexity both within the group and in our listing order. The specific environment names can be found in Table 1, 2, 3, and 4. For the navigation tasks, we list the environments inline here.\nWe introduce the following environment groups:\n\u2022 Modified environments with different gravity parameters6\n\u2022 Modified environments with sensor readouts (simply reading zero if no wall) and permuted with a random wall in the runner path\n\u2022 The OpenAI Gym Striker environment with both random start position of the object as well as random goal state\n\u2022 The OpenAI Gym Pusher environment with both random start position of the object as well as random goal state\n\u2022 Learning to standup and run for a Humanoid model \u2022 Learning to standup, run, and jump over walls for a Hu-\nmanoid model\n\u2022 Learning to run with different sized limbs with the base set of limbs encompassing {Torso, Leg, Thigh, Foot} and specific extra limbs listed below (i.e. example combinations look like: HumanoidBigArm-v0, HopperSmallFoot-v0).\n\u2022 Learning to navigate and search in 2D environments using only current position and distance to closest obstacles (State-Based-Navigation-2d-Map{0-9}-Goal{0-2}-v0)\n\u2022 Learning to navigate and search in 2D environments observing current position, distance to closest obstacles, and known goal position (State-Based-Navigation-2d-Map{09}-Goal{0-2}-KnownGoalPosition-v0)\n\u2022 Learning to navigate and search in 2D environments observing only raytracing distance readouts (Limited-RangeBased-Navigation-2d-Map{0-9}-Goal{0-2}-v0)\n\u2022 Learning to navigate and search in 2D environments observing current position, raytracing distance readouts, and known goal position (Limited-Range-Based-Navigation-2dMap{0-9}-Goal{0-2}-KnownPositions-v0)\n\u2022 Learning to navigate and search in 2D environments observing only the 2D map image with goal location and current position highlighted in different colors (Image-BasedNavigation-2d-Map{0-9}-Goal{0-2}-v0)"}, {"heading": "5. Baseline Experiments", "text": "We develop a basic experiment to run on the aforementioned groupings of the environments to demonstrate learning on a series of multiple similar tasks consecutively. We then evaluate the generalized performance across all of the environments using the final learned policy. For an initial baseline, we simply run the RLLab (Duan et al., 2016) implementation of Trust Region Policy Optimization (Schulman et al., 2015) (TRPO) using an identical policy network to (Gu et al., 2016)7. We train the same policy consecutively on each environment in a group in the same order as listed in Section 4. After having trained on a specific environment, we evaluate the current policy on that environ-\n6 {BaseRunningEnv} denotes one of the OpenAI Gym environments from: Humanoid, Hopper, Walker2d, HalfCheetah with {GravityVariation} from {Half, ThreeQuarters, OneAndQuarter, OneAndHalf}.\n7Size 100, 50, 25 hidden layers with rectified linear activations and a tanh output activation, and hyperparameters: step-size, 0.01; GAE lambda, 1.0; regularization coefficient, 1.0 \u00b7 10\u22125; number of epochs, 1000; batch size, 50000\nment by running 20 sample rollouts. We then train on the next environment in the group, starting from that same policy. We finally evaluate the reward across 20 sample rollouts on each environment in a group using the final learned policy (which by then has been trained on every variation of the environment). While this isn\u2019t an explicit multitask learning approach, this provides basic insights (using a well-known reinforcement learning algorithm) into forward transfer and generalization of a policy on these task groupings.\nOur baseline experiment results are found in Tables 1, 2, 3, 4. In the case of modified Hopper tasks, modifying gravity and body part size has a profound effect on the system dynamics. As a result, we see that catastrophic forgetting (McCloskey & Cohen, 1989) in the policy prevents generalization to earlier tasks. This can be seen in Tables 1 and 2. First, when evaluating the final policy on all of the previously trained environments (the \u201cFully Trained\u201d column), performance decreases monotonically as we move backwards over the environments. Additionally, immediately after training on the earlier environments (the \u201cAfter Env Training\u201d column), the performance on the sample rollouts is much higher than that of the final policy (which has seen all the environments). This indicates that this group of environments are good indicators for demonstrating and overcoming catastrophic forgetting in multitask learning.\nIn other environment variations (modified HalfCheetah and Walker2d environments), the agent\u2019s final policy outperforms both training from scratch (as in Table 1) and the \u2018After Env Training\u201d result (as in Tables 1 and 2), which is evidence of positive forward transfer. The dynamics of these environment are not significantly perturbed by changes in physics, as the models have inherent stability. There remains significant room for future improvement upon our baseline. Future methods may achieve more efficient forward transfer between sequential environments. Furthermore, generalization across multiple tasks may come at a cost of higher variance in the policy (e.g. in Walker2d environments here). Future improvements may also focus on generalization with constrained variance across trials (and thus higher safety when learning on new environments).\nFor the Humanoid-exclusive variations and Wall variations (as in Tables 2 and 3), TRPO is not able to learn a policy which can jump over a wall or learn a good policy on Humanoid tasks in the small number of iterations which we ran (1000 iterations). The results we see are on a comparable scale to (Duan et al., 2016).\nIn our new map navigation tasks, rewards remain at -1000, which is the initial lowest reward. That is, the agent never learns to find the goal using our default parameters and TRPO. This is to be expected as TRPO may not be suited for such a navigation task which requires large amounts of\nexploration with an extremely delayed reward. Other methods which encourage principled exploration and have a memory component to the policy may be more suitable for such tasks. We nevertheless share these environments with the community in an effort to drive investigation into creating complex policies for simultaneous localization, exploration and goal searching in settings where goals and obstacles vary between tasks."}, {"heading": "6. Conclusion", "text": "Our initial release investigates adding flexibility to standard OpenAI gym MuJoCo environments: modifying gravity, adding sensor readouts and a random wall obstacle, perturbing body-part sizes, and adding random goal/start state positions for arm environments. We also add an original set of environments for learning policies in continuous navigation tasks. In future releases we also plan to add standard environments for: adding motor noise, arm environments where the end-goal position has a velocity (such that the arm must track the target), and making the sensor-based environments more realistic (and thus more transferable to real-world systems)."}], "references": [{"title": "Online multi-task learning for policy gradient methods", "author": ["Ammar", "Haitham B", "Eaton", "Eric", "Ruvolo", "Paul", "Taylor", "Matthew"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Multitask learning. In Learning to learn, pp. 95\u2013133", "author": ["Caruana", "Rich"], "venue": null, "citeRegEx": "Caruana and Rich.,? \\Q1998\\E", "shortCiteRegEx": "Caruana and Rich.", "year": 1998}, {"title": "Transfer from simulation to real world through learning deep inverse dynamics model", "author": ["Christiano", "Paul", "Shah", "Zain", "Mordatch", "Igor", "Schneider", "Jonas", "Blackwell", "Trevor", "Tobin", "Joshua", "Abbeel", "Pieter", "Zaremba", "Wojciech"], "venue": "arXiv preprint arXiv:1610.03518,", "citeRegEx": "Christiano et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2016}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Generalizing skills with semisupervised reinforcement learning", "author": ["Finn", "Chelsea", "Yu", "Tianhe", "Fu", "Justin", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1612.00429,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Q-prop: Sample-efficient policy gradient with an off-policy critic", "author": ["Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Gupta", "Abhishek", "Devin", "Coline", "Liu", "YuXuan", "Abbeel", "Pieter", "Levine", "Sergey"], "venue": "arXiv preprint arXiv:1703.02949,", "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Adapting learned robotics behaviours through policy adjustment", "author": ["Higuera", "Juan Camilo Gamboa", "Meger", "David", "Dudek", "Gregory"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Higuera et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Higuera et al\\.", "year": 2017}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Catastrophic interference in connectionist networks: The sequential learning problem", "author": ["McCloskey", "Michael", "Cohen", "Neal J"], "venue": "Psychology of learning and motivation,", "citeRegEx": "McCloskey et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCloskey et al\\.", "year": 1989}, {"title": "Adaptive smoothed online multi-task learning", "author": ["Murugesan", "Keerthiram", "Liu", "Hanxiao", "Carbonell", "Jaime", "Yang", "Yiming"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Murugesan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Murugesan et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1511.06342,", "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Policy distillation", "author": ["Rusu", "Andrei A", "Colmenarejo", "Sergio Gomez", "Gulcehre", "Caglar", "Desjardins", "Guillaume", "Kirkpatrick", "James", "Pascanu", "Razvan", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Sim-toreal robot learning from pixels with progressive nets", "author": ["Rusu", "Andrei A", "Vecerik", "Matej", "Roth\u00f6rl", "Thomas", "Heess", "Nicolas", "Pascanu", "Razvan", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "Rusu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["Thrun", "Sebastian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Thrun and Sebastian.,? \\Q1996\\E", "shortCiteRegEx": "Thrun and Sebastian.", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "Additionally, by training on multiple tasks, the agent can exploit common traits to gain efficiency and generalize to unseen tasks (Caruana, 1998; Murugesan et al., 2016; Finn et al., 2016).", "startOffset": 131, "endOffset": 189}, {"referenceID": 4, "context": "Additionally, by training on multiple tasks, the agent can exploit common traits to gain efficiency and generalize to unseen tasks (Caruana, 1998; Murugesan et al., 2016; Finn et al., 2016).", "startOffset": 131, "endOffset": 189}, {"referenceID": 11, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 12, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 8, "context": "However, despite several authors\u2019 attempts to demonstrate multitask learning in both Atari tasks and the DeepMind Lab Labyrinth (Parisotto et al., 2015; Rusu et al., 2015; Jaderberg et al., 2016), this task that has yet to be convincingly solved.", "startOffset": 128, "endOffset": 195}, {"referenceID": 4, "context": "These tasks include: navigating around a wall (where a wall separates an agent from its goal); the OpenAI Gym Reacher environment with an added image state space of the environment; jumping over a wall using a model similar to the OpenAI HalfCheetah environment (Finn et al., 2016); varying the gravity of various standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same set of environments (Christiano et al.", "startOffset": 262, "endOffset": 281}, {"referenceID": 2, "context": ", 2016); varying the gravity of various standard OpenAI Gym benchmark environments (Reacher, Hopper, Humanoid, HalfCheetah) and transferring between the modified environments; adding motor noise to the same set of environments (Christiano et al., 2016); simulated grasping and stacking using a Jaco arm (Rusu et al.", "startOffset": 227, "endOffset": 252}, {"referenceID": 13, "context": ", 2016); simulated grasping and stacking using a Jaco arm (Rusu et al., 2016); and several custom grasping and manipulation tasks to demonstrate learning invariant feature spaces (Gupta et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": ", 2016); and several custom grasping and manipulation tasks to demonstrate learning invariant feature spaces (Gupta et al., 2017).", "startOffset": 109, "endOffset": 129}, {"referenceID": 0, "context": "These include: a simple mass spring damper task, cart-pole with continuous control; a threelink inverted pendulum with continuous control; a quadrotor control task (Ammar et al., 2014); a double-linked pendulum task; a modified cartpole balancing task which can transfer to physical system (Higuera et al.", "startOffset": 164, "endOffset": 184}, {"referenceID": 7, "context": ", 2014); a double-linked pendulum task; a modified cartpole balancing task which can transfer to physical system (Higuera et al., 2017).", "startOffset": 113, "endOffset": 135}, {"referenceID": 4, "context": "Inspired by the wall jumping experiment in (Finn et al., 2016), we build a set of similar environments by extending the OpenAI running tasks to use a multi-beam noiseless range sensor.", "startOffset": 43, "endOffset": 62}, {"referenceID": 3, "context": "For an initial baseline, we simply run the RLLab (Duan et al., 2016) implementation of Trust Region Policy Optimization (Schulman et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 14, "context": ", 2016) implementation of Trust Region Policy Optimization (Schulman et al., 2015) (TRPO) using an identical policy network to (Gu et al.", "startOffset": 59, "endOffset": 82}, {"referenceID": 5, "context": ", 2015) (TRPO) using an identical policy network to (Gu et al., 2016)7.", "startOffset": 52, "endOffset": 69}, {"referenceID": 3, "context": "The results we see are on a comparable scale to (Duan et al., 2016).", "startOffset": 48, "endOffset": 67}], "year": 2017, "abstractText": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.", "creator": "LaTeX with hyperref package"}}}