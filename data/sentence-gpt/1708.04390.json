{"id": "1708.04390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Fluency-Guided Cross-Lingual Image Captioning", "abstract": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual setting. Different from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of fluency in the translated sentences, we propose in this paper a fluency-guided learning framework. The framework comprises a module to automatically estimate the fluency of the sentences and another module to utilize the estimated fluency scores to effectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both fluency and relevance of the generated captions in Chinese, but without using any manually written sentences from the target language. Importantly, this approach provides a clear and robust framework for interpreting the captions of the text.\n\n\n\n\n\nThe dataset was designed to produce a clear and coherent response for the fluency of the sentences. There were a number of constraints that caused the sample to differ significantly between English and Mandarin. The most important constraint was the amount of time each sentence had to be repeated or repeated. The following conditions were met for the fluency of the sentences:\n1. Time to reproduce the captions of the text\n2. The task of transcribing captions of the text\n3. The duration and duration of a sentence in the captions\n4. The time to repeat the captions of the text\n5. The time to repeat the captions of the text\n6. The time to repeat the captions of the text\n7. The duration of a sentence in the captions\n8. The time to repeat the captions of the text\n9. The time to repeat the captions of the text\n10. The duration of a sentence in the captions\n11. The time to repeat the captions of the text\n12. The time to repeat the captions of the text\n13. The time to repeat the captions of the text\n14. The time to repeat the captions of the text\n15. The time to repeat the captions of the text\n16. The time to repeat the captions of the text\n17. The time to repeat the captions of the text\n18. The time to repeat the", "histories": [["v1", "Tue, 15 Aug 2017 03:46:31 GMT  (1014kb,D)", "http://arxiv.org/abs/1708.04390v1", "9 pages, 2 figures, accepted as ORAL by ACM Multimedia 2017"]], "COMMENTS": "9 pages, 2 figures, accepted as ORAL by ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["weiyu lan", "xirong li", "jianfeng dong"], "accepted": false, "id": "1708.04390"}, "pdf": {"name": "1708.04390.pdf", "metadata": {"source": "META", "title": "Fluency-Guided Cross-Lingual Image Captioning", "authors": ["Weiyu Lan", "Xirong Li", "Jianfeng Dong"], "emails": ["(xirong@ruc.edu.cn).", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Computing methodologies \u2192 Scene understanding; Natural language generation;\nKEYWORDS Cross-lingual image captioning, English-Chinese, Sentence uency ACM Reference format: Weiyu Lan, Xirong Li, and Jianfeng Dong. 2017. Fluency-Guided CrossLingual Image Captioning. In Proceedings of MM \u201917, October 23\u201327, 2017, Mountain View, CA, USA., , 9 pages. DOI: h ps://doi.org/10.1145/3123266.3123366"}, {"heading": "1 INTRODUCTION", "text": "Given a picture, human can give a concise description in the form of a well-organized sentence, identifying salient objects in the image and their relationship with the surrounding. But for computers, image captioning is a challenging task. Not only does the computer need to capture concise concepts in the picture, but it also has to learn a language model that generates proper sentences. Aided by advances in training deep neural networks and large datasets\n\u2217Corresponding author (xirong@ruc.edu.cn).\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. MM \u201917, October 23\u201327, 2017, Mountain View, CA, USA. \u00a9 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4906-2/17/10. . .$15.00 DOI: h ps://doi.org/10.1145/3123266.3123366\nthat associate images with text, recent works have signi cantly improved the quality of caption generation [20, 24, 27, 36, 37].\nWith few exceptions, the task of image caption generation has so far been explored only in English since most available datasets are in this language. e application of image captioning, however, should not be restricted by language. e study of cross-lingual image captioning is essential for a large population on the planet who cannot speak English. In this paper, we study cross-lingual image captioning that aims to generate captions in another language, as exempli ed in Figure 1. We target at Chinese, which is the most spoken language on the earth yet undeveloped in the image captioning research.\nOnly few studies have been conducted for image captioning in a cross-lingual se ing [8, 23, 29]. ey tackle this problem by constructing a new dataset in the target language. Such an approach is constrained by the availability of manual annotation, and thus di cult to scale up and cover other languages. Instead of building a large dataset in a new language manually, we target at learning from machine-translated text.\nWhile the use of web-scale data has substantially improved machine translation quality [1, 40, 44], we observe that the uency of machine-translated Chinese sentences is o en unsatisfactory. Fluency here means \u201cthe extent to which each sentence reads naturally\u201d [17]. For instance, the sentence \u2018A couple sit on the grass with a baby and stroller\u2019 is translated to \u2018\u4e00\u5bf9\u592b\u5987\u5750\u5728\u5a74\u513f\u63a8\u8f66\u7684\u8349\u2019 by Baidu translation, which is among the best English-to-Chinese translation systems. e keywords in the sentence are basically correctly translated, but the inappropriate conjunction of sentence elements makes the translated sentence not uent. It tends to get even worse as English sentences becomes longer. Due to the lack of uency, directly learning a cross-lingual image captioning model from machine-translated text is problematic. For the same reason,\nar X\niv :1\n70 8.\n04 39\n0v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 7\ndirectly translating the output of an English captioning model is questionable also. Moreover, the generated English captions are not always relevant to the image content, and the irrelevant part can be exaggerated via translation.\nTo conquer the obstacle of exploiting machine-translated text, we propose in this paper uency-guided learning. Instead of revising the translated sentences to make them more uent, which remains open in machine translation, we introduce a neural classi er to automatically estimate the uency of these sentences. is provides an e ective means to measure the importance of the sentences for training. For instance, sentences with lower uency scores tend to be excluded from training or have a reduced e ect on the captioning model. We make the intuition concrete by introducing three uencyguided learning strategies. Automated and human evaluations on two datasets show the viability of the proposed framework. Code and data are available at h ps://github.com/weiyuk/ uent-cap.\ne rest of the paper is organized as follows. We review recent progress on image captioning in Section 2. We then propose our strategies in Section 3. A quantitative evaluation is given in Section 4, with major ndings reported in Section 5."}, {"heading": "2 PROGRESS ON IMAGE CAPTIONING", "text": "Monolingual image captioning. Our approach is developed on the top of monolingual image captioning. So we will rst review recent progress in this direction. ree leading approaches have been explored [2]. e rst formulates image captioning as a retrieval problem. Hodosh et al. [16] propose to exploit similarity in the visual space to transfer candidate training descriptions to a query image. Some other works, e.g., [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data. Following the progress in object detecting, detection based approaches [9, 10] generate descriptions using templates or grammar rules or language models based on the detected a ributes of the objects in the image. Farhadi et al. [10], for instance, ll a xed template by an inferred triplet of scene elements. More recently, Fang et al. [9] uses a deep convolutional neural network (CNN) to predict a number of words that are likely to be present in a caption and generates description by a maximum-entropy language model. is approach constrains the diversity of generated descriptions as it relies on a prede ned set of words or semantic concepts of objects, a ributes and actions.\ne recent dominant line in image captioning, inspired by the success of deep learning in image classi cation and sequence generation, is to apply deep neural networks which typically contain a CNN and an RNN to automatically generate new captions for images. In [13, 20, 36] , a CNN pretrained on the ImageNet classication task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu ing a sequence of words as the caption. Xu et al. [41] introduced an a ention mechanism that incorporates visual context during sentence generation. More recently, using scene information [24] and high-level concepts / a ributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model. Some new architectures are continuous developed. For instance, Wang et al. [37] propose a deeper bidirectional variant of\nLong Short Term Memory (LSTM) to take both history and future context into account in image captioning. A concept and syntax transition network [19] is presented to deal with large real-world captioning datasets such as YFCC100M [34]. Furthermore, in [31], reinforcement learning is also utilized to train the CNN-RNN based model directly on test metrics of the captioning task, showing significant gains in performance. We take a direction orthogonal to these works, aiming to exploit an existing model in the new cross-lingual context. Hence, our work naturally bene ts from the continuous progress in monolingual image captioning.\nCross-lingual image captioning. Comparing to the large amount of interests in studying how to generate English captions, few studies have been conducted on cross-lingual image captioning. Ellio et al. [8] address this topic as a translation problem, generating a description in the target language for a given image with a strong assumption that source-language descriptions are already provided for the image. To train a Japanese captioning model, Miyazaki and Shimizu [29] use crowd sourcing to collect Japanese descriptions of the MSCOCO training set [26]. Di erent from the above works that require image descriptions manually wri en in the target language, our approach trains a cross-lingual image captioning model on machine-translated text. Li et al. [23] have made a rst a empt in this direction. However, they use the translated text as it is, directly training a Chinese captioning model using machine-translated sentences from the Flickr8k dataset [16]. As such, their model tends to generate Chinese captions with illformed structures and thus bad user experience as exempli ed in Fig. 1. e uency problem is completely untouched in their model training and evaluation."}, {"heading": "3 OUR APPROACH", "text": "Our goal is to build an image captioning model for a target language, but without the need of any manually wri en captions in that language for training. is is achieved by a novel cross-lingual use of training corpus from a source language. Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se ing. Let {Se } be English sentences describing a given set of training images. Performing machine translation on these sentences allows us to automatically obtain their Chinese counterparts {Sc }. As we have noted, the main challenge in learning an image captioning model from {Sc } is that many of the machine translated sentences lack uency. To conquer the challenge, the uency of the training sentences needs to be taken into account. To this end, we proposed a uency-guided learning framework, as illustrated in Fig. 2. We introduce Sentence Fluency Estimation as an automated measure of the uency of each translated sentence. We then exploit the estimated uency to guide the learning process to emphasize be er translated sentences. Individual components of the proposed framework are detailed as follows."}, {"heading": "3.1 Sentence Fluency Estimation", "text": "It is worth noting that we do not intend to revise {Sc } to make them more uent, as this remains an open problem in machine translation [4, 33]. Rather, we aim to automatically measure their\nuency so that we might discard sentences that are deemed to be not uent or minimize their e ect during the training process. As a given sentence can be either uent or not uent, we approach the problem of sentence uency estimation by binary classi cation.\nIn order to construct a classi er for sentence uency estimation, we need to encode sentences of varied length into xed-size feature vectors, and build a speci c classi er on the top of the features. LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22]. We therefore develop an LSTM based classi er, using the LSTM module for sentence encoding followed by a fully connected layer for classi cation. Suppose we have access to a set of labeled sentences D = {Sc ,y} where y = 1 indicates the translated sentence is uent and y = 0 otherwise. Unlike western languages, many east Asian languages including Chinese are wri en without explicit word delimiters. erefore, word segmentation is performed to tokenize a given sentence to a sequence of Chinese words. We employ BOSON [28], a cloud based platform providing rich Chinese natural\nlanguage processing service. Given Sc as a sequence of n words (w1,w2, . . . ,wn ), we feed the embedding vector of each word into the LSTM module sequentially, using the hidden state vector at the last time step as the feature vector h(Sc ). e vector then goes through the classi cation module, yielding two outputs f (Sc ) and f\u0302 (Sc ) indicating the probability of the sentence being uent and not uent, respectively. More formally, we have\n(f (Sc ), f\u0302 (Sc )) = so max(W \u00b7 h(Sc ) + b), (1) whereW is a ne transformation matrix and b is a bias term. We optimize the encoding module and the classi cation module jointly, representing all the parameters by \u0398 = [We ,W ,b,\u03d5], where We is the word embedding matrix, and \u03d5 parameterizes a ne transformations inside LSTM. We train the classi er by minimizing the cross-entropy loss:\nargmin \u0398 \u2211 (Sc ,y)\u2208D \u2212 ( y \u00b7 log(f (Sc )) + (1 \u2212 y) \u00b7 log( f\u0302 (Sc )) ) . (2)\nAs the Chinese sentences are generated by machine translation, a not uent Sc means the corresponding Se is di cult to be\ntranslated. Hence, the original English sentences might be another clue for sentence uency estimation. Moreover, as part of speech (POS) tags of a Chinese / English sentence re ects to some extent grammatical structures of the sentence, they might be helpful for uency estimation as well. In that regard, we train three more LSTM based classi ers, denoted as f (Se ), f (Sc,pos ), and f (Se,pos ), which respectively takes a sequence of English words, a sequence of Chinese POS tags and a sequence of English POS tags as input. As a consequence, we obtain a four-way LSTM based classi er, which predicts the uency of a translated sentence by combining the prediction of the four individual classi ers, i.e.,\nf (Sc ) \u2190 1 4 (f (Sc ) + f (Sc,pos ) + f (Se ) + f (Se,pos )). (3)\nA translated Chinese sentence is classi ed as uent if f (Sc ) > 0.5. Notice that the correspondence between the English and the translated Chinese sentences allows us to use the same labels from D to train all the classi ers.\nWe solve Eq. (2) using stochastic gradient descent with Adam [21] on batches of size 64. We empirically set the initial learning rate \u03b7 = 0.0001, decay weights \u03b21 = 0.9, \u03b22 = 0.9 and small constant \u03f5 = 10\u22126 for Adam. We apply dropout to output of the word embedding layer and LSTM to mitigate model over ing. e size of the word embeddings and the size of LSTM are both set to be 512. We employ BOSON and a Stanford parser [5] to acquire Chinese and English POS tags, respectively."}, {"heading": "3.2 Model for Image Captioning", "text": "For the Chinese caption generation model, we follow a popular CNN + LSTM approach developed by Vinyals et al. [36]. More formally, for a given image I , we aim to automatically predict a Chinese sentence S = (w1,w2, ...,wn ) that describes in brief the visual content of the image. A probabilistic model is used to estimate the posterior probability of a speci c sequence of words given the image. Given\u03b8 as the model parameters, the probability is expressed as p(S |I ;\u03b8 ). Applying the chain rule together with log probability for the ease of computation, we have\nlogp(S |I ;\u03b8 ) = n+1\u2211 t=1 logp(wt |I ,w0, . . . ,wt\u22121;\u03b8 ), (4)\nwhere w0 = wn+1 = START/END is a special token indicating the beginning or the end of the sentence. Consequently, the image will be annotated with the sentence that yields the maximal posterior probability.\nConditional probabilities in Eq. (4) are estimated by the LSTM network in an iterative manner. e LSTM network maintains a cell vector c and a hidden state vector h to adaptively memorize the information fed to it. As shown in Fig. 2, the recurrent connections of LSTM carry on previous context. In the training stage, pairs of image and translated Chinese sentence are fed to the model. At the very beginning, the embedding vector of an image, x\u22121, obtained by applying an a ne transformation on its visual representation CNN (I ), is fed to the network to initialize the two memory vectors. e word sequence (w0, . . . ,wn ), a er applying a linear transformation on the word embedding vectors, is iteratively fed to the LSTM. In the t-th iteration, new probabilities pt over all the candidate words are re-estimated given the current context. To express\nthe above process in a more formal way, we write\nx\u22121 :=Wv \u00b7CNN (I ), (5) xt :=Ws \u00b7wt , t = 0, 1, . . . , (6)\np0, c0,h0 \u2190 LSTM(x\u22121, 0, 0), (7) pt+1, ct+1,ht+1 \u2190 LSTM(xt , ct ,ht ). (8)\ne parameter set \u03b8 consists ofWv ,Ws , and parameters w.r.t. a ne transformations inside LSTM.\ne loss is the sum of the negative log likelihoods of the next correct word at each step. We use SGD with mini-batches of m image-sentence pairs. Given training samples {(Ii , Si )|i = 1, ...,m} in a batch, the loss is formulated as follows:\nbLoss = \u2212 1 m m\u2211 i=1 logp(Si |Ii ;\u03b8 )\nIn the inference period, a er feeding the image embedding vector, the so max layer a er the LSTM produces a probability distribution over all words. e word with the maximum probability is picked up, and fed to LSTM in the next iteration. Following [20, 36], per iteration we apply beam search to maintain the k best candidate sentences, with a beam of size 5. e iteration stops once a special END token is selected.\nTo extract image representations, we use a pre-trained ResNet152 [14] which achieved state-of-the-art results for image classication and detection in both ImageNet and COCO competitions. e image feature is extracted as a 2048-dimensional vector from the pool5 layer a er ReLU. We conduct l2 normalization on the extracted features since it leads to be er results according to our preliminary experiments. e dimension of image and word embeddings, and the hidden size of LSTM are all set to be 512. We replace words that occurring less than ve times in the training set with a special \u2018UNK\u2019 token. We set the initial learning rate \u03b7 = 0.001, decaying every ten epochs with a decay weight of 0.999."}, {"heading": "3.3 Fluency-Guided Training", "text": "Having the sentence uency classi er and the image captioning model introduced, we are now ready to discuss how to guide the training process in light of the estimated uency and consequently generate be er-formed Chinese captions. While the question is new, if we view uency as a measure of the importance of the individual training samples, we see some conceptual resemblance to a machine learning scenario where some samples are more important than others. A typical case is learning from a data set with highly unbalanced classes, where one might consider downsampling classes in majority, over-sampling classes in minority or re-weighting samples [7, 38]. In our context, uent sentences are in short supply relatively. Inspired by such a connection, we propose three strategies for uency-guided training,\nStrategy I: Fluency only. is strategy preserves only sentences classi ed as uent for training the captioning model. Models derived from such cleaned dataset tend to generate more uent captions. Nonetheless, this bene t is obtained at the risk of learning from insu cient data. As aforementioned, translated sentences with low uency can still contain correct keywords which can\nprovide connections between the visual representation and the language model. To overcome the downside of the rst strategy, the following two strategies are introduced.\nStrategy II: Rejection sampling. We introduce a samplingbased strategy that allows the sentences classi ed as not uent to be used for training with a certain chance, besides preserving all sentences classi ed as uent. Naturally this chance shall be proportional to the sentences\u2019 probability of being uent. As f (Sc ) is a classi er output, directly sampling w.r.t. f (Sc ) is hard. We thus leverage rejection sampling, a type of Monte Carlo method developed for handling such di culties. For a sentence having f (Sc ) < 0.5, a number u is randomly drawn from the uniform distribution U (0, 0.5). e sentence will be included in the current mini-batch if f (Sc ) > u, and rejected otherwise.\nStrategy III: Weighted loss. is strategy makes full use of the translated sentences by cost-sensitive learning [7]. In particular, we multiply the uency score f (Si ) to a training sample\u2019s loss as a penalty weight when calculating the loss in every mini-batch. In particular, the weighted loss for a mini-batch is computed as\nbLosswei\u0434hted = \u2212 1 m m\u2211 i=1 \u00b5i \u00b7 logp(Si |Ii ;\u03b8 ), (9)\nwhere \u00b5i = 1 if f (Sc ) > 0.5, i.e., classi ed as uent, otherwise \u00b5i = f (Sc ).\nIn what follows we will evaluate the viability of the three uencyguided training strategies."}, {"heading": "4 EXPERIMENTS", "text": "e main purpose of our experiments is to verify if a cross-lingual captioning model trained by uency-guided learning can generate Chinese captions that are more uent, meanwhile maintaining the level of relevance when compared to learning from the complete set of machine-translated sentences. We term this baseline as \u2018Without uency\u2019. As sentence uency estimation is a prerequisite for uency-guided learning, we rst evaluate this component."}, {"heading": "4.1 Sentence Fluency Estimation", "text": "Setup. In order to train the four-way sentence uency classi er, a number of paired bilingual sentences labeled as uent / not uent are a prerequisite. We aim to select a representative and diverse set of sentences for manual veri cation, meanwhile keeping the manual annotation a ordable. To this end we sample at random 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] respectively. e 8k sentences were automatically translated into the same amount of Chinese sentences by the Baidu translation API. Manual veri cation was performed by eight students (all native Chinese speakers) in our lab. In particular, each Chinese sentence was separately presented to two annotators, asking them to grade the sentence as uent, not uent, or di cult to tell. A sentence is considered uent if it does not contain obvious grammatical errors and is in line with language habits of Chinese. Sentences receiving inconsistent grades or graded as di cult to tell were ignored. is resulted in 6,593 labeled sentences in total. ey are then randomly split into three folds, i.e., 4,593 / 1,000 / 1,000 for training / validation / test, as summarized in Table 1. e fact that less than 30% of the translated sentences are considered uent indicates much room for\nfurther improvement for the current machine translation system. It also shows the necessity of uency-guided learning when deriving cross-lingual image captioning models from machine-translated corpus.\nBaselines. In order to obtain a more comprehensive picture, we consider two baselines. One is random guess. e other is to predict uency in terms of sentence length. is is based on our observation that longer sentences are more di cult to be translated. In particular, a sentence, let it be Se or Sc , is classi ed as uent if its length is less than the average length of the uent sentences in the training set.\nResults. Table 2 shows the performance of di erent models for sentence uency classi cation on the test set. e proposed four-way LSTM achieves the highest precision at the cost of recall. is is desirable as sentences incorrectly classi ed as not uent still have a chance to get back in the subsequent uency-guided learning stage. Some qualitative results are provided in Table 3."}, {"heading": "4.2 Image Caption Generation", "text": "Setup. While we target at learning from machine-translated corpus, manually wri en sentences are needed to evaluate the e ectiveness of the proposed framework. To the best of our knowledge, Flickr8k-cn [23] is the only public dataset suited for this purpose. Each test image in Flickr8k-cn is associated with ve Chinese sentences, obtained by manually translating the corresponding ve English sentences from Flickr8k [16]. In addition to Flickr8k-cn, we construct another test set by extending Flickr30k [43] to a bilingual version. For each image in the Flickr30k training / validation sets, we employ Baidu translation to automatically translate its sentences\nfrom English to Chinese. e sentences associated with the test images are manually translated. Similar to [23], we hire ve Chinese students who are uent in English (passing the national College English Test 6). Notice that an English word might have multiple translations, e.g., football can be translated into \u2018\u8db3\u7403\u2019(soccer) and \u2018\u6a44\u6984\u7403\u2019(American football). For disambiguation, translators were shown an English sentence together with the image. For the sake of clarity, we use Flickr30k-cn to denote the bilingual version of Flickr30k. Besides the translation of English captions, Flickr8kcn also contains independent manually wri en Chinese captions. Main statistics of Flickr8k-cn and Flickr30k-cn are given in Table 4.\nBaselines. To verify the e ectiveness of our uency-guided approach, we compare with the following three alternatives:\n(1) \u2018Late translation\u2019 [23], which generates Chinese captions by automatically translating the output of an English captioning model. (2) \u2018Late translation rerank\u2019, which reranks the top 5 sentences generated by \u2018Late translation\u2019 according to their estimated uency scores in descending order. (3) \u2018Without uency\u2019, which learns from the full set of machinetranslated sentences.\nFurthermore, to understand the performance gap between the proposed approach and the method directly using manually wri en Chinese captions, we train a Chinese model using Flickr8k-cn [23], the only dataset that provides manually wri en Chinese captions for training. We term this model \u2018Manual Flickr8k-cn\u2019.\nAutomated evaluation. We adopt performance metrics widely used in the literature, i.e., BLEU-4, ROUGE-L, and CIDEr. e only exception is METEOR [6], which is inapplicable for evaluating Chinese sentences due to the lack of a structured thesaurus such as WordNet in Chinese. BLEU is originally designed for automatic machine translation where they compute the geometric mean of n-gram based precision for the candidate sentence with respect to the references and adds a brevity-penalty to discourage overly short sentences [30]. ROUGE is an evaluation metric based on F-measure of longest common sub-sequence [25]. CIDEr is a metric developed speci cally for evaluating image captioning [35]. It performs a Term Frequency Inverse Document Frequency (TF-IDF) weighting for each n-gram to give less-informative n-grams lower weight. e CIDEr score is computed using average cosine similarity between the candidate sentence and the reference sentences. We use the coco-evaluation code1 to compute the three metrics, using human translated captions as ground truth.\nPerformance on the automatically computed metrics of di erent approaches is presented in Table 5. e reranking strategy improves over \u2018Late translation\u2019 showing the bene t of uency modeling. Nevertheless, both \u2018Late translation\u2019 and \u2018Late translation rerank\u2019 perform worse than the \u2018Without uency\u2019 run. Fluency-only is inferior to other proposed approaches as this model is trained on much less amounts of data, more concretely, 2,350 sentences in Flickr8k and 15,100 sentences in Flickr30k that are predicted to be uent. Both rejection sampling and weighted loss are on par with the \u2018Without uency\u2019 run, showing the e ectiveness of the two strategies for preserving relevant information.\nHuman evaluation. Although BLEU [30] is designed to account for uency, it has been criticized in the context of machine translation for being loosely approximate human judgments [3]. In particular, the n-gram based measure is insu cient to guarantee the overall uency of a generated sentence. We therefore perform a human evaluation as follows. Given a test image, sentences generated by distinct approaches are shown together to a subject, who is to rate the sentences using a Likert scale of 1 to 5 (higher is be er) in two aspects, namely relevance and uency. While rating is inevitably subjective, pu ing the sentences together helps the subject provide more comparable scores. Eight persons in our lab including paper authors participate the evaluation. Notice that to avoid bias, sentences are always randomly shu ed before presenting to the subjects. To reduce the workload, the evaluation is performed on a\n1h ps://github.com/tylin/coco-caption\nrandom subset of 100 images for each test set, and each image is rated by two distinct subjects. Average scores are reported.\nAs shown in Table 6, the reranking strategy results in more uent captions compared to the \u2018Late translation\u2019 approach, improving uency from 4.34 to 4.41 on Flickr8k-cn and from 4.60 to 4.75 on Flickr30k-cn, showing the e ectiveness of the proposed LSTM classi er for sentence uency estimation.\nOn both test sets, the three proposed strategies improve the uency of the generated captions compared to the baselines. ough receiving high uency rate on Flickr8k-cn, the uency-only model still su ers from lower relevance. e user study suggests that rejection sampling outperforms weighted loss in terms of both relevance and uency. In addition, we nd that for rejection sampling, the average number of mini-batches in each training epoch is 75 on Flickr8k and 616 on Flickr30k, which is less than half of the number of mini-batches for weighted loss. Compared to \u2018Late translation rerank\u2019, rejection sampling performs be er in describing images, suggesting that both relevance and uency have to be taken into account for cross-lingual image captioning.\nModel trained on manual annotation performs be er than uencyguided learning on Flickr8k-cn, improving relevance from 3.27 to 3.32 and uency from 4.66 to 4.79. However, the model is less e ective when tested on Flickr30k-cn, with relevance decreased from 3.20 to 2.83 and uency from 4.76 to 4.12. Learning from many translated text guided by uency results in cross-lingual models with be er generalization ability.\nFor a more intuitive understanding, some qualitative results are shown with human evaluation in Table 7."}, {"heading": "4.3 Discussion", "text": "While we investigate English-to-Chinese as an instantiation of cross-lingual image captioning, the proposed method can be easily extended to another target language, given the availability of some uency annotations in that language. Notice that compared to manually writing sentences for training images given the associated English captions and their machine translation results, manual annotation e ort for uency modeling is much less. Labeling uency just needs a click. By contrast, one has to perform a number of edits on the provided translated caption when the translation is unsatisfactory. According to our experiments, 89% of the provided translations are reedited by annotators. Consequently, on average\nit takes around 64 seconds to get a decent Chinese caption, while only 5 seconds to obtain a uency label. So collecting uency annotation is more e cient. Moreover, the uency labels are discrete, allowing us to easily obtain consistent and reliable uency annotation by majority voting on labels from distinct annotators. Also note that uency prediction as binary classi cation is less challenging than caption generation, so less amount of training samples is needed. In summary, uency-guided learning allows us to perform cross-lingual image captioning with a ordable annotation e orts."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we present an approach to cross-lingual image captioning by utilizing machine translation. A uency-guided learning framework is proposed to deal with the lack of uency in machinetranslated sentences. Experiments on two English-Chinese datasets, i.e., Flickr8k-cn and Flickr30-cn, support our conclusions as follows. Less than 30% of the translated sentences are considered uent, indicating much room for further improvement for current machine translation. Meanwhile, the proposed uency-guided learning by rejection sampling e ectively a acks the challenge. When measured by BLEU-4, ROUGE and CIDEr which emphasize on predicting relevant terms, the proposed approach is on par with the baseline that learns from all the translated sentences. Human evaluation shows that our approach outperforms the baseline in terms of both relevance and uency.\nOur proposed uency-guided learning framework takes a substantial step towards practical use of machine translation for crosslingual image captioning with minimal manual annotation e orts. Extending our work to multimedia content analysis and repurposing in a multilingual se ing opens up promising avenues for future research."}, {"heading": "ACKNOWLEDGMENTS", "text": "is work was supported by National Science Foundation of China (No. 61672523, 71531012). We thank the anonymous reviewers for their insightful comments. A Titan X Pascal GPU used for this research was donated by the NVIDIA Corporation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures", "author": ["R. Bernardi", "R. Cakici", "D. Ellio", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "J. Artif. Intell. Res", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Re-evaluation the Role of Bleu in Machine Translation Research", "author": ["C. Callison-Burch", "M. Osborne", "P. Koehn"], "venue": "In EACL", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Improving Translation Fluency with Search- Based Decoding and a Monolingual Statistical Machine Translation Model for Automatic Post-Editing", "author": ["J.-S. Chang", "S.-S. Lin"], "venue": "ROCLING", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["D. Chen", "C. Manning"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Meteor Universal: Language Speci\u0080c Translation Evaluation for Any Target Language", "author": ["M. Denkowski", "A. Lavie"], "venue": "In EACL Workshop", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "\u008ce Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "In IJCAI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Multilingual Image Description with Neural Sequence Models", "author": ["D. Ellio", "S. Frank", "E. Hasler"], "venue": "arXiv preprint arXiv:1510.04709", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "DeViSE: A Deep Visual-Semantic Embedding", "author": ["A. Frome", "G. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image \u008bestion Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "Journal of Arti\u0080cial Intelligence Research", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Principles of context-based machine translation evaluation", "author": ["E. Hovy", "M. King", "A. Popescu-Belis"], "venue": "Machine Translation 17,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Generating A\u0082ective Captions using Concept And Syntax Transition Networks", "author": ["T. Karayil", "P. Blandfort", "D. Borth", "A. Dengel"], "venue": "In MM", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Adding Chinese Captions to Images", "author": ["X. Li", "W. Lan", "J. Dong", "H. Liu"], "venue": "In ICMR", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Image Captioning with both Object and Scene Information", "author": ["X. Li", "X. Song", "L. Herranz", "Y. Zhu", "S. Jiang"], "venue": "In MM", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C. Lin"], "venue": "In ACL workshop", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Microso\u0089 coco: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "In ICLR", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "BosonNLP: An Ensemble Approach for Word Segmentation and POS Tagging", "author": ["K. Min", "C. Ma", "T. Zhao", "H. Li"], "venue": "NLPCC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Cross-lingual image caption generation", "author": ["T. Miyazaki", "N. Shimizu"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Self-critical Sequence Training for Image Captioning", "author": ["S. J Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Grounded compositional semantics for \u0080nding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "\u008b. Le", "C. Manning", "A. Ng"], "venue": "TACL", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Statistical machine translation with readability constraints. In Nodalida", "author": ["S. Stymne", "J. Tiedemann", "C. Hardmeier", "J. Nivre"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Yfcc100m: \u008ce new data in multimedia research", "author": ["B. \u008comee", "D. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L. Li"], "venue": "Commun. ACM 59,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C Lawrence Zitnick", "D. Parikh"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Image captioning with deep bidirectional LSTMs", "author": ["C. Wang", "H. Yang", "C. Bartz", "C. Meinel"], "venue": "In MM", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs", "author": ["G. Weiss", "K. McCarthy", "B. Zabar"], "venue": "DMIN", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Q. Wu", "C. Shen", "L. Liu", "A. Dick", "A. van den Hengel"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Boosting image captioning with a\u008aributes", "author": ["T. Yao", "Y. Pan", "Y. Li", "Z. Qiu", "T. Mei"], "venue": "arXiv preprint arXiv:1611.01646", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["J. Zhou", "Y. Cao", "X. Wang", "P. Li", "W. Xu"], "venue": "TACL", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 21, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 24, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 33, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 34, "context": "that associate images with text, recent works have signi\u0080cantly improved the quality of caption generation [20, 24, 27, 36, 37].", "startOffset": 107, "endOffset": 127}, {"referenceID": 7, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 20, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 26, "context": "Only few studies have been conducted for image captioning in a cross-lingual se\u008aing [8, 23, 29].", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "While the use of web-scale data has substantially improved machine translation quality [1, 40, 44], we observe that the \u0083uency of machine-translated Chinese sentences is o\u0089en unsatisfactory.", "startOffset": 87, "endOffset": 98}, {"referenceID": 40, "context": "While the use of web-scale data has substantially improved machine translation quality [1, 40, 44], we observe that the \u0083uency of machine-translated Chinese sentences is o\u0089en unsatisfactory.", "startOffset": 87, "endOffset": 98}, {"referenceID": 14, "context": "Fluency here means \u201cthe extent to which each sentence reads naturally\u201d [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "\u008cree leading approaches have been explored [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "[16] propose to exploit similarity in the visual space to transfer candidate training descriptions to a query image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 29, "context": ", [11, 18, 32] similarly rank existing descriptions but in a common multimodal space for the visual and textual data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 17, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 33, "context": "In [13, 20, 36] , a CNN pretrained on the ImageNet classi\u0080cation task is used to encode an image, and a Recurrent Neural Network (RNN) is then used to decode the visual representation, outpu\u008aing a sequence of words as the caption.", "startOffset": 3, "endOffset": 15}, {"referenceID": 37, "context": "[41] introduced an a\u008aention mechanism that incorporates visual context during sentence generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "More recently, using scene information [24] and high-level concepts / a\u008aributes as visual representation [39] or as an external input for RNN [42] is shown to obtain encouraging improvements over a standard CNN-RNN image captioning model.", "startOffset": 142, "endOffset": 146}, {"referenceID": 34, "context": "[37] propose a deeper bidirectional variant of Long Short Term Memory (LSTM) to take both history and future context into account in image captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "A concept and syntax transition network [19] is presented to deal with large real-world captioning datasets such as YFCC100M [34].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "A concept and syntax transition network [19] is presented to deal with large real-world captioning datasets such as YFCC100M [34].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Furthermore, in [31], reinforcement learning is also utilized to train the CNN-RNN based model directly on test metrics of the captioning task, showing significant gains in performance.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "[8] address this topic as a translation problem, generating a description in the target language for a given image with a strong assumption that source-language descriptions are already provided for the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "To train a Japanese captioning model, Miyazaki and Shimizu [29] use crowd sourcing to collect Japanese descriptions of the MSCOCO training set [26].", "startOffset": 59, "endOffset": 63}, {"referenceID": 23, "context": "To train a Japanese captioning model, Miyazaki and Shimizu [29] use crowd sourcing to collect Japanese descriptions of the MSCOCO training set [26].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "[23] have made a \u0080rst a\u008aempt in this direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "However, they use the translated text as it is, directly training a Chinese captioning model using machine-translated sentences from the Flickr8k dataset [16].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 23, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 39, "context": "Because public datasets for image captioning are in English [16, 26, 43] while Chinese is the most spoken language in the world, we consider English-toChinese as the cross-lingual se\u008aing.", "startOffset": 60, "endOffset": 72}, {"referenceID": 3, "context": "It is worth noting that we do not intend to revise {Sc } to make them more \u0083uent, as this remains an open problem in machine translation [4, 33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 30, "context": "It is worth noting that we do not intend to revise {Sc } to make them more \u0083uent, as this remains an open problem in machine translation [4, 33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 12, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 178, "endOffset": 186}, {"referenceID": 19, "context": "LSTM [15], for its capability of modeling long-term word dependency in natural language text, has been used to learn a meaningful and compact representation for a given sentence [12, 22].", "startOffset": 178, "endOffset": 186}, {"referenceID": 25, "context": "We employ BOSON [28], a cloud based platform providing rich Chinese natural language processing service.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "(2) using stochastic gradient descent with Adam [21] on batches of size 64.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "We employ BOSON and a Stanford parser [5] to acquire Chinese and English POS tags, respectively.", "startOffset": 38, "endOffset": 41}, {"referenceID": 33, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Following [20, 36], per iteration we apply beam search to maintain the k best candidate sentences, with a beam of size 5.", "startOffset": 10, "endOffset": 18}, {"referenceID": 33, "context": "Following [20, 36], per iteration we apply beam search to maintain the k best candidate sentences, with a beam of size 5.", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "To extract image representations, we use a pre-trained ResNet152 [14] which achieved state-of-the-art results for image classi\u0080cation and detection in both ImageNet and COCO competitions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "A typical case is learning from a data set with highly unbalanced classes, where one might consider downsampling classes in majority, over-sampling classes in minority or re-weighting samples [7, 38].", "startOffset": 192, "endOffset": 199}, {"referenceID": 35, "context": "A typical case is learning from a data set with highly unbalanced classes, where one might consider downsampling classes in majority, over-sampling classes in minority or re-weighting samples [7, 38].", "startOffset": 192, "endOffset": 199}, {"referenceID": 6, "context": "\u008cis strategy makes full use of the translated sentences by cost-sensitive learning [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "To this end we sample at random 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] respectively.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "To this end we sample at random 2k and 6k English sentences from Flickr8k [16] and MSCOCO [26] respectively.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "To the best of our knowledge, Flickr8k-cn [23] is the only public dataset suited for this purpose.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Each test image in Flickr8k-cn is associated with \u0080ve Chinese sentences, obtained by manually translating the corresponding \u0080ve English sentences from Flickr8k [16].", "startOffset": 160, "endOffset": 164}, {"referenceID": 39, "context": "In addition to Flickr8k-cn, we construct another test set by extending Flickr30k [43] to a bilingual version.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "Besides Flickr8k-cn [23], we construct Flickr30k-cn, a bilingual version of Flickr30k [43] obtained by English-toChinese machine translation of its train / val sets and human translation of its test set.", "startOffset": 20, "endOffset": 24}, {"referenceID": 39, "context": "Besides Flickr8k-cn [23], we construct Flickr30k-cn, a bilingual version of Flickr30k [43] obtained by English-toChinese machine translation of its train / val sets and human translation of its test set.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Flickr8k-cn [23] Flickr30k-cn (this work)", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "Similar to [23], we hire \u0080ve Chinese students who are \u0083uent in English (passing the national College English Test 6).", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "To verify the e\u0082ectiveness of our \u0083uency-guided approach, we compare with the following three alternatives: (1) \u2018Late translation\u2019 [23], which generates Chinese captions by automatically translating the output of an English captioning model.", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "Furthermore, to understand the performance gap between the proposed approach and the method directly using manually wri\u008aen Chinese captions, we train a Chinese model using Flickr8k-cn [23], the only dataset that provides manually wri\u008aen Chinese captions for training.", "startOffset": 184, "endOffset": 188}, {"referenceID": 5, "context": "\u008ce only exception is METEOR [6], which is inapplicable for evaluating Chinese sentences due to the lack of a structured thesaurus such as WordNet in Chinese.", "startOffset": 28, "endOffset": 31}, {"referenceID": 27, "context": "BLEU is originally designed for automatic machine translation where they compute the geometric mean of n-gram based precision for the candidate sentence with respect to the references and adds a brevity-penalty to discourage overly short sentences [30].", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "ROUGE is an evaluation metric based on F-measure of longest common sub-sequence [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 32, "context": "CIDEr is a metric developed speci\u0080cally for evaluating image captioning [35].", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "Although BLEU [30] is designed to account for \u0083uency, it has been criticized in the context of machine translation for being loosely approximate human judgments [3].", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "Although BLEU [30] is designed to account for \u0083uency, it has been criticized in the context of machine translation for being loosely approximate human judgments [3].", "startOffset": 161, "endOffset": 164}], "year": 2017, "abstractText": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual se\u008aing. Di\u0082erent from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of \u0083uency in the translated sentences, we propose in this paper a \u0083uency-guided learning framework. \u008ce framework comprises a module to automatically estimate the \u0083uency of the sentences and another module to utilize the estimated \u0083uency scores to e\u0082ectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both \u0083uency and relevance of the generated captions in Chinese, but without using any manually wri\u008aen sentences from the target language.", "creator": "LaTeX with hyperref package"}}}