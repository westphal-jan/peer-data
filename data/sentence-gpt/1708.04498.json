{"id": "1708.04498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jun-2017", "title": "Self-adaptive node-based PCA encodings", "abstract": "In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it is able to calculate the principal component analysis (PCA) in a distributed fashion across nodes. It simplifies existing network structures by removing intralayer weights, essentially cutting the number of weights that need to be trained in half. The algorithm can compute all of the node-specific weights needed to compute the principal component analysis. It can estimate how many nodes have to be trained in order to compute the principal component analysis.", "histories": [["v1", "Fri, 16 Jun 2017 12:29:41 GMT  (19kb)", "http://arxiv.org/abs/1708.04498v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.SE", "authors": ["leonard johard", "victor rivera", "manuel mazzara", "jooyoung lee"], "accepted": false, "id": "1708.04498"}, "pdf": {"name": "1708.04498.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Leonard Johard", "Victor Rivera", "Manuel Mazzara"], "emails": ["l.johard@innopolis.ru", "v.rivera@innopolis.ru", "m.mazzara@innopolis.ru", "j.lee@innopolis.ru"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n04 49\n8v 1\n[ cs\n.N E\n] 1\n6 Ju\nn 20\n17"}, {"heading": "1 Introduction", "text": "Innovative engineering always looks for smart solutions that can be deployed on the territory for both civil and military applications and, at the same time, aims at creating adequate instruments to support developers all along the development process so that correct software can be deployed. Modern technological solutions imply a vast use of sensors to monitor an equipped area and collect data, which will be then mined and analyzed for specific purposes. Classic examples are smart buildings and smart cities [1,2]. Sensor integration across multiple platforms can generate vast amounts of data that need to be analyzed in real-time both by algorithmic means and by human operators. The nature of this information is unpredictable a priori, given that sensors are likely to encounter both naturally variable conditions in the field and disinformation attempts targeting the network protocols. This information needs to be transmitted through a distributed combat cloud with variable but limited bandwidth available at each node. Furthermore, the protocol has to be resistant to multiple node failures. The scaling of the information distribution also benefits from a pure feedforward nature, since the need for bidirectional communication scales poorly with the likely network latency and information loss, both of which are considerable in practical scenarios [3,4]. This requirement puts our desired adaptive system into the wider framework of recent highly scalable feedforward algorithms that have been inspired by biology [5]."}, {"heading": "2 Linear sensor encodings", "text": "Linear encoding of sensor information has a disadvantage in that it cannot make certain optimizations, such as highly efficient Hoffman-like encodings on the bit level. On the other hand, it is very robust when it\nencodes continuous data, since it is isometric. This means that we will not see large disruptions in the sample distance and makes linear encodings highly suitable for later machine learning analysis and human observation. This isometry also makes the encoding resistant to noisy data transfers, which is essential in order to achieve efficient network scaling of real-time data.\nThe advantage of a possible non-linear encoding is further diminished if we consider uncertainty in our data distribution estimate. A small error in our knowledge can cause a large inefficiency in the encoding and large losses for lossy compression. For linear encodings all these aspects are limited, especially considering the easy use of regularization methods.\nThe advantage of linear encodings is that they possess a particular set of series of useful properties. To start with, if our hidden layer Y forms an orthonormal basis of the input layer we can represent the encoding as :\nItot = I1 + I2...+ In + e 2 (1)\nHere Itot is the variance \u2211\ni\n(X2i ) in the input space, In is the variance of\neach component of Y and e2 is the squared error of the encoding. This is obvious if we add the excluded variables yn+1...ym and consider a single data point:\nx 2 i = y 2 1 + y 2 2 ...+ y 2 n + y 2 n+1...+ y 2 m (2)\nand\nyn+1...+ ym = e 2 i (3)\nwhere ei is the error for data point I . Summing both sides and dividing by number of data points and we get:\nvar(I) = var(y1) + ... + var(yn) + e 2 (4)"}, {"heading": "3 PCA in networks", "text": "The problem of encoding in node networks is usually considered from the perspective of neural networks. We will keep this terminology to retain the vocabulary predominant in literature. A recent review of current algorithms for performing principal component analysis (PCA) in a node network or neural network is [6]. We will proceed here with deriving PCA in linear neural networks using a new simple notation, that we will later use to illustrate the new algorithms.\nAssume inputs are normalized so that they have zero mean. In this case, each output yi can be described as yi = X\nTw, where x is the input vector and w is the weights of the neuron and i is the index of the input in the training data. The outputs form a basis of the input space and if \u2016wi\u2016 = 1 and w T i wj = 0 for all i, j, then the basis is orthonormal. Let us first consider the simple case of a single neuron. We would like to maximize the variance on training data E [ y2\n2\n]\n, where we define y =\nXTw, given an input matrix formed by placing column wise listing of\nall the presented inputs X = [x1, x2...] with the constraint \u2016w\u2016 = 1. Expanding:\nE\n[\ny2\n2\n]\n= (XTw)T (XTw) = wTXXTw = wTCw (5)\nwhere C is the correlation matrix of our data, using the assumtions that inputs have zero mean. The derivative \u2202 \u2202w E [ y2 2 ] is given by\n\u2202\n\u2202w\nwTCw\n2 = XXTw = Xy (6)\nNote that the vector above describes the gradient of the variance in weight space. Taking a step of fixed length along the positive direction of this gradient derives the Hebb rule:\nw = w +\u2206w (7)\n\u2206w = \u03b7Xy (8)\nSince we have no restrictions on the length of our weight vector, this will always have a component in the positive direction of w. This unlimited growth of the weigth vector is easily limited by normalizing the weight vector w after each step by dividing by length, wnorm =\nw \u2016w\u2016\n. If we thus restrict our weight vector to unit length and note that C is a positive semidefinite matrix we end up with a semi-definite programming problem:\nmax w T Cw (9)\nsubject to\nw T w = 1 (10)\nIt is thus guaranteed, except if we start at an eigenvector, that gradient ascent converges to the global maximum, i.e. the largest principal component. Alternatives to weight normalization is to subtract the ew component of the gradient explicitly, where ew is the unit vector in the direction of w. In this case we would calculate:\n\u2202 \u2202w ( y2 2 )\u2212 ( \u2202 \u2202w ( y2 2 ) \u00b7 ew)ew (11)\nFor a step-based gradient ascent we can not assume \u2016wi\u2016 will be kept constant in the step direction. We can instead use the closely related\n\u2202 \u2202w ( y2 2 )\u2212 wTw( \u2202 \u2202w ( y2 2 ) \u00b7 ew)ew (12)\nThe difference is that the w overcompensates for the ew component if wTw > 1 and vice versa. This essentially means that \u2016wi\u2016 will converge towards 1.\n\u2206w = \u03b7(XywyT y) = \u03b7(XXTw \u2212 wTXXTww) (13)\n= \u03b7(Cw \u2212 wTCww) (14)\nThe derivative orthogonal to the constraint can be calculated as follows:\n\u2206w \u00b7 ew = \u03b7w T (Cw \u2212 wTCww) = \u03b7(wTCw \u2212 wTwTCww) (15)\nThis means that we have an optimum if\n((wTCw)\u2212 wwT (wTCw)) = 0 (16)\nSince wTCw is a scalar, w is an eigenvector of C with eigenvalue wTCw. Equation 16 gives that wTw = 1 This is learning algorithm is equivalent to Oja\u2019s rule [7]."}, {"heading": "3.1 Generalized Hebbian Algorithm", "text": "The idea behind the generalized Hebbian algorithm (GHA) [8] is as follows:\n1. Use Oja\u2019s rule to get wi 2. Use deflation to remove variance along ewi 3. i := i +1 4. Go to step 1\nSubtraction of the w-dimension projects the space into the subspace spanned by the remaining principal components. The target function y(vi) 2\n2 for all eigenvectors vi not eliminated by this projection, while\ny(w)2\n2\n= 0 in the eliminated direction w. Repeating the algorithm after this step guarantees that we will get the largest remaining component at each step. The GHA requires several steps to calculate the smaller components and uses a specialized architecture.The signal needs to pass through 2(n\u2212 1) neurons in order to calculate the n-th principal component and uses two different types of neurons to achieve this. We define information as the square variance of the transmitted signal and seek encodings that will attempt to maximize the transmitted information. In other words, the total transmitted variance by a linear transform is equal to the variance of data projected onto a subspace of the original input space. The variance in this subspace plus the square error of our reconstruction is equal to the variance of the input. Summarizing, minimizing the reconstruction error of our encoding is equivalent to maximizing the variance of the output. This is complementary and not antagonistic to the concept of sparse encodings disentangling the factors of variation [9]."}, {"heading": "3.2 Distributed PCA", "text": "Principal component analysis is the optimal linear encoding minimizing the reconstruction error, but still leaves room open for improvement. Can we do better? In PCA, as much as information as possible is put in each consecutive component. This leaves the encoding vulnerable to the loss of a node or neuron, potentially losing a majority of the information as a result. The PCA subspace remains the optimal subspace in this sense regardless the vectors chosen to span it. Thus, any rotation the orthonormal basis is also an optimal linear encoding.\nTheorem 1. There exists an encoding of the PCA space such that the information along each component is equivalent, In = Im,\u2200n,m. This encoding minimizes the maximum possible error of any combination n\u22121 components.\nProof. Starting from the eigenvectors vi, we can rotate any pair of vectors in the plane spanned by these vectors. As long as orthogonality is preserved, the sum of the variance in the dimensions spanned by these vectors is constant. Expressed as an average:\n\u2211\ni\nIi = \u2211\ni\nk (17)\nNow for this to be true and if not all variances Ii are identical there has to exist a pair of indices i and j such that Ii < k < Ij . We can then find a rotation in the plane spanned by these vectors such that Ii = k.\nThis simple algorithm can be repeated until \u2200i : Ii = k.\nIn matrix form this can be formulated as:\ndiag(WCW T ) = kI (18)\nOrthonormal basis:\nWW T = I (19)\n=>\ndiag(WCW T )\u2212WW T = (c\u2212 1)I (20)\ndiag((WCW \u2212W )W T ) = (c1)I (21)\nW (C \u2212 I)W T = (c\u2212 1)I\u2200I (22)\nThis seems like a promising candidate for a robust linear encoding and future work will further explore the possibility for calculating these using Hebbian algorithms. For the moment, we will instead focus on the eigenvectors to the correlation matrix used in regular PCA."}, {"heading": "3.3 Simple Hebbian PCA", "text": "We propose a new method for calculating the PCA encoding X \u2192 Y in a single time step and using a single weight matrix W .\nFor use in distributed transmission systems an ideal algorithm should process only local and explicitly transmitted information in terms of X and Y from its neighbors. In other words, each node possesses knowledge about its neighbors\u2019 transmission signal, but not their weights or other information. The Simple Hebbian PCA is described in pseudocode in algorithm 1.\nAlgorithm 1 ASHP\nRequire: Initialized weight vector wi Require: Input matrix X Require: Number of iterations T Require: Number of nodes N Require: Step size \u03b7\nfor t \u2190 1 to T do for i \u2190 1 to N do\nyi \u2190 Xwi\nfor i \u2190 1 to N do\nwi \u2190 wi + \u03b7(Xyi \u2212 i \u2211\nj =1\nXyjy T i yj\nyTj yj )\nwi \u2190 wi\nwT i wi\n3.3.1 Convergence property The first principal component can be calculated as \u2206w = Xy. This step is equivalent to Oja\u2019s algorithm. Let n be the index of the largest eigenvector calculated so far. The known eigenvectors v1, v2...vn of the correlation matrix C have corresponding eigenvalues \u03bb1, \u03bb2...\u03bbn. We can now calculate component vn+1.\nLemma 1.\nfn(w) = y2\n2 \u2212\nn \u2211\ni=1\nyT yiy T yi\n2\u03bb2i (23)\nhas for wTw = 1 a maximum atw = vn + 1, where y = w TX and yn = v T nX\nProof. We have an optimum if the gradient lies in the direction of the constraint wTw = 1, i.e.\n\u2202\n\u2202w fn = kw (24)\nfor some constant k.\n\u2202\n\u2202w fn = Cw \u2212\nn \u2211\ni=1\nCviw TCvi\n\u03bb2i (25)\nWhich further simplifies to\n(C \u2212 n \u2211\ni=1\nCviv T i C\n\u03bb2i )w = Cnw (26)\nwhere we define Cn as the resulting matrix of the above parenthesis. To reach an optimum we seek\nw T Cn = cw (27)\nwhere c is some scalar.\nOur optimal solution has the following properties:\n1. Assume w = vi, i \u2264 n : Substituting w = vi in 26 we get\n\u2202\n\u2202w fn(vi) = \u03bbivi \u2212 \u03bbivi = 0 \u00b7 vi (28)\nthen vi is an eigenvector of \u2202 \u2202w fn with eigenvalue 0.\n2. Assume w = vi of C, i > n: Substituting w = vi in 26 we get\n\u2202\n\u2202w f(vi) = Cw = \u03bbiw (29)\nthen vi is an eigenvector of \u2202 \u2202w fn with eigenvalue \u03bbi.\nC is symmetric and real. Hence, the eigenvectors v1...vn span the space R\nn. Cn is a sum of symmetric matrices. Consequently Cn is symmetric with the same number of orthogonal eigenvectors. As we see in equations 28 and 29, every eigenvector vi of C is an eigenvector of Cn, with eigenvalue \u03bbn,i = 0 if i \u2264 n and \u03bbn,i = \u03bbi if i > n. Since \u03bbn are ordered by definition, \u03bbn+1 is the largest eigenvalue of Cn + 1. Cn is symmetric with positive eigenvalues. As a result Cn is positive semi-definite. For this reason the maximization problem\nsup(wTCnw) (30)\nw T w = 1 (31)\nforms another convex optimization problem and gradient ascent will reach the global optimum, except if we start our ascent at an eigenvector where \u2202\n\u2202w fn(vi) = 0. For random starting vectors the probability\nof this is zero. The projection of the gradient onto the surface wTw = 1 created by weight normalization follows \u03b4w \u00b7 \u03b4w\nwT w > 0, i.e. even for steps not in the\nactual direction of the unconstrained gradient the step lies in a direction of positive gradient. This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12]. In contrast to these, we only require learning of a single set of weights w per node and avoid the weight set L for connections within each layer."}, {"heading": "4 Conclusions", "text": "We have proposed algorithm, Simple Hebbian PCA, and proof that it is able to calculate the PCA in a distributed fashion across nodes. It\nsimplifies existing network structures by removing intralayer weights, essentially cutting the number of weights that need to be trained in half. This means that the proposed algorithm has an architecture that can be used to organize information flow with a minimum of communication overhead in distributed networks. It automatically adjusts itself in real-time so that the transmitted data covers the optimal subspace for reconstructing the original sensory data and is reasonably resistant to data corruption. In future work we will provide empirical results of the convergence properties. We also seek to derive symmetric versions of our algorithm that uses the same learning algorithm for each node, or in an alternative formulation, that uses symmetric intralayer connections. Eventually we also strive toward arguing for biological analogies of the proposed communication protocol as way of transmitting information in biological and neural networks."}], "references": [{"title": "Microservice-based iot for smart buildings,", "author": ["K. Khanda", "D. Salikhov", "K. Gusmanov", "M. Mazzara", "N. Mavridis"], "venue": "in 31st International Conference on Advanced Information Networking and Applications Workshops,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Jolie good buildings: Internet of things for smart building infrastructure supporting concurrent apps utilizing distributed microservices,", "author": ["D. Salikhov", "K. Khanda", "K. Gusmanov", "M. Mazzara", "N. Mavridis"], "venue": "Selected Papers of the First International Scientific Conference Convergent Cognitive Information Technologies (Convergent 2016),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Implementing the internet of things vision in industrial wireless sensor networks,", "author": ["C. Kruger", "G.P. Hancke"], "venue": "Industrial Informatics (INDIN),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A connectionist actor-critic algorithm for faster learning and biological plausibility,", "author": ["L. Johard", "E. Ruffaldi"], "venue": "IEEE International Conference on Robotics and Automation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Neural network implementations for pca and its extensions,", "author": ["J. Qiu", "H. Wang", "J. Lu", "B. Zhang", "K.-L. Du"], "venue": "ISRN Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Simplified neuron model as a principal component analyzer,", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1982}, {"title": "Optimal unsupervised learning in a single-layer linear feedforward neural network,", "author": ["T.D. Sanger"], "venue": "Neural networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Representation learning: A review and new perspectives,", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A self-organizing network for principalcomponent analysis,", "author": ["J. Rubner", "P. Tavan"], "venue": "EPL (Europhysics Letters),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "A neural network learning algorithm for adaptive principal component extraction (apex),", "author": ["S. Kung", "K. Diamantaras"], "venue": "in International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data,", "author": ["C. Pehlevan", "T. Hu", "D.B. Chklovskii"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Classic examples are smart buildings and smart cities [1,2].", "startOffset": 54, "endOffset": 59}, {"referenceID": 1, "context": "Classic examples are smart buildings and smart cities [1,2].", "startOffset": 54, "endOffset": 59}, {"referenceID": 2, "context": "The scaling of the information distribution also benefits from a pure feedforward nature, since the need for bidirectional communication scales poorly with the likely network latency and information loss, both of which are considerable in practical scenarios [3,4].", "startOffset": 259, "endOffset": 264}, {"referenceID": 3, "context": "This requirement puts our desired adaptive system into the wider framework of recent highly scalable feedforward algorithms that have been inspired by biology [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 4, "context": "A recent review of current algorithms for performing principal component analysis (PCA) in a node network or neural network is [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "Equation 16 gives that ww = 1 This is learning algorithm is equivalent to Oja\u2019s rule [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "1 Generalized Hebbian Algorithm The idea behind the generalized Hebbian algorithm (GHA) [8] is as follows:", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "This is complementary and not antagonistic to the concept of sparse encodings disentangling the factors of variation [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 8, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "This algorithm has some degree of similarity to several existing algorithms, namely the Rubner-Tavan PCA algorithm [10], the APEX-algorithm [11] and their symmetric relatives [12].", "startOffset": 175, "endOffset": 179}], "year": 2017, "abstractText": "In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it is able to calculate the principal component analysis (PCA) in a distributed fashion across nodes. It simplifies existing network structures by removing intralayer weights, essentially cutting the number of weights that need to be trained in half.", "creator": "LaTeX with hyperref package"}}}