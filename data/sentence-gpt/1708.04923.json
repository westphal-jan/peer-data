{"id": "1708.04923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "mAnI: Movie Amalgamation using Neural Imitation", "abstract": "Cross-modal data retrieval has been the basis of various creative tasks performed by Artificial Intelligence (AI). One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative film makers do as of today. In this research, we take the first step towards it by visualizing the content of a book using its corresponding movie visuals. It is not just visualizing the books, but also using the movies as a whole. In this way, each individual book is presented with one visual reference and the subsequent plot lines are displayed by a camera to allow the user to see the various sections of the book. By using images from different sources, the users are able to discern the overall story by using the imagery in the movie. In our research we discovered that the visual representation of a movie in its visual direction is so great that most of the creative movie makers still use it to illustrate their films. In this way, each individual book is represented in a different visual reference and the next scene is presented by a camera to allow the user to see the entire series and see the various sections of the book. We hope this tutorial allows you to study the visual representation of a movie in its visual direction.\n\n\nThe images for this video are of different perspectives. These images are the most commonly made and the most commonly made of the films. Here, our images represent the best quality of a movie with a view of the main scenes in a different visual context. In this video, we are also able to view the main scenes in a different visual context using the images, in the same way as the movie that we have seen before. To be a good example, when we put together the movie with the same visual visual background, we see the same effect as when we create the movie in different visual contexts using different visual backgrounds. By using a camera, we can see how the film looks from different backgrounds and how a specific background has changed throughout the entire film. So, we have made a great example where we do not have to use any visual effects or make a certain amount of adjustments. Here we show the images in a different visual context using the images as a whole. These images are the most commonly made and the most commonly made of the films.\nIn this example, we used the same visual backgrounds as we did for the movie and also used the movies to show the different scenes in different visual contexts. In this example, we use a camera to view the main scenes in a different visual context using the", "histories": [["v1", "Wed, 16 Aug 2017 15:12:20 GMT  (1878kb,D)", "http://arxiv.org/abs/1708.04923v1", "Accepted in ML4Creativity workshop in KDD 2017. Preprint"]], "COMMENTS": "Accepted in ML4Creativity workshop in KDD 2017. Preprint", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["naveen panwar", "shreya khare", "neelamadhav gantayat", "rahul aralikatte", "senthil mani", "anush sankaran"], "accepted": false, "id": "1708.04923"}, "pdf": {"name": "1708.04923.pdf", "metadata": {"source": "META", "title": "mAnI: Movie Amalgamation using Neural Imitation", "authors": ["Naveen Panwar", "Shreya Khare", "Neelamadhav Gantayat", "Rahul Aralika\u008ae", "Senthil Mani", "Anush Sankaran"], "emails": ["naveen.panwar@in.ibm.com", "shkhare4@in.ibm.com", "neelamadhav@in.ibm.com", "rahul.a.r@in.ibm.com", "sentmani@in.ibm.com", "anussank@in.ibm.com"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Computingmethodologies\u2192 Cognitive science; Learning latent representations;\nKEYWORDS creative AI, multi-modal learning, deep learning ACM Reference format: Naveen Panwar, Shreya Khare, Neelamadhav Gantayat, Rahul Aralika e, Senthil Mani, Anush Sankaran. 2017. mAnI: Movie Amalgamation using Neural Imitation. In Proceedings of Workshop on Machine Learning for Creativity, SIGKDD, Nova Scotia, Canada, August 2017 (ML4Creativity\u201917), 8 pages. DOI:"}, {"heading": "1 INTRODUCTION", "text": "Being able to uently understand, retrieve, and generate crossmodal data, like humans do, has been the holy grail search in Arti cial Intelligence (AI). Language and vision has been considered as the most common and challenging domains to measure the growth of arti cial intelligence. Describing an image in words (image captioning) and imagining a text through images (visual abstraction/ description) is highly natural and seamless for human beings. While reading a gripping novel or a book, we o en tend imagine the storyline and the plots through visuals. If a corresponding movie or video exists for a book, most of the imaginative visuals are borrowed from the movie and mapped with the book stories. Another common example is of the movie director (or a movie creation crew), who produces a movie from a book or storyline through creative visualizations.\nConsider the example book snippet from Harry Po er and the Philosopher\u2019s Stone -\nProfessor McGonagall peered sternly over her glasses at Harry. \u201dI want to hear you\u2019re training hard, Po er, or I may change my mind about punishing you.\u201d en she suddenly smiled. \u201dYour father would have been proud,\u201d she said. \u201dHe was an excellent idditch player himself.\u201d\nProf. McGonagall\nIt is natural for readers to imagine and visualize this book snippet through snippets from the corresponding movie. Figure 1 shows two possible visualizations of the book snippet as imagined by two di erent readers. ese visualizations provide information rich interpretations to the books. e examples are not only restricted to the actual book snippets but also towards any fandom, as the following one -\n\u201dAs such I do not expect anyone to understand the subtleties of using machine learning in creativity\u201d, scowled Snape, as he charged into the dark classroom and glared into Harry\u2019s pale blue eyes. \u201dHowever, our celebrity Harry Po er,\u201d he paused, \u201dcould probably enlighten us with what would happen if I added a LSTM over a CNN\u201d\nFan Fiction\nFigure 2 shows two possible visualizations of the fan ction of the same book, Harry Po er and the Philosopher\u2019s Stone. Motivated by such human behaviors, in this research, we a empt to describe constituent parts of a book or a story through its corresponding movie visuals.\ne publicly available MovieBook [18] dataset contains manually de ned alignment of 11 movies with their corresponding books. Given a book snippet, we retrieve a sequence of movie snippets describing that book snippet, using three independent models:\n(1) Dialog model: Relevant movie snippets are retrieved by matching the text dialog of the movie with the input book snippet using a skip-thought model [4] (2) Visual model: Relevant movie snippets are retrieved by matching only the visual cues of the movie scene with the input book snippet using a neural-storyteller model 1 (3) Hybrid model: Relevant movie snippets are retrieved using both the text dialog and the visual cues from the movie scene\ne rest of the paper is organized as follows; Section 2 talks about existing literature, Section 3 details the dataset used in this 1h ps://github.com/ryankiros/neural-storyteller\nar X\niv :1\n70 8.\n04 92\n3v 1\n[ cs\n.C L\n] 1\n6 A\nug 2\n01 7\nML4Creativity\u201917, August 2017, Nova Scotia, Canada Panwar et al.\nVisual Imagination #2:\nVisual Imagination #2:\nresearch, Section 3 explains the technical details of the proposed approach, Section 5 discusses the experimental results, and Section 6 concludes with some future directions."}, {"heading": "2 LITERATURE STUDY", "text": "e de ned problem statement requires the understanding of both the domains: video analysis and natural langauge processing. Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8]. Yang and Meinel [16] used Optical Character Recognition (OCR) and Automatic Speech Recognition (ASR) to transcribe content from video lectures and perform querying over the extracted content. Tian et al. [12] further extended this by tracking textual content across the frames in a video for be er content generation. Xu et al. [15] learnt a joint text-video embedding model built over independently learnt deep models of language semantic understanding and video embedding. us, they were able to perform both video to text generation and text based video retrieval using the joint model.\nNg et al. [17] considered each frame of a video as a word in a sentence and learnt an LSTM network to temporally embed the video. e representation for each frame was obtained using a deep CNN making the overall network as a CNN-LSTM deep network. Donahue et al. [3] proposed a Long-Term Recurrent Convolutional Network (LRCN) model for conditionally embedding the video based on the task to be performed. Venugopalan et al. [14] learnt a sequence to sequence model to encode a video frame sequence using an LSTM network and decode its corresponding caption using a conditional LSTM. For understanding large pieces of text, Le and Mikolov [5] extended a word representation word2vec to learn paragraph and document level representation. Arora et al. [1] proposed a simple method of averaging the word embeddings over a sentence and modifying it using PCA. Recently, Kiros et al. [4] came up with an unsupervised method of learning sentence representation called skip-thought vectors which provided comparable results in 8 di erent tasks without the need for task adaptation.\nOne of the closest work to our research is the MovieQA system proposed by Tapaswi et al. [11]. A memory network based question answering system is built on a movie corpus using multiple sources of information such as movie plot, movie video, subtitle, scripts, and Described Video Service (DVS) transcriptions. Our work considers the book content as the input which is, in general, more prose and descriptive than a movie plot or script. Tapaswi et al. [10] further proposed Book2Movie which aims to align book chapters to its\ncorresponding movie scenes. We are working at a much granular level of sentences rather than an entire chapter, which is a more challenging task. Our work is built upon Zhu et al. [18] aligning books and movies at sentence level. While they have performed experiments on describing movies in terms of the book, we a empt to describe the book in terms of the movie which is considered a much more creative problem."}, {"heading": "3 DATASET", "text": "Built upon the work by Zhu et al. [18], the MovieBook dataset is the highly relevant for our problem statement. e dataset contains visual clips (roughly spanning for few seconds) from movies, corresponding dialogue text (SRT) for the visual clips, and small chunks of book text (roughly 3-10 lines) for 11 di erent books. A manual alignment is available for a part of each book and each alignment is done using one of the three cues: (i) Visual cue based on the movie clip, (ii) Dialog cue based on the dialog spoken during that clip, and (iii) Audio cue based on the audio during that clip. e properties of this dataset are shown in Table 1. From the collection of 11 book-movie pairs, there are a total of 29, 436 book paragraphs, 19, 985 movie shots, and 16, 909 sentences in dialog subtitles. Using this corpus, a total of 1, 449 (book paragraph, movie shot) pairs were manually aligned using the dialog subtitles while 621 pairs were aligned using the visual content of the movie shot.\nAdditional to the MovieBook dataset, a huge corpus of books is used to train a model for sentence representation. e BookCorpus dataset has more than 11, 000 from 16 di erent genres containing more than 11 million sentences and a skip-thought model [4] is trained to learn a sentence representation. e pretrained model is already publicly available at: h ps://github.com/ ryankiros/skip-thoughts."}, {"heading": "4 PROPOSED APPROACH", "text": "e overall proposed approach has three di erent models and is illustrated in Figure 3. e individual steps and their training procedure is explained in detail in this section."}, {"heading": "4.1 Book Sentence and Movie Dialog Representation", "text": "For every chunk of the book or a dialog snippet, a sentence representation model is learnt using skip-thought vectors [4], one of the state-of-art models for unsupervised learning of text sequences. e skip-thought vector model is a natural encoder-decoder style\nextension of skip-gram model for word embedding learning. Given a tuple of three sentences, {si\u22121, si , si+1}, the a RNNmodel encodes the sentence si and two decoders a empts to predict the sentence si\u22121 and si+1, conditional on the encoding, as shown in Figure 4. us, such a model requires tuples of three sentences and can be trained in an unsupervised fashion. Kiros et al. [4] further show that a generic sentence representation model trained on a huge corpus of books can be directly used in eight di erent applications without the need for ne-tuning or task adaption. Owing to the generalizable nature of the skip-thought model, we use the available pre-trained model for directly extracting the representation of both book sentences and movie dialogue."}, {"heading": "4.2 Movie Video Representation", "text": "Representing videos as an embedded vector representation is well studied. In this research, we want to textually describe a movie clip so that semantic similarity could be computedwith the book snippet. Image captioning and video captioning techniques could generate a single sentence caption for an image or a video. Recently, neuralstoryteller 2 conditioned the image caption on an RNN to generate\n2h ps://github.com/ryankiros/neural-storyteller\na longer story to explain a single image. In this research, we explain the neural-storyteller style of model to generate a longer story for a video than for a single image. Given a video frame, an image caption is generated for every frame using an encoder-decoder model as proposed in [10]. Conditioned on the combined frame captions, an RNN decoder generates a story explaining the entire video clip. e details of the model is explained here: h ps://github.com/ryankiros/ neural-storyteller and the process is illustrated in Figure 5. From an input video sequence, frame are sampled are regular intervals at 2fps. For every frame, a caption is generated using a standard image captioning model. All the generated captions are pooled and provided as input to a conditional LSTM decoder, which generates story that represents the entire video sequence and not each frame in the video. It can be observed from the generated story shown in Figure 5, that the swimming pool is semantically being mapped to water, the person is being mapped as \u201cher\u201d due to the presence of long hair, and speci c semantic a ributes are extracted such as shirtless man, being top of sur oard. ese semantically extracted text could be used to map with the book paragraph which are typically descriptive, in nature."}, {"heading": "4.3 Extraction through Dialog Model", "text": "In this model, a similarity metric learnt between the book sentences and only the dialog (SRT) in the video, without leveraging any visual content of the movie. Given a pair of book sentence and dialog, their respective representations, \u00aeb and \u00aed are computed using the skipthought model. As proposed in [9], \u00aeb . \u00aed and abs(\u00aeb\u2212 \u00aed) are computed and concatenated. Over these representations, a regression based semantic similarity model is trained [9]. Here the regression model is binary, predicting the input pair as {match, non-match}.\nDuring test phase, for a given book sentence or a random fan ction sentence, its skip-thought representation is calculated and the semantic relevance is computed against all the dialog sentences available. A list of those dialog sentences above a threshold, t , is shortlisted as the relevant movie parts that explains the input book sentence. e video clips corresponding to the retrieved dialog sentences are stitched together and provided to the user."}, {"heading": "4.4 Extraction through Visual Model", "text": "For this model, only the book sentences and the video clips are used while the dialog sentences are not used. For a given video clip, a story explaining that video clip is automatically generated using the approach proposed in the Section 4.2. For the automatically extracted story, a skip-thought representation is extracted, so that, both the book sentence and video clips is in the same feature space.\nIn this space, the similarity classi er can be trained and tested in the similar way, as explained in Section 4.3."}, {"heading": "4.5 Extraction through Hybrid Model", "text": "To match a book with the corresponding video clip, in this hybrid model, we leverage both the video information as well as the dialog information. For a given book sentence, the similarity score for all the dialog sentences is obtained using the dialog model explained in Section 4.3 and the similarity score is obtained with all the video clips using the Visual model explained in Section 4.4. A sum score fusion is performed between the two lists of obtained similarity score, and a the threshold is applied on the fused score. e movie clips corresponding to the retrievals are stitched to provide the book visualization."}, {"heading": "5 EXPERIMENTAL STUDY", "text": "e experiments are conducted on the publicly availableMovieBook Corpus. e only trainable model is the semantic similarity model explained in Section 4.3. e entire data is split between 60% for training, 20% for validation, and 20% testing. us, for the Dialog model, there are 1842 for training samples and 616 test samples while for the Visual model 776 samples are used for training and 258 samples for testing. To compare with the proposed similarity model, a cosine distance based similaritymetric as well the similaritymodel proposed by Tai et al. [9] and trained on SICK dataset, are used.\ne performance of the proposed pipeline is evaluated using top-k Movie Retrieval Accuracy (MRA). is measure calculates the percentage of book sentences input for which all the retrieved movie clips are from the same movie as the input. e performance of the Dialog model and the Visual model are shown in Figure 6 and Figure 7, respectively. e major observations obtained from the results are as follows:\n(1) A rank-10 MRA of 80% is obtained for the Dialog model and 71% MRA is obtained for the Visual model, using the proposed approach. e proposed semantic similarity fared be er or comparable to the other two approaches, showing the e ectiveness of the similarity method. (2) Although the dataset provides the ground truth alignment, the exact aligned video snippet retrieval accuracy for a given book sentence is irrelevant for our experiments. For a given book sentence, there can be multiple parts in the movie that is semantically related and retrieving those movie snippets is the creative task at hand and not just the manually aligned movie snippet. us, the movie retrieval accuracy is a strong measure to evaluate our creative system rather than the exact alignment retrieval accuracy.\nTo show the e ectiveness of the combined Dialog and Visual Model, a Hybrid model was trained on the entire test set and the results are shown in Figure 8. e results shows that the hybrid model performs be er than the individual models at all ranks, suggesting to use both the modalities for matching during movie retrieval. From Figure 8, it can observed that the Dialog model performs much be er than the Visual model, suggesting that the dialog has richer information than the visual content. e same observation is extended to the Hybrid model, as the Hybrid does not show a rapid improvement compared to the Dialog model. However, there are certain caveats in this comparison as the Visual model is trained on\na much smaller dataset compared with the Dialog model. A working example of the Dialog Model and the Visual Model is shown in Figure 9 and Figure 10, respectively."}, {"heading": "6 CONCLUSION AND FUTUREWORK", "text": "In this research, we proposed a creative system which could visualize a snippet of book content using its corresponding movie visuals. We devised three models to retrieve semantically similar movie content of a book snippet: (i) a dialog model which use only the dialog content from the movie, (ii) a visual model which uses only the visual content from the movie, and (iii) a hybrid model which combines both the visual and dialog content from the movie.\nA frame-wise conditional LSTM based decoder is used to generate a single story explaining a movie snippet. Experimental results on the publicly available MovieBook dataset, shows the e ectiveness of the proposed hybrid model providing around 80% rank-10 retrieval accuracy.\nIn future, we plan to extend this approach by creatively generating animated images and video snippets that explains a book snippets [7] [13]. us, the proposed pipeline could be used for unseen book or for books which do not have a corresponding movie and their corresponding visual abstractions could be generated. Such a creative system would eventually be of great use for creative directors and advertisment lm makers as they can visualize stories and scripts before the movie is being produced."}], "references": [{"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Video Retrieval System", "author": ["Murray Campbell", "Alexander Haubold", "Ming Liu", "Apostol Natsev", "John R Smith", "Jelena Tesic", "Lexing Xie", "Rong Yan", "Jun Yang"], "venue": "IBM Research TRECVID-", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Je\u0082rey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": "In Proceedings of the IEEE conference on computer vision and pa\u0088ern recognition", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["\u008boc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Contentbased multimedia information retrieval: State of the art and challenges", "author": ["Michael S Lew", "Nicu Sebe", "Chabane Djeraba", "Ramesh Jain"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Don\u2019t just listen, use your imagination: Leveraging visual common sense for non-visual tasks", "author": ["Xiao Lin", "Devi Parikh"], "venue": "In International Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["Josef Sivic", "AndrewZisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Book2movie: Aligning video scenes with book chapters", "author": ["Makarand Tapaswi", "Martin Bauml", "Rainer Stiefelhagen"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A Uni\u0080ed Framework for Tracking Based Text Detection and Recognition from Web Videos", "author": ["Shu Tian", "Xu-Cheng Yin", "Ya Su", "Hong-Wei Hao"], "venue": "IEEE Transactions on Pa\u0088ern Analysis and Machine Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Learning common sense through visual abstraction", "author": ["Ramakrishna Vedantam", "Xiao Lin", "Tanmay Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In International Conference on Computer", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Uni\u0080ed Framework", "author": ["Ran Xu", "Caiming Xiong", "Wei Chen", "Jason J Corso"], "venue": "In AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Content based lecture video retrieval using speech and video text information", "author": ["Haojin Yang", "Christoph Meinel"], "venue": "IEEE Transactions on Learning Technologies 7,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Beyond short snippets: Deep networks for video classi\u0080cation", "author": ["Joe Yue-Hei Ng", "Ma\u008ahew Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": "In Computer Vision and Pa\u0088ern Recognition", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In IEEE International Conference on Computer Vision", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "\u008ce publicly available MovieBook [18] dataset contains manually de\u0080ned alignment of 11 movies with their corresponding books.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "\u0087e visuals are obtained from the MovieBook dataset [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "\u0087e visuals are obtained from the MovieBook dataset [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Table 1: Statistics for MovieBook dataset [18] with ground-truth for alignment between books and their movie releases.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 4, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 6, "context": "Textual content and concept based video retrieval has been well explored in the literature [2, 6, 8].", "startOffset": 91, "endOffset": 100}, {"referenceID": 14, "context": "Yang and Meinel [16] used Optical Character Recognition (OCR) and Automatic Speech Recognition (ASR) to transcribe content from video lectures and perform querying over the extracted content.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[12] further extended this by tracking textual content across the frames in a video for be\u008aer content generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] learnt a joint text-video embedding model built over independently learnt deep models of language semantic understanding and video embedding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] considered each frame of a video as a word in a sentence and learnt an LSTM network to temporally embed the video.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed a Long-Term Recurrent Convolutional Network (LRCN) model for conditionally embedding the video based on the task to be performed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[14] learnt a sequence to sequence model to encode a video frame sequence using an LSTM network and decode its corresponding caption using a conditional LSTM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "For understanding large pieces of text, Le and Mikolov [5] extended a word representation word2vec to learn paragraph and document level representation.", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "[1] proposed a simple method of averaging the word embeddings over a sentence and modifying it using PCA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] further proposed Book2Movie which aims to align book chapters to its corresponding movie scenes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] aligning books and movies at sentence level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18], the MovieBook dataset is the highly relevant for our problem statement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Given a video frame, an image caption is generated for every frame using an encoder-decoder model as proposed in [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "As proposed in [9], \u00ae b .", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Over these representations, a regression based semantic similarity model is trained [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "[9] and trained on SICK dataset, are used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In future, we plan to extend this approach by creatively generating animated images and video snippets that explains a book snippets [7] [13].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "In future, we plan to extend this approach by creatively generating animated images and video snippets that explains a book snippets [7] [13].", "startOffset": 137, "endOffset": 141}], "year": 2017, "abstractText": "Cross-modal data retrieval has been the basis of various creative tasks performed by Arti\u0080cial Intelligence (AI). One such highly challenging task for AI is to convert a book into its corresponding movie, which most of the creative \u0080lm makers do as of today. In this research, we take the \u0080rst step towards it by visualizing the content of a book using its corresponding movie visuals. Given a set of sentences from a book or even a fan-\u0080ction wri\u008aen in the same universe, we employ deep learning models to visualize the input by stitching together relevant frames from the movie. We studied and compared three di\u0082erent types of se\u008aing to match the book with the movie content: (i) Dialog model: using only the dialog from the movie, (ii) Visual model: using only the visual content from the movie, and (iii) Hybrid model: using the dialog and the visual content from the movie. Experiments on the publicly available MovieBook dataset shows the e\u0082ectiveness of the proposed models.", "creator": "LaTeX with hyperref package"}}}