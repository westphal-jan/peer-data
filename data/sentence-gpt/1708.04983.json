{"id": "1708.04983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2017", "title": "Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE", "abstract": "T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots). However, a fully modular approach to visualization is necessary for the performance and application of large datasets such as large datasets (such as large databases like MongoDB).\n\n\n\n\n\nTSE is a tool to optimize scalability using tSNE. This approach does not restrict the performance of our code. TSE reduces the amount of computation time for the data-oriented solution. Our solution is based on the existing stochastic neighbor embedding algorithm.\n\nIn general, the TSE algorithm improves scalability using tSNE. We can apply these optimisations by running a full-stack model. This simplifies the problem further.\nThe most significant performance optimization algorithm has been found in SSE, and is still available at the top of this paper.\nTSE and tSNE have proven very powerful.\nIn the latest papers, we present a complete suite of new functionalities that are optimized using TSE. It shows how this approach can be implemented with TSE.\nTSE is the best-known and most powerful tool in the area of performance optimization. The main reason is that it provides a new functionality model to the task of building efficient and robust applications (such as visualization, high-dimensional datasets). This approach has proven popular with many developers, but it has been used only by more than 1.5 million. The results of this research are expected to be useful in the next few years.\nTSE is a tool to improve scalability by optimizing scalability using TSE.\nA version of the TSE tool is available here.", "histories": [["v1", "Wed, 16 Aug 2017 17:17:56 GMT  (1630kb,D)", "http://arxiv.org/abs/1708.04983v1", "44 pages, 24 figures, 7 tables, planned for submission"]], "COMMENTS": "44 pages, 24 figures, 7 tables, planned for submission", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andrey boytsov", "francois fouquet", "thomas hartmann", "yves letraon"], "accepted": false, "id": "1708.04983"}, "pdf": {"name": "1708.04983.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE", "authors": ["Andrey Boytsov", "Francois Fouquet", "Thomas Hartmann"], "emails": ["name.surname@uni.lu"], "sections": [{"heading": null, "text": "In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation with Outlier coNtrol) - a novel approach for incorporating new data into tSNE representation. LION-tSNE is based on local interpolation in the vicinity of training data, outlier detection and a special outlier mapping algorithm. We show that LION-tSNE method is robust both to outliers and to new samples from existing clusters. We also discuss multiple possible improvements for special cases.\nWe compare LION-tSNE to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descent for new data, and neural network approximation.\nKeywords: tSNE, visualization, exploration, dimensionality reduction, embedding, interpolation, inverse distance weighting, approximation, gradient descent, neural networks"}, {"heading": "1 Introduction", "text": "Exploring high-dimensional datasets is a problem that frequently arises in many areas of science, and visualization is an important aspect of data exploration. Throughout the years many approaches were developed to reduce the data dimensionality and to visualize multidimensional data. Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7]. The choice of dimensionality reduction method depends on what are the main characteristics of interest\nar X\niv :1\n70 8.\n04 98\n3v 1\n[ cs\n.A I]\nin the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.\nT-distributed stochastic neighbor embedding (tSNE) [20] is a very popular prize-winning [1] algorithm for dimensionality reduction. The main strength of tSNE is visualizing clusters: if the data points are close in high-dimensional space, tSNE aims to keep data close in reduced dimensions as well. Hence, the clusters in original data should be visible after dimensionality is reduced. Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.\nThe main limitation of tSNE is that it learns non-parametric mapping: once the data are visualized, adding new data samples to existing visualization is not trivial. The same problem also highly limits the possibility to work with timevarying data: if the data point has changed, it should be visualized at a new location, but tSNE is not designed for finding new data locations within existing visualizations. While there were some attempts to extend tSNE to incorporate new data [19, 11], a large amount of possible approaches remains unexplored, and there was no attempt to create a systematic comparative study.\nThis work addresses the problem of embedding new data into existing tSNE representations. We propose LION-tSNE (Local Interpolation with Outlier coNtrol) - a novel method for incorporating new data samples into existing tSNE representation. We identify that most interpolation and approximation methods have a lack of robustness to newly introduced outliers. Another problem is that when a new data sample is introduced, very distant existing visualized points have disproportionate influence on the mapping of a new point, and often it results in significant performance loss. Those two challenges can be naturally solved by a single approach: use local interpolation/approximation, and invoke a special outlier mapping procedure when necessary. This is the main idea of LION-tSNE, and section 4 provides detailed description of the algorithm.\nThis paper is structured as follows. Understanding of tSNE algorithm is a requirement, so section 2 provides basic overview of tSNE. It also describes the limitations of the original tSNE algorithm. Section 3 describes related work and defines the goals of the paper. Section 4 gives a detailed description of the LION-tSNE approach. Section 5 proposes classification and a list of other plausible mapping methods for the tSNE algorithm. Those methods are then used as benchmarks for LION-TSNE in section 6. Section 6 provides analysis, evaluation and comparison of LION-tSNE to various interpolation and approximation methods. Section 7 provides the discussion and summarizes obtained results. Section 8 discusses the directions of future work and concludes the paper."}, {"heading": "2 T-Distributed Stochastic Neighbor Embedding", "text": "T-Distibuted Stochastic Neighbor Embedding algorithm was developed by L. van der Maaten and G. Hinton [20]. TSNE itself is based on SNE algorithm [14] by G. Hinton and S. Roweis. TSNE takes multidimensional data as an input and returns the representation of that data in a space with reduced dimensions (socalled embedded space, usually 2-dimensional). TSNE treats distances between all pairs of data samples as a distribution. There are two such distributions: one\nfor distances in the original space and one for distances in the embedded space. TSNE preserves the distances by minimizing Kullback-Leibler divergence (i.e. distance) between those two distributions. Mainly due to the asymmetric nature of Kullback-Leibler divergence, the tSNE algorithm favors keeping close points close. Here we provide only a brief explanation of tSNE algorithm. Please, refer to original articles [20] and [14] for more details and explanations.\nConsider a dataset X \u2208 RNxK , which is referred to as the training set. It consists of N data points, each point is K-dimensional, and there is a distance metric defined in K-dimensional space. The task is to find an embedding Y \u2208 RNxd, where d is usually 2 or 3. Here we refer to some value in the original high dimensional space as x, denoting xi if we are talking about a sample from the training set X. Similarly, we refer to a point in the embedded space as y, denoting yi if it is a point found by tSNE algorithm for training set sample xi. For simplicity we are going to refer to original K-dimensional space as x space, and to reduced dimensionality d-dimensional space as y space.\nFor the purpose of finding a mapping f : X \u2192 Y , in this paper we assume that all xi are different. It can be assumed without loss of generality - if xi = xj we can just treat them as one data point and remove duplicate row from the training set.\nThe algorithm accepts one parameter, perplexity, which can be roughly thought as the expected number of neighbors. Proper perplexity is often chosen manually by running tSNE several times and picking best option, but it should be noted that tSNE is fairly robust to different perplexity choices.\nThroughout the paper we will use the MNIST dataset (Modified National Institute of Standards and Technology dataset) as a running example. MNIST is a dataset of 28x28 grayscale handwritten digits [17]. Among many other use cases, MNIST was used as one of the benchmarks for the original tSNE algorithm [20]. Figure 1 refers to the tSNE algorithm with a perplexity of 30 applied to 2500 random distinct samples of MNIST digits, compressed to first 30 principal components. Here X refers to entire 2500x30 digit dataset, Y refers to 2500x2 visualizations of each point, and xi and yi refer to any particular handwritten digit and its visualization respectively. Throughout this paper we are going to deploy new 28x28 grayscale images into the visualization (figure 1) and evaluate whether new images are at the plausible position.\nTSNE operates in a following manner. At first, pairwise distances are represented as a following distribution (formula 1).\npi|j = exp \u2016xi\u2212xj\u20162 2\u03c32i\u2211\nk,i 6=k exp \u2016xi\u2212xk\u20162\n2\u03c32i\n(1)\nThe variances \u03c3i for each point is determined using equation 2.\nPerp(i) = 2H(Pi) (2)\nIn formula 2 Perp(i) is perplexity of the i-th point. All perplexities (usually a single value for all the points) are given as input parameters, and H(Pi) is Shannon entropy and can be calculated as follows (equation 3).\nH(Pi) = \u2211 j,i 6=j pi|j log2(pi|j) (3)\nThen the variances sigmai can be calculated using root finding techniques (the authors of tSNE [20] suggested binary search).\nSymmetric joint distribution can then be achieved by symmetrizing pi|j in the following manner (expression 4)\npij = pi|j + pj|i\n2N (4)\nDistances in the embedded space are represented as distributions in quite similar manner, but t-distribution is used instead of Gaussian distribution. Distribution in the embedded space is referred to as Q matrix, where each element qij is calculated as follows (formula 5).\nqi|j = (1 + \u2016yi \u2212 yj\u20162)\u22121\u2211\nk,i 6=k(1 + \u2016yi \u2212 yk\u20162)\u22121 (5)\nThe goal is to find vectors yi for each data point, such that Kullback-Leibler divergence of distributions P and Q (i.e. distance between distributions P and Q) is minimal. So, the task is to minimize following function w.r.t. Y (see expression 6).\nC(Y ) = KL(P\u2016Q) = \u2211 i,j,i 6=j pij log pij qij\n(6)\nResulting embedded coordinates Y are calculated by minimizing the function 6 using gradient descent, and the gradient can be expressed analytically (formula 7).\n\u2202C \u2202yi = 4 \u2211 j,j 6=i (pij \u2212 qij)(yi \u2212 yj)(1 + \u2016yi \u2212 yj\u20162)\u22121 (7)\nThe authors of original tSNE algorithm [20] suggest several improvements for gradient descent to avoid poor local optima. Those include using momentum and early exaggeration - multiplying P matrix by a factor for first multiple iterations of gradient descent algorithm.\nThe essential limitation of the tSNE algorithm is that it builds non-parametric mapping. Once the embedding for X is built, there is no straightforward formula or approach to visualize a new sample x in the same visualization. This new x can be either some completely new sample, or an update of an existing sample xi in the time-varying data scenario. The next section discusses existing attempts to address that limitation and refines the research subject of this paper."}, {"heading": "3 Related Work", "text": "The idea of incorporating new data into tSNE is a recognized problem. Existing solutions mainly took the form of finding a mapping function f : X \u2192 Y , which accepts a point in K-dimensional original space and returns its embedding in 2 or 3 dimensions. Once this mapping is found, the function can be used to incorporate new samples.\nL. van der Maaten [19], the author of the original tSNE algorithm, addressed the problem of building f : X \u2192 Y representation for tSNE. The author proposed using restricted Boltzmann machines to build an approximation of tSNE mapping and compared the results to non-tSNE based visualization approaches. Rather than interpolating or approximating existing tSNE results, the author learned both tSNE representation and mapping together. Parametric tSNE is one of the benchmark approaches to compare against LION-tSNE. The main difference in the approaches is that LION-tSNE aims to represent existing tSNE visualization, while parametric tSNE aims to learn both mapping and tSNE together. Performance comparison is provided in section 6.3.\nGisbrecht et. al [11] proposed kernelized tSNE - parametric tSNE based on normalized Gaussian kernels. This approach is close to RBF interpolation [5] and also has some resemblance to IDW interpolation [28]. Earlier version of the same method was also proposed in [10]. We use kernelized tSNE method as one of the benchmarks to compare against LION-tSNE in terms of cluster attribution accuracy and outlier robustness. The main difference between LION-tSNE and kernelized tSNE are two main features of LION-tSNE method: its use of local interpolation and its special focus on outlier handling. Extensive performance comparison of LION-tSNE and kernelized tSNE is provided in section 6.\nSeveral related work items also dealt with either the question of adding new data to tSNE or using tSNE for dynamic data, but they did it in another context and solved different, although adjacent, problems. Due to that, those approaches cannot be compared directly with LION-tSNE, but section 7 discusses, in which practical circumstances can those methods be chosen, and in which practical scenarios LION-tSNE approach should be used.\nPezotti et al. [24] proposed A-tSNE - an improvement over tSNE for progressive visual analytics. The authors used K-Nearest Neighbors (KNN) to approximate distance functions for tSNE calculations, and used degree of approximation to vary the tradeoff between precision and calculation time. Pezotti et al. [24] also included procedure for removing and adding point to tSNE based\non adding/removing points from KNN neighborhoods. There were also multiple attempts to visualize time-varying data using tSNE, but avoiding the problem of incorporating new samples. Notable examples include the paper of Rauber et al. [25]. The authors proposed dynamic tSNE, along with several strategies for calculating P matrices at each time step. Nguen et al. [23] proposed mTSNE algorithm, where the authors created single visualization for the entire set of time series, using time series distance metrics to produce P matrix.\nWhile several papers proposed to build a f : X \u2192 Y mapping for the tSNE algorithm, they mostly focus on a single mapping approach, rather than comparing alternatives. A large variety of plausible mapping approaches remain unexplored. There is a lack of classification and a comparative study. It is not explored how different mapping approaches perform under different circumstances, e.g. what happens if an outlier is added. These are the questions our paper addresses.\nIn this paper we explore, evaluate and compare a comprehensive list of plausible approaches for incorporating new data into tSNE representation. The list goes beyond the approaches suggested in related work, and we use it as a benchmark to compare with our newly proposed algorithm. We show that while some approaches work well in the vicinity of training data, and a few approaches can handle outliers, there is no approach that achieves both goals, and the performance on those two tasks is often inversely related. We propose, analyze and evaluate LION-tSNE - a method that can handle well those two challenges at once.\nThe next section introduces LION-tSNE (Local Interpolation with Outlier coNtrol) - an algorithm for incorporating new data into tSNE representation."}, {"heading": "4 LION-tSNE Algorithm", "text": "LION-tSNE algorithm is designed to have two important features:\nOutlier robustness. Many existing methods work well if a new sample x is in the vicinity of an existing training points xi. However, those methods struggle when presented with outliers (see section 6 for details). Outlier detection and special outlier handling algorithm can help to alleviate that problem.\nLocality. As will be shown in section 6, for a new sample x even very distant training points can significantly influence its embedding y. And the influence of distant points can very significantly reduce the performance. This problem is applicable to all benchmark methods, while being especially visible in inverse distance weighting interpolation. A solution to this problem is to ensure locality - embedding of new sample x should depend only on training samples xi within a certain radius.\nOutlier detection and locality can naturally combine into a single approach: we can use local interpolation when a new sample x has neighbors in a certain radius rx, and we can invoke outlier placement procedure otherwise. The radius r is referred to as neighbor radius and it is a parameter of the algorithm. On a very general level LION-tSNE approach can be summarized in algorithm 1.\nAlgorithm 1 LION-tSNE - General Approach\n1: function LION-tSNE(x, p, rx, rclose, Xtrain, Ytrain) 2: neighbor_indices = select_neighbors_in_radius(x,Xtrain, rx) 3: Xneighb = Xtrain[neighbor_indices] 4: Yneighb = Ytrain[neighbor_indices] 5: if len(neighbor_indices) > 1 then 6: y = local_IDW_interpolation(Xneighb, Yneighb) 7: else if len(neighbor_indices) == 1 then 8: y = single_neighbor_placement(Xneighb, Yneighb) 9: else\n10: y = outlier_placement() 11: end ifreturn y 12: end function\nFor local interpolation here we use inverse distance weighting (IDW) [28]. The value for new point x is determined as weighted sum of values yi, where weight is proportional to inverse distances. It can be formalized as follows (formula 8)\nF (x) = \u2211\n\u2016x\u2212xi\u2016\u2264r\nwi(x)yi, where wi(x) = \u2016x\u2212 xi\u2016\u2212p\u2211\n\u2016x\u2212xj\u2016\u2264r \u2016x\u2212 xj\u2016 \u2212p (8)\nWhen the point x \u2192 xi, the inverse distance \u2016x \u2212 xi\u2016\u22121 \u2192 \u221e, the corresponding weight wi(x) \u2192 1 (and all other weights \u2192 0 due to normalization) and F (x) \u2192 yi. If x = xi the mapping y is forced to yi. The power p is the parameter of the algorithm. Note that for LION-tSNE we use local version of IDW interpolation - algorithm considers only those training samples where \u2016x\u2212 xi\u2016 \u2264 r.\nIDW interpolation is relatively fast and produces high accuracy in the vicinity of training samples (see section 6 for accuracy estimates and complexity analysis). As an interpolation method, it ensures consistency: mapping for x = xi will produce at y = yi. Also it is capable of producing multidimensional output y.\nIDW requires to carefully choose the algorithm parameter p. Also IDW has serious shortcomings that will be discussed in sections 5 and 6, but those shortcomings mainly appear when sample x is far away from the training samples, and LION-tSNE uses IDW approach only locally. RBF interpolation [5] and some approximation methods are plausible substitutes of IDW in LION-tSNE.\nThere is a special case when for some new sample x there is only one neighbor in rx - meaningful interpolation requires at least two neighbor points. In that case behavior depends on whether that single neighbor xi is an outlier itself.\n\u2022 If xi is an outlier too, it will be more meaningful from a visualization perspective to put embedding y of new sample x in the vicinity of yi - we have several outlier cases that are close to each other.\n\u2022 In case if xi is not an outlier, it is recommended to denote x as an outlier and use the outlier placement procedure - x does not really belong to the same cluster as xi and should not be treated as such.\nBy placing close to some y or placing in the vicinity of some y from now on we mean placing at some random position at a distance not exceeding rclose, where rclose can be set to some low percentile of distribution of nearest neighbor distances in y space (like 10% or 20%). That way all points placed close together will form a cluster as dense as the clusters in current visualization."}, {"heading": "4.1 Outlier Placement", "text": "The main goal of the outlier placement is following: if a new sample x is an outlier, its mapping y should be perceived as an outlier in the visualization. In order to place an outlier, we need to find a position for y, such that there are no neighbors in a certain radius ry. Using something like y ymax for each dimension of y is not an option - placing new sample too far from existing values will complicate the visualization.\nThe radius ry is a parameter of the algorithm. Too large ry can put outlier points too far from the existing plot area and reduce the readability of the visualization, while too small ry can make clusters and outliers indistinguishable. Radius ry can be based on ryNN - some percentile of the distribution of the nearest neighbor distances in y space, e.g. 95th, or 99th percentile depending on what percentage of original training points we assume to be outliers; or ryNN can be set to maximum distance between nearest neighbors in y space. For better visibility ryNN can be further multiplied by a coefficient k \u2248 2. Also\nit is recommended to add some safety margin to ry - if several outliers are close to each other, they will be placed at a distance of rclose, and distance between one of them and different outlier can become lower than the chosen radius. In addition, one of the embedded points could get close to outlier if it is placed close to its only neighbor. Therefore, the recommended choice is ry \u2265 k \u2217 ryNN + rclose. However, ry can be chosen relatively arbitrary: its main purpose is just the clarity of visualization, and as long as outliers and clusters are clearly distinguishable, any choice of ry is acceptable.\nThe goal of outlier placement algorithm is to find positions in y space, such that there are no training data yi within ry radius. Additional challenge is that multiple different outliers can be introduced at once, and LION-tSNE should find positions for all of them. Multiple outliers should be at least ry away both from training samples and from each other. Also if possible the size of the plot should stay the same - new data should not be mapped to the area outside of [min(yi),max(yi)] (for each dimension of y), unless it is necessary.\nThe main idea of outlier placement algorithm is following: in order to find possible outlier mapping positions in y space it is sufficient to find set of squareshaped cells with the side 2ry, such that there are no training data yi within each cell and those cells do not overlap. If y space is not 2-dimensional, cubes or hypercubes are used instead of square-shaped cells. If an outlier is mapped to the center of that cell, it is guaranteed that no training samples are within ry radius. And if each outlier mapping occupies a different cell, then outlier mappings are at least at ry distance from each other as well. In practice the size of each cell along each dimension is slightly increased in order to fit exact number of equally sized cells between [min(yi),max(yi)]. The positions of those cells can be found using the approach described below in the algorithm 2.\nAlgorithm 2 Computing Possible Outlier Positions\n1: function PrecomputeOutlierPositions(ry, Ytrain) 2: // Adjust cell size per dimension to have exact number of cells 3: n_cells_per_dimension = bmax(Ytrain)\u2212min(Ytrain)ry c 4: radj = (max(Ytrain)\u2212min(Ytrain))/n_cells_per_dimension 5: // Find how many cells can we fit per each dimension 6: // Generate all possible cell coordinates 7: all_cells = all_combinations(n_cells_per_dimension) 8: free_cells = list() 9: for all cell \u2208 all_cells do\n10: // Find cell boundaries 11: for all i \u2208 cell.dimensions do 12: cell_bounds.min = min(Ytrain) + celli \u2217 radj,i 13: cell_bounds.max = min(Ytrain) + (celli + 1) \u2217 radj,i 14: end for 15: if no_samples_in_cell(cell_bounds, Ytrain) then 16: free_cells.append(cell) 17: end if 18: end forreturn free_cells.centers() 19: end function\nThe case when several outliers need to be placed at once is more compli-\ncated. In that case not only the distance to existing training points, but also the distance between the outliers themselves needs to be maintained. If some outliers are distinct from existing training points, but have distance \u2264 r from each other, it is recommended to use outlier placement procedure only for one of those outliers, and place all other ones at some close distance in y space to that sample. Again, close distance can be defined as some low percentile of distribution of nearest neighbors distances in y space (e.g. 10th or 20th percentile). For all remaining distinct outliers the positions precomputed by algorithm 2 can be chosen at random. The work of the algorithm is illustrated on MNIST dataset in figure 2.\nTo summarize, the positions for outliers are precomputed in advance. Then, if necessary, position for outlier placement can be taken from the pool of available positions in constant time. If there are no more precomputed available positions, new positions outside current plotting area can be precomputed as shown in figure 2.\nWhile this concludes the general description of LION-tSNE, the algorithm still requires setting several parameters. The next section summarizes methods for setting those algorithm parameters and proposes an approach for selecting power parameter p."}, {"heading": "4.2 Selecting Power Parameter", "text": "The LION-tSNE algorithm has 3 parameters: rx, ry and power p. The parameter rx can be chosen based on the percentile of nearest neighbor distances: e.g. if we assume 1% of initial data to be outliers, rx can be set to 99th percentile of nearest neighbor distances in x space. The parameter ry can be chosen relatively arbitrary, and some heuristics for selection are proposed in section 4.1. However, the selection of the remaining parameter p is not that straightforward.\nProper choice of power parameter p is paramount for IDW interpolation. In order to understand the effect of parameter p consider figure 3. It shows a simple example of IDW interpolation for one-dimensional x and one-dimensional y, but the same effect is applicable to multidimensional IDW interpolation as well. In the example interpolated (x,y) pairs are (10,10),(20,40),(30,1), and (40,50), and the powers 0.2, 2 and 20 were tested. For low power even when x looks relatively close to xi, the weight distribution is still close to uniform, and predicted value is around the center: y \u2248 mean(yi) (unless x is really very close\nto some xi). When the distance \u2016x\u2212xi\u2016 is low and the power is high, the weight wi(x) becomes dominating if x is even slightly closer to xi than to any other training sample. As a result, y \u2248 yi, where i = argmin\u2016x\u2212xj\u2016. Too small and too large parameter p can be viewed as different forms of overfitting - in both cases training samples are remembered, rather than generalized. Still properly chosen power p produces good interpolation. To summarize, the parameter p determines the generalization capabilities, and the goal is to evaluate those capabilities using only the training data.\nAlgorithm 3 Metrics for parameter p\n1: function ValidationMetrics(p, rx, Xtrain, Ytrain) 2: for all xi, yi \u2208 Xtrain, Ytrain do 3: neighbor_indices = select_neighbors_in_radius(Xtrain, xi, rx) 4: neighbor_indices.exclude(i) 5: Xneighb = Xtrain[neighbor_indices] 6: Yneighb = Ytrain[neighbor_indices] 7: yestimate = local_IDW_interpolation(Xneighb, Yneighb) 8: sum_square_error+ = \u2016yestimate \u2212 yi\u2016 9: end for\n10: mean_square_error = sum_square_error/length(Xtrain) return mean_square_error 11: end function\nIn order to test generalization capabilities we propose a variation of leaveone-out cross-validation, that works in a manner described in algorithm 3. The algorithm iterates over training samples, and for each sample it performs local IDW interpolation using neighbors of the sample, but not the sample itself. Then we estimate the value at xi and compare it with expected yi. The average square distance between estimated yi and real yi is the final metrics returned by the algorithm. An assumption is that if only one sample is left out at a time, the optimal power p should not shift much, and power p that performs best on cross-validation should effectively remain good choice. Results for MNIST training data are presented in figure 4. If power is too low, the interpolation\nfunction effectively returns mean y almost all the time, and the error is high. In case if power is too high, interpolation effectively returns nearest training sample yi, and it also increases error metrics. However, there is a clear minimum in the metric function. Note that the average mean square error looks relatively high, but the value itself is not the metric of how good the algorithm works, it provides ground only for comparison. Evaluation metrics will be further discussed in section 6. Also later in section 6 we investigate further the influence of power parameter and confirm that the proposed metrics really allows choosing proper power p. However, this metric is a heuristic, not an exact criterion. As will be shown in section 6, for low rx choosing slightly higher power values can produce slightly better results. Also both figure 4 and results from section 6 show that LION-tSNE is more robust to choosing too high values of p, rather than to choosing too low values of p.\nSelected p are summarized in table 1. For each rx selection we set power p at the minimum of cross validation metrics.\nThis concludes the description of all aspects of the LION-tSNE approach. Next section introduces several benchmark methods, to which LION-tSNE will be compared."}, {"heading": "5 Benchmark Methods", "text": "This section provides the description of benchmark algorithms for incorporating new data into existing tSNE visualization. There are several major approaches to that problem.\nRepeated gradient descent. A possible approach is to add new sample x to training matrix X, and then continue running tSNE - repeat gradient descent again, until new minimum of cost function is reached. The approach can be summarized as follows.\n1. Add new sample x to the matrix of training samples X. Let it be matrix X \u2032 that now contains N + 1 rows, new x corresponding to the last row.\n2. Recalculate new P \u2032 matrix for X \u2032. The option is to keep old values of \u03c3i or calculate them again using the new data.\n3. Run gradient descent to find new embedding y by minimizing KL divergence (formula 6). In order for original yi points to stay in place, the gradients \u2202KL(P\u2016Q)/\u2202yi should be forced to 0 for i = 1 . . . N . The choice is whether to use early exaggeration for new gradient descent or not. Another choice is how to initialize starting y for gradient descent.\nBuilding embedding function is another possible approach to incorporating new data into tSNE mapping. After tSNE run we have a set of training samples xi and corresponding yi. The task is to find a function f : x \u2192 y,\nwhich then will be used to produce mapping for new samples. It can be viewed as an interpolation or approximation task. Interpolation ensures consistency: for each training point xi the produced mapping will be exactly yi. However, approximation might generalize better. Approximation function does not fit all the data points exactly, and at first glance consistency will be lost. However, there is a way to avoid the loss of consistency: we can build approximation function right after the first application of tSNE, and from that point and on work with approximated embedding only.\nAs one of the benchmark options we use inverse distance weighting interpolation, which was discussed in section 4. Note that for benchmark we use non-local IDW interpolation - all training samples participate in the weighted sum in formula 8.\nAnother option that we explore is radial basis functions (RBF) interpolation. Buhmann [5] provides a thorough introduction to radial basis functions. Interpolation using RBFs is performed in a following manner (formula 9).\nF (x) = N\u2211 i=1 \u03bbi\u03c6(\u2016x\u2212 xi\u2016) (9)\nIn formula 9 \u03c6(\u2016x\u2212xi\u2016) is a radial function, i.e. it accepts multidimensional input, but depends only on distance between the input argument and some reference point [5].\nThe interpolation should pass through all the points in a training set, therefore, the equations 10 should be satisfied.\nyj = F (xj) = N\u2211 i=1 \u03bbi\u03c6(\u2016xj \u2212 xi\u2016), for j=1. . . N (10)\nCoefficients \u03bbi can be found by solving the system of linear equations (SLE) 10, and under some conditions invertibility of SLE matrix is guaranteed [5].\nAlong with interpolation, we also use approximation for benchmark. Approximating tSNE result requires multivatiate input and mutlivariate output. For that purpose neural networks [12] are a natural choice. Applying neural networks for approximating tSNE visulization can be a topic of its own research. Here we compare the performance of LION-tSNE against several benchmark neural network configurations. In order to determine the best neural network configuration we performed the following procedure. Original tSNE run resulted in 2500 (xi, yi) mapped pairs. They were split in 80/20 proportion into neural network training set (2000 samples) and neural network validation set (500 samples). Neural network was trained using the 2000-length training set in batch mode for 5000 epochs. The optimization criterion was average square distance between predicted y and corresponding point yi from validation set. Each combination of following neural network parameters was tested.\n\u2022 We tested neural networks with 1 and 2 hidden layers. Quick check showed that just adding the next layer does not improve performance much.\n\u2022 The number of nodes in a layer was iterated from 50 to 500 with a step of 50. According to quick check, greater number of nodes did not improve the performance significantly. For benchmark we used the same number of nodes in all layers.\n\u2022 For activation function we tested rectified linear units (ReLu, see [22]) ReLu(x) = max(0, x) and hyperbolic tangent tanh(x) = e\nx\u2212e\u2212x ex+e\u2212x .\n\u2022 In order to avoid overfitting, we used dropout regularization [29]. Dropout rates of 0.1, 0.25 and 0.5 were tested. An option without regularization was also tested.\nThe results of grid search are presented in figure 5. It should be noted that all benchmark configurations have signs of severe overfit: validation set error is significant (consider distance of 10 in figure 1), especially in comparison with training set error (below 0.1 for most configurations). Dropout regularization moderately helps. Approximating tSNE results with neural networks can be a topic of its own research, and further efforts in this area are out of scope of this paper.\nFor benchmark we picked 3 parameter sets that performed well on validation.\n2 hidden layers, 250 nodes per layer, ReLu activation, dropout regularization with rate of 0.25. This approach performed had the least validation error.\n2 hidden layers, 500 nodes per layer, ReLu activation, dropout regularization with rate of 0.5. This approach performed had second best validation error."}, {"heading": "1 hidden layers, 500 nodes; tanh activation unit; no regularization. It", "text": "showed the best validation performance among one hidden layer models. Though it is likely not the best model choice, it will be interesting for benchmark comparison.\nChosen neural networks were retrained using all 2500 samples of the dataset. Although the parameter values are specific to this particular dataset, the same procedure can be used to determine proper parameters for any dataset.\nFor benchmark we intentionally used most generic neural network configurations, suitable for any kind of dataset. More specialized solutions can be chosen for special cases. For example, if input x is an image, convolutional neural networks [16] might be fitting choice.\nIn addition, we decided to compare LION-tSNE to several methods described in related work. One of them is kernelized tSNE, developed by Gisbrecht et. al [11]. The mapping is interpolated using function 11. The parameters \u03c3i are determined as a distance from xi to nearest neighbor xj , then multiplied by some coefficient K, which is a parameter of the algorithm. The coefficients ai can be calculated by plugging training data into formula 11, getting system of linear equations similar to 10 and solving it. We use K = 0.05, 0.25 and 0.50 for benchmark tests, but the effect of parameter K on performance is investigated extensively in section 6.\nF (x) = \u2211 wi(x)ai, where wi(x) = exp(\u2212\u2016x\u2212xi\u2016 2 2\u03c32i )\u2211\nj exp( \u2212\u2016x\u2212xj\u20162\n2\u03c32j )\n(11)\nAlso we compare LION-tSNE with parametric tSNE approach described by van der Maaten [19]. In that approach tSNE representation is trained together\nwith restricted Boltzmann machines (RBM) model, which is then used for x\u2192 y mapping. RBM are undirected multilayer models used, among other applications, as the earliest deep learning models (see [13]).\nTo summarize, we compare performance of LION-tSNE to the following benchmark methods.\n\u2022 Repeated gradient descent\n\u2022 RBF interpolation using a range of popular kernels: \u2013 Multiquadric \u03c6(\u2016r\u2016) = 2 \u221a (r/ )2 + 1\n\u2013 Gaussian: \u03c6(\u2016r\u2016) = exp(\u2212(r/ )2) \u2013 Inverse multiquadric: \u03c6(\u2016r\u2016) = 1\n2 \u221a\n(r/ )2+1\n\u2013 Linear: \u03c6(\u2016r\u2016) = r \u2013 Cubic: \u03c6(\u2016r\u2016) = r3\n\u2013 Quintic: \u03c6(\u2016r\u2016) = r5\n\u2013 Thin plate spline: \u03c6(\u2016r\u2016) = r2log(r)\nFor all of the RBF methods parameter was set to average distance between xi.\n\u2022 IDW interpolations with powers 1, 10, 20, 27.9 and 40. The power 27.9 was selected as a minimum of metrics described in section 4.2.\n\u2022 Neural networks with 3 parameter combinations described above.\n\u2022 Kernelized tSNE as described by Gisbrecht et. al [11] with K equal to 0.05, 0.25 and 0.50.\n\u2022 Parametric tSNE as described by van der Maaten [19], using 60000 samples of non-processed 784-dimensional tSNE and 3 hidden layers of restricted Boltzmann machines of 500, 500 and 200 nodes per layer respectively.\n\u2022 LION-tSNE with radius set to 90th, 95th and 99th and 100th percentile of nearest neighbors distance distribution. Power was chosen according to the heuristics described in section 4.2, but the effect of power parameter on performance is also investigated.\nNext section proposes evaluation criteria, introduces the tests and evaluates LION-tSNE performance against all the benchmarks described above."}, {"heading": "6 Evaluation", "text": "In order to be useful, mapping method should preserve local structure of the data, just like original tSNE representation did. There are two aspects of preserving structure when adding new data:\n\u2022 If a new data point x belongs to some structure in the original space, and this structure is successfully represented in tSNE, then the point x should belong to the embedding of that structure. For example, if new handwritten digit is added to visualization of MNIST dataset, its embedding\nshould belong to the cluster of the same digits. We will show that most interpolation and approximation methods handle this problem relatively well.\n\u2022 If a new data point x does not belong to any structure, its embedding should not belong to any structure in y space as well. For example, if we attempt to add data point with noise to visualization of MNIST dataset, embedding of new point should not belong to any clusters of handwrtten digits. We show that current embedding methods cannot handle this requirement, and propose a new approach to finding f : X \u2192 Y mapping in presence of outliers."}, {"heading": "6.1 Cluster attribution test", "text": "One of the requirements for mapping method is following: if a new sample belongs to some local structure in x, it should belong to the same local structure in y. For example, in MNIST dataset, if we pick typically written digit \"1\", and there are clusters of \"1\" digits in tSNE representation, we expect the new sample to be mapped to a cluster of \"1\" digits. This property can be tested and evaluated as follows. For MNIST dataset we pick 1000 samples outside of the chosen 2500 training set. In order to make sure that those samples firmly belong to some local structure, we pick only those samples that have a very close neighbor in a training set. In particular, we start from random training set item xi and pick a new MNIST sample x (outside of the training set) such that \u2203i\u2200j, \u2016x \u2212 xi\u2016 < \u2016xj \u2212 xi\u2016 - any other training sample xj is further from xi than the new sample x. Then we generate embedding y for x using various methods, and compare the results. We expect y to belong to the same cluster as yi, or at least belong to another cluster corresponding to the same class. Figure 6a depicts first 10 test images and corresponding xi from training set. Note that nominal closest neighbor can be of different class (see example 4) due to PCA preprocessing and due to imperfections of distance metrics. Still we expect the mapping method to attribute the test case properly and put in among the neighbors of the proper class.\nThis test can be evaluated by following criteria: Nearest neighbor accuracy. New sample should be mapped firmly among the samples of the same class. In order to evaluate that, for each new embedding x\u2192 y we pick K (here we chose K = 10) nearest neighbors of y, and see what percentage of those neighbors is of the same class as the new sample. We compare accuracy of each method to training accuracy that is calcualted as follows. Each test case is designed to be close to some training sample, and even for those training samples 10 nearest neighbors will not always be of the same class. So, even if embedding y is very close to yi, accuracy will not be 100%. For each test case we estimate 10 nearest neighbors accuracy of the closest training sample. We count it is a practicall upper bound of achievable accuracy, although in theory it can be exceeded (e.g. by chance or by mapping to different cluster of the same class).\nNearest neighbor distance. Test cases are designed to be close to existing training examples. In this test we do not expect new y to be an outlier. Outliers are far from other points, so it can be evaluated by measuring distance to nearest neighbor yj in the training set. For each training sample we obtain\nthe distance to nearest neighbor and treat those distances as a distribution DNN . Then we obtain distance to nearest neighbor for y and determine at what percentile of distribution DNN is that distance. Test cases should be firmly within clusters, so values of around 50% or below are expected. High values (like 90% or above) indicate that mapping method produced outliers, rather than put the test samples in the clusters.\nKullback-Leibler divergence between new P and Q distribution matrices (both (N + 1)x(N + 1) - with new test sample incorporated) shows how well data are clustered and how well mapping algorithm keeps original cost function of tSNE. KL divergence can be compared between the methods and also to original KL divergence of the first tSNE run.\nCluster attribution test results are presented in table 2. RBF interpolation performance was quite similar across all kernels, except for quintic. Refer to figure 9 for additional insights on RBF interpolation results. Embeddings of all shown examples except 7stayed in the same clusters as their nearest neighbors in the training dataset. The example 7was embedded in different cluster, but still it was a cluster of the same digits \"2\". This is an indirect indication that RBF generalized training data, rather than memorized and overfitted the examples.\nThe performance of IDW interpolation is very sensitive to correct choice of the power parameter. Refer to figure 10 for additional insight. For power 1 most of the results are almost at the center of the plot. This effect was already described in section 4.2 and depicted in 3. The power is too low, and the output is close to meani(yi) almost all the time. Figure 7 shows the dependency between power and 10-nearest-neighbors accuracy for cluster attribution test. Although it might look that accuracy converged at p \u224812, figure 7b shows that accuracy levels off only at p \u2248 25 \u2212 30, which is the value shown by technique described in 4.2. After that increasing p will produce the same accuracy with increased overfit risks. IDW shows consistent, although small, improvement even over the baseline accuracy. It should be noted that power selection metrics described in section 4.2 identified good power parameters both for LION-tSNE and for non-local IDW interpolation. It also should be noted that LION-tSNE is more robust to parameter values, the reason is that if the power is too low, in\nlocal interpolation the sample is placed around the average y of a neighborhood (see figure 3), and it is usually a better choice than placing it at the global average y for non-local IDW interpolation.\nSome of the gradient descent variations showed relatively high accuracy. The performance of gradient descent depends mainly on one choice: whether y was initialized at yi, corresponding to xi closest to x, or whether y was initialized randomly. Closest initialization ensures starting close to the cost function minimum, and it takes not that many iterations to reach it. In case of random initialization gradient descent gets stuck in poor local minima. See figure 11a for illustration. Early exaggeration helps to get out of poor local minima, and improves the result in case of random initialization. Recalculating \u03c3i has no practical effect. Initializing at the closest x seems to have particular synergy with early exaggeration.\nFigure 12 shows the performance of the chosen benchmark neural network models. Neural networks approximate tSNE results, and for each model the placement of yi is slightly different (though visually barely distinguishable from original tSNE results). In 12 for comparability all mappings y are depicted in comparison to a single mapping - original tSNE mappings of yi. The accuracy of neural networks with 2 is almost 10% lower than baseline accuracy. One hidden layer model has even lower performance, which is understandable - it showed worse performance comparing to two hidden layer models on validation as well, and it was picked as a best one-hidden-layer model. It should be noted that nearest neighbors for accuracy calculations and figure 12b, as well as nearest neighbors distance distribution for percentile metric in table 2 were selected using corresponding approximated yi, not original ones. There is also notable increase in KL divergence. However, the latter is due to approximation, not due to adding new data. KL divergences of neural network approximations (before adding any new data) are presented in table 3.\nKernelized tSNE described in [11] showed near-baseline accuracy for properly chosen K parameters. Performance is presented in figure 13 and table 2. Figure 8 shows the dependency between accuracy and parameter K. There is a clear window of maximum performance, after which accuracy decreases.\nLION-tSNE performance is presented in figure 14 and table 2. Accuracy converged to the baseline accuracy for nearly all choices of rx. Actually, figure 7b shows that LION-tSNE configurations with large rx converged to better than baseline accuracy. Figure 14a illustrates the reason why small rx can cause accuracy loss: some test samples were incorrectly labeled as outliers. The probability of it happening increases with decreasing radius rx. Larger radius rx converges at a higher level of p, but convergence results in better accuracy. This effect becomes more pronounced in higher dimensionality, so it will be explained in more details in section 6.3. Still it should be noted that even in worst configuration accuracy was just\u2248 1% lower than baseline accuracy. LIONtSNE also showed one of the lowest KL divergences and very close distance to\nnearest neighbors in the cluster. Figure 7b shows some important insights on LION-tSNE performance. Also note that all chosen rx converged when power p was approximately equal to the power selected by the procedure described in section 4.2, although for small rx slightly higher power seems to result in better accuracy. IDW interpolation accuracy is an upper limit here - it can be viewed as the case when rx \u2192\u221e. However, increasing radius rx over 100th percentile of nearest neighbors distance can produce accuracy improvement only by a tiny fraction. Although IDW interpolation did show better accuracy, the improvement is only around 0.0005 (0.8792 vs 0.8797 for power 50), and as we will show further, IDW interpolation is one of the least successful in dealing with outliers.\nIt can be safely said that in this test LION-tSNE is tied for the best performance with IDW interpolation and exceeded all other considered benchmark methods. This was one of the reasons why IDW was chosen for local interpolation in LION-tSNE - it works well when new sample x is close to existing training data. However, as we shall see further, IDW interpolation is struggling to handle outliers properly. Also it should be noted that LION-tSNE is much more robust to power parameter p (see figure 7) - too low power will result in y converging to the average y among close neighbors in x, rather than to average y."}, {"heading": "6.2 Outlier Test", "text": "The second test deals with outlier samples. If a new sample does not belong belongs to any local structure in x, it should not belong to any local structure in y. For MNIST dataset we generate uniform random noise, then retain 1000 examples where distance from generated x to nearest neighbor xi is greater than any distance between nearest neighbors in the training set. The examples of those generated outliers are in figure 15. After generating embeddings y for test samples x, we evaluate the results using the following metrics, largely similar to the metrics of the previous test.\nNearest neighbor distance. This time we do expect y to be an outlier.\nLike in previous test, we obtain distance to nearest neighbor for x and determine at what percentile of the distribution DNN is that distance. Since y should be outlier, the values of 90% or above are acceptable. Average nearest neighbor distance is also reported - it is more meaningful comparison criteria for higher percentiles.\nKullback-Leibler divergence Outliers have small values of pij , so even if qij is large (i.e. outlier was placed close to some exiting point in y), the increase in KL divergence will not be significant. Still the increase in KL divergence can be an additional criteria for method comparison.\nOutliers test results are presented in table 4. RBF interpolation turned out to be not robust to outliers (see figure 16). Most outliers were placed in the existing clusters or close to them. The only exception was RBF interpolation with quintic kernel. However, this method had lower accuracy on cluster attribution test. Also it placed outliers outside of plot boundaries - it can complicate visualization.\nIDW interpolation also did not show outlier robustness (see figure 17). Interpolation with power p = 1 resulted in all samples gathered at the center, an effect described in section 4.2 and illustrated in figure 3. Larger power p resulted in somewhat better performance, but still outliers overlap with existing clusters a lot, and the distances to outliers are close to the distances within the cluster.\nNeural network approximation placed outliers relatively close or inside original clusters (see figure 19). It might be a subject of future work whether any other neural network configuration can achieve both high accuracy and outlier robustness.\nRepeated gradient descent showed better outlier robustness than most other methods (see figure 18), along with high accuracy in previous test. Still many outliers are indistinguishable from points in clusters. Although distance from sample to outlier is at high percentile of nearest neighbor distance, much cleaner separation could be achieved. And it should be noted that repeated gradient descent was the slowest method by far, which also limits its applicability.\nKernelized tSNE [11] showed excellent accuracy on previous test, but in this test it placed most outliers firmly within existing clusters (see figure 20). The approach is an interpolation method, and it tends to have somewhat similar struggles with outliers as all interpolation methods did. Exact influence of K parameter on kernelized tSNE performance in outliers test is presented in figure 22.\nOutlier placement test by LION-tSNE is illustrated in figure 21. LIONtSNE achieved the best outlier separation by far, and yet did not place outliers\noutside current plot bounds. LION-tSNE by construction places all outliers at a distance greater than the chosen threshold, so that level of performance was expected. It should be noted that in this test we generated samples that were very clearly outliers (and, it should be said, the test case definition of an outlier was consistent with LION-tSNE definition of an outlier). However, LION-tSNE performed better than any other considered method, and significantly better than the majority of considered methods. For many benchmark methods, even though outlier placement results are good on average, for each particular outlier its proper placement is not guaranteed, and figures 16, 17, 18 show examples of that - some mapped points are definitely not outliers. LION-tSNE guarantees by construction that if a sample is recognized as an outlier it will placed away from other points.\nMost LION-tSNE configurations achieved both high cluster attribution accuracy and proper outlier placement - those two features are not ususally seen together on any benchmark methods, usually it was either one or the other. Moreover, LION-tSNE in both cases showed the best performance among compared methods. In cluster attribution test LION-tSNE was tied for the best accuracy several other methods. For outlier placement LION-tSNE showed the best results by far. Some methods came close to LION-tSNE in outlier placement test in terms of percentile (several gradient descent variations, RBF interpolation with quintic kernel, and gradient descent), however, actual nearest neighbor distance show that LION-tSNE achieves much better separation between clusters and outliers.\nOne more prominent method, to which LION-tSNE was not yet compared, is parametric tSNE [19]. This method approximates tSNE using restricted Boltzmann machines (RBMs). RBMs are the earliest version of deep learning models [13], and they often require a lot of data to be trained. Despite extensive grid search we could not find configuration that provided good performance after training 2500 samples in 30 dimensions. So, in order to provide fair comparison, we used dataset to which it was originally applied: 784-dimensional MNIST dataset with 60000 samples. The next section describes application of LIONtSNE to larger and higher-dimensional datasets using parametric tSNE [19] as\na comparison benchmark."}, {"heading": "6.3 Increasing Data Set Size and Dimensionality", "text": "This section aims to evaluate LION-tSNE performance when the size and the dimensionality of the dataset is increased. For evaluation we used 60000 samples from MNIST dataset in 28x28 = 784 dimensional space. Test set consists of 10000 samples, and those samples were used for cluster attribution test. As a comparison benchmark we used parametric tSNE with 3 hidden layers having 500,500, and 2000 nodes respectively. This layer configuration was used for evaluation by parametric tSNE author [19], source code of parametric tSNE and the training/test dataset was made available by the original algorithm author. For outlier robustness test we used the same generated outliers like in previous section. In order to give both algorithms equal footing, we use Y values found by parametric tSNE with mentioned configuration and the perplexity of 30 as the ground truth for LION-tSNE interpolation.\nThe accuracy of cluster attribution test and its dependence on power parameter p are presented in figure 23. Baseline accuracy is now defined as the average 10 nearest neighbors accuracy in y space for all 60000 samples of the training set. Note that the effect described in section 6.1 became more pronounced - smaller radius rx converges faster, but to lower accuracy. Still for large rx values the accuracy was consistently better than the baseline. The converged performance is mentioned in the table 5.\nSo, why different rx converge to different accuracy? The table 5 provides results and intermediate calculations. Consider what happens with an embedding y of a test sample x for a very large p. In IDW interpolation for p \u2192 \u221e the embedding y \u2192 yi, where yi is the embedding of closest nearest neighbor xi (see figure 3 for illustration). In LION-tSNE it depends on the radius rx whether that nearest neighbor is used for local approximation or not. However, if two nearest neighbors are within rx, the radius rx no longer plays a role, the result is determined: y \u2192 yi. Consider the accuracy for rx \u2192 \u221e and p \u2192 \u221e: it can be calculated straightforwardly by finding nearest neighbor xi for every test case x, then picking yi and calculating its 10 nearest neighbor accuracy (including the sample yi itself in the neighbor list, but using expected label of y as the class label). That value is 0.9200, and that\u2019s what accuracy values should converge to when p\u2192\u221e and if at least two nearest neighbors are in rx for all\ntest samples. The latter is not always the case, that\u2019s why most rx converged to even higher accuracy values. The accuracy is averaged not for all test samples, but only for those that had at least 2 nearest neighbors within smaller rx. The samples with larger number of close neighbors are in denser clusters and tend to have higher than average 10 nearest neighbor accuracy.\nHowever, as further lines of table 5 illustrate, for small rx higher accuracy for non-outliers is outweighed by larger number of outliers. Accuracy can be viewed as a weighted sum of 3 components: acc = f2N \u2217 acc2N + f1N \u2217 acc1N + fO \u2217 accO. The value f2N , f1N and fO represent fraction of test cases that have 2 or more neighbors in rx, that have one a neighbor in rx (and still were not considered an outlier) and the outliers (with one or zero neighbors) respectively. The values acc2N , acc1N and accO represent corresponding accuracy values respectively. Note that only one component of accuracy depends on the power p - it is acc2N . As table 5 shows, the component f1N \u2217acc1N is not significant due to very low number of samples with 1 outlier nearest neighbor in the training set. The component fO \u2217 accO also have insignificant impact due to low 10 nearest neighbor accuracy for outlier samples. The component f2N \u2217 acc2N has dominating impact. For lower rx higher accuracy for non-outliers is outweighed by larger number of outliers, and as a result final accuracy values are smaller. That is why lower rx converge to worse accuracy. This also shows importance of proper selection of radius rx for LION-tSNE.\nIn order to illustrate that power selection procedure described in section 4.2 still works for higher dimensionality and larger dataset, consider figure 24. Power values that result in minimum of the validation function are presented in\ntable 5, accuracy corresponding to the chosen power is also presented in table 5. It should still be noted that it is a heuristic, not an exact solution, and better performance might be achievable (here selected power values resulted in \u22481-2% accuracy loss). Note that although for this test case high power p results in higher accuracy, setting too high power is not a good solution - embedding y of any new sample x will be barely distinguishable from embedding yi of its nearest training set neighbor xi (see figure 3).\nFor outlier test we used similar outlier samples like in the test described in section 6.2.\nThe resuts of both cluster atribution test and outlier robustness test is presented in table 6. The accuracy of LION-tSNE and parametric tSNE is approximately equal, and exact results might depend on configuration. However, like for smaller dataset, LION-tSNE achieved very good outlier separation. Both desired properties of LION-tSNE - high accuracy and clean outlier separation - scaled well with increased dataset size and dimensionality.\nThe next section provides complexity analysis for LION-tSNE and benchmark algorithms and concludes evaluation."}, {"heading": "6.4 Complexity Analysis", "text": "LION-tSNE was tied for highest accuracy and shown the best outlier separation. However, another comparison factor is the complexity of the algorithm. High complexity can make very accurate algorithm unusable, and low complexity can make less accurate algorithm the preferred choice. Complexity analysis is summarized in table 7 and described in more details below. In this analysis we take into account not only the number of training samples, but the number of original dimensions K. The dimensionality of reduced space d is 2 in almost all practical cases, it will be treated as constant. We assume that N K, so complexity like O(N+K) is equivalent to O(N), and O(N2+NK) is equivalent\nto O(N2). However, complexity like O(NK) is not equivalent to O(N). Building RBF interpolation function requires solving system of linear equations (SLE) to obtain \u03bbi (formula 10). Therefore, it takes O(N3) of upfront calculations to construct interpolation function. In order to build the matrix for SLE it requires O(N2) distance computations in K dimensions, but its complexity O(N2K) is subsumed by O(N3). Once interpolation function is built, it takes O(NK) to use it for any new x. The same is applicable for kernelized tSNE.\nIDW interpolation does not require precomputations. Building interpolation function comes at no upfront cost. It takes O(NK) to obtain value for new input x: determine all N inverse distances in K spaces, normalize weights and calculate weighted sum.\nRepeated gradient descent also does not require any upfront calculations. However, the practical complexity to obtain y for new x can vary a lot depending on how many gradient descent steps are required. Straightforward implementation of tSNE algorithm is has quadratic complexity in number of points [20] (if dimensionality is taken into account, O(N2K)), and it is the upper bound of repeated gradient descent complexity. Practically at runtime gradient descent worked much slower than any other method.\nNeural network approximation and restricted Boltzmann machines for parametric tSNE [19] require upfront training using backpropagation. Once they are trained, it takes several matrix multiplications and activation functions to determine the final value. So, both upfront and runtime complexity highly depend on the model structure: number of layers, number of nodes in each layer, etc. The main advantage of those methods is that runtime complexity does not depend on N - it can be important for very large training sets. Considering that input layer has K nodes and output layer has d nodes, complexity depends on K and d, but reporting it as merely O(K) can be very misleading, there are too many other factors involved.\nThe complexity of LION-tSNE is close to the complexity of IDW interpolation, upon which it is based. It takes O(NK) to determine the proper neighborhood for local interpolation, and building the interpolation itself takes O(NneighbK), which has an upper bound of O(NK). Unlike original IDW, there is also specific upfront cost related to obtaining the coordinates for outlier positioning. Outliers are placed in special free cells (see section 4.1). The number of cells to build grows exponentially with the dimensionality of y space, but the dimensionality of y space is usually 2 and here it is treated as constant.\nIf a straightforward approach is used, for each cell it takes N distance comparisons in K dimensions O(NK) to determine what training data (if any) are in the cell. Once potential coordinates are determined, they can be retrieved randomly from the pool at a constant cost. Another computation (which can be calculated upfront) is determining the coordinates of additional cells to place outliers. Again the complexity is exponential of number of reduced dimensions d. If separation distance in ry or close proximity radius rclose are determined as a percentile of nearest neighbor distance in y space, it takes O(N2d2) to build that distance distribution, but it can be skipped if necessary. To summarize, unavoidable upfront costs are O(NK)\nIn summary, LION-tSNE is tied for the lowest runtime complexity with most interpolation methods. Linear complexity shows decent scaling to growing N and K. LION-tSNE does require some upfront calculations related to outlier placement. If necessary these costs can be avoided by simplifying outlier placement procedures - placing outliers outside main plot area only, and skip detecting free cells inside the main plot area. This solution will drop upfront complexity to const(N,K) (assuming maximum and minimums of y for each dimension were calculated before), but might come at a cost of growing plot area."}, {"heading": "7 Discussion", "text": "In evaluation LION-tSNE achieved both good accuracy and outlier robustness. One could argue that LION-tSNE worked well on the cases where it was designed to work well. On another hand \"a sample belongs to a certain cluster\" and \"a sample is an outlier\" are usually the cases of most practical interest. In addition, borderline cases are difficult to evaluate, so comparing the performance and judging what method is a better fit is complicated.\nIn order to understand applicability of LION-tSNE, consider several alternative methods and their use cases. Consider a dynamic data scenario, where at any time a sample can appear, disappear or change. The scenario can be viewed also as a series of data snapshots, but in case of asynchronous updates the changes between each snapshot will be in one sample only. There are several possible approaches to the challenge:\n- Re-run tSNE. This is a most straightforward approach, and most likely we will see the same data structures (along with new clusters, if those appeared). However, the changes for each particular data sample will not be visible. If there are many data snapshots over time, or if there is a stream of new samples, then re-run of tSNE is not feasible approach - each snapshot visualization will look completely different, and each new or modified sample will result in a completely new visualization (can be especially important when data change asynchronously - one sample at a time). Moreover, running tSNE again is often much slower than using interpolation or approximation. To summarize, tSNE is highly useful for static snapshots, but it requires extension to handle dynamic data scenarios. Re-running tSNE is feasible if the only question is which data structures are appearing and disappearing, and if we can afford to analyze and compare the snapshots manually.\n- Use LION-tSNE (or any other interpolation approach over tSNE data). These approaches can handle scenarios where there is a stream of new samples\nover time (including both new samples and modified samples), or when there is a stream of data snapshots. It will be immediately visible for each sample, if that sample stayed in place, or moved insignificantly, or moved to a different cluster, or (in case of LION-tSNE) if the sample became an outlier. Also it will be visible if some cluster is disappearing, and it might be visible if some new cluster is being formed. Building visualization animations is possible. Handling asynchronously changing data does not present any additional challenges. To summarize, those approaches can handle a lot of dynamic data scenarios.\n- Approximation of tSNE results. The approach is close to the previous one and mainly shares the same pros and cons. There is a potential for increased KL divergence and outlier robustness might be a challenge, but on the upside runtime complexity does not depend on the number of training samples N . It can be highly beneficial for large N . The ways to improve LION-tSNE and reduce the complexity in term of N are discussed below in this section.\n- Using tSNE time series visualization approaches (like points on a single tSNE plot [23] or like a series of plots [25]). Those approaches are designed to visualize dynamic data as a whole. Handling the stream of completely new samples (unrelated to previous ones) is out of scope. Visualizing the facts on the level of exact samples like \"sample xi stays in place/moves/changes cluster at time t\" is also out of scope. To summarize, those methods were built for a different use case, and the preference should depend on exact task.\nThere are several special cases that might need explicit attention and improvements. Most of them are especially important for the data that changes over time. For illustration consider a moving data point x(t) with its embedding y(t).\nFirst special case is when x(t) leaves the rx-neighborhood of all raining samples and becomes an outlier. In that case there will be a sudden change in position. In our opinion, this is acceptable behavior: it will attract attention to the point, but it won\u2019t mislead the user while exploring the data.\nAnother special case is when x(t) leaves or enters r-neighborhood of certain training point xi. It is another source of possible discontinuity: situation is equivalent to forcing interpolation weight wi(x) to zero (or from zero to some value) instantly. If it is not acceptable, it can be alleviated as follows: each participating inverse distance \u2016x\u2212 xi\u2016\u2212p can be further multiplied by any continuous function that gradually transitions from 1 when \u2016x\u2212xi\u2016 = 0 to 0 when \u2016x\u2212 xi\u2016 \u2265 r. For example, it can be a triangular or trapezoid-shaped function. Only then, after multiplication, the weights are normalized. As a result, when transitioning out of r-neighborhood of some training point xi, the weight wi(x) reaches exactly zero when \u2016x\u2212 xi\u2016 = r, and continuity is maintained.\nThird special case is moving between similar clusters. For example, in figure 1 consider two clusters that correspond to digit \"1\" (large lower center cluster and smaller cluster on upper left). Consider a data sample x(t), which gradually transitions from some point in one cluster xi to a point in another cluster xj . And consider a situation when x(t) is exactly between clusters: it did not become an outlier while transitioning, and there are members of both clusters in its rneighborhood in original space. Then local interpolation will place this point somewhere between those clusters, i.e. in completely different cluster that has nothing in common with current point x(t). It can be highly misleading during data exploration, and this is another case when discontinuity is better behavior. Plausible solution is to limit the distance between neighbors in y space - if\nthe point xi is in r-neighborhood of point x(t), but its distance between yi and other local neighborhood points exceed some threshold, then this point is a candidate for exclusion from local interpolation. We can remove candidates starting from most distant point argmax(\u2016x\u2212xi\u2016), until there are no candidates for exclusion. If this method is applied, then y(t) will make a sudden transition from one cluster to another, when the number of neighbors in the second cluster reaches some critical point.\nThere is still a room for improvement of outlier handling. At the moment outlier location is chosen randomly, but it can be chosen to better reflect if a point x is close to some training sample (e.g. by choosing the outlier position which is closest to that sample). Also what happens if outlier changes its position over time? If the change of position is insignificant, i.e. |xnew \u2212 xold| \u2264 rx, and the point did not stop being an outlier, then it can be reflected by randomly moving outlier at close proximity to its old position. It will both indicate that position has changed, but the change is not significant. Another question is: what if new cluster is being formed? Though grouping outliers together allows detecting that, exact performance of LION-tSNE in that scenario is yet to be determined.\nNext section summarizes future work directions and concludes the article."}, {"heading": "8 Conclusion and Future Work", "text": "Some possible improvements and future work opportunities were already mentioned in section 7. However, there is several major future work direction that were not mentioned before, and those can have major impact on LION-tSNE performance.\nPerhaps, the most impactful future work direction is using fast nearest neighbors search. Straightforward approach takes O(N) comparisons to determine neighbors in a certain radius around new sample x. It can be a problem when N is large. Search for nearest neighbors in a certain fixed radius is a recognized problem on its own [3]. Also fixed radius nearest neighbors search can be approximated with fast K nearest neighbors search techniques [8] with sufficient number of neighbors and additional check whether the distance is small enough. Another option is to use orthogonal range search [4], i.e. search for neighbors in a box, rather than in a sphere.\nAnd, of course, application of LION-tSNE to large variety of practical tasks can help to identify further room for improvement of the algorithm.\nIn summary, tSNE algorithm is highly useful for exploring and visualizing high dimensional datasets. In this article we addressed the challenge of using tSNE to visualize dynamic data streams. We proposed, analyzed, implemented and evaluated LION-tSNE - a novel approach based on local interpolation and special outlier handling. The approach was compared with large set of benchmark approaches, and it was tied for highest accuracy and showed the best outlier separation by far. There is still room for future research, but LIONtSNE can already be applied to a variety of practical data analytics tasks."}, {"heading": "Acknowledgments", "text": "This research was supported by BGL BNP Paribas and Alphonse Weicker Foundation. We\u2019d like to thank Anne Goujon and Fabio Nozza from BGL BNP Paribas for their assistance."}], "references": [{"title": "Data-driven identification of prognostic tumor subpopulations using spatially mapped t-SNE of mass spectrometry imaging data", "author": ["Walid M. Abdelmoula", "Benjamin Balluff", "Sonja Englert", "Jouke Dijkstra", "Marcel J.T. Reinders", "Axel Walch", "Liam A. McDonnell", "Boudewijn P.F. Lelieveldt"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A Survey of Techniques for Fixed Radius Near Neighbor Searching", "author": ["Jon L Bentley"], "venue": "Technical report,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["Mark de Berg", "Otfried Cheong", "Marc van Kreveld", "Mark Overmars"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Radial Basis Functions: Theory and Implementations", "author": ["Martin D. Buhmann"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Gait recognition based on DWT and t-SNE", "author": ["Linlin Che", "Yinghui Kong"], "venue": "In Third International Conference on Cyberspace Technology (CCT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "The Use of Faces to Represent Points in K-Dimensional Space Graphically", "author": ["Herman Chernoff"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1973}, {"title": "k-Nearest neighbour classifiers", "author": ["Padraig Cunningham", "Sarah Jane Delany"], "venue": "Multiple Classifier Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Dimensionality reduction techniques to analyze heating systems in buildings", "author": ["Manuel Dom\u00ednguez", "Seraf\u00edn Alonso", "Antonio Mor\u00e1n", "Miguel A. Prada", "Juan J. Fuertes"], "venue": "Information Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Out-of-sample kernel extensions for nonparametric dimensionality reduction", "author": ["Andrej Gisbrecht", "Wouter Lueks", "Bassam Mokbel", "Barbara Hammer"], "venue": "ESANN", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Parametric nonlinear dimensionality reduction using kernel t-SNE", "author": ["Andrej Gisbrecht", "Alexander Schulz", "Barbara Hammer"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Neural Networks and Learning Machines", "author": ["Simon S. Haykin"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Stochastic Neighbor Embedding", "author": ["Geoffrey E Hinton", "Sam T. Roweis"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["Harold Hotelling"], "venue": "Journal of educational psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1933}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Application of t-SNE to Human Genetic Data. bioRxiv", "author": ["Wentian Li", "Jane E. Cerise", "Yaning Yang", "Henry Han"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Learning a Parametric Embedding by Preserving Local Structure", "author": ["Laurens Maaten"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Visualizing Data using t- SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Analysis of Electricity Consumption Profiles by Means of Dimensionality Reduction Techniques", "author": ["Antonio Mor\u00e1n", "Juan J. Fuertes", "Miguel A. Prada", "Seraf\u00edn Alonso", "Pablo Barrientos", "Ignacio D\u00edaz"], "venue": "In Engineering Applications of Neural Networks, Communications in Computer and Information Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "m- TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series. In VAHC2016 Workshop on Visual Analytics in Healthcare in conjunction with AMIA", "author": ["Minh Nguyen", "Sanjay Purushotham", "Hien To", "Cyrus Shahabi"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "author": ["N. Pezzotti", "B.P.F. Lelieveldt", "L. v d Maaten", "T. H\u00f6llt", "E. Eisemann", "A. Vilanova"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Visualizing time-dependent data using dynamic t-SNE", "author": ["Paulo E. Rauber", "Alexandre X. Falc\u00e3o", "Alexandru C. Telea"], "venue": "Proc. EuroVis Short Papers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Nonlinear Dimensionality Reduction by Locally Linear Embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "A Nonlinear Mapping for Data Structure Analysis", "author": ["J.W. Sammon"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1969}, {"title": "A Two-dimensional Interpolation Function for Irregularlyspaced Data", "author": ["Donald Shepard"], "venue": "In Proceedings of the 1968 23rd ACM National Conference,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1968}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1958}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science (New York, N.Y.),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}], "referenceMentions": [{"referenceID": 13, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 81, "endOffset": 85}, {"referenceID": 27, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 123, "endOffset": 127}, {"referenceID": 5, "context": "Notable examples include principal component analysis (PCA) [15], Sammon mapping [27], Isomap [30], local linear embedding [26], and Chernoff\u2019s faces [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 24, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 63, "endOffset": 66}, {"referenceID": 13, "context": "in the data: those can be inter-point distances [27], outliers [7], linear subspaces [15], and more.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "T-distributed stochastic neighbor embedding (tSNE) [20] is a very popular prize-winning [1] algorithm for dimensionality reduction.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 113, "endOffset": 117}, {"referenceID": 4, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 130, "endOffset": 133}, {"referenceID": 18, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 162, "endOffset": 166}, {"referenceID": 7, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": "Throughout the years tSNE has been successfully applied to visualizing genomic data [18], healthcare information [23], human gait [6], power consumption profiles [21], heating systems [9], tumor subpopulations [2], and many other application areas.", "startOffset": 210, "endOffset": 213}, {"referenceID": 16, "context": "While there were some attempts to extend tSNE to incorporate new data [19, 11], a large amount of possible approaches remains unexplored, and there was no attempt to create a systematic comparative study.", "startOffset": 70, "endOffset": 78}, {"referenceID": 9, "context": "While there were some attempts to extend tSNE to incorporate new data [19, 11], a large amount of possible approaches remains unexplored, and there was no attempt to create a systematic comparative study.", "startOffset": 70, "endOffset": 78}, {"referenceID": 17, "context": "Hinton [20].", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "TSNE itself is based on SNE algorithm [14] by G.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "Please, refer to original articles [20] and [14] for more details and explanations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Please, refer to original articles [20] and [14] for more details and explanations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Among many other use cases, MNIST was used as one of the benchmarks for the original tSNE algorithm [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Then the variances sigmai can be calculated using root finding techniques (the authors of tSNE [20] suggested binary search).", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "The authors of original tSNE algorithm [20] suggest several improvements for gradient descent to avoid poor local optima.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "van der Maaten [19], the author of the original tSNE algorithm, addressed the problem of building f : X \u2192 Y representation for tSNE.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "al [11] proposed kernelized tSNE - parametric tSNE based on normalized Gaussian kernels.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "This approach is close to RBF interpolation [5] and also has some resemblance to IDW interpolation [28].", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "This approach is close to RBF interpolation [5] and also has some resemblance to IDW interpolation [28].", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Earlier version of the same method was also proposed in [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "[24] proposed A-tSNE - an improvement over tSNE for progressive visual analytics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] also included procedure for removing and adding point to tSNE based", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] proposed mTSNE algorithm, where the authors created single visualization for the entire set of time series, using time series distance metrics to produce P matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "For local interpolation here we use inverse distance weighting (IDW) [28].", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "RBF interpolation [5] and some approximation methods are plausible substitutes of IDW in LION-tSNE.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "Buhmann [5] provides a thorough introduction to radial basis functions.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "it accepts multidimensional input, but depends only on distance between the input argument and some reference point [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Coefficients \u03bbi can be found by solving the system of linear equations (SLE) 10, and under some conditions invertibility of SLE matrix is guaranteed [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 10, "context": "For that purpose neural networks [12] are a natural choice.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "\u2022 For activation function we tested rectified linear units (ReLu, see [22]) ReLu(x) = max(0, x) and hyperbolic tangent tanh(x) = e x\u2212e\u2212x ex+e\u2212x .", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "\u2022 In order to avoid overfitting, we used dropout regularization [29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "For example, if input x is an image, convolutional neural networks [16] might be fitting choice.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "al [11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Also we compare LION-tSNE with parametric tSNE approach described by van der Maaten [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "RBM are undirected multilayer models used, among other applications, as the earliest deep learning models (see [13]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "al [11] with K equal to 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "\u2022 Parametric tSNE as described by van der Maaten [19], using 60000 samples of non-processed 784-dimensional tSNE and 3 hidden layers of restricted Boltzmann machines of 500, 500 and 200 nodes per layer respectively.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Kernelized tSNE described in [11] showed near-baseline accuracy for properly chosen K parameters.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Kernelized tSNE [11] showed excellent accuracy on previous test, but in this test it placed most outliers firmly within existing clusters (see figure 20).", "startOffset": 16, "endOffset": 20}, {"referenceID": 16, "context": "One more prominent method, to which LION-tSNE was not yet compared, is parametric tSNE [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "RBMs are the earliest version of deep learning models [13], and they often require a lot of data to be trained.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "The next section describes application of LIONtSNE to larger and higher-dimensional datasets using parametric tSNE [19] as", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "This layer configuration was used for evaluation by parametric tSNE author [19], source code of parametric tSNE and the training/test dataset was made available by the original algorithm author.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "Straightforward implementation of tSNE algorithm is has quadratic complexity in number of points [20] (if dimensionality is taken into account, O(NK)), and it is the upper bound of repeated gradient descent complexity.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "Neural network approximation and restricted Boltzmann machines for parametric tSNE [19] require upfront training using backpropagation.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "- Using tSNE time series visualization approaches (like points on a single tSNE plot [23] or like a series of plots [25]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "- Using tSNE time series visualization approaches (like points on a single tSNE plot [23] or like a series of plots [25]).", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Search for nearest neighbors in a certain fixed radius is a recognized problem on its own [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "Also fixed radius nearest neighbors search can be approximated with fast K nearest neighbors search techniques [8] with sufficient number of neighbors and additional check whether the distance is small enough.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Another option is to use orthogonal range search [4], i.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing highdimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots). In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation with Outlier coNtrol) a novel approach for incorporating new data into tSNE representation. LION-tSNE is based on local interpolation in the vicinity of training data, outlier detection and a special outlier mapping algorithm. We show that LION-tSNE method is robust both to outliers and to new samples from existing clusters. We also discuss multiple possible improvements for special cases. We compare LION-tSNE to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descent for new data, and neural network approximation.", "creator": "LaTeX with hyperref package"}}}