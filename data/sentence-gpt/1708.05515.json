{"id": "1708.05515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Syllable-level Neural Language Model for Agglutinative Language", "abstract": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes. We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16 more inferences, which have recently been reported in language models which use the most common inferences in the English spoken world.\n\n\n\nThe first and most frequent inferences in English speak primarily in languages of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English, there is a wide range of other inferences such as these. The first inferences in English speak mainly in language of sub-categories (e.g., words), which are more frequent in English. In English,", "histories": [["v1", "Fri, 18 Aug 2017 06:02:16 GMT  (1451kb)", "http://arxiv.org/abs/1708.05515v1", "Accepted at EMNLP 2017 workshop on Subword and Character level models in NLP (SCLeM)"]], "COMMENTS": "Accepted at EMNLP 2017 workshop on Subword and Character level models in NLP (SCLeM)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["seunghak yu", "nilesh kulkarni", "haejun lee", "jihie kim"], "accepted": false, "id": "1708.05515"}, "pdf": {"name": "1708.05515.pdf", "metadata": {"source": "CRF", "title": "Syllable-level Neural Language Model for Agglutinative Language", "authors": ["Seunghak Yu", "Nilesh Kulkarni", "Haejun Lee Jihie Kim"], "emails": ["jihie.kim}@samsung.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n05 51\n5v 1\n[ cs\n.C L\n] 1\n8 A\nug 2\n01 7\nguages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes.We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized."}, {"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) exhibit dynamic temporal behavior which makes them ideal architectures to model sequential data. In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (Mikolov et al., 2010; Lin et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016). RNN-LMs model the probability distribution over the words in vocabulary conditioned on a given input context. The sizes of these networks are primarily dependent on their vocabulary size.\nSince agglutinative languages, such as Korean, Japanese, and Turkish, have a huge number of words in the vocabulary, it is considerably hard to train word-level RNN-LM. Korean is agglutinative in its morphology; words mainly contain different morphemes to determine the meaning of the word hence increasing the vocabulary size for language model training. A given word in Korean\n\u2217 Equal contribution\ncould have similar meaning with more than 10 variations in the suffix as shown in Table 1.\nVarious language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015). (Chen et al., 2015b) explored the idea of joint training for character and word embedding. Morpheme based segmentation has been explored in both Large Vocabulary Continuous Speech Recognition (LVCSR) tasks for Egyptian Arabic (Mousa et al., 2013) and German newspaper corpus (Cotterell and Schu\u0308tze, 2015). (Sennrich et al., 2015) used subword units to perform machine translation for rare words.\nMorpheme distribution has a relatively smaller frequency tail as compared to the word distribution from vocabulary, hence avoids over-fitting for tail units. However, even with morpheme segmentation the percentage of out-of-vocabulary (OOV) words is significantly high in Korean. Character embedding in Korean is unfeasible as the context of the word is not sufficiently captured by the long sequence which composes the word. We select as features syllable-level embedding which has shorter sequence length and morpheme-level embedding to capture the semantics of the word.\nWe deploy our model for input word prediction on mobile devices. To achieve desirable performance we are required to create a model that has as small as possible memory and CPU footprint without compromising its performance. We use differentiated softmax (Chen et al., 2015a) for the output layer. This method uses more parameters for the words that are frequent and less for the ones that occur rarely. We achieve better performance than existing approaches in terms of Key Stroke Savings (KSS) (Fowler et al., 2015) and our approach has been commercialized."}, {"heading": "2 Proposed Method", "text": "Following sections propose a model for agglutinative language. In Section 2.1 we discuss the basic architecture of the model as detailed in Figure 1, followed by Section 2.2 that describes our embeddings. In Section 2.3 we propose an adaptation of differentiated softmax to reduce the number of model parameters and improve computation speed."}, {"heading": "2.1 Language Model", "text": "Overall architecture of our language model consists of a) embedding layer, b) hidden layer, c) softmax layer. Embedding comprises of syllablelevel and morpheme-level embedding as described in Section 2.2. We combine both embedding features and pass them through a highway network (Srivastava et al., 2015) which act as an input to the hidden layers. We use a single layer of LSTM as hidden units with architecture similar to the non-regularized LSTM model by (Zaremba et al., 2014). The hidden state of the LSTM unit is affinetransformed by the softmax function, which is a probability distribution over all the words in the output vocabulary."}, {"heading": "2.2 Syllable & Morphological Embedding", "text": "We propose syllable-level embedding that attenuates OOV problem. (Santos and Zadrozny, 2014; Kim et al., 2016) proposed character aware neural networks using convolution filters to create character embedding for words. We use convolution neural network (CNN) based embedding method to get syllable-level embedding for words. We use 150 filters that consider uni, bi, tri and quad syllable-grams to create a feature representation for the word. This is followed by max-pooling to concatenate the features from each class of filters resulting in a syllable embedding representation\nfor the word. Figure 2 in the left half shows an example sentence embedded using the syllable-level embedding.\nFigure 3 highlights the difference between various embedding and the features they capture. The syllable embedding is used along with a morphological embedding to provide richer features for the word. The majority of words (95%) in Korean has at most three morphological units. Each word can be broken into start, middle, and end unit. We embed each morphological unit by concatenating to create a joint embedding for the word. Advantage of morphological embedding over syllable is all the sub-units have an abstract value in the language and this creates representation for words relying on the usage of these morphemes. Both morphological and syllable embeddings are concatenated and fed through a highway network (Srivastava et al., 2015) to get a refined representation for the word as shown in the embedding layer for Figure 1."}, {"heading": "2.3 Differentiated Softmax", "text": "The output layer models a probability distribution over words in vocabulary conditioned on the given context. There is a trade-off between required memory and computational cost which determines the level of prediction. To generate a complete word, using morpheme-level predictions requires beam search which is expensive as compared to word-level predictions. Using beam search to predict the word greedily does not adhere to the com-\nputational requirements set forth for mobile devices. Thus, we have to choose word-level outputs although it requires having a vocabulary of over 0.2M words to cover 95% of the functional word forms. Computing a probability distribution function for 0.2M classes is computational intensive and overshoots the required run-time and the allocated memory to store the model parameters.\nTherefore, the softmax weight matrix, Wsoftmax, needs to be compressed as it is contributing to huge model parameters. We initially propose to choose an appropriate rank for the Wsoftmax in the following approximation problem; Wsoftmax = WA \u00d7WB , whereWA and WB have ranks less than r. We extend the idea of low rank matrix factorization in (Sainath et al., 2013) by further clustering words into groups and allowing a different low rank r\u2032 for each cluster. The words with high frequency are given a rank, r1, such that r1 \u2265 r2 where r2 is the low rank for the words with low frequency. The core idea being, words with higher frequency have much richer representation in higher dimensional space, whereas words with low frequency cannot utilize the higher dimensional space well.\nWe observe that 87% of the words appear in the tail of the distribution by the frequency of occurrence. We provide a higher rank to the top 2.5% words and much lower rank to the bottom 87%. This different treatment reduces the number of pa-\nrameters and leads to better modeling."}, {"heading": "3 Experiment Results", "text": ""}, {"heading": "3.1 Setup", "text": "We apply our method to web crawled dataset consisting on news, blogs, QA. Our dataset consists of over 100M words and over 10M sentences. For morpheme-level segmentation, we use lexical analyzer and for syallable-level we just syllabify the dataset. We empirically test our model and its input vocabulary size is around 20K morphemes and 3K syllables. The embedding size for morpheme is 52 and that for syllable is 15. We use one highway layer to combine the embeddings from syllable and morpheme. Our hidden layer consists of 500 LSTM units. The differentiated softmax outputs the model\u2019s distribution over the 0.2M words in the output vocabulary with top 5K (by frequency) getting a representation dimension (low rank in Wsoftmax) of 152, next 20K use a representation dimension of 52 and the rest 175K get a representation dimension of 12. All the compared models have word level outputs and use differentiated softmax."}, {"heading": "3.2 Comparison of embedding methods", "text": "We randomly select 10% of our crawled data (10M words, 1M sentences) to compare embedding methods as shown in Table 2. We test character, syllable, morpheme and word-level embeddings. The word-level embedding has the highest number of parameters but has the worst performance. As expected breaking words into their subforms improves the language model. However, our experiment reaches its peak performance when we use syllable level embeddings. To improve the performance even further we propose using syllable\nand morpheme which outperforms all the other approaches in terms of perplexity."}, {"heading": "3.3 Performance evaluation", "text": "Proposed method shows the best performance compared to other solutions in terms of Key Stroke Savings (KSS) as shown in Table 3. KSS is a percentage of key strokes not pressed compared to a vanilla keyboard which does not have any prediction or completion capabilities. Every user typed characters using the predictions of the language model counts as key stroke saving. The dataset1 used to evaluate KSS was manually curated to mimic user keyboard usage patterns.\nThe results in Table 3 for other commercialized solutions are manually evaluated due to lack of access to their language model. We use three evaluators from inspection group to cross-validate the results and remove human errors. Each evaluator performed the test independently for all the other solutions to reach a consensus. We try to minimize user personalization in predictions by creating a new user profile while evaluating KSS.\nThe proposed method shows 37.62% in terms of KSS and outperforms compared solutions. We have achieved more than 13% improvement over the best score among existing solutions which is 33.20% in KSS. If the user inputs a word with our solution, we require on an average 62.38% of the word prefix to recommend the intended word, while other solutions need 66.80% of the same. Figure 4 shows an example of word prediction across different solutions. In this example, the predictions from other solutions are same irrespective of the context, while the proposed method treats\n1The dataset consists of 67 sentences (825 words, 7,531 characters) which are collection of formal and informal utterances from various sources. It is available at https://github.com/meinwerk/SyllableLevelLanguageModel\nthem differently with appropriate predictions."}, {"heading": "4 Conclusion", "text": "We have proposed a practical method for modeling agglutinative languages, in this case Korean. We use syllable and morpheme embeddings to tackle large portion of OOV problem owing to practical limit of vocabulary size and word-level prediction with differentiated softmax to compress size of model to a form factor making it amenable to running smoothly on mobile device. Our model has 9.50M parameters and achieves better perplexity than character-level embedding by 16.87. Our proposed method outperforms the existing commercialized keyboards in terms of key stroke savings and has been commercialized. Our commercialized solution combines above model with n-gram statistics to model user behavior thus supporting personalization."}], "references": [{"title": "Strategies for training large vocabulary neural language models", "author": ["Welin Chen", "David Grangier", "Michael Auli."], "venue": "arXiv preprint arXiv:1512.04906 .", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Joint learning of character and word embeddings", "author": ["Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huanbo Luan."], "venue": "Twenty-Fourth International Joint Conference on Artificial Intelligence.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Language modelling for turkish as an agglutinative language", "author": ["T Ciloglu", "M Comez", "S Sahin."], "venue": "Signal Processing and Communications Applications Conference, 2004. Proceedings of the IEEE 12th. IEEE, pages 461\u2013462.", "citeRegEx": "Ciloglu et al\\.,? 2004", "shortCiteRegEx": "Ciloglu et al\\.", "year": 2004}, {"title": "Morphological word-embeddings", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "HLT-NAACL. pages 1287\u20131292.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2015}, {"title": "Learning effective word embedding using morphological word similarity", "author": ["Qing Cui", "Bin Gao", "Jiang Bian", "Siyu Qiu", "Tie-Yan Liu."], "venue": "arXiv preprint arXiv:1407.1687 .", "citeRegEx": "Cui et al\\.,? 2014", "shortCiteRegEx": "Cui et al\\.", "year": 2014}, {"title": "Effects of language modeling and its personalization on touchscreen typing performance", "author": ["Andrew Fowler", "Kurt Partridge", "Ciprian Chelba", "Xiaojun Bi", "Tom Ouyang", "Shumin Zhai."], "venue": "Proceedings of the 33rd Annual ACMConference on Human Fac-", "citeRegEx": "Fowler et al\\.,? 2015", "shortCiteRegEx": "Fowler et al\\.", "year": 2015}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "Thirtieth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li."], "venue": "EMNLP. pages 899\u2013907.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech. volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "HaiSon Le", "Stefan Kombrink", "Jan Cernocky."], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf) .", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Gated word-character recurrent language model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1606.01700 .", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "Morpheme-based feature-rich language models using deep neural networks for lvcsr of egyptian arabic", "author": ["Amr El-Desoky Mousa", "Hong-Kwang Jeff Kuo", "Lidia Mangu", "Hagen Soltau."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE", "citeRegEx": "Mousa et al\\.,? 2013", "shortCiteRegEx": "Mousa et al\\.", "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran."], "venue": "Acoustics, Speech and Signal Processing", "citeRegEx": "Sainath et al\\.,? 2013", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D Santos", "Bianca Zadrozny."], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14). pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387 .", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP. pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (Mikolov et al., 2010; Lin et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).", "startOffset": 163, "endOffset": 245}, {"referenceID": 7, "context": "In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (Mikolov et al., 2010; Lin et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).", "startOffset": 163, "endOffset": 245}, {"referenceID": 6, "context": "In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (Mikolov et al., 2010; Lin et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).", "startOffset": 163, "endOffset": 245}, {"referenceID": 11, "context": "In recent times, RNNs have shown state of the art performance on tasks of language modeling (RNN-LM), beating the statistical modeling techniques by a huge margin (Mikolov et al., 2010; Lin et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).", "startOffset": 163, "endOffset": 245}, {"referenceID": 2, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 4, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 6, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 10, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 18, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 8, "context": "Various language modeling methods that rely on character or morpheme like segmentation of words have been developed (Ciloglu et al., 2004; Cui et al., 2014; Kim et al., 2016; Mikolov et al., 2012; Zheng et al., 2013; Ling et al., 2015).", "startOffset": 116, "endOffset": 235}, {"referenceID": 1, "context": "(Chen et al., 2015b) explored the idea of joint training for character and word embedding.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Morpheme based segmentation has been explored in both Large Vocabulary Continuous Speech Recognition (LVCSR) tasks for Egyptian Arabic (Mousa et al., 2013) and German newspaper corpus (Cotterell and Sch\u00fctze, 2015).", "startOffset": 135, "endOffset": 155}, {"referenceID": 3, "context": ", 2013) and German newspaper corpus (Cotterell and Sch\u00fctze, 2015).", "startOffset": 36, "endOffset": 65}, {"referenceID": 15, "context": "(Sennrich et al., 2015) used subword units to perform machine translation for rare words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "We use differentiated softmax (Chen et al., 2015a) for the output layer.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "We achieve better performance than existing approaches in terms of Key Stroke Savings (KSS) (Fowler et al., 2015) and our approach has been commercialized.", "startOffset": 92, "endOffset": 113}, {"referenceID": 16, "context": "We combine both embedding features and pass them through a highway network (Srivastava et al., 2015) which act as an input to the hidden layers.", "startOffset": 75, "endOffset": 100}, {"referenceID": 17, "context": "We use a single layer of LSTM as hidden units with architecture similar to the non-regularized LSTM model by (Zaremba et al., 2014).", "startOffset": 109, "endOffset": 131}, {"referenceID": 14, "context": "(Santos and Zadrozny, 2014; Kim et al., 2016) proposed character aware neural networks using convolution filters to create character embedding for words.", "startOffset": 0, "endOffset": 45}, {"referenceID": 6, "context": "(Santos and Zadrozny, 2014; Kim et al., 2016) proposed character aware neural networks using convolution filters to create character embedding for words.", "startOffset": 0, "endOffset": 45}, {"referenceID": 16, "context": "Both morphological and syllable embeddings are concatenated and fed through a highway network (Srivastava et al., 2015) to get a refined representation for the word as shown in the embedding layer for Figure 1.", "startOffset": 94, "endOffset": 119}, {"referenceID": 13, "context": "We extend the idea of low rank matrix factorization in (Sainath et al., 2013) by further clustering words into groups and allowing a different low rank r for each cluster.", "startOffset": 55, "endOffset": 77}], "year": 2017, "abstractText": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes.We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.", "creator": "LaTeX with hyperref package"}}}