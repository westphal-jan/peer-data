{"id": "1708.06039", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "abstract": "The increasing availability of affect-rich multimedia resources has bolstered interest in understanding sentiment and emotions in and from visual content. Adjective-noun pairs (ANP) are a popular mid-level semantic construct for capturing affect via visually detectable concepts such as \"cute dog\" or \"beautiful landscape\". Current state-of-the-art methods approach ANP prediction by considering each of these compound concepts as individual tokens, ignoring the underlying relationships in ANPs. This work aims at disentangling the contributions of the `adjectives' and `nouns' in the visual prediction of ANPs. Two specialised classifiers, one trained for detecting adjectives and another for nouns, are fused to predict 553 different ANPs. The resulting ANP prediction model is more interpretable as it allows us to study contributions of the adjective and noun components. Source code and models are available at the NPN and PNAS; they are distributed across multiple languages. In addition, the computational resources required to develop a model can be used to further interpret the effects of multiple ANPs. The ANP model was developed to predict the impact of the lexical and semantic elements on the corpus in which words are spoken, written, or spoken, respectively. The model for identifying adjectives and noun elements is available on Github ( https://github.com/neol-c.co.uk/neol-c.co.uk/project/neol-c.co.uk/tree ) and on GitHub ( https://github.com/neol-c.co.uk/neol-c.co.uk/tree ) or on GitHub ( https://github.com/neol-c.co.uk/neol-c.co.uk/tree ) and on GitHub ( https://github.com/neol-c.co.uk/neol-c.co.uk/tree ) in a single classifier. The model of the ANP (https://github.com/neol-c.co.uk/neol-c.co.uk/tree ) and on GitHub ( https://github.com/neol-c.co.uk/neol-c.co.uk/tree ) are distributed in two packages. The package is distributed in two packages: the default, the default, and the default, respectively. The classifier has three options:", "histories": [["v1", "Mon, 21 Aug 2017 00:33:05 GMT  (9006kb,D)", "http://arxiv.org/abs/1708.06039v1", "Oral paper at ACM Multimedia 2017 Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes (MUSA2)"]], "COMMENTS": "Oral paper at ACM Multimedia 2017 Workshop on Multimodal Understanding of Social, Affective and Subjective Attributes (MUSA2)", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["delia fernandez", "alejandro woodward", "victor campos", "xavier giro-i-nieto", "brendan jou", "shih-fu chang"], "accepted": false, "id": "1708.06039"}, "pdf": {"name": "1708.06039.pdf", "metadata": {"source": "META", "title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "authors": ["D\u00e8lia Fern\u00e1ndez", "Alejandro Woodward", "V\u00edctor Campos", "Brendan Jou", "Shih-Fu Chang", "Xavier Gir\u00f3-i-Nieto"], "emails": ["delia@vilynx.com", "alejandro.woodward@alu-etsetb.upc.", "victor.campos@bsc.es", "xavier.giro@upc.edu", "bjou@ee.columbia.edu", "sfchang@ee.columbia.edu", "permissions@acm.org."], "sections": [{"heading": "CCS CONCEPTS", "text": "\u2022 Information systems\u2192Multimedia information systems; \u2022 Computing methodologies\u2192 Scene understanding;"}, {"heading": "KEYWORDS", "text": "affective computing, convolutional neural networks, compound concepts, adjective noun pairs, interpretable models"}, {"heading": "ACM Reference format:", "text": "D\u00e8lia Fern\u00e1ndez, Alejandro Woodward, V\u00edctor Campos, Xavier Gir\u00f3-i-Nieto, Brendan Jou, and Shih-Fu Chang. 2017. More cat than cute? Interpretable Prediction of Adjective-Noun Pairs. In Proceedings of MUSA2\u201917, Mountain View, CA, USA, October 27, 2017, 9 pages. https://doi.org/10.1145/3132515.3132520\n\u2217Work partially developed while D\u00e8lia Fern\u00e1ndez was a visiting scholar at Columbia University.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MUSA2\u201917, October 27, 2017, Mountain View, CA, USA \u00a9 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5509-4/17/10. . . $15.00 https://doi.org/10.1145/3132515.3132520"}, {"heading": "1 INTRODUCTION", "text": "Computers are acquiring increasing ability for understanding high level visual concepts such as objects and actions in images and videos, but often lack an affective comprehension of such content. Technologies have largely obviated emotion from data, while neurology demonstrates how emotions are fundamental to human experience by influencing cognition, perception and everyday tasks such as learning, communication and decision-making [18].\nDuring the last decade, with the growing availability and popularity of opinion-rich resources such as social networks, the interest\nar X\niv :1\n70 8.\n06 03\n9v 1\n[ cs\n.C V\n] 2\n1 A\nug 2"}, {"heading": "MUSA2\u201917, October 27, 2017, Mountain View, CA, USA D. Fern\u00e1ndez et al.", "text": "on the computational analysis of sentiment has increased. Every day, Internet users post and share billions of multimedia content in online platforms to express sentiment and opinions about several topics [13], which has motivated research on automated affect understanding for large-scale multimedia [3, 12, 16]. The ability of analyzing and understanding this kind of information opens the door to behavioral sciences and applications such as brand monitoring or advertisement effect [22].\nOne of the main challenges for automated affect understanding in visual media is overcoming the affective gap between low-level visual features and high-level affective semantics. Such task goes beyond overcoming the semantic gap, i.e. recognizing the objects in an image, and poses a challenging task in computer vision. In [3], adjective-noun pair (ANP) semantics are proposed as a midlevel representation that convey strong affective content while being visually detectable by traditional computer vision methods, e.g. \u201chappy dog\", \u201cmisty morning\" or \u201clittle girl\".\nIt has been argued that the noun in an ANP grounds the visual appearance of the concept, while the adjective works as a bias carrying most of the conveyed affect [16]. However, we hypothesize that for some ANPs the adjective may carry most of the visual cues that are key for its detection, as depicted by the examples in Figure 1. While the most salient visual cues for \u201ccute dog\u201d or \u201cdelicious cupcake\u201d are related to \u201cdog\u201d and \u201ccupcake\u201d, we expect that in other cases such as \u201cdark night\u201d or \u201cbright day\u201d the visual features related to \u201cdark\u201d and \u201cbright\u201d to contributemore in the detection of the ANP. The analysis developed in this paper allows classifying between noun or adjective oriented ANPs by comparing the contribution of the adjective and noun concept in the final prediction.\nThe examples depicted in Figure1 point at a second factor influencing the relative contributions between adjective and nouns. If focused on the nouns only, it can be observed that some of them are related to the objects depicted in the images (eg. dog and cupcake), while other nouns are more related to the whole scene represented by the image (eg. night and day). Our analysis will also discuss the behavior of adjective and noun contributions from this perspective.\nThe prediction of these adjective and noun structured labels has been traditionally addressed with single branch classifiers, ignoring the particular structure of these pairs. In order to verify our hypothesis, we propose fusing specialized adjective and noun detectors and then analyze their contribution by means of state of the art methods. The proposed two-stage training process allows to decompose the decision of the final classifier in terms of the contribution of different understandable concepts, i.e. the adjectives and nouns in the dataset. Given these contribution results, a thorough analysis is performed in order to understand how the classifier leverages the information coming from each of the branches and shed some light into the particularities of the ANP detection task.\nThe contributions of this work include (1) a new ANP classifier architecture that provides comparable performance to the state of the art results while allowing interpretability, (2) a method to evaluate adjective and noun contributions on ANP prediction, and (3) an extended analysis on the contributions from a numerical and semantical point of view.\nThis paper is structured as follows. Section 2 reviews the related models in ANP prediction and the previous works on decomposing this task into adjective and noun classification. We present our\ninterpretable model ANPnet in Section 3, which is trained with the dataset and parameters described in Section 4. Accuracy results are presented in Section 5, while the contributions in terms of adjective and nouns are discussed in 6. Final conclusions and discussions are contained in Section 7. Source code and models are available at https://imatge-upc.github.io/affective-2017-musa2/."}, {"heading": "2 RELATEDWORK", "text": "Automated sentiment and emotion detection from visual media has received increasing interest by the multimedia community during the past years. These tasks have been addressed with traditional low-level feature based techniques, such as color features [11], SIFT-based Bag of Words [19] and aesthetic effects [25]. Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].\nMethods for detecting adjective-noun pairs (ANP) have strongly relied on state-of-the-art vision algorithms. Similarly to other computer vision tasks, early approaches based on handcrafted features [2] were soon replaced with CNNs [5, 15, 16]. CNNs have proven their efficiency for large-scale image datasets [9, 10, 17, 23]. DeepSentiBank [5] presented the first application of CNNs for ANP prediction. MVSO detector-banks [15] showed performance improvement by using a more modern architecture, GoogLeNet [23], which also reduced the amount of parameters of the model.\nThe multi-task nature of the ANP detection task has been exploited by using a fan-out architecture, where a first set of layers is shared for all tasks, and then splits in different network heads that specialize on each task [14]. Inter-task influence is increased through the use of cross-residual connections between the different heads. Despite this approach improves on the single-task models, the hierarchical structure of ANPs is not explicitly encoded in the architecture and the influence of the adjective and noun branches on the ANP detection lacks interpretability as compared to the approaches presented in this work.\nFactorized nets [21] explicitly leverage the hierarchical nature of ANPs in the model architecture. Decomposing the ANP detection task into factorized adjective and noun classification problems allows the model to classify unseen adjective and noun combinations that are not available in the training set. However, the use of an M-dimensional latent space for adjectives and nouns complicates the interpretability of their combination in terms of understandable semantic concepts.\nThe task of sentiment analysis is addressed with Deep Coupled Adjective and Noun neural networks (DCAN) [24] by learning a mid-level visual representation from two convolutional networks jointly trained with VSO. One of this networks is specialized in adjectives and the other one in nouns. The learned representations shows superior performance in the task of sentiment analysis, but does not provide an interpretation about which concepts triggered the predictions.\nThe model proposed in this work follows a fan-in architecture which allows to understand and decompose the final classification in terms of the contribution of specific adjectives and nouns.\nMore cat than cute? Interpretable Prediction of Adjective-Noun Pairs MUSA2\u201917, October 27, 2017, Mountain View, CA, USA"}, {"heading": "3 ANPNET", "text": "This section presents ANPnet, an interpretable architecture for ANP detection constructed by fusing the outputs of two specialized networks for adjective and noun classification. Given an input image, x , we estimate its corresponding Adjective, Noun and ANP labels, yad j , ynoun and yANP , as\ny\u0302ad j = fad j (x) (1) y\u0302noun = fnoun (x) (2)\ny\u0302ANP = \u0434 ( y\u0302ad j , y\u0302noun ) (3)\nwhere fad j , fnoun and \u0434 are parametrized by neural networks, namely AdjNet, NounNet and the Fusion network. We aim at studying the contribution of the different adjectives and nouns by analyzing the behavior of \u0434 with respect to its inputs. The method for computing such contributions is described in Section 6.\nThe architecture for the specialized networks are based on the well-known ResNet-50 model [9]. Residual Networks are convolutional neural network architectures that introduce residual functions with reference to the layer inputs, achieving better results than their non-residual counterparts. All residual layers use \u201cB option\u201d shortcut connections as described in [9], where projections are only used when matching dimensions (1 \u00d7 1 convolutions with stride 2) and other shortcuts are identity. This architecture represents a good trade-off between classification accuracy and computational cost. Besides, it allows comparison in terms of accuracy with the ResNet-50 network trained for ANP detection in [14].\nThe last layer in ResNet-50, originally predicting the 1,000 classes in ImageNet, is replaced in AdjNet and NounNet to predict, respectively, the adjectives and nouns considered in our dataset. Each of these two networks is trained separately with the same images and the corresponding adjective or noun labels. The probability ouputs of AdjNet and NounNet are fused by means of a fully connected neural network with a ReLU non-linearity. On top of that, a softmax linear classifier predicts the detection probabilities for each ANP class considered in the model.\nFigure 2 depicts the deeper layers of ANPnet, showing how AdjNet and NounNet are fused by a fully connected layer of 1,024 neurons. This size is chosen to allow the network to learn sparse\nrepresentations between the neighboring layers of smaller sizes. Inputs to the fusion layer are whitened by computing the mean and standard deviation for all the samples in the training set. The number of output neurons in AdjNet and NounNet is determined by the number of adjective and noun classes in our dataset, which were 117 and 167 respectively.\nNext sections present how ANPNet was trained to simultaneously provide competitive and interpretable results."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "The ANPnet network presented in Section 3 was trained with a subset of the Visual Sentiment Ontology (VSO) [3] dataset. Firstly, AdjNet and NounNet were trained independently for adjective and noun predictions, respectively, and in a second stage the fusion layers were trained to predict the ANPs in the input image. This section describes in detail the dataset used and the training parameters of the whole architecture."}, {"heading": "4.1 Dataset", "text": "The presented work uses a subset of the Visual Sentiment Ontology (VSO) [3] dataset, the same part used in [14] to facilitate the comparison in terms of accuracy.\nThe original VSO dataset contains over 1200 different ANPs and about 500k images retrieved from the social multimedia platform Flickr 1. Those images were found by querying Flickr search engine with keywords from the Plutchik\u2019s Wheel of Emotions [7], a wellknown emotion model derived from psychological studies. This wheel contains 24 basic emotions, such as joy, fear or anger. The discovery of affective-related ANPs was based on their co-occurrences with the emotion tags. The initial list of ANP candidates was manually filtered in order to ensure semantics correctness, sentiment strength and popular usage on Flickr. Finally, each resulting ANP concepts was used to query again Flickr and build this way a collection of images and metadata associated to the ANP.\nThe full VSO dataset presents certain limitations already pointed out in [14]. First, some adjective-noun pair concepts are singletons and do not share any adjectives or nouns with other concept pairs. Also, some nouns are massively over-represented and there are far less adjectives to compose the adjective-noun pairs. Due to these drawbacks, our experiments are based on a subset of VSO build according to the more restrictive constraints proposed in [14]. The considered subset of ANPs satisfy the following conditions: (1) the adjective is paired with at least two more different nouns, (2) nouns that are not overwhelmingly biasing and abstract, and (3) all ANPs are related with 500 or more images. The final VSO subset contains 167 nouns and 117 adjectives that form 553 adjective-noun pairs over 384,258 Flickr images. A stratified 80-20 split is performed, resulting in 307,185 images for training and 77,073 for test. The partition used in our experiments is the same for which results in [14] are reported2.\n1https://www.flickr.com 2Dataset splits were obtained through personal communication with the authors of [14]."}, {"heading": "MUSA2\u201917, October 27, 2017, Mountain View, CA, USA D. Fern\u00e1ndez et al.", "text": ""}, {"heading": "4.2 Training", "text": "All CNNs were trained using stochastic gradient descent with momentum of 0.9, on batches of 128 samples and a learning rate of 0.01. Data augmentation, consisting in random crops and/or horizontal flips on the input images, together with \u21132-regularization with a weight decay rate of 10\u22124 is used in order to reduce overfitting. When possible, layers were initialized using weights from a model pre-trained on ImageNet [6]. Otherwise, weights were initialized following the method proposed by Glorot and Bengio [8] and the initial value for the biases was set to 0.\nThe training is performed in two stages. First, AdjNet and NounNet are trained independently for the tasks of adjective and noun classification, respectively. The learned weights are then frozen in order to train the fusion network on top of the specialized CNNs. Thanks to this two-step training strategy, the inputs to the fusion layer become an intermediate and semantically interpretable representation.\nAll experiments were run using two NVIDIA GeForce GTX Titan X GPUs and implemented with TensorFlow [1]."}, {"heading": "5 ANP PREDICTION", "text": "The performance of our interpretable model in terms of ANP detection is presented in Table 1. This table shows a decrease of performance in terms of accuracy of ANPnet with respect to a ResNet-50 fine-tuned end-to-end for ANP prediction. The table includes the results of two ResNet-50, the ones published in [14] and new ones obtained with the training parameters described in Section 4.2. The similar values obtained by our model with respect to the ones reported in [14] confirm that the training hyperparameters were\nappropriate. According to the baseline set by our ResNet-50 for ANP prediction, the loss of accuracy associated building ANPNet is of 3.8% for top-1 accuracy and 4.7% for top-5.\nThe loss is also present when comparing ANPnet with a noninterpretable version of the same architecture for which the output layers of AdjNet and NounNet are also initialized randomly and trained. In this case the drop in accuracy is of 2.2% when considering top-1 and 2.5% for top-5. With this setup, the network is not forced to use adjective and noun probabilities as an intermediate representation and has additional degrees of freedom to optimize for the target task of ANP classification. The decrease in accuracy of the interpretable model with respect to this latter configuration is then expected. These results quantize the price to pay in terms of accuracy for making our model interpretable.\nA more general overview of the ANPnet performance in terms of top-5 accuracy is presented in Figure 4 as the histogram of values considering the full test dataset. The distribution of accuracies across the different ANPs follows a Gaussian-like distribution centered around the average score of 43.28%.\nThe results in Table 1 show both accuracy in terms of top-1 and top-5 because ANP prediction can be highly affected by synonyms.\nMore cat than cute? Interpretable Prediction of Adjective-Noun Pairs MUSA2\u201917, October 27, 2017, Mountain View, CA, USA\nAdjective concepts such as \u201csmiling\u201d and \u201chappy\u201d, or noun concepts like \u201ccat\u201d and \u201canimal\u201d are considered absolutely different in the accuracy metric, so relaxing its detection by considering the top-5 predictions may provide a metric that expresses better the obtained results [15]. This observation motivates the use of top-5 accuracy as the referencemetric for evaluating the correctness of the predictions in the remaining sections of this work.\nThe accuracy results in Table 1 also show how adjectives are more difficult to detect than nouns, as accuracy values are lower, even distinguishing among a larger number of classes. There are two reasons for this gap of performance. The first one is that adjectives usually describe more abstract concepts than nouns, with a larger associated visual variance. For example, there may be a wide range of visual features required to describe the concept \u201chappy\u201d. The second reason is that ResNet-50 was initially trained for object classification on ImageNet, a type of concepts that are associated to nouns.\nA closer look at the results allows to distinguish which of the 553 considered ANPs can be better detected and which ones present more problems. Table 2 presents the ANPs with best and worst top5 accuracy predictions with ANPNet, comparing their accuracy per ANP with the individual accuracies of their composing adjective and nouns. Qualitative results of two of the best and worst detected ANPs are depicted in Figure 3.\nThese results show how the best detected ANPs correspond to object-oriented nouns, i.e. well defined entities with a low variation from a visual perspective and usually represented by a localized region withing the image. These would the the cases of \u201criver\u201d, \u201cdeer\u201d or \u201cmushrooms\u201d. On the other hand, the worst predictions are associated to scene-oriented ANPs, which depict more abstract concepts and thus have a larger visual variance, such as \u201cplaces\u201d, \u201cview\u201d or \u201cscene\u201d itself. The top-5 accuracy tends to be significantly better in the case of object-oriented nouns than in the case of sceneoriented ones. This difference in performance may be related to using a ResNet-50 pre-trained on the ImageNet dataset for AdjNet and NounNet. ImageNet is a dataset build for object classification, so our model is more specialized in these type of classes than those related to scenes [28].\nANPNet allows a finer analysis under the form of a co-detection matrix. The contents of the matrix provide the percentage of images for which the adjective, noun or ANP are correctly detected (columns) among those images for which the adjective, noun or ANP has been correctly predicted (rows). As previously stated, a detection is considered correct when the ground truth label is among the top-5, whether adjective, noun or ANP. The diagonal of the co-detection matrix corresponds to 100%, as it represents the ratio of the whole set of detected adjective, noun or ANP with respect to itself. The rest of values do not necessarily need to meet the 100% as, for example, a correct detection of an ANP among the top-5 predictions does not also imply that the composing adjective or noun was among the top-5 predicted adjectives or nouns. The co-detection matrix allows to study the correlation between ANPs and their composing parts, providing insights about how a correct detection of adjectives and nouns is related to a correct detection of an ANP.\nThe co-detection matrix of ANPNet is presented in Table 3. Its results indicate that the detection of an ANP also implies in many\ncases a correct detection of the adjective, in 87.47% of the cases, and of the noun, in 95.76% of the cases. On the other hand, a correct detection of the adjective or noun does not necessarily imply a correct detection of the ANP, as the ANP top-5 accuracy for these cases is 66.38% for adjectives and 59.87% for nouns.\nIn addition, the matrix also indicates that a detection of the adjective is also related to a correct prediction of the noun in 84.58% of the samples, but the inverse situation only corresponds to 69.67% of the considered images."}, {"heading": "6 ADJECTIVE AND NOUN CONTRIBUTIONS", "text": "Understanding neural networks has attracted much research interest. For CNNs in particular, most methods rely on extracting the most salient points in the original image space that triggered a particular decision. The particularities of the ANPNet model presented in Section 3 allows to interpret the ANP predictions obtained in Section 5 in terms of adjective and noun contributions.\nWe adopted Deep Taylor Decomposition [20] to compute the contribution of each element in y\u0302ad j and y\u0302noun to the final ANP prediction, y\u0302ANP (Equation 3). This method consists in computing and propagating relevance backwards in the network (i.e. from the output to the input), where layer-wise relevance is approximated by a domain-specific Taylor decomposition. Two different rules are then derived for ReLU layers, namely the z+\u2212rule and zB\u2212rule, which apply to layers with unbounded and bounded inputs, respectively [20]. In our model, zB\u2212rule is used for the relevance model between the fully connected layer in the Fusion network and the bounded Adjective and Noun probabilities, whereas z+\u2212rule is used otherwise."}, {"heading": "6.1 Adjective Noun Ratio", "text": "This first analysis explores the nature of different ANPs depending on how much their prediction is influenced by the adjective and noun classes that compose it. For this purpose, we define the Adjective-to-Noun Ratio (ANR) as the normalized contribution of the adjectives with respect to the nouns during the prediction of an ANP. These normalized contributions are computed by summing the individual contributions of all adjectives and nouns considered by AdjNet and NounNet, and normalizing them by the total amount of adjective and nouns, respectively. Based on this definition, a uniform distribution of activations at the 117 outputs of AdjNet and another uniform distribution of activations at the 167 outputs of NounNet will result in an ANR equal to the unit.\nWe present two types of analysis from the ANR perspective: when considering a correct predictions of the ANP and its composing adjective and nouns, and when considering every prediction in the top-5 for every image. The ANRs of the ANP with highest and lowest ANPs are presented in Table 4, together with their top-5 accuracy values for the prediction, which allows interpreting the ANRs values in the context of the predictability of the adjective, noun and ANP.\n6.1.1 Average ANR per correct predictions. A first analysis considers high quality predictions from two perspectives: focusing on the correctness of the ANP only, or requiring a correct prediction the adjective and/or noun as well. As in previous sections, a prediction is considered correct if the ground truth adjective, noun or"}, {"heading": "MUSA2\u201917, October 27, 2017, Mountain View, CA, USA D. Fern\u00e1ndez et al.", "text": "ANP is among the top-5 prediction. This analysis corresponds to the columns 2 to 5 in Table 4.\nA first observation is that ANR takes values higher or lower than one, indicating that some ANP predictions are more influenced by the adjective branch of ANPnet, while others are more influenced by the noun branch. This diversity allows us to classify ANPs between adjective oriented or noun oriented ANPs, depending on whether the ANR is higher or lower than one, correspondingly.\nA second observation indicates that the difficulty on predicting the adjective favors most of the ANPs to be noun oriented. We can observe in table 4 that all the five ANPs with lowest ANR (noun oriented) have an adjective accuracy prediction lower than the adjective predictions from the five ANPs with highest ANR.\nFinally, filtering the results by forcing a correct prediction of the adjective and/or noun shows almost no impact in the final estimated ANR. This lack of variation can be explained by the co-detection matrix previously presented in Table 3. The third row of the matrix indicates that, when an ANP is correctly detected, the adjective is also well predicted in 87.47% of the cases and the noun in 95.76%. This allows a small variation when samples are filtered to force that adjective and/or noun must also be detected.\n6.1.2 Average ANR per all images. The results of Section 6.1.1 indicate how each ANP prediction is affected by the composing adjective and noun in the case of correct predictions. We extend this analysis to amore generic approachwere ground truth labels are not considered, so that all top-5 predicted ANP for each image are used in estimating the ANR values. This analysis provides more samples\nbecause in this case each image in the dataset allows drawing five ANR values, one for each of the top-5 predicted ANPs. In the case of Section 6.1.1, each image could only contribute in estimating the ANR of the ground truth ANP, and only in the case that the ANP were predicted among the top-5 ones.\nThe results of this analysis corresponds to the sixth column in Table 4. The estimated values show some slight variations with respect to the ANRs predicted over correct predictions only. The conclusions about adjective and noun oriented ANP remain the same for the cases depicted in the table."}, {"heading": "6.2 Visually equivalent ANPs", "text": "The interpretation of ANP predictions in terms of their contributing adjectives and nouns permits a novel description approach for ANPs. We define as visually equivalent those ANPs whose top-5 adjective and noun contributions are identical. Table 5 contains two pairs of visually equivalent ANPs. In the first example, \u201chappy dog\u201d and \u201csmiling dog\u201d have an identical noun and very similar adjective from a semantic perspective, while the second case presents the opposite situation, in which the same \u201cgolden\u201d adjective is used to build the \u201cgolden autumn\u201d and \u201cgolden leaves\u201d ANPs. In both examples their two sets of top-5 contributing adjectives and nouns is identical, but not in the exact order. Also, from a visual perspective, the presented examples depict how the two ANPs are visually described by the same class of images.\nA more extensive list of visually equivalent ANPs is provided in Table 5, together with the ANR of each ANP. These visually equivalent ANPs also share in all cases the adjective or the noun. Notice how they also present similar ANR values, an expected behavior as the most contributing adjectives and nouns are the same in each member of the pair.\nThese results show how our interpretable model is able to identify equivalent ANPs, which are a common case given the subjectivity and richness of an affective-aware dataset. These observations reinforce the choice of a top-5 accuracy metric instead of a more rigid top-1, as the boundaries between classes are very dim and often overlap.\nMore cat than cute? Interpretable Prediction of Adjective-Noun Pairs MUSA2\u201917, October 27, 2017, Mountain View, CA, USA"}, {"heading": "6.3 Related Adjectives and Nouns", "text": "Themost contributing adjectives and nouns detected byANPnet can also be used as semantic labels themselves. This way, our model can\nANP ANR ANP ANR ancient architecture 1.044 ancient building 1.075 dead fly 1.045 dead bug 1.080 traditional architecture 1.027 traditional house 1.004 dry tree 0.962 dying tree 0.832 tiny boat 0.924 little boat 0.909 weird bug 0.925 ugly bug 0.964 heavy clouds 0.921 dark clouds 0.948 beautiful clouds 0.920 beautiful sky 0.895 angry cat 0.820 evil cat 0.816\nTable 6: Pairs of Visually Equivalent ANPs\ndetect in a single pass additional concepts related to the predicted ANP, with applications to image tagging, captioning or retrieval.\nTable 7 shows the top-5 related adjective and nouns of four ANPs. We verify that the top contributing adjective and nouns have correspondence with the image contents, by randomly picking six images in the dataset for the considered ANPs. In example a) it can be seen how ANPnet learned that the most related concepts for an \u201celegant wedding\u201d scene are the names \u201ccake\u201d, \u201crose\u201d, \u201cdress\u201d and \u201clady\u201d, and the adjectives \u201coutdoor\u201d, \u201cfresh\u201d, \u201ctasty\u201d and \u201cdelicious\u201d. Notice the high contribution of food-adjectives as \u201cfresh\u201d, \u201ctasty\u201d and \u201cdelicious\u201d, which apply to the wedding cake and wedding meal. In the example b) we show the highest contributions for a more scene-oriented ANP, as \u201ccharming place\u201d. We notice how the network has been able to learn that adjectives describing \u201ccharming places\u201d are often also related to \u201ccomfortable\u201d, \u201cexcellent\u201d, \u201ctraditional\u201d and \u201cexpensive\u201d; and that elements appearing on these types of scenes are \u201chotel\", \u201chouse\u201d, \u201chome\u201d and \u201cfood\u201d. Examples c) and d) show additional cases of adjectives and nouns that match the contents of some images. For example, to describe \u201cdelicious food\u201d we could use adjectives as \u201ctraditional\u201d, \u201cexcellent\u201d, \u201ctasty\u201d or \u201cyummy\u201d. And to describe \u201cgolden hair\u201d images, other concepts that are related are: \u201cshiny\u201d, \u201cpretty\u201d, \u201csexy\u201d, \u201csmooth\u201d, \u201clady\u201d, \u201cblonde\u201d, \u201csunlight\u201d and \u201cgirl\u201d."}, {"heading": "MUSA2\u201917, October 27, 2017, Mountain View, CA, USA D. Fern\u00e1ndez et al.", "text": ""}, {"heading": "7 CONCLUSIONS AND FUTUREWORK", "text": "This work has presented ANPnet as an interpretable model capable of disentangling the adjective and noun contributions for the predictions of Adjective Noun Pairs (ANPs). This tools has allowed us to validate our hypothesis that the contribution of adjectives and nouns varies depending on each ANP and have introduced Adjective-to-Noun Ratio (ANR) as a measure to quantize it.\nANPnet is based on the fusion of two specialized networks for adjective and noun detection. Keeping the interpretation of the model when fusing the two specialized networks has also provoked a loss of accuracy of the model, establishing a trade-off between interpretability and performance. It has been observed that better detection accuracies are often associated to object-oriented nouns, while worse ones are related to scene-oriented nouns. As future work, one may explore pre-training AdjNet and NounNet not only with an object-oriented dataset as ImageNet, but also with a sceneoriented one such as Places [28].\nThe unbalanced contributions of adjective and nouns in ANP predictions also allows a classification between adjective- and nounoriented ANPs. Adjective-oriented ANPs tend to be harder to detect because adjectives themselves are also harder to detect than nouns. As in the case of scene-oriented nouns, adjective-oriented ANPs may be difficult to predict because AdjNet and NounNet were pretrained with the objects in ImageNet, a type of nouns. In addition, qualitative results also indicate how adjective concepts are much more visually diverse than noun ones.\nOur work has also shown how different ANPs may have the same top adjective and noun contributions, allowing the detection of visually equivalent ANPs. As a final analysis, we have shown how ANPnet can also be used to generate adjective and noun labels to enrich the semantic description of the images.\nThe presented work, while focus on affective-aware ANPs, could be extended to any other problem of adjective and noun detection, and even to more complex cases with more composing concepts. Our interpretable model aims at contributing in the field of better understanding why deep neural networks produce their predictions in terms of intermediate activations with a straightforward semantic interpretation. In the other hand, relationship between concepts can be used in order to modify network loss when training.\nAs future work, the accuracy gap between the interpretable model and the baseline should be reduced. An option to do that is by introducing multi-task and weight sharing between AdjNet and NounNet to reduce the number of parameters. A different architecture were the detection of an adjective or noun could be conditioned by the prior detection of the other (or viceversa) could reduce the visual variance in the detection and help improving its performance. Finally, the introduction of external knowledge bases could be explored to explore prior knowledge on the domain.\nThe source code and models used in this work are publicly available at https://imatge-upc.github.io/affective-2017-musa2/."}, {"heading": "ACKNOWLEDGMENTS", "text": "D\u00e8lia Fern\u00e1ndez is funded by contract 2017-DI-011 of the Industrial Doctorate Programme of the Government of Catalonia. This work was partially supported by the Spanish Ministry of Economy and Competitivity under contracts TIN2012-34557 by the BSC-CNS Severo Ochoa program (SEV-2011-00067), and contracts TEC201343935-R and TEC2016-75976-R. It has also been supported by grants 2014-SGR-1051 and 2014-SGR-1421 by the Government of Catalonia, and the European Regional Development Fund (ERDF). We gratefully acknowledge the support of NVIDIA Corporation for the donation of GPUs used in this work."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content", "author": ["Damian Borth", "Tao Chen", "Rongrong Ji", "Shih-Fu Chang"], "venue": "In ACM MM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "From pixels to sentiment: Fine-tuning cnns for visual sentiment prediction. Image and Vision Computing (2017).  More cat than cute? Interpretable Prediction of Adjective-Noun Pairs  MUSA2\u201917", "author": ["Victor Campos", "Brendan Jou", "Xavier Giro-i Nieto"], "venue": "October 27,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks", "author": ["Tao Chen", "Damian Borth", "Trevor Darrell", "Shih-Fu Chang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian QWeinberger"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Can we understand van gogh\u2019s mood?: learning to infer affects from images", "author": ["Jia Jia", "Sen Wu", "Xiaohui Wang", "Peiyun Hu", "Lianhong Cai", "Jie Tang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Predicting Emotions in User-Generated Videos", "author": ["Yu-Gang Jiang", "Baohan Xu", "Xiangyang Xue"], "venue": "In AAAI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Large-scale affective computing for visual multimedia", "author": ["Brendan Jou"], "venue": "Ph.D. Dissertation. Columbia University", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Deep Cross Residual Learning for Multitask Visual Recognition", "author": ["Brendan Jou", "Shih-Fu Chang"], "venue": "In ACM MM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Going Deeper for Multilingual Visual Sentiment Detection", "author": ["Brendan Jou", "Shih-Fu Chang"], "venue": "arXiv preprint arXiv:1605.09211", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology", "author": ["Brendan Jou", "Tao Chen", "Nikolaos Pappas", "Miriam Redi", "Mercan Topkara", "Shih-Fu Chang"], "venue": "In ACM MM", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Cognitive neuroscience of emotion", "author": ["Richard D Lane", "Lynn Nadel"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Scaring or pleasing: exploit emotional impact of an image", "author": ["Bing Li", "Songhe Feng", "Weihua Xiong", "Weiming Hu"], "venue": "In ACM MM", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Explaining nonlinear classification decisions with deep taylor decomposition", "author": ["Gr\u00e9goire Montavon", "Sebastian Lapuschkin", "Alexander Binder", "Wojciech Samek", "Klaus-Robert M\u00fcller"], "venue": "Pattern Recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural Nets", "author": ["Takuya Narihira", "Damian Borth", "Stella X Yu", "Karl Ni", "Trevor Darrell"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Beyond Object Recognition: Visual Sentiment Analysis with Deep Coupled Adjective and Noun Neural Networks", "author": ["JingwenWang", "Jianlong Fu", "Yong Xu", "Tao Mei"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Understanding the emotional impact of images", "author": ["Xiaohui Wang", "Jia Jia", "Peiyun Hu", "Sen Wu", "Jie Tang", "Lianhong Cai"], "venue": "In ACM MM", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Robust Visual- Textual Sentiment Analysis: When Attention meets Tree-structured Recursive Neural Networks", "author": ["Quanzeng You", "Liangliang Cao", "Hailin Jin", "Jiebo Luo"], "venue": "In ACM MM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks", "author": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Learning deep features for scene recognition using places database", "author": ["Bolei Zhou", "Agata Lapedriza", "Jianxiong Xiao", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Technologies have largely obviated emotion from data, while neurology demonstrates how emotions are fundamental to human experience by influencing cognition, perception and everyday tasks such as learning, communication and decision-making [18].", "startOffset": 240, "endOffset": 244}, {"referenceID": 10, "context": "Every day, Internet users post and share billions of multimedia content in online platforms to express sentiment and opinions about several topics [13], which has motivated research on automated affect understanding for large-scale multimedia [3, 12, 16].", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "Every day, Internet users post and share billions of multimedia content in online platforms to express sentiment and opinions about several topics [13], which has motivated research on automated affect understanding for large-scale multimedia [3, 12, 16].", "startOffset": 243, "endOffset": 254}, {"referenceID": 13, "context": "Every day, Internet users post and share billions of multimedia content in online platforms to express sentiment and opinions about several topics [13], which has motivated research on automated affect understanding for large-scale multimedia [3, 12, 16].", "startOffset": 243, "endOffset": 254}, {"referenceID": 13, "context": "It has been argued that the noun in an ANP grounds the visual appearance of the concept, while the adjective works as a bias carrying most of the conveyed affect [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 8, "context": "These tasks have been addressed with traditional low-level feature based techniques, such as color features [11], SIFT-based Bag of Words [19] and aesthetic effects [25].", "startOffset": 108, "endOffset": 112}, {"referenceID": 16, "context": "These tasks have been addressed with traditional low-level feature based techniques, such as color features [11], SIFT-based Bag of Words [19] and aesthetic effects [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "These tasks have been addressed with traditional low-level feature based techniques, such as color features [11], SIFT-based Bag of Words [19] and aesthetic effects [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 6, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 78, "endOffset": 89}, {"referenceID": 14, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 78, "endOffset": 89}, {"referenceID": 19, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 78, "endOffset": 89}, {"referenceID": 2, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 238, "endOffset": 249}, {"referenceID": 22, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 238, "endOffset": 249}, {"referenceID": 23, "context": "Due to the success of deep learning techniques in reference vision benchmarks [9, 17, 23], Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNNs) have replaced and surpassed handcrafted features for affect-related tasks [4, 26, 27].", "startOffset": 238, "endOffset": 249}, {"referenceID": 1, "context": "Similarly to other computer vision tasks, early approaches based on handcrafted features [2] were soon replaced with CNNs [5, 15, 16].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Similarly to other computer vision tasks, early approaches based on handcrafted features [2] were soon replaced with CNNs [5, 15, 16].", "startOffset": 122, "endOffset": 133}, {"referenceID": 12, "context": "Similarly to other computer vision tasks, early approaches based on handcrafted features [2] were soon replaced with CNNs [5, 15, 16].", "startOffset": 122, "endOffset": 133}, {"referenceID": 13, "context": "Similarly to other computer vision tasks, early approaches based on handcrafted features [2] were soon replaced with CNNs [5, 15, 16].", "startOffset": 122, "endOffset": 133}, {"referenceID": 6, "context": "CNNs have proven their efficiency for large-scale image datasets [9, 10, 17, 23].", "startOffset": 65, "endOffset": 80}, {"referenceID": 7, "context": "CNNs have proven their efficiency for large-scale image datasets [9, 10, 17, 23].", "startOffset": 65, "endOffset": 80}, {"referenceID": 14, "context": "CNNs have proven their efficiency for large-scale image datasets [9, 10, 17, 23].", "startOffset": 65, "endOffset": 80}, {"referenceID": 19, "context": "CNNs have proven their efficiency for large-scale image datasets [9, 10, 17, 23].", "startOffset": 65, "endOffset": 80}, {"referenceID": 3, "context": "DeepSentiBank [5] presented the first application of CNNs for ANP prediction.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "MVSO detector-banks [15] showed performance improvement by using a more modern architecture, GoogLeNet [23], which also reduced the amount of parameters of the model.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "MVSO detector-banks [15] showed performance improvement by using a more modern architecture, GoogLeNet [23], which also reduced the amount of parameters of the model.", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "The multi-task nature of the ANP detection task has been exploited by using a fan-out architecture, where a first set of layers is shared for all tasks, and then splits in different network heads that specialize on each task [14].", "startOffset": 225, "endOffset": 229}, {"referenceID": 18, "context": "Factorized nets [21] explicitly leverage the hierarchical nature of ANPs in the model architecture.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "The task of sentiment analysis is addressed with Deep Coupled Adjective and Noun neural networks (DCAN) [24] by learning a mid-level visual representation from two convolutional networks jointly trained with VSO.", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "The architecture for the specialized networks are based on the well-known ResNet-50 model [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "All residual layers use \u201cB option\u201d shortcut connections as described in [9], where projections are only used when matching dimensions (1 \u00d7 1 convolutions with stride 2) and other shortcuts are identity.", "startOffset": 72, "endOffset": 75}, {"referenceID": 11, "context": "Besides, it allows comparison in terms of accuracy with the ResNet-50 network trained for ANP detection in [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The presented work uses a subset of the Visual Sentiment Ontology (VSO) [3] dataset, the same part used in [14] to facilitate the comparison in terms of accuracy.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "The full VSO dataset presents certain limitations already pointed out in [14].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Due to these drawbacks, our experiments are based on a subset of VSO build according to the more restrictive constraints proposed in [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "The partition used in our experiments is the same for which results in [14] are reported2.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "com 2Dataset splits were obtained through personal communication with the authors of [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "When possible, layers were initialized using weights from a model pre-trained on ImageNet [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "Otherwise, weights were initialized following the method proposed by Glorot and Bengio [8] and the initial value for the biases was set to 0.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "All experiments were run using two NVIDIA GeForce GTX Titan X GPUs and implemented with TensorFlow [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 11, "context": "The table includes the results of two ResNet-50, the ones published in [14] and new ones obtained with the training parameters described in Section 4.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "The similar values obtained by our model with respect to the ones reported in [14] confirm that the training hyperparameters were Figure 4: Histogram of top-5 accuracy for ANP prediction", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "AdjNet [14] Adj 117 28.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "NounNet [14] Noun 167 41.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "ResNet-50 [14] ANP 553 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Adjective concepts such as \u201csmiling\u201d and \u201chappy\u201d, or noun concepts like \u201ccat\u201d and \u201canimal\u201d are considered absolutely different in the accuracy metric, so relaxing its detection by considering the top-5 predictions may provide a metric that expresses better the obtained results [15].", "startOffset": 278, "endOffset": 282}, {"referenceID": 24, "context": "ImageNet is a dataset build for object classification, so our model is more specialized in these type of classes than those related to scenes [28].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "We adopted Deep Taylor Decomposition [20] to compute the contribution of each element in \u0177ad j and \u0177noun to the final ANP prediction, \u0177ANP (Equation 3).", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Two different rules are then derived for ReLU layers, namely the z+\u2212rule and zB\u2212rule, which apply to layers with unbounded and bounded inputs, respectively [20].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "As future work, one may explore pre-training AdjNet and NounNet not only with an object-oriented dataset as ImageNet, but also with a sceneoriented one such as Places [28].", "startOffset": 167, "endOffset": 171}], "year": 2017, "abstractText": "The increasing availability of affect-rich multimedia resources has bolstered interest in understanding sentiment and emotions in and from visual content. Adjective-noun pairs (ANP) are a popular midlevel semantic construct for capturing affect via visually detectable concepts such as \u201ccute dog\" or \u201cbeautiful landscape\". Current stateof-the-art methods approach ANP prediction by considering each of these compound concepts as individual tokens, ignoring the underlying relationships in ANPs. This work aims at disentangling the contributions of the \u2018adjectives\u2019 and \u2018nouns\u2019 in the visual prediction of ANPs. Two specialised classifiers, one trained for detecting adjectives and another for nouns, are fused to predict 553 different ANPs. The resulting ANP prediction model is more interpretable as it allows us to study contributions of the adjective and noun components.", "creator": "LaTeX with hyperref package"}}}