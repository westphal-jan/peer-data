{"id": "1708.06426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models", "abstract": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. This has been applied to multiple tasks, in particular language processing, in a series of tasks, in the first two tasks (Seq2Seq2Seq1Seq2Seq2Seq1Seq1Seq1Seq1Seq2Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Seq1Se", "histories": [["v1", "Mon, 21 Aug 2017 21:28:07 GMT  (146kb,D)", "http://arxiv.org/abs/1708.06426v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anuroop sriram", "heewoo jun", "sanjeev satheesh", "adam coates"], "accepted": false, "id": "1708.06426"}, "pdf": {"name": "1708.06426.pdf", "metadata": {"source": "META", "title": "Cold Fusion: Training Seq2Seq Models Together with Language Models", "authors": ["Anuroop Sriram", "Heewoo Jun", "Sanjeev Satheesh", "Adam Coates"], "emails": ["nuroop@baidu.com>."], "sections": [{"heading": "1. Introduction", "text": "Sequence-to-sequence (Seq2Seq) (Bahdanau et al., 2015) models have achieved state-of-the-art results on many natural language processing problems including automatic speech recognition (Bahdanau et al., 2016; Chan et al., 2015), neural machine translation (Wu et al., 2016), conversational modeling (Vinyals & Le, 2015) and many more. These models learn to generate a variable-length sequence of tokens (e.g. texts) from a variable-length sequence of input data (e.g. speech or the same texts in another language). With a sufficiently large labeled dataset, vanilla Seq2Seq can model sequential mapping well, but it is often augmented with a language model to further improve the fluency of the generated text.\nBecause language models can be trained from abundantly available unsupervised text corpora which can have as many as one billion tokens (Jozefowicz et al., 2016; Shazeer et al., 2017), leveraging the rich linguistic information of the label domain can considerably improve Seq2Seq\u2019s performance. A standard way to integrate language models is to linearly combine the score of the task-\n1Baidu Research, Sunnyvale, CA, USA. Correspondence to: Anuroop Sriram <sriramanuroop@baidu.com>.\nspecific Seq2Seq model with that of an auxiliary langauge model to guide beam search (Chorowski & Jaitly, 2016; Sutskever et al., 2014a). Gulcehre et al. (2015) proposed an improved algorithm called Deep Fusion that learns to fuse the hidden states of the Seq2Seq decoder and a neural language model with a gating mechanism, after the two models are trained independently.\nWhile this approach has been shown to improve performance over the baseline, it has a few limitations. First, because the Seq2Seq model is trained to output complete label sequences without a language model, its decoder learns an implicit language model from the training labels, taking up a significant portion of the decoder capacity to learn redundant information. Second, the residual language model baked into the Seq2Seq decoder is biased towards the training labels of the parallel corpus. For example, if a Seq2Seq model fully trained on legal documents is later fused with a medical language model, the decoder still has an inherent tendency to follow the linguistic structure found in legal text. Thus, in order to adapt to novel domains, Deep Fusion must first learn to discount the implicit knowledge of the language.\nIn this work, we introduce Cold Fusion to overcome both these limitations. Cold Fusion encourages the Seq2Seq decoder to learn to use the external language model during training. This means that Seq2Seq can naturally leverage potentially limitless unsupervised text data, making it particularly proficient at adapting to a new domain. The latter is especially important in practice as the domain from which the model is trained can be different from the real world use case for which it is deployed. In our experiments, Cold Fusion can almost completely transfer to a new domain for the speech recognition task with 10 times less data. Additionally, the decoder only needs to learn task relevant information, and thus trains faster.\nThe paper is organized as follows: Section 2 outlines the background and related work. Section 3 presents the Cold Fusion method. Section 4 details experiments on the speech recognition task that demonstrate Cold Fusion\u2019s generalization and domain adaptation capabilities.\nar X\niv :1\n70 8.\n06 42\n6v 1\n[ cs\n.C L\n] 2\n1 A\nug 2\n01 7"}, {"heading": "2. Background and Related work", "text": ""}, {"heading": "2.1. Sequence-to-Sequence Models", "text": "A basic Seq2Seq model comprises an encoder that maps an input sequence x = (x1, . . . , xT ) into an intermediate representation h, and a decoder that in turn generates an output sequence y = (y1, . . . , yK) from h (Sutskever et al., 2014b). The decoder can also attend to a certain part of the encoder states with an attention mechanism. The attention mechanism is called hybrid attention (Chorowski et al., 2015b), if it uses both the content and the previous context to compute the next context. It is soft if it computes the expectation over the encoder states (Bahdanau et al., 2015) as opposed to selecting a slice out of the encoder states.\nFor the automatic speech recognition (ASR) task, the Seq2Seq model is called an acoustic model (AM) and maps a sequence of spectrogram features extracted from a speech signal to characters."}, {"heading": "2.2. Inference and Language Model Integration", "text": "During inference, we aim to compute the most likely sequence y\u0302:\ny\u0302 = argmax y\nlog p(y|x). (1)\nHere, p(y|x) is the probability that the task-specific Seq2Seq model assigns to sequence y given input sequence x. The argmax operation is intractable in practice so we use a left-to-right beam search algorithm similar to the one presented in (Sutskever et al., 2014a). We maintain a beam of K partial hypothesis starting with the start symbol \u3008s\u3009. At each time-step, the beam is extended by one additional character and only the top K hypotheses are kept. Decoding continues until the stop symbol \u3008/s\u3009 is emitted, at which point the hypothesis is added to the set of completed hypotheses.\nA standard way to integrate the language model with the Seq2Seq decoder is to change the inference task to:\ny\u0302 = argmax y\nlog p(y|x) + \u03bb log pLM(y), (2)\nwhere pLM(y) is the language model probability assigned to the label sequence y. (Chorowski & Jaitly, 2016; Wu et al., 2016) describe several heuristics that can be used to improve this basic algorithm. We refer to all of these methods collectively as Shallow Fusion, since pLM is only used during inference.\n(Gulcehre et al., 2015) proposed Deep Fusion for machine translation that tightens the connection between the decoder and the language model by combining their states\nwith a parametric gating:\ngt = \u03c3(v >sLMt + b) (3a) sDFt = [st; gts LM t ] (3b)\nyt = softmax(DNN(s DF t )), (3c)\nwhere st, sLMt and s DF t are the states of the task specific Seq2Seq model, language model and the overall deep fusion model. In (3c), DNN can be a deep neural network with any number of layers. [a; b] is the concatenation of vectors a and b.\nIn Deep Fusion, the Seq2Seq model and the language model are first trained independently and later combined as in Equation (3). The parameters v and b are trained on a small amount of data keeping the rest of the model fixed, and allow the gate to decide how important each of the models are for the current time step.\nThe biggest disadvantage with Deep Fusion is that the taskspecific model is trained independently from the language model. This means that the Seq2Seq decoder needs to learn a language model from the training data labels, which can be rather parsimonious compared to the large text corpora available for language model training. So, the fused output layer of (3) should learn to overcome this bias in order to incorporate the new language information. This also means that a considerable portion of the decoder capacity is wasted."}, {"heading": "2.3. Semi-supervised Learning in Seq2Seq Models", "text": "A few methods have been proposed for leveraging unlabeled text corpora in the target domain, for both better generalization and domain transfer.\nSennrich et al. (2015) proposed backtranslation as a way of using unlabeled data for machine translation. Backtranslation improves the BLEU score by increasing the parallel training corpus of the neural machine translation model by automatically translating the unlabeled target domain text. However, this technique does not apply well to other tasks where backtranslation is infeasible or of very low quality (like image captioning or speech recogntion).\nRamachandran et al. (2016) proposed warm starting the Seq2Seq model from language models trained on source and target domains separately. Unsupervised pre-training shows improvements in the BLEU scores. (Ramachandran et al., 2016) also show that this improvement is from improved generalization, and not only better optimization. While this is a promising approach, the method is potentially difficult to leverage for the transfer task since training on the parallel corpus could end up effectively erasing the knowledge of the language models. Both back-translation and unsupervised pre-training are simple methods that require no change in the architecture."}, {"heading": "3. Cold Fusion", "text": "Our proposed Cold Fusion method is largely motivated from the Deep Fusion idea but with some important differences. The biggest difference is that in Cold Fusion, the Seq2Seq model is trained from scratch together with a fixed pre-trained language model.\nBecause the Seq2Seq model is aware of the language model throughout training, it learns to use the language model for language specific information and capture only the relevant information conducive to mapping from the source to the target sequence. This disentanglement can increase the effective capacity of the model significantly. This effect is demonstrated empirically in Section 4 where Cold Fusion models perform well even with a very small decoder.\nWe also improve on some of the modeling choices of the fusion mechanism.\n1. First, both the Seq2Seq hidden state st and the language model hidden state sLMt can be used as inputs to the gate computation. The task-specific model\u2019s embedding contains information about the encoder states which allows the fused layer to decide its reliance on the language model in case of input uncertainty. For example, when the input speech is noisy or a token unseen by the Seq2Seq model is presented, the fusion mechanism learns to pay more attention to the language model.\n2. Second, we employ fine-grained (FG) gating mechanism as introduced in (Yang et al., 2016). By using a different gate value for each hidden node of the language model\u2019s state, we allow for greater flexibility\nin integrating the language model because the fusion algorithm can choose which aspects of the language model it needs to emphasize more at each time step.\n3. Third, we replace the language model\u2019s hidden state with the language model probability. The distribution and dynamics of sLMt can vary considerably across different language models and data. As a concrete example, any fusion mechanism that uses the LM state is not invariant to the permutation of state hidden nodes. This limits the ability to generalize to new LMs. By projecting the token distribution onto a common embedding space, LMs that model novel uses of the language can still be integrated without state discrepancy issues. This also means that we can train with or swap on n-gram LMs during inference.\nThe Cold Fusion layer works as follows:\nhLMt = DNN(` LM t ) (4a)\ngt = \u03c3(W [st;h LM t ] + b) (4b) sCFt = [st; gt \u25e6 hLMt ] (4c) rCFt = DNN(s CF t ) (4d)\nP\u0302 (yt|x, y<t) = softmax(rCFt ) (4e)\n`LMt is the logit output of the language model, st is the state of the task specific model, and sCFt is the final fused state used to generate the output. Since logits can have arbitrary offsets, the maximum value is subtracted off before feeding into the layer. In (4a), (4d), the DNN can be a deep neural network with any number of layers. In our experiments, we found a single affine layer, with ReLU activation prior to softmax, to be helpful."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Setup", "text": "For our experiments, we tested the Cold Fusion method on the speech recognition task. The results are compared using the character error rate (CER) and word error rate (WER) on the evaluation sets. For all models which were trained on the source domain, the source CER and WER indicate in-domain performance and the target CER and WER indicate out-of-domain performance.\nWe collected two data sets: one based on search queries which served as our source domain, and another based on movie transcripts which served as our target domain. For each dataset, we used Amazon Mechanical Turk to collect audio recordings of speakers reading out the text. We gave identical instructions to all the turkers in order to ensure that the two datasets only differed in the text domain. The source dataset contains 411,000 utterances (about 650 hours of audio), and the target dataset contains 345,000 utterances (about 676 hours of audio). We held out 2048 utterances from each domain for evaluation.\nThe text of the two datasets differ significantly. Table 2 shows results of training character-based recurrent neural network language models (Mikolov, 2012) on each of the datasets and evaluating on both datasets. Language models very easily overfit the training distribution, so models trained on one corpus will perform poorly on a different distribution. We see this effect in Table 2 that models optimized for the source domain have worse perplexity on the target distribution."}, {"heading": "4.2. Neural Network Architectures", "text": "The language model described in the final row of Table 2 was trained on about 25 million words. This model con-\ntains three layers of gated recurrent units (GRU) (Chung et al., 2014) with a hidden state dimension of 1024. The model was trained to minimize the cross-entropy of predicting the next character given the previous characters. We used the Adam optimizer (Kingma & Ba, 2014) with a batch size of 512. The model gets a perplexity of 2.49 on the source data and 2.325 on the target data.\nFor the acoustic models, we used the Seq2Seq architecture with soft attention based on (Bahdanau et al., 2016). The encoder consists of 6 bidirectional LSTM (BLSTM) (Hochreiter & Schmidhuber, 1997) layers each with a dimension of 480. We also use max pooling layers with a stride of 2 along the time dimension after the first two BLSTM layers, and add residual connections (He et al., 2015) for each of the BLSTM layers to help speed up the training process. The decoder consisted of a single layer of 960 dimensional Gated Recurrent Unit (GRU) with a hybrid attention (Chorowski et al., 2015b).\nThe final Cold Fusion mechanism had one dense layer of 256 units followed by ReLU before softmax."}, {"heading": "4.3. Training", "text": "The input sequence consisted of 40 mel-scale filter bank features. We expanded the datasets with noise augmentation; a random background noise is added with a 40% probability at a uniform random SNR between 0 and 15 dB. We did not use any other form of regularization.\nWe trained the entire system end-to-end with Adam (Kingma & Ba, 2014) with a batch size of 64. The learning rates were tuned separately for each model using random search. To stabilize training early on, the training examples were sorted by increasing input sequence length in the first epoch (Amodei et al., 2015). During inference, we used beam search with a fixed beam size of 128 for all of our experiments.\nWe also used scheduled sampling (Bengio et al., 2015) with a sampling rate of 0.2 which was kept fixed throughout training. Scheduled sampling helped reduce the effect of exposure bias due to the difference in the training and inference mechanisms."}, {"heading": "4.4. Improved Generalization", "text": "Leveraging a language model that has a better perplexity on the distribution of interest should directly mean an improved WER for the ASR task. In this section, we compare how the different fusion methods fare in achieving this effect.\nSwapping the language model is not possible with Deep Fusion because of the state discrepancy issue motivated in Section 3. All fusion models were therefore trained and evaluated with the same language model that achieved a\nlow perplexity on both the source and target domains (See Table 2). This way, we can measure improvements in transfer capability over Deep Fusion due to the training and architectural changes.\nTable 3 compares the performance of Deep Fusion and Cold Fusion on the source and target held-out sets. Clearly, Cold Fusion consistently outperforms on both metrics on both domains than the baselines. For the task of predicting in-domain, the baseline model gets a word error of 14.68%, while our best model gets a relative improvement of more than 21% over that number. Even compared to the recently proposed Deep Fusion model (Gulcehre et al., 2015), the best Cold Fusion model gets a relative improvement of 15%.\nWe get even bigger improvements in out-of-domain results. The baseline attention model, when trained on the source domain but evaluated on the target domain gets, 43.5% WER. This is significantly worse than the 17.6% that we can get by training the same model on the target dataset. The goal of domain adaptation is to bridge the gap between these numbers. The final column in Table 3 shows the remaining gap as a fraction of the difference for each model.\nThe Deep Fusion models can only narrow the domain gap to 76.57% while Cold Fusion methods can reduce it to 38.17%. The same table also shows the incremental effects of the three architectural changes we have made to the Cold Fusion method. Note that applying the same changes to the Deep Fusion method does not yield much improvements, indicating the need for cold starting Seq2Seq training with language models. The use of probability projection instead of the language model state in the fusion layer substantially helps with generalization. Intuitively, the character probability space shares the same structure across different language models unlike the hidden state space."}, {"heading": "4.5. Decoder Efficiency", "text": "We test whether cold fusion does indeed relieve the decoder of learning a language model. We do so by checking how a decrease in the decoder capacity affected the error rates. As evidenced in Table 4, the performance of the Cold Fusion models degrades gradually as the decoder cell size is decreased whereas the performance of the attention models deteriorates abruptly beyond a point. It is remarkable that the Cold Fusion decoder still outperforms the full attentional decoder with 4\u00d7 fewer number of parameters.\nAlso, we find that training is accelerated by a factor of 3 (see Figure 1). Attention models typically need hundreds of thousands of iterations to converge (Chorowski et al., 2015a). Most of the training time is spent in learning the attention mechanism. One can observe this behavior by plotting the attention context over time and seeing that the diagonal alignment pattern emerges in later iterations. Because the pretrained, fixed language model infuses the model with lower level language features like the likely spelling of a word, error signals propagate more directly into the attention context."}, {"heading": "4.6. Fine-tuning for Domain Adaptation", "text": "In the presence of limited data from the target distribution, fine tuning a model for domain transfer is often a promising approach. We test how much labeled data from the target distribution is required for Cold Fusion models to effectively close the domain adaptation gap.\nThe same language model from Section 4.4 trained on both the source and target domains was used for all fine-tuning experiments. The learning rate was restored to its initial value. Then, we fine-tuned only the fusion mechanism of the best Cold Fusion model from Table 3 on various amounts of the labeled target dataset.\nResults are presented in Table 5. With just 0.6% of labeled data, the domain gap decreases from 38.2% to 21.3%. With less than 10% of the data, this gap is down to only 8%. Note that because we keep the Seq2Seq parameters fixed during the fine-tuning stage, all of the improvements from finetuning come from combining the acoustic and the language model better. It\u2019s possible that we can see bigger gains by fine-tuning all the parameters. We do not do this in our experiments because we are only interested in studying the effects of language model fusion in the Seq2Seq decoder.\nSome examples are presented in Table 1. Recall that all models are trained on the source domain consisting of the read speech of search queries and evaluated on the read speech of movie scripts to measure out-of-domain performance. Because search queries tend to be sentence fragments, we see that the main mode of error for vanilla attention and Deep Fusion is due to weak grammar knowledge. Cold Fusion on the other hand demonstrates a better grasp of grammar and is able to complete sentences."}, {"heading": "5. Conclusion", "text": "In this work, we presented a new general Seq2Seq model architecture where the decoder is trained together with a pre-trained language model. We study and identify architectural changes that are vital for the model to fully leverage information from the language model, and use this to generalize better; by leveraging the RNN language model, Cold Fusion reduces word error rates by up to 18% compared to Deep Fusion. Additionally, we show that Cold Fusion models can transfer more easily to new domains, and with only 10% of labeled data nearly fully transfer to the new domain."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Bahdanau", "Dzmitry", "Chorowski", "Jan", "Serdyuk", "Dmitriy", "Brakel", "Philemon", "Bengio", "Yoshua"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["Chan", "William", "Jaitly", "Navdeep", "Le", "Quoc V", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Towards better decoding and language model integration in sequence to sequence models", "author": ["Chorowski", "Jan", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1612.02695,", "citeRegEx": "Chorowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Gulcehre", "Caglar", "Firat", "Orhan", "Xu", "Kelvin", "Cho", "Kyunghyun", "Barrault", "Loic", "Lin", "Huei-Chi", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1503.03535,", "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology,", "citeRegEx": "Mikolov,? \\Q2012\\E", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Unsupervised pretraining for sequence to sequence learning", "author": ["Ramachandran", "Prajit", "Liu", "Peter J", "Le", "Quoc V"], "venue": "arXiv preprint arXiv:1611.02683,", "citeRegEx": "Ramachandran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2016}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Sennrich", "Rico", "Haddow", "Barry", "Birch", "Alexandra"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["Shazeer", "Noam", "Mirhoseini", "Azalia", "Maziarz", "Krzysztof", "Davis", "Andy", "Le", "Quoc", "Hinton", "Geoffrey", "Dean", "Jeff"], "venue": "arXiv preprint arXiv:1701.06538,", "citeRegEx": "Shazeer et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2017}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. 2014b. http://arxiv.org/abs/1409.3215", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Words or characters? fine-grained gating for reading comprehension", "author": ["Yang", "Zhilin", "Dhingra", "Bhuwan", "Yuan", "Ye", "Hu", "Junjie", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1611.01724,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Sequence-to-sequence (Seq2Seq) (Bahdanau et al., 2015) models have achieved state-of-the-art results on many natural language processing problems including automatic speech recognition (Bahdanau et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 2, "context": ", 2015) models have achieved state-of-the-art results on many natural language processing problems including automatic speech recognition (Bahdanau et al., 2016; Chan et al., 2015), neural machine translation (Wu et al.", "startOffset": 138, "endOffset": 180}, {"referenceID": 3, "context": ", 2015) models have achieved state-of-the-art results on many natural language processing problems including automatic speech recognition (Bahdanau et al., 2016; Chan et al., 2015), neural machine translation (Wu et al.", "startOffset": 138, "endOffset": 180}, {"referenceID": 11, "context": "Because language models can be trained from abundantly available unsupervised text corpora which can have as many as one billion tokens (Jozefowicz et al., 2016; Shazeer et al., 2017), leveraging the rich linguistic information of the label domain can considerably improve Seq2Seq\u2019s performance.", "startOffset": 136, "endOffset": 183}, {"referenceID": 16, "context": "Because language models can be trained from abundantly available unsupervised text corpora which can have as many as one billion tokens (Jozefowicz et al., 2016; Shazeer et al., 2017), leveraging the rich linguistic information of the label domain can considerably improve Seq2Seq\u2019s performance.", "startOffset": 136, "endOffset": 183}, {"referenceID": 8, "context": "Gulcehre et al. (2015) proposed an improved algorithm called Deep Fusion that learns to fuse the hidden states of the Seq2Seq decoder and a neural language model with a gating mechanism, after the two models are trained independently.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "It is soft if it computes the expectation over the encoder states (Bahdanau et al., 2015) as opposed to selecting a slice out of the encoder states.", "startOffset": 66, "endOffset": 89}, {"referenceID": 8, "context": "(Gulcehre et al., 2015) proposed Deep Fusion for machine translation that tightens the connection between the decoder and the language model by combining their states with a parametric gating:", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "(Ramachandran et al., 2016) also show that this improvement is from improved generalization, and not only better optimization.", "startOffset": 0, "endOffset": 27}, {"referenceID": 19, "context": "Second, we employ fine-grained (FG) gating mechanism as introduced in (Yang et al., 2016).", "startOffset": 70, "endOffset": 89}, {"referenceID": 13, "context": "Table 2 shows results of training character-based recurrent neural network language models (Mikolov, 2012) on each of the datasets and evaluating on both datasets.", "startOffset": 91, "endOffset": 106}, {"referenceID": 7, "context": "tains three layers of gated recurrent units (GRU) (Chung et al., 2014) with a hidden state dimension of 1024.", "startOffset": 50, "endOffset": 70}, {"referenceID": 2, "context": "For the acoustic models, we used the Seq2Seq architecture with soft attention based on (Bahdanau et al., 2016).", "startOffset": 87, "endOffset": 110}, {"referenceID": 9, "context": "We also use max pooling layers with a stride of 2 along the time dimension after the first two BLSTM layers, and add residual connections (He et al., 2015) for each of the BLSTM layers to help speed up the training process.", "startOffset": 138, "endOffset": 155}, {"referenceID": 0, "context": "To stabilize training early on, the training examples were sorted by increasing input sequence length in the first epoch (Amodei et al., 2015).", "startOffset": 121, "endOffset": 142}, {"referenceID": 8, "context": "Even compared to the recently proposed Deep Fusion model (Gulcehre et al., 2015), the best Cold Fusion model gets a relative improvement of 15%.", "startOffset": 57, "endOffset": 80}], "year": 2017, "abstractText": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.", "creator": "LaTeX with hyperref package"}}}