{"id": "1708.06834", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at http://github.com/Kylix/rnn-prob.\n\n\n\nA few examples of an RNN model can be seen from the following image:\n// 1. The RNN is a single data model (2D) in a single dimension in a single dimensions in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a single dimension in a", "histories": [["v1", "Tue, 22 Aug 2017 21:53:34 GMT  (259kb,D)", "http://arxiv.org/abs/1708.06834v1", null], ["v2", "Thu, 24 Aug 2017 00:54:45 GMT  (259kb,D)", "http://arxiv.org/abs/1708.06834v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["victor campos", "brendan jou", "xavier giro-i-nieto", "jordi torres", "shih-fu chang"], "accepted": false, "id": "1708.06834"}, "pdf": {"name": "1708.06834.pdf", "metadata": {"source": "CRF", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "authors": ["V\u00edctor Campos", "Brendan Jou", "Xavier Gir\u00f3-i-Nieto", "Jordi Torres", "Shih-Fu Chang"], "emails": ["jordi.torres}@bsc.es,", "bjou@google.com,", "xavier.giro@upc.edu,", "sfchang@ee.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs) have become the standard approach for practitioners when addressing machine learning tasks involving sequential data. Such success has been enabled by the appearance of larger datasets, more powerful computing resources and improved architectures and training algorithms. Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8]. These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].\nSome of the main limitations of RNNs are their challenging training and deployment when dealing with long sequences, due to their inherently sequential behaviour. These challenges include throughput degradation, slower convergence during training and memory leakage, even for gated architectures [38]. Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues. The most common approaches, such as cropping discrete signals or reducing the sampling rate in continuous signals, are heuristics and can be suboptimal. In contrast, we propose a model that is able to learn which samples (i.e. elements in the input sequence) need to be used in order to solve the target task. Consider a video understanding task as an example: scenes with large motion may benefit from high frame rates, whereas only a few frames are needed to capture the semantics of a mostly static scene.\nThe main contribution of this work is a novel modification for existing RNN architectures that allows them to skip state updates, decreasing the number of sequential operations to be performed, without requiring any additional supervision signal. This model, called Skip RNN, adaptively determines whether the state needs to be updated or copied to the next time step, thereby allow a \u201cskip\u201d in the\n\u2217Work done while V\u00edctor Campos was a visiting scholar at Columbia University.\nar X\niv :1\n70 8.\n06 83\n4v 1\n[ cs\n.A I]\n2 2\nA ug\ncomputation graph. We show how the network can be encouraged to perform fewer state updates by adding a penalization term during training, allowing us to train models of different target computation budgets. The proposed modification is implemented on top of well known RNN architectures, namely LSTM and GRU, and the resulting models show promising results in a series of sequence modeling tasks. In particular, the proposed Skip RNN architecture is evaluated on five sequence learning problems: an adding task, sine wave frequency discrimination, digit classification, sentiment analysis in movie reviews and action classification in video.\nThis paper is structured as follows: Section 2 provides an overview of the related work, Section 3 describes the proposed model, experimental evaluation of Skip RNN in a series of sequence modeling tasks is presented in Section 4, and Section 5 summarizes the main results and some potential extensions of this work. Source code is publicly available at https://github.com/imatge-upc/ skiprnn-2017-telecombcn."}, {"heading": "2 Related work", "text": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41]. This idea has been extended in the temporal domain, either by learning how many times an input needs to be pondered before moving to the next one [18] or building RNNs whose number of layers depends on the input data [12]. Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38]. However, due to the inherently sequential nature of RNNs and the parallel computation capabilities of modern hardware, reducing the size of the matrices involved in the computations performed at each time step does not accelerate inference. The proposed Skip RNN model can be seen as form of conditional computation in time, where the computation associated to the RNN updates may or may not be executed at every time step. This is related to the UPDATE and COPY operations in hierarchical multiscale RNNs [12], but applied to the whole stack of RNN layers at the same time. This difference is key to allowing our approach to skip input samples, effectively reducing sequential computation and shielding the hidden state over longer time lags. Learning whether to update or copy the hidden state through time steps can be seen as a learnable Zoneout mask [30] which is shared between all the units in the hidden state. Similarly, it can be interpretted as an input-dependent recurrent version of stochastic depth [25].\nSelecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3]. Our model can be understood to generate a hard temporal attention mask on the fly given the previously seen samples, deciding which time steps should be attended and operating on a subset of input samples. Subsampling input sequences has been explored for visual storylines generation [43], although jointly optimizing the RNN weights and the subsampling mechanism is computationally unfeasible and the Expectation Maximization algorithm is used instead. Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46]. Motivated by the advantages of training recurrent models on shorter subsequences, efforts have been conducted towards learning differentiable subsampling mechanisms [40], although the computational complexity of the proposed method precludes its application to long input sequences. In contrast, our proposed method can be trained with backpropagation and does not degrade the complexity of the baseline RNNs.\nAccelerating inference in RNNs is difficult due to their inherently sequential nature, leading to the design of Quasi-Recurrent Neural Networks [10], which relax the temporal dependency between consecutive steps. With the goal of speeding up RNN inference, LSTM-Jump [51] augments an LSTM cell with a classification layer that will decide how many steps to jump between RNN updates. Despite its promising results on text tasks, the model needs to be trained with REINFORCE [48], which requires the definition of a reward signal. Determining such reward signal is not trivial and does not necessarily generalize across tasks, e.g. regression and classification tasks may require from different reward signals. Moreover, the number of tokens read between jumps, the maximum jump distance and the number of jumps allowed need to be chosen ahead of time. These hyperparameters define a reduced set of subsequences that the model can sample, instead of allowing the network to learn any arbitrary sampling scheme. Unlike LSTM-Jump, our proposed approach is differentiable,\nthus not requiring any modifications to the loss function and simplifying the optimization process, and is not limited to a predefined set of sample selection patterns."}, {"heading": "3 Model Description", "text": "An RNN takes an input sequence x = (x1, . . . , xT ) and generates a state sequence s = (s1, . . . , sT ) by iteratively applying a parametric state transition model S from t = 1 to T :\nst = S(st\u22121, xt) (1)\nWe augment the network with a binary state update gate, ut \u2208 {0, 1}, selecting whether the state of the RNN will be updated or copied from the previous time step. At every time step t, the probability u\u0303t+1 \u2208 [0, 1] of performing a state update at t+ 1 is emitted. The resulting architecture is depicted in Figure 1 and can be characterized as follows:\nut = fbinarize(u\u0303t) (2) st = ut \u00b7 S(st\u22121, xt) + (1\u2212 ut) \u00b7 st\u22121 (3) \u2206u\u0303t = \u03c3(Wpst + bp) (4) u\u0303t+1 = ut \u00b7\u2206u\u0303t + (1\u2212 ut) \u00b7 (u\u0303t + min(\u2206u\u0303t, 1\u2212 u\u0303t)) (5)\nwhere \u03c3 is the sigmoid function and fbinarize : [0, 1] \u2192 {0, 1} binarizes the input value. Should the network be composed of several layers, some columns of Wp can be fixed to 0 so that \u2206u\u0303t depends only on the states of a subset of layers (see Section 4.5 for an example with two layers). We implement fbinarize as a deterministic step function ut = round(u\u0303t), although a stochastic sampling from a Bernoulli distribution ut \u223c Bernoulli(u\u0303t) would be possible as well. The model formulation implements the observation that the likelihood of requesting a new input increases with the number of consecutively skipped samples. Whenever a state update is omitted, the pre-activation of the state update gate for the following time step, u\u0303t+1, is incremented by \u2206u\u0303t. On the other hand, if a state update is performed, the accumulated value is flushed and u\u0303t+1 = \u2206u\u0303t.\nThe number of skipped time steps can be computed ahead of time. For the particular formulation used in this work, where fbinarize is implemented by means of a rounding function, the number of skipped samples after performing a state update at time step t is given by:\nNskip(t) = min{n : n \u00b7\u2206u\u0303t \u2265 0.5} \u2212 1 (6) where n \u2208 Z+. This enables more efficient implementations where no computation at all is performed whenever ut = 0. These computational savings are possible because \u2206u\u0303t = \u03c3(Wpst + bp) = \u03c3(Wpst\u22121 + bp) = \u2206u\u0303t\u22121 when ut = 0 and there is no need to evaluate it again, as depicted in Figure 1d.\nThere are several advantages in reducing the number of RNN updates. From the computational standpoint, fewer updates translates into fewer required sequential operations to process an input signal, leading to faster inference and reduced energy consumption. Unlike some other models that aim to reduce the average number of operations per step [38, 26], ours enables skipping steps completely. Replacing RNN updates with copy operations increases the memory of the network and its ability to model long term dependencies even for gated units, since the exponential memory decay observed in LSTM and GRU [38] is alleviated. During training, gradients are propagated through fewer updating time steps, providing faster convergence in some tasks involving long sequences. Moreover, the proposed model is orthogonal to recent advances in RNNs and could be used in conjunction with such techniques, e.g. normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47]."}, {"heading": "3.1 Error gradients", "text": "The whole model is differentiable except for fbinarize, which outputs binary values. A common method for optimizing functions involving discrete variables is REINFORCE [48], although several estimators have been proposed for the particular case of neurons with binary outputs [7]. We select\nthe straight-through estimator [23], which consists in approximating the step function by the identity when computing gradients during the backward pass:\n\u2202fbinarize (x)\n\u2202x = 1 (7)\nThis yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12]. By using the straight-through estimator as the backward pass for fbinarize, all the model parameters can be trained to minimize the target loss function with standard backpropagation and without defining any additional supervision or reward signal."}, {"heading": "3.2 Limiting computation", "text": "The Skip RNN is able to learn when to update or copy the state without explicit information about which samples are useful to solve the task at hand. However, a different operating point on the trade-off between performance and number of processed samples may be required depending on the application, e.g. one may be willing to sacrifice a few accuracy points in order to run faster on machines with low computational power, or to reduce energy impact on portable devices. The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26]. In particular, we consider a cost per sample:\nLbudget = \u03bb \u00b7 T\u2211\nt=1\nut (8)\nwhere Lbudget is the cost associated to a single sequence, \u03bb is the cost per sample and T is the sequence length. This formulation bears a similarity to weight decay regularization, where the network is encouraged to slowly converge towards a solution where the norm of the weights is smaller. Similarly, in this case the network is encouraged to slowly converge towards a solution where fewer state updates are required.\nDespite this formulation has been extensively studied in our experiments, different budget loss terms can be used depending on the application. For instance, a specific number of samples may be encouraged by applying an L1 or L2 loss between the target value and the number of updates per sequence, \u2211T t=1 ut."}, {"heading": "4 Experiments", "text": "In the following section, we investigate the advantages of adding this state skipping to LSTMs and GRUs for a variety of tasks. In addition to the evaluation metric for each task, we also report the number of RNN state updates (i.e. the number of elements in the input sequence that are used by the model) as a measure of the computational load for each model. Since skipping an RNN update results in ignoring its corresponding input, we will refer to the number of updates and the number of used samples (i.e. elements in a sequence) interchangeably.\nTraining is performed with Adam [28], learning rate of 10\u22124, \u03b21 = 0.9, \u03b22 = 0.999 and = 10\u22128 on batches of 256. Gradient clipping [39] with a threshold of 1 is applied to all trainable variables. Bias bp in Equation 4 is initialized to 1, so that all samples are used at the beginning of training2. The initial hidden state s0 is learned during training, whereas u\u03030 is set to a constant value of 1 in order to force the first update at t = 1.\nExperiments are implemented with TensorFlow3 and run on a single NVIDIA K80 GPU."}, {"heading": "4.1 Adding Task", "text": "We revisit one of the original LSTM tasks [24], where the network is given a sequence of (value, marker) tuples. The desired output is the addition of only the two values that are marked with a 1, whereas those marked with a 0 need to be ignored. We follow the experimental setup by Neil et al. [38], where the first marker is randomly placed among the first 10% of samples (drawn with uniform probability) and the second one is placed among the last half of samples (drawn with uniform probability). This marker distribution yields sequences where at least 40% of the samples are distractors and provide no useful information at all. However, it is worth noting that in this task the risk of missing a marker is very large as compared to the benefits of working on shorter subsequences.\nWe train RNN models with 110 units each on sequences of length 50, where the values are uniformly drawn from U(\u22120.5, 0.5). The final RNN state is fed to a fully connected layer that regresses the scalar output. The model is trained to minimize the Mean Squared Error (MSE) between the output and the ground truth. We consider that a model is able to solve the task when its MSE on a held-out set of examples is at least two orders of magnitude below the variance of the output distribution. This criterion is a stricter version of the one followed in [24].\nWhile all models learn to solve the task, results in Table 1 show that Skip RNN models are able to do so with roughly half of the updates of their corresponding counterparts. Interestingly, Skip LSTM tends to skip more updates than the Skip GRU when no cost per sample is set, behavior that may be related to the lack of output gate in the latter. We hypothesize that there are two possible reasons why the output gate makes the LSTM more prone to skipping updates: (a) it introduces an additional source of memory decay, and (b) it allows to mask out some units in the cell state that may specialize in deciding when to update or copy, making the final regression layer agnostic to such process.\nWe observed that the models using fewer updates never miss any marker, since the penalization in terms of MSE would be very large (see Figure 2 for examples). These models learn to skip most of the samples in the 40% of the sequence where there are no markers. Moreover, most updates are\n2In practice, forcing the network to use all samples at the beginning of training improves its robustness against random initializations of its weights and increases the reproducibility of the presented experiments. A similar behavior was observed in other augmented RNN architectures such as Neural Stacks [21].\n3https://www.tensorflow.org\nskipped once the second marker is found, since all the relevant information in the sequence has been already seen. This last pattern provides evidence that the proposed models effectively learn to decide whether to update or copy the hidden state based on the input sequence, as opposed to learning biases in the dataset only. As a downside, Skip RNN models show some difficulties skipping a large number of updates at once, probably due to the cumulative nature of u\u0303t."}, {"heading": "4.2 Frequency Discrimination Task", "text": "In this experiment, the network is trained to classify between sinusoids whose period is in range T \u223c U (5, 6) milliseconds and those whose period is in range T \u223c {(1, 5) \u222a (6, 100)} milliseconds [38]. Every sine wave with period T has a random phase shift drawn from U(0, T ). At every time step, the input to the network is a single scalar representing the amplitude of the signal. Since sinusoid are continuous signals, this tasks allows to study whether Skip RNNs converge to the same solutions when their parameters are fixed but the sampling period is changed. We study two different sampling periods, Ts = {0.5, 1} milliseconds, for each set of hyperparameters. We train RNNs with 110 units each on input signals of 100 milliseconds. Batches are stratified, containing the same number of samples for each class, yielding a 50% chance accuracy. The last state of the RNN is fed into a 2-way classifier and trained with cross-entropy loss. We consider that a model is able to solve the task when it achieves an accuracy over 99% on a held-out set of examples.\nTable 2 summarizes results for this task. When no cost per sample is set (\u03bb = 0), the number of updates differ under different sampling conditions. We attribute this behavior to the potentially large number of local minima in the cost function, since there are numerous subsampling patterns for which the task can be successfully solved and we are not explicitly encouraging the network to converge to a particular solution. On the other hand, when \u03bb > 0 Skip RNN models with the same cost per sample use roughly the same number of input samples even when the sampling frequency is doubled. This is a desirable property, since solutions are robust to oversampled input signals."}, {"heading": "4.3 MNIST Classification from a Sequence of Pixels", "text": "The MNIST handwritten digits classification benchmark [32] is traditionally addressed with Convolutional Neural Networks (CNNs) that can efficiently exploit spatial dependencies through weight\nsharing. By flattening the 28\u00d7 28 images into 784-d vectors, however, it can be reformulated as a challenging task for RNNs where long term dependencies need to be leveraged [31]. We follow the standard data split and set aside 5,000 training samples for validation purposes. After processing all pixels with an RNN with 110 units, the last hidden state is fed into a linear classifier predicting the digit class. All models are trained for 600 epochs to minimize cross-entropy loss.\nTable 3 summarizes classification results on the test set after 600 epochs of training. Skip RNNs are not only able to solve the task using fewer updates than their counterparts, but also show a lower variation among runs and train faster (see Figure 3). We hypothesize that skipping updates make the Skip RNNs work on shorter subsequences, simplifying the optimization process and allowing the networks to capture long term dependencies more easily. A similar behavior was observed for Phased LSTM, where increasing the sparsity of cell updates accelerates training for very long sequences [38].\nSequences of pixels can be reshaped back into 2D images, allowing to visualize the samples used by the RNNs as a sort of hard visual attention model [49]. Examples such as the ones depicted in Figure 4 show how the model learns to skip pixels that are not discriminative, such as the padding regions in the top and bottom of images. Similarly to the qualitative results for the adding task (Section 4.1), attended samples vary depending on the particular input being given to the network."}, {"heading": "4.4 Sentiment Analysis on IMDB", "text": "The IMDB dataset [34] contains 25,000 training and 25,000 testing movie reviews annotated into two classes, positive and negative sentiment, with an approximate average length of 240 words per review. We set aside 15% of training data for validation purposes. Words are embedded into 300-d vector representations before being fed to an RNN with 128 units. The embedding matrix is initialized using pre-trained word2vec4 embeddings [36] when available, or random vectors drawn from U(\u22120.25, 0.25) otherwise [27]. Dropout with rate 0.2 is applied between the last RNN state\n4https://code.google.com/archive/p/word2vec/\nand the classification layer in order to reduce overfitting. We evaluate the models on sequences of length 200 and 400 by cropping longer sequences and padding shorter ones [51].\nResults on the test are reported in Table 4. In a task where it is hard to predict which input tokens will be discriminative, the Skip RNN models are able to achieve similar accuracy rates to the baseline models while reducing the number of required updates. These results highlight the trade-off between accuracy and the available computational budget, since a larger cost per sample results in lower accuracies. However, allowing the network to select which samples to use instead of cropping sequences at a given length boosts performance, as observed for the Skip LSTM (length 400, \u03bb = 10\u22124), which achieves a higher accuracy than the baseline LSTM (length 200) while seeing roughly the same number of words per review. A similar behavior can be seen for the Skip RNN models with \u03bb = 10\u22123, where allowing them to select words from longer reviews boosts classification accuracy while using a comparable number of tokens per sequence."}, {"heading": "4.5 Action classification on UCF-101", "text": "One of the most accurate and scalable pipelines for video analysis consists in extracting frame level features with a CNN and modeling their temporal evolution with an RNN [17, 52]. Videos are commonly recorded at high sampling rates, rapidly generating long sequences with strong temporal redundancy that are challenging for RNNs. Moreover, processing frames with a CNN is computationally expensive and may become prohibitive for high framerates. These issues have been alleviated in previous works by using short clips [17] or by downsampling the original data in order to cover long temporal spans without increasing the sequence length excessively [52]. Instead of addressing the long sequence problem at the input data level, we train RNN models using long frame sequences without downsampling and let the network learn which frames need to be used.\nUCF-101 [44] is a dataset containing 13,320 trimmed videos belonging to 101 different action categories. We use 10 seconds of video sampled at 25fps, cropping longer ones and padding shorter\nexamples with empty frames. Activations in the Global Average Pooling layer from a ResNet-50 [22] CNN pretrained on the ImageNet dataset [16] are used as frame level features, which are fed into two stacked RNN layers with 512 units each. The weights in the CNN are not tuned during training to reduce overfitting. The hidden state in the last RNN layer is used to compute the update probability for the Skip RNN models.\nWe evaluate the different models on the first split of UCF-101 and report results in Table 5. Skip RNN models do not only improve the classification accuracy with respect to the baseline, but require very few updates to do so, possibly due to the low motion between consecutive frames resulting in frame level features with high temporal redundancy [42]. Moreover, Figure 5 shows how models performing fewer updates converge faster thanks to the gradients being preserved during longer spans when training with backpropagation through time."}, {"heading": "5 Conclusion", "text": "We presented Skip RNNs as an extension to existing recurrent architectures enabling them to skip state updates thereby reducing the number of sequential operations in the computation graph. Unlike other approaches, all parameters in Skip RNN are trained with backpropagation without requiring the introduction of task-dependent hyperparameters like a dropout rate. Experiments conducted with LSTMs and GRUs showed that Skip RNNs can match or in some cases even outperform the baseline models while relaxing their computational requirements. Skip RNNs provide faster and more stable training for long sequences and complex models, likely due to gradients being backpropagated through fewer time steps resulting in a simpler optimization task. Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].\nThe presented results motivate several new research directions toward designing efficient RNN architectures. Introducing stochasticity in neural network training has proven beneficial for generalization [45, 30], and in this work we propose a deterministic rounding operation with stochastic sampling. We showed that the addition of a loss term penalizing the number of updates is important in the performance of Skip RNN and allows flexibility to specialize to tasks of varying budget requirements, e.g. the cost can be increased at each time step to encourage the network to emit a decision earlier [1], or the number of updates can be strictly bounded and enforced. Finally, understanding and analyzing the patterns followed by the model when deciding whether to update or copy the RNN state may provide insight for developing better and more efficient architectures."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Spanish Ministry of Economy and Competitivity under contracts TIN2012-34557 by the BSC-CNS Severo Ochoa program (SEV-2011-00067), and contracts TEC2013-43935-R and TEC2016-75976-R. It has also been supported by grants 2014-SGR-1051 and 2014-SGR-1421 by the Government of Catalonia, and the European Regional Development Fund (ERDF). We would also like to thank the technical support team at the Barcelona Supercomputing Center."}], "references": [{"title": "Encouraging LSTMs to anticipate actions very early", "author": ["M.S. Aliakbarian", "F. Saleh", "M. Salzmann", "B. Fernando", "L. Petersson", "L. Andersson"], "venue": "arXiv preprint arXiv:1703.07023", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2017}, {"title": "Dynamic capacity networks", "author": ["A. Almahairi", "N. Ballas", "T. Cooijmans", "Y. Zheng", "H. Larochelle", "A. Courville"], "venue": "ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "SLSP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Y. Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Minimally needed evidence for complex event recognition in unconstrained videos", "author": ["S. Bhattacharya", "F.X. Yu", "S.-F. Chang"], "venue": "ICMR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Quasi-recurrent neural networks", "author": ["J. Bradbury", "S. Merity", "C. Xiong", "R. Socher"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Recurrent batch normalization", "author": ["T. Cooijmans", "N. Ballas", "C. Laurent", "\u00c7. G\u00fcl\u00e7ehre", "A. Courville"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank approximations for conditional feedforward computation in deep neural networks", "author": ["A. Davis", "I. Arel"], "venue": "arXiv preprint arXiv:1312.4461", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In ICASSP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["E. Grefenstette", "K.M. Hermann", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural networks for machine learning", "author": ["G. Hinton"], "venue": "Coursera video lectures", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "ECCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Variable computation in recurrent neural networks", "author": ["Y. Jernite", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A. Courville"], "venue": "Zoneout: Regularizing rnns by randomly preserving hidden activations. In ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2017}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution", "author": ["L. Liu", "J. Deng"], "venue": "arXiv preprint arXiv:1701.00299", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "ACL", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Deciding how to decide: Dynamic routing in artificial neural networks", "author": ["M. McGill", "P. Perona"], "venue": "ICML", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "Recurrent models of visual attention. In NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Phased LSTM: accelerating recurrent network training for long or event-based sequences", "author": ["D. Neil", "M. Pfeiffer", "S. Liu"], "venue": "NIPS", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Training a subsampling mechanism in expectation", "author": ["C. Raffel", "D. Lawson"], "venue": "ICLR Workshop Track", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer", "author": ["N. Shazeer", "A. Mirhoseini", "K. Maziarz", "A. Davis", "Q. Le", "G. Hinton", "J. Dean"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2017}, {"title": "Clockwork convnets for video semantic segmentation", "author": ["E. Shelhamer", "K. Rakelly", "J. Hoffman", "T. Darrell"], "venue": "arXiv preprint arXiv:1608.03609", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning visual storylines with skipping recurrent neural networks", "author": ["G.A. Sigurdsson", "X. Chen", "A. Gupta"], "venue": "ECCV", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "arXiv preprint arXiv:1212.0402", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Leaving some stones unturned: dynamic feature prioritization for activity detection in streaming video", "author": ["Y.-C. Su", "K. Grauman"], "venue": "ECCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1992}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In ICML", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end learning of action detection from frame glimpses in videos", "author": ["S. Yeung", "O. Russakovsky", "G. Mori", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to skim text", "author": ["A.W. Yu", "H. Lee", "Q.V. Le"], "venue": "ACL", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2017}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J. Yue-Hei Ng", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "CVPR", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ICLR", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "Gated units, such as the Long Short-Term Memory [24] (LSTM) and the Gated Recurrent Unit [11] (GRU), were designed to deal with the vanishing gradients problem commonly found in RNNs [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 129, "endOffset": 132}, {"referenceID": 52, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "These architectures have become popularized thanks to their impressive results in a variety of tasks such as machine translation [5], language modeling [53] or speech recognition [19].", "startOffset": 179, "endOffset": 183}, {"referenceID": 37, "context": "These challenges include throughput degradation, slower convergence during training and memory leakage, even for gated architectures [38].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 5, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 14, "context": "Sequence shortening techniques, which can be seen as a sort of conditional computation [7, 6, 15] in time, can alleviate these issues.", "startOffset": 87, "endOffset": 97}, {"referenceID": 6, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 32, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 1, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 34, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 40, "context": "Conditional computation has been shown to allow gradual increases in model capacity without a proportional increases in computational cost by exploiting certain computation paths for each input [7, 33, 2, 35, 41].", "startOffset": 194, "endOffset": 212}, {"referenceID": 17, "context": "This idea has been extended in the temporal domain, either by learning how many times an input needs to be pondered before moving to the next one [18] or building RNNs whose number of layers depends on the input data [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "This idea has been extended in the temporal domain, either by learning how many times an input needs to be pondered before moving to the next one [18] or building RNNs whose number of layers depends on the input data [12].", "startOffset": 217, "endOffset": 221}, {"referenceID": 25, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 191, "endOffset": 199}, {"referenceID": 37, "context": "Some works have addressed time-dependent computation in RNNs by updating only a fraction of the hidden states based on the current hidden state and input [26], or following periodic patterns [29, 38].", "startOffset": 191, "endOffset": 199}, {"referenceID": 11, "context": "This is related to the UPDATE and COPY operations in hierarchical multiscale RNNs [12], but applied to the whole stack of RNN layers at the same time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Learning whether to update or copy the hidden state through time steps can be seen as a learnable Zoneout mask [30] which is shared between all the units in the hidden state.", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "Similarly, it can be interpretted as an input-dependent recurrent version of stochastic depth [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 130, "endOffset": 134}, {"referenceID": 48, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "Selecting parts of the input signal is similar in spirit to the hard attention mechanisms that have been applied to image regions [37], where only some patches of the input image are attended in order to generate captions [49] or detect objects [3].", "startOffset": 245, "endOffset": 248}, {"referenceID": 42, "context": "Subsampling input sequences has been explored for visual storylines generation [43], although jointly optimizing the RNN weights and the subsampling mechanism is computationally unfeasible and the Expectation Maximization algorithm is used instead.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 122, "endOffset": 125}, {"referenceID": 49, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 228, "endOffset": 236}, {"referenceID": 45, "context": "Similar research has been conducted for video analysis tasks, discovering minimally needed evidence for event recognition [9] and training agents that decide which frames need to be observed in order to localize actions in time [50, 46].", "startOffset": 228, "endOffset": 236}, {"referenceID": 39, "context": "Motivated by the advantages of training recurrent models on shorter subsequences, efforts have been conducted towards learning differentiable subsampling mechanisms [40], although the computational complexity of the proposed method precludes its application to long input sequences.", "startOffset": 165, "endOffset": 169}, {"referenceID": 9, "context": "Accelerating inference in RNNs is difficult due to their inherently sequential nature, leading to the design of Quasi-Recurrent Neural Networks [10], which relax the temporal dependency between consecutive steps.", "startOffset": 144, "endOffset": 148}, {"referenceID": 50, "context": "With the goal of speeding up RNN inference, LSTM-Jump [51] augments an LSTM cell with a classification layer that will decide how many steps to jump between RNN updates.", "startOffset": 54, "endOffset": 58}, {"referenceID": 47, "context": "Despite its promising results on text tasks, the model needs to be trained with REINFORCE [48], which requires the definition of a reward signal.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "At every time step t, the probability \u0169t+1 \u2208 [0, 1] of performing a state update at t+ 1 is emitted.", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "where \u03c3 is the sigmoid function and fbinarize : [0, 1] \u2192 {0, 1} binarizes the input value.", "startOffset": 48, "endOffset": 54}, {"referenceID": 37, "context": "Unlike some other models that aim to reduce the average number of operations per step [38, 26], ours enables skipping steps completely.", "startOffset": 86, "endOffset": 94}, {"referenceID": 25, "context": "Unlike some other models that aim to reduce the average number of operations per step [38, 26], ours enables skipping steps completely.", "startOffset": 86, "endOffset": 94}, {"referenceID": 37, "context": "Replacing RNN updates with copy operations increases the memory of the network and its ability to model long term dependencies even for gated units, since the exponential memory decay observed in LSTM and GRU [38] is alleviated.", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 14, "endOffset": 21}, {"referenceID": 3, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 14, "endOffset": 21}, {"referenceID": 52, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 38, "endOffset": 46}, {"referenceID": 29, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 38, "endOffset": 46}, {"referenceID": 25, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 69, "endOffset": 77}, {"referenceID": 37, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 69, "endOffset": 77}, {"referenceID": 19, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 102, "endOffset": 110}, {"referenceID": 46, "context": "normalization [13, 4], regularization [53, 30], variable computation [26, 38] or even external memory [20, 47].", "startOffset": 102, "endOffset": 110}, {"referenceID": 47, "context": "A common method for optimizing functions involving discrete variables is REINFORCE [48], although several estimators have been proposed for the particular case of neurons with binary outputs [7].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "A common method for optimizing functions involving discrete variables is REINFORCE [48], although several estimators have been proposed for the particular case of neurons with binary outputs [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 22, "context": "the straight-through estimator [23], which consists in approximating the step function by the identity when computing gradients during the backward pass:", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 129, "endOffset": 132}, {"referenceID": 13, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 11, "context": "This yields a biased estimator that has proven more efficient than other unbiased but high-variance estimators such as REINFORCE [7] and has been successfully applied in different works [14, 12].", "startOffset": 186, "endOffset": 194}, {"referenceID": 32, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 34, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 17, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 25, "context": "The proposed model can be encouraged to perform fewer state updates through additional loss terms, a common practice in neural networks with dynamically allocated computation [33, 35, 18, 26].", "startOffset": 175, "endOffset": 191}, {"referenceID": 27, "context": "Training is performed with Adam [28], learning rate of 10\u22124, \u03b21 = 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 38, "context": "Gradient clipping [39] with a threshold of 1 is applied to all trainable variables.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "We revisit one of the original LSTM tasks [24], where the network is given a sequence of (value, marker) tuples.", "startOffset": 42, "endOffset": 46}, {"referenceID": 37, "context": "[38], where the first marker is randomly placed among the first 10% of samples (drawn with uniform probability) and the second one is placed among the last half of samples (drawn with uniform probability).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "This criterion is a stricter version of the one followed in [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "A similar behavior was observed in other augmented RNN architectures such as Neural Stacks [21].", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "In this experiment, the network is trained to classify between sinusoids whose period is in range T \u223c U (5, 6) milliseconds and those whose period is in range T \u223c {(1, 5) \u222a (6, 100)} milliseconds [38].", "startOffset": 196, "endOffset": 200}, {"referenceID": 31, "context": "The MNIST handwritten digits classification benchmark [32] is traditionally addressed with Convolutional Neural Networks (CNNs) that can efficiently exploit spatial dependencies through weight", "startOffset": 54, "endOffset": 58}, {"referenceID": 30, "context": "By flattening the 28\u00d7 28 images into 784-d vectors, however, it can be reformulated as a challenging task for RNNs where long term dependencies need to be leveraged [31].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "A similar behavior was observed for Phased LSTM, where increasing the sparsity of cell updates accelerates training for very long sequences [38].", "startOffset": 140, "endOffset": 144}, {"referenceID": 48, "context": "Sequences of pixels can be reshaped back into 2D images, allowing to visualize the samples used by the RNNs as a sort of hard visual attention model [49].", "startOffset": 149, "endOffset": 153}, {"referenceID": 33, "context": "The IMDB dataset [34] contains 25,000 training and 25,000 testing movie reviews annotated into two classes, positive and negative sentiment, with an approximate average length of 240 words per review.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "The embedding matrix is initialized using pre-trained word2vec4 embeddings [36] when available, or random vectors drawn from U(\u22120.", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "25) otherwise [27].", "startOffset": 14, "endOffset": 18}, {"referenceID": 50, "context": "We evaluate the models on sequences of length 200 and 400 by cropping longer sequences and padding shorter ones [51].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "One of the most accurate and scalable pipelines for video analysis consists in extracting frame level features with a CNN and modeling their temporal evolution with an RNN [17, 52].", "startOffset": 172, "endOffset": 180}, {"referenceID": 51, "context": "One of the most accurate and scalable pipelines for video analysis consists in extracting frame level features with a CNN and modeling their temporal evolution with an RNN [17, 52].", "startOffset": 172, "endOffset": 180}, {"referenceID": 16, "context": "These issues have been alleviated in previous works by using short clips [17] or by downsampling the original data in order to cover long temporal spans without increasing the sequence length excessively [52].", "startOffset": 73, "endOffset": 77}, {"referenceID": 51, "context": "These issues have been alleviated in previous works by using short clips [17] or by downsampling the original data in order to cover long temporal spans without increasing the sequence length excessively [52].", "startOffset": 204, "endOffset": 208}, {"referenceID": 43, "context": "UCF-101 [44] is a dataset containing 13,320 trimmed videos belonging to 101 different action categories.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Activations in the Global Average Pooling layer from a ResNet-50 [22] CNN pretrained on the ImageNet dataset [16] are used as frame level features, which are fed into two stacked RNN layers with 512 units each.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Activations in the Global Average Pooling layer from a ResNet-50 [22] CNN pretrained on the ImageNet dataset [16] are used as frame level features, which are fed into two stacked RNN layers with 512 units each.", "startOffset": 109, "endOffset": 113}, {"referenceID": 41, "context": "Skip RNN models do not only improve the classification accuracy with respect to the baseline, but require very few updates to do so, possibly due to the low motion between consecutive frames resulting in frame level features with high temporal redundancy [42].", "startOffset": 255, "endOffset": 259}, {"referenceID": 28, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 37, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 11, "context": "Moreover, the introduced computational savings are better suited for modern hardware than those methods that reduce the amount of computation required at each time step [29, 38, 12].", "startOffset": 169, "endOffset": 181}, {"referenceID": 44, "context": "Introducing stochasticity in neural network training has proven beneficial for generalization [45, 30], and in this work we propose a deterministic rounding operation with stochastic sampling.", "startOffset": 94, "endOffset": 102}, {"referenceID": 29, "context": "Introducing stochasticity in neural network training has proven beneficial for generalization [45, 30], and in this work we propose a deterministic rounding operation with stochastic sampling.", "startOffset": 94, "endOffset": 102}, {"referenceID": 0, "context": "the cost can be increased at each time step to encourage the network to emit a decision earlier [1], or the number of updates can be strictly bounded and enforced.", "startOffset": 96, "endOffset": 99}], "year": 2017, "abstractText": "Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://github.com/imatge-upc/skiprnn-2017-telecombcn.", "creator": "LaTeX with hyperref package"}}}