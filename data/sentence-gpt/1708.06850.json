{"id": "1708.06850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems", "abstract": "The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. networks with an expected power density of about 9 times the current state of an actual world.\n\n\n\n\nIn this article, we use the following data to show that our network has been able to use the Koopman method for the analysis of large network data using a combination of parallel algorithms and a nonlinear model discovery algorithm. While the approach we used to explore was relatively simple to implement in a practical manner, we have found that in the near future our main focus will be on exploring other high-level networks with large computational complexity. We will also present several practical examples of this approach, including the high performance and low-throughput method of using a highly dynamic dynamical system discovery algorithm to create applications that will allow us to measure both large and nonlinear networks in a way that will increase our accuracy in a given situation.\nTo explore the feasibility of our approach, we present a series of papers on the subject of a distributed system discovery algorithm.\nA distributed system discovery algorithm is an approach based on the idea that it is necessary to learn a wide range of information about a single system and to use them in a variety of applications. This approach is well documented, in several studies, and is widely used in many other field areas including network analysis, distributed system discovery and distributed network discovery, including machine learning, distributed network discovery and distributed network discovery.\nTo be considered, a distributed system discovery algorithm is an approach based on the idea that it is necessary to learn a wide range of information about a single system and to use them in a variety of applications. This approach is well documented, in several studies, and is widely used in many other field areas including network analysis, distributed system discovery and distributed network discovery, including machine learning, distributed system discovery and distributed network discovery.\nWe will address two questions regarding distributed systems discovery algorithm:\nWhat can we do?\nThis research focus on the role of distributed network discovery for distributed systems discovery, or whether they provide an equivalent (as opposed to only one) or not, the use of a distributed system discovery algorithm to identify network problems or to identify network problems.\nHow do we perform this research?\nThe first question we will ask for is what", "histories": [["v1", "Tue, 22 Aug 2017 23:32:19 GMT  (6231kb,D)", "http://arxiv.org/abs/1708.06850v1", "16 pages, 5 figures"]], "COMMENTS": "16 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["enoch yeung", "soumya kundu", "nathan hodas"], "accepted": false, "id": "1708.06850"}, "pdf": {"name": "1708.06850.pdf", "metadata": {"source": "CRF", "title": "Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems", "authors": ["Enoch Yeung"], "emails": ["enoch.yeung@pnnl.gov", "soumya.kundu@pnnl.gov", "nathan.hodas@pnnl.gov"], "sections": [{"heading": null, "text": "The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a deep learning framework for learning Koopman operators of nonlinear dynamical systems. We show that this novel method automatically selects efficient deep dictionaries, outperforming stateof-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict quantitatively 100 steps into the future, using only a single timepoint, and qualitative oscillatory behavior 400 steps into the future. ar X iv :1 70 8.\n06 85\n0v 1"}, {"heading": "1 Introduction", "text": "In 1931, B. O. Koopman published a paper showing that the evolution of any set of observables on a dynamical system can be expressed through the action of an infinite dimensional linear operator, the Koopman operator [1]. Because the Koopman operator is a canonical representation of any autonomous dynamical system, in principle, its use can bring to bear linear analysis methods on nonlinear systems.\nThe Koopman operator is especially powerful for inferring properties of dynamical systems that are either partially or completely unknown or that are too complex to express using standard methods in analysis. Examples of such systems include biological networks, extremely large physical systems (which are intractable to analyze as white-box models), social networks, cyber-physical communication networks, and distributed computing systems that are subject to varying degrees of uncertainty. For this reason, the Koopman operator has gained attention as an effective tool for data-driven model discovery [2, 3, 4].\nThe prevailing method for learning the Koopman operator from data is dynamic mode decomposition. Dynamic mode decomposition is the process of identifying a linear operator from temporally or spatially-linked data, ultimately with the objective of characterizing the spectrum of the operator. There are many variants of dynamic mode decomposition, but the most recent advances in Koopman operator learning have emerged from extended dynamic mode decomposition.\nIn extended dynamic mode decomposition, the idea is to lift the set of system observables from its native vector space into a higher dimensional space, usually through a nonlinear transformation [2]. When studying nonlinear systems, the hope is to lift the observables onto a nonlinear manifold, where there trajectories then evolve according to a linear law (the Koopman operator). The set of nonlinear transformations is often referred to as a dictionary, while the choice of what dictionary functions to include is left to the discretion of the scientist, or based on a priori knowledge of the physical system of interest.\nThus, extended dynamic mode decomposition allows for the study of complex nonlinear phenomena, e.g. the spectrum of nonlinear flows, but it is currently limited by a scientist\u2019s knowledge or intuition. This inherently makes the learning process manual, since a dictionary must be intuited, implemented, and then evaluated, followed by careful review of the results by the scientist. If the dictionary is not sufficiently complex, then the scientist must engineer prospective functions into the dictionary (see Figure 1).\nIn this paper we introduce a deep learning approach for training Koopman operators from data. We show that deep neural networks can be used to both efficiently generate dictionaries and traverse\nfunction space during training, to obtain an accurate estimate of the Koopman operator. We argue that deep neural networks lend themselves to larger problems than existing dynamic mode decomposition methods and highlight improved performance on a range of application problems. Most importantly, we show how deep Koopman operators can be used to learn higher fidelity Koopman operator models, which improve on state-of-the-art multi-step prediction tasks (long-term forecasting). We show how single-step prediction error can be misleading when evaluating dynamic mode decomposition (DMD) approaches and discuss the advantages of training dictionaries over one-shot computations (using DMD).\nThe ideas in this paper are complementary to the recent work presented in [5]. In [5], the authors consider a 3-layer neural network with fixed depth and tanh activation functions to learn the dynamics of the Duffing oscillator and Kuramoto-Sivahinsky system. Similarly we consider the use neural networks to dynamically update dictionaries for Koopman operator learning. Additionally, our computational framework is implemented Tensorflow, allowing for direct manipulation of the objective functions, variations in the choice of activation functions such as RELUS, cRELUs, or ELUs over the entire network, variations in network depth, width, and composition, as well as direct access to deep training algorithms, e.g. AdaGrad [6], ADAM [7] with hiearchical dropout [8] for network regularization."}, {"heading": "2 Deep Dynamic Mode Decomposition for Koopman Operator Learning", "text": ""}, {"heading": "2.1 Extended Dynamic Mode Decomposition", "text": "We consider a discrete-time nonlinear dynamical system of the form Consider a discrete time system of the form xn+1 = f(xn)\nyk = h(xn) (1)\nwhere f \u2208 Rn is continuously differentiable and h \u2208 Rp is continuously differentiable. The function f is the state-space model and the function h maps current state values xk \u2208 Rn to a vector of observables or outputs yk \u2208 Rp. The Koopman operator of this system must satisfy the equation\n\u03c8(yn+1) = K(\u03c8(yn))\nyn = h(\u03c8(yn)) (2)\nwhere \u03c8(yk) \u2208 Rm is an observable function, where m \u2264 \u221e, that defines the lifted space of observables. In theory, \u03c8 must belong to a space of observables under which \u03c8(yk) is K-invariant for all k. This ensures that the Koopman operator comprehensively describes the flow of the observable\nExtended Dynamic Mode Decomposition\ntrajectory (y1, y2, ...). More precisely, we suppose that the span of the dictionary matrix\nD \u2261 \u03a8(y) = [ \u03c81(y) \u03c82(y) . . . ] (3)\ncontains all observables \u03c8(yk) for all k.\nThe challenge, in practice, is that the dictionary matrix D is unknown. This makes it difficult to ascertain what and how many functions to include, let alone the minimal number of functions, to ensure K-invariance. In reality, the data is provided and set as a series of points (y1, ..., yN ), so whatever functions picked must satisfy the invariance property with respect to the set\n{\u03c8(y1), ...., \u03c8(yN )}.\nIn extended dynamic mode decomposition, we use an expansive set of orthonomal polynomial basis functions. However, this approach does not scale well and suffers from overfitting with an increasing number of dictionary functions. Each element in yk, k = 1, ..., p is lifted to m polynomials, making for mp distinct dictionary functions. These dictionary functions are often monomial, so quadratic or cubic cross terms are computed to model nonlinear interaction between states, which further exponentiates the size of the dictionary matrix. Finally, since the Koopman operator is unknown, it is generally presumed to be fully parameterized and gradually regularized during optimization. This makes it difficult to solve problems with even only 10-20 outputs, since the Koopman operator will quickly grow to contain thousands of potential entries. Thus, it is critical to regularize the learning problem, to minimize overfit. For dynamic mode decomposition of a Koopman operator, the learning problem is typically cast as follows.\nLet \u03c8 be an observables vector defined on yn, for n = 1, ..., N . We stack the observables in two matrices Yf and Yp where\nYf =  \u03c8(y (0) n+1) . . . \u03c8(y (0) 1 ) ... . . . ...\n\u03c8(y (p) n+1) . . . \u03c8(y (p) 1 )\n , Yp =  \u03c8(y (0) n ) . . . \u03c8(y (0) 0 ) ... . . . ...\n\u03c8(y (p) n ) . . . \u03c8(y (p) 0 )  . (4) From these definitions, it is straightforward to horizontally stack the Koopman equation for a single observable update (2) to obtain\nYf = KYp. (5)\nThe L1-regularized version of this problem is\n||Yf \u2212KYp||2 + \u03bb||K||2,1 (6)\nwhere the second term is computed first as the 2-norm of each column vector, followed by a 1-norm on all of the 2-norms, to encourage sparsity. The parameter \u03bb is a hyper-parameter for tuning the sparsity constraint. We remark that the problem formulated here is the transpose of the problem formulated in [9] and is convex. This formulation thus is guaranteed to converge to a unique global estimate of the Koopman operator, so long as it is computationally tractable. Our subsequent experimental results will draw comparisons against this state-of-the-art approach for estimating Koopman operators."}, {"heading": "2.2 Deep Dynamic Mode Decomposition", "text": "In deep dynamic mode decomposition, we define the dictionary as the output of a deep neural network. The deep neural network can be specified to have any architecture upstream, so long as it takes yk as an input. The advantage of employing deep neural networks is that they have rich expressivity [10, 11], can be trained automatically, and scale to extremely large networks [12]. With each additional layer, the number of new nonlinear functions grows combinatorially, but only with a linear increase in the model parameters. We setup the deep Koopman operator learning problem as follows. Define the deep dictionary as\nDNN = N\u03a8(yn) (7)\nwhere N\u03a8(yn) is a neural network of choice. The Koopman operator problem is cast similarly as with extended dynamic mode decomposition:\nmin K,\u03b8 ||N\u03a8(yn+1, \u03b8)\u2212KN\u03a8(yn, \u03b8)||2 + \u03bb1||K||2 + \u03bb2||(\u03b8)||1 (8)\nwhere\n\u03b8 = (W1, ...,Wd, b1, ..., bd, ...)\nare the numerical parameters of the neural network. For example, in the case of a multi-layer deep feedforward residual network,\nN\u03a8(yn) = hd \u25e6 hd\u22121 \u25e6 . . . \u25e6 h1(yn) (9)\nwith hj(hj\u22121) = \u03c3j(Wj\u22121hj\u22121 +bj\u22121), andWj \u2208 Rrj\u00d7rj\u22121 . The parameters d, rj are hyperparameters that are optimized using a combination of dropout and regularization, while the the activation function \u03c3j can be chosen to be one of the many activation functions that result in fast convergence, e.g. ELU, cRELU, or the RELU.\nGiven this formulation, we remark it is also possible to initialize a neural network, akin to selecting a randomly parameterized dictionary, and directly solve the dynamic mode decomposition problem. Computing Yf and Yp as before, yields\nYf =  N\u03a8(y (0) n+1) . . . N\u03a8(y (0) 1 ) ... . . . ...\nN\u03a8(y (p) n+1) . . . N\u03a8(y (p) 1 )\n , Yp =  N\u03a8(y (0) n ) . . . N\u03a8(y (0) 0 ) ... . . . ...\nN\u03a8(y (p) n ) . . . N\u03a8(y (p) 0 )  . (10) However, we observed in practice that this approach did not produce accurate estimates of the Koopman operator. The key difference between deep Koopman operator learning and dynamic mode decomposition is that in deep Koopman operator learning, the dictionary and the Koopman operator\nare learned simultaneously. This increases the number of degrees of freedom in the problem, but no more than was initially present. In reality, the dictionary functions and the Koopman operator are unknown. Dynamic mode decomposition simplifies the problem by having the user impose a particular choice of dictionary, followed by direct optimization of the Koopman operator assuming a fixed dictionary. However, both (sets of) variables are usually unknown in data-driven applications.\nFinally, we remark that once the Koopman operator is obtained, as with dynamic mode decomposition, an eigendecomposition of K will yield the eigenmodes for the neural network basis functions. The weighted combination of those basis functions will define the Koopman eigenfunctions, with corresponding Koopman eigenvalues in the diagonalization or Jordan form of K.\nWhich neural network is used ultimately impacts the nature of the dictionary embedding the observables. If the neural network has memory, e.g. an LSTM network or a recurrent neural network, then the dictionary functions can be viewed as time or space varying. There properties change according to some memory state, which is dynamic from one time point yn to the next yn+1. If the network has hybrid or bifurcative behavior, e.g. a switching network, then the dictionary functions may have the capacity to model multiple invariant subspaces of the Koopman operator simultaneously. The challenge is knowing when the dictionary functions are trained accurately to recover the true Koopman operator. In reality, unless the system can be Carleman linearized [13], it is difficult to ascertain the complete dictionary. An empirical measure for the fidelity of the Koopman operator is its forecasting or prediction accuracy. In the next section, we review a collection of several experiments performed to 1) gain insight into how deep Koopman operators learn basis functions, 2) show the ability of deep Koopman operators to learn completely unknown governing laws for a biological network and forecast multiple steps into the future, and 3) demonstrate the ability of deep Koopman operators to learn an approximate coarse-grained model for a power transmission system."}, {"heading": "3 Experimental Results", "text": ""}, {"heading": "3.1 The Emergence of Koopman Basis Functions During Training", "text": "As a first experiment, we trained deep and classical Koopman operators on randomly generated linear systems with partially observed dynamics. When linear systems have partial state output, their Koopman operator is a coarse grained model of the underlying latent dynamics. For discrete time linear systems, a closed form expression for the Koopman operator exists, called the dynamical structure function [14]. The transfer function entries within a dynamical structure function act as linear fractional transformations on the input signals. Estimating the Koopman operator in this scenario requires discovering the basis functions (in the time-domain) for those transfer function entries.\nWe first performed experiments on a range of small networks, ranging from 4-128 nodes. A subset of the nodes were allocated as \u201clatent\" during the experiment. The systems were simulated in Python and time-series data was collected and divided into training and test data. For optimization, we used the AdaGrad algorithm, as implemented in Tensorflow. We tried randomly shuffling and varying batch size; we found that shuffling did not have a significant effect on the performance of the deep Koopman operator. As expected, if batch size became too small, e.g. less than 10 samples per batch, the algorithm would not converge. All training and validation was done using Python Tensorflow 1.0, on an NVIDIA TitanX Pascal or P40 system.\nThe results of the study are presented in Table 1. The deep Koopman operator had on average 1% one-step training error and one-step test error after 10,000 iterations (see Table 1). Further iterations reduced this error down to 0.1%. In contrast, the E-DMD Koopman operator averaged over 10% on all network sizes. It was also difficult to execute regularized extended dynamic mode decomposition on larger dynamical systems. The primary challenge is that the convex optimization routines run out of memory. We tested the algorithm both on a NVIDIA GPU TitanX Pascal system, as well as a NVIDIA P40 system. For moderate sized networks with more than 10 variables, the quadratic and cubic terms induces rapid expansion in the size of the Koopman operator, which ultimately slowed the CVXPY solvers. On the other hand, we were able to compute the deep Koopman operator for increasingly higher dimensional systems. For small systems, a network width of 20 nodes and a depth of 3-5 layers was sufficient. With networks ranging from 10-100 states, we found the deep Koopman operator could be trained if the neural network was sufficiently deep. This may be because the deeper the neural network, the more efficiently it encodes complex nonlinear functions. Once again, the one-step training error and test error were approximately 1%. To better understand what the deep Koopman operator was learning, we visualized the response of the neural network dictionary functions to basis perturbations along each observable axis. We found that much of the drastic changes in the basis function profile occurs in the first 10,000 iterations of training. During this\nprocess, the basis functions initialize as flat constant functions across all points and gradually train to spike, dip, or ramp in response to input signals (see Figure 3). This allows our Koopman dictionary to be updated during training, while simultaneously optimizing the Koopman operator. This may explain why the deep Koopman operator is able to achieve a higher fidelity than the extended DMD Koopman operator."}, {"heading": "3.2 Learning A Koopman Operator for the Glycolysis Pathway", "text": "The ultimate benchmark of whether or not a Koopman operator has been successfully learned is if the operator is able to predict multiple points into the future, using only a single root time-point. Any small amount of error in the Koopman operator estimate will propagate and compound on itself. Moreover, the ultimate challenge is to learn the Koopman operator on a nonlinear system for which there is no known Carleman linearization or Koopman invariant subspace.\nThe glycolysis pathway is a fundamental pathway to metabolism in biology. We use the standard model introduced in [15], a seven state nonlinear model with Michaelis-Menten dynamics. It is given\nas follows: dS1 dt = J0 \u2212 k1S1S6 1 + (S6/K1) q ,\ndS2 dt = 2 k1S1S6 1 + (S6/K1) q \u2212 k2S2(N \u2212 S5)\u2212 k6S2S5, dS3 dt = k2S2(N \u2212 S5)\u2212 k3S3(A\u2212 S6), dS4 dt = k3S3(A\u2212 S6)\u2212 k4S4S5 \u2212 \u03ba(S4 \u2212 S7), dS5 dt = k2S2(N \u2212 S5)\u2212 k4S4S5 \u2212 k6S2S5, dS6 dt = \u22122 k1S1S6 1 + (S6/K1)q + 2k3S3(A\u2212 S6)\u2212 k5S6, dS7 dt = \u00b5\u03ba(S4 \u2212 S7)\u2212 kS7,\n(11)\nThe network has 19 edges and 7 nodes, autoregulatory feedback, both positive and negative feedback.\nWe consider a challenging scenario, where the network parameters are tuned to oscillate. As seen in Table 1, the deep Koopman operator achieved less than .1% error on all test cases, whether n = 7, 5, or 3 nodes in the glycolysis network were measured. Interestingly, the deep Koopman operator is\nable to predict multiple time-steps into the future (\u223c 100 timepoints) with quantitative accuracy and qualitative accuracy for the full duration of the simulation (500 timepoints)."}, {"heading": "3.3 Learning Koopman Operators on Power Systems", "text": "Our next example is a multi-machine power systems network [16]. There are multiple time-scales of dynamics involved in power systems; transient dynamics represent the fastest of all time-scales. Transient instability often leads to instability in frequency and voltage levels, which can result in cascading blackouts. Transient dynamic models are often calibrated, but small fluctuations in parameters, as well real-time state uncertainty is often unknown. However, measurement data for rotor speed and angle is often available, motivating the use of data-driven methods for discovering real-time relationships between power flow variables and swing dynamics. In addition, discovering a Koopman operator from data enables direct application of linear analysis methods for contingency planning [18], which has traditionally been addressed using brute-force simulation studies [19].\nMore specifically, we are interested in power systems transient dynamics, which models the evolution of rotor angles of the synchronous machines at sub-second to a couple of seconds timescales. At this timescale, the dynamics of interest can be modeled as the classical swing dynamics:\n\u2200i \u2208 {1, 2, . . . , n} : \u03b4\u0307i = \u03c9i , (12a)\n\u03c9\u0307i = 1\nMi (\u2212Di \u03c9i + Pm,i \u2212 Pe,i) , (12b)\nwhere Pe,i = Vi n\u2211 j=1 Vj (Gij cos(\u03b4i \u2212 \u03b4j) +Bij sin(\u03b4i \u2212 \u03b4j)) . (12c)\n\u03b4i and \u03c9i represent the generator rotor angle and speed, respectively. Rotational inertia (Mi) and damping (Di) are the parameters associated with each synchronous machine, while Pm,i is its mechanical power input. Pe,i is the network term that represents the total electrical power output from the generator terminal into the power grid. In the Kron-reduced network representation (each node in the network is a machine), the electrical output is a sum of the total power flowing from the generator to all other generators connected to it. Power flowing between two generators is a function of their voltages (Vi, Vj), rotor angles (\u03b4i, \u03b4j) and the transfer conductance (Gij) and susceptance (Bij). The values of Gij and Bij are zero if there is no power line between i and j. Note that since only the rotor angle difference (and not their absolute values) matter, It is generally customary to use a generator angle as the reference angle, and express all the rotor angles relative to the reference angle.\nThe perturbed system dynamics, assuming a disturbance \u2206i to the initial condition of the relative angle \u03b4i(0) is modeled as\n\u2200i \u2208 {1, 2, . . . , n} : \u03b4\u0307i = \u03c9i + (1)\u2206i\u03b4(t) = \u03c9i + 1ui(t), (13a)\n\u03c9\u0307i = 1\nMi (\u2212Di \u03c9i + Pm,i \u2212 Pe,i) , (13b)\nwhere Pe,i = Vi n\u2211 j=1 Vj (Gij cos(\u03b4i \u2212 \u03b4j) +Bij sin(\u03b4i \u2212 \u03b4j)) . (13c)\nWe next explored the use of deep Koopman operators to learn the dynamics of a IEEE 39 bus benchmark system. The system has 39 buses and 10 generators; it is a classical model system that has been well studied in the context of coherency and spectral modes, especially using the Koopman operator [20]. For a more detailed exposition on the relationships between spectral stability of a data-driven Koopman operator and that of the underlying system, we refer the reader to [21, 22].\nIn our numerical experiments, we sought to characterize the ability of deep dynamic mode decomposition to learn the transient dynamics of a power system over a wide range of initial conditions. For training, we generated 1000 randomly generated trajectories of the swing dynamics. The same training data was provided to both deep and extended dynamic mode decomposition algorithms. For extended dynamic mode decomposition, we used Legendre basis polynomials [2], using L1 regularization to find the best scoring Koopman operator (in terms of 1-step prediction training error). We iterated over a range of maximum polynomial degrees, selecting the best scoring Koopman operator. Our choice of Legendre polynomials was based on the recommendations of [2]. We remark that for the IEEE 39 bus benchmark system, it may be possible that a better choice of bases could be thin-plate radial basis functions (RBFs) or Hermite polynomials.\nDuring deep dynamic mode decomposition, we selected the deep Koopman operator that minimized 1-step prediction training error. At the test stage, we then generated a random initial condition different from any of the initial conditions used to generate training trajectories. The outcomes are plotted in Figure 5. Our benchmark again is multi-step prediction accuracy, given time-series data.\nWe observed that the deep dynamic mode decomposition algorithm performed much better at multistep prediction than Legendre-polynomial based dynamic mode decomposition. Again it is possible that with another choice of dictionary functions, e.g. Hermite polynomials or thin-plate RBFs, we would obtain more accurate results. The insight from these results is that the deep dynamic mode decomposition algorithm appears to be able to adaptively learn the Koopman basis required for both glycolytic and swing dynamic oscillations. Notice that the oscillatory nature of the glycolysis dynamics are more of a relaxation-type oscillator, while the swing dynamics follow a damped\nsinusoidal response. The deep dynamic decomposition algorithm appears to be able capture both types of nonlinearities, albeit with potentially different weighting parameters within the deep neural networks. For the power system example, we found that a 20-layer ELU feedforward network was adequate to recapitulate the swing dynamics of the system. However, Liao et al. showed that a shallow 3-layer neural network with Tikhonov regularization using the tanh activation function, can also learn oscillatory dynamics for the Duffing oscillator [5]. Future research will investigate ways to characterize the minimal depth, or neural net complexity, as well as the impact of different activation functions, required to learn the Koopman operator for different types of systems."}, {"heading": "4 Conclusion", "text": "In this paper we presented a deep learning approach to calculating the Koopman operator. Our automated dictionary approach, enabled by deep learning, improves the performance of the Koopman operator for longer term forecasting. We compared it with the current state of the art methods, extended dynamic mode decomposition, and we showed that in many cases the deep Koopman operator performs relatively well at multistep prediction tasks. Our results suggest that the deep Koopman operator provide a complementary and promising alternative to extended dynamic mode decomposition approaches for data-driven modeling problems of complex nonlinear systems."}], "references": [{"title": "Hamiltonian systems and transformation in Hilbert space", "author": ["B.O. Koopman"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1931}, {"title": "A data-driven approximation of the koopman operator: Extending dynamic mode decomposition", "author": ["M.O. Williams", "I.G. Kevrekidis", "C.W. Rowley"], "venue": "Journal of Nonlinear Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems", "author": ["S.L. Brunton", "J.L. Proctor", "J.N. Kutz"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Dynamic mode decomposition with control", "author": ["J.L. Proctor", "S.L. Brunton", "J.N. Kutz"], "venue": "SIAM Journal on Applied Dynamical Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Extended dynamic mode decomposition with dictionary learning: a data-driven adaptive spectral decomposition of the Koopman operator. arXiv preprint arXiv:1707.00225", "author": ["Q. Li", "F. Dietrich", "E.M. Bollt", "I.G. Kevrekidis"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of machine learning research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "On the expressive power of deep neural networks. arXiv preprint arXiv:1606.05336", "author": ["M. Raghu", "B. Poole", "J. Kleinberg", "S. Ganguli", "J. Sohl-Dickstein"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "On the expressive power of deep architectures.", "author": ["Bengio", "Yoshua", "Olivier Delalleau"], "venue": "International Conference on Algorithmic Learning Theory. Springer Berlin Heidelberg,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Better Mixing via Deep Representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Nonlinear dynamical systems and Carleman linearization", "author": ["K. Kowalski", "W.H. Steeb"], "venue": "World Scientific", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1991}, {"title": "Necessary and sufficient conditions for dynamical structure reconstruction of LTI networks", "author": ["J. Gona\u0327lves", "S. Warnick"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Efficient inference of parsimonious phenomenological models of cellular dynamics using S-systems and alternating regression", "author": ["B.C. Daniels", "I. Nemenman"], "venue": "PloS one,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Power system stability and control", "author": ["P. Kundur"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Power system transient stability analysis using the transient energy function method", "author": ["Fouad", "Abdel-Azia", "Vijay Vittal"], "venue": "Pearson Education,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Massive contingency analysis with high performance computing", "author": ["Z. Huang", "Y. Chen", "Nieplocha", "July"], "venue": "In Power & Energy Society General Meeting,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Nonlinear Koopman modes and coherency identification of coupled swing dynamics", "author": ["Y. Susuki", "I. Mezic"], "venue": "IEEE Transactions on Power Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Nonlinear Koopman modes and a precursor to power system swing instabilities", "author": ["Y. Susuki", "I. Mezic"], "venue": "IEEE Transactions on Power Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Global stability analysis using the eigenfunctions of the Koopman operator", "author": ["A. Mauroy", "I. Mezi"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Koopman published a paper showing that the evolution of any set of observables on a dynamical system can be expressed through the action of an infinite dimensional linear operator, the Koopman operator [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "For this reason, the Koopman operator has gained attention as an effective tool for data-driven model discovery [2, 3, 4].", "startOffset": 112, "endOffset": 121}, {"referenceID": 2, "context": "For this reason, the Koopman operator has gained attention as an effective tool for data-driven model discovery [2, 3, 4].", "startOffset": 112, "endOffset": 121}, {"referenceID": 3, "context": "For this reason, the Koopman operator has gained attention as an effective tool for data-driven model discovery [2, 3, 4].", "startOffset": 112, "endOffset": 121}, {"referenceID": 1, "context": "In extended dynamic mode decomposition, the idea is to lift the set of system observables from its native vector space into a higher dimensional space, usually through a nonlinear transformation [2].", "startOffset": 195, "endOffset": 198}, {"referenceID": 4, "context": "The ideas in this paper are complementary to the recent work presented in [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "In [5], the authors consider a 3-layer neural network with fixed depth and tanh activation functions to learn the dynamics of the Duffing oscillator and Kuramoto-Sivahinsky system.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "AdaGrad [6], ADAM [7] with hiearchical dropout [8] for network regularization.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "AdaGrad [6], ADAM [7] with hiearchical dropout [8] for network regularization.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "AdaGrad [6], ADAM [7] with hiearchical dropout [8] for network regularization.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "The advantage of employing deep neural networks is that they have rich expressivity [10, 11], can be trained automatically, and scale to extremely large networks [12].", "startOffset": 84, "endOffset": 92}, {"referenceID": 9, "context": "The advantage of employing deep neural networks is that they have rich expressivity [10, 11], can be trained automatically, and scale to extremely large networks [12].", "startOffset": 84, "endOffset": 92}, {"referenceID": 10, "context": "The advantage of employing deep neural networks is that they have rich expressivity [10, 11], can be trained automatically, and scale to extremely large networks [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "In reality, unless the system can be Carleman linearized [13], it is difficult to ascertain the complete dictionary.", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "For discrete time linear systems, a closed form expression for the Koopman operator exists, called the dynamical structure function [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "We use the standard model introduced in [15], a seven state nonlinear model with Michaelis-Menten dynamics.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Our next example is a multi-machine power systems network [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "In addition, discovering a Koopman operator from data enables direct application of linear analysis methods for contingency planning [18], which has traditionally been addressed using brute-force simulation studies [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "In addition, discovering a Koopman operator from data enables direct application of linear analysis methods for contingency planning [18], which has traditionally been addressed using brute-force simulation studies [19].", "startOffset": 215, "endOffset": 219}, {"referenceID": 17, "context": "The system has 39 buses and 10 generators; it is a classical model system that has been well studied in the context of coherency and spectral modes, especially using the Koopman operator [20].", "startOffset": 187, "endOffset": 191}, {"referenceID": 18, "context": "For a more detailed exposition on the relationships between spectral stability of a data-driven Koopman operator and that of the underlying system, we refer the reader to [21, 22].", "startOffset": 171, "endOffset": 179}, {"referenceID": 19, "context": "For a more detailed exposition on the relationships between spectral stability of a data-driven Koopman operator and that of the underlying system, we refer the reader to [21, 22].", "startOffset": 171, "endOffset": 179}, {"referenceID": 1, "context": "For extended dynamic mode decomposition, we used Legendre basis polynomials [2], using L1 regularization to find the best scoring Koopman operator (in terms of 1-step prediction training error).", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Our choice of Legendre polynomials was based on the recommendations of [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "showed that a shallow 3-layer neural network with Tikhonov regularization using the tanh activation function, can also learn oscillatory dynamics for the Duffing oscillator [5].", "startOffset": 173, "endOffset": 176}], "year": 2017, "abstractText": "The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a deep learning framework for learning Koopman operators of nonlinear dynamical systems. We show that this novel method automatically selects efficient deep dictionaries, outperforming stateof-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict quantitatively 100 steps into the future, using only a single timepoint, and qualitative oscillatory behavior 400 steps into the future. ar X iv :1 70 8. 06 85 0v 1 [ cs .L G ] 2 2 A ug 2 01 7", "creator": "LaTeX with hyperref package"}}}