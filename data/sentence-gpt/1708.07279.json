{"id": "1708.07279", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Combining Discrete and Neural Features for Sequence Labeling", "abstract": "Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks. Second, they provide an efficient way to estimate, predict, and improve neural network performance. Thus, these are the first large-scale computational models that offer novel, yet non-intuitive computational methods to perform such task. In this paper, we introduce the first practical computational approach to network modeling in the natural language processing community. We examine the first such approach in a systematic language learning library (MIS) as described above. The first method was used to test a specific type of network model that can be applied to discrete input features, including non-local features. The second method has a limited number of properties, including the flexibility and complexity of input features. We are also developing a new and more efficient way to test a specific type of network model, the NLP or other model. To investigate and evaluate the performance of the network model, we demonstrate how neural network models can be applied to multiple input features.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 24 Aug 2017 05:24:26 GMT  (575kb,D)", "http://arxiv.org/abs/1708.07279v1", "Accepted by International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) 2016, April"]], "COMMENTS": "Accepted by International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) 2016, April", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jie yang", "zhiyang teng", "meishan zhang", "yue zhang"], "accepted": false, "id": "1708.07279"}, "pdf": {"name": "1708.07279.pdf", "metadata": {"source": "CRF", "title": "Combining Discrete and Neural Features for Sequence Labeling", "authors": ["Jie Yang", "Zhiyang Teng", "Meishan Zhang", "Yue Zhang"], "emails": ["jie_yang@mymail.sutd.edu.sg", "zhiyang_teng@mymail.sutd.edu.sg", "meishan_zhang@sutd.edu.sg", "yue_zhang@sutd.edu.sg"], "sections": [{"heading": null, "text": "Keywords: Discrete Features, Neural Features, LSTM"}, {"heading": "1 Introduction", "text": "There has been a surge of interest in neural methods for natural language processing over the past few years. Neural models have been explored for a wide\n? Corresponding author 1 This paper was accepted by International Conference on Intelligent Text Processing\nand Computational Linguistics (CICLing) 2016, April; Konya, Turkey.\nar X\niv :1\n70 8.\n07 27\n9v 1\n[ cs\n.C L\n] 2\n4 A\nug 2\n01 7\nrange of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.\nCompared with discrete models with manual indicator features, the main advantage of neural networks is two-fold. First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity. Second, non-linear neural layers can be used for combining features automatically, which saves the expense of feature engineering. The resulting neural features can capture complex non-local syntactic and semantic information, which discrete indicator features can hardly encode.\nOn the other hand, discrete manual features have been studied over decades for many NLP tasks, and effective feature templates have been well-established for them. This source of information can be complementary to automatic neural features, and therefore a combination of the two feature sources can led to improved accuracies. In fact, some previous work has attempted on the combination. Turian et al. [26] integrated word embedding as real-word features into a discrete Conditional Random Field [27] (CRF) model, finding enhanced results for a number of sequence labeling tasks. Guo et al. [28] show that the integration can be improved if the embedding features are carefully discretized. On the reverse direction, Ma et al. [29] treated a discrete perception model as a neural layer, which is integrated into a neural model. Wang & Manning [30] integrated a discrete CRF model and a neural CRF model by combining their output layers. Greg & Dan [6] and Zhang et al. [18] also followed this method. Zhang & Zhang [31] compared various integration methods for parsing, and found that the second type of integration gives better results.\nWe follow Zhang & Zhang [31], investigating the effect of feature combination for a range of sequence labeling tasks, including word segmentation, Part-OfSpeech (POS) tagging and named entity recognition (NER) for Chinese and English, respectively. For discrete features, we adopt a CRF model with stateof-the art features for each specific task. For neural features, we adopt a neural CRF model, using a separated Long Short-Term Memory [32] (LSTM) layer to extract input features. We take standard benchmark datasets for each task. For all the tasks, both the discrete model and the neural model give accuracies that are comparable to the state-of-the-art. A combination of discrete and neural feature, gives significantly improved results with no exception.\nThe main contributions that we make in this investigation include:\n- We systematically investigate the effect of discrete and neural feature combination for a range of fundamental NLP tasks, showing that the two types of feature are complimentary.\n- We systematically report results of a state-of-the-art neural network sequence labeling model on the NLP tasks, which can be useful as reference to future work.\n- We report the best results in the literatures for a number of classic NLP tasks by exploiting neural feature integration.\n- The source code of the LSTM and CRF implementations of this paper are released under GPL at https://github.com/SUTDNLP/NNSegmentation, ../NNPOSTagging and ../NNNamedEntity ."}, {"heading": "2 Related Work", "text": "There has been two main kinds of methods for word segmentation. Xue [33] treat it as a sequence labeling task, using B(egin)/I(nternal) /E(nding)/S(inglecharacter word) tags on each character in the input to indicate its segmentation status. The method was followed by Peng et al. [34], who use CRF to improve the accuracies. Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions. This kind of research is commonly referred to as the character-based method. Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods. On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44]. We focus on the character-based method, which is a typical sequence labeling problem.\nPOS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used. These handcrafted features basically include words, the context of words, word morphologies and word shapes. Various neural network models have also been used for this task. In order to include word morphology and word shape knowledge, a convolutional neural network (CNN) for automatically learning character-level representations is investigated in [48]. Collobert et al. [25] built a CNN neural network for multiple sequence labeling tasks, which gives state-of-the-art POS results. Recurrent neural network models have also been used for this task [49,50]. Huang et al. [50] combines bidirectional LSTM with a CRF layer, their model is robust and has less dependence on word embedding.\nNamed entity recognition is also a classical sequence labeling task in the NLP community. Similar to other tasks, most works access NER problem through feature engineering. McCallum & Li [51] use CRF model for NER task and exploit Web lexicon as feature enhancement. Chieu & Ng [52], Krishnan & Manning [53] and Che et al. [54] tackle this task through non-local features [55]. Besides, many neural models, which are free from handcrafted features, have been proposed in recent years. In Collobert et al. [25] model we referred before, NER task has also been included. Santos et al. [56] boost the neural model by adding character embedding on Collobert\u2019s structure. James et al. [57] take the lead by employing LSTM for NER tasks. Chiu et al. [58] use CNN model to extract character embedding and attach it with word embedding and afterwards feed them into Bi-directional LSTM model. Through adding lexicon features, Chiu\u2019s NER system get state-of-the-art performance on both CoNLL2003 and OntoNotes 5.0 NER datasets."}, {"heading": "3 Method", "text": "The structures of our discrete and neural models are shown in Fig. 1(a) and 1(b), respectively, which are used for all the tasks in this paper. Black and white elements represent binary features for discrete model and gray elements are continuous representation for word/character embedding. The only difference between different tasks are the definition of input and out sequences, and the features used.\nThe discrete model is a standard CRF model. Given a sequence of input x = x1, x2, . . . , xn, it models the output sequence y = y1, y2, . . . , yn by calculating two potentials. In particular, the output clique potential shows the correlation between inputs and output labels,\n\u03a8(x, yi) = exp(\u03b8o \u00b7 fo(x, yi)) (1)\nwhere fo(x, yi) is a feature vector extracted from x and yi, and \u03b8o is a parameter vector.\nThe edge clique potential shows the correlation between consecutive output labels,\n\u03a6(x, yi, yi\u22121) = exp(\u03b8e \u00b7 fe(x, yi, yi\u22121)) (2)\nwhere fe(x, yi, yi\u22121) is a feature vector extracted from x, yi and yi\u22121, and \u03b8e is a parameter vector.\nThe final probability of y is estimated as\np(y|x) = \u220f|y| i=1 \u03a8(x, yi) \u220f|y| j=1 \u03a6(x, yj , yj\u22121)\nZ(x) (3)\nwhere Z(x) is the partition function,\nZ(x) = \u2211 y |y|\u220f i=1 \u03a8(x, yi) |y|\u220f j=1 \u03a6(x, yj , yj\u22121) (4)\nThe overall features {fo(x, yi), fe(x, yi, yi\u22121)} are extracted at each location i according to a set of feature templates for each task.\nThe neural model takes the neural CRF structure. Compared with the discrete model, it replaces the output clique features fo(x, yi)) with a dense neural feature vector hi, which is computed using neural network layer,\nhi = BiLSTM((e(xi\u22122), e(xi\u22121),e(xi), e(xi+1), e(xi+2)),W , b,hi\u22121)\n\u03a8(x, yi) = exp(\u03b8o \u00b7 hi) (5)\nwhere e(xi) is the embedding form of xi, BiLSTM represents bi-directional LSTM structure for calculating hidden state hi for input xi, which considers both the left-to-right and right-to-left information flow in a sequence. The BiLSTM structure receives input of the embeddings from a window of size 5 as shown in Fig. 1(b).\nFor the neural model, the edge clique is replaced with a single transition weight \u03c4(yi, yi\u22121). The remainder of the model is the same as the discrete model\np(y|x) = \u220f|y| i=1 \u03a8(x, yi) \u220f|y| j=1 \u03a6(x, yj , yj\u22121)\nZ(x)\n=\n\u220f|y| i=1 exp(\u03b8o \u00b7 hi) \u220f|y| j=1 exp(\u03c4(yj , yj\u22121))\nZ(x)\n(6)\nHere \u03b8o and \u03c4(yi, yi\u22121) are model parameters, which are different from the discrete model.\nThe joint model makes a concatenation of the discrete and neural features at the output cliques and edge cliques,\n\u03a8(x, yi) = exp(\u03b8o \u00b7 (hi \u2295 fo(x, yi))) \u03a6(x, yj , yj\u22121) = exp(\u03b8e \u00b7 ([\u03c4(yj , yj\u22121)]\u2295 fe(x, yj , yj\u22121)))\n(7)\nwhere the \u2295 operator is the vector concatenation operation. The training objective for all the models is to maximize the margin between gold-standard and model prediction scores. Given a set of training examples {xn,yn)}Nn=1, the objective function is defined as follows\nL = 1\nN N\u2211 n=1 loss(xn,yn,\u0398) + \u03bb 2 ||\u0398||2 (8)\nHere \u0398 is the set of model parameters \u03b8o, \u03b8e, W , b and \u03c4 , and \u03bb is the L2 regularization parameter.\nThe loss function is defined as\nloss(xn,yn,\u0398) = max y\n(p(y|xn;\u0398) + \u03b4(y,yn))\u2212 p(yn|xn,\u0398) (9)\nwhere \u03b4(y,yn) denotes the hamming distance between y and yn. Online Adagrad [59] is used to train the model, with the initial learning rate set to be \u03b7. Since the loss function is not differentiable, a subgradient is used, which is estimated as\n\u2202loss(xn,yn,\u0398) \u2202\u0398 = \u2202p(y\u0302|xn,\u0398) \u2202\u0398 \u2212 \u2202p(yn|xn,\u0398) \u2202\u0398 (10)\nwhere y\u0302 is the predicted label sequence. Chinese Word Segmentation Features. Table 1 shows the features used in Chinese word segmentation. For \u201ctype\u201d, each character has five possibilities: 0/Punctuation, 1/Alphabet, 2/Date, 3/Number and 4/others.\nPOS Tagging Features. Table 2 lists features for POS tagging task on both English and Chinese datasets. The prefix and suffix include 5 characters for English and 3 characters for Chinese.\nNER Features. Table 3 shows the feature template used in English NER task. For \u201cword shape\u201d, each character in word is located in one of these four types: number, lower-case English character, upper-case English character and others. \u201cConnect\u201d word has five categories: \u201cof\u201d, \u201cand\u201d, \u201cfor\u201d, \u201c-\u201d and other. Table 4 presents the features used in Chinese NER task. We extend the features used in Che et al. [54] by adding part-of-speech information. Both POS tag on\nEnglish and Chinese datasets are labeled by ZPar [60], prefix and suffix on two datasets are both including 4 characters. Word clusters in both English and Chinese tasks are same with Che\u2019s work [54]."}, {"heading": "4 Experiments", "text": "We conduct our experiments on different sequence labeling tasks, including Chinese word segmentation, Part-of-speech tagging and Named entity recognition.\nFor these three tasks, their input embeddings are different. For Chinese word segmentation, we take both character embeddings and character bigram embeddings for calculating e(xi). For POS tagging, e(xi) consists of word embeddings and character embeddings. For NER, we include word embeddings, character embeddings and POS embeddings for e(xi). Character embeddings, character bigram embeddings and word embeddings are pretrained separately using word2vec[23]. English word embedding is chosen as SENNA [25]. We make use of Chinese Gigaword Fifth Edition1 to pretrain necessary embeddings for Chinese words. The Chinese corpus is segmented by ZPar [60]. During training, all these aforementioned embeddings will be fine-tuned. The hyper-parameters in our experiments are shown at Table 5. Dropout [61] technology has been used to suppress over-fitting in the input layer."}, {"heading": "4.1 Chinese Word Segmentation", "text": "For Chinese word segmentation, we choose PKU, MSR and CTB60 as evaluation datasets. The PKU and MSR dataset are obtained from SIGHAN Bakeoff 2005 corpus3. We split the PKU and MSR datasets in the same way as Chen et al. [62], and the CTB60 set as Zhang et al. [63]. Table 6 shows the statistical results for these three datasets. We evaluate segmentation accuracy by Precision (P), Recall (R) and F-measure (F).\nThe experiment results of Chinese word segmentation is shown in Table 7. Our joint models shown comparable results to the state-of-the-art results reported by Zhang & Clark [42], where they adopt a word-based perceptron model with carefully designed discrete features. Compared with both discrete and neural models, our joint models can achieve best results among all three datasets. In particular, the joint model can outperform the baseline discrete model by 0.43, 0.42 and 0.37 on PKU, MSR, CTB60 respectively. In order to investigate whether the discrete model and neural model can benefit from each other, we scatter sentence-level segmentation accuracy of two models for three datasets in Fig. 2. As we can see from Fig. 2, some sentences can obtain higher accuracies in the neural model, while other sentences can win out in the discrete model. This common phenomenon among three datasets suggests that the neural model and the discrete model can be combined together to enjoy the merits from each side."}, {"heading": "4.2 POS Tagging", "text": "We compare our models on both English and Chinese datasets for the POS tagging task. The English dataset is chosen following Toutanova et al. [64] and Chinese dataset by Li et al. [65] on CTB. Statistical results are shown in Table 8. Toutanova\u2019s model [64] exploits bidirectional dependency networks to capture\nboth preceding and following tag contexts for English POS tagging task. Li et al. [65] utilize heterogeneous datasets for Chinese POS tagging through bundling two sets of tags and training in enlarged dataset, their system got state-of-the-art accuracy on CTB corpus.\nBoth the discrete and neural models get comparable accuracies with state-ofthe-art system on English and Chinese datasets. The joint model has significant enhancement compared with separated model, especially in Chinese POS tagging task, with 1% accuracy increment. Fig. 3 shows the accuracy comparison for the discrete and neural models based on each sentence. There are many sentences that are not located at the diagonal line, which indicates the two models gives different results and have the potential for combination. Our joint model outperforms state-of-the-art accuracy with 0.23% and 0.97% on English and Chinese datasets, respectively."}, {"heading": "4.3 NER", "text": "For the NER task, we split Ontonotes 4.0 following Che et al. [54] to get both English and Chinese datasets. Table 10 shows the sentence numbers of train/develop/test datasets.\nWe follow Che et al. [54] on choosing both the English and the Chinese datasets. Their work induces bilingual constrains from parallel dataset which gives significant enhancement of F-scores on both English and Chinese datasets.\nOur discrete and neural models show comparable recall values compared with Che\u2019s results [54] on both datasets. Similar with the previous two tasks, the joint model gives significant enhancement compared with separated models\n(discrete/neural) on all metrics. This shows that discrete and neural model can identify entities using different indicator features, and they can be complementary with each other. The comparison of sentence F-measures in Fig. 4 confirms this observation.\nThe joint model outperforms the state-of-the-art on both datasets in the two different languages. The precision of joint models are less than state-of-the-art system. This may be caused by the bilingual constrains in baseline system, which ensures the precision of entity recognition."}, {"heading": "5 Conclusion", "text": "We proposed a joint sequence labeling model that combines neural features and discrete indicator features which can integrate the advantages of carefully designed feature templates over decades and automatically induced features from\nneural networks. Through experiments on various sequence labeling tasks, including Chinese word segmentation, POS tagging and named entity recognition for Chinese and English respectively, we demonstrate that our joint model can unanimously outperform models which only contain discrete features or neural features and state-of-the-art systems on all compared tasks.The accuracy/Fmeasure distribution comparison for discrete and neural model also indicate that discrete and neural model can reveal different related information, this explains why combined model can outperform separate models.\nIn the future, we will investigate the effect of our joint model on more NLP tasks, such as parsing and machine translation."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their detailed comments. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301."}], "references": [{"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Jiajun Chen"], "venue": "In ACL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein"], "venue": "ACL-IJCNLP, pages 302\u2013312,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "In ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u0131cero Nogueira dos Santos", "Ma\u0131ra Gatti"], "venue": "In COLING,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Target-dependent twitter sentiment classification with rich automatic features", "author": ["Duy-Tin Vo", "Yue Zhang"], "venue": "In IJCAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Neural networks for open domain targeted sentiment", "author": ["Meishan Zhang", "Yue Zhang", "Duy-Tin Vo"], "venue": "In EMNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Effect of non-linear deep architecture in sequence labeling", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Deep learning for event-driven stock prediction", "author": ["Xiao Ding", "Yue Zhang", "Ting Liu", "Junwen Duan"], "venue": "In ICJAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "What happens next? event prediction using a compositional neural network", "author": ["Granroth-Wilding Mark", "Clark Stephen"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Revisiting embedding features for simple semi-supervised learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Tagging the web: Building a robust web tagger with neural network", "author": ["Ji Ma", "Yue Zhang", "Jingbo Zhu"], "venue": "In ACL,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a product of experts with elitist lasso", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Combining discrete and continuous features for deterministic transition-based dependency parsing", "author": ["Meishan Zhang", "Yue Zhang"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Chinese word segmentation as character tagging", "author": ["Nianwen Xue"], "venue": "Computational Linguistics and Chinese Language Processing,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum"], "venue": "In Coling,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Character-level dependencies in chinese: usefulness and learning", "author": ["Hai Zhao"], "venue": "In EACL, pages 879\u2013887,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "A cascaded linear model for joint chinese word segmentation and part-of-speech tagging", "author": ["Wenbin Jiang", "Liang Huang", "Qun Liu", "Yajuan L\u00fc"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging", "author": ["Weiwei Sun"], "venue": "In HLT-ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Domain adaptation for crf-based chinese word segmentation using free annotations", "author": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Ting Liu", "Fan Wu"], "venue": "In EMNLP,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu"], "venue": "In EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Chang Baobao"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang"], "venue": "In EMNLP,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark"], "venue": "In ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Word-based and character-based word segmentation models: Comparison and combination", "author": ["Weiwei Sun"], "venue": "In Coling, pages 1211\u20131219,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Unsupervised domain adaptation for joint segmentation and pos-tagging", "author": ["Yang Liu", "Yue Zhang"], "venue": "In COLING (Posters),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In EMNLP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1996}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In EMNLP, pages", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D Santos", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Part-of-speech tagging with recurrent neural networks", "author": ["Juan Antonio Perez-Ortiz", "Mikel L Forcada"], "venue": "Universitat d\u2019Alacant,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2001}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["Andrew McCallum", "Wei Li"], "venue": "In HLTNAACL,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2003}, {"title": "Named entity recognition with a maximum entropy approach", "author": ["C Hai Leong", "N Hwee Tou"], "venue": "In HLT-NAACL,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "An effective two-stage model for exploiting non-local dependencies in named entity recognition", "author": ["Vijay Krishnan", "Christopher D Manning"], "venue": "In Coling and ACL,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2006}, {"title": "Named entity recognition with bilingual constraints", "author": ["Wanxiang Che", "Mengqiu Wang", "Christopher D Manning", "Ting Liu"], "venue": "In HLT-NAACL,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth"], "venue": "In Coling,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["C\u0131cero dos Santos", "Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In NEWS,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2003}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Yoram Singer John Duchi", "Elad Hazan"], "venue": "JMLR, 12:2121\u20132159,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2011}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark"], "venue": "Computational linguistics,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1929}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In EMNLP,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2015}, {"title": "Character-level chinese dependency parsing", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In NAACL,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2003}, {"title": "Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study", "author": ["Li Zhenghua", "Chao Jiayuan", "Zhang Min", "Chen Wenliang"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 1, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 2, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 3, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 5, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 34, "endOffset": 49}, {"referenceID": 6, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 7, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 8, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 9, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 10, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 11, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 71, "endOffset": 88}, {"referenceID": 12, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 13, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 14, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 15, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 16, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 109, "endOffset": 125}, {"referenceID": 17, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 18, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 19, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 20, "context": "range of tasks, including parsing [1,2,3,4,5,6,7], machine translation [8,9,10,11,12,13], sentiment analysis [14,15,16,17,18] and information extraction [19,20,21,22], achieving results competitive to the best discrete models.", "startOffset": 153, "endOffset": 166}, {"referenceID": 21, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 22, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 23, "context": "First, neural network models take low-dimensional dense embeddings [23,24,25] as inputs, which can be trained from large-scale test, thereby overcoming the issue of sparsity.", "startOffset": 67, "endOffset": 77}, {"referenceID": 24, "context": "[26] integrated word embedding as real-word features into a discrete Conditional Random Field [27] (CRF) model, finding enhanced results for a number of sequence labeling tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] integrated word embedding as real-word features into a discrete Conditional Random Field [27] (CRF) model, finding enhanced results for a number of sequence labeling tasks.", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "[28] show that the integration can be improved if the embedding features are carefully discretized.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] treated a discrete perception model as a neural layer, which is integrated into a neural model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Wang & Manning [30] integrated a discrete CRF model and a neural CRF model by combining their output layers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Greg & Dan [6] and Zhang et al.", "startOffset": 11, "endOffset": 14}, {"referenceID": 16, "context": "[18] also followed this method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Zhang & Zhang [31] compared various integration methods for parsing, and found that the second type of integration gives better results.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "We follow Zhang & Zhang [31], investigating the effect of feature combination for a range of sequence labeling tasks, including word segmentation, Part-OfSpeech (POS) tagging and named entity recognition (NER) for Chinese and English, respectively.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "For neural features, we adopt a neural CRF model, using a separated Long Short-Term Memory [32] (LSTM) layer to extract input features.", "startOffset": 91, "endOffset": 95}, {"referenceID": 31, "context": "Xue [33] treat it as a sequence labeling task, using B(egin)/I(nternal) /E(nding)/S(inglecharacter word) tags on each character in the input to indicate its segmentation status.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "[34], who use CRF to improve the accuracies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 33, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 34, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 35, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 36, "context": "Most subsequent work follows [34,35,36,37,38] and feature engineering has been out of the key research questions.", "startOffset": 29, "endOffset": 45}, {"referenceID": 37, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 38, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 39, "context": "Recently, neural networks have been applied to character-based segmentation [39,40,41], giving results comparable to discrete methods.", "startOffset": 76, "endOffset": 86}, {"referenceID": 40, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 41, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 42, "context": "On the other hand, the second kind of work studies wordbased segmentation, scoring outputs based on word features directly [42,43,44].", "startOffset": 123, "endOffset": 133}, {"referenceID": 43, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 44, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 45, "context": "POS-tagging has been investigated as a classic sequence labeling problem [45,46,47], for which a well-established set of features are used.", "startOffset": 73, "endOffset": 83}, {"referenceID": 46, "context": "In order to include word morphology and word shape knowledge, a convolutional neural network (CNN) for automatically learning character-level representations is investigated in [48].", "startOffset": 177, "endOffset": 181}, {"referenceID": 23, "context": "[25] built a CNN neural network for multiple sequence labeling tasks, which gives state-of-the-art POS results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Recurrent neural network models have also been used for this task [49,50].", "startOffset": 66, "endOffset": 73}, {"referenceID": 48, "context": "Recurrent neural network models have also been used for this task [49,50].", "startOffset": 66, "endOffset": 73}, {"referenceID": 48, "context": "[50] combines bidirectional LSTM with a CRF layer, their model is robust and has less dependence on word embedding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "McCallum & Li [51] use CRF model for NER task and exploit Web lexicon as feature enhancement.", "startOffset": 14, "endOffset": 18}, {"referenceID": 50, "context": "Chieu & Ng [52], Krishnan & Manning [53] and Che et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 51, "context": "Chieu & Ng [52], Krishnan & Manning [53] and Che et al.", "startOffset": 36, "endOffset": 40}, {"referenceID": 52, "context": "[54] tackle this task through non-local features [55].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[54] tackle this task through non-local features [55].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "[25] model we referred before, NER task has also been included.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[56] boost the neural model by adding character embedding on Collobert\u2019s structure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[57] take the lead by employing LSTM for NER tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[58] use CNN model to extract character embedding and attach it with word embedding and afterwards feed them into Bi-directional LSTM model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Online Adagrad [59] is used to train the model, with the initial learning rate set to be \u03b7.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "[54] by adding part-of-speech information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "English and Chinese datasets are labeled by ZPar [60], prefix and suffix on two datasets are both including 4 characters.", "startOffset": 49, "endOffset": 53}, {"referenceID": 52, "context": "Word clusters in both English and Chinese tasks are same with Che\u2019s work [54].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Character embeddings, character bigram embeddings and word embeddings are pretrained separately using word2vec[23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "English word embedding is chosen as SENNA [25].", "startOffset": 42, "endOffset": 46}, {"referenceID": 58, "context": "The Chinese corpus is segmented by ZPar [60].", "startOffset": 40, "endOffset": 44}, {"referenceID": 59, "context": "Dropout [61] technology has been used to suppress over-fitting in the input layer.", "startOffset": 8, "endOffset": 12}, {"referenceID": 60, "context": "[62], and the CTB60 set as Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[63].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "Our joint models shown comparable results to the state-of-the-art results reported by Zhang & Clark [42], where they adopt a word-based perceptron model with carefully designed discrete features.", "startOffset": 100, "endOffset": 104}, {"referenceID": 62, "context": "[64] and Chinese dataset by Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "[65] on CTB.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "Toutanova\u2019s model [64] exploits bidirectional dependency networks to capture", "startOffset": 18, "endOffset": 22}, {"referenceID": 63, "context": "[65] utilize heterogeneous datasets for Chinese POS tagging through bundling two sets of tags and training in enlarged dataset, their system got state-of-the-art accuracy on CTB corpus.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] to get both English and Chinese datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[54] on choosing both the English and the Chinese datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Our discrete and neural models show comparable recall values compared with Che\u2019s results [54] on both datasets.", "startOffset": 89, "endOffset": 93}], "year": 2017, "abstractText": "Neural network models have recently received heated research attention in the natural language processing community. Compared with traditional models with discrete features, neural models have two main advantages. First, they take low-dimensional, real-valued embedding vectors as inputs, which can be trained over large raw data, thereby addressing the issue of feature sparsity in discrete models. Second, deep neural networks can be used to automatically combine input features, and including non-local features that capture semantic patterns that cannot be expressed using discrete indicator features. As a result, neural network models have achieved competitive accuracies compared with the best discrete models for a range of NLP tasks. On the other hand, manual feature templates have been carefully investigated for most NLP tasks over decades and typically cover the most useful indicator pattern for solving the problems. Such information can be complementary the features automatically induced from neural networks, and therefore combining discrete and neural features can potentially lead to better accuracy compared with models that leverage discrete or neural features only. In this paper, we systematically investigate the effect of discrete and neural feature combination for a range of fundamental NLP tasks based on sequence labeling, including word segmentation, POS tagging and named entity recognition for Chinese and English, respectively. Our results on standard benchmarks show that state-of-the-art neural models can give accuracies comparable to the best discrete models in the literature for most tasks and combing discrete and neural features unanimously yield better results.", "creator": "LaTeX with hyperref package"}}}