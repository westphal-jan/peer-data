{"id": "1708.07303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2017", "title": "Learning Grasping Interaction with Geometry-aware 3D Representations", "abstract": "Learning to interact with objects in the environment is a fundamental AI problem involving perception, motion planning, and control. However, learning representations of such interactions is very challenging due to a high dimensional state space, difficulty in collecting large-scale data, and many variations of an object's visual appearance (i.e., whether it is a human or a human), and a wide range of cognitive and behavioral conditions (i.e., whether it is a human or a human). In the long run, some individuals will likely have a much more complex visual experience. It is possible to learn from this experience while remaining a strong-arm type. Therefore, learning techniques to recognize objects and behaviors are much more challenging. In most cases, this requires a degree of knowledge and practice and an education to be able to fully understand and understand objects and behaviors.\n\n\n\nTo start learning from a simple computer, we needed a computer to interpret and recognize objects using a series of standard software. One of the basic software techniques is a number of computer programs. One is the X-Ray library. This programming system can be found here:\nQE-Cpy\nA-DjLmRk4f\nA-SdMcTxKzR1M4i\nQE-NpCXeD2QoRcPfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2QpfD2", "histories": [["v1", "Thu, 24 Aug 2017 08:09:04 GMT  (4226kb,D)", "http://arxiv.org/abs/1708.07303v1", "Deep Geometry-aware Grasping"], ["v2", "Fri, 25 Aug 2017 02:50:28 GMT  (4226kb,D)", "http://arxiv.org/abs/1708.07303v2", "Deep Geometry-aware Grasping"]], "COMMENTS": "Deep Geometry-aware Grasping", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG", "authors": ["xinchen yan", "mohi khansari", "yunfei bai", "jasmine hsu", "arkanath pathak", "abhinav gupta", "james davidson", "honglak lee"], "accepted": false, "id": "1708.07303"}, "pdf": {"name": "1708.07303.pdf", "metadata": {"source": "CRF", "title": "Learning grasping interaction with geometry-aware 3D representations", "authors": ["Xinchen Yan", "Mohi Khansari", "Yunfei Bai", "Jasmine Hsu", "Arkanath Pathak", "Abhinav Gupta", "James Davidson", "Honglak Lee"], "emails": ["xcyan@umich.edu,", "khansari@x.team", "yunfeibai@x.team", "honglak}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Learning to interact with objects is a fundamental and challenging problem in artificial intelligence that involves perception, motion planning, and control. The problem is challenging because it not only requires understanding geometry (global shape of object, local surface around interaction) but it also requires estimating physical properties, such as weight of object, density and friction. Furthermore, it requires invariance to illumination, objects\u2019 location and viewpoint. To handle this, current datadriven approaches use thousands of examples and learn end-to-end models. But collecting such large-scale data is extremely difficult and time-consuming. Is it possible to constrain the learning process somehow so that we can learn representation for interaction with less data?\nWe argue that geometry is at the heart of this type of interaction and propose the concept of geometryaware learning agent with the following properties: (1) agent has a clear notion of the geometry: the location, orientation, and shape of object from visual input. Therefore, the agent is able to distinguish its ego-motion from the motions in the environment. (2) agent is able to relate the geometry of novel object with its previous experience and (3) agent is able to reason about the relationship between interaction and feedback (i.e., success or failure) by taking the geometry factors into consideration.\n\u2217The work was done while interning at Google Brain.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 8.\n07 30\n3v 1\n[ cs\n.R O\n] 2\n4 A\nug 2\nCompared to a learning agent that does not have explicit notion of geometry, we believe that the geometry-aware agent will learn a better understanding of interaction, feedback and geometry.\nIn this work, we propose a two-stage procedure for learning grasping interaction from demonstrations. First, the agent learns to build the geometry-aware representation from 2D visual input. Second, the agent learns to predict grasping interaction from demonstrations with the built-in geometry-aware representation. More specifically, we design an encoder-decoder deep neural network for learning this representation. Our geometry-aware encoder-decoder network has two components: a shape prediction network and a grasping outcome prediction network. The shape prediction network has an image encoder, a 3D shape decoder, and a learning-free OpenGL projection layer. The image encoder transforms the 2D visual data into the high-level geometry representation. The shape decoder network takes in the geometry representation and outputs the 3D volume of the object. To enable supervision with only 2D visual data, we propose a novel learning-free OpenGL projection layer similar to Yan et al. [2016], Rezende et al. [2016]. The grasping outcome prediction network has a state encoder and an outcome predictor. The state encoder network transforms the current visual state (e.g., object and gripper) to a high-level state representation. The outcome predictor network takes in an action, a state, and the geometry representations to produce an outcome (e.g., success or failure) of the grasping interaction.\nWe have built a large database consisting of 101 everyday objects with more than 150K grasping demonstrations in Virtual Reality (VR) with both human and artificial interactions. For each object, we collect 10-20 grasping attempts with a 1-DoF virtual gripper from right-handed users. For each attempt, we record a pre-grasp status which includes the location and orientation of the object and gripper, as well as the grasping outcome (e.g., success or failure). Additionally, we augment the data by perturbing the gripper location and orientation based on grasping demonstrations in the Bullet Physics simulator.\nOur main contributions are summarized below:\n\u2022 We build a database with rich visual sensory data and grasping annotations. \u2022 We demonstrate that the proposed geometry-aware encoder-decoder network is able to learn\nthe shape as well as grasping outcome better than models without notion of geometry. \u2022 We demonstrate that the proposed model has advantages in guiding grasping exploration\nand achieves better generalization to novel viewpoints and novel object instances."}, {"heading": "2 Related Work", "text": "A common approach for robotic grasping is to detect the optimal grasping location from 2D visual input (RGB or RGBD images) Saxena et al. [2008], Montesano and Lopes [2012], Lenz et al. [2015],\nPinto and Gupta [2016]. Earlier work Saxena et al. [2008], Montesano and Lopes [2012] studied the planar grasping problem using visual features extracted from 2D sensory input and adopted logistic regression for fitting optimal grasping location with visual features. Lenz et al. [2015] proposed a two-step detection pipeline (object detection and grasping part detection) with deep neural networks. Pinto and Gupta [2016] built a robotic system for learning grasping from large-scale real-world trial-and-error experiments. In this work, a deep convolutional neural network was trained on 700 hours robotic grasping data collected from the system.\nFine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training. Li et al. [2016] investigated the hand pose estimation in robotic grasping by decoupling contact points and hand configuration with parametrized object shape. Building upon the compositional aspect of everyday objects, Vahrenkamp et al. [2016] proposed a part-based model for robotic grasping that has better generalization to novel object. Very recently, effort was also made in building DexNet Mahler et al. [2016, 2017], a large-scale point cloud database for planar grasping. In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al. [2014], Nikandrova and Kyrki [2015].\nIn contrast to existing learning frameworks applied to robotic grasping, our approach features (1) an end-to-end deep learning framework for generative 3D shape modeling and predictive grasping interaction and (2) learning-free projection layer that links the 2D observations with 3D object shape."}, {"heading": "3 Multi-objective framework with geometry-aware representation", "text": "In this section, we develop a two-stage learning framework that performs 3D shape prediction and grasping outcome prediction with geometry-aware representation. Being able to generate 3D object shapes (e.g., volumetric representation) from any scene given 2D sensory input is a very important feature of our geometry-aware agent. More specifically, in our formulation, the geometry-aware representation is (1) an occupancy grid representation of the scene centered at camera target in the world frame and (2) invariant to camera viewpoint and distance."}, {"heading": "3.1 Learning generative 3D geometry-aware representation from 2D sensory input", "text": "For simplicity, we begin with a single-view formulation. If the ground-truth 3D volumetric representation V is given, we can fit a functional mapping fV : I \u2192 V that approximates the 3D object shape from 2D sensory input I . However, the ground-truth 3D object shape (e.g., explicit supervision of 3D volume or occupancy grid) is not always directly available. Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision. In Yan et al. [2016], an in-network projection layer is introduced for 3D shape learning from 2D masks (e.g. 2D silhouette of object). However, a 2D silhouette is an insufficient supervision signal (e.g., consider the concave shape) in robotic grasping. Therefore, we also consider a 2D depth map D as the supervision signal for learning the object geometry.2 To find the correspondence between a 3D shape and 2D depth map, we introduce a learning-free projective operator similar to Yan et al. [2016] that implements the exact rendering procedure for 2D depth estimation from the 3D world.\nWe formulate the projective operation by fD : V \u00d7 P \u2192 D that transforms a 3D shape into a 2D depth map with the camera transformation matrix P. Here, the camera transformation matrix decomposes as P = K[R; t], where K is the camera intrinsic matrix, R is the camera rotation matrix, and t is the camera translation vector. In our implementation, we also use a 2D silhouette as an object maskM for learning. Empirically, we found that this additional objective makes the learning easier during training. Finally, given a 2D observation I from a single-view, the loss function is defined as follows:\nLsingle\u03b8 = \u03bbDL depth \u03b8 (D\u0302;D) + \u03bbML mask \u03b8 (M\u0302;M) (1)\n2Our design choice of using RGBD as an input signal is also motivated from the common availability of RGBD sensors in most robot platforms.\nHere, \u03bbD and \u03bbM are the constant coefficients for the depth and mask prediction terms, respectively.\nLearning-free projective operator for depth estimation. Following the OpenGL camera transformation standard, for each point psj = (x s j , y s j , z s j , 1) in 3D world frame, we compute the corresponding point pnj = (x n j , y n j , z n j , 1) in the normalized device coordinate system (\u22121 \u2264 xnj , ynj , znj \u2264 1) using the transformation: pnj \u223c Ppsj . Here, the conversion from depth buffer znj to real depth zej is given by zej = f e(znj ) = \u22121/(\u03b1 \u2217 znj + \u03b2) where \u03b1 = Znear\u2212Zfar 2ZnearZfar\nand \u03b2 = Znear+Zfar2ZnearZfar . Here, Zfar and Znear represents the far and near clipping planes of the camera.\nSimilar to the transformer network proposed in Yan et al. [2016], Jaderberg et al. [2015], our learning-free projection can be considered as: (1) performing dense sampling from input volume (in the 3D world frame) to output volume (in normalized device coordinates); and (2) flattening the 3D spatial output across one dimension. Again, each 3D point (xsj , y s j , z s j ) in input volume V \u2208 RH\u00d7W\u00d7D and corresponding point (xnj , ynj , znj ) in output volume U \u2208 RH \u2032\u00d7W \u2032\u00d7D\u2032 are related by the transformation matrix P. Here, (W,H,D) and (W \u2032, H \u2032, D\u2032) are the width, height, and depth of the input and output volume, respectively. We summarize the dense sampling step and channel-wise flattening step in Eq. 2.\nUj = H\u2211 n W\u2211 m D\u2211 l Vnmlmax(0, 1\u2212 |xsj \u2212m|)max(0, 1\u2212 |ysj \u2212 n|)max(0, 1\u2212 |zsj \u2212 l|)\nMn\u2032m\u2032 = max l\u2032 Un\u2032m\u2032l\u2032\nDn\u2032m\u2032 = { Zfar, ifMn\u2032m\u2032 = 0 fe( 2l \u2032\nD\u2032 \u2212 1) where l \u2032 = argminl\u2032(Un\u2032m\u2032l\u2032 > 0.5), otherwise\n(2)\nIntuitively, the learning-free projective operator is performing ray-tracing along the projection axis.\nLearning from multi-view observations. Learning to predict 3D shape from single-view 2D sensory input is a challenging task in computer vision due to shape ambiguity. To reduce ambiguity in shape prediction, we assume multiple observations of the scene are available during model training. From the interaction perspective, multi-view observations also provide useful additional input to the system. Given a series of n observations I1, I2, \u00b7 \u00b7 \u00b7 , In of the scene, the 3D reconstruction can be formulated as fV : {Ii}ni=1 \u2192 V. Similarly, the projective operation from i-th viewpoint is fD : V \u00d7 Pi \u2192 Di, where Di and Pi are the depth and camera transformation matrix from corresponding viewpoint, respectively. We define the multi-view loss Lmulti in Eq. 3 with an emphasis on the shape prediction consistency across viewpoints.\nLmulti\u03b8 = \u03bbD n\u2211 i=1 Ldepth\u03b8 (D\u0302i,Di) + \u03bbM n\u2211 i=1 Lmask\u03b8 (M\u0302i,Mi) (3)"}, {"heading": "3.2 Learning predictive grasping interaction with geometry-aware representation.", "text": "In general, motion planning and control for grasping is very challenging due to many factors involved. In this work, we focus on modeling the pre-grasp status as fine-grained motion planning becomes increasingly important when the gripper reaches close to target object. In our formulation, we assume grasping outcome is binary: either success or failure. The interaction is classified as success only if the action results in a valid grasp. Based on our formulation, the grasping success probability can be directly inferred from the visual observation I of current state and proposed action a = [p,o]. Inspired by previous work Oh et al. [2015], Finn et al. [2016], Dosovitskiy and Koltun [2016], Yang et al. [2015], Pinto et al. [2016], where outcomes are high-order mappings from observations and actions, a straight-forward approach is to fit a functional mapping f lvanilla : I \u00d7 a\u2192 l. We refer to this model as a vanilla grasping interaction prediction model (see Figure 2(a)).\nBuilding upon the vanilla prediction model, we propose a novel geometry-aware prediction model. That is, the agent learns to predict the grasping interaction by taking the geometry-aware representation as an additional input. The benefits of such geometry-aware representation are two-folds:\n\u2022 The geometry-aware representation provides a global shape prior for interaction predictions. \u2022 The geometry-aware representation provides 3D information about the local surface centered\naround the interaction event.\nLocal shape inference via projection. In our implementation, we reuse the learning-free projection operator to obtain the local depth given the gripper position and orientation.\nFinally, given a current observation I, proposed action a, and inferred 3D shape representation V, we fit a functional mapping f lgeometry\u2212aware : I \u00d7a\u00d7V\u2192 l, where l is the binary label of whether it is a valid grasp."}, {"heading": "3.3 Deep geometry-aware encoder-decoder network", "text": "To implement the two components proposed in the previous sections, we introduce a deep geometryaware encoder-decoder network (see Figure 2). Our model is composed of a shape prediction network and a grasping outcome prediction network. The shape prediction network has a 2D convolutional shape encoder and a 3D deconvolutional shape decoder followed by a global projection layer. Our shape encoder network takes RGBD images of resolution 128\u00d7 128 and corresponding 4-by-4 camera view matrices as input; the network and outputs identity units as an intermediate representation. Our shape decoder is a 3D deconvolutional neural network that outputs voxels at a resolution of 32 \u00d7 32 \u00d7 32. We implemented the projection layer (with camera view and projection matrix) that transforms the voxels back into foreground object silhouettes and depth maps at an input resolution (128 \u00d7 128). Here, the purpose of generative pre-training is to learn viewpoint invariant units (e.g., object identity units) through object segmentation and depth prediction. The outcome prediction network has a 2D convolutional state encoder and a fully connected outcome predictor with an additional local shape projection layer. Our state encoder takes RGBD images (pre-grasp scene) of resolution 128 \u00d7 128 and corresponding actions (position and orientation of the gripper end-effector) and outputs state unit as intermediate representation. Our outcome predictor takes both current state (e.g., pre-grasp scene and action) and shape features (e.g., global shape from identity units and the local surface from the local shape projection layer) into consideration. Note that the local dense sampling transforms the surface around the gripper fingers into a foreground silhouette and a depth map at resolution 48 \u00d7 48. For the purpose of better shape representation during training, we feed observations taken from multiple viewpoints to the neural networks. During evaluation, we only provide single-view observation for the model as input."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset collection", "text": "VR-Grasping-101. We collected grasping demonstrations on seven categories of objects, which include a total of 101 everyday objects. To collect grasping demonstrations, we setup the HTC Vive system in Virtual Reality (VR) and assign target objects randomly to five right-handed users (three males and two females). In total, 1597 human grasps are demonstrated, with an average of 15 grasps per object (with lowest and highest number of grasps at 7 and 39 for a plate and a wine glass, respectively). We randomly split 101 objects into three sets (e.g., training, validation and testing) and make sure each set covers the seven categories (70% for training, 10% for validation and 20% for\ntesting). For detailed description of our dataset and collection protocol, please refer to the our project website: http://goo.gl/gNCywJ.\nGrasping data perturbation. In order to collect sufficient grasping demonstrations for model training and evaluation, we generate more grasps by perturbing the human demonstrations using Bullet Physics engine 3 In total, we collected 150K grasping demonstrations covering 101 objects. For each demonstration, we take a snapshot of the pre-grasp scene (e.g., before closing the two gripper fingers). To minimize the bias introduced from the data generation pipeline, we randomly posit the camera at a distance ranging between 35 centimetres and 45 centimetres. We draw a camera target position from a normal distribution with its mean as the object center and a desired variance (in our experiment, we use 3 centimetres as standard deviation). Furthermore, we rotate the camera position from 8 different azimuth angles (with steps of 45 degrees) and adjust the elevation from 4 different angles (e.g., 15, 30, 45, and 60 degrees). Here, we include only two elevation angles (e.g., 15 and 45 degrees) in the training set while leaving the other two angles for evaluation. Finally, we also save a state of the scene without a gripper, which is used for shape pre-training; this will be referred to as the static scene for the rest of the paper."}, {"heading": "4.2 Implementation details", "text": "Baseline model. We adopt the vanilla prediction model as our grasping baseline. We trained the model using the ADAM optimizer with a learning rate of 10\u22125 for 200K iterations and a mini-batch of size of 4. As an ablation study, we added view and static scene as an additional input channel on top of the baseline model but didn\u2019t observe significant improvements.\nGeometry-aware model. As mentioned previously, we adopted the two-stage training procedure. First, we pre-trained the shape prediction model (shape encoder and shape decoder) using the ADAM optimizer with a learning rate of 10\u22125 for 400K iterations and a mini-batch of size of 4. In each batch, we sample 4 random viewpoints as our multi-view training. We observed that this setting led to a more stable shape prediction performance compared to single-view training. In addition, we used L1 loss for foreground depth prediction and L2 loss for silhouette prediction with coefficients \u03bbD = 0.5 and \u03bbM = 10.0. In the second stage, we fine-tuned the state encoder and outcome predictor using ADAM optimizer with a learning rate of 3 \u2217 10\u22126 for 200K iterations and a mini-batch of size of\n3https://github.com//bulletphysics\n4. We used cross-entropy as our objective function since the grasping prediction is formulated as a binary classification task.\nIn our experiments, all the models are trained using 20 GPU workers and 32 parameter servers with asynchronized updates. Both baseline and our geometry-aware model adopt convolutional encoder-decoder architecture with residual connections. The bottleneck layer (e.g., the identity unit in the geometry-aware model) is a 768 dimensional vector."}, {"heading": "4.3 Visualization: 3D shape prediction", "text": "To evaluate the quality of generative shape prediction model, we performed inference using the shape encoder and decoder network. In our evaluations, we used single-view RGBD image and corresponding camera view matrix as input to the network. As shown in Figure 3(a), our shape prediction model is able to generate fine-grained 3D voxels from single-view input without explicitly providing 3D voxels as supervision during training. As shown in Figure 3(b), our model demonstrates reasonable generalization ability even when applying to novel object instances."}, {"heading": "4.4 Model evaluation: grasping outcome prediction", "text": "With a learned geometry-aware representation, our model achieves better classification performance in predicting the grasping outcome. We compared the classification accuracy of the baseline model and our geometry-aware model by conducting extensive evaluations on novel objects (from the testing set) from multiple observation viewpoints. For each human demonstration, we prepared 100 random grasps through perturbation (among which 50% of them are success grasps) and computed the average accuracy on 100 grasps (i.e., random guess achieves 50% accuracy). To investigate the model performance due to viewpoint changes, we repeat the evaluation experiment for four different elevation angles (e.g, 15, 30, 45, and 60 degrees). The results are summarized in Table 1 and Table 2. Overall, the geometry-aware model performs consistently better than baseline model in outcome classification. As we can see, \u201cteapot\u201d and \u201cplate\u201d are comparatively more challenging categories for outcome prediction, since \u201cteapot\u201d has irregular shape parts (e.g., tip and handle) and \u201cplate\u201d has a fairly flat shape. When it comes to novel elevation angles (e.g., compare Table 1 and Table 2), our geometry-aware model is less affected, especially for categories like \u201cteapot\u201d and \u201cplate\u201d where viewpoint invariant shape understanding is crucial.\nAnalysis: local shape inference via projection. One advantage of our generative shape prediction component is that we can obtain additional local shape information via projection (see the red-dashed box in Figure 2(c)). At testing time, our shape prediction component first generates the 3D voxels given 2D observation (at a distance). With the 3D voxels as part of the intermediate representation, we can further acquire the local shape by running a projection from the gripper\u2019s perspective (i.e., simply treat the gripper as another virtual camera). To further understand the advantages of our generative shape prediction component, we visualized the intermediate local shape representation projected from predicted 3D voxels. As shown in Figure 3, our generative shape prediction component provides reasonably accurate local shape estimation that is useful for grasping outcome prediction.\nApplication: analysis-by-synthesis grasping optimization. With improved prediction over the grasping outcome, a natural question is whether this improvement can be used to guide better grasping\nplanning. Given a seed grasping proposal, we conducted grasping optimization by sequentially proposing grasping locations until a grasp success. For grasping optimization, we performed a simplified version of cross-entropy method (CEM) Rubinstein and Kroese [2004], Levine et al. [2016]. We initialized with a failure grasp in order to force the model to find better grasping location (e.g., position and orientation). At each iteration, we sample 10 random directions and selected the top one based on the score returned by the neural network (output of outcome predictor). We repeat the iterations until success with an upper bound of 20 steps. We ran the same evaluation for both the baseline model and our geometry-aware model. To account for the variations in observation viewpoints and initial seeds, we repeat the evaluation for eight times per testing demonstration in our dataset and reported the average success rate after 20 iterations (marked as failure only if there is no success in 20 steps). As shown in Table 3, CEM guided our geometry-aware model performance consistently better than baseline model. We believe that the improvement results from the explicit use of modeling the object shape in our geometry-aware model. Our model achieved the most significant improvement in the \u201cbottle\u201d category, since a bottle shape is relatively easy to reconstruct. Our improvement in the \u201cbowl\u201d category is less significant, partly due to the failure in predicting its concave shape for testing object instances. Figure 5 shows example grasping optimization trials with different types of objects. The baseline model was stuck at the local region while our geometry-aware model was able to transit from one side of the object to the other optimal position and orientation."}, {"heading": "5 Conclusions", "text": "In this work, we studied the grasping interaction from a geometry-aware learning agent\u2019s perspective. We proposed an encoder-decoder network that performs shape prediction as well as grasping outcome prediction with a learning-free OpenGL projection layer. Compared to the baseline, experimental results demonstrated improved performance in outcome prediction thanks to generative shape training. Guided by the improved outcome predictor, we achieved better planning via analysis-by-synthesis grasping optimization. We have demonstrated the benefits of having geometry-aware representation in perception and motion planning. In the future, we will explore possibilities that performs robotic control with our geometry-aware representation."}, {"heading": "Acknowledgments", "text": "We thank Kurt Konolige, Erwin coumans, Vincent Vanhoucke, Ethan Holly, Marek Fiser, Eric Jang, Jie Tan, Lajanugan Logeswaran, Ruben Villegas , the Google Brain Team and X for the help with the project."}], "references": [{"title": "Semantic grasping: planning task-specific stable robotic grasps", "author": ["H. Dang", "P.K. Allen"], "venue": "Autonomous Robots,", "citeRegEx": "Dang and Allen.,? \\Q2014\\E", "shortCiteRegEx": "Dang and Allen.", "year": 2014}, {"title": "Learning to act by predicting the future", "author": ["A. Dosovitskiy", "V. Koltun"], "venue": "arxiv preprint:", "citeRegEx": "Dosovitskiy and Koltun.,? \\Q2016\\E", "shortCiteRegEx": "Dosovitskiy and Koltun.", "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "The columbia grasp database", "author": ["C. Goldfeder", "M. Ciocarlie", "H. Dang", "P.K. Allen"], "venue": "In Robotics and Automation,", "citeRegEx": "Goldfeder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldfeder et al\\.", "year": 2009}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Deep learning a grasp function for grasping under gripper pose uncertainty", "author": ["E. Johns", "S. Leutenegger", "A.J. Davison"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "Johns et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2016}, {"title": "Perceiving, learning, and exploiting object affordances for autonomous pile manipulation", "author": ["D. Katz", "A. Venkatraman", "M. Kazemi", "J.A. Bagnell", "A. Stentz"], "venue": "Autonomous Robots,", "citeRegEx": "Katz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Katz et al\\.", "year": 2014}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Lenz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lenz et al\\.", "year": 2015}, {"title": "Opengrasp: A toolkit for robot grasping simulation", "author": ["B. Leon", "S. Ulbrich", "R. Diankov", "G. Puche", "M. Przybylski", "A. Morales", "T. Asfour", "S. Moisio", "J. Bohg", "J. Kuffner"], "venue": "SIMPAR,", "citeRegEx": "Leon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Leon et al\\.", "year": 2010}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "J. Ibarz", "D. Quillen"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Dexterous grasping under shape uncertainty", "author": ["M. Li", "K. Hang", "D. Kragic", "A. Billard"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Dex-net 1.0: A cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards", "author": ["J. Mahler", "F.T. Pokorny", "B. Hou", "M. Roderick", "M. Laskey", "M. Aubry", "K. Kohlhoff", "T. Kr\u00f6ger", "J. Kuffner", "K. Goldberg"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Mahler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mahler et al\\.", "year": 2016}, {"title": "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp", "author": ["J. Mahler", "J. Liang", "S. Niyaz", "M. Laskey", "R. Doan", "X. Liu", "J.A. Ojea", "K. Goldberg"], "venue": "metrics. arxiv preprint:", "citeRegEx": "Mahler et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mahler et al\\.", "year": 2017}, {"title": "Active learning of visual descriptors for grasping using non-parametric smoothed beta distributions", "author": ["L. Montesano", "M. Lopes"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Montesano and Lopes.,? \\Q2012\\E", "shortCiteRegEx": "Montesano and Lopes.", "year": 2012}, {"title": "Category-based task specific grasping", "author": ["E. Nikandrova", "V. Kyrki"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Nikandrova and Kyrki.,? \\Q2015\\E", "shortCiteRegEx": "Nikandrova and Kyrki.", "year": 2015}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Pinto and Gupta.,? \\Q2016\\E", "shortCiteRegEx": "Pinto and Gupta.", "year": 2016}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y.-L. Park", "A. Gupta"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Pinto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2016}, {"title": "Unsupervised learning of 3d structure from images", "author": ["D.J. Rezende", "S.A. Eslami", "S. Mohamed", "P. Battaglia", "M. Jaderberg", "N. Heess"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Rezende et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2016}, {"title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "author": ["R. Rubinstein", "D. Kroese"], "venue": null, "citeRegEx": "Rubinstein and Kroese.,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein and Kroese.", "year": 2004}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Saxena et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2008}, {"title": "Part-based grasp planning for familiar objects", "author": ["N. Vahrenkamp", "L. Westkamp", "N. Yamanobe", "E.E. Aksoy", "T. Asfour"], "venue": "In Humanoid Robots (Humanoids),", "citeRegEx": "Vahrenkamp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vahrenkamp et al\\.", "year": 2016}, {"title": "Shape completion enabled robotic grasping", "author": ["J. Varley", "C. DeChant", "A. Richardson", "A. Nair", "J. Ruales", "P. Allen"], "venue": "arxiv preprint:", "citeRegEx": "Varley et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Varley et al\\.", "year": 2016}, {"title": "Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision", "author": ["X. Yan", "J. Yang", "E. Yumer", "Y. Guo", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Weakly-supervised disentangling with recurrent transformations for 3d view synthesis", "author": ["J. Yang", "S.E. Reed", "M.-H. Yang", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "To enable supervision with only 2D visual data, we propose a novel learning-free OpenGL projection layer similar to Yan et al. [2016], Rezende et al.", "startOffset": 116, "endOffset": 134}, {"referenceID": 18, "context": "[2016], Rezende et al. [2016]. The grasping outcome prediction network has a state encoder and an outcome predictor.", "startOffset": 8, "endOffset": 30}, {"referenceID": 18, "context": "A common approach for robotic grasping is to detect the optimal grasping location from 2D visual input (RGB or RGBD images) Saxena et al. [2008], Montesano and Lopes [2012], Lenz et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 12, "context": "[2008], Montesano and Lopes [2012], Lenz et al.", "startOffset": 8, "endOffset": 35}, {"referenceID": 7, "context": "[2008], Montesano and Lopes [2012], Lenz et al. [2015],", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": "[2008], Montesano and Lopes [2012] studied the planar grasping problem using visual features extracted from 2D sensory input and adopted logistic regression for fitting optimal grasping location with visual features.", "startOffset": 8, "endOffset": 35}, {"referenceID": 3, "context": "Lenz et al. [2015] proposed a two-step detection pipeline (object detection and grasping part detection) with deep neural networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Lenz et al. [2015] proposed a two-step detection pipeline (object detection and grasping part detection) with deep neural networks. Pinto and Gupta [2016] built a robotic system for learning grasping from large-scale real-world trial-and-error experiments.", "startOffset": 0, "endOffset": 155}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al.", "startOffset": 148, "endOffset": 172}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al.", "startOffset": 148, "endOffset": 192}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al.", "startOffset": 148, "endOffset": 213}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al.", "startOffset": 148, "endOffset": 235}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al.", "startOffset": 148, "endOffset": 253}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al.", "startOffset": 148, "endOffset": 279}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al.", "startOffset": 148, "endOffset": 425}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al.", "startOffset": 148, "endOffset": 445}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training.", "startOffset": 148, "endOffset": 467}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training. Li et al. [2016] investigated the hand pose estimation in robotic grasping by decoupling contact points and hand configuration with parametrized object shape.", "startOffset": 148, "endOffset": 674}, {"referenceID": 2, "context": "Fine-grained grasping planning and control often involves 3D modeling of object shape, modeling dynamics of robot hands, and local surface modeling Goldfeder et al. [2009], Leon et al. [2010], Johns et al. [2016], Varley et al. [2016], Li et al. [2016], Vahrenkamp et al. [2016], Mahler et al. [2016, 2017]. Some work focused on analytic modeling of robotic grasps with known object shape information Goldfeder et al. [2009], Leon et al. [2010]. Varley et al. [2016] proposed a shape completion model that reconstructs the 3D occupancy grid for robotic grasping from partial observations, where ground-truth 3D occupancy grid is used during model training. Li et al. [2016] investigated the hand pose estimation in robotic grasping by decoupling contact points and hand configuration with parametrized object shape. Building upon the compositional aspect of everyday objects, Vahrenkamp et al. [2016] proposed a part-based model for robotic grasping that has better generalization to novel object.", "startOffset": 148, "endOffset": 901}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al. [2014], Nikandrova and Kyrki [2015].", "startOffset": 113, "endOffset": 155}, {"referenceID": 0, "context": "In addition to general robotic grasping, several recent work investigated the semantic or task-specific grasping Dang and Allen [2014], Katz et al. [2014], Nikandrova and Kyrki [2015]. In contrast to existing learning frameworks applied to robotic grasping, our approach features (1) an end-to-end deep learning framework for generative 3D shape modeling and predictive grasping interaction and (2) learning-free projection layer that links the 2D observations with 3D object shape.", "startOffset": 113, "endOffset": 184}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al.", "startOffset": 12, "endOffset": 34}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision.", "startOffset": 12, "endOffset": 53}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision. In Yan et al. [2016], an in-network projection layer is introduced for 3D shape learning from 2D masks (e.", "startOffset": 12, "endOffset": 176}, {"referenceID": 18, "context": "Inspired by Rezende et al. [2016], Yan et al. [2016], we tackle the 3D shape learning in a weakly supervised manner without explicit 3D shape supervision. In Yan et al. [2016], an in-network projection layer is introduced for 3D shape learning from 2D masks (e.g. 2D silhouette of object). However, a 2D silhouette is an insufficient supervision signal (e.g., consider the concave shape) in robotic grasping. Therefore, we also consider a 2D depth map D as the supervision signal for learning the object geometry.2 To find the correspondence between a 3D shape and 2D depth map, we introduce a learning-free projective operator similar to Yan et al. [2016] that implements the exact rendering procedure for 2D depth estimation from the 3D world.", "startOffset": 12, "endOffset": 657}, {"referenceID": 22, "context": "Similar to the transformer network proposed in Yan et al. [2016], Jaderberg et al.", "startOffset": 47, "endOffset": 65}, {"referenceID": 4, "context": "[2016], Jaderberg et al. [2015], our learning-free projection can be considered as: (1) performing dense sampling from input volume (in the 3D world frame) to output volume (in normalized device coordinates); and (2) flattening the 3D spatial output across one dimension.", "startOffset": 8, "endOffset": 32}, {"referenceID": 13, "context": "Inspired by previous work Oh et al. [2015], Finn et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 1, "context": "[2015], Finn et al. [2016], Dosovitskiy and Koltun [2016], Yang et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al.", "startOffset": 8, "endOffset": 38}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al. [2015], Pinto et al.", "startOffset": 8, "endOffset": 58}, {"referenceID": 1, "context": "[2016], Dosovitskiy and Koltun [2016], Yang et al. [2015], Pinto et al. [2016], where outcomes are high-order mappings from observations and actions, a straight-forward approach is to fit a functional mapping f l vanilla : I \u00d7 a\u2192 l.", "startOffset": 8, "endOffset": 79}, {"referenceID": 18, "context": "For grasping optimization, we performed a simplified version of cross-entropy method (CEM) Rubinstein and Kroese [2004], Levine et al.", "startOffset": 91, "endOffset": 120}, {"referenceID": 9, "context": "For grasping optimization, we performed a simplified version of cross-entropy method (CEM) Rubinstein and Kroese [2004], Levine et al. [2016]. We initialized with a failure grasp in order to force the model to find better grasping location (e.", "startOffset": 121, "endOffset": 142}], "year": 2017, "abstractText": "Learning to interact with objects in the environment is a fundamental AI problem involving perception, motion planning, and control. However, learning representations of such interactions is very challenging due to a high dimensional state space, difficulty in collecting large-scale data, and many variations of an object\u2019s visual appearance (i.e. geometry, material, texture, and illumination). We argue that knowledge of 3D geometry is at the heart of grasping interactions and propose the notion of a geometry-aware learning agent. Our key idea is constraining and regularizing interaction learning through 3D geometry prediction. Specifically, we formulate the learning process of a geometry-aware agent as a two-step procedure: First, the agent learns to construct its geometry-aware representation of the scene from 2D sensory input via generative 3D shape modeling. Finally, it learns to predict grasping outcome with its built-in geometry-aware representation. The geometry-aware representation plays a key role in relating geometry and interaction via a novel learning-free depth projection layer. Our contributions are threefold: (1) we build a grasping dataset from demonstrations in virtual reality (VR) with rich sensory and interaction annotations; (2) we demonstrate that the learned geometry-aware representation results in a more robust grasping outcome prediction compared to a baseline model; and (3) we demonstrate the benefits of the learned geometry-aware representation in grasping planning.", "creator": "LaTeX with hyperref package"}}}