{"id": "1708.08177", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Hyperprior on symmetric Dirichlet distribution", "abstract": "In this article we introduce how to put vague hyperprior on Dirichlet distribution, and we update the parameter of it by adaptive rejection sampling (ARS). Finally we analyze this hyperprior in an over-fitted mixture model by some synthetic experiments. In this study, we show that there is no statistical difference between the model (1), and the hyperprior in the model (2). It is based on the assumption that the parameter is a variable in the hyperprior model, which is not a constant, but an approximation. The variable is a constant variable in the hyperprior model, which is not a constant. This is based on a non-parametric model, which is not a constant, but an approximation. Thus, the model is only a constant in the hyperprior model, which is not a constant. This is based on the assumption that the parameter is a variable in the hyperprior model, which is not a constant. Therefore, the hyperprior model is only a constant.\n\n\n\n\nA problem with the hyperprior model is that it is not always the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5). The problem is that it is often the most common model (5", "histories": [["v1", "Mon, 28 Aug 2017 03:52:40 GMT  (2251kb,D)", "http://arxiv.org/abs/1708.08177v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun lu"], "accepted": false, "id": "1708.08177"}, "pdf": {"name": "1708.08177.pdf", "metadata": {"source": "CRF", "title": "Hyperprior on symmetric Dirichlet distribution", "authors": ["Jun Lu"], "emails": ["jun.lu.locky@gmail.com"], "sections": [{"heading": null, "text": "parameter of it by adaptive rejection sampling (ARS). Finally we analyze this hyperprior in an over-fitted mixture model by some synthetic experiments."}, {"heading": "1 Introduction", "text": "It has become popular to use over-fitted mixture models in which number of cluster K is chosen as a conservative upper bound on the number of components under the expectation that only relatively few of the components K\u2032 will be occupied by data points in the samples X . This kind of over-fitted mixture models has been successfully due to the ease in computation.\nPreviously Rousseau & Mengersen (2011) proved that quite generally, the posterior behaviour of overfitted mixtures depends on the chosen prior on the weights, and on the number of free parameters in the emission distributions (here D, i.e. the dimension of data). Specifically, they have proved that (a) If \u03b1=min(\u03b1k, k \u2264 K)>D/2 and if the number of components is larger than it should be, asymptotically two or more components in an overfitted mixture model will tend to merge with non-negligible weights. (b) In contrast, if \u03b1=max(\u03b1k, k 6 K) < D/2, the extra components are emptied at a rate of N\u22121/2. Hence, if none of the components are small, it implies that K is probably not larger than K0. In the intermediate case, if min(\u03b1k, k \u2264 K) \u2264 D/2 \u2264 max(\u03b1k, k 6 K), then the situation varies depending on the \u03b1k\u2019s and on the difference between K and K0. In particular, in the case where all \u03b1k\u2019s are equal to D/2, then although the author does not prove definite result, they conjecture that the posterior distribution does not have a stable limit."}, {"heading": "2 Hyperprior on symmetric Dirichlet distribution", "text": "Inspired by Rasmussen (1999) and further discussed by G\u00f6r\u00fcr & Edward Rasmussen (2010), they introduced a hyperprior on symmetric Dirichlet distribution prior. We here put a vague prior of Gamma shape on the concentration parameter \u03b1 and use the standard \u03b1 setting where \u03b1k = \u03b1 = \u03b1+/K for k = 1, . . . ,K.\n\u03b1|a, b \u223c Gamma(a, b) =\u21d2 p(\u03b1|a, b) \u221d \u03b1a\u22121e\u2212b\u03b1. To get the conditioned posterior distributions on \u03b1 we need to derive the conditioned posterior distributions on all the other parameters,. But for a graphical model, this conditional distribution is a function only of the nodes in the Markov blanket. In our case, the Bayesian finite Gaussian mixture model, a Directed acyclic graphical (DAG) model, the Markov blanket includes the parents, the children, and the co-parents, as shown in Figure 1. From this graphical representation, we can find the Markov blanket for each parameter in the model, and then figure out their conditional posterior distribution to be derived:\np(\u03b1|\u03c0, a, b) \u221d p(\u03b1|a, b)p(\u03c0|\u03b1)\n\u221d \u03b1a\u22121e\u2212b\u03b1 \u0393(K\u03b1)\u220fK k=1 \u0393(\u03b1) K\u220f k=1 \u03c0\u03b1\u22121k\n= \u03b1a\u22121e\u2212b\u03b1(\u03c01 . . . \u03c0K) \u03b1\u22121 \u0393(K\u03b1)\n[\u0393(\u03b1)]K .\nar X\niv :1\n70 8.\n08 17\n7v 1\n[ cs\n.L G\n] 2\n8 A\nug 2\n01 7\nThe following two theorems give the proof that the conditional posterior distribution of \u03b1 is logconcave.\nTheorem 2.1. Define G(x) = \u0393(Kx)\n[\u0393(x)]K .\nFor x > 0 and an arbitrary positive integer K, the function G is strictly log-concave.\nProof. From Abramowitz et al. (1966) we get \u0393(Kx) = (2\u03c0) 1 2 (1\u2212K)KKx\u2212 1 2 \u220fK\u22121 i=0 \u0393(x+ i K ). Then\nlogG(x) = 1 2 (1\u2212K) log(2\u03c0) + (Kx\u2212 1 2 ) logK + K\u22121\u2211 i=0 log \u0393(x+ i K )\u2212K log \u0393(x)\nand\n[logG(x)]\u2032 = K logK + K\u22121\u2211 i=0 \u03a8(x+ i K )\u2212K\u03a8(x),\nwhere \u03a8(x) is the Digamma function, and\n\u03a8\u2032(x) = \u221e\u2211 h=0\n1\n(x+ h)2 . (1)\nThus\n[logG(x)]\u2032\u2032 = [ K\u22121\u2211 i=0 \u03a8\u2032(x+ i K ) ] \u2212K\u03a8\u2032(x) < 0, (x > 0).\nThe last inequality comes from (1) and concludes the theorem.\nThis theorem is a general case of Theorem 1 in Merkle (1997).\nTheorem 2.2. In p(\u03b1|\u03c0, a, b), when a \u2265 1, p(\u03b1|\u03c0, a, b) is log-concave\nProof. It is easy to verify that \u03b1a\u22121e\u2212b\u03b1(\u03c01 . . . \u03c0K)\u03b1\u22121 is log-concave when a \u2265 1. In view of that the product of two log-concave functions is log-concave and Theorem 2, it follows that \u0393(K\u03b1)\n[\u0393(\u03b1)]K is log-concave.\nThis concludes the proof.\nFrom the two theorems above, we can find the conditional posterior for \u03b1 depends only on the weight of each cluster. The distribution p(\u03b1|\u03c0, a, b) is log-concave, so we may efficiently generate independent samples from this distribution using Adaptive Rejection Sampling (ARS) technique (Gilks & Wild, 1992).\nAlthough the proposed hyperprior on Dirichlet distribution prior for mixture model is generic, we focus on its application in Gaussian mixture models for concreteness. We develop a collapsed Gibbs sampling algorithm based on Neal (2000) for posterior computation.\nLet X be the observations, assumed to follow a mixture of multivariate Gaussian distributions. We use a conjugate Normal-Inverse-Wishart (NIW) prior p(\u00b5,\u03a3|\u03b2) for the mean vector \u00b5 and covariance matrix \u03a3 in each multivariate Gaussian component, where \u03b2 consists of all the hyperparameters in NIW. A key quantity in a collapsed Gibbs sampler is the probability of each customer i sitting with table k: p(zi = k|z\u2212i,X , \u03b1,\u03b2), where z\u2212i are the seating assignments of all the other customers and \u03b1 is the concentration parameter in Dirichlet distribution. This probability is calculated as follows:\np(zi = k|z\u2212i,X , \u03b1,\u03b2) \u221d p(zi = k|z\u2212i, \u03b1, \u03b2)p(X|zi = k, z\u2212i, \u03b1,\u03b2) = p(zi = k|z\u2212i, \u03b1)p(xi|X\u2212i, zi = k, z\u2212i,\u03b2)p(X\u2212i| zi = k, z\u2212i,\u03b2) \u221d p(zi = k|z\u2212i, \u03b1)p(xi|X\u2212i, zi = k, z\u2212i,\u03b2) \u221d p(zi = k|z\u2212i, \u03b1)p(xi|Xk,\u2212i,\u03b2),\nwhere Xk,\u2212i are the observations in table k excluding the ith observation. Algorithm 1 gives the pseudo code of the collapsed Gibbs sampler to implement hyperprior for Dirichlet distribution prior in Gaussian mixture models. Note that ARS may require even 10-20 times the computational effort per iteration over sampling once from a gamma density and there is the issue of mixing being worse if we don\u00e2\u0102\u0179t marginalize out the \u03c0 in updating \u03b1. So this might have a very large impact on effective sample size (ESS) of the Markov chain. Hence, marginalizing out \u03c0 and using an approximation to the conditional distribution (perhaps with correction through an accept/reject step via usual Metropolis-Hastings or even just using importance weighting without the accept/reject) or even just a Metropolis-Hastings normal random walk for log(\u03b1) may be much more efficient than ARS in practice. We here only introduce the updating by ARS.\ninput : Choose an initial z, \u03b1 and \u03b2;\nfor T iterations do for i\u2190 1 to N do\nRemove xi\u2019s statistics from component zi ; for k \u2190 1 to K do\nCalculate p(zi = k|z\u2212i, \u03b1) ; Calculate p(xi|Xk,\u2212i,\u03b2); Calculate p(zi = k|z\u2212i,X , \u03b1,\u03b2) \u221d p(zi = k|z\u2212i, \u03b1)p(xi|Xk,\u2212i,\u03b2);\nend Sample knew from p(zi|z\u2212i,X , \u03b1,\u03b2) after normalizing; Add xi\u2019s statistics to the component zi = knew ;\nend ? Draw current weight variable \u03c0 = {\u03c01, \u03c02, . . . , \u03c0K} ; ? Update \u03b1 using ARS;\nend Algorithm 1: Collapsed Gibbs sampler for a finite Gaussian mixture model with hyperprior on Dirichlet distribution"}, {"heading": "3 Experiments", "text": "In the following experiments we evaluate the effect of a hyperprior on symmetric Dirichlet prior in finite Bayesian mixture model."}, {"heading": "3.1 Synthetic simulation", "text": "The parameters of the simulations are as follows, where K0 is the true cluster number. And we use K to indicate the cluster number we used in the test:\nSim 1: K0 = 3, with N=300, \u03c0={0.5, 0.3, 0.2}, \u00b5={-5, 0, 5} and \u03c3={1, 1, 1}; In the test we put \u03b1 \u223c Gamma(1, 1) as the hyperprior. Figure 2 shows the result on Sim 1 with different set of K. Figure 3 shows the posterior density of \u03b1 in each set of K. We can find that the larger K \u2212K0, the smaller the poserior mean of \u03b1. This is what we expect, as the larger overfitting, the smaller \u03b1 will shrink the weight vector in the edge of a probability simplex."}, {"heading": "4 Conclusion", "text": "We have proposed a new hyperprior on symmetric Dirichlet distribution in finite Bayesian mixture model. This hyperprior can learn the concentration parameter in Dirichlet prior due to over-fitting of the mixture model. The larger the overfitting (i.e. K \u2212K0 is larger, more overfitting), the smaller the concentration parameter.\nAlthough Rousseau & Mengersen (2011) proved that \u03b1=max(\u03b1k, k 6 K) < D/2, the extra components are emptied at a rate of N\u22121/2, it is still risky to use such small \u03b1 in practice, for example, how much do we over-fit (i.e. how large the K \u2212 K0). If K \u2212 K0 is small, we will get very poor mixing from MCMC. Some efforts has been done further by van Havre et al. (2015). But simple hyperprior on Dirichlet distribution will somewhat release the burden."}], "references": [{"title": "Handbook of mathematical functions", "author": ["Milton Abramowitz", "Irene A Stegun"], "venue": "Applied mathematics series,", "citeRegEx": "Abramowitz and Stegun,? \\Q1966\\E", "shortCiteRegEx": "Abramowitz and Stegun", "year": 1966}, {"title": "Adaptive rejection sampling for gibbs sampling", "author": ["Walter R Gilks", "Pascal Wild"], "venue": "Applied Statistics,", "citeRegEx": "Gilks and Wild.,? \\Q1992\\E", "shortCiteRegEx": "Gilks and Wild.", "year": 1992}, {"title": "Dirichlet process gaussian mixture models: Choice of the base distribution", "author": ["Dilan G\u00f6r\u00fcr", "Carl Edward Rasmussen"], "venue": "Journal of Computer Science and Technology,", "citeRegEx": "G\u00f6r\u00fcr and Rasmussen.,? \\Q2010\\E", "shortCiteRegEx": "G\u00f6r\u00fcr and Rasmussen.", "year": 2010}, {"title": "On log-convexity of a ratio of gamma functions. Publikacije Elektrotehni\u010dkog fakulteta", "author": ["Milan Merkle"], "venue": "Serija Matematika,", "citeRegEx": "Merkle.,? \\Q1997\\E", "shortCiteRegEx": "Merkle.", "year": 1997}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["Radford M Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal.,? \\Q2000\\E", "shortCiteRegEx": "Neal.", "year": 2000}, {"title": "The infinite gaussian mixture model", "author": ["Carl Edward Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmussen.,? \\Q1999\\E", "shortCiteRegEx": "Rasmussen.", "year": 1999}, {"title": "Asymptotic behaviour of the posterior distribution in overfitted mixture models", "author": ["Judith Rousseau", "Kerrie Mengersen"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Rousseau and Mengersen.,? \\Q2011\\E", "shortCiteRegEx": "Rousseau and Mengersen.", "year": 2011}, {"title": "Overfitting Bayesian mixture models with an unknown number of components", "author": ["Zo\u00e9 van Havre", "Nicole White", "Judith Rousseau", "Kerrie Mengersen"], "venue": "PloS one,", "citeRegEx": "Havre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Havre et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "2 Hyperprior on symmetric Dirichlet distribution Inspired by Rasmussen (1999) and further discussed by G\u00f6r\u00fcr & Edward Rasmussen (2010), they introduced a hyperprior on symmetric Dirichlet distribution prior.", "startOffset": 61, "endOffset": 78}, {"referenceID": 5, "context": "2 Hyperprior on symmetric Dirichlet distribution Inspired by Rasmussen (1999) and further discussed by G\u00f6r\u00fcr & Edward Rasmussen (2010), they introduced a hyperprior on symmetric Dirichlet distribution prior.", "startOffset": 61, "endOffset": 135}, {"referenceID": 3, "context": "This theorem is a general case of Theorem 1 in Merkle (1997). Theorem 2.", "startOffset": 47, "endOffset": 61}, {"referenceID": 4, "context": "We develop a collapsed Gibbs sampling algorithm based on Neal (2000) for posterior computation.", "startOffset": 57, "endOffset": 69}, {"referenceID": 7, "context": "Some efforts has been done further by van Havre et al. (2015). But simple hyperprior on Dirichlet distribution will somewhat release the burden.", "startOffset": 42, "endOffset": 62}], "year": 2017, "abstractText": "In this article we introduce how to put vague hyperprior on Dirichlet distribution, and we update the parameter of it by adaptive rejection sampling (ARS). Finally we analyze this hyperprior in an over-fitted mixture model by some synthetic experiments.", "creator": "LaTeX with hyperref package"}}}