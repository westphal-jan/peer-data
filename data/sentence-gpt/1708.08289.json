{"id": "1708.08289", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Generating Query Suggestions to Support Task-Based Search", "abstract": "We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the first place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. These queries should be used by researchers who have performed previous work on the topic, or at least in the context of a new field of research to investigate. The framework is now based on an idea called a random-sum distribution algorithm. This is used to model random-sum distributions, where a number of potential answers are given, and a specific distribution is generated.\n\n\n\n\nA random distribution algorithm may be useful for solving problem-solving problems, but one that can be applied to all data types, including the underlying query suggestions, is more suited to problem solving than it is to problem solving. This approach is based on a concept called the random-sum distribution algorithm.\nRandom-sum distributions are used to test, detect, and determine hypotheses based on a set of variables. The first parameter for each set of variables is a variable that evaluates the answer to the question.\nThe second parameter for each variable is a variable that evaluates the answer to the question. The first parameter for each variable is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the condition that is the", "histories": [["v1", "Mon, 28 Aug 2017 12:44:14 GMT  (381kb,D)", "http://arxiv.org/abs/1708.08289v1", "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17), 2017"]], "COMMENTS": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17), 2017", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["dar\\'io garigliotti", "krisztian balog"], "accepted": false, "id": "1708.08289"}, "pdf": {"name": "1708.08289.pdf", "metadata": {"source": "CRF", "title": "Generating\u0083ery Suggestions to Support Task-Based Search", "authors": ["Dar\u0131\u0301o Gariglio\u008ai", "Krisztian Balog"], "emails": ["dario.gariglioi@uis.no", "krisztian.balog@uis.no", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022Information systems \u2192 ery suggestion;\nKEYWORDS ery suggestions, task-based search, supporting search tasks ACM Reference format: Dar\u0131\u0301o Gariglio i and Krisztian Balog. 2017. Generating ery Suggestions to Support Task-Based Search. In Proceedings of SIGIR\u201917, August 7\u201311, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080745"}, {"heading": "1 INTRODUCTION", "text": "Search is o en performed in the context of some larger underlying task [11]. ere is a growing stream of research aimed at making search engines more task-aware (i.e., recognizing what task the user is trying to accomplish) and customizing the search experience accordingly (see \u00a72). In this paper, we focus our a ention on one particular tool for supporting task-based search: query suggestions. ery suggestions are an integral part of modern search engines [16]. We envisage an user interface where these suggestions are presented once the user has issued an initial query; see Figure 1. Note that this is di erent from query autocompletion, which tries to recommend various possible completions while the user is still typing the query. e task-aware query suggestions we propose are intended for exploring various aspects (subtasks) of the given task a er inspecting the initial search results. Selecting them would allow the user to narrow down the scope of the search.\ne Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20]. Speci cally, given an initial query,\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR\u201917, August 7\u201311, 2017, Shinjuku, Tokyo, Japan \u00a9 2017 ACM. 978-1-4503-5022-8/17/08. . .$15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080745\nthe system should return a ranked list of keyphrases \u201cthat represent the set of all tasks a user who submi ed the query may be looking for\u201d [20]. e goal is to provide a complete coverage of subtasks for an initial query, while avoiding redundancy. We use these keyphrases as query suggestions.\nOur aim is to generate such suggestions in a se ing where past usage data and query logs are not available or cannot be utilized. is would be typical for systems that have a smaller user base (e.g., in the enterprise domain) or when a search engine has been newly deployed [4]. One possibility is to use query suggestion APIs, which are o ered by all major web search engines. ese are indeed one main source type we consider. Additionally, we use the initial query to search for relevant documents, using web search engines, and extract keyphrases from search snippets and from full text documents. Finally, given the task-based focus of our work, we lend special treatment to the WikiHow site,1 which is an extensive database of how-to guides.\ne main contribution of this paper is twofold. First, we propose a transparent architecture, using generative probabilistic modeling, for extracting keyphrases from a variety of sources and generating query suggestions from them. Second, we provide a detailed analysis of the components of our framework using di erent estimation methods. Many systems that participated in the TREC Tasks track have relied on strategic combinations of di erent sources to produce query suggestions, see, e.g., [7\u20139]. However, no systematic comparison of the di erent source types has been performed yet\u2014we ll this gap. Additional components include estimating a document\u2019s importance within a given source, extracting keyphrases from documents, and forming query suggestions from these keyphrases. Finally, we check whether our ndings are consistent across the 2015 and 2016 editions of the TREC Tasks track."}, {"heading": "2 RELATEDWORK", "text": "ere is a large body of work on understanding and supporting users in carrying out complex search tasks. Log-based studies have been one main area of focus, including the identi cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query 1h p://www.wikihow.com/\nar X\niv :1\n70 8.\n08 28\n9v 1\n[ cs\n.I R\n] 2\n8 A\nug 2\n01 7\nreformulations [10] or search trails [19]. Another theme is supporting exploratory search, where users pursue an information goal to learn or discover more about a given topic. Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17]. Our main interest is in query suggestions, as a distinguished support mechanism. Most of the related work utilizes large-scale query logs. For example, Craswell and Szummer [6] perform a random walk on a query-click graph. Boldi et al. [5] model the query ow in user search sessions via chains of queries. Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus. However, their focus is on query autocompletion, representing the completed and partial terms in a query. Kelly et al. [12] have shown that users prefer query suggestions, rather than term suggestions. We undertake the task of suggesting queries to users, related to the task they are performing, as we shall explain in the next section."}, {"heading": "3 PROBLEM STATEMENT", "text": "We adhere to the problem de nition of the task understanding task of the TREC Tasks track. Given an initial query q0, the goal is to return a ranked list of query suggestions \u3008q1, . . .qn\u3009 that cover all the possible subtasks related to the task the user is trying to achieve. In addition to the initial query string, the entities mentioned in it are also made available (identi ed by their Freebase IDs).\nFor example, for the query \u201clow wedding budget,\u201d subtasks include (but are not limited to) \u201cbuy a used wedding gown,\u201d \u201ccheap wedding cake,\u201d and \u201cmake your own invitations.\u201d ese subtasks have been manually identi ed by the track organizers based on information extracted from the logs of a commercial search engine. e suggested keyphrases are judged with respect to each subtask on a three point scale (non-relevant, relevant, and highly relevant). Note that subtasks are only used in the evaluation, these are not available when generating the keyphrases."}, {"heading": "4 APPROACH", "text": "We now present our approach for generating query suggestions. As Figure 2 illustrates, we obtain keyphrases from a variety of sources, and then construct a ranked list of query suggestions from these."}, {"heading": "4.1 Generative Modeling Framework", "text": "We introduce a generative probabilistic model for scoring the candidate query suggestions according to P(q |q0), i.e., the probability that a query suggestion q was generated by the initial query q0.\nFormally:\nP(q |q0) = \u2211 s P(q |q0, s)P(s |q0)\n= \u2211 s (\u2211 d P(q |q0, s,d)P(d |q0, s) ) P(s |q0)\n= \u2211 s (\u2211 d (\u2211 k P(q |q0, s,k)P(k |s,d) ) P(d |q0, s) ) P(s |q0) .\nis model has four components: (i) P(s |q0) expresses the importance of a particular information source s for the initial query q0; (ii) P(d |q0, s) represents the importance of a document d originating from source s , with respect to the initial query; (iii) P(k |d, s) is the relevance of a keyphrase k extracted from a document d from source s ; and (iv) P(q |q0, s,k) is the probability of generating query suggestion q, given keyphrase k , source s , and the initial query q0. Below, we detail the estimation of each of these components."}, {"heading": "4.2 Source Importance", "text": "We collect relevant information from four kinds of sources: query suggestions (QS), web search snippets (WS), web search documents (WD), and WikiHow (WH). For the rst three source types, we use three di erent web search engines (Google, Bing, and DuckDuckGo), thereby having a total of 10 individual sources. In this work, we assume conditional independence between a source s and the initial query q0, i.e., set P(s |q0) = P(s)."}, {"heading": "4.3 Document Importance", "text": "From each source s , we obtain the top-K (K = 10) documents for the query q0. We propose two ways of modeling the importance of a document d originating from s: (i) uniform and (ii) inversely proportional to the rank of d among the top-K documents, that is:\nP(d |q0, s) = K \u2212 r + 1\u2211K i=1 K \u2212 i + 1 = K \u2212 r + 1 K(K + 1)/2 ,\nwhere r is the rank position of d (r \u2208 [1..K])."}, {"heading": "4.4 Keyphrase Relevance", "text": "We obtain keyphrases from each document, using an automatic keyphrase extraction algorithm. Speci cally, we use the RAKE keyword extraction system [15]. For each keyphrase k , extracted from document d , the associated con dence score is denoted by c(k,d). Upon a manual inspection of the extraction output, we introduce some data cleansing steps. We only retain keyphrases that: (i) have an extraction con dence above an empirically set threshold of 2; (ii) are at most 5 terms long; (iii) each of the terms has a length between 4 and 15 characters, and is either a meaningful number (i.e., max. 4 digits) or a term (excluding noisy substrings and reserved keywords from mark-up languages). Finally, we set the relevance of k as P(k |d, s) = c(k,d)/\u2211k \u2032 c(k \u2032,d).\nIn case s is of type QS, each returned document actually corresponds to a query suggestion. us, we treat each of these documents d as a single keyphrase k , for which we set P(k |d, s) = 1."}, {"heading": "4.5 ery Suggestions", "text": "As a nal step, we need to generate query suggestions from the extracted keyphrases. As a baseline option, we take each raw keyphrase k as-is, i.e., with q = k we set P(q |q0, s,k) = 1.\nAlternatively, we can form query suggestions by expanding keyphrases. Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a sufx to q0; (ii) adding k as a su x to an entity mentioned in q0; and (iii) using k as-is. Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details. Each query suggestion q, that is generated from keyword k , has an associated con dence score c(q,q0, s,k). We then set P(q |q0, s,k) = c(q,q0, s,k)/ \u2211 q\u2032 c(q\u2032,q0, s,k). By conditioning the suggestion probability on s , it is possible to apply a di erent approach for each source. Like in the previous subsection, we treat sources of type QS distinctly, by simply taking q = k and se ing P(q |q0, s,k) = 1.\nWe note that it is possible that multiple query suggestions have the same nal probability P(q |q0). We resolve ties using a deterministic algorithm, which orders query suggestions by length (favoring short queries) and then sorts them alphabetically."}, {"heading": "5 RESULTS", "text": "In this section we present our experimental setup and results."}, {"heading": "5.1 Experimental Setup", "text": "We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20]. ese contain 34 and 50 queries with relevance judgments, respectively. We report on the o cial evaluation metrics used at the TREC Tasks track, which are ERR-IA@20 and \u03b1-NDCG@20. In accordance with the track\u2019s se ings, we use ERR-IA@20 as our primary metric. (For simplicity, we omit mentioning the cut-o rank of 20 in all the table headers.) We noticed that in the ground truth the initial query itself has been judged as a highly relevant suggestion in numerous cases. We removed these cases, as they make li le sense for the envisioned scenario; we note that this leads to a drop in absolute terms of performance. We report on statistical signi cance using a two-tailed paired t-test at p < 0.05 and p < 0.001, denoted by \u2020 and \u2021, respectively.\nIn a series of experiments, we evaluate each component of our approach, in a bo om-up fashion. For each query set, we pick the\ncon guration that performed best on that query set, which is an idealized scenario. Note that our focus is not on absolute performance gures, but on answering the following research questions:\nRQ1 What are the most useful information sources? RQ2 What are e ective ways of (i) estimating the importance\nof documents and (ii) generating query suggestions from keyphrases? RQ3 Are our ndings consistent across the two query sets?"}, {"heading": "5.2 ery Suggestion Generation", "text": "We start our experiments by focusing on the generation of query suggestions and compare the two methods described in \u00a74.5. e document importance is set to be uniform. We report performance separately for each of the four source types S (that is, we set P(s) uniformly among sources s \u2208 S and set P(s) = 0 for s < S). Table 1 presents the results. It is clear that, with a single exception (2015 WH), it is be er to use the raw keyphrases, without any expansion. e di erences are signi cant on the 2016 query set for all source types but QS. Regarding the comparison of di erent source types, we nd that QS >WS >WD >WH on the 2016 query set, meanwhile for 2015, the order is WS >WD > QS, WH."}, {"heading": "5.3 Document Importance", "text": "Next, we compare the two document importance estimator methods, uniform and rank-based decay (cf. \u00a74.3), for each source type. Table 2 reports the results. We nd that rank-based document importance is bene cial for the query suggestion (QS) source types, for both years, and for WikiHow (WH) on the 2015 topics. However, the di erences are only signi cant for QS 2015. For all other source types, the uniform se ing performs be er.\nWe also compare performance across the 10 individual sources. Figure 3 shows the results, in terms of ERR-IA@20, using the uniform estimator. We observe a very similar pa ern using the rankbased estimator (which is not included due to space constraints). On the 2016 query set, the individual sources follow the exact same pa erns as their respective types (i.e., QS >WS >WD >WH), with one exception. e Bing API returned an empty set of search suggestions for many queries, hence the low performance of QSBin\u0434 . We can observe a similar pa ern on the 2015 topics, with the exception of sources of type QS, which are the least e ective here."}, {"heading": "5.4 Source Importance", "text": "Finally, we combine query suggestions across di erent sources; for that, we need to set the importance of each source. We consider three di erent strategies for se ing P(s): (i) uniformly; (ii) proportional to the importance of the corresponding source type (QS, WS, WD, and WH) from the previous step (cf. Table 2); (iii) proportional to the importance of the individual source (cf. Figure 3). e results are presented in Table 3. Firstly, we observe that the combination of sources performs be er than any individual source type on its own. As for se ing source importance, on the 2015 query set we nd that (iii) delivers the best results, which is in line with our expectations. On the 2016 query set, only minor di erences are observed between the three methods, none of which are signi cant."}, {"heading": "5.5 Summary of Findings", "text": "(RQ1) ery suggestions provided by major web search engines are unequivocally the most useful information source on the 2016 queries. We presume that these search engine suggestions are already diversi ed, which we can directly bene t from for our task. ese are followed, in order, by keyphrases extracted from (i) web search snippets, (ii) web search results, i.e., full documents, and (iii) WikiHow articles. On the 2015 query set, query suggestions proved much less e ective; see RQ3 below. (RQ2) With a single exception, using the raw keyphrases, as-is, performs be er than expanding them by taking the original query into account. For web query suggestions it is bene cial to consider the rank order of suggestions, while for web search snippets and documents the uniform se ing performs be er. For WikiHow, it varies across query sets. (RQ3) Our main observations are consistent across the 2015 and 2016 query sets, regarding documents importance estimation and suggestions generation methods. It is worth noting that some of our methods were o cially submi ed to TREC 2016 [7] and were included in the assessment pools. is is not the case for 2015, where many of our query suggestions are missing relevance assessments (and, thus, are considered irrelevant). is might explain the low performance of QS sources on the 2015 queries."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we have addressed the task of generating query suggestions that can assist users in completing their tasks. We have proposed a probabilistic generative framework with four components:\nsource importance, document importance, keyphrase relevance, and query suggestions. We have proposed and experimentally compared various alternatives for these components.\nOne important element, missing from our current model, is the representation of speci c subtasks. As a next step, we plan to cluster query suggestions together that belong to the same subtask. is would naturally enable us to provide diversi ed query suggestions."}], "references": [{"title": "IntentStreams: Smart Parallel Search Streams for Branching Exploratory Search", "author": ["Salvatore Andolina", "Khalil Klouche", "Jaakko Peltonen", "Mohammad E. Hoque", "Tuukka Ruotsalo", "Diogo Cabral", "Arto Klami", "Dorota Glowacka", "Patrik Flor\u00e9en", "Giulio Jacucci"], "venue": "In Proc. of IUI", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Supporting Complex Search Tasks", "author": ["Ahmed H. Awadallah", "Ryen W. White", "Patrick Pantel", "Susan T. Dumais", "Yi- Min Wang"], "venue": "In Proc. of CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Task-completion Engines: A Vision with a Plan", "author": ["Krisztian Balog"], "venue": "In Proc. of the 1st International Workshop on Supporting Complex Search Tasks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "\u008bery Suggestions in the Absence of \u008bery Logs", "author": ["Sumit Bhatia", "Debapriyo Majumdar", "Prasenjit Mitra"], "venue": "In Proc. of SIGIR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "\u008ce \u008bery-\u0083ow Graph: Model and Applications", "author": ["Paolo Boldi", "Francesco Bonchi", "Carlos Castillo", "Debora Donato", "Aristides Gionis", "Sebastiano Vigna"], "venue": "In Proc. of CIKM", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Random walks on the click graph", "author": ["Nick Craswell", "Martin Szummer"], "venue": "In Proc. of SIGIR", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "\u008ce University of Stavanger at the TREC 2016 Tasks Track", "author": ["Dar\u0131\u0301o Gariglio\u008ai", "Krisztian Balog"], "venue": "In Proc. of TREC", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Webis at TREC 2015: Tasks and Total Recall Tracks", "author": ["Ma\u008ahias Hagen", "Steve G\u00f6ring", "Magdalena Keil", "Olaoluwa Anifowose", "Amir Othman", "Benno Stein"], "venue": "In Proc. of TREC", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks", "author": ["Ma\u008ahias Hagen", "Johannes Kiesel", "Payam Adineh", "Masoud Alahyari", "Ehsan Fatehifar", "Arafeh Bahrami", "Pia Fichtl", "Benno Stein"], "venue": "In Proc. of TREC", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Contextual Evaluation of \u008bery Reformulations in a Search Session by User Simulation", "author": ["Jiepu Jiang", "Daqing He", "Shuguang Han", "Zhen Yue", "Chaoqun Ni"], "venue": "In Proc. of CIKM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A Comparison of \u008bery and Term Suggestion Features for Interactive Searching", "author": ["Diane Kelly", "Karl Gyllstrom", "Earl W. Bailey"], "venue": "In Proc. of SIGIR", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Deriving query suggestions for site", "author": ["Udo Kruschwitz", "Deirdre Lungley", "M-Dyaa Albakour", "Dawei Song"], "venue": "search. JASIST 64,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Discovering Tasks from Search Engine \u008bery Logs", "author": ["Claudio Lucchese", "Salvatore Orlando", "Ra\u0082aele Perego", "Fabrizio Silvestri", "Gabriele Tolomei"], "venue": "ACM Trans. Inf. Syst", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Modi\u0080ed RAKE algorithm. h\u008aps://github.com/zelandiya/ RAKE-tutorial", "author": ["Alyona Medelyan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning to Suggest: A Machine Learning Framework for Ranking \u008bery Suggestions", "author": ["Umut Ozertem", "Olivier Chapelle", "Pinar Donmez", "Emre Velipasaoglu"], "venue": "In Proc. of SIGIR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "\u008ce Forgo\u008aen Needle in My Collections: Task-Aware Ranking of Documents in Semantic Information Space", "author": ["Tuan A. Tran", "Sven Schwarz", "Claudia Nieder\u00e9e", "Heiko Maus", "Na\u008aiya Kanhabua"], "venue": "In Proc. of CHIIR", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Overview of the TREC Tasks Track 2016", "author": ["Manisha Verma", "Evangelos Kanoulas", "Emine Yilmaz", "Rishabh Mehrotra", "Ben Cartere\u008ae", "Nick Craswell", "Peter Bailey"], "venue": "In Proc. of TREC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs", "author": ["Ryen W. White", "Je\u0082 Huang"], "venue": "In Proc. of SIGIR", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Overview of the TREC 2015 Tasks Track", "author": ["Emine Yilmaz", "Manisha Verma", "Rishabh Mehrotra", "Evangelos Kanoulas", "Ben Cartere\u008ae", "Nick Craswell"], "venue": "In Proc. of TREC", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "\u008bery suggestions are an integral part of modern search engines [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "\u008ce Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "the system should return a ranked list of keyphrases \u201cthat represent the set of all tasks a user who submi\u008aed the query may be looking for\u201d [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 3, "context": ", in the enterprise domain) or when a search engine has been newly deployed [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 8, "context": ", [7\u20139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 1, "context": "Log-based studies have been one main area of focus, including the identi\u0080cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query", "startOffset": 135, "endOffset": 142}, {"referenceID": 12, "context": "Log-based studies have been one main area of focus, including the identi\u0080cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query", "startOffset": 135, "endOffset": 142}, {"referenceID": 9, "context": "reformulations [10] or search trails [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "reformulations [10] or search trails [19].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 2, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 15, "context": "Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17].", "startOffset": 89, "endOffset": 99}, {"referenceID": 5, "context": "For example, Craswell and Szummer [6] perform a random walk on a query-click graph.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "[5] model the query \u0083ow in user search sessions via chains of queries.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus.", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus.", "startOffset": 62, "endOffset": 69}, {"referenceID": 10, "context": "[12] have shown that users prefer query suggestions, rather than term suggestions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Speci\u0080cally, we use the RAKE keyword extraction system [15].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a suf\u0080x to q0; (ii) adding k as a su\u0081x to an entity mentioned in q0; and (iii) using k as-is.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20].", "startOffset": 61, "endOffset": 69}, {"referenceID": 18, "context": "We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20].", "startOffset": 61, "endOffset": 69}, {"referenceID": 6, "context": "It is worth noting that some of our methods were o\u0081cially submi\u008aed to TREC 2016 [7] and were included in the assessment pools.", "startOffset": 80, "endOffset": 83}], "year": 2017, "abstractText": "We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the \u0080rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.", "creator": "LaTeX with hyperref package"}}}