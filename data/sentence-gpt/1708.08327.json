{"id": "1708.08327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2017", "title": "Feature Conservation in Adversarial Classifier Evasion: A Case Study", "abstract": "Machine learning is widely used in security applications, particularly in the form of statistical classification aimed at distinguishing benign from malicious entities. Recent research has shown that such classifiers are often vulnerable to evasion attacks, whereby adversaries change behavior to be categorized as benign while preserving malicious functionality. Research into evasion attacks has followed two paradigms: attacks in problem space, where the actual malicious instance is modified, and attacks in feature space, where the attack is abstracted into modifying numerical features of an instance to evade a classifier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 28 Aug 2017 14:18:35 GMT  (707kb,D)", "http://arxiv.org/abs/1708.08327v1", "14 pages, 15 figures"]], "COMMENTS": "14 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["liang tong", "bo li", "chen hajaj", "yevgeniy vorobeychik"], "accepted": false, "id": "1708.08327"}, "pdf": {"name": "1708.08327.pdf", "metadata": {"source": "CRF", "title": "Feature Conservation in Adversarial Classifier Evasion: A Case Study", "authors": ["Liang Tong", "Bo Li", "Chen Hajaj", "Yevgeniy Vorobeychik"], "emails": ["liang.tong@vanderbilt.edu", "crystalboli@berkeley.edu", "chen.hajaj@vanderbilt.edu", "yevgeniy.vorobeychik@vanderbilt.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMachine learning techniques are increasingly deployed in security applications such as network intrusion detection, fraud detection, and malware detection. Most of the traditional malware detection approaches are based on dynamic analysis, which incurs significant computational overhead and latency since such procedures depend on execution of shellcodes [31]. Machine learning\u2014especially statistical classification\u2014offers an effective alternative. The general approach is to extract a set of features, or numerical attributes, of entities in question, collect a training data set of labeled examples of malicious and benign instances, and learn a model which categorizes a previously unseen instance, presented in terms of its extracted features, as either malicious or benign. State-of-the-art approaches of this kind enable static detection of malicious entities, such as malware, efficiently and with accuracy often exceeding 99% [31], [30].\nRecent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27]. A fundamental reason for such vulnerabilities is that classification learning algorithms generally assume that the distribution of training and test (or production) data is similar. This assumption\nis violated in security applications, where malicious entities correspond to attackers who can, and do, take deliberate action to evade defensive measures. In the context of machine learning, evasion takes the form of modifying the structure of the malicious instances, such as malicious code, to make these look benign to the classifier. The net effect of the evasion attack is a modification of the feature vector corresponding to a malicious instance. However, a central challenge on the road to designing evasion-robust classifiers is an effective model of such malicious transformations in response to a particular classifier.\nTwo paradigms have emerged for model classifier evasion attacks. The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21]. While not guaranteed to exactly simulate behavior of real attackers, these faithfully replicate the constraints faced by real attackers in designing evasions, such as the inability to effect arbitrary changes in the feature space, and the requirement that malicious functionality of the instance is preserved (typically evaluated using a sandbox, such as WEPAWET [6] and the Cuckoo sandbox [13]). The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].\nA number of approaches have been proposed aiming to address vulnerabilities of classifiers to evasion attacks. These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16]. What is noteworthy about these approaches is that they rely on feature space attack models.1 Two questions therefore naturally arise: (1) can we develop techniques for evasion-robust classification which rely on problem space evasion models, and (2) how well do feature space evasion models represent actual attacks (if we take problem space attacks to be a reasonable representation thereof)? We explore these questions in the context of PDF malware detection, making use of the SL2013 machine learn-\n1A subset of methods [11], [15] focused on adversarial deep neural network learning for vision tasks consider modified pixels in images, but these are also the features of the learning algorithms. Generalization of these methods to other domains relies on feature space attack models. Moreover, these tend to ignore realistic constraints on attacker, such as modifications of physical environments which are subsequently translated into images, although these have been explored in considering attacks independently of defensive techniques [29].\nar X\niv :1\n70 8.\n08 32\n7v 1\n[ cs\n.C R\n] 2\n8 A\nug 2\n01 7\ning tool for detecting malicious PDFs [31], and EvadeML, an automated problem space method for PDF malware classifier evasion [37]. SL2013 is a state of the art PDF malware classifier, noteworthy because it was designed specifically to be resistant to evasion attacks.2 EvadeML makes use of genetic programming to automatically change PDF source in order to achieve benign classification while preserving malicious functionality and was specifically shown to successfully evade SL2013, achieving only 16% correct classification on selected malicious instances (see, however, the discussion below).\nOur first contribution is an affirmative answer to the first question. Specifically, we exhibit an approach which leverages EvadeML to iteratively generate evasion instances and retrain the classifier and show that the resulting classifier becomes robust to evasion attacks by EvadeML after only 10 iterations of this process (achieving an accuracy of 96% on EvadeML instances). A major shortcoming of this approach, however, is speed: it takes several days to complete 10 iterations. Next, we demonstrate that a similar technique using a natural featurespace evasion model does not produce a classifier which is robust against the problem space attack implemented in EvadeML (exhibiting only \u223c 60% accuracy on EvadeML instances). This finding, which obtains even when we consider a principled approach for differentiating features in terms of their evasion costs, casts a deep shadow on the prevailing approaches for designing evasion-robust classifiers to date. Exploring the reasons for the apparent failure of feature-space defense, we discover, surprisingly, that the traditional model is too adversarial: it generates adversarial examples which are atypical of real attacks in that certain feature modifications present in these are absent in EvadeML instances. To systematically address the problem, we identify a set of conserved features which are (essentially) invariant in problem space attacks, likely representing functionally vital regions of the feature space. These conserved features echo an analogous notion in epidemiology, where, for example, characterization of conserved regions of viral proteins is an important step toward designing effective vaccines [8], [9]. The existence of these is especially surprising in light of past work on adversarial evasion of machine learning in cyber security, which has been duly skeptical that such problem invariance exists, or could successfully be leveraged [10]. Remarkably, we show that a classifier which only uses 8 conserved features is surprisingly effective, with AUC>94% on test data without adversarial evasion (in contrast, SL2013 uses >6000 features), and admits no successful attacks by EvadeML!\nThe key limitation of using only the conserved features in classification is the drop in overall accuracy without evasion attacks, particularly if we are to maintain a low false positive rate\u2014a critical consideration in practice. However, we demonstrate that a modified version of feature space retraining in which conserved features are constrained to remain unchanged, proves remarkably effective, achieving 94% accuracy on EvadeML instances (that is, robustness to these attacks) while maintaining AUC at nearly the same level as the nonrobust SL2013 classifier. In addition to the resulting featurespace robust classification approach yielding a classifier which\n2An updated version of this classifier [32] has made changes to the feature space. We use the 2013 version, as this was the one attacked by EvadeML and can, therefore, be used for our case study.\nis nearly as robust as the problem space variant, it is also considerably faster. Finally, we present a novel systematic approach for effectively identifying conserved features for a given classifier and automated evasion attack method."}, {"heading": "II. RELATED WORK", "text": "a) Evasion Attacks on Learning Algorithms: Problem Space Approaches: One of the first problem space evasion attacks on machine learning was devised by Fogla et al. [10], who developed a polymorphic blending attack on anomalybased intrusion detection systems. Fogla et al. subsequently generalize and systematize the polymorphic blending attack. S\u030crndic and Lasov [30] present a case study of an evasion attack on a state of the art PDF malware classifier, PDFRate. Their mimicry attack, which primarily adds content to a PDF to make it appear as benign as possible, actually leverages an initial feature-space evasion, which is subsequently modified to effect the actual PDF source. Xu et al. [37] propose EvadeML, a fully problem space attack on PDF malware classifiers which generates evasion instances by using genetic programming to modify PDF source directly, using a sandbox to ensure that malicious functionality is preserved. The fully automated nature of EvadeML makes it a natural candidate for our indepth exploration of the relationship between problem space and feature space evasion attacks.\nIn addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17]. These are in a sense both problem and feature space, as actual pixels of the images are modified in the so-called adversarial examples which are crafted to effect errors in deep classifiers. Several efforts explored the impact of indirect modification when an adversarial image must be printed, showing that effectiveness of such techniques can nevertheless be preserved [29], [17]. Recently, an approach for printing specifically designed glass frames had been shown to mislead vision-based biometric systems to either mistakenly grant authorization, or enable evasion of face recognition techniques [29].\nb) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16]. This line of research has become prominent in part for the relative simplicity of modeling attacks in feature spaces, and additionally because it is the primary set of methods informing general-purpose methods for evasion-robust learning. Dalvi et al. [7] present one of the earliest such models as a part of a robust learning approach. An algorithmic investigation of the feature space evasion problem\u2014modeling it formally as minimizing the cost of changing features subject to a constraint that the resulting instance is classified as benign\u2014was initiated by Lowd and Meek [20]. A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].\nc) Evasion-Robust Classification: Dalvi et al. [7] presented the first approach for evasion-robust classification, making use of a model in which the attacker aims to transform feature vectors into benign instances in response to the\noriginal (non-robust) classifier, and then subsequently devising an approach which is robust to the former attack. A series of approaches formulate robust classification as minimizing maximum loss, where maximization is attributed to the evading attacker aiming to maximize the learner\u2019s loss through feature space transformations [33], [39]. All of these effectively assume that the interaction between the learner and attacker is zero sum. A number of alternative methods for designing classifiers robust to evasion relax the somewhat unreasonable assumption that the adversary aims to maximize the defender\u2019s loss. Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28]. In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16]. Recently, a systematic iterative retraining procedure had been proposed which leverages general-purpose adversarial evasion models, and offers a theoretical connection to an underlying Stackelberg game played between the learner and the evading adversary [19]. These diverse efforts share one common property: attack models that they leverage use feature space manipulations, which are only a proxy for problem space evasion attacks.3"}, {"heading": "III. BACKGROUND", "text": "This section provides background on PDF document structure, the target PDF malware classifier (SL2013) [31], and the automated evasion approach, EvadeML [37], we use to evaluate evasion robustness of classifiers.\n3Indeed, even evasion attacks on deep classifiers, in their general form, are effectively in feature space, since they rely on the gradient calculation of the training loss with respect to underlying features."}, {"heading": "A. PDF Document Structure", "text": "The Portable Document Format (PDF) is an open standard format used to present content and layout on different platforms. A PDF file structure consists of four parts: header, body, cross-reference table (CRT), and trailer, which are presented on the left-hand side of Figure 1. The header contains information such as the magic number and format version. The body is the most important element of a PDF file, which comprises multiple PDF objects that constitute the content of the file. These objects can be one of the eight basic types: Boolean, Numeric, String, Null, Name, Array, Dictionary, and Stream. They could be referred from other objects via indirect references. There are other types of objects such as JavaScript which contains executable JavaScript code. The CRT indexes objects in the body, while the trailer points to the CRT.\nThe syntax of the body of a PDF file is illustrated in the middle of Figure 1. In this example, the PDF body contains three indirect objects being referred by others. The first one is a catalog object, which contains two additional entries: OpenAction and Pages. These two entries are dictionaries. The OpenAction entry has two internal entries: S and JS, which are JavaScript codes to be executed. The Pages entry refers to the second object with the type Pages. The second object contains an entry \u201dkids\u201d which refers to the third object. The third one is a Page object which refers back to the second object.\nThe relations between the three objects above could be described as a directed graph to present their logic structure by using edges representing reference relations, and nodes representing different objects as shown on the right-hand-side of Figure 1. As an object could be referred to by its child node, the resulting logic structure is a directed cyclic graph. For example in Figure 1, the second and third objects refer to each other and constitute a directed circle. To eliminate the redundant references, the logic structure could be reduced to\na structural tree with the breadth-first search procedure."}, {"heading": "B. The Target Classifier", "text": "Several PDF malware classifiers have been proposed [6], [31]. For our study, we selected SL2013 [31]. SL2013 is a well-documented and open-source machine learning system using Support Vector Machines (SVM) with a radial basis function (RBF) kernel, and was shown to have state-of-theart performance. While it has since undergone a revision [32], the SL2013 was the version evaluated by the EvadeML tool, and the pair provides a natural evaluation framework for our purposes.\nSL2013 employs structural properties of PDF files to discriminate between malicious and benign PDFs. Specifically, SL2013 uses the presence of particular structural paths as binary features to present PDF files in feature space. A structural path of an object is a sequence of edges in the reduced logic structure, starting from the catalog dictionary and ending at this object. Therefore, the structural path reveals the shortest reference path to an object. For example, Figure 1 has the following structural paths:\n/OpenAction/JS /OpenAction/S /Pages/Kids/Parent /Pages/Kids/Resources /Pages/Kids/Type /Pages/Type /Type\nSL2013 uses a uniform set of structural paths to classify a PDF file. To get these paths, it first obtains a total of 658,763 benign and malicious PDF files (around 595 GB), then selects only the structural paths that occur in at least 1,000 PDF files. This process reduced the number of features from over 9 million to 6,087. Trained using 5,000 malicious and 5,000 benign PDF files, SL2013 was shown to have 99.8% accuracy on test data and AUC > 99.9%."}, {"heading": "C. Automated Evasion", "text": "To evaluate the robustness of a PDF classifier against adversarial evasion attacks, we adopt EvadeML [37], a systematic and automated method to craft evasion instances of PDF malware in problem space. EvadeML starts with a malicious PDF which is correctly classified as malicious and aims to produce evasive variants which have the same malicious behavior but are classified as benign. It assumes that no internal information of the target classifier is available to the adversary, such as the set of features, the training dataset, and the classification algorithm. Rather, the adversary has black-box access to the target classifier, and it can repeatedly submit PDF files to get corresponding classification scores. Based on the scores, the adversary can adapt its strategy to craft evasive variants.\nEvadeML employs genetic programming (GP) to search the space of possible PDF instances to find ones that evade the classifier while maintaining malicious features. The GP process is illustrated in Figure 2. First, an initial population is produced by randomly manipulating a malicious seed. As the seed contains multiple PDF objects, each object is set to be a target and mutated with exogenously specified probability. The mutation is either a deletion, an insertion or a swap operation. A deletion operation deletes a target object from the seed malicious PDF file. As a result, the corresponding structural path is deleted. An insertion operation inserts an object from external benign PDF files (also provided exogenously) after the target object. EvadeML uses 3 most benignly scoring PDF files. A swap operation replaces the entry of the target object with that of another object in the external PDFs.\nAfter the population is initialized, each variant is assessed by the Cuckoo sandbox [13] and the target classifier to evaluate its fitness. The sandbox is used to determine if a variant preserves malicious behavior. It opens and reads the variant PDF in a virtual machine and detects malicious behaviors such as API or network anomalies, by detecting malware signatures. The target classifier (SL2013 in our case) provides a classification score for each variant. If the score is above a threshold, then the variant is classified as malicious. Otherwise, it is classified as a benign PDF. If a variant is classified as benign but displays malicious behavior, or if GP reaches the maximum number of generations, then GP terminates with the variant\nachieving the best fitness score and the corresponding mutation trace is stored in a pool for future population initialization. Otherwise, a subset of the population is selected for the next generation based on their fitness evaluation. Afterward, the variants selected are randomly manipulated to generate the next generation of the population.\nEvadeML was used to evade SL2013 in [37]. The reported results show that it can automatically find evasive variants for all 500 selected malicious test seeds. However, we found a small error in the implementation which caused the reported evasion results to be slightly inflated; we were able to reproduce an approximately 84% evasion rate, as reported below. Throughout, we use 400 malicious seeds as a part of evasionbased training and evaluate on the remaining 100 malicious seed PDFs used by EvadeML."}, {"heading": "D. Problem Space vs. Feature Space Attacks", "text": "A distinction that is crucial in this paper is between problem space and feature space attacks. Feature space attacks assume that features in the feature vector representation of a malicious entity can be modified directly and essentially arbitrarily (modulo domain definition constraints; e.g., binary features can only be flipped). Problem space attacks, in contrast, have two important aspects:\n1) they are performed directly on the malicious entity, such as the malware source code before said entity is translated into feature space, and 2) after the malicious entity is modified, the attack is verified to still be effective, for example, using a sandbox such as the Cuckoo sandbox in the EvadeML framework above.\nTo appreciate the importance of the first aspect of problem space attacks, as distinct from those modeled directly through feature modifications, consider a simple example with two features: one which counts the number of Javascript objects, and the second which is just the size of the PDF file. Suppose that fewer Javascript objects, and larger files, are both indicative of benign PDFs according to a classifier. Clearly, however, removing Javascript objects will reduce the size of the PDF\u2014 that is, the two features are not independent, and cannot be modified independently in arbitrary ways.\nThe importance of the second aspect of problem space attacks may seem self-evidence, but it imposes strong, and very complex constraints on the nature of modifications to the malicious entity, such as a PDF file, that can be made, and these are very challenging to reflect through direct feature space modifications. One approach to side-step this issue is to restrict attacks to only modify features which are unlikely to impact malicious functionality, as was done by Grosse et al. [12]. However, strong restrictions on adversarial behavior are inadequate for considering defense against adversarial evasion attacks; moreover, one can rarely guarantee that seemingly innocuous modifications will not impact malicious functionality, and verification through a sandbox is still a necessary step.\nOne interesting case where the distinction between problem and feature space attack models appears to blur is in vision tasks involving image classification. In these domains, it is\ncommon to use deep neural network classifiers with features which are actual pixel values of images, and attacks, too, operate by modifying the pixel values. If the attack is, indeed, on the digitally stored image, the first aspect of problem space attacks in our definition above is, indeed, not relevant. However, attackers are unlikely to have access to such a digital copy, and would instead need to modify the physical environment. In this case, features can no longer be modified directly [17]. In either case, a common additional constraint is that the added adversarial noise to the image is not visible to a human eye; this constraint is only heuristically quantified through norm constraints on the noise magnitude, and a genuine problem space attack would verify that a particular added noise is, indeed, not detectable by actual human observers."}, {"heading": "IV. SYSTEMATIC RETRAINING", "text": "The experiments by Xu et al. [37] demonstrate that although SL2013 was designed specifically to be resistant to evasion attacks, it can be successfully evaded. In this section, we present a general-purpose method for boosting robustness of SL2013 to attacks by EvadeML. The approach builds on feature-space iterative retraining methods proposed by Li et al. [19] and Kantchelian et al. [16], but is, to our knowledge, the first method which uses problem space evasion for evaluating and boosting evasion robustness."}, {"heading": "A. The Retraining Framework", "text": "The proposed iterative retraining method is a modified version of the one presented by Li et al. [19], and is schematically shown in Figure 3. The approach starts with the initial classifier, and begins by executing an evasion oracle for each malicious instance in the training data. Successful evasions are then added to the training data, and the classifier is retrained. Next, the evasion oracle is executed again for each malicious instance, and the process repeats, either for a predefined number of iterations, or until convergence. We note that our main novelty claim is not the general retraining concept itself (which is not new), but the approach for using it in the context of problem space attacks, and, more significantly, the empirical evaluation of its effectiveness (which is far from clear given the complexity of problem space attacks, coupled with the random nature of genetic programming used by EvadeML).\nWe made two modifications to the above approach. First, we used only a set of 40 malicious instances which were seeds to EvadeML to generate evasions, to remain consistent with the prior use of EvadeML, reduce running time, and make the experiment more consistent with realistic settings where a large proportion of malicious data is not directly adapting to the classifier. Nevertheless, as shown below, this set of 40 instances was sufficient to generate a model robust to evasions from held out 100 malicious seed PDFs. The second, and more fundamental, modification is to use EvadeML as the oracle for generating actual malicious PDF instances, from which features are subsequently extracted and added to the training data, rather than a feature space evasion model as in prior work.\nA central challenge with the resulting framework is scalability. In particular, generation of evasive instances using EvadeML is a very slow process, particularly as the classifier\nbecomes more robust (and, consequently, more iterations of EvadeML are required to evade it), but also because the use of a sandbox is a major limiting factor. Consequently, even running 10 iterations of the algorithm takes several days, and it is unclear whether so few iterations are sufficient to engender evasion robustness, particularly since retraining with feature space oracles often runs hundreds of iterations before converging. Finally, it may even be the case that perfect robustness to evasions is impossible if one wishes to preserve a sufficiently high performance on the \u201cstatic\u201d data and especially a sufficiently low false positive rate. As our experiments below demonstrate, the procedure is nevertheless extremely effective even with very few iterations."}, {"heading": "B. Experiments", "text": "1) Experiment setup: The dataset involved in our experiment is from the Contagio Archive. 4 We use 5,586 malicious and 4,476 benign PDF files to train SL2013, and another 5,276 malicious and 4,459 benign files as the non-adversarial test dataset. The training and test datasets also contain 500 seeds selected by [37], with 400 in the training data and 100 in the test dataset. These seeds are filtered from 10,980 PDF malware samples and are suitable for evaluation since they are detected with reliable malware signatures by the Cuckoo sandbox [13]. We randomly select 40 seeds from the training data as the retraining seeds and use the 100 seeds in the test data as the test seeds. We empirically set the RBF parameters for training SL2013 as C = 12 and \u03b3 = 0.0025.\nWe set the GP parameters in EvadeML as the same as in the experiments by Xu et al. [37]. The population size in each generation is 48. The maximum number of generations is 20. The mutation rate for each PDF object is 0.1. The fitness threshold of SL2013 is 0. We use the same external benign\n4The malicious PDF files are available at http://contagiodump. blogspot.com/2010/08/malicious-documents-archive-for.html, and the benign files are available at http://contagiodump.blogspot.com/2013/03/ 16800-clean-and-11960-malicious-files.html.\nPDF files as Xu et al. [37], for both retraining and robustness evaluation.\nWe distribute both retraining and adversarial test tasks in two servers (Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz, 18 cores, 71 processors and 64 GB memory running Ubuntu 16.04). For retraining SL2013, we assign each server 20 seeds; each seed is processed by EvadeML to produce the adversarial evasion instances. We then add the 40 examples obtained to the training data, retrain SL2013, and then split the seeds into the two servers in next iteration. In the evaluation phase, we assign each server 50 seeds from the 100 test instances, and each seed is further used to evade SL2013 by using EvadeML.\nThroughout, we evaluate performance in two ways: 1) evaluation of evasion robustness, and 2) traditional evaluation. To evaluate robustness, we compute the proportion of 100 malicious test seed PDFs for which EvadeML successfully evades the classifier; we call this metric evasion robustness. Thus, evasion robustness of 0% means that the classifier is successfully evaded in every instance, while evasion robustness of 100% means that evasion fails every time. Our traditional evaluation metric uses test data of malicious and benign PDFs, where no evasions are attempted. On this data, we compute the ROC (receiver operating characteristic) curve and the corresponding AUC (area under the curve).\n2) Results: Before the target classifier SL2013 was retrained, its robustness was first evaluated by EvadeML. For all the 100 adversarial examples produced by EvadeML, SL2013 could only achieve a 16% evasion robustness.5 This result provides a baseline with which we compare the robustness after retraining SL2013. We conducted an EvadeML test to\n5This result is different from the experiments in [37] which shows a 0% evasion robustness. We found a flaw in the implementation of feature extraction in EvadeML which has been reported to the authors and fixed in our experiments. We also evaluated the robustness of Hidost [32] (the updated version of SL2013) by EvadeML. The result shows a 2% robustness which is even worse than SL2013.\nevaluate the robustness of SL2013 after each iteration of retraining.\nThe experiment retraining SL2013 took approximately six days to execute. The process terminated after 10 iterations as no evasive variants of the 40 retraining seeds could be generated. Our key result is that the retrained classifier obtained by this approach achieves a 96% evasion robustness (that is, EvadeML terminates without successfully finding an evasion in 96% of the instances).\nWe first evaluate the robustness of our systematic retraining approach in terms of evasion robustness and SVM score on the EvadeML test. The results are summarized in Figure 4 and Figure 5. Figure 4 shows the improvement of evasion robustness as a function of the number of iterations. It indicates that by iteratively retraining SL2013, its robustness against evasions in problem space is gradually improved. The SVM scores are shown in Figure 5 with 0 as the classification threshold. Each score is presented as a box plot, with the thick red line as the median, the blue box as the interquartile range (IQR) which shows the 25th and 75th percentiles, the whiskers as the scores within 1.5 IQR from the median, and other scores represented as single points. Both the median score and its corresponding 25th percentile increase with iterations, statistically demonstrating the effectiveness of the proposed approach.\nNext, we evaluate the accuracy of the retrained classifier on the non-adversarial test data. The corresponding ROC curves of the classifier obtained by this and the baseline classifier are presented in Figure 6. The retrained classifier achieves a comparable accuracy (> 99.9% AUC) on non-adversarial data with the baseline classifier, which indicates that the systematic retraining approach maintains the performance of SL2013 on non-adversarial instances while significantly improving its robustness against evasions in problem space.\n3) Discussion: Our experiment results reveal that it is feasible to develop techniques for evasion-robust classification which rely on problem space evasion models. The proposed\nsystematic method achieves nearly 100% accuracy on both non-adversarial instances and adversarial instances with evasion attacks in problem space. Although the proposed approach is effective, it has a major limitation: training time. The first bottleneck is the use of the sandbox to evaluate maliciousness of a PDF sample, which requires multiple virtual machines running simultaneously and consumes memory and other hardware resources. Second, as mentioned earlier, evasions take an increasing number of genetic programming iterations as the robustness of the classifier improves, so that retraining takes longer with successive iterations. Feature space methods for boosting classifier robustness, on the other hand, do not have these bottlenecks, and have the additional advantage of being considerably easier to analyze theoretically and algorithmically. Next, we evaluate the effectiveness of a natural feature space counterpart to our problem space method for boosting classifier robustness to evasion attacks."}, {"heading": "V. LIMITATIONS OF FEATURE SPACE ATTACK MODELS", "text": "We have now answered the first question raised in Section I, by developing a systematic retraining approach based on a problem space adversarial evasion model. Since effectiveness of this approach is significantly tampered by its overhead, we now evaluate the viability of using a feature space counterpart in its place. While a number of such approaches have been proposed, we attempt to draw the most direct comparison by simply replacing the oracle in the retraining method with a feature space attack model."}, {"heading": "A. Methodology: the Feature Space Evasion Model", "text": "There have been a number of evasion models in feature space proposed in prior literature. All involve casting evasion as an optimization problem essentially trading off two considerations: ensuring that the adversarially modified feature vector is classified as benign, and minimizing the total cost of feature modification, where the latter is commonly measured using an lp norm difference between the original malicious instance and the modified feature vector[19], [2]. In typical problem space attacks, including EvadeML, a consideration is not merely to move to the benign side of the classifier decision boundary, but to appear as benign as possible. This naturally translates into the following multi-objective optimization in feature space:\nminimize x Q(x) = f(x) + \u03bbc(xM , x), (1)\nwhere f(x) is the score of a feature vector x, with the actual classifier (such as SVM) g(x) = sgn(f(x)), xM is the malicious seed, x an evasion instance, c(xM , x) the cost of transforming xM into x, and \u03bb a parameter which determines the relative importance of appearing more benign and feature transformation cost. We use the standard weighted lp norm distance between xM and x as the cost function, which is equivalent to weighted l1 norm: c(xM , x) = \u2211 i \u03b1i|xi\u2212xM,i|, since the features are binary. Below we consider two variations of this cost function: first, using uniform weights, with \u03b1i = 1 for all features i, and second, using non-uniform weights (which we term weighted distance (WD) below).\nAs the optimization problem in Equation (1) is non-convex, we use a common local search method, Coordinate Greedy, to compute a local optimum. In this method, we optimize one randomly chosen coordinate of the feature vector at a time, until a local optimum is reached. To improve the quality of the resulting solution, we repeat this process from several random starting points."}, {"heading": "B. Experiments", "text": "1) Experiment setup: We conduct experiments to evaluate the effectiveness of the retraining approach to boost evasion robustness which uses a feature space evasion model using the same setup with Section IV, using EvadeML as before for evaluation. We first consider the setting with uniform weights, and evaluate \u03bb = 0.05 and \u03bb = 0.005.\nIn addition, we consider a natural attack model where feature weights are non-uniform, setting \u03bb = 1 in this case without loss of generality (since \u03b1is are simply rescaled). While it is difficult to find a principled means of assigning\nheterogeneous feature weights based on training data alone, we distinguish high- and low-weight features depending on how frequently the feature appears among malicious and benign instances. If a feature is common in malicious, but rare in benign instances, we assume that changing this feature is associated with a high cost due to the possible modification of the malicious functionality; otherwise, we assume that changing this feature is associated with a low cost.\nFormally, let nm and nb be the counts of malicious and benign instances, respectively, while nim and n i b are the counts of appears of feature i in malicious and benign instances. If nim nm \u2265 n i b nb , we set \u03b1i = 0.5, and set it to 0.05 otherwise.\n2) Results: The results are summarized in Figure 7 and Figure 8. Compared to the SL2013 baseline, retraining with feature space evasion models boosts evasion robustness from 16% to 60%, but only achieves a 10% evasion robustness when weighted distance (denoted as WD in Figures 7, 8 and 9) is employed. The SVM scores increase as a statistical validation of the enhancement of robustness. However, the robustness of the resulting classifier is far below that resulting from systematic retraining with a problem space oracle, which achieves 96% evasion robustness. A deeper look at the statistics in Figure 8 is also revealing: while a smaller \u03bb (a more aggressive adversary) yields a higher evasion robustness, the median score is only slightly above the 0 threshold. Consequently, the resulting classifier appears quite vulnerable, even though most evasion instances do clear the maliciousness threshold. The additional cause for concern is that the smaller value of \u03bb, while resulting in a more robust classifier, induces a significantly slower training process, with training time approaching that for systematic problem space retraining due to the large number of iterations required for convergence.\nEvaluating the quality of the synthetically retrained classifier (using a feature space oracle) on non-adversarial test data (i.e., not involving deliberate evasions) in terms of the ROC curves, we can see that robustness boosting does not much degrade its performance, with AUC remaining above 99%.\nHowever, we do see slight degradation for small values of the false positive rates, compared both to the baseline (non-robust) classifier, as well as the classifier obtained through systematic problem space retraining. This is an issue of some concern, as this is the region of most practical import: real systems would necessarily operate at the low false positive rate level.\n3) Discussion: While we observed some value of using feature space methods for boosting classifier robustness to evasion attacks, it falls far short of the robustness we know we can achieve by using a problem space oracle. Despite the advantages\u2014in some cases substantial\u2014in terms of running time, our results suggest that relying on feature space models may not be satisfactory in practice. There are two general reasons for the observed gap. First, synthetically generated adversarial instances may in actuality not preserve malicious functionality, since they need not abide by the system-level attack constraints. This would introduce noise and potentially\nbias into the retraining process. Second, realistic adversarial instances may not be generated, as they do not possess a sufficiently high objective value according to Equation (1), in part because better solutions according to this evasion model may not abide by realistic attack constraints.\nIn spite of the above limitations, the feature space methods have tremendous appeal, due to their relative speed and amenability to analysis. In the remainder of the paper, we present and evaluate a surprisingly simple \u201cfix\u201d to enable feature space model not merely to become competitive with problem space approaches, but to actually exhibit better robustness."}, {"heading": "VI. EVASION-ROBUST CLASSIFICATION WITH CONSERVED FEATURES", "text": "So far, we learned that we can successfully \u201crobustify\u201d a classifier against evasions with systematic iterative retraining when attacks are modeled in problem space, but not when attacks are abstracted in feature space. Next we propose a simple idea for bridging the gap between problem and feature space: explicitly accounting for a subset of features which are conserved during evasions. More precisely, conserved features are those which are invariant under (are unaffected by) evasion attacks.\nNext, we present three surprising findings. First, conserved features do exist (for EvadeML), and can be effectively identified. Previous accounts have generally been skeptical of the ability to find, or use, such attack invariants for defense. Second, we show that a classifier using only the conserved features is (a) completely robust to the EvadeML attack (essentially by construction), and (b) quite effective on test data not involving evasion attacks. We also observe that conserved features cannot be recovered using standard feature reduction, and feature reduction methods do not lead to robust classifiers. The reason is that conservation is connected to evasion attacks, rather than statistical properties of non-evasion data; for example, features which are strongly correlated with malicious behavior are often a consequence of attacker \u201claziness\u201d (for example, size of non-malicious content of a PDF), and are actually easy for attackers to modify. Third, we demonstrate that the limitations of feature-space robust classification approaches can be essentially eliminated by incorporating conserved features as attack invariants in the evasion model, and the resulting classifier outperforms the one using only conserved features on non-evasion data. Finally, we present a systematic approach for identifying the conserved features."}, {"heading": "A. Conserved Features", "text": "In our case study, the target classifier (SL2013) employs structural paths as features to discriminate between malicious and benign PDFs. As the shellcode which triggers malicious functionality is embedded in certain PDF objects, those corresponding structural paths should be conserved in each variant crafted from the same malicious seed. In the example of Figure 1, the PDF file has 7 structural paths, among which /OpenActionJS and /OpenActionS are conserved features, as the JavaScript code is placed in the corresponding objects of these two structural paths. On the other hand, there exist features that are irrelevant to malicious functionality. For\nexample, the structural path /Type is unessential to preserve malicious behaviors, and we do not expect it to be conserved.\nAs discussed by Xu et al. [37], three fundamental operations can be used to craft adversarial examples, which directly modify PDF objects and the corresponding structural paths: insertion, deletion, and swap. The insertion operation does not change malicious functionality as it only inserts an external object after a target object. In contrast, the deletion and swap operations may impact malicious functionality by removing or replacing structural paths corresponding to it. Hence, in our case study of SL2013, conserved features are structural paths that would be neither deleted nor replaced with an external object while preserving malicious functionality."}, {"heading": "B. Classifying with Conserved Features", "text": "We begin by exploring the effectiveness of using conserved features for classification; in Section VI-D we describe a systematic approach for identifying these.\nOur experiments use a conserved feature set based on EvadeML and the 40 malicious seeds employed in training procedure for robustness boosting in Sections IV and V. Our conserved feature set contains only 8 features out of a total of 6,087. These are shown in Table I.\nThis conserved set raises two key questions which we answer: 1) are these sufficient to make a classifier robust to evasions, and 2) do these effectively discriminate between benign and malicious instances? To address these questions, we trained a classifier which uses only the 8 conserved features (FR8 henceforth), and evaluate both its evasion robustness, and effectiveness on non-evasion test data. As a comparison, we also learn a linear SVM classifier with l1 regularization where we empirically adjust the SVM parameter C to perform feature reduction until the number of the features is also 8. We use both EvadeML and non-adversarial data set to evaluate these two classifiers. The Baseline is, as before, the original target classifier.\nAs shown in Figure 10, the classifier using only the 8 conserved features appears resistant to evasion attacks by EvadeML, attaining 100% evasion robustness. In contrast, the linear SVM with sparse (l1) regularization yielding only 8 most statistically important features is easily evaded. Of some interest is an observation that sparse linear SVM is actually slightly more robust to evasion than SL2013 (20% evasion robustness, in contrast to 16% for SL2013). Figure 11 compares SVM scores of the three classifiers under EvadeML test. It can be seen that FR8 achieves the highest median which is even comparable to systematic retraining approach shown in Figure 5.\nFigure 12 shows performance of the three classifiers on non-evasive test data. Surprisingly enough, linear SVM using only 8 features (out of >6000) yields better than 99% AUC, approaching the performance of the baseline classifier. More significantly, and surprisingly, even FR8, which is robust to evasions, achieves AUC just under 95%!\nNevertheless, the FR8 classifier can be seen to perform relatively poorly in the region of the ROC curve where the false positive rate is low, which in practice is the most consequential part of it. Next, we consider whether the feature space retraining procedure can be fixed by incorporating conserved features into the evasion model."}, {"heading": "C. Feature Space Retraining with Conserved Features", "text": "1) Modified Model: As discussed above, the feature space evasion model in Equation (1) fails in presenting representative evasion attacks in problem space, and does not facilitate\nsufficient boosting of evasion robustness. Since conserved features represent malicious functionality in feature space, we offer a natural modification of the model in Equation (1) where we impose an additional constraint that conserved features are preserved in evasive instances. Formally, the new optimization problem is shown in Equation (2), where S is the set of conserved features:\nminimize x Q(x) = f(x) + \u03bbc(xM , x), subject to xi = xM,i, \u2200i \u2208 S. (2)\nOther than this modification, we use the same coordinate greedy algorithm with random restarts as before to compute an adversarial evasion in feature space. We adopt the evasion model in Equation (2) to retrain the target classifier using the same procedure as in Sections IV and V.\nSince we use uniform conserved features for each malicious seed, the malicious functionality of some PDFs may still be removed as a result. To mitigate such degradation, we additionally generate mimicry instances in feature space and add these instances into the training data after the termination of retraining with conserved features. We then retrain the classifier again. In our case, we generate such instances by combining feature vectors of malicious and benign PDFs. For each benign file in the training data, we randomly select a malicious PDF, then copy all the feature of the malicious PDF to the benign one. Therefore, the resulting feature vector has both conserved and non-conserved features, by which its malicious functionality is preserved but it appears more benign compared to the malicious PDF.\n2) Experiments: We now evaluate the robustness and effectiveness of the feature space retraining approach, which uses conserved features and mimicry instances. We use the same setup with Section V and employ EvadeML to evaluate the classifiers. We set the retraining parameter \u03bb to be 0.005.\nThe iterative retraining process converges after 220 iterations, producing 4,461 adversarial instances. We term the resulting classifier CF. Afterward, we produce 4,496 mimicry\ninstances and add them to the training data, and then retrain the classifier again. The resulting classifier is termed Mimicry-CF.\nWe first evaluate the evasion robustness of the SL2013 baseline classifier and other classifiers obtained by different retraining. The results are summarized in Figure 13 and Figure 14. Figure 13 shows that CF now significantly improves evasion robustness of the baseline classifier, with evasion robustness rising from 16% to 87%. By adding mimicry instances to the training data and retraining CF again, evasion robustness is further improved to 94%, which is comparable with the systematic retraining approach in Section IV. Figure 13 also shows that the evasion robustness is significantly boosted even we retrain SL2013 with the mimicry instances, which is a naive approach to generate adversarial instances in feature space. However, a deeper look at the statistics in Figure 14 indicates that the resulting classifier retrained with mimicry instances is still quite vulnerable, as the median score is only slightly\nabove the 0 threshold. In contrast, the median scores of CF and Mimicry-CF rise to slightly less than 1.0, with both the 25th and 75th percentiles above the threshold. These results demonstrate that by leveraging conserved features, the feature space evasion models are now quite effective as a means to boost evasion robustness of a PDF malware classifier.\nIn Figure 15 we evaluate the quality of these classifiers on non-adversarial test data in terms of ROC curves. We can observe that all variants of CF classifiers are now comparable in terms of performance on non-adversarial instances to the baseline classifier, as well as the one obtained using problem space evasions, with AUC well above 99%.\nD. Identifying Conserved Features\nHaving demonstrated the effectiveness of conserved features in bridging the gap between problem space and feature space evasion models, we now describe a systematic procedure for obtaining these.\nThe key to identifying the conserved features of a malicious PDF is to discriminate them from non-conserved ones. Since merely applying statistical approaches on training data is insufficient to discriminate between these two classes of features, as demonstrated above, we need a qualitatively different approach which relies on the nature of evasions (as implemented in EvadeML) and the sandbox (which determines whether malicious functionality is preserved) to identify features that are conserved.\nWe use a modified version of pdfrw [22]6 to parse the objects of PDF file with a logic structure as shown on the right-hand side of Figure 1 and repack objects to produce a new PDF file. We use Cuckoo [13] as the sandbox to evaluate malicious functionality. In the discussion below, we define xi to be the malicious file, Si the conserved feature set of xi, and Oi the set of its non-conserved features. Initially, Si = Oi = \u2205.\nAt the high level, our first step is to sequentially delete each object of a malicious file and eliminate non-conserved\n6The modified version is available at https://github.com/mzweilin/pdfrw.\nfeatures by evaluating the existence of a malware signature in a sandbox for each resulting PDF, which provides a preliminary set of conserved features. Then, we replace the object of each corresponding structural path in the resulting preliminary set with an external benign object and assess the corresponding functionality, which allows us to further prune non-conserved features. Next, we describe these procedures in detail.\n1) Structural Path Deletion: In the first step, we filter out non-conserved features by deleting each object and its corresponding structural path, and then checking whether this eliminates malicious functionality (and should therefore be conserved). First, we obtain all the structural paths (objects) by parsing a PDF file. These objects are organized as a treetopology and are sequentially deleted. Each time an object is removed, we produce a resulting PDF file by repacking the remaining objects. Then, we employ the sandbox to detect malicious functionality of the PDF after the object deletion. If any malware signature is captured, the corresponding structural path of the object is deleted as a non-conserved feature, and added to Oi. On the other hand, if no malware signature is detected, the corresponding feature is added in Si as a possibly conserved feature.\nOne important challenge in this process is that features are not necessarily independent. Thus, in addition to identifying Si and Oi, we explore interdependence between features by deleting objects. As the logic structure of a PDF file is with a tree-topology, the presence of some structural path depends on the presence of other structural paths whose object refers to the object of the prior one. For example in some PDFs, the presence of /OpenAction/JS and /OpenAction/S depend on /OpenAction. If the object OpenAction is deleted, so as its structural path /OpenAction, then both /OpenAction/JS and /OpenAction/S are deleted as well. We call /OpenAction/JS and /OpenAction/S as the dependents of /OpenAction. For any feature j of xi, the set of its dependent features is denoted by Dji .\nNote that for a given structural path (feature), there could be multiple corresponding PDF objects. In such case, these objects are deleted simultaneously, so as the corresponding feature value is shifted from 1 to 0.\n2) Structural Path Replacement: In the second step, we subtract the remaining non-conserved features in the preliminary Si and move them to Oi. Similar to the prior step, we first obtain all the structural paths and objects of the malicious PDF file. Then for each object of the PDF that is in Si, we replace it with an external object from a benign PDF file and produce the resulting PDF, which is further evaluated in the sandbox. If the sandbox detects any malware signature, then the corresponding structural path of the object replaced is moved from Si to Oi. Otherwise, the structural path is a conserved feature since both deletion and replacement of the corresponding object removes the malicious functionality of the PDF file. Note that in the case of multiple corresponding and identical objects of a structural path, all of these objects are replaced simultaneously.\nAfter structural path deletion and replacement, for each malicious PDF file xi, we can get its conserved feature set Si, non-conserved feature set Oi, and dependent feature set Dj for any feature j \u2208 Si \u222aOi, which could be further leveraged\nAlgorithm 1 Forward Elimination for uniform conserved feature set. Input:\nThe set of conserved features for xi(i \u2208 [1, n]), Si; The set of non-conserved features for xi(i \u2208 [1, n]), Oi; The set of dependent features for j \u2208 Si \u222a Oi , Dji\nOutput: The uniform conserved feature set for {x1, x2, ..., xn}, S;\n1: S \u2190 \u22c3n\ni=1 Si; 2: S \u2032 \u2190 S; 3: Q \u2190 \u2205; 4: Dj = \u22c3n i=1D j i 5: for each j \u2208 S \u2032 do 6: if j /\u2208 Q then 7: if \u2211n i=1 1j\u2208Oi \u2265 \u03b2 \u00b7 \u2211n i=1 1j\u2208Si then 8: S \u2190 S \\ ({j} \u222a Dj); 9: Q \u2190 Q\u222a ({j} \u222a Dj);\n10: end if 11: end if 12: end for 13: return S;\nto design evasion-robust classifiers.\n3) Uniform conserved feature set: The systematic approach discussed above provides a conserved feature set for each malicious seed to retrain a classifier. Our goal, however, is to identify a single set of conserved features which is independent of the specific malicious PDF seed file. We now develop an approach for transforming a collection of Si, Oi, and Dji for a set of malicious seeds i into a uniform set of conserved features.\nObtaining a uniform set of conserved features faces two challenges: 1) minimizing conflicts among different conserved features, as a conserved feature for one malicious instance could be a non-conserved feature for another, and 2) abiding by feature interdependence if a conserved feature should be further eliminated.\nTo address these challenges, we propose a Forward Elimination algorithm to compute the uniform conserved feature set for a set of malicious seeds {x1, x2, ..., xn}, given the conserved feature sets, non-conserved feature sets and dependent sets for each seed. As Algorithm 1 shows, we first obtain a union of the conserved feature sets. Then, we explore the contradiction of each feature in the union with the others, by comparing the total number of the feature being selected as a non-conserved feature and conserved feature. If the former one is greater than \u03b2 times the latter one, then this feature, together with its dependents, are eliminated from the union. Otherwise, the feature is added to the uniform feature set. We use \u03b2 as a parameter to adjust the balance between conserved and nonconserved features. Typically, \u03b2 > 1 as we are inclined to preserve malicious functionality associated with a conserved feature, even it could be a non-conserved feature of another PDF file. In particular, we set \u03b2 = 3 in our experiments."}, {"heading": "VII. DISCUSSION", "text": "The subject of adversarial classifier evasion has received considerable attention in the literature, both in the cybersecurity and machine learning communities. The former has been primarily interested in practical attacks in machine learning systems\u2014that is, evasion attacks in problem space\u2014 whereas fundamental algorithmic developments in the machine learning community focused on modeling attacks as feature space optimization problems. The first central takeaway of our investigation is that there is a significant gap between the two approaches. To put it another way, current feature space models fall far short of providing effective means to boost classifier robustness to realistic evasion attacks. Our second major message is that this gap can be effectively bridged if we can successfully identify conserved features, or features which capture invariant properties of malicious files. Identifying such a small set of invariants seems like an elusive holy grail, but we demonstrate that these can in fact be successfully identified using a systematic automated procedure.\nThe idea of focusing on a small subset of conserved features to discriminate between benign and malicious instances may appear superficially just another form of statistical regularization, common in machine learning. We emphasize that this is not so: rather, feature conservation is a qualitatively distinct notion. Conceptually, statistical regularization aims to identify a small set of features which are statistically indicative of malicious or benign status. This may pick up entirely spurious relationships, such as relative simplicity or regularity of malicious file structure which are entirely a byproduct of typical behavior of malware authors (e.g., laziness), rather than a fundamental limitation on malware structure. Conserved features, on the other hand, capture fundamental limitations on what changes can be introduced into a malware file without eliminating malicious functionality. This is a function not of statistical correlations, but of actual evasion attacks. For example in the training data of SL2013, /Names/JavaScript/Names/JS and /Names/JavaScript/Names/S coexist in every PDF file that has either. However, the former is a conserved feature, while the latter is not. Indeed, /Names/JavaScript/Names/S is deleted during some of the EvadeML attacks, which results in PDFs that appear benign, but successfully preserve malicious functionality.\nOur study was enabled by a recent advance in automated evasion attacks in problem space. However, it is a case study, focused on a specific target PDF malware classifier (SL2013), and utilizing one automated attack generator (EvadeML). To be sure, EvadeML is quite powerful, and likely subsumes prior evasion attacks in problem space, such as mimicry attacks (e.g. [30]). Nevertheless, a more extensive investigation of other classifiers, malware, and attack models is needed to fully understand the scope and generality of our observations. In addition, our study suggests new directions in algorithmic research on adversarial machine learning, such as investigating alternative models of adversarial behavior which incorporate feature conservation, more scalable approaches for boosting classifier robustness, and more general and scalable algorithms for identifying conserved features."}], "references": [{"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "in ACM Asia Conference Computer and Communications Security,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. Srndic", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "European Conference on Machine Learning and Knowledge Discovery in Databases, 2013, pp. 387\u2013402.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 4, pp. 984\u2013996, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011, pp. 547\u2013555.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Static prediction games for adversarial learning problems", "author": ["\u2014\u2014"], "venue": "Journal of Machine Learning Research, no. 13, pp. 2617\u20132654, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Detection and analysis of driveby-download attacks and malicious javascript code", "author": ["M. Cova", "C. Kruegel", "G. Vigna"], "venue": "International Conference on World Wide Web, 2010, pp. 281\u2013290.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004, pp. 99\u2013108.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A vaccine based on conserved regions could prove radical", "author": ["K. Dorans"], "venue": "Nature Medicine, vol. 351, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Identification of novel conserved functional motifs across most influenza a viral strains", "author": ["M. ElHefnawi", "O. AlAidi", "N. Mohamed", "M. Kamar", "I. El-Azab", "S. Zada", "R. Siam"], "venue": "Virology Journal, vol. 8, no. 44, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "USENIX Security Symposium, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel"], "venue": "European Symposium on Research in Computer Security, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Cuckoo sandbox: A malware analysis system", "author": ["C. Guarnieri", "A. Tanasi", "J. Bremer", "M. Schloesser"], "venue": "2012, http://www.cuckoosandbox.org/.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Strategic classification", "author": ["M. Hardt", "N. Megiddo", "C. Papadimitriou", "M. Wootters"], "venue": "ACM Conference on Innovations in Theoretical Computer Science, 2016, pp. 111\u2013122.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": "International Conference on Learning Representations, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Evasion and hardening of tree ensemble classifiers", "author": ["A. Kantchelian", "J.D. Tygar", "A.D. Joseph"], "venue": "International Conference on Machine Learning, 2016, pp. 2387\u20132396.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I.J. Goodfellow", "S. Bengio"], "venue": "2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Feature cross-substitution in adversarial classification", "author": ["B. Li", "Y. Vorobeychik"], "venue": "Neural Information Processing Systems, 2014, pp. 2087\u20132095.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A general retraining framework for scalable adversarial classification", "author": ["B. Li", "Y. Vorobeychik", "X. Chen"], "venue": "2016, arXiv preprint.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, 2005, pp. 641\u2013647.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Looking at the bag is not enough to find the bomb: an evasion of structural methods for malicious PDF files detection", "author": ["D. Maiorca", "I. Corona", "G. Giacinto"], "venue": "ACM Asia Conference on Computer and Communications Security, 2013, pp. 119\u2013130.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Pdfrw: A pure python library that reads and writes pdfs", "author": ["P. Maupin"], "venue": "https://github.com/pmaupin/pdfrw, 2017, accessed: 2017-05-18.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2017}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["S. Mei", "X. Zhu"], "venue": "AAAI Conference on Artificial Intelligence, 2015, pp. 2871\u20132877.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-optimal evasion of convex-inducing classifiers", "author": ["B. Nelson", "B.I.P. Rubinstein", "L. Huang", "A.D. Joseph", "S. Lau", "S.J. Lee", "S. Rao", "A. Tran", "J.D. Tygar"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 549\u2013556.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifier evasion: Models and open problems", "author": ["B. Nelson", "B.I.P. Rubinstein", "L. Huang", "A.D. Joseph", "J.D. Tygar"], "venue": "Privacy and Security Issues in Data Mining and Machine Learning - International ECML/PKDD Workshop, 2010, pp. 92\u201398.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J. Tygar"], "venue": "Journal of Machine Learning Research, pp. 1293\u20131332, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P.D. McDaniel", "I.J. Goodfellow"], "venue": "2016, arxiv preprint.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Secure kernel machines against evasion attacks", "author": ["P. Russu", "A. Demontis", "B. Biggio", "G. Fumera", "F. Roli"], "venue": "ACM Workshop on Artificial Intelligence and Security, 2016, pp. 59\u201369.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter"], "venue": "ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1528\u20131540.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["N. \u0160rndic", "P. Laskov"], "venue": "2014 IEEE Symposium on Security and Privacy, 2014, pp. 197\u2013211.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection of malicious PDF files based on hierarchical document structure", "author": ["N. Srndic", "P. Laskov"], "venue": "Network and Distributed System Security Symposium, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidost: a static machine-learning-based detector of malicious files", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "EURASIP Journal on Information Security, vol. 2016, no. 1, p. 22, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Convex learning with invariances", "author": ["C.H. Teo", "A. Globerson", "S. Roweis", "A.J. Smola"], "venue": "Neural Information Processing Systems, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal randomized classification in adversarial settings", "author": ["Y. Vorobeychik", "B. Li"], "venue": "International Conference on Autonomous Agents and Multiagent Systems, 2014, pp. 485\u2013492.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust regression and lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Neural Information Processing Systems, 2008, pp. 1801\u20131808.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Robustness and regularization of support vector machines", "author": ["\u2014\u2014"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1485\u20131510, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatically evading classifiers: A case study on PDF malware classifiers", "author": ["W. Xu", "Y. Qi", "D. Evans"], "venue": "Network and Distributed System Security Symposium, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial feature selection against evasion attacks", "author": ["F. Zhang", "P. Chan", "B. Biggio", "D. Yeung", "F. Roli"], "venue": "IEEE Transactions on Cybernetics, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial support vector machine learning", "author": ["Y. Zhou", "M. Kantarcioglu", "B.M. Thuraisingham", "B. Xi"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2012, pp. 1059\u2013 1067. 14", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "Most of the traditional malware detection approaches are based on dynamic analysis, which incurs significant computational overhead and latency since such procedures depend on execution of shellcodes [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 30, "context": "State-of-the-art approaches of this kind enable static detection of malicious entities, such as malware, efficiently and with accuracy often exceeding 99% [31], [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 29, "context": "State-of-the-art approaches of this kind enable static detection of malicious entities, such as malware, efficiently and with accuracy often exceeding 99% [31], [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 130, "endOffset": 133}, {"referenceID": 19, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 29, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 147, "endOffset": 151}, {"referenceID": 36, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "Recent research has shown that machine learning approaches, and especially classifier learning, are vulnerable to evasion attacks [7], [20], [31], [30], [37], [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 29, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 36, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "The first involves attacks in problem space: systematic approaches for designing evasions through modifying actual malicious instances [30], [37], [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "While not guaranteed to exactly simulate behavior of real attackers, these faithfully replicate the constraints faced by real attackers in designing evasions, such as the inability to effect arbitrary changes in the feature space, and the requirement that malicious functionality of the instance is preserved (typically evaluated using a sandbox, such as WEPAWET [6] and the Cuckoo sandbox [13]).", "startOffset": 363, "endOffset": 366}, {"referenceID": 12, "context": "While not guaranteed to exactly simulate behavior of real attackers, these faithfully replicate the constraints faced by real attackers in designing evasions, such as the inability to effect arbitrary changes in the feature space, and the requirement that malicious functionality of the instance is preserved (typically evaluated using a sandbox, such as WEPAWET [6] and the Cuckoo sandbox [13]).", "startOffset": 390, "endOffset": 394}, {"referenceID": 6, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 194, "endOffset": 197}, {"referenceID": 19, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 205, "endOffset": 209}, {"referenceID": 24, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 211, "endOffset": 215}, {"referenceID": 1, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 217, "endOffset": 220}, {"referenceID": 15, "context": "The second paradigm models attacks as modifications directly in feature space, imposing a modification cost typically captured as (weighted) norm difference from the original malicious instance [7], [20], [24], [25], [2], [16].", "startOffset": 222, "endOffset": 226}, {"referenceID": 34, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 22, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 229, "endOffset": 232}, {"referenceID": 17, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 234, "endOffset": 238}, {"referenceID": 38, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 240, "endOffset": 244}, {"referenceID": 4, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 246, "endOffset": 249}, {"referenceID": 18, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 299, "endOffset": 303}, {"referenceID": 14, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 305, "endOffset": 309}, {"referenceID": 15, "context": "These range from leveraging traditional machine learning techniques, such as regularization [35], [36], [23], [28], to game theoretic approaches in which the defender chooses a classifier that the attacker attempts to evade [7], [4], [18], [39], [5], to retraining with adversarial evasion examples [19], [15], [16].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "1A subset of methods [11], [15] focused on adversarial deep neural network learning for vision tasks consider modified pixels in images, but these are also the features of the learning algorithms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "1A subset of methods [11], [15] focused on adversarial deep neural network learning for vision tasks consider modified pixels in images, but these are also the features of the learning algorithms.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "Moreover, these tend to ignore realistic constraints on attacker, such as modifications of physical environments which are subsequently translated into images, although these have been explored in considering attacks independently of defensive techniques [29].", "startOffset": 255, "endOffset": 259}, {"referenceID": 30, "context": "ing tool for detecting malicious PDFs [31], and EvadeML, an automated problem space method for PDF malware classifier evasion [37].", "startOffset": 38, "endOffset": 42}, {"referenceID": 36, "context": "ing tool for detecting malicious PDFs [31], and EvadeML, an automated problem space method for PDF malware classifier evasion [37].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "These conserved features echo an analogous notion in epidemiology, where, for example, characterization of conserved regions of viral proteins is an important step toward designing effective vaccines [8], [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 8, "context": "These conserved features echo an analogous notion in epidemiology, where, for example, characterization of conserved regions of viral proteins is an important step toward designing effective vaccines [8], [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "The existence of these is especially surprising in light of past work on adversarial evasion of machine learning in cyber security, which has been duly skeptical that such problem invariance exists, or could successfully be leveraged [10].", "startOffset": 234, "endOffset": 238}, {"referenceID": 31, "context": "2An updated version of this classifier [32] has made changes to the feature space.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "[10], who developed a polymorphic blending attack on anomalybased intrusion detection systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "\u0160rndic and Lasov [30] present a case study of an evasion attack on a state of the art PDF malware classifier, PDFRate.", "startOffset": 17, "endOffset": 21}, {"referenceID": 36, "context": "[37] propose EvadeML, a fully problem space attack on PDF malware classifiers which generates evasion instances by using genetic programming to modify PDF source directly, using a sandbox to ensure that malicious functionality is preserved.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 26, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "In addition to malware evasion attacks, a series of efforts explore evasion in the context of image classification by deep neural networks [11], [15], [27], [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 28, "context": "Several efforts explored the impact of indirect modification when an adversarial image must be printed, showing that effectiveness of such techniques can nevertheless be preserved [29], [17].", "startOffset": 180, "endOffset": 184}, {"referenceID": 16, "context": "Several efforts explored the impact of indirect modification when an adversarial image must be printed, showing that effectiveness of such techniques can nevertheless be preserved [29], [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 28, "context": "Recently, an approach for printing specifically designed glass frames had been shown to mislead vision-based biometric systems to either mistakenly grant authorization, or enable evasion of face recognition techniques [29].", "startOffset": 218, "endOffset": 222}, {"referenceID": 6, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 220, "endOffset": 223}, {"referenceID": 19, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 225, "endOffset": 229}, {"referenceID": 0, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 231, "endOffset": 234}, {"referenceID": 15, "context": "b) Feature Space Methods for Classifier Evasion: In addition to classifier evasion methods which change the actual malicious instances, a number of techniques have sprouted for evasion models acting directly on features [7], [20], [1], [16].", "startOffset": 236, "endOffset": 240}, {"referenceID": 6, "context": "[7] present one of the earliest such models as a part of a robust learning approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "An algorithmic investigation of the feature space evasion problem\u2014modeling it formally as minimizing the cost of changing features subject to a constraint that the resulting instance is classified as benign\u2014was initiated by Lowd and Meek [20].", "startOffset": 238, "endOffset": 242}, {"referenceID": 1, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 239, "endOffset": 242}, {"referenceID": 13, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 244, "endOffset": 248}, {"referenceID": 37, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 250, "endOffset": 254}, {"referenceID": 15, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 256, "endOffset": 260}, {"referenceID": 17, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 262, "endOffset": 266}, {"referenceID": 18, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 268, "endOffset": 272}, {"referenceID": 25, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 274, "endOffset": 278}, {"referenceID": 33, "context": "A series of methods follow their framework, but consider more general classes of classifiers, introducing a constraint on the evasion cost, and explicitly trading off the degree to which an instance appears as benign and evasion cost [2], [3], [14], [38], [16], [18], [19], [26], [34].", "startOffset": 280, "endOffset": 284}, {"referenceID": 6, "context": "[7] presented the first approach for evasion-robust classification, making use of a model in which the attacker aims to transform feature vectors into benign instances in response to the", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "A series of approaches formulate robust classification as minimizing maximum loss, where maximization is attributed to the evading attacker aiming to maximize the learner\u2019s loss through feature space transformations [33], [39].", "startOffset": 216, "endOffset": 220}, {"referenceID": 38, "context": "A series of approaches formulate robust classification as minimizing maximum loss, where maximization is attributed to the evading attacker aiming to maximize the learner\u2019s loss through feature space transformations [33], [39].", "startOffset": 222, "endOffset": 226}, {"referenceID": 3, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 222, "endOffset": 225}, {"referenceID": 17, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 227, "endOffset": 231}, {"referenceID": 15, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 233, "endOffset": 237}, {"referenceID": 27, "context": "Instead, these consider the interaction as a non-zero-sum game, either played simultaneously between the learner and the attacker, or a Stackelberg game, in which the learner is the leader, while the attacker the follower [4], [18], [16], [28].", "startOffset": 239, "endOffset": 243}, {"referenceID": 10, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 15, "context": "In the deep learning literature involving adversarial manipulations of images, somewhat ad hoc procedures for retraining the classifier to boost its robustness have been proposed [11], [15], and these have been adapted to other classification models, such as decision tree classifiers [16].", "startOffset": 285, "endOffset": 289}, {"referenceID": 18, "context": "Recently, a systematic iterative retraining procedure had been proposed which leverages general-purpose adversarial evasion models, and offers a theoretical connection to an underlying Stackelberg game played between the learner and the evading adversary [19].", "startOffset": 255, "endOffset": 259}, {"referenceID": 30, "context": "This section provides background on PDF document structure, the target PDF malware classifier (SL2013) [31], and the automated evasion approach, EvadeML [37], we use to evaluate evasion robustness of classifiers.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "This section provides background on PDF document structure, the target PDF malware classifier (SL2013) [31], and the automated evasion approach, EvadeML [37], we use to evaluate evasion robustness of classifiers.", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "2: Classifier evasion with genetic programming [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Several PDF malware classifiers have been proposed [6], [31].", "startOffset": 51, "endOffset": 54}, {"referenceID": 30, "context": "Several PDF malware classifiers have been proposed [6], [31].", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "For our study, we selected SL2013 [31].", "startOffset": 34, "endOffset": 38}, {"referenceID": 31, "context": "While it has since undergone a revision [32], the SL2013 was the version evaluated by the EvadeML tool, and the pair provides a natural evaluation framework for our purposes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "To evaluate the robustness of a PDF classifier against adversarial evasion attacks, we adopt EvadeML [37], a systematic and automated method to craft evasion instances of PDF malware in problem space.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "After the population is initialized, each variant is assessed by the Cuckoo sandbox [13] and the target classifier to evaluate its fitness.", "startOffset": 84, "endOffset": 88}, {"referenceID": 36, "context": "EvadeML was used to evade SL2013 in [37].", "startOffset": 36, "endOffset": 40}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In this case, features can no longer be modified directly [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 36, "context": "[37] demonstrate that although SL2013 was designed specifically to be resistant to evasion attacks, it can be successfully evaded.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] and Kantchelian et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16], but is, to our knowledge, the first method which uses problem space evasion for evaluating and boosting evasion robustness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], and is schematically shown in Figure 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The training and test datasets also contain 500 seeds selected by [37], with 400 in the training data and 100 in the test dataset.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "These seeds are filtered from 10,980 PDF malware samples and are suitable for evaluation since they are detected with reliable malware signatures by the Cuckoo sandbox [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37], for both retraining and robustness evaluation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "5This result is different from the experiments in [37] which shows a 0% evasion robustness.", "startOffset": 50, "endOffset": 54}, {"referenceID": 31, "context": "We also evaluated the robustness of Hidost [32] (the updated version of SL2013) by EvadeML.", "startOffset": 43, "endOffset": 47}, {"referenceID": 36, "context": "5: SVM scores of systematic retraining as a function of iterations under EvadeML [37] test.", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "All involve casting evasion as an optimization problem essentially trading off two considerations: ensuring that the adversarially modified feature vector is classified as benign, and minimizing the total cost of feature modification, where the latter is commonly measured using an lp norm difference between the original malicious instance and the modified feature vector[19], [2].", "startOffset": 372, "endOffset": 376}, {"referenceID": 1, "context": "All involve casting evasion as an optimization problem essentially trading off two considerations: ensuring that the adversarially modified feature vector is classified as benign, and minimizing the total cost of feature modification, where the latter is commonly measured using an lp norm difference between the original malicious instance and the modified feature vector[19], [2].", "startOffset": 378, "endOffset": 381}, {"referenceID": 36, "context": "8: SVM scores of the baseline and synthetic retraining as a function of \u03bb under EvadeML [37] test.", "startOffset": 88, "endOffset": 92}, {"referenceID": 36, "context": "[37], three fundamental operations can be used to craft adversarial examples, which directly modify PDF objects and the corresponding structural paths: insertion, deletion, and swap.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We use a modified version of pdfrw [22]6 to parse the objects of PDF file with a logic structure as shown on the right-hand side of Figure 1 and repack objects to produce a new PDF file.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "We use Cuckoo [13] as the sandbox to evaluate malicious functionality.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "[30]).", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Machine learning is widely used in security applications, particularly in the form of statistical classification aimed at distinguishing benign from malicious entities. Recent research has shown that such classifiers are often vulnerable to evasion attacks, whereby adversaries change behavior to be categorized as benign while preserving malicious functionality. Research into evasion attacks has followed two paradigms: attacks in problem space, where the actual malicious instance is modified, and attacks in feature space, where the attack is abstracted into modifying numerical features of an instance to evade a classifier. In contrast, research into designing evasion-robust classifiers generally relies on feature space attack models. We make several contributions to address this gap, using PDF malware detection as a case study. First, we present a systematic retraining procedure which uses an automated problem space attack generator to design a more robust PDF malware detector. Second, we demonstrate that replacing problem space attacks with feature space attacks dramatically reduces the robustness of the resulting classifier, severely undermining feature space defense methods to date. Third, we demonstrate the existence of conserved (or invariant) features, and show how these can be leveraged to design evasionrobust classifiers that are nearly as effective, and far more efficient, than those relying on the problem space attack. Finally, we present a general approach for identifying conserved features.", "creator": "LaTeX with hyperref package"}}}