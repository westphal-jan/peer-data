{"id": "1708.08572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue", "abstract": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. These three methods work for identifying the word as either \"nice,\" or \"nice\" in the sense that the person perceives the word favorably or negatively. However, this paper does not investigate whether such language can be employed in language-based speech systems in particular. In a final, case-by-case scenario, the use of the \"nice\" keyword will not result in a \"nice\" (i.e., \"nice\" + \"nice\") sentence, but rather a \"nice\" (i.e., \"nice\" + \"nice\"). To evaluate how much of a particular sentence would be written, we employ a second-generation approach. We use a second-generation method called the \"nastiness\" keyword (if the language does not recognize it, then the sentence will not be written in NLP and therefore not in the NLP language) to identify the word as an \"nastiness\" (i.e., \"nice\" + \"nice\"). We use the \"nastiness\" keyword to identify the word as \"nice\" (i.e., \"nice\" + \"nice\") sentence, but rather a \"nice\" (i.e., \"nice\" + \"nice\"). To evaluate how much of a particular sentence would be written, we employ a second-generation approach. We use a second-generation approach called the \"nastiness\" keyword (if the language does not recognize it, then the sentence will not be written in NLP and therefore not in the NLP language). We use a second-generation method called the \"nastiness\" keyword (if the language does not recognize it, then the sentence will not be written in NLP and therefore not in the NLP language). We use a second-generation method called the \"nastiness\" keyword (if the language does not recognize it, then the sentence will not be written in NLP and therefore not in the NLP language). We use a second-generation method called the \"nastiness\"", "histories": [["v1", "Tue, 29 Aug 2017 02:05:14 GMT  (234kb,D)", "http://arxiv.org/abs/1708.08572v1", "Workshop on Language Analysis in Social Media (LASM 2013), at the North American Chapter of the Association for Computational Linguistics (NAACL 2013)"]], "COMMENTS": "Workshop on Language Analysis in Social Media (LASM 2013), at the North American Chapter of the Association for Computational Linguistics (NAACL 2013)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephanie lukin", "marilyn walker"], "accepted": false, "id": "1708.08572"}, "pdf": {"name": "1708.08572.pdf", "metadata": {"source": "CRF", "title": "Really? Well. Apparently Bootstrapping Improves the Performance of Sarcasm and Nastiness Classifiers for Online Dialogue", "authors": ["Stephanie Lukin", "Marilyn Walker"], "emails": ["slukin@soe.ucsc.edu", "maw@soe.ucsc.edu"], "sections": [{"heading": "1 Introduction", "text": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is very frequent in social media, as illustrated in the snippets in Fig. 1 from the publicly available Internet Argument Corpus (IAC) (Walker et al.,\n2012). Utterances are frequently sarcastic, e.g., Really? Well, when I have a kid, I\u2019ll be sure to just leave it in the woods, since it can apparently care for itself (R2 in Fig. 1 as well as Q1 and R1), and are often nasty, e.g. Here come the Christians, thinking they can know everything by guessing, and commiting the genetic fallacy left and right (R3 in Fig. 1). Note also the frequent use of dialogue specific discourse cues, e.g. the use of No in R1, Really? Well in R2, and okay, well in Q3 in Fig. 1 (Fox Tree and Schrock, 1999; Bryant and Fox Tree, 2002; Fox Tree, 2010).\nar X\niv :1\n70 8.\n08 57\n2v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\nThe IAC comes with annotations of different types of social language categories including sarcastic vs not sarcastic, nasty vs nice, rational vs emotional and respectful vs insulting. Using a conservative threshold of agreement amongst the annotators, an analysis of 10,003 Quote/Response pairs (Q/R pairs) from the 4forums portion of IAC suggests that social subjective language is fairly frequent: about 12% of posts are sarcastic, 23% are emotional, and 12% are insulting or nasty. We select sarcastic and nasty dialogic turns to test our method on more than one type of subjective language and explore issues of generalization; we do not claim any relationship between these types of social language in this work.\nDespite their frequency, expanding this corpus of sarcastic or nasty utterances at scale is expensive: human annotation of 100% of the corpus would be needed to identify 12% more examples of sarcasm or nastiness. An explanation of how utterances are annotated in IAC is detailed in Sec. 2.\nOur aim in this paper is to explore whether it is possible to extend a method for bootstrapping a classifier for monologic, subjective sentences proposed by Riloff & Wiebe, henceforth R&W (Riloff and Wiebe, 2003; Thelen and Riloff, 2002), to automatically find sarcastic and nasty utterances in unannotated online dialogues. Sec. 3 provides an overview of R&W\u2019s bootstrapping method. To apply bootstrapping, we:\n1. Explore two different methods for identifying cue words and phrases in two types of subjective language in dialogues: sarcasm and nasty (Sec. 4);\n2. Use the learned indicators to train a sarcastic (nasty) dialogue act classifier that maximizes precision at the expense of recall (Sec. 5);\n3. Use the classified utterances to learn general syntactic extraction patterns from the sarcastic (nasty) utterances (Sec. 6);\n4. Bootstrap this process on unannotated text to learn new extraction patterns to use for classification.\nWe show that the Extraction Pattern Learner improves the precision of our sarcasm classifier by 17% and the recall by 24%, and improves the precision of the nastiness classifier by 14% and recall by 13%. We discuss previous work in Sec. 2 and compare to ours in Sec. 7 where we also summarize our results and discuss future work."}, {"heading": "2 Previous Work", "text": "IAC provides labels for sarcasm and nastiness that were collected with Mechanical Turk on Q/R pairs such as those in Fig. 1. Seven Turkers per Q/R pair answered a binary annotation question for sarcasm Is the respondent using sarcasm? (0,1) and a scalar annotation question for nastiness Is the respondent attempting to be nice or is their attitude fairly nasty? (-5 nasty . . . 5 nice). We selected turns from IAC Table 1 with sarcasm averages above 0.5, and nasty averages below -1 and nice above 1. Fig. 1 included example nastiness and sarcasm values.\nPrevious work on the automatic identification of sarcasm has focused on Twitter using the #sarcasm (Gonza\u0301lez-Iba\u0301n\u0303ez et al., 2011) and #irony (Reyes et al., 2012) tags and a combined variety of tags and smileys (Davidov et al., 2010). Another popular domain examines Amazon product reviews looking for irony (Reyes and Rosso, 2011), sarcasm (Tsur et al., 2010), and a corpus collection for sarcasm (Filatova, 2012). (Carvalho et al., 2009) looks for irony in comments in online newpapers which can have a thread-like structure. This primary focus on monologic venues suggests that sarcasm and irony can be detected with a relatively high precision but have a different structure from dialogues (Fox Tree and Schrock, 1999; Bryant and Fox Tree, 2002; Fox Tree, 2010), posing the question, can we generalize from monologic to dialogic structures? Each of these works use methods including LIWC unigrams, affect, polarity, punctuation and more, and achieve on average a precision of 75% or accuracy of between 45% and 85%.\nAutomatically identifying offensive utterances is also of interest. Previous work includes identifying flames in emails (Spertus, 1997) and other messaging interfaces (Razavi et al., 2010), identifying insults in Twitter (Xiang et al., 2012), as well as comments from new sites (Sood et al., 2011). These approaches achieve an accuracy between 64% and 83% using a variety of approaches. The accuracies for nasty utterances has a much smaller spread and higher average than sarcasm accuracies. This suggests that nasty language may be easier to identify than sarcastic language."}, {"heading": "3 Method Overview", "text": "Our method for bootstrapping a classifier for sarcastic (nasty) dialogue acts uses R&W\u2019s model adapted to our data as illustrated for sarcasm in Fig. 2. The\noverall idea of the method is to find reliable cues and then generalize. The top of Fig. 2 specifies the input to the method as an unannotated corpus of opinion dialogues, to illustrate the long term aim of building a large corpus of the phenomenon of interest without human annotation. Although the bootstrapping method assumes that the input is unannotated text, we first need utterances that are already labeled for sarcasm (nastiness) to train it. Table 1 specifies how we break down into datasets the annotations on the utterances in IAC for our various experiments.\nThe left circle of Fig. 2 reflects the assumption that there are Sarcasm or Nasty Cues that can identify the category of interest with high precision (R&W call this the \u201cKnown Subjective Vocabulary\u201d). The aim of first developing a high precision classifier, at the expense of recall, is to select utterances that are reliably of the category of interest from unannotated text. This is needed to ensure that the generalization step of \u201cExtraction Pattern Learner\u201d does not introduce too much noise.\nR&W did not need to develop a \u201cKnown Subjective Vocabulary\u201d because previous work provided one (Wilson et al., 2005; Wiebe et al., 1999; Wiebe et al., 2003). Thus, our first question with applying R&W\u2019s method to our data was whether or not it is possible to develop a reliable set of Sarcasm (Nastiness) Cues (O1 below). Two factors suggest that it might not be. First, R&W\u2019s method assumes that the cues are in the utterance to be classified, but it has been claimed that sarcasm (1) is context dependent, and (2) requires world knowledge to recognize,\nat least in many cases. Second, sarcasm is exhibited by a wide range of different forms and with different dialogue strategies such as jocularity, understatement and hyberbole (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Filatova, 2012). In Sec. 4 we devise and test two different methods for acquiring a set of Sarcasm (Nastiness) Cues on particular development sets of dialogue turns called the \u201cMT exp dev\u201d in Table 1.\nThe boxes labeled \u201cHigh Precision Sarcastic Post Classifier\u201d and \u201cHigh Precision Not Sarcastic Post Classifier\u201d in Fig. 2 involves using the Sarcasm (Nastiness) Cues in simple combinations that maximize precision at the expense of recall. R&W found cue combinations that yielded a High Precision Classifier (HP Classifier) with 90% precision and 32% recall on their dataset. We discuss our test of these steps in Sec. 5 on the \u201cHP train\u201d development sets in Table 1 to estimate parameters for the High Precision classifier, and then test the HP classifier with these parameters on the test dataset labeled \u201cHP dev test\u201d in Table 1.\nR&W\u2019s Pattern Based classifier increased recall to 40% while losing very little precision. The open question with applying R&W\u2019s method to our data, was whether the cues that we discovered, by whatever method, would work at high enough precision to support generalization (O2 below). In Sec. 6 we\ndescribe how we use the \u201cPE eval\u201d development set (Table 1) to estimate parameters for the Extraction Pattern Learner, and then test the Pattern Based Sarcastic (Nasty) Post classifier on the newly classified utterances from the dataset labeled \u201cHP dev test\u201d (Table 1). Our final open question was whether the extraction patterns from R&W, which worked well for news text, would work on social dialogue (O3 below). Thus our experiments address the following open questions as to whether R&W\u2019s bootstrapping method improves classifiers for sarcasm and nastiness in online dialogues:\n\u2022 (O1) Can we develop a \u201cknown sarcastic (nasty) vocabulary\u201d? The LH circle of Fig. 2 illustrates that we use two different methods to identify Sarcasm Cues. Because we have utterances labeled as sarcastic, we compare a statistical method that extracts important features automatically from utterances, with a method that has a human in the loop, asking annotators to select phrases that are good indicators of sarcasm (nastiness) (Sec. 5); \u2022 (O2) If we can develop a reliable set of sarcasm (nastiness) cues, is it then possible to develop an HP classifier? Will our precision be high enough? Is the fact that sarcasm is often context dependent an issue? (Sec. 5); \u2022 (O3) Will the extraction patterns used in R&W\u2019s work allow us to generalize sarcasm cues from the HP Classifiers? Are R&W\u2019s patterns general enough to work well for dialogue and social language? (Sec. 6)."}, {"heading": "4 Sarcasm and Nastiness Cues", "text": "Because there is no prior \u201cKnown Sarcastic Vocabulary\u201d we pilot two different methods for discovering lexical cues to sarcasm and nastiness, and experiment with combinations of cues that could yield a high precision classifier (Gianfortoni et al., 2011). The first method uses \u03c72 to measure whether a word or phrase is statistically indicative of sarcasm (nastiness) in the development sets labeled \u201cMT exp dev\u201d (Table 1). This method, a priori, seems reasonable because it is likely that if you have a large enough set of utterances labeled as sarcastic, you could be able to automatically learn a set of reliable cues for sarcasm.\nThe second method introduces a step of human annotation. We ask Turkers to identify sarcastic (nasty) indicators in utterances (the open question\nO1) from the development set \u201cMT exp dev\u201d (Table 1). Turkers were presented with utterances previously labeled sarcastic or nasty in IAC by 7 different Turkers, and were told \u201cIn a previous study, these responses were identified as being sarcastic by 3 out of 4 Turkers. For each quote/response pair, we will ask you to identify sarcastic or potentially sarcastic phrases in the response\u201d. The Turkers then selected words or phrases from the response they believed could lead someone to believing the utterance was sarcastic or nasty. These utterances were not used again in further experiments. This crowdsourcing method is similar to (Filatova, 2012), but where their data is monologic, ours is dialogic."}, {"heading": "4.1 Results from Indicator Cues", "text": "Sarcasm is known to be highly variable in form, and to depend, in some cases, on context for its interpretation (Sperber and Wilson, 1981; Gibbs, 2000; Bryant and Fox Tree, 2002). We conducted an initial pilot on 100 of the 617 sarcastic utterances in\nthe development set \u201cMT exp dev\u201d to see if this was necessarily the case in our dialogues. (Snow et al., 2008) measures the quality of Mechanical Turk annotations on common NLP tasks by comparing them to a gold standard. Pearson\u2019s correlation coefficient shows that very few Mechanical Turk annotators were required to beat the gold standard data, often\nless than 5. Because our sarcasm task does not have gold standard data, we ask 100 annotators to participate in the pilot. Fig. 3 plots the average interannotator agreement (ITA) as a function of the number of annotators, computed using Pearson correlation counts, for 40 annotators and for trigrams which require more data to converge. In all cases (unigrams, bigrams, trigrams) ITA plateaus at around 20 annotators and is about 90% with 10 annotators, showing that the Mechanical Turk tasks are well formed and there is high agreement. Thus we elicited only 10 annotations for the remainder of the sarcastic and all the nasty utterances from the development set \u201cMT exp dev\u201d.\nWe begin to form our \u201cknown sarcastic vocabulary\u201d from these indicators, (open question O1). Each MT indicator has a FREQ (frequency): the number of times each indicator appears in the training set; and an IA (interannotator agreement): how many annotators agreed that each indicator was sarcastic or nasty. Table 2 shows the best unigrams, bigrams, and trigrams from the \u03c72 test and from the sarcasm Mechanical Turk experiment and Table 3 shows the results from the nasty experiment. We compare the MT indicators to the \u03c72 indicators as part of investigating open question O1.\nAs a pure statistical method, \u03c72 can pick out things humans might not. For example, if it just happened that the word \u2018we\u2019 only occurs in sarcastic utterances in the development set, then \u03c72 will select it as a strong sarcastic word (row 3 of Table 2). However, no human would recognize this word as corresponding to sarcasm. \u03c72 could easily be overtrained if the \u201cMT exp dev\u201d development set is not large enough to eliminate such general words from consideration, \u201cMT exp dev\u201d only has 617 sarcastic utterances and 510 nasty utterances (Table 1).\nWords that the annotators select as indicators (columns labeled MT in Table 2 and Table 3) are much more easily identifiable although they do not appear as often. For example, the IA of 0.95 for \u2018ah\u2019 in Table 2 means that of all the annotators who saw \u2018ah\u2019 in the utterance they annotated, 95% selected it to be sarcastic. However the FREQ of 2 means that \u2018ah\u2019 only appeared in 2 utterances in the \u201cMT exp dev\u201d development set.\nWe test whether any of the methods for selecting indicators provide reliable cues that generalize to a larger dataset in Sec. 5. The parameters that we estimate on the development sets are exactly how frequent (compared to a \u03b81) and how reliable (com-\npared to a \u03b82) a cue has to be to be useful in R&W\u2019s bootstrapping method."}, {"heading": "5 High-Precision Classifiers", "text": "R&W use their \u201cknown subjective vocabulary\u201d to train a High Precision classifier. R&W\u2019s HP classifier searches for exact surface matches of the subjective indicators and classifies utterances as subjective if two subjective indicators are present. We follow similar guidelines to train HP Sarcasm and Nasty Classifiers. To test open question O1, we use a development set called \u201cHP train\u201d (Table 1) to test three methods for measuring the \u201cgoodness\u201d of an indicator that could serve as a high precision cue: (1) interannotator agreement based on annotators consensus from Mechanical Turk, on the assumption that the number of annotators that select a cue indicates its strength and reliability (IA features); (2) percent sarcastic (nasty) and frequency statistics in the HP train dataset as R&W do (percent features); and (3) the \u03c72 percent sarcastic (nasty) and frequency statistics (\u03c72 features).\nThe IA features use the MT indicators and the IA and FREQ calculations introduced in Sec. 4 (see Tables 2 and 3). First, we select indicators such that \u03b81 <= FREQ where \u03b81 is a set of possible thresholds. Then we introduce two new parameters \u03b1 and \u03b2 to divide the indicators into three \u201cgoodness\u201d groups that reflect interannotator agreement.\nindicatorstrength = { weak if 0 \u2264 IA < \u03b1 medium if \u03b1 \u2264 IA < \u03b2 strong if \u03b2 \u2264 IA < 1\nFor IA features, an utterance is classified as sarcastic if it contains at least one strong or two medium indicators. Other conditions were piloted. We first hypothesized that weak cues might be a way of classifying \u201cnot sarcastic\u201d utterances. But HP train showed that both sarcastic and not sarcastic utterances contain weak indicators yielding no information gain. The same is true for Nasty\u2019s counterclass Nice. Thus we specify that counter-class utterances must have no strong indicators or at most one medium indicator. In contrast, R&W\u2019s counter-class classifier looks for a maximum of one subjective indicator.\nThe percent features also rely on the FREQ of each MT indicator, subject to a \u03b81 threshold, as well as the percentage of the time they occur in a sarcastic utterance (%SARC) or nasty utterance\n(%NASTY). We select indicators with various parameters for \u03b81 and \u03b82 \u2264 %SARC. At least two indicators must be present and above the thresholds to be classified and we exhaust all combinations. Less than two indicators are needed to be classified as the counter-class, as in R&W.\nFinally, the \u03c72 features use the same method as percent features only using the \u03c72 indicators instead of the MT indicators.\nAfter determining which parameter settings performs the best for each feature set, we ran the HP classifiers, using each feature set and the best parameters, on the test set labeled \u201cHP dev test\u201d. The HP Classifiers classify the utterances that it is confident on, and leave others unlabeled."}, {"heading": "5.1 Results from High Precision Classifiers", "text": "The HP Sarcasm and Nasty Classifiers were trained on the three feature sets with the following parameters: IA features we exhaust all combinations of \u03b2 = [.70, .75, .80, .85, .90, .95, 1.00], \u03b1 = [.35, .40, .45, .50, .55, .60, .65, .7], and \u03b81 = [2, 4, 6, 8, 10]; for the percent features and \u03c72 features we again exhaust \u03b81 = [2, 4, 6, 8, 10] and \u03b82 = [.55, .60, .65, .70, .75, .80, .85, .90, .95, 1.00].\nTables 4 and 5 show a subset of the experiments with each feature set. We want to select parameters that maximize precision without sacrificing too much recall. Of course, the parameters that yield the highest precision also have the lowest recall, e.g. Sarcasm percent features, parameters \u03b81 = 4 and \u03b82 = 0.75 achieve 92% precision but the recall is 1% (Table 4), and Nasty percent features with parameters \u03b81 = 8 and \u03b82 = 0.8 achieves 98% precision but a recall of 3% (Table 5). On the other end of the spectrum, the parameters that achieve the highest recall yield a precision equivalent to random chance.\nExamining the parameter combinations in Tables 4 and 5 shows that percent features do better than IA features in all cases in terms of precision. Compare the block of results labeled % in Tables 4 and 5 with the IA and \u03c72 blocks for column P. Nasty appears to be easier to identify than Sarcasm, especially using the percent features. The performance of the \u03c72 features is comparable to that of percent features for sarcasm, but lower than percent features for Nasty.\nThe best parameters selected from each feature set are shown in the PARAMS column of Table 6. With the indicators learned from these parameters, we run the Classifiers on the test set labeled \u201cHP\ndev test\u201d (Table 1). The performance on test set \u201cHP dev test\u201d (Table 6) is worse than on the training set (Tables 4 and 5). However we conclude that both the % and \u03c72 features provide candidates for sarcasm (nastiness) cues that are high enough precision (open question O2) to be used in the Extraction Pattern Learner (Sec. 6), even if Sarcasm is more context dependent than Nastiness."}, {"heading": "6 Extraction Patterns", "text": "R&W\u2019s Pattern Extractor searches for instances of the 13 templates in the first column of Table 7 in utterances classified by the HP Classifier. We reimplement this; an example of each pattern as instantiated in test set \u201cHP dev test\u201d for our data is shown in the second column of Table 7. The template <subj> active-verb <dobj> matches utterances where a subject is followed by an active verb and a direct object. However, these matches are not limited to exact surface matches as the HP Classifiers required, e.g. this pattern would match the phrase \u201chave a problem\u201d. Table 10 in the Appendix provides example utterances from IAC that match the instantiated template patterns. For example, the excerpt from the first row in Table 10 \u201cIt is quite strange to encounter someone in this day and age who lacks any knowledge whatsoever of the mechanism of adaptation since it was explained 150 years ago\u201d matches the <subj> passive-verb pattern. It appears 2 times (FREQ) in the test set and is sarcastic both times (%SARC is 100%). Row 11 in Table 10 shows an utterance matching the active-verb prep <np> pattern with the phrase \u201cAt the time of the Constitution there weren\u2019t exactly vast suburbs that could be prowled by thieves looking for an open window\u201d. This phrase appears 14 times (FREQ) in the test set and is sarcastic (%SARC) 92% of the time it appears.\nThe Pattern Based Classifiers are trained on a development set labeled \u201cPE eval\u201d (Table 1). Utterances from this development set are not used again\nin any further experiments. Patterns are extracted from the dataset and we again compute FREQ and %SARC and %NASTY for each pattern subject to \u03b81 \u2264 FREQ and \u03b82 \u2264 %SARC or % NASTY. Classifications are made if at least two patterns are present and both are above the specified \u03b81 and \u03b82, as in R&W. Also following R&W, we do not learn \u201cnot sarcastic\u201d or \u201cnice\u201d patterns.\nTo test the Pattern Based Classifiers, we use as input the classifications made by the HP Classifiers. Using the predicted labels from the classifiers as the true labels, the patterns from test set \u201cHP test dev\u201d are extracted and compared to those patterns found in development set \u201cPE eval\u201d. We have two feature sets for both sarcasm and nastiness: one using the predictions from the MT indicators in the HP classifier (percent features) and another using those instances from the \u03c72 features."}, {"heading": "6.1 Results from Pattern Classifier", "text": "The Pattern Classifiers classify an utterance as Sarcastic (Nasty) if at least two patterns are present and above the thresholds \u03b81 and \u03b82, exhausting all combinations of \u03b81 = [2, 4, 6, 8, 10] and \u03b82 = [.55, .60, .65, .70, .75, .80, .85, .90, .95, 1.00]. The counterclasses are predicted when the utterance contains less than two patterns. The exhaustive classifications are first made using the utterances in the development set labeled \u201cPE eval\u201d. Fig. 4 shows the precision and recall trade-off for \u03b81 = [2, 10] and all \u03b82 values on sarcasm development set\u201cPE eval\u201d. As recall increases, precision drops. By including patterns that only appear 2 times, we get better recall. Limiting \u03b81 to 10 yields fewer patterns and lower recall.\nTable 8 shows the results for various parameters. The PE dev dataset learned a total of 1,896 sarcastic extraction patterns above a minimum threshold of \u03b81 < 2 and \u03b82 < 0.55, and similarly 847 nasty extraction patterns. Training on development set \u201cPE dev\u201d yields high precision and good recall. To select the best parameters, we again look for a balance between precision and recall. Both Classifiers have very high precision. In the end, we select parameters that have a better recall than the best parameter from the HP Classifiers which is recall = 38% for sarcasm and recall = 49% for nastiness. The best parameters and their test results are shown in Table 9.\nThe Pattern Classifiers are tested on \u201cHP dev test\u201d with the labels predicted by our HP Classifiers, thus we have two different sets of classifications for both Sarcasm and Nastiness: percent features and \u03c72 features. Overall, the Pattern Classification performs better on Nasty than Sarcasm. Also, the percent features yield better results than \u03c72 features, possibly because the precision for \u03c72 is high from the HP Classifiers, but the recall is very low. We believe that \u03c72 selects statistically predictive indicators that are tuned to the dataset, rather than general. Having a human in the loop guarantees more general features from a smaller dataset. Whether this remains true on the size as the dataset increases to 1000 or more is unknown. We conclude that R&W\u2019s patterns generalize well on our Sarcasm and Nasty datasets (open question O3), but suspect that there may be better syntactic patterns for bootstrapping sarcasm and nastiness, e.g. involving cue words or semantic categories of words rather than syntactic categories, as we discuss in Sec. 7.\nThis process can be repeated by taking the newly classified utterances from the Pattern Based Classifiers, then applying the Pattern Extractor to learn new patterns from the newly classified data. This\ncan be repeated for multiple iterations. We leave this for future work."}, {"heading": "7 Discussion and Future Work", "text": "In this work, we apply a bootstrapping method to train classifiers to identify particular types of subjective utterances in online dialogues. First we create a suite of linguistic indicators for sarcasm and nastiness using crowdsourcing techniques. Our crowdsourcing method is similar to (Filatova, 2012). From these new linguistic indicators we construct a classifier following previous work on bootstrapping subjectivity classifiers (Riloff and Wiebe, 2003; Thelen and Riloff, 2002). We compare the performance of the High Precision Classifier that was trained based on statistical measures against one that keeps human annotators in the loop, and find that Classifiers using statistically selected indicators appear to be overtrained on the development set because they do not generalize well. This first phase achieves 54% precision and 38% recall for sarcastic utterances using the human selected indicators. If we bootstrap by using syntactic patterns to create more general sarcasm indicators from the utterances identified as sarcastic in the first phase, we achieve a higher precision of 62% and recall of 52%.\nWe apply the same method to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap with syntactic patterns, possibly suggesting that nastiness (insults) are less nuanced and easier to detect than sarcasm.\nPrevious work claims that recognition of sarcasm (1) depends on knowledge of the speaker, (2) world knowledge, or (3) use of context (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Carvalho et al., 2009). While we also believe that certain types of subjective language cannot be de-\ntermined from cue words alone, our Pattern Based Classifiers, based on syntactic patterns, still achieves high precision and recall. In comparison to previous monologic works whose sarcasm precision is about 75%, ours is not quite as good with 62%. While the nasty works do not report precision, we believe that they are comparable to the 64% - 83% accuracy with our precision of 75%.\nOpen question O3 was whether R&W\u2019s patterns are fine tuned to subjective utterances in news. However R&W\u2019s patterns improve both precision and recall of our Sarcastic and Nasty classifiers. In future work however, we would like to test whether semantic categories of words rather than syntactic categories would perform even better for our problem, e.g. Linguistic Inquiry and Word Count categories. Looking again at row 1 in Table 10, \u201cIt is quite strange to encounter someone in this day and age who lacks any knowledge whatsoever of the mechanism of adaptation since it was explained 150 years ago\u201d, the word \u2018quite\u2019 matches the \u2018cogmech\u2019 and \u2018tentative\u2019 categories, which might be interesting to generalize to sarcasm. In row 11 \u201cAt the time of the Constitution there weren\u2019t exactly vast suburbs that could be prowled by thieves looking for an open window\u201d, the phrase \u201cweren\u2019t exactly\u201d could also match the LIWC categories \u2018cogmech\u2019 and \u2018certain\u2019 or, more specifically, certainty negated.\nWe also plan to extend this work to other categories of subjective dialogue acts, e.g. emotional and respectful as mentioned in the Introduction, and to expand our corpus of subjective dialogue acts. We will experiment with performing more than one iteration of the bootstrapping process (R&W complete two iterations) as well as create a Hybrid Classifier combining the subjective cues and patterns into a single Classifier that itself can be bootstrapped.\nFinally, we would like to extend our method to different dialogue domains to see if the classifiers trained on our sarcastic and nasty indicators would achieve similar results or if different social media sites have their own style of displaying sarcasm or nastiness not comparable to those in forum debates."}, {"heading": "8 Appendix A. Instances of Learned", "text": "Patterns"}], "references": [{"title": "Clues for detecting", "author": ["Carvalho et al.2009] P. Carvalho", "L. Sarmento", "M.J. Silva", "E. de Oliveira"], "venue": null, "citeRegEx": "Carvalho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carvalho et al\\.", "year": 2009}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Davidov et al.2010] D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "In Proc. of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Reactions to irony in discourse: Evidence for the least disruption principle", "author": ["S. Attardo", "D. Boxer"], "venue": "Journal of Pragmatics,", "citeRegEx": "Eisterhold et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Eisterhold et al\\.", "year": 2006}, {"title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing", "author": ["E. Filatova"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Filatova.,? \\Q2012\\E", "shortCiteRegEx": "Filatova.", "year": 2012}, {"title": "Modeling of stylistic variation in social media with stretchy patterns", "author": ["D. Adamson", "C.P. Ros\u00e9"], "venue": "In Proc. of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties,", "citeRegEx": "Gianfortoni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gianfortoni et al\\.", "year": 2011}, {"title": "Irony in talk among friends", "author": ["R.W. Gibbs"], "venue": "Metaphor and Symbol,", "citeRegEx": "Gibbs.,? \\Q2000\\E", "shortCiteRegEx": "Gibbs.", "year": 2000}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["S. Muresan", "N. Wacholder"], "venue": "In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies: short papers,", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.", "year": 2011}, {"title": "Offensive language detection using multi-level classification", "author": ["Razavi et al.2010] A. Razavi", "D. Inkpen", "S. Uritsky", "S. Matwin"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "Razavi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Razavi et al\\.", "year": 2010}, {"title": "Mining subjective knowledge from customer reviews: a specific case of irony detection", "author": ["Reyes", "Rosso2011] A. Reyes", "P. Rosso"], "venue": "In Proc. of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011),", "citeRegEx": "Reyes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2011}, {"title": "From humor recognition to irony detection: The figurative language of social media", "author": ["Reyes et al.2012] A. Reyes", "P. Rosso", "D. Buscaldi"], "venue": "Data & Knowledge Engineering", "citeRegEx": "Reyes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "Learning extraction patterns for subjective expres", "author": ["Riloff", "Wiebe2003] E. Riloff", "J. Wiebe"], "venue": null, "citeRegEx": "Riloff et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2003}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["Snow et al.2008] R. Snow", "B. O\u2019Conner", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Automatic identification of personal insults on social news sites", "author": ["Sood et al.2011] S.O. Sood", "E.F. Churchill", "J. Antin"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "Sood et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sood et al\\.", "year": 2011}, {"title": "Irony and the use-mention distinction", "author": ["Sperber", "Wilson1981] Dan Sperber", "Deidre Wilson"], "venue": "Radical Pragmatics,", "citeRegEx": "Sperber et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Sperber et al\\.", "year": 1981}, {"title": "Smokey: Automatic recognition of hostile messages", "author": ["E. Spertus"], "venue": "In Proc. of the National Conference on Artificial Intelligence,", "citeRegEx": "Spertus.,? \\Q1997\\E", "shortCiteRegEx": "Spertus.", "year": 1997}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["Thelen", "Riloff2002] M. Thelen", "E. Riloff"], "venue": "In Proc. of the ACL-02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Thelen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Thelen et al\\.", "year": 2002}, {"title": "Icwsm\u2013a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews", "author": ["Tsur et al.2010] O. Tsur", "D. Davidov", "A. Rappoport"], "venue": "In Proc. of the fourth international AAAI conference on weblogs and social media,", "citeRegEx": "Tsur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tsur et al\\.", "year": 2010}, {"title": "A corpus for research on deliberation and debate", "author": ["Pranav Anand", "Robert Abbott", "Jean E. Fox Tree"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Walker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Development and use of a goldstandard data set for subjectivity classifications", "author": ["Wiebe et al.1999] J.M. Wiebe", "R.F. Bruce", "T.P. O\u2019Hara"], "venue": "In Proc. of the 37th annual meeting of the Association for Computational Linguistics,", "citeRegEx": "Wiebe et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 1999}, {"title": "Recognizing and organizing opinions expressed in the world press. In Working Notes-New Directions in Question Answering", "author": ["Wiebe et al.2003] J. Wiebe", "E. Breck", "C. Buckley", "C. Cardie", "P. Davis", "B. Fraser", "D. Litman", "D. Pierce", "E. Riloff", "T. Wilson"], "venue": null, "citeRegEx": "Wiebe et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2003}, {"title": "Opinionfinder: A system for subjectivity analysis", "author": ["Wilson et al.2005] T. Wilson", "P. Hoffmann", "S. Somasundaran", "J. Kessler", "J. Wiebe", "Y. Choi", "C. Cardie", "E. Riloff", "S. Patwardhan"], "venue": "In Proc. of HLT/EMNLP on Interactive Demonstrations,", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Detecting offensive tweets", "author": ["Xiang et al.2012] G. Xiang", "B. Fan", "L. Wang", "J. Hong", "C. Rose"], "venue": null, "citeRegEx": "Xiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "Previous work on the automatic identification of sarcasm has focused on Twitter using the #sarcasm (Gonz\u00e1lez-Ib\u00e1\u00f1ez et al., 2011) and #irony (Reyes et al.", "startOffset": 99, "endOffset": 129}, {"referenceID": 9, "context": ", 2011) and #irony (Reyes et al., 2012) tags and a combined variety of tags and smileys (Davidov et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": ", 2012) tags and a combined variety of tags and smileys (Davidov et al., 2010).", "startOffset": 56, "endOffset": 78}, {"referenceID": 16, "context": "Another popular domain examines Amazon product reviews looking for irony (Reyes and Rosso, 2011), sarcasm (Tsur et al., 2010), and a corpus collection for sarcasm (Filatova, 2012).", "startOffset": 106, "endOffset": 125}, {"referenceID": 3, "context": ", 2010), and a corpus collection for sarcasm (Filatova, 2012).", "startOffset": 45, "endOffset": 61}, {"referenceID": 0, "context": "(Carvalho et al., 2009) looks for irony in comments in online newpapers which can have a thread-like structure.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Previous work includes identifying flames in emails (Spertus, 1997) and other messaging interfaces (Razavi et al.", "startOffset": 52, "endOffset": 67}, {"referenceID": 7, "context": "Previous work includes identifying flames in emails (Spertus, 1997) and other messaging interfaces (Razavi et al., 2010), identifying insults in Twitter (Xiang et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 21, "context": ", 2010), identifying insults in Twitter (Xiang et al., 2012), as well as comments from new sites (Sood et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 12, "context": ", 2012), as well as comments from new sites (Sood et al., 2011).", "startOffset": 44, "endOffset": 63}, {"referenceID": 20, "context": "R&W did not need to develop a \u201cKnown Subjective Vocabulary\u201d because previous work provided one (Wilson et al., 2005; Wiebe et al., 1999; Wiebe et al., 2003).", "startOffset": 95, "endOffset": 156}, {"referenceID": 18, "context": "R&W did not need to develop a \u201cKnown Subjective Vocabulary\u201d because previous work provided one (Wilson et al., 2005; Wiebe et al., 1999; Wiebe et al., 2003).", "startOffset": 95, "endOffset": 156}, {"referenceID": 19, "context": "R&W did not need to develop a \u201cKnown Subjective Vocabulary\u201d because previous work provided one (Wilson et al., 2005; Wiebe et al., 1999; Wiebe et al., 2003).", "startOffset": 95, "endOffset": 156}, {"referenceID": 5, "context": "Second, sarcasm is exhibited by a wide range of different forms and with different dialogue strategies such as jocularity, understatement and hyberbole (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Filatova, 2012).", "startOffset": 152, "endOffset": 233}, {"referenceID": 2, "context": "Second, sarcasm is exhibited by a wide range of different forms and with different dialogue strategies such as jocularity, understatement and hyberbole (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Filatova, 2012).", "startOffset": 152, "endOffset": 233}, {"referenceID": 3, "context": "Second, sarcasm is exhibited by a wide range of different forms and with different dialogue strategies such as jocularity, understatement and hyberbole (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Filatova, 2012).", "startOffset": 152, "endOffset": 233}, {"referenceID": 4, "context": "Because there is no prior \u201cKnown Sarcastic Vocabulary\u201d we pilot two different methods for discovering lexical cues to sarcasm and nastiness, and experiment with combinations of cues that could yield a high precision classifier (Gianfortoni et al., 2011).", "startOffset": 227, "endOffset": 253}, {"referenceID": 3, "context": "This crowdsourcing method is similar to (Filatova, 2012), but where their data is monologic, ours is dialogic.", "startOffset": 40, "endOffset": 56}, {"referenceID": 5, "context": "Sarcasm is known to be highly variable in form, and to depend, in some cases, on context for its interpretation (Sperber and Wilson, 1981; Gibbs, 2000; Bryant and Fox Tree, 2002).", "startOffset": 112, "endOffset": 178}, {"referenceID": 11, "context": "(Snow et al., 2008) measures the quality of Mechanical Turk annotations on common NLP tasks by comparing them to a gold standard.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Our crowdsourcing method is similar to (Filatova, 2012).", "startOffset": 39, "endOffset": 55}, {"referenceID": 5, "context": "Previous work claims that recognition of sarcasm (1) depends on knowledge of the speaker, (2) world knowledge, or (3) use of context (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Carvalho et al., 2009).", "startOffset": 133, "endOffset": 221}, {"referenceID": 2, "context": "Previous work claims that recognition of sarcasm (1) depends on knowledge of the speaker, (2) world knowledge, or (3) use of context (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Carvalho et al., 2009).", "startOffset": 133, "endOffset": 221}, {"referenceID": 0, "context": "Previous work claims that recognition of sarcasm (1) depends on knowledge of the speaker, (2) world knowledge, or (3) use of context (Gibbs, 2000; Eisterhold et al., 2006; Bryant and Fox Tree, 2002; Carvalho et al., 2009).", "startOffset": 133, "endOffset": 221}], "year": 2017, "abstractText": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the first level with generalized syntactic patterns.", "creator": "LaTeX with hyperref package"}}}