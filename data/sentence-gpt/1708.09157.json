{"id": "1708.09157", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Cross-lingual, Character-Level Neural Morphological Tagging", "abstract": "Even for common NLP tasks, sufficient supervision is not available in many languages -- morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30% with each learning sequence.\n\n\n\nThe key question to be asked is how the approach has been applied, and what its practical implications. The NLP technique enables us to train the NLP technique directly, using different techniques to target and improve neural tagging over time. For example, using the method of training, the task is to train individual neural tags for high-resource languages to perform a comparison. In this paper, we use the LSE algorithm, which enables a deep learning model for the classification of words by the language (e.g., the BOLD is expressed as the mean number of words in the word or the word), and to test whether we can distinguish two words, for example, using the LSE algorithm. The LSE algorithm is designed to improve classification by using a high-resource language over time. However, our goal is to build a more general model for the classification of words from a high-resource language such as C, where a high-resource language is used, as well as by training its semantic tagging abilities to identify a particular target and classify the corresponding word. To train a single, high-resource language, we first train the same neural tags over time, which are generated as the words in the sentence at the top of the sentence. To train the semantic tagging over time, we first train the same neural tags over time, which are generated as the words in the sentence at the top of the sentence. To perform this task, we first train the same neural tags over time, which are generated as the words in the sentence at the top of the sentence. To train the semantic tagging over time, we first train the same neural tags over time, which are generated as the words in the sentence at the top of the sentence. To train the semantic tagging over time, we first train the same neural tags over time, which are generated as the words in the sentence at the top of the sentence. To train the semantic tagging over time, we first train the same neural tags over time,", "histories": [["v1", "Wed, 30 Aug 2017 08:14:34 GMT  (729kb,D)", "http://arxiv.org/abs/1708.09157v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryan cotterell", "georg heigold"], "accepted": true, "id": "1708.09157"}, "pdf": {"name": "1708.09157.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual, Character-Level Neural Morphological Tagging", "authors": ["Ryan Cotterell", "Georg HeigoldK"], "emails": ["@Center", "ryan.cotterell@jhu.edu", "georg.heigold@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "State-of-the-art morphological taggers require thousands of annotated sentences to train. For the majority of the world\u2019s languages, however, sufficient large-scale annotation is not available and obtaining it would often be infeasible. Accordingly, an important road forward in low-resource NLP is the development of methods that allow for the training of high-quality tools from smaller amounts of data. In this work, we focus on transfer learning\u2014we train a recurrent neural tagger for a low-resource language jointly with a tagger for a related highresource language. Forcing the models to share character-level features among the languages allows large gains in accuracy when tagging the lowresource languages, while maintaining (or even improving) accuracy on the high-resource language.\nRecurrent neural networks constitute the state of the art for a myriad of tasks in NLP, e.g., multilingual part-of-speech tagging (Plank et al., 2016), syntactic parsing (Dyer et al., 2015; Zeman et al., 2017), morphological paradigm completion (Cotterell et al., 2016, 2017) and language modeling\n(Sundermeyer et al., 2012; Melis et al., 2017); recently, such models have also improved morphological tagging (Heigold et al., 2016, 2017). In addition to increased performance over classical approaches, neural networks also offer a second advantage: they admit a clean paradigm for multitask learning. If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap benefits from each other and often performance improves for all (Collobert et al., 2011b). We exploit this idea for language-to-language transfer to develop an approach for cross-lingual morphological tagging.\nWe experiment on 18 languages taken from four different language families. Using the Universal Dependencies treebanks, we emulate a lowresource setting for our experiments, e.g., we attempt to train a morphological tagger for Catalan using primarily data from a related language like Spanish. Our results demonstrate the successful transfer of morphological knowledge from the highresource languages to the low-resource languages without relying on an externally acquired bilingual lexicon or bitext. We consider both the single- and multi-source transfer case and explore how similar two languages must be in order to enable highquality transfer of morphological taggers.1"}, {"heading": "2 Morphological Tagging", "text": "Many languages in the world exhibit rich inflectional morphology: the form of individual words mutates to reflect the syntactic function. For example, the Spanish verb son\u0303ar will appear as suen\u0303o in the first person present singular, but son\u0303a\u0301is in the second person present plural, depending on the bundle of syntaco-semantic attributes associated with\n1While we only experiment with languages in the same family, we show that closer languages within that family are better candidates for transfer. We remark that future work should consider the viability of more distant language pairs.\nar X\niv :1\n70 8.\n09 15\n7v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 7\nthe given form (in a sentential context). For concreteness, we list a more complete table of Spanish verbal inflections in Table 1. Note that some languages, e.g., Archi, Northeast Caucasian language, display a veritable cornucopia of potential forms with the size of the verbal paradigm exceeding 10,000 (Kibrik, 1998).\nStandard NLP annotation, e.g., the scheme in Sylak-Glassman et al. (2015), marks forms in terms of universal key-attribute pairs, e.g., the first person present singular is represented as [pos=V, per=1, num=SG, tns=PRES ]. This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuruo\u0308z, 1994; Hajic\u030c and Hladka\u0301, 1998). As the part-of-speech (POS) is a component of the tag, we may view morphological tagging as a strict generalization of POS tagging, where we have significantly refined the set of available tags. All of the experiments in this paper make use of the universal morphological tag set available in the Universal Dependencies (UD) (Nivre et al., 2016). As an example, we have provided a Russian sentence with its UD tagging in Figure 1.\nTransferring Morphology. The transfer of morphology is arguably more dependent on the relatedness of the languages in question than other annotations in NLP, such as POS and named entity recognition (NER). POS lends itself nicely to a universal annotation scheme (Petrov et al., 2012) and traditional NER is limited to a small number of cross-linguistically compliant categories, e.g., PERSON and PLACE. Even universal dependency arcs employ cross-lingual labels (Nivre et al., 2016).\nMorphology, on the other hand, typically requires more fine-grained annotation, e.g., grammatical case and tense. It is often the case that one language will make a semantic distinction in the form (or at all) that another does not. For example, the Hungarian noun overtly marks 17 grammatical cases and Slavic verbs typically distinguish two aspects through morphology, while English marks none of these distinctions. If the word form in the source language does not overtly mark a grammatical category in the target language, it is nighimpossible to expect a successful transfer. For this reason, much of our work focuses on the transfer of related languages\u2014specifically exploring how close two languages must be for a successful transfer. Note that the language-specific nature of morphology does not contradict the universality of the annotation; each language may mark a different subset of categories, i.e., use a different set of the universal keys and attributes, but there is a single, universal set, from which the key-attribute pairs are drawn. See Newmeyer (2007) for a linguistic treatment of cross-lingual annotation.\nNotation. We will discuss morphological tagging in terms of the following notation. We will consider two (related) languages: a high-resource source language `s and a low-resource target language `t. Each of these languages will have its own (potentially overlapping) set of morphological tags, denoted Ts and Tt, respectively. We will work with the union of both sets T = Ts \u222a Tt. An individual tag mi = [k1=v1, . . . , kM=vM ] \u2208 T is comprised of universal keys and attributes, i.e., the pairs (ki, vi) are completely language-agnostic. In the case where a language does not mark a distinction, e.g., case on English nouns, the corresponding keys are excluded from the tag. Typically, |T | is large (see Table 3). We denote the set of training sentences for the high-resource source language as Ds and the set of training sentences for the lowresource target language asDt. In the experimental section, we will also consider a multi-source setting\nwhere we have multiple high-resource languages, but, for ease of explication, we stick to the singlesource case in the development of the model."}, {"heading": "3 Character-Level Neural Transfer", "text": "Our formulation of transfer learning builds on work in multi-task learning (Caruana, 1997; Collobert et al., 2011b). We treat each individual language as a task and train a joint model for all the tasks. We first discuss the current state of the art in morphological tagging: a character-level recurrent neural network. After that, we explore three augmentations to the architecture that allow for the transfer learning scenario. All of our proposals force the embedding of the characters for both the source and the target language to share the same vector space, but involve different mechanisms, by which the model may learn language-specific features."}, {"heading": "3.1 Character-Level Neural Networks", "text": "Character-level neural networks currently constitute the state of the art in morphological tagging (Heigold et al., 2017). We draw on previous work in defining a conditional distribution over taggings t for a sentence w of length |w| = N as\np\u03b8(t | w) = N\u220f i=1 p\u03b8(ti | w), (1)\nwhich may be seen as a 0th order conditional random field (CRF) (Lafferty et al., 2001) with parameter vector \u03b8.2 Importantly, this factorization of the distribution p\u03b8(t | w) also allows for efficient exact decoding and marginal inference in O(|T | \u00b7 N)-time, but at the cost of not admitting any explicit interactions in the output structure, i.e., between adjacent tags.3 We parameterize the distribution over tags at each time step as\np\u03b8(ti | w) = softmax (Wei + b) , (2) 2The parameter vector \u03b8 is a vectorization of all the parameters discussed below. 3As an aside, it is quite interesting that a model with the factorization in Equation (1) outperforms the MARMOT model (Mu\u0308ller et al., 2013), which focused on modeling higher-order interactions between the morphological tags, e.g., they employ up to a (pruned) 3rd order CRF. That such a model achieves state-of-the-art performance indicates, however, that richer source-side features, e.g., those extracted by our characterlevel neural architecture, are more important for morphological tagging than higher-order tag interactions, which come with the added unpleasantness of exponential (in the order) decoding.\nwhere W \u2208 R|T |\u00d7n is an embedding matrix, b \u2208 R|T | is a bias vector and positional embeddings ei\n4 are taken from a concatenation of the output of two long short-term memory recurrent neural networks (LSTMs) (Hochreiter and Schmidhuber, 1997), folded forward and backward, respectively, over a sequence of input vectors. This constitutes a bidirectional LSTM (Graves and Schmidhuber, 2005). We define the positional embedding vector as follows\nei = [LSTM(v1:i); LSTM(vN,i+1)] , (3)\nwhere each vi \u2208 Rn is, itself, a word embedding. Note that the function LSTM returns the last final hidden state vector of the network. This architecture is the context bidirectional recurrent neural network of Plank et al. (2016). Finally, we derive each word embedding vector vi from a characterlevel bidirectional LSTM embedder. Namely, we define each word embedding as the concatenation\nvi = [ LSTM ( \u3008ci1 , . . . , ciMi \u3009 ) ; (4)\nLSTM ( \u3008ciMi , . . . , ci1\u3009 )] .\nIn other words, we run a bidirectional LSTM over the character stream. This bidirectional LSTM is the sequence bidirectional recurrent neural network of Plank et al. (2016). Note a concatenation of the sequence of character symbols \u3008ci1 , . . . , ciMi \u3009 results in the word string wi. Each of the Mi characters cik is a member of the set \u03a3. We take \u03a3 to be the union of sets of characters in the languages considered.\nWe direct the reader to Heigold et al. (2017) for a more in-depth discussion of this and various additional architectures for the computation of vi; the architecture we have presented in Equation (5) is competitive with the best performing setting in Heigold et al.\u2019s study."}, {"heading": "3.2 Cross-Lingual Morphological Transfer as Multi-Task Learning", "text": "Cross-lingual morphological tagging may be formulated as a multi-task learning problem. We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one. The first loss function we consider is the following:\n4Note that |ei| = n; see \u00a74.4 for the exact values used in the experimentation.\nsoftmax\nlinear\nBidirectional LSTM\nsoftmax\nlinear\np\u2713(t1 | w) p\u2713(tN | w)\nv1 vN\n\u2026\n\u2026\n(a) Vanilla architecture for neural morphological tagging.\nsoftmax\nlinear\nBidirectional LSTM\nsoftmax\nlinear\np\u2713(t1, ` | w) p\u2713(tN , ` | w)\nv1 vN\n\u2026\n\u2026\n(b) Joint morphological tagging and language identification.\nconcatenation\nBidirectional LSTM\nLUTLUT\nvi\nci1 ciMi\u2026\n(c) Character-level biLSTM embedder.\nconcatenation\nBidirectional LSTM\nLUT\nvi\nci1 ciMi\u2026 LUT LUTLUT id` id`\n(d) Language-specific biLSTM embedder.\nFigure 2: We depict four subarchitectures used in the models we develop in this work. Combining (a) with the character embeddings in (c) gives the vanilla morphological tagging architecture of Heigold et al. (2017). Combining (a) with (d) yields the language-universal softmax architecture and (b) and (c) yields our joint model for language identification and tagging.\nLmulti(\u03b8) = \u2211\n(t,w)\u2208Ds log p\u03b8(t | w, `s) (5) + \u2211 (t,w)\u2208Dt log p\u03b8 (t | w, `t) .\nCrucially, our cross-lingual objective forces both taggers to share part of the parameter vector \u03b8, which allows it to represent morphological regularities between the two languages in a common embedding space and, thus, enables transfer of knowledge. This is no different from monolingual multitask settings, e.g., jointly training a chunker and a tagger for the transfer of syntactic information (Collobert et al., 2011b). We point out that, in contrast to our approach, almost all multi-task transfer learning, e.g., for dependency parsing (Guo et al., 2016), has shared word-level embeddings rather than character-level embeddings. See \u00a76 for a more complete discussion.\nWe consider two parameterizations of this distribution p\u03b8(ti | w, `). First, we modify the initial character-level LSTM embedding such that it also encodes the identity of the language. Second, we modify the softmax layer, creating a languagespecific softmax.\nLanguage-Universal Softmax. Our first architecture has one softmax, as in Equation (2), over all morphological tags in T (shared among all the languages). To allow the architecture to encode morphological features specific to one language, e.g., the third person present plural ending in Spanish is -an, but -a\u0303o in Portuguese, we modify the creation of the character-level embeddings. Specifically, we augment the character alphabet \u03a3 with a distinguished symbol that indicates the language: id`. We then pre- and postpend this symbol to the character stream for every word before feeding the\ncharacters into the bidirectional LSTM Thus, we arrive at the new language-specific word embeddings, v`i = [ LSTM ( \u3008id`, ci1 , . . . , ciMi ,id`\u3009 ) ; (6)\nLSTM ( \u3008id`, ciMi , . . . , ci1 ,id`\u3009 )] .\nThis model creates a language-specific embedding vector vi, but the individual embeddings for a given character are shared among the languages jointly trained on. The remainder of the architecture is held constant.\nLanguage-Specific Softmax. Next, inspired by the architecture of Heigold et al. (2013), we consider a language-specific softmax layer, i.e., we define a new output layer for every language:\np\u03b8 (ti | w, `) = softmax (W`ei + b`) , (7)\nwhere W` \u2208 R|T |\u00d7n and b` \u2208 R|T | are now language-specific. In this architecture, the embeddings ei are the same for all languages\u2014the model has to learn language-specific behavior exclusively through the output softmax of the tagging LSTM.\nJoint Morphological Tagging and Language Identification. The third model we exhibit is a joint architecture for tagging and language identification. We consider the following loss function: Ljoint(\u03b8) = \u2211\n(t,w)\u2208Ds log p\u03b8(`s, t | w) (8) + \u2211 (t,w)\u2208Dt log p\u03b8 (`t, t | w) ,\nwhere we factor the joint distribution as\np\u03b8 (`, t | w) = p\u03b8 (` | w) \u00b7 p\u03b8 (t | w, `) . (9)\nJust as before, we define p\u03b8 (t | w, `) above as in Equation (7) and we define\np\u03b8(` | w) = softmax ( u> tanh(V ei) ) , (10)\nwhich is a multi-layer perceptron with a binary softmax (over the two languages) as an output layer; we have added the additional parameters V \u2208 R|T |\u00d7n and u \u2208 R|T |. In the case of multi-source transfer, this is a softmax over the set of languages.\nComparative Discussion. The first two architectures discussed in \u00a73.2 represent two possibilities for a multi-task objective, where we condition on the language of the sentence. The first integrates this knowledge at a lower level and the second at a higher level. The third architecture discussed in \u00a73.2 takes a different tack\u2014rather than conditioning on the language, it predicts it. The joint model offers one interesting advantage over the two architectures proposed. Namely, it allows us to perform a morphological analysis on a sentence where the language is unknown. This effectively alleviates an early step in the NLP pipeline, where language id is performed and is useful in conditions where the language to be tagged may not be known a-priori, e.g., when tagging social media data.\nWhile there are certainly more complex architectures one could engineer for the task, we believe we have found a relatively diverse sampling, enabling an interesting experimental comparison. Indeed, it is an important empirical question which architectures are most appropriate for transfer learning. Since transfer learning affords the opportunity to reduce the sample complexity of the \u201cdata-hungry\u201d neural networks that currently dominate NLP research, finding a good solution for cross-lingual transfer in state-of-the-art neural models will likely be a boon for low-resource NLP in general."}, {"heading": "4 Experiments", "text": "Empirically, we ask three questions of our architectures. i) How well can we transfer morphological tagging models from high-resource languages to low-resource languages in each architecture? (Does one of the three outperform the others?) ii) How many annotated data in the low-resource language do we need? iii) How closely related do the languages need to be to get good transfer?"}, {"heading": "4.1 Experimental Languages", "text": "We experiment with the language families: Romance (Indo-European), Northern Germanic (IndoEuropean), Slavic (Indo-European) and Uralic. In the Romance sub-grouping of the wider IndoEuropean family, we experiment on Catalan (ca), French (fr), Italian (it), Portuguese (pt), Romanian (ro) and Spanish (es). In the Northern Germanic family, we experiment on Danish (da), Norwegian (no) and Swedish (sv). In the Slavic family, we experiment on Bulgarian (bg), Czech (bg), Polish (pl), Russian (ru), Slovak (sk) and Ukrainian (uk). Finally, in the Uralic family we experiment on Estonian (et), Finnish (fi) and Hungarian (hu)."}, {"heading": "4.2 Datasets", "text": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the 4th and 6th columns of the file format) (Nivre et al., 2016). We list the size of the training, development and test splits of the UD treebanks we used in Table 2. Also, we list the number of unique morphological tags in each language in Table 3, which serves as an approximate measure of the morphological complexity each language exhibits. Crucially, the data are annotated in a cross-linguistically consistent manner, such that words in the different languages that have the same syntacto-semantic function have the same bundle of tags (see \u00a72 for a discussion). Potentially, further gains would be possible by using a more\nuniversal scheme, e.g., the UNIMORPH scheme."}, {"heading": "4.3 Baselines", "text": "We consider two baselines in our work. First, we consider the MARMOT tagger (Mu\u0308ller et al., 2013), which is currently the best performing non-neural model. The source code for MARMOT is freely available online,5 which allows us to perform fully controlled experiments with this model. Second, we consider the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines in turn."}, {"heading": "4.3.1 Higher-Order CRF Tagger", "text": "The MARMOT tagger is the leading non-neural approach to morphological tagging. This baseline is important since non-neural, feature-based approaches have been found empirically to be more efficient, in the sense that their learning curves tend to be steeper. Thus, in the low-resource setting we would be remiss to not consider a feature-based approach. Note that this is not a transfer approach, but rather only uses the low-resource data."}, {"heading": "4.3.2 Alignment-based Projection", "text": "The projection approach of Buys and Botha (2016) provides an alternative method for transfer learning. The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003). Then, one trains a standard tagger using the projected annotations. The specific tagger employed is the WSABIE model of Weston et al. (2011), which\u2014like our approach\u2014 is a 0th-order discriminative neural model. In contrast to ours, however, their network is shallow. We compare the two methods in more detail in \u00a76."}, {"heading": "4.3.3 Architecture Study", "text": "Additionally, we perform a thorough study of the neural transfer learner, considering all three architectures. A primary goal of our experiments is to determine which of our three proposed neural transfer techniques is superior. Even though our experiments focus on morphological tagging, these architectures are more general in that they may be\n5http://cistern.cis.lmu.de/marmot/ 6We do not have access to the code as the model was developed in industry, so we compare to the numbers reported in the original paper, as well as additional numbers provided to us by the first author in a personal communication. The numbers will not be, strictly speaking, comparable. However, we hope they provide insight into the relative performance of the tagger.\neasily applied to other tasks, e.g., parsing or machine translation. We additionally explore the viability of multi-source transfer, i.e., the case where we have multiple source languages. All of our architectures generalize to the multi-source case without any complications."}, {"heading": "4.4 Experimental Details", "text": "We train our models with the following conditions.\nEvaluation Metrics. We evaluate using average per token accuracy, as is standard for both POS tagging and morphological tagging, and per feature F1 as employed in Buys and Botha (2016). The per feature F1 calculates a key F k1 for each key in the target language\u2019s tags by asking if the keyattribute pair ki=vi is in the predicted tag. Then, the key-specific F k1 values are averaged equally. Note that F1 is a more flexible metric as it gives partial credit for getting some of the attributes in the bundle correct, where accuracy does not.\nHyperparameters. Our networks are four layers deep (two LSTM layers for the character embedder, i.e., to compute vi and two LSTM layers for the tagger, i.e., to compute ei) and we use an embedding size of 128 for the character input vector size and hidden layers of 256 nodes in all other cases. All networks are trained with the stochastic gradient method RMSProp (Tieleman and Hinton, 2012), with a fixed initial learning rate and a learning rate decay that is adjusted for the other languages according to the amount of training data. The batch size is always 16. Furthermore, we use dropout (Srivastava et al., 2014). The dropout probability is set to 0.2. We used Torch 7 (Collobert et al., 2011a) to configure the computation graphs implementing the network architectures."}, {"heading": "5 Results and Discussion", "text": "We report our results in three tables. First, we report a detailed cross-lingual evaluation in Table 4. Secondly, we report a comparison against two baselines in Table 5 (accuracy) and Table 6 (F1). We see two general trends of the data. First, we find that genetically closer languages yield better source languages. Second, we find that the multi-softmax architecture is the best in terms of transfer ability, as evinced by the results in Table 4. We find a wider gap between our model and the baselines under the accuracy than under F1. We attribute this\ntarget language |Dt| = 100 |Dt| = 1000\n(ca) (es) (fr) (it) (pt) (ro) (ca) (es) (fr) (it) (pt) (ro) so ur ce la ng ua ge (ca) \u2014 87.9% 84.2% 84.6% 81.1% 67.4% \u2014 94.1% 93.5% 93.1% 89.0% 89.8% (es) 88.9% \u2014 85.5% 85.6% 81.8% 69.5% 95.5% \u2014 93.5% 93.5% 88.9% 89.7% (fr) 88.3% 87.0% \u2014 83.6% 79.5% 69.9% 95.4% 93.8% \u2014 93.3% 88.6% 89.7% (it) 88.4% 87.8% 84.2% \u2014 80.6% 69.1% 95.4% 94.0% 93.3% \u2014 88.7% 90.3% (pt) 88.4% 88.9% 85.1% 84.7% \u2014 69.6% 95.3% 94.2% 93.5% 93.6% \u2014 89.8% (ro) 87.6% 87.2% 85.0% 84.4% 79.9% \u2014 95.3% 93.6% 93.4% 93.2% 88.5% \u2014\nmulti-source 89.8% 90.9% 86.6% 86.8% 83.4% 67.5% 95.4% 94.2% 93.4% 93.8% 88.7% 88.9%\n(a) Results for the Romance languages.\nto the fact that F1 is a softer metric in that it assigns credit to partially correct guesses.\nSource Language. As discussed in \u00a72, the transfer of morphology is language-dependent. This intuition is borne out in the results from our study (see Table 4). We see that in the closer grouping of the Western Romance languages, i.e., Catalan, French, Italian, Portuguese, and Spanish, it is easier to transfer than with Romanian, an Eastern Romance language. Within the Western grouping, we see that the close pairs, e.g., Spanish and Portuguese, are amenable to transfer. We find a similar pattern in the other language families, e.g., Russian is the best source language for Ukrainian, Czech is the best language source for Slovak and Finnish is the best source language for Estonian.\nMulti-Source Transfer. In many cases, we find that multiple sources noticeably improve the results over the single-source case. For instance, when we have multiple Romance languages as a source language, we see gains of up to 2%. We also see gains\nin the Northern Germanic languages when using multiple source languages. From a linguistic point of view, this is logical as different source languages may be similar to the target language along different dimensions, e.g., when transferring among the Slavic languages, we note that Russian retains the complex nominal case system of Serbian, but south Slavic Bulgarian is lexically more similar.\nPerformance Against the Two Baselines. As shown in Table 5 and Table 6, our model outperforms the projection tagger of Buys and Botha (2016) even though our approach does not utilize bitext, large-scale alignment or monolingual corpora\u2014rather, all transfer between languages happens through the forced sharing of characterlevel features.7 Our model, does, however, require\n7 We would like to highlight some issues of comparability with the results in Buys and Botha (2016). Strictly speaking, the results are not comparable and our improvement over their method should be taken with a grain of salt. As the source code is not publicly available and developed in industry, we resorted to numbers in their published work and additional numbers\nannotation of a small number of sentences in the target language for training. We note, however, that this does not necessitate a large number of human annotation hours (Garrette and Baldridge, 2013).\nReducing Sample Complexity. Another interesting a point about our model that is best evinced in Figure 3 is the feature-based CRF approach seems to be a better choice for the low-resource setting, i.e., the neural model has greater sample complexity. However, in the multi-task scenario, we find that the neural tagger\u2019s learning curve is even steeper. In other words, if we have to train a tagger on very little data, we are better off using a neural multi-task approach than a feature-based approach; preliminary attempts to develop a multitask version of MARMOT failed (see Figure 3)."}, {"heading": "6 Related Work", "text": "We divide the discussion of related work topically into three parts for ease of intellectual digestion."}, {"heading": "6.1 Alignment-Based Distant Supervision.", "text": "Most cross-lingual work in NLP\u2014focusing on morphology or otherwise\u2014has concentrated on indirect supervision, rather than transfer learning. The goal in such a regime is to provide noisy labels for\nobtained through direct communication with the authors. First, we used a slightly newer version of UD to incorporate more languages: we used v2 whereas they used v1.2. There are minor differences in the morphological tagset used between these versions. Also, in the |Dt| = 1000 setting, we are training on significantly more data than the models in Buys and Botha (2016). A much fairer comparison is to our models with |Dt| = 100. Also, we compare to their method using their standard (non) setup. This method is fair in so far as we evaluate in the same manner, but it disadvantages their approach, which cannot predict tags that are not in the source language.\ntraining the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation. While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; Ta\u0308ckstro\u0308m et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.\nAs we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself. We contrast the approaches. The main difference lies therein, that our approach is not projection-based and, thus, does not require the construction of a bilingual lexicon for projection based on bitext.\nRather, our method jointly learns multiple taggers and forces them to share features\u2014a true transfer learning scenario. In contrast to projection-based methods, our procedure always requires a minimal amount of annotated data in the low-resource target language\u2014in practice, however, this distinction is non-critical as projection-based methods without a small mount of seed target language data perform poorly (Buys and Botha, 2016)."}, {"heading": "6.2 Character-level NLP.", "text": "Our work also follows a recent trend in NLP, whereby traditional word-level neural representations are being replaced by character-level representations for a myriad tasks, e.g., POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon. Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016)."}, {"heading": "6.3 Neural Cross-lingual Transfer in NLP.", "text": "In terms of methodology, however, our proposal bears similarity to recent work in speech and machine translation\u2013we discuss each in turn. In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages. The architecture bears similarity to our multi-language softmax approach. Dependency parsing benefits from cross-lingual learning in a similar fashion (Guo et al., 2015, 2016).\nIn neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al.,\n2016) has explored the possibility of jointly train translation models for a wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. Kann et al. (2017) offer a similar method for cross-lingual transfer in morphological inflection generation."}, {"heading": "7 Conclusion", "text": "We have presented three character-level recurrent neural network architectures for multi-task crosslingual transfer of morphological taggers. We provided an empirical evaluation of the technique on 18 languages from four different language families, showing wide-spread applicability of the method. We found that the transfer of morphological taggers is an eminently viable endeavor among related language and, in general, the closer the languages, the easier the transfer of morphology becomes. Our technique outperforms two strong baselines proposed in previous work. Moreover, we define standard low-resource training splits in UD for future research in low-resource morphological tagging. Future work should focus on extending the neural morphological tagger to a joint lemmatizer (Mu\u0308ller et al., 2015) and evaluate its functionality in the low-resource setting."}, {"heading": "Acknowledgements", "text": "RC acknowledges the support of an NDSEG fellowship. Also, we would like to thank Jan Buys and Jan Botha who help us compare to the numbers reported in their paper. We are grateful to Schloss Dagstuhl Leibniz Center for Informatics for hosting the event at which work on this paper started."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Cross-lingual morphological tagging for low-resource languages", "author": ["Jan Buys", "Jan A. Botha."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1954\u20131964, Berlin, Germany. Association for Com-", "citeRegEx": "Buys and Botha.,? 2016", "shortCiteRegEx": "Buys and Botha.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning, 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011a", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011b", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL),", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora", "author": ["Victoria Fossum", "Steven P. Abney."], "venue": "Second International Joint Conference on Natural Language Processing (IJCNLP), pages 862\u2013", "citeRegEx": "Fossum and Abney.,? 2005", "shortCiteRegEx": "Fossum and Abney.", "year": 2005}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Garrette and Baldridge.,? 2013", "shortCiteRegEx": "Garrette and Baldridge.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5-6):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), pages 2734\u20132740.", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Tagging inflective languages: Prediction of morphological categories for a rich structured tagset", "author": ["Jan Haji\u010d", "Barbora Hladk\u00e1."], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Inter-", "citeRegEx": "Haji\u010d and Hladk\u00e1.,? 1998", "shortCiteRegEx": "Haji\u010d and Hladk\u00e1.", "year": 1998}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "Guenter Neumann", "Josef van Genabith."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Compu-", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Neural morphological tagging from characters for morphologically rich languages", "author": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith."], "venue": "CoRR, abs/1606.06640.", "citeRegEx": "Heigold et al\\.,? 2016", "shortCiteRegEx": "Heigold et al\\.", "year": 2016}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["Georg Heigold", "Vincent Vanhoucke", "Andrew Senior", "Patrick Nguyen", "Marc\u2019Aurelio Ranzato", "Matthieu Devin", "Jeffrey Dean"], "venue": "In IEEE International Conference on Acous-", "citeRegEx": "Heigold et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heigold et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "One-shot neural cross-lingual transfer for paradigm completion", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), Vancouver, Canada. Asso-", "citeRegEx": "Kann et al\\.,? 2017", "shortCiteRegEx": "Kann et al\\.", "year": 2017}, {"title": "Archi (Caucasian \u2013 Daghestanian)", "author": ["Aleksandr E. Kibrik."], "venue": "The Handbook of Morphology, pages 455\u2013476. Blackwell Oxford.", "citeRegEx": "Kibrik.,? 1998", "shortCiteRegEx": "Kibrik.", "year": 1998}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the 2015", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "On the state of the art of evaluation in neural language models", "author": ["G\u00e1bor Melis", "Chris Dyer", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1707.05589.", "citeRegEx": "Melis et al\\.,? 2017", "shortCiteRegEx": "Melis et al\\.", "year": 2017}, {"title": "Joint lemmatization and morphological tagging with Lemming", "author": ["Thomas M\u00fcller", "Ryan Cotterell", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages", "citeRegEx": "M\u00fcller et al\\.,? 2015", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 322\u2013332, Seattle, Washington,", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Linguistic typology requires crosslinguistic formal categories", "author": ["Frederick J. Newmeyer."], "venue": "Linguistic Typology, 11(1):133\u2013157.", "citeRegEx": "Newmeyer.,? 2007", "shortCiteRegEx": "Newmeyer.", "year": 2007}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Haji\u010d", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Tagging and morphological disambiguation of Turkish text", "author": ["Kemal Oflazer", "\u00cclker Kuru\u00f6z."], "venue": "Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 144\u2013149. Association for Computational Linguistics.", "citeRegEx": "Oflazer and Kuru\u00f6z.,? 1994", "shortCiteRegEx": "Oflazer and Kuru\u00f6z.", "year": 1994}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan T. McDonald."], "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC), pages 2089\u20132096.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Conll 2017 shared task: Multilingual parsing from raw text to universal dependencies", "author": ["Li."], "venue": "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1\u201319, Vancouver, Canada. Association", "citeRegEx": "Li.,? 2017", "shortCiteRegEx": "Li.", "year": 2017}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 30\u201334, San", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": ", multilingual part-of-speech tagging (Plank et al., 2016), syntactic parsing (Dyer et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 8, "context": ", 2016), syntactic parsing (Dyer et al., 2015; Zeman et al., 2017), morphological paradigm completion (Cotterell et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 25, "context": ", 2016, 2017) and language modeling (Sundermeyer et al., 2012; Melis et al., 2017); recently, such models have also improved morphological tagging (Heigold et al.", "startOffset": 36, "endOffset": 82}, {"referenceID": 5, "context": "If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap benefits from each other and often performance improves for all (Collobert et al., 2011b).", "startOffset": 188, "endOffset": 213}, {"referenceID": 22, "context": "10,000 (Kibrik, 1998).", "startOffset": 7, "endOffset": 21}, {"referenceID": 31, "context": "This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998).", "startOffset": 196, "endOffset": 246}, {"referenceID": 16, "context": "This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998).", "startOffset": 196, "endOffset": 246}, {"referenceID": 29, "context": "All of the experiments in this paper make use of the universal morphological tag set available in the Universal Dependencies (UD) (Nivre et al., 2016).", "startOffset": 130, "endOffset": 150}, {"referenceID": 32, "context": "POS lends itself nicely to a universal annotation scheme (Petrov et al., 2012) and traditional NER is limited to a small number of cross-linguistically compliant categories, e.", "startOffset": 57, "endOffset": 78}, {"referenceID": 29, "context": "Even universal dependency arcs employ cross-lingual labels (Nivre et al., 2016).", "startOffset": 59, "endOffset": 79}, {"referenceID": 28, "context": "See Newmeyer (2007) for a linguistic treatment of cross-lingual annotation.", "startOffset": 4, "endOffset": 20}, {"referenceID": 3, "context": "Our formulation of transfer learning builds on work in multi-task learning (Caruana, 1997; Collobert et al., 2011b).", "startOffset": 75, "endOffset": 115}, {"referenceID": 5, "context": "Our formulation of transfer learning builds on work in multi-task learning (Caruana, 1997; Collobert et al., 2011b).", "startOffset": 75, "endOffset": 115}, {"referenceID": 17, "context": "Character-level neural networks currently constitute the state of the art in morphological tagging (Heigold et al., 2017).", "startOffset": 99, "endOffset": 121}, {"referenceID": 23, "context": "which may be seen as a 0th order conditional random field (CRF) (Lafferty et al., 2001) with parameter vector \u03b8.", "startOffset": 64, "endOffset": 87}, {"referenceID": 27, "context": "As an aside, it is quite interesting that a model with the factorization in Equation (1) outperforms the MARMOT model (M\u00fcller et al., 2013), which focused on modeling higher-order interactions between the morphological tags, e.", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "where W \u2208 R|T |\u00d7n is an embedding matrix, b \u2208 R|T | is a bias vector and positional embeddings ei 4 are taken from a concatenation of the output of two long short-term memory recurrent neural networks (LSTMs) (Hochreiter and Schmidhuber, 1997), folded forward and backward, respectively, over a sequence of input vectors.", "startOffset": 209, "endOffset": 243}, {"referenceID": 13, "context": "This constitutes a bidirectional LSTM (Graves and Schmidhuber, 2005).", "startOffset": 38, "endOffset": 68}, {"referenceID": 33, "context": "This architecture is the context bidirectional recurrent neural network of Plank et al. (2016). Finally, we derive each word embedding vector vi from a characterlevel bidirectional LSTM embedder.", "startOffset": 75, "endOffset": 95}, {"referenceID": 30, "context": "This bidirectional LSTM is the sequence bidirectional recurrent neural network of Plank et al. (2016). Note a concatenation of the sequence of character symbols \u3008ci1 , .", "startOffset": 82, "endOffset": 102}, {"referenceID": 17, "context": "We direct the reader to Heigold et al. (2017) for a more in-depth discussion of this and various additional architectures for the computation of vi; the architecture we have presented in Equation (5) is competitive with the best performing setting in Heigold et al.", "startOffset": 24, "endOffset": 46}, {"referenceID": 17, "context": "Combining (a) with the character embeddings in (c) gives the vanilla morphological tagging architecture of Heigold et al. (2017). Combining (a) with (d) yields the language-universal softmax architecture and (b) and (c) yields our joint model for language identification and tagging.", "startOffset": 107, "endOffset": 129}, {"referenceID": 5, "context": "(Collobert et al., 2011b).", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": ", for dependency parsing (Guo et al., 2016), has shared word-level embeddings rather than character-level embeddings.", "startOffset": 25, "endOffset": 43}, {"referenceID": 17, "context": "Next, inspired by the architecture of Heigold et al. (2013), we consider a language-specific softmax layer, i.", "startOffset": 38, "endOffset": 60}, {"referenceID": 29, "context": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the 4th and 6th columns of the file format) (Nivre et al., 2016).", "startOffset": 162, "endOffset": 182}, {"referenceID": 27, "context": "First, we consider the MARMOT tagger (M\u00fcller et al., 2013),", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Second, we consider the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines in turn.", "startOffset": 63, "endOffset": 85}, {"referenceID": 2, "context": "The projection approach of Buys and Botha (2016) provides an alternative method for transfer learn-", "startOffset": 27, "endOffset": 49}, {"referenceID": 30, "context": "The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003).", "startOffset": 86, "endOffset": 105}, {"referenceID": 30, "context": "The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003). Then, one trains a standard tagger using the projected annotations. The specific tagger employed is the WSABIE model of Weston et al. (2011), which\u2014like our approach\u2014 is a 0th-order discriminative neural model.", "startOffset": 87, "endOffset": 248}, {"referenceID": 2, "context": "We evaluate using average per token accuracy, as is standard for both POS tagging and morphological tagging, and per feature F1 as employed in Buys and Botha (2016). The per feature F1 calculates a key F k 1 for each key in the target language\u2019s tags by asking if the keyattribute pair ki=vi is in the predicted tag.", "startOffset": 143, "endOffset": 165}, {"referenceID": 4, "context": "We used Torch 7 (Collobert et al., 2011a) to configure the computation graphs implementing the network architectures.", "startOffset": 16, "endOffset": 41}, {"referenceID": 2, "context": "As shown in Table 5 and Table 6, our model outperforms the projection tagger of Buys and Botha (2016) even though our approach does not utilize bitext, large-scale alignment or monolingual corpora\u2014rather, all transfer between languages happens through the forced sharing of characterlevel features.", "startOffset": 80, "endOffset": 102}, {"referenceID": 2, "context": "7 We would like to highlight some issues of comparability with the results in Buys and Botha (2016). Strictly speaking, the results are not comparable and our improvement over their method should be taken with a grain of salt.", "startOffset": 78, "endOffset": 100}, {"referenceID": 2, "context": "We compare on only those languages in Buys and Botha (2016). Note that tag-level accuracy was not reported in the original B&B paper, but was acquired through personal communication with the first author.", "startOffset": 38, "endOffset": 60}, {"referenceID": 12, "context": "We note, however, that this does not necessitate a large number of human annotation hours (Garrette and Baldridge, 2013).", "startOffset": 90, "endOffset": 120}, {"referenceID": 2, "context": "Also, in the |Dt| = 1000 setting, we are training on significantly more data than the models in Buys and Botha (2016). A much fairer comparison is to our models with |Dt| = 100.", "startOffset": 96, "endOffset": 118}, {"referenceID": 11, "context": "While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 21, "endOffset": 91}, {"referenceID": 7, "context": "While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 21, "endOffset": 91}, {"referenceID": 32, "context": "training the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation.", "startOffset": 85, "endOffset": 205}, {"referenceID": 2, "context": ", 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "As we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself.", "startOffset": 95, "endOffset": 117}, {"referenceID": 2, "context": "Table 6: Comparison of our approach to various baselines for low-resource tagging under F1 to allow for a more complete comparison to the model of Buys and Botha (2016). All architectures presented in this work are used in their multi-source setting.", "startOffset": 147, "endOffset": 169}, {"referenceID": 2, "context": "small mount of seed target language data perform poorly (Buys and Botha, 2016).", "startOffset": 56, "endOffset": 78}, {"referenceID": 1, "context": ", POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 24, "context": ", 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 36, "context": ", 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 9, "context": "Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016).", "startOffset": 111, "endOffset": 155}, {"referenceID": 34, "context": "Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016).", "startOffset": 111, "endOffset": 155}, {"referenceID": 1, "context": ", POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon.", "startOffset": 55, "endOffset": 208}, {"referenceID": 15, "context": "In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages.", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al.", "startOffset": 30, "endOffset": 77}, {"referenceID": 10, "context": ", 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages.", "startOffset": 21, "endOffset": 86}, {"referenceID": 37, "context": ", 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages.", "startOffset": 21, "endOffset": 86}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. Kann et al. (2017) offer a similar method for cross-lingual transfer in morphological", "startOffset": 8, "endOffset": 390}, {"referenceID": 26, "context": "Future work should focus on extending the neural morphological tagger to a joint lemmatizer (M\u00fcller et al., 2015) and evaluate its functionality in the low-resource setting.", "startOffset": 92, "endOffset": 113}], "year": 2017, "abstractText": "Even for common NLP tasks, sufficient supervision is not available in many languages\u2014morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30%.", "creator": "LaTeX with hyperref package"}}}