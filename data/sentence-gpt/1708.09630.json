{"id": "1708.09630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning", "abstract": "This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leader-follower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents.\n\n\n\n\n\nThe most important approach to modeling and developing a neural network is to classify a model-free reinforcement learning (RL) and consider whether this model-free reinforcement learning model might be able to reduce the learning costs associated with reinforcement learning. However, our dataset is incomplete and we do not yet have sufficient data to support that conclusion. In order to investigate the implications of this approach, we used a series of random-effects analysis (random-effects analysis) in our neural networks. We selected a subset of models (N = 50) with an average posterior value of 50. In order to evaluate the effects of RL on learning costs, we used the model-free reinforcement learning model. We then selected a subset of models (N = 50) with an average posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to investigate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to assess the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25. In order to evaluate the effects of RL on learning costs, we used a set of models (N = 50) with a posterior value of 25.", "histories": [["v1", "Thu, 31 Aug 2017 09:21:08 GMT  (5160kb,D)", "http://arxiv.org/abs/1708.09630v1", null], ["v2", "Sat, 30 Sep 2017 13:52:23 GMT  (4789kb,D)", "http://arxiv.org/abs/1708.09630v2", null]], "reviews": [], "SUBJECTS": "cs.MA cs.LG cs.SY", "authors": ["rohollah moghadam", "hamidreza modares"], "accepted": false, "id": "1708.09630"}, "pdf": {"name": "1708.09630.pdf", "metadata": {"source": "CRF", "title": "Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning", "authors": ["Rohollah Moghadam"], "emails": ["moghadamr@mst.edu,", "modaresh@mst.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014Reinforcement Learning, Optimal Control, Multi-agent System, Distributed Control, H\u221e control.\nI. Introduction\nDistributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]\u2013[4]. Due to their networked nature, however, adversarial inputs such as disturbances or attacks on sensors and actuators can significantly degrade their performance. In a contested environment with adversarial inputs, corrupted data communicated by a single compromised agent and used by neighbors for learning can mislead the entire network to a wrong understanding of the environment and consequently cause no emergent behavior or an emergent misbehavior.\nDesign of optimal control protocols that possess an ability to learn the uncertainties online has attracted considerable attention for both single-agent and multi-agent systems. Reinforcement learning (RL) [5]\u2013[10], inspired by learning\nRohollah Moghadam and Hamidreza Modares are with the Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Emerson Hall, 301 W 16th St, Rolla, MO 65409 USA (e-mails: moghadamr@mst.edu, modaresh@mst.edu).\nManuscript received April 11, 2017\nmechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20]. All of the aforementioned approaches ignore the effects of the adversarial inputs on the learning and performance of the overall system. RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems. However, in a multi-agent system, the adverse effects of adversarial inputs are considerably more serious, because one compromised agent can degrade the performance of the entire network. Attacks on multi-agent systems have been investigated by several researchers [29]\u2013[39]. Besides, the H\u221e control of multi-agent systems is considered in [40]\u2013[46] to attenuate the effects of disturbances on agents. However, no learning is used in these approaches. Moreover, these methods use the full knowledge of the agent\u2019s dynamics and/or the network topology to identify and mitigate attacks or disturbances, which may not be available in a fully distributed system. Developing learning-based control protocols that can find an optimal control strategy in adversarial environments using only measured data is of utmost importance to increase autonomy for multi-agent systems.\nIn this paper, a unified resilient model-free RL-based distributed control protocol for leader-follower multi-agent systems under adversarial inputs is designed. The contributions are as follows:\n\u2022 A comprehensive analysis of the adverse effects of the adversarial inputs on the performance of the multi-agent systems is provided. It is shown that corrupted data communicated by a single compromised agent can mislead the entire network and cause no emergent behavior. A more sophisticated adversarial input can also mislead existing disturbance attenuation techniques. \u2022 A unified distributed RL-based control framework is presented for both homogeneous and heterogeneous multiagent systems so that only compromised agents will be affected by an adversarial input and their corrupted sensory data will not be used for learning by intact agents. \u2022 To attenuate the effect of adversarial inputs on compromised agents themselves, a distributed H\u221e control protocol is designed. Decoupled game algebraic Riccati equations are derived for solving the proposed distributed H\u221e control problem. This is in contrast to existing RLbased H\u221e solutions for multi-agent systems that end up ar X\niv :1\n70 8.\n09 63\n0v 1\n[ cs\n.M A\n] 3\n1 A\nug 2\n01 7\n2 with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.\n\u2022 A novel off-policy RL algorithm is developed to solve the disturbance H\u221e control problem without requiring an admissible policy. This is in contrast to existing RL algorithms that require partial knowledge of the system dynamics to find an admissible policy.\nThe simulation results show the performance of the proposed control protocol.\nII. Preliminaries\nIn this section, a background of the graph theory is provided.\nA. Graph Theory\nA directed graph G consists of a pair (V,E) in which V = {v1, \u00b7 \u00b7 \u00b7 , vN} is a set of nodes and E \u2286 V \u00d7 V is a set of edges. The adjacency matrix is defined as A = [ ai j ] , with ai j > 0 if (v j, vi) \u2208 E, and ai j = 0 otherwise. The set of nodes vi with edges incoming to node v j is called the neighbors of node vi, namely Ni = {v j : (v j, vi) \u2208 E}. The graph Laplacian matrix is defined as L = D\u2212A, where D = diag(di) is the indegree matrix, with di = \u2211 j\u2208Ni ai j as the weighted in-degree of node vi. A (directed) tree is a connected digraph where every node, except the root node, has the in-degree of one. A graph is said to have a spanning tree if a subset of edges forms a directed tree. A leader can be pinned to multiple nodes, resulting in a diagonal pinning matrix G = diag (gi) \u2208 RN\u00d7N with the pinning gain gi > 0 if the node has access to the leader node and gi = 0 otherwise. Finally, 1N is the N-vector of ones and Im(R) denotes the range space of R.\nAssumption 1. The communication graph has a spanning tree, and the leader is pinned to at least one root node.\nThe following theorems are used in this paper.\n1) Cayley-Hamilton Theorem [49]: The matrix exponential eAt with A \u2208 Rn\u00d7n can be written by\neAt = n\u22121\u2211 k=0 \u00b5k Ak (1)\n2) Binomial Theorem [50]: For a positive integer n, one has\n(x + y)n = n\u2211\nk=0\nCnk x n\u2212kyk, Cnk = n! (n \u2212 k)! k! (2)\nIII. Synchronization ofMulti-Agent Systems and Their Vulnerability to Adversarial Inputs\nIn this section, the adversarial inputs are first defined. Then, the standard synchronization control protocol is reviewed for homogeneous multi-agent systems. It is shown that the adversarial inputs launched on a single agent can snowball into a much larger and more catastrophic one.\nA. Adversarial Inputs\nThree different main sources of adversarial inputs that can affect the performance of the multi-agent systems are attacks, disturbances/noises and faults. Disturbance, noise and fault are mainly bounded signals that are caused unintentionally. Conversely, an attack can be intentionally designed to maximize the damage to the network. Attacks can be launched on agents (for example on sensors and actuators) or on the communication network. We only considered attacks on agents in this paper and, therefore, the following assumption is considered.\nAssumption 2. The communication network is secure.\nRemark 1. Although adversarial inputs on communication links are not considered here, as will be shown in the subsequent sections, our method provides resiliency to adversarial inputs on agents without removing the disrupted agents or compromising the network performance and with minimal extra cost for security. This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values. This can harm the network connectivity and prevent the network synchronization.\nRemark 2. Attacks on agents are more common than attacks on the communication links. They can be launched without tampering with the physical system, i.e., spoofing a global positioning system (GPS) of a vehicle or jamming the communication channel from the controller to the actuator. Moreover, an attack on agents can be more serious than those on links. For example, if an agent is fully compromised, it can act as an illegitimate leader and all its neighbors receive corrupted information. However, if an outgoing link is under attack, then only the neighbor connected by the infected link receives disrupted information. Remark 3. Note that, in general, the attacker signal has a limited energy, and therefore, it is reasonable to assume that it is an L2-gain signal. This is a commonplace assumption in the literature [32], [54].\nB. Vulnerability of the Standard Output Synchronization approach to Adversarial Inputs\nConsider N agents with identical dynamics given by\nx\u0307i = Axi + Bui + D\u03c9i yi = Cxi i = 1, . . . , n\n(3)\nwhere xi(t) \u2208 Rn, ui(t) \u2208 Rm, \u03c9i(t) \u2208 Rd and yi(t) \u2208 Rq are the state, control input, adversarial input and output of agent i, respectively. A, B, C and D are the drift, input, output and adversarial dynamics, respectively.\nAssumption 3. The system dynamic is marginally stable.\nAssumption 4. (A, B) is stabilizable.\nAssumption 5. The full state information of agents is available for feedback.\n3 Let the leader dynamics be defined as\n\u03b6\u03070 = S \u03b60 y0 = F\u03b60\n(4)\nwhere \u03b60(t) \u2208 Rn, y0(t) \u2208 Rq, S \u2208 Rn\u00d7n and F \u2208 Rq\u00d7n. Define the local output tracking error for agent i as\n\u03b4i = yi \u2212 y0 (5)\nDefine the local neighborhood state tracking error ei \u2208 Rn for agent i as [55]\nei = \u2211 j\u2208Ni ai j(x j \u2212 xi) + gi(\u03b60 \u2212 xi) (6)\nwhere gi > 0 is the pinning gain, and gi > 0 for at least one root node i. The standard distributed control protocol is then given by\nui = cKei (7)\nwhere c is a scalar coupling gain, and K is a design matrix gain. [55] shows how c and K can be designed by solving an Algebraic Riccati Equation (ARE) to assure synchronization of all agents to the leader. That is, to assure\n\u03b4i \u2192 0\u21d4 yi \u2192 y0 (8)\nRemark 4. For the homogeneous leader follower multi-agent system (3) and (4), state synchronization results in output synchronization. On the other hand, the state synchronization does not make sense for heterogeneous multi-agent systems (see (67)), as the dimension of the agents states might not be the same. In this paper, a unified approach is proposed for both homogeneous and heterogeneous multi-agent systems, and therefore, the output synchronization is considered.\nRemark 5. It was shown in [55] that in the absence of the adversarial input \u03c9i(t), if the controller ui(t) in (7) is designed to make the local neighborhood tracking error (6) zero, it guarantees that (8) is satisfied and, therefore, the synchronization problem is solved. However, in the following, we will show that if in the presence of adversarial inputs (6) goes to zero, it does not guarantee that (8) is satisfied.\nDefinition 1. In a graph, agent i is reachable from agent j if there is a directed path of any length from agent j to agent i.\nDefinition 2: An agent is called a disrupted agent, if it is directly under adversarial input. Otherwise, it is called an intact agent.\nFor simplicity, it is assumed S = A for the following Theorems 1 and 2.\nTheorem 1. (Susceptibility of Nodes to Adversarial Inputs) Consider the multi-agent system (3) under the adversarial input \u03c9i(t). Let the control input be designed as (7) such that\nAc = IN \u2297 A \u2212 c(L + G) \u2297 (BK) (9)\nis Hurwitz, which guarantees (8) when \u03c9i(t) = 0, \u2200i = 1, . . . ,N. Then, if \u03c9 j(t) , 0 for agent j, the synchronization error is nonzero for all intact agents reachable from the disrupted agent j.\nProof. Let the synchronization error be \u03b5i = xi \u2212 \u03b60. Then, the global synchronization error dynamics becomes\n\u03b5\u0307 = Ac\u03b5 + (IN \u2297 D)\u03c9 (10)\nwhere Ac is defined in (9). The solution to (10) is \u03b5(t) = eAct\u03b5(0) + \u222b t\n0 eAc(t\u2212\u03c4)(Dc\u03c9)d\u03c4, (11)\nwhere Dc = (IN \u2297 D). Using (1) and the fact that eAct \u2192 0 as t \u2192 \u221e, one has\n\u03b5(t)\u2192 \u222b t\n0 ( N\u22121\u2211 k=0 \u00b5mAkc)(Dc\u03c9)d\u03c4 (12)\nThe first term of Akc in (9) does not deal with the agent interactions as (IN \u2297 A)k = ( IN \u2297 Ak ) . For the second term of Akc, one has\n((L + G) \u2297 BK)k = (L + G)k \u2297 (BK)k (13)\nUsing (2) and (13), (12) can be written as\n\u03b5 (t)\u2192 N\u22121\u2211 k=0 k\u2211 m=0 \u222b t 0 \u00b5kh (k,m) (Dc\u03c9) d\u03c4 (14)\nwhere\nh(k,m) = (\u22121)mcmCkm(IN \u2297 Ak\u2212m)((L + G) \u2297 BK)m\n= (\u22121)mcmCkm((L + G)m \u2297 Ak\u2212m(BK)m) (15)\nThe last equality in (15) is obtained using Kronecker product property (A \u2297 B) (C \u2297 D) = (AC \u2297 BD) and (13). If m > 0 is the first integer, such that\nlmi j , [ (L + G)m ] i j , 0 (16)\nwhere []i j denotes the element (i, j) of a matrix, then agent i is reachable from agent j, and 0 < m < N \u2212 1 is the length of the shortest directed path from j to i.\nExploiting (14) and (15), the synchronization error for the agent i can be expressed as\n\u03b5i (t)\u2192 N\u2211\nj=1 N\u22121\u2211 k=0 k\u2211 m=1 (\u22121)mlmi jcmCkm \u222b t 0 \u00b5kAk\u2212m(BK)mv jd\u03c4\n(17)\nwhere v j = D\u03c9 j (18)\nSince agent j is under adversarial input, v j , 0. Moreover, lmi j , 0 if agent i is reachable from agent j. Therefore, it can be seen from (17) that \u03b5i(t) , 0. This completes the proof.\nLemma 1. [56] Let \u03a3 be a diagonal matrix with at least one nonzero positive element, and L be the Laplacian matrix. Then, (L + \u03a3) is nonsingular.\nTheorem 2. Consider the multi-agent system (3) and (4) with the control protocol (7). Assume that agent i is under an adversarial input \u03c9i that is generated by \u03c9\u0307i = \u0393\u03c9i where eigenvalues of \u0393 are a subset of eigenvalues of A. Then, the local neighborhood tracking error (6) is nonzero for disrupted agent i, i.e., ei , 0 and is zero for all other intact agents, i.e., e j = 0\u2200 j, j , i.\n4 Proof. Let L\u0304 be the graph Laplacian matrix of the entire network including the leader as the only root node and followers as non-root nodes. Then, it can be partitioned as\nL\u0304 = 0 0 . . . 0\u2212\u2206 L f  (19) where \u2206 \u2208 RN is a vector whose ith element is nonzero and indicates that the follower i is connected to the leader. L f is a matrix which indicates the interaction among leader and followers.\nThe global dynamic of the multi-agent system (3) in terms of the Laplacian matrix (19) can be written as\nx\u0307 = (IN+1 \u2297 A)x \u2212 c(L\u0304 \u2297 BK)x + (IN+1 \u2297 D)\u03c9\u0304 (20)\nwhere \u03c9\u0304 = [0, \u03c91, . . . , \u03c9N]T which 0 in \u03c9\u0304 indicates that the leader is a trusted node and is not under adversarial input. Since adversarial effects are considered on agents, for simplicity, assume Di = Bi. Then, (20) turns into\nx\u0307 = (IN+1 \u2297 A)x + (IN+1 \u2297 B) ( \u2212c(L\u0304 \u2297 K)x + \u03c9\u0304 ) (21)\nIt can be seen that if the second term of (21) tend to zero, i.e., \u03c9\u0304 \u2208 Im(c(L\u0304 \u2297 K)), then, the agent\u2019s dynamics becomes x\u0307i \u2192 Axi, which indicates their stability. Note that, \u03c9\u0304 \u2208 Im(c(L\u0304 \u2297K)) if there exists a nonzero vector xs such that\nc(L\u0304 \u2297 K)xs = \u03c9\u0304 (22)\nDefine xs = [xls, x f s]T , where xls and x f s are the steady states of the leader and followers, respectively.\nUsing (19), (22) becomes 0 0 . . . 0\u2212c\u2206 \u2297 K cL f \u2297 K  [ xlsx f s ] = [ 0 \u03c9 ] (23)\nSince the leader is not under adversarial input, x\u0307ls = Ax0. From (22), for the followers one has\n\u2212 c(\u2206 \u2297 K)xls + c(L f \u2297 K)x f s = \u03c9 (24)\nBased on Assumption 1, followers have at least one incoming link from the leader. On the other hand, L f captures the interaction between all followers as well as the incoming link from the leader. The former is a positive semi-definite Laplacian matrix and the latter can be considered as a diagonal matrix \u03a3 with at least one nonzero positive element added to it. Therefore, as stated in Lemma 1, L f is nonsingular and consequently the solution to (24) becomes\nx f s = (c(L f \u2297 K))\u22121(\u03c9 + c(\u2206 \u2297 K)xrs) (25)\nSince dynamics of the adversarial input are subset of the system dynamics A, therefore, for every \u03c9\u0304 there exists a nonzero vector xs such that (22) holds.\nNow, using (19), the global form of the state neighborhood tracking error (6) can be written as\ne = \u2212(L\u0304 \u2297 In)x (26)\nSince (22) is satisfied, one has\nc(L\u0304 \u2297 K)xs = \u03c9\u0304\u21d2 c(IN+1 \u2297 K)e = \u2212\u03c9\u0304 (27)\nor, equivalently cKei = \u2212\u03c9i (28)\nThe intact agents are not directly under adversarial inputs, i.e., \u03c9 j = 0 \u2200 j , i, and, therefore, (28) implies that the local neighborhood tracking error is zero for them, i.e., e j = 0. Moreover, \u03c9i , 0 for the disrupted agent i and (28) indicates that it\u2019s local neighborhood tracking error is nonzero. The proof is completed.\nRemark 6. Existing H\u221e disturbance attenuation techniques for multi-agent systems [40] aim to minimize the effect of the disturbance on the local neighborhood tracking error. However, as shown in Theorem 2, in the presence of a stealthy attack, the attacker can deceive agents by assuring that their local neighborhood tracking error is zero, even in the presence of the attack. Therefore, these approach do not work in this case. To overcome this, we propose a distributed control framework to prevent propagating the effects of the disrupted agent throughout the network. In this framework, the local H\u221e controller is then designed to further attenuate the effect of the adversarial input on the disrupted agent.\nThe following example verifies the results of Theorem 1 and Theorem 2 and necessitates designing resilient control protocols against adversarial inputs. The adversarial input is assumed as a stealthy attack that has the knowledge of the agent dynamics.\nExample 1. Consider 5 agents communicating with each other according to the communication graph depicted in Fig. 1.\nAgents are assumed scalar by dynamics as\nx\u0307i = ui, i = 1, . . . , 5 (29)\nwhere ui is defined in (7). In this example, the designing parameters are considered as k = 3.16 and c = 2. Let the leader dynamics be generated by\nx\u03070 = 0, x0(0) = 2 (30)\nand the control protocol given by (7) be used. Assume that a constant attack signal \u03c92 = 5 is injected into the actuator of Agent 2 at t = 10 sec. Then, the dynamic of Agent 2 becomes\nx\u03072 = u2 + \u03c92 (31)\nFigure 2 shows the performance of the multi-agent system (29). It can be seen that using the control protocol (7) agents\n5 0 5 10 15 20\nTime(s)\n-2\n-1\n0\n1\n2\n3\nLeader Agent 1 Compromised Agent 2 Agent 3 Agent 4 Agent 5\n(a)\n0 5 10 15 20\nTime(s)\n-10\n-5\n0\n5 e1 e2 e3 e4 e5\n(b)\n0 5 10 15 20\nTime(s)\n-1\n0\n1\n2\n3\n4 \u03b41 \u03b42 \u03b43 \u03b44 \u03b45\n(c)\nFig. 2: The multi-agent system (29) response when Agent 2 is under an adversarial input. (a) The agent\u2019s output. (b) The local neighborhood tracking error (6). (c) The tracking error (5).\nsynchronize to the leader before that attacker affects Agent 2. After injecting the attack signal \u03c92 into Agent 2, as shown in Fig. 2a, Agents 4 and 5 that have a path to Agent 2 do not synchronize to the leader. This complies with the result of Theorem 1. The final values of agents before and after the attack are shown in red in Fig. 1. From Fig. 2b, one can see that the local neighborhood tracking error (6) is zero for all agents except the disrupted Agent 2. This is consistent with the result of Theorem 2. It can be seen from Fig. 2c that although the local neighborhood error (6) goes to zero for all intact agents in the presence of the adversarial input, it does not guarantee that the synchronization error (5) converges to zero. This is stated in Remark 5. One can assume that the attack signal is removed after a finite time, which is greater than 20 sec in Fig. 2, to satisfy this condition.\nIV. Overall Structure of the Proposed Control Protocol\nIn this section, a control protocol is proposed that prevents the adversarial inputs effects on a disrupted agent from propagating throughout the network, and attenuates its effects on the disrupted agent itself.\nAccording to Theorem 1, as shown in Fig. 3, when the standard framework is used for synchronization in distributed multi-agent systems, the corrupted data caused by an adversarial input on the physical system of one agent is directly sent to the communication network. Therefore, its effect propagates across the network and affects intact agents.\nThe leader is the only agent that cannot be affected by an adversarial input on other agents. Any adversarial input on the leader, on the other hand, affects all other agents, based on Theorem 1. In this paper, we assume that the leader is not under any adversarial input.\nBased on this observation, we develop a distributed control framework that does not allow a compromised agent to\nFig. 3: Standard synchronization control protocol under adversarial input.\npropagate its corrupted data across the network. That is, in the event of an adversarial input on a portion of agents, the intact agents still operate normally and remain unaffected. Figure 4 shows the structure of the control framework to prevent adversarial input on one agent from propagating across the network. In this framework, only the leader communicates its actual sensory information and agents do not exchange their actual state information. They estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input.\nIt can be seen from Fig. 4 that if an agent is under the adversarial input, it only affects its own dynamics, and cannot affect the distributed observer state. The corrupted state of the disrupted agent is not transmitted to other intact agents. To further increase resiliency in the local level, agents design a local H\u221e controller to attenuate the effects of the adversarial input on their own dynamics.\nRemark 7. Using the proposed approach, attacks on sensors and actuators can be recovered without removing the com-\n6 promised agents, under a safe communication. However, the observer output for each agent can indeed be modified by attacks on communication links. Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer. These methods, however, require restrictive connectivity assumptions on the communication network. Separating attacks on nodes from attacks on edges helps recovering from the former without removing any number of compromised nodes (which harm the network connectivity) or making any restrictive assumption on network connectivity and the number of agents under attack.\nThe H\u221e controller in Fig. 4 is detailed in Fig. 5. In this figure, ri is the observer state that estimates the leader state for agent i and zi is the controlled or performance output defined such that\n\u2016zi\u20162 = (Cxi \u2212 Fri)T Qi(Cxi \u2212 Fri) + uTi Riui (32)\nwhere the weight matrices Qi and Ri are symmetric positive definite.\nThe bounded L2-gain condition for the agent i for any \u03c9i \u2208 L2 [0,\u221e) can be defined as\u222b \u221e\n0 e \u2212\u03b1it\u2016zi(t)\u20162dt\u222b \u221e\n0 e \u2212\u03b1it\u2016\u03c9i(t)\u20162dt\n6 \u03b32i (33)\nwhere \u03b1i is the discount factor and \u03b3i represents the attenuation level of the adversarial input \u03c9i.\nCondition (33) is satisfied, if the H\u221e norm of T\u03c9i,zi , i.e., the transfer function from the adversarial input \u03c9i to the performance output zi, is less than or equal to \u03b3i. The goal is now to design the control protocol (7) to satisfy (8) and (33).\nV. The Proposed Control Protocol Approach\nIn this section, a distributed control protocol is presented in a unified framework for both homogeneous and heterogeneous multi-agent systems. Resiliency of this approach to the adversarial input is shown.\nThe proposed dynamic control approach is r\u0307i = S ri + c  N\u2211 j=1 ai j(r j \u2212 ri) + gi(\u03b60 \u2212 ri) \nui = Kxixi + Kriri\n(34a)\n(34b)\nwhere ri is the observer state shown in Fig. 5, c is a coupling gain, Kxi and Kri are design control gains.\nDefine the error between the observer state for agent i and the leader state as an observation error\n\u03c3i = ri \u2212 \u03b60 (35)\nLemma 2. [56] Consider the dynamic observer defined in (34a). Then, \u03c3i \u2192 0 for all agents, if c > 1/2\u03bbmin(L + G).\nProblem 1. (H\u221e Output Synchronization Problem) Consider N identical systems in (3). Design a distributed dynamic controller (34) for agent i, i = 1, . . .N, such that\n1) The bounded L2-gain condition (33) is satisfied when \u03c9i , 0. 2) The output synchronization problem is solved, i.e., \u2016yi(t) \u2212 Fri(t)\u2016 \u2192 0, i = 1, . . . ,N when \u03c9i = 0.\nConsider the system dynamic (3) and the dynamic observer defined in (34a). Define the augmented system state as\nXi(t) = [ xi(t)T ri(t)T ]T \u2208 R2n (36)\nUsing (3),(32) and (34) together yields the augmented system\nX\u0307i = T Xi + B1ui + D1\u03c9i + E1\u03b7i Yi = C1Xi\n(37)\nwhere\nT = [\nA 0 0 S\n] , B1 = [ B 0 ] ,D1 = [ D 0 ] E1 = [ 0\ncIn\n] , C1 = [ C \u2212F ] (38) and\nYi = Cxi(t) \u2212 Fri(t) (39)\nand\n\u03b7i = N\u2211 j=1 ai j(r j \u2212 ri) + gi(\u03b60 \u2212 ri) (40)\nis the local neighborhood observer error for agent i. The performance output (32) in terms of the augmented state (36) becomes\n\u2016Zi\u20162 = XTi CT1 QiC1Xi + uTi Riui = YiT QiYi + uTi Riui (41)\nUsing (34) and (37), the control protocol for augmented system can be written as\nui = [Kxi Kri] Xi , KiXi (42)\nIn the following, it is shown that Problem 1 can be solved if the control protocol (42) is designed assuming that \u03b7i = 0 in (37). That is, the separation principle holds.\nTheorem 3. Consider the multi-agent system (3). Then, Problem 1 is solved, if the control protocol (34) is designed to guarantee that Yi in (37) approaches zero assuming \u03b7i = 0.\nProof. We first show that Part 2 of Problem 1 is satisfied, if the condition in the statement of the theorem holds.\nConsider the augmented system (37). Let \u03b7i = 0,\u2200i. Then, based on (40) one has \u03b7 = ((L + G) \u2297 In)\u03c3 = 0, where \u03c3 is the global observation error vector. Since (L+G) is a positive definite, this concludes that \u03c3 = 0. To complete the proof, one\n7 needs to show that Problem 1 is solved if the control protocol ui in (34) guarantees that Yi goes to zero when \u03c3i = 0.\nUsing (4), (34) and (35), the global observer error dynamic becomes\n\u03c3\u0307 = (IN \u2297 S \u2212 c(L + G) \u2297 In)\u03c3 = A\u03c3\u03c3 (43)\nLet Yi = 0, \u2200i. Then, there exists a zero-error invariant and attractive set \u2126 = { (x, \u03b6 0 ) \u2223\u2223\u2223\u2223 x = \u03c0(\u03b60)}.\nAs stated in Lemma 2, \u03c3 \u2192 0 and therefore, based on converse Lyapunov theorem [58], there exists a smooth positive definite function V(\u03c3) such that\nV\u0307\u03c3(\u03c3) 6 0 (44)\nDefine e\u0302 = [(y1 \u2212 Fr1)T , . . . , (yN \u2212 FrN)T ]T = [ YT1 , . . . ,Y T N ]T as the global synchronization error. That is\ne\u0302 = C\u0304X (45)\nwhere X = [XT1 , . . . , X T N] T and C\u0304 = diag(C1, . . . ,C1) with C1 defined in (38). Now, consider the Lyapunov-like function V(\u03c3, e\u0302) = V\u03c3(\u03c3) for the augmented system (37) and the global synchronization error defined in (45). Then, based on (44), one has\nV\u0307(\u03c3, e\u0302) 6 0 (46)\nBased on LaSalles invariance principle [58], as t \u2192 \u221e all trajectories of (37) and (43) converge to the largest invariant subset of points where V\u0307(\u03c3, e\u0302) = V\u0307\u03c3(\u03c3) = 0. Using (44), V\u0307\u03c3(\u03c3) = 0 if and only if \u03c3 = 0. On the other hand, it was shown that the system (37) has the invariant set \u2126 = { (x, \u03b6 0 ) \u2223\u2223\u2223\u2223 x = \u03c0(\u03b60)} when \u03c3 = 0. Since the tracking error is zero in the invariant set, the point (\u03c3, e\u0302) = (0, 0) is the largest invariant set for V\u0307(\u03c3, e\u0302) = 0. Since \u03c3 \u2192 0, using a similar procedure, one can also show that the L2-gain condition (Part 1 of Problem 1) also holds, if the controller is designed as stated in the theorem. The proof is completed.\nIn the following, we show how to design the control protocol (42) to solve Problem 1 based on results of Theorem 3. We assume for the following analysis that \u03b7i = 0. Based on (33), define the discounted performance function in terms of the augmented system (37) as\nJ(Xi, ui, \u03c9i) = \u222b \u221e\n0 e\u2212\u03b1i(\u03c4\u2212t)\n[ XTi C T 1 QiC1Xi\n+ uTi Riui \u2212 \u03b3i2\u03c9iT\u03c9i ] d\u03c4 (47)\nThe value function for linear systems is quadratic and therefore\nVi(Xi(t)) = J(Xi, ui, \u03c9i) = Xi(t)T PiXi(t) (48)\nThe corresponding Hamiltonian function is\nH (Xi, ui, \u03c9i) , YTi QiYi + u T i Riui \u2212 \u03b32i \u03c9Ti \u03c9i \u2212 \u03b1iVi\n+ ( dVi dXi )T (T Xi + B1ui + D1\u03c9i)\n(49)\nUsing (48) for the left-hand side of (47) and differentiating (47) along with the augmented system (38) gives the following Bellman equation\nH (Xi, ui, \u03c9i) = (T Xi + B1ui + D1\u03c9i)T PiXi+\nXTi Pi(T Xi + B1ui + D1\u03c9i) \u2212 \u03b1iXTi PiXi + YTi QiYi + uTi Riui \u2212 \u03b32i \u03c9Ti \u03c9i = 0\n(50)\nBy applying the stationary conditions [59] as \u2202Hi/\u2202ui = 0, \u2202Hi/\u2202\u03c9i = 0, the optimal control and the worst case adversarial input are\nu\u2217i = \u2212RiBT1 PiXi \u03c9\u2217i = 1 \u03b32i DT1 PiXi (51)\nSubstituting (51) into (50), results the following tracking game ARE\nH ( Xi, u\u2217i , \u03c9 \u2217 i ) = XTi [T T Pi + T Pi \u2212 \u03b1iPi+\nCT1 QiC1 \u2212 PiB1R\u22121i BT1 Pi + 1 \u03b32i PiD1DT1 Pi]Xi = 0 (52)\nThe following theorem shows that the control protocol (51) along with (52) solves the H\u221e output synchronization problem.\nTheorem 4. Consider the multi-agent system (3) under the adversarial input \u03c9i. Let the control protocol be defined as (34). Then, Problem 1 is solved if K\u2217i is designed as\nK\u2217i = \u2212R\u22121i BT1 Pi (53)\nwhere Pi > 0, Pi = PTi is a solution of\nT T Pi + PiT \u2212 \u03b1iPi + CT1 QiC1\u2212\nPiB1R\u22121i B T 1 Pi + 1 \u03b32i PiD1DT1 Pi = 0 (54)\nand\n\u03b1i 6 \u03b1 \u2217 i = 2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 (B1R\u22121i BT1 + 1\u03b32i D1DT1 )(CT1 QiC1) 1/2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 (55)\nwhere \u03b1\u2217i is the upper bound for \u03b1i.\nProof. The proof has two parts. Part1. For any Xi(t), ui(t) and \u03c9i(t), Hamiltonian function (49) can be expressed by\nH (Xi, ui, \u03c9i) = H(Xi, u\u2217i , \u03c9 \u2217 i ) + (ui \u2212 u\u2217i )T Ri(ui \u2212 u\u2217i ) \u2212 \u03b32i \u2225\u2225\u2225\u03c9i \u2212 \u03c9\u2217i \u2225\u2225\u22252 (56)\nBased on (54) and (52), H(Xi, u\u2217i , \u03c9 \u2217 i ) = 0. Lemma 2 shows that \u03c3i in (35) and consequently \u03b7i in (40) converges to zero. Therefore, based on Theorem 3, one needs to show that the proposed control protocol solves Problem 1 assuming that \u03b7i = 0 in (37). Using this fact and (56), (54) and\ndVi(Xi) dt = ( \u2202Vi dXi )T X\u0307i \u21d2\ndVi(Xi) dt = ( \u2202Vi dXi )T (T Xi + B1ui + D1\u03c9i)\n(57)\n8 (56) can be written as\ndVi (Xi) dt\n\u2212 \u03b1iVi (Xi) + XTi ( CT1 QiC1 ) X + uTi Riui\n\u2212\u03b32i \u03c9Ti \u03c9i = (ui \u2212 u\u2217i )T R(ui \u2212 u\u2217i ) \u2212 \u03b32i \u2225\u2225\u2225\u03c9i \u2212 \u03c9\u2217i \u2225\u2225\u22252 (58)\nSelecting ui = u\u2217i = K \u2217 i Xi, with K \u2217 i defined in (53), gives\ndVi (Xi) dt \u2212 \u03b1iVi (Xi) \u2212 \u03b32i \u03c9Ti \u03c9i+\nXTi ( CT1 QiC1 + (K \u2217 i ) T RiK\u2217i ) Xi = \u2212\u03b32i \u2225\u2225\u2225\u03c9i \u2212 \u03c9\u2217i \u2225\u2225\u22252 6 0 (59)\nMultiplying (59) by e\u2212\u03b1it yields\nd dt (e\u2212\u03b1itXTi PiXi)+\ne\u2212\u03b1it(XTi (C T 1 QiC1 + (K \u2217 i ) T RiK\u2217i )X \u2212 \u03b32i \u03c9Ti \u03c9i) 6 0 (60)\nIntegrating (60) yields\ne\u2212\u03b1iT V[Xi(T )] \u2212 V[Xi(0)]\n+ \u222b T 0 e\u2212\u03b1it(XTi (C T 1 QiC1 + (K \u2217 i )\nT RiK\u2217i )X \u2212 \u03b32i \u03c9Ti \u03c9i)dt 6 0 (61)\nSelecting Xi(0) = 0 and noting that nonegativity of Lyapunov function imply that e\u2212\u03b1iT V[Xi(T )] > 0 \u2200T , (61) can be written as \u222b T\n0 e\u2212\u03b1itXTi (C T 1 QiC1 + (K \u2217 i ) T RiK\u2217i )Xidt 6\n\u03b32i \u222b T 0 e\u2212\u03b1it\u03c9Ti \u03c9idt (62)\nfor all T . This gives the value function (47) and therefore L2-gain condition (33) and completes the proof of Part 1.\nPart 2. For the poof of Part 2, we first show that A + BKxi is Hurwitz. Define\nPi = [ Pi1 P i 2\nPi2 P i 3\n] (63)\nUsing (38) for the upper left hand side of the discounted game ARE (54), one has\nAT Pi1 + P i 1A \u2212 \u03b1iPi1 + CT QiC\u2212\nPi1BR \u22121 i B T Pi + 1 \u03b32i Pi1DD T Pi1 = 0\n(64)\nand the control gain Kxi becomes\nKxi = \u2212R\u22121i BT Pi1 (65)\nIt is shown in [21] that if (55) is satisfied, then A + BKxi is Hurwitz. Multiplying left and right hand side of (54) by XTi and Xi, respectively, one has\n2XTi (PiXi) \u2212 \u03b1iXTi (PiXi) + XTi CT1 QiC1Xi\u2212 (PiXi)T B1R\u22121i BT1 + 1\u03b32i D1DT1  (PiXi) = 0 (66) The rest of the proof is similar to [60], and therefore, is\nomitted.\nRemark 8. The control protocol in Theorem 4 can be also extended to solving Problem 1 for heterogeneous multi-agent systems. Consider the heterogeneous multi-agent system as\nx\u0307i = Aixi + Biui + Di\u03c9i yi = Cixi i = 1, . . . , n\n(67)\nwhere xi(t) \u2208 Rni , ui(t) \u2208 Rmi , \u03c9i(t) \u2208 Rdi and yi(t) \u2208 Rq are the state, control input, adversarial input and output of agent i, respectively. In the case of heterogeneous systems, the ARE in (54) and, consequently, the control gain (53) are different from one agent to another. That is, (54) and (53) in Theorem 4 become\nK\u2217i = \u2212R\u22121i BT1iPi (68)\nT Ti Pi + PiTi \u2212 \u03b1iPi + CT1iQiC1i \u2212 PiB1iR\u22121i BT1iPi\n+ 1 \u03b32i PiD1iDT1iPi = 0 (69)\nwhere Ti = [\nAi 0 0 S\n] , B1i = [ Bi 0 ] ,D1i = [ Di 0 ] ,C1i = [ Ci \u2212F ] and\n\u03b1i 6 \u03b1 \u2217 i = 2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 (B1iR\u22121i BT1i + 1\u03b32i D1iDT1i)(CT1iQiC1i) 1/2 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 (70)\nRemark 9. The advantage of the standard control protocol (7) is that it does not require the absolute state of each agent and only uses the relative state information of agents. However, as shown in this paper, it is vulnerable to adversarial inputs. Moreover, designing the control protocol (7) in an optimal manner by minimizing the performance function [48] as\nJ = \u222b \u221e\n0\n N\u2211\ni=1 i\u22121\u2211 j qi j[xi(t) \u2212 x j(t)]2 + N\u2211 i=1 riu2i (t)  (71) ends up with coupled AREs which are extremely difficult to solve, even if the complete knowledge of agents is known. By contrast, we show that the proposed control protocol is resilient to adversarial inputs and decouples the AREs for each agent. Moreover, we will show that the proposed control method does not need any knowledge of the agents dynamics.\nVI. Model Free Off-Policy RL for Solving Optimal Output Synchronization\nIn this section, an RL algorithm is proposed to solve Problem 1 online and without requiring any knowledge of the agents dynamics.\nThe off-policy RL allows to separate the behavior policy from the target policy for both control input and adversarial input. This brings the opportunity to design an independent adaptive controller as a behavior policy to generate data for learning of the target policy while assuring stability during learning without requiring any initial admissible policy. We leverage this and use an adaptive controller as a behavior policy as shown in Fig. 6. This controller does not need\n9 to have any knowledge of the agents dynamics and can generate data used for target policy to learn the optimal solution. The learning has two stages. In the first stage, the adaptive controller is applied to the system to generate data for learning. In the second stage, the target policy with a controller approximated from the adaptive controller on its convergence and reuse of the data obtained in the first stage to update as many policies as required until the optimal solution is found.\nA. Adaptive Controller Structure (Behavior Policy)\nIn this subsection, an adaptive controller for RL is presented. Existing RL methods require an initial admissible policy. However, to obtain an admissible policy, one needs to know partial knowledge of the system dynamics. By integrating the adaptive controller approach and off-policy RL here, no initial control policy is required.\nThe initial admissible policy for RL should satisfy the stability condition when \u03c9i = 0. Using this fact, consider the agent dynamic (3) becomes\nx\u0307i = Axi + Bui (72)\nDefine the tracking error as\nei = xi \u2212 ri (73)\nwhere ri is the observer state. The error dynamics is\ne\u0307i = x\u0307i \u2212 r\u0307i = Axi + Bui \u2212 r\u0307i. (74)\nDefine Lyapunov function as\nVi (ei) = 1 2 eTi Piei (75)\nThe defined Lyapunov function is radially unbounded such that\n\u2202Vi (ei) \u2202t = \u2202Vi (ei) \u2202ei (Axi + Bui \u2212 r\u0307i) 6 0\n\u21d2 eTi Pi (Axi + Bui \u2212 r\u0307i) 6 0 (76)\nNow, without loss of generality, the unknown terms in (76) are estimated using a linear weight neural networks as f (ei, xi) = eTi PiAxi = W \u2217T ai S a(ei, xi) g (ei) = eTi PiB = W \u2217T bi S b(ei)\nc (ei) = eTi Pi = W \u2217T ci S c(ei)\n(77)\nwhere f (ei, xi), g(ei), and c(ei) are the outputs of the neural network, xi and ei are its inputs. Wai, Wbi, and Wci are synaptic\nweights vectors and S a, S b, and S c are regressor matrices. The regressors are considered as sigmoid activated functions.\nTo determine the adaptive laws for the neural network weights, consider Lyapunov function candidate for the system (72) as\nLi = kiVi (ei) + 1 2 \u2223\u2223\u2223W\u0303ai\u2223\u2223\u22232 + 12 \u2223\u2223\u2223W\u0303bi\u2223\u2223\u22232 + 12 \u2223\u2223\u2223W\u0303ci\u2223\u2223\u22232 (78) where Vi (ei) is defined in (75) and W\u0303ai, W\u0303bi and W\u0303ci are the neural network weights errors as W\u0303ai = Wai\u2212W\u2217ai, W\u0303bi = Wbi\u2212 W\u2217bi, and W\u0303ci = Wci \u2212 W\u2217ci where W\u2217ai, W\u2217bi, and W\u2217ci are the optimal values of the neural network weights.\nDifferentiating (78) with respect to time, one has\nL\u0307i = kiV\u0307i(ei) + W\u0303TaiW\u0307ai + W\u0303 T biW\u0307bi + W\u0303 T ciW\u0307ci = kW\u2217Tai S a(ei, xi) + kiW \u2217T bi S b(ei)ui \u2212 kiW\u2217Tci S c(ei)r\u0307i +W\u0303TaiW\u0307ai + W\u0303 T biW\u0307bi + W\u0303 T ciW\u0307ci\n(79)\nSimilar to [61], one can show that L\u0307i 6 0 which indicates that the tracking error (73) converges to zero, if we design the following adaptation laws to update the neural network weights  W\u0307ai = \u2212kaiWai + kiS a(ei, xi) W\u0307bi = \u2212kbiWbi + kiS b(ei)ui W\u0307ci = \u2212kciWci \u2212 kiS c(ei)r\u0307i (80)\nand the control law as ui = \u2212 WTai S a(ei, xi) \u2212WTci S c(ei)r\u0307i + \u03c4i |ei|\u2223\u2223\u2223WTbi S b(ei)\u2223\u2223\u22232  (WTbi S b(ei))T (81) where \u03c4i is positive and bounded and is given by\n\u03c4i |ei| ki kai \u2223\u2223\u2223W\u2217ai\u2223\u2223\u22232 + kikbi \u2223\u2223\u2223W\u2217bi\u2223\u2223\u22232 + kikci \u2223\u2223\u2223W\u2217ci\u2223\u2223\u22232 It is shown in [61] that the control law (81) with updating laws (80) guarantee the uniform ultimate boundedness of the error ei. Moreover, [61] suggested a resetting method to keep away the value of\n\u2223\u2223\u2223WTbi S b(ei)\u2223\u2223\u2223 from zero. Not that one can use either off-policy policy iteration or offpolicy value iteration to solve the optimal control problem in hand. In both cases, during learning, the system needs to be stable to collect meaningful data for learning. That is, although value iteration does not require an admissible target policy, if the behavior policy is not admissible, the system can grow unbounded in a short time and learning will not take place. If off-policy policy iteration is used, one can approximate an appropriate initial admissible control gain from (81) and using least square to start the target policy with.\nB. Model-free off-policy RL for solving optimal output regulation\nIn order to find the optimal gain (54) without the requirement of the knowledge of the system dynamics, offpolicy RL algorithm [21] is used in this subsection. Offpolicy algorithm has two separate stages. In the first stage, the adaptive controller in previous section is applied to the system and the system information is recorded over the time interval T . Then, in the second stage, without requiring any\n10\nknowledge of the system dynamics, the information gathered in stage 1 is repeatedly used to find a sequence of updated policies uki and d k i converging to u \u2217 i and d \u2217 i . To this end, the system dynamics (50) is first written as\nX\u0307i = T kXi + B1 ( ui \u2212 uki ) + D1 ( di \u2212 dki ) (82)\nwhere T k = T + B1Kki \u2212DiKkwi. The uki = Kki Xi and dki = KkwiXi are the control and disturbance target policies, which evaluated and updated. The Bellman equation becomes\ne\u2212\u03b1i\u03b4tXi(t + \u03b4t)T Pki Xi (t + \u03b4t) \u2212 Xi(t)T Pki Xi (t)\n= \u222b t+\u03b4t t d d\u03c4 ( e\u2212\u03b1i(\u03c4\u2212t)XiT Pki Xi ) d\u03c4\n= \u2212 \u222b t+\u03b4t\nt e\u2212\u03b1i(\u03c4\u2212t)XTi\n( Q\u0303ki ) Xid\u03c4\n\u22122 \u222b t+\u03b4t\nt e\u2212\u03b1i(\u03c4\u2212t)\n( ui \u2212 Kki Xi )T RiKk+1i Xid\u03c4\n+2 \u222b t+\u03b4t\nt e\u2212\u03b1(\u03c4\u2212t)\n( di \u2212 KkwiXi )T \u03b32i K k+1 wi Xid\u03c4\n(83)\nwhere Q\u0303ki = Q\u0304i + K T i RiKi\u2212\u03b3i2KwiT Kwi. Note that uki and dki are not applied to the system. The behavior control policy ui is the control input which is applied to the system and the actual adversarial input di comes from an external source such as an attacker and is not under our control. The initial target policy can be approximated from (81) after its convergence. Note that (83) is a scalar equation, and can be solved using least-square method after collecting enough number of data samples from the system.\nThe model-free Algorithm 1 uses the Bellman equation (83) to solve the ARE equation (55) simultaneously and find the gain (54). In Algorithm 1, the control policy which is applied to the systems, i.e., ui, can be a fixed stabilizing policy. The data which is gathered by applying this fixed policy to the system is then used in (84) to find the matrix Pki and the improved policy uk+1i = K k+1 i Xi and the disturbance policy dk+1i = K k+1 wi Xi. This corresponds to an updated new policy ui = KiXi.\nVII. Simulation Results\nIn this section, two examples are provided to verify the effectiveness of the proposed control protocol. The communication graph shown in Fig. 1 is used.\nA. Example1.(Homogeneous multi-agent system)\nConsider 5 agents with dynamics given by x\u0307i = [\n0 \u22124 1 0\n] xi + [ 1 0 ] ui + [ 1 0 ] \u03c9i,\nyi = [1 0] xi , i = 1, ..., 5 (85)\nThe leader dynamics is x\u03070 = [\n0 \u22124 1 0\n] x0, y0 = [1 0] x0 (86)\nAlgorithm 1. Off-Policy RL Solution for H\u221e Optimal Output Synchronization Problem\nStage 1 (Data Gathering using Adaptive Controller): The adaptive controller is applied to the system to generate data for learning and collect required system information as state, control input and disturbance at N different sampling interval T .\nStage 2 (Reuse the gathered data sequentially to find an optimal policy iteratively): Given uki and d k i , use the collected information in Stage1 to solve the following Bellman equation for Pki and K k+1 i and K k+1 wi simultaneously.\ne\u2212\u03b1i\u03b4tXi(t + \u03b4t)T Pki Xi (t + \u03b4t) \u2212 Xi(t)T Pki Xi (t) = \u2212 \u222b t+\u03b4t\nt e\u2212\u03b1i(\u03c4\u2212t)XTi\n( Q\u0303ki ) Xid\u03c4\n\u22122 \u222b t+\u03b4t\nt e\u2212\u03b1i(\u03c4\u2212t)\n( ui \u2212 Kki Xi )T RiKk+1i Xid\u03c4\n+2 \u222b t+\u03b4t\nt e\u2212\u03b1(\u03c4\u2212t)\n( di \u2212 KkwiXi )T \u03b32i K k+1 wi Xid\u03c4\n(84)\nStop if convergence is achieved, otherwise set k = k + 1 and go to Stage 2.\nUsing (85) and (34), the augmented system dynamics (37) becomes\nX\u0307i =  0 \u22124 0 0 1 0 0 0 0 0 0 \u22124 0 0 1 0  Xi +  1 0 0 0  ui +  1 0 0 0 \u03c9i +  0 0 0 0 c 0 0 c  \u03b7i Yi = [ 1 0 \u22121 0 ] Xi\n(87) The design parameters are Qi = 100, Ri = 1, \u03b1i = 0.1 and, \u03b3i = 10 for all agents. The offline solution to the game ARE (54) and consequently the optimal control policy (51) are\nP\u2217i =  9.95 \u22120.47 \u22129.95 0.47 \u22120.47 35.17 0.47 \u221235.17 \u22129.95 0.47 9.95 \u22120.47 0.47 \u221235.17 \u22120.47 35.17  u\u2217i = [ 9.95 \u22120.47 \u22129.95 0.47 ] Xi\n(88)\nNote that the drift dynamics of the leader is considered the same as that of the followers for the standard approach to be considered as a benchmark. The proposed method, however, handles both heterogeneous and homogeneous systems and does not need the drift dynamics of the agents be equal to that of the leader. Figure 7 shows that without any adversarial inputs, agents converge to the leader, if the standard control protocol (7) is applied.\nNow, assume that Agent 2 is affected by an adversarial input defined as\n\u03c92 = { t cos(5t) t 30 0 otherwise\n(89)\nThe agents\u2019 outputs is shown in Fig. 8, when the standard distributed controller is used. It is observed that the intact agents which have a path to the compromised agent do not synchronize to the leader. These results are consistent with Theorem 1.\nThe initial admissible policy required for the behavior policy stage is calculated using the control law (81) and the adaptive\n11\nlaws (80). The design parameters are considered as ki = 10, Ka = kb = 0.2, \u03c4i = 0.4.\nWe now implement the off-policy RL Algorithm 1 using the initial control policy provided by the adaptive controller (81). The reinforcement interval is selected as T = 0.1. As can be seen in Fig. 9, the control gain Ki and matrix Pi converge to their optimal values.\nFig. 10 shows the results for the case that the proposed controller in (34) is applied to the system, in the presence of the same adversarial input. One can see that the compromised agent is the only one that does not follow the leader. Moreover, the H\u221e controller attenuates the effect of the adversarial input on the disrupted agent which can be seen by comparing the deviation level of the compromised agent state from its desired value in Figs. 8 and 10.\nNow, consider the adversarial input (89) has a limited energy and it\u2019s effect is over on t = 70 sec. In Fig. 11 the\nvulnerability of the standard controller and the performance of the proposed approach are shown. Although after removing the adversarial input agents converge to the leader, the effects of the adversarial input for a short period of time on the network performance cannot be neglected.\nB. Example2.(Heterogeneous multi-agent system)\nConsider a group of the heterogeneous followers with dynamics as A1,3,5 = [ \u22120.3 \u22122 0.1 \u22120.2 ] , B1,3,5 = [ 1.8 0.9 ] C1,3,5 = [ \u22120.1 1.2\n]  A2,4 =  0 1 0 0 0 1 1 0 \u22122  , B2,4 =  6 0 1  C2,4 = [ 1 0 0 ] (90)\n12\nAssume the leader dynamic as S = [\n0 1 \u22121 0\n] , F = [ 1 0 ] (91)\nThe standard control protocol proposed in [62] is used to solve the synchronization problem for heterogeneous multiagent systems as  ui = Kiziz\u0307i = G\u03041izi + G\u03042ieiv (92) where\neiv =  yi \u2212 y0, gi , 0\u2211 j\u2208Ni 1 |Ni| (yi \u2212 y j)\nG\u03041i = [\nAi + BiKxi \u2212 L\u0304iCi BiKzi 0 G1\n] , G\u03042i = [ L\u0304i G2 ] (93)\nwhere |Ni| is the cardinality of the set Ni, L\u0304i is designed to make Ai \u2212 L\u0304iCi Hurwitz and Ki = [Kxi Kzi] is designed to stabilize [\nAi 0 G2Ci G1\n] + [ Bi 0 ] [ Kxi Kzi ] .\nDefine 1-copy internal model for the leader dynamic (91) as G1 = [\n0 \u22122 0.5 0\n] , G2 = [ 0 1 ] (94)\nThe design parameters for followers, i = 1, 3, 5 are considered as\nKi = [ 0.72 \u22129.21 2.61 \u221215.07 ]\nG\u0304i1 =  1.8385 \u221228.7771 4.6907 \u221227.1298 1.0818 \u221212.538 2.3453 \u221213.5649\n0 0 0 \u22122 0 0 0.5 0  G\u0304i2 =  8.5\n3.375 0 1\n , L\u0304i =  1.5 0.5 1  and for i = 2, 4 as\nKi = [ \u22120.78 \u22120.42 \u22120.13 \u22120.0021 \u22120.98 ]\nG\u0304i1 =  \u22126.1664 \u22121.5449 \u22120.8018 \u22120.0126 \u22125.8713 \u22120.5 0 1.0 0 0 \u22120.7777 \u22120.4241 \u22122.1336 \u22120.0021 \u22120.9786\n0 0 0 0 \u22122.0 0 0 0 \u22120.5 0\n\nG\u0304i2 =  1.5 0.5 1.0 0\n1.0\n , L\u0304i =  1.5 0.5 1  The initial conditions are selected randomly. Figure 12 shows the synchronization of all agents to the leader when there is no adversarial input on agents.\nNow, assume that Agent 2 is under an adversarial input as (89). Figure 13 shows the network performance when Agent 2 is under an adversarial input. It is shown that Agents 4 and 6 that have a path to the compromised agent 2 can not converge to the leader. This complies with the result of Theorem 1.\nNow, let the control protocol (34) be used. The control gain Ki can be obtained using (64) and (65). The design parameters are considered as \u03b3i = 10, \u03b1i = 0.1 and Qi = 100 for all agents. The control gain Ki and the ARE solution Pi for i = 1, 3, 5 are\nP1,3,5 =  0.097 \u22121.128 1.02 0.0093 \u22121.128 15.44 \u221212.99 \u22121.20\n1.02 \u221212.99 13.966 0.737 0.0093 \u22121.20 0.737 3.386  K1,3,5 = [ 0.8406 \u221211.8661 9.8559 1.0660 ] (95)\nand for i = 2, 4 one has\nP2,4 =  1.666 0.020 \u22120.0032 \u22121.667 \u22120.32 0.020 0.1098 0.5030 0.0086 0.0455 \u22120.0032 0.5030 0.0245 0.0148 0.0291 \u22121.667 0.0086 0.0148 1.955 0.0293 \u22120.32 0.0455 0.0291 0.0293 0.3184  K2,4 = [ \u22129.993 \u22120.1716 \u22120.0052 9.9887 0.1637\n] (96)\n13\nFigure 14 shows the system performance without any adversarial inputs using the control gains in (95) and (96) which complies with the result of Theorem 4.\nConsider Agent 2 is under the adversarial input (89). It can be seen from Fig. 15 that only the disrupted agent does not converge to the leader. Moreover, the proposed control protocol has attenuated the effects of the adversarial input (89) on the disrupted agent.\nNow, the RL Algorithm 1 is utilized to calculate the control gain. The process of finding the initial admissible policy is as same as Example 1. In Fig. 16 and Fig. 17 the convergence of the RL parameters for both two group of agents are shown.\nVIII. Conclusion It is first shown that existing standard synchronization control protocols are prone to adversarial inputs. A unified\nresilient model-free reinforcement learning based distributed H\u221e controller for leader-follower multi-agent systems is then presented for both homogeneous and heterogeneous multiagent systems under adversarial inputs. The effect of adversarial inputs on compromised agents is attenuated using a local H\u221e controller. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H\u221e control protocol. No knowledge of the agents dynamics are required and it is shown that the proposed RL-based H\u221e control protocol is resilient against adversarial inputs. In this paper, we assume that the full state information of agents is available and thus the state-feedback control protocol is used. The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]\u2013[65]. Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.\nReferences\n[1] R. Olfati-Saber, J. A. Fax, and R. M. Murray, \u201cConsensus and cooperation in networked multi-agent systems,\u201d Proceedings of the IEEE, vol. 95, pp. 215\u2013233, Jan 2007. [2] A. Jadbabaie, J. Lin, and A. S. Morse, \u201cCoordination of groups of mobile autonomous agents using nearest neighbor rules,\u201d IEEE Transactions on Automatic Control, vol. 48, pp. 988\u20131001, June 2003. [3] J. Fax and R. Murray, \u201cInformation flow and cooperative control of vehicle formations,\u201d IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1465\u20131476, 2004. cited By 2455. [4] W. Ren, R. W. Beard, and E. M. Atkins, \u201cInformation consensus in multivehicle cooperative control,\u201d IEEE Control Systems, vol. 27, pp. 71\u201382, April 2007. [5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol. 1(1). MIT press Cambridge, 1998. [6] W. B. Powell, Approximate Dynamic Programming: Solving the curses of dimensionality, vol. 703. John Wiley & Sons, 2007. [7] W. T. Miller, R. S. Sutton, and P. J. Werbos, A Menu of Designs for Reinforcement Learning Over Time, pp. 67\u201395. MIT Press, 1995. [8] F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, \u201cReinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers,\u201d IEEE Control Systems, vol. 32, no. 6, pp. 76\u2013105, 2012. [9] F. Y. Wang, H. Zhang, and D. Liu, \u201cAdaptive dynamic programming: An introduction,\u201d IEEE Computational Intelligence Magazine, vol. 4, pp. 39\u201347, May 2009. [10] K. Doya, \u201cReinforcement learning in continuous time and space,\u201d Neural computation, vol. 12, no. 1, pp. 219\u2013245, 2000. [11] D. Liu and Q. Wei, \u201cPolicy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 621\u2013634, March 2014.\n14\n[12] D. Liu, D. Wang, and H. Li, \u201cDecentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 418\u2013428, Feb 2014. [13] Y. Zhu, D. Zhao, and X. Li, \u201cIterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 28, pp. 714\u2013725, March 2017. [14] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-b. NaghibiSistani, \u201cReinforcement q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics,\u201d Automatica, vol. 50, no. 4, pp. 1167\u20131175, 2014. [15] Z. Ni, H. He, and J. Wen, \u201cAdaptive learning in tracking control based on the dual critic network design,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 24, pp. 913\u2013928, June 2013. [16] Y. Zhu, D. Zhao, and X. Li, \u201cUsing reinforcement learning techniques to solve continuous-time non-linear optimal tracking problem without system dynamics,\u201d IET Control Theory Applications, vol. 10, no. 12, pp. 1339\u20131347, 2016. [17] B. Kiumarsi and F. L. Lewis, \u201cActor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 140\u2013151, 2015. [18] K. G. Vamvoudakis and F. L. Lewis, \u201cOnline solution of nonlinear two-player zero-sum games using synchronous policy iteration,\u201d International Journal of Robust and Nonlinear Control, vol. 22, no. 13, pp. 1460\u20131483, 2012. [19] M. I. Abouheaf and F. L. Lewis, \u201cMulti-agent differential graphical games: Nash online adaptive learning solutions,\u201d in 52nd IEEE Conference on Decision and Control, pp. 5803\u20135809, Dec 2013. [20] L. Buoniu, R. B. hatska, and B. D. Schutter, \u201cA comprehensive survey of multiagent reinforcement learning,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, pp. 156\u2013172, March 2008. [21] H. Modares, F. L. Lewis, and Z. P. Jiang, \u201cH\u221e tracking control of completely unknown continuous-time systems via off-policy reinforcement learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 26, pp. 2550\u20132562, Oct 2015. [22] B. Luo, H. N. Wu, and T. Huang, \u201cOff-policy reinforcement learning for h\u221e control design,\u201d IEEE Transactions on Cybernetics, vol. 45, pp. 65\u2013 76, Jan 2015. [23] H. Zhang, Q. Wei, and D. Liu, \u201cAn iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games,\u201d Automatica, vol. 47, no. 1, pp. 207\u2013214, 2011. [24] H. Modares, F. L. Lewis, and M.-B. N. Sistani, \u201cOnline solution of nonquadratic two-player zero-sum games arising in the h control of constrained input systems,\u201d International Journal of Adaptive Control and Signal Processing, vol. 28, no. 3-5, pp. 232\u2013254, 2014. [25] H. N. Wu and B. Luo, \u201cNeural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear control,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 23, pp. 1884\u20131895, Dec 2012. [26] D. Vrabie and F. Lewis, \u201cAdaptive dynamic programming for online solution of a zero-sum differential game,\u201d Journal of Control Theory and Applications, vol. 9, no. 3, pp. 353\u2013360, 2011. [27] H. Li, D. Liu, and D. Wang, \u201cIntegral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics,\u201d IEEE Transactions on Automation Science and Engineering, vol. 11, pp. 706\u2013714, July 2014. [28] K. G. Vamvoudakis, J. P. Hespanha, B. Sinopoli, and Y. Mo, \u201cDetection in adversarial environments,\u201d IEEE Transactions on Automatic Control, vol. 59, pp. 3209\u20133223, Dec 2014. [29] K. G. Vamvoudakis, L. R. G. Carrillo, and J. P. Hespanha, \u201cLearning consensus in adversarial environments,\u201d in SPIE Defense, Security, and Sensing, pp. 87410K\u201387410K, International Society for Optics and Photonics, 2013. [30] S. Amin, A. A. Ca\u0301rdenas, and S. S. Sastry, \u201cSafe and secure networked control systems under denial-of-service attacks,\u201d in International Workshop on Hybrid Systems: Computation and Control, pp. 31\u201345, Springer, 2009. [31] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, \u201cA secure control framework for resource-limited adversaries,\u201d Automatica, vol. 51, pp. 135\u2013148, 2015. [32] Z. Feng, G. Hu, and G. Wen, \u201cDistributed consensus tracking for multiagent systems under two types of attacks,\u201d International Journal of Robust and Nonlinear Control, vol. 26, no. 5, pp. 896\u2013918, 2016. [33] S. Amin, G. A. Schwartz, and S. S. Sastry, \u201cSecurity of interdependent and identical networked control systems,\u201d Automatica, vol. 49, no. 1, pp. 186\u2013192, 2013. [34] F. Pasqualetti, A. Bicchi, and F. Bullo, \u201cConsensus computation in unreliable networks: A system theoretic approach,\u201d IEEE Transactions on Automatic Control, vol. 57, pp. 90\u2013104, Jan 2012. [35] F. Pasqualetti, F. Drfler, and F. Bullo, \u201cAttack detection and identification in cyber-physical systems,\u201d IEEE Transactions on Automatic Control, vol. 58, pp. 2715\u20132729, Nov 2013. [36] S. Sundaram and C. N. Hadjicostis, \u201cDistributed function calculation via linear iterative strategies in the presence of malicious agents,\u201d IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495\u20131508, 2011. [37] Y. Mo, R. Chabukswar, and B. Sinopoli, \u201cDetecting integrity attacks on scada systems,\u201d IEEE Transactions on Control Systems Technology, vol. 22, pp. 1396\u20131407, July 2014. [38] H. He and J. Yan, \u201cCyber-physical attacks and defences in the smart grid: a survey,\u201d IET Cyber-Physical Systems: Theory & Applications, vol. 1, no. 1, pp. 13\u201327, 2016. [39] Y. Wu and X. He, \u201cSecure consensus control for multiagent systems with attacks and communication delays,\u201d IEEE/CAA Journal of Automatica Sinica, vol. 4, pp. 136\u2013142, Jan 2017. [40] Q. Jiao, H. Modares, F. L. Lewis, S. Xu, and L. Xie, \u201cDistributed l2-gain output-feedback control of homogeneous and heterogeneous systems,\u201d Automatica, vol. 71, pp. 361\u2013368, 2016. [41] J. Qin, Q. Ma, W. X. Zheng, H. Gao, and Y. Kang, \u201cRobust h-infinity group consensus for interacting clusters of integrator agents,\u201d IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017. [42] H. Hong, W. Yu, G. Wen, and X. Yu, \u201cDistributed robust fixedtime consensus for nonlinear and disturbed multiagent systems,\u201d IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. PP, no. 99, pp. 1\u201310, 2016. [43] I. Saboori and K. Khorasani, \u201cConsensus achievement of multi-agent systems with directed and switching topology networks,\u201d IEEE Transactions on Automatic Control, vol. 59, pp. 3104\u20133109, Nov 2014. [44] J. Wang, Z. Duan, Z. Li, and G. Wen, \u201cDistributed h and h 2 consensus control in directed networks,\u201d IET Control Theory & Applications, vol. 8, no. 3, pp. 193\u2013201, 2013. [45] Z. Li, Z. Duan, and G. Chen, \u201cOn h\u221e and h2 performance regions of multi-agent systems,\u201d Automatica, vol. 47, no. 4, pp. 797\u2013803, 2011. [46] P. Lin, Y. Jia, and L. Li, \u201cDistributed robust h consensus control in directed networks of agents with time-delay,\u201d Systems & Control Letters, vol. 57, no. 8, pp. 643\u2013653, 2008. [47] Q. Jiao, H. Modares, S. Xu, F. L. Lewis, and K. G. Vamvoudakis, \u201cDisturbance rejection of multi-agent systems: A reinforcement learning differential game approach,\u201d in 2015 American Control Conference (ACC), pp. 737\u2013742, July 2015. [48] Y. Cao and W. Ren, \u201cOptimal linear-consensus algorithms: An lqr perspective,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, pp. 819\u2013830, June 2010. [49] G. Birkhoff and S. Lane, A Survey of Modern Algebra. AKP classics, Taylor & Francis, 1977. [50] M. Abramowitz and I. A. Stegun, Handbook of mathematical functions: with formulas, graphs, and mathematical tables, vol. 55. Courier Corporation, 1964. [51] J. R. Klotz, A. Parikh, T. H. Cheng, and W. E. Dixon, \u201cDecentralized synchronization of uncertain nonlinear systems with a reputation algorithm,\u201d IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1\u20131, 2016. [52] H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram, \u201cResilient asymptotic consensus in robust networks,\u201d IEEE Journal on Selected Areas in Communications, vol. 31, pp. 766\u2013781, April 2013. [53] Z. Feng and G. Hu, \u201cDistributed secure average consensus for linear multi-agent systems under dos attacks,\u201d in 2017 American Control Conference (ACC), pp. 2261\u20132266, May 2017. [54] H. J. LeBlanc and X. D. Koutsoukos, \u201cResilient synchronization in robust networked multi-agent systems,\u201d in Proceedings of the 16th International Conference on Hybrid Systems: Computation and Control, HSCC \u201913, (New York, NY, USA), pp. 21\u201330, ACM, 2013. [55] H. Zhang, F. L. Lewis, and A. Das, \u201cOptimal design for synchronization of cooperative systems: State feedback, observer and output feedback,\u201d IEEE Transactions on Automatic Control, vol. 56, pp. 1948\u20131952, Aug 2011. [56] F. L. Lewis, H. Zhang, K. Hengster-Movric, and A. Das, Cooperative control of multi-agent systems: optimal and adaptive design approaches. Springer Science & Business Media, 2013.\n15\n[57] W. Zeng and M. Y. Chow, \u201cResilient distributed control in the presence of misbehaving agents in networked control systems,\u201d IEEE Transactions on Cybernetics, vol. 44, pp. 2038\u20132049, Nov 2014. [58] A. Isidori, Nonlinear control systems. Springer Science & Business Media, 2013. [59] F. L. Lewis, D. Veabie, and V. L. Syrmos, Optimal control. John Wiley & Sons, 2012. [60] H. Modares, S. P. Nageshrao, G. A. D. Lopes, R. Babus\u030cka, and F. L. Lewis, \u201cOptimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning,\u201d Automatica, vol. 71, pp. 334\u2013341, 2016. [61] G. A. Rovithakis, \u201cStable adaptive neuro-control design via lyapunov function derivative estimation,\u201d Automatica, vol. 37, no. 8, pp. 1213\u2013 1221, 2001. [62] X. Wang, Y. Hong, J. Huang, and Z. P. Jiang, \u201cA distributed control approach to a robust output regulation problem for multi-agent linear systems,\u201d IEEE Transactions on Automatic Control, vol. 55, pp. 2891\u2013 2895, Dec 2010. [63] Y. Shoukry and P. Tabuada, \u201cEvent-triggered projected luenberger observer for linear systems under sparse sensor attacks,\u201d in 53rd IEEE Conference on Decision and Control, pp. 3548\u20133553, Dec 2014. [64] C. Lee, H. Shim, and Y. Eun, \u201cSecure and robust state estimation under sensor attacks, measurement noises, and process disturbances: Observerbased combinatorial approach,\u201d in 2015 European Control Conference (ECC), pp. 1872\u20131877, July 2015. [65] Y. Shoukry, P. Nuzzo, A. Puggelli, A. L. Sangiovanni-Vincentelli, S. A. Seshia, and P. Tabuada, \u201cSecure state estimation for cyber physical systems under sensor attacks: A satisfiability modulo theory approach,\u201d IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017. [66] Y. Shoukry and P. Tabuada, \u201cEvent-triggered state observers for sparse sensor noise/attacks,\u201d IEEE Transactions on Automatic Control, vol. 61, pp. 2079\u20132091, Aug 2016. [67] W. P. M. H. Heemels, K. H. Johansson, and P. Tabuada, \u201cAn introduction to event-triggered and self-triggered control,\u201d in 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 3270\u20133285, Dec 2012."}], "references": [{"title": "Consensus and cooperation in networked multi-agent systems", "author": ["R. Olfati-Saber", "J.A. Fax", "R.M. Murray"], "venue": "Proceedings of the IEEE, vol. 95, pp. 215\u2013233, Jan 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Coordination of groups of mobile autonomous agents using nearest neighbor rules", "author": ["A. Jadbabaie", "J. Lin", "A.S. Morse"], "venue": "IEEE Transactions on Automatic Control, vol. 48, pp. 988\u20131001, June 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Information flow and cooperative control of vehicle formations", "author": ["J. Fax", "R. Murray"], "venue": "IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1465\u20131476, 2004. cited By 2455.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Information consensus in multivehicle cooperative control", "author": ["W. Ren", "R.W. Beard", "E.M. Atkins"], "venue": "IEEE Control Systems, vol. 27, pp. 71\u201382, April 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality, vol. 703", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A Menu of Designs for Reinforcement Learning Over Time, pp. 67\u201395", "author": ["W.T. Miller", "R.S. Sutton", "P.J. Werbos"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers", "author": ["F.L. Lewis", "D. Vrabie", "K.G. Vamvoudakis"], "venue": "IEEE Control Systems, vol. 32, no. 6, pp. 76\u2013105, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive dynamic programming: An introduction", "author": ["F.Y. Wang", "H. Zhang", "D. Liu"], "venue": "IEEE Computational Intelligence Magazine, vol. 4, pp. 39\u201347, May 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning in continuous time and space", "author": ["K. Doya"], "venue": "Neural computation, vol. 12, no. 1, pp. 219\u2013245, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems", "author": ["D. Liu", "Q. Wei"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 621\u2013634, March 2014.  14", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Decentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach", "author": ["D. Liu", "D. Wang", "H. Li"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 418\u2013428, Feb 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Iterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data", "author": ["Y. Zhu", "D. Zhao", "X. Li"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, pp. 714\u2013725, March 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics", "author": ["B. Kiumarsi", "F.L. Lewis", "H. Modares", "A. Karimpour", "M.-b. Naghibi- Sistani"], "venue": "Automatica, vol. 50, no. 4, pp. 1167\u20131175, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive learning in tracking control based on the dual critic network design", "author": ["Z. Ni", "H. He", "J. Wen"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, pp. 913\u2013928, June 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Using reinforcement learning techniques to solve continuous-time non-linear optimal tracking problem without system dynamics", "author": ["Y. Zhu", "D. Zhao", "X. Li"], "venue": "IET Control Theory Applications, vol. 10, no. 12, pp. 1339\u20131347, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Actor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems", "author": ["B. Kiumarsi", "F.L. Lewis"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 140\u2013151, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Online solution of nonlinear two-player zero-sum games using synchronous policy iteration", "author": ["K.G. Vamvoudakis", "F.L. Lewis"], "venue": "International Journal of Robust and Nonlinear Control, vol. 22, no. 13, pp. 1460\u20131483, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-agent differential graphical games: Nash online adaptive learning solutions", "author": ["M.I. Abouheaf", "F.L. Lewis"], "venue": "52nd IEEE Conference on Decision and Control, pp. 5803\u20135809, Dec 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Buoniu", "R.B. hatska", "B.D. Schutter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, pp. 156\u2013172, March 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "H\u221e tracking control of completely unknown continuous-time systems via off-policy reinforcement learning", "author": ["H. Modares", "F.L. Lewis", "Z.P. Jiang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, pp. 2550\u20132562, Oct 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Off-policy reinforcement learning for h\u221e control design", "author": ["B. Luo", "H.N. Wu", "T. Huang"], "venue": "IEEE Transactions on Cybernetics, vol. 45, pp. 65\u2013 76, Jan 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "An iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games", "author": ["H. Zhang", "Q. Wei", "D. Liu"], "venue": "Automatica, vol. 47, no. 1, pp. 207\u2013214, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Online solution of nonquadratic two-player zero-sum games arising in the h control of constrained input systems", "author": ["H. Modares", "F.L. Lewis", "M.-B.N. Sistani"], "venue": "International Journal of Adaptive Control and Signal Processing, vol. 28, no. 3-5, pp. 232\u2013254, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear control", "author": ["H.N. Wu", "B. Luo"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, pp. 1884\u20131895, Dec 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1884}, {"title": "Adaptive dynamic programming for online solution of a zero-sum differential game", "author": ["D. Vrabie", "F. Lewis"], "venue": "Journal of Control Theory and Applications, vol. 9, no. 3, pp. 353\u2013360, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Integral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics", "author": ["H. Li", "D. Liu", "D. Wang"], "venue": "IEEE Transactions on Automation Science and Engineering, vol. 11, pp. 706\u2013714, July 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection in adversarial environments", "author": ["K.G. Vamvoudakis", "J.P. Hespanha", "B. Sinopoli", "Y. Mo"], "venue": "IEEE Transactions on Automatic Control, vol. 59, pp. 3209\u20133223, Dec 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning consensus in adversarial environments", "author": ["K.G. Vamvoudakis", "L.R.G. Carrillo", "J.P. Hespanha"], "venue": "SPIE Defense, Security, and Sensing, pp. 87410K\u201387410K, International Society for Optics and Photonics, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Safe and secure networked control systems under denial-of-service attacks", "author": ["S. Amin", "A.A. C\u00e1rdenas", "S.S. Sastry"], "venue": "International Workshop on Hybrid Systems: Computation and Control, pp. 31\u201345, Springer, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A secure control framework for resource-limited adversaries", "author": ["A. Teixeira", "I. Shames", "H. Sandberg", "K.H. Johansson"], "venue": "Automatica, vol. 51, pp. 135\u2013148, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed consensus tracking for multiagent systems under two types of attacks", "author": ["Z. Feng", "G. Hu", "G. Wen"], "venue": "International Journal of Robust and Nonlinear Control, vol. 26, no. 5, pp. 896\u2013918, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Security of interdependent and identical networked control systems", "author": ["S. Amin", "G.A. Schwartz", "S.S. Sastry"], "venue": "Automatica, vol. 49, no. 1, pp. 186\u2013192, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Consensus computation in unreliable networks: A system theoretic approach", "author": ["F. Pasqualetti", "A. Bicchi", "F. Bullo"], "venue": "IEEE Transactions on Automatic Control, vol. 57, pp. 90\u2013104, Jan 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Attack detection and identification in cyber-physical systems", "author": ["F. Pasqualetti", "F. Drfler", "F. Bullo"], "venue": "IEEE Transactions on Automatic Control, vol. 58, pp. 2715\u20132729, Nov 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed function calculation via linear iterative strategies in the presence of malicious agents", "author": ["S. Sundaram", "C.N. Hadjicostis"], "venue": "IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495\u20131508, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting integrity attacks on scada systems", "author": ["Y. Mo", "R. Chabukswar", "B. Sinopoli"], "venue": "IEEE Transactions on Control Systems Technology, vol. 22, pp. 1396\u20131407, July 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Cyber-physical attacks and defences in the smart grid: a survey", "author": ["H. He", "J. Yan"], "venue": "IET Cyber-Physical Systems: Theory & Applications, vol. 1, no. 1, pp. 13\u201327, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Secure consensus control for multiagent systems with attacks and communication delays", "author": ["Y. Wu", "X. He"], "venue": "IEEE/CAA Journal of Automatica Sinica, vol. 4, pp. 136\u2013142, Jan 2017.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed l2-gain output-feedback control of homogeneous and heterogeneous systems", "author": ["Q. Jiao", "H. Modares", "F.L. Lewis", "S. Xu", "L. Xie"], "venue": "Automatica, vol. 71, pp. 361\u2013368, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust h-infinity group consensus for interacting clusters of integrator agents", "author": ["J. Qin", "Q. Ma", "W.X. Zheng", "H. Gao", "Y. Kang"], "venue": "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed robust fixedtime consensus for nonlinear and disturbed multiagent systems", "author": ["H. Hong", "W. Yu", "G. Wen", "X. Yu"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. PP, no. 99, pp. 1\u201310, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Consensus achievement of multi-agent systems with directed and switching topology networks", "author": ["I. Saboori", "K. Khorasani"], "venue": "IEEE Transactions on Automatic Control, vol. 59, pp. 3104\u20133109, Nov 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed h and h 2 consensus control in directed networks", "author": ["J. Wang", "Z. Duan", "Z. Li", "G. Wen"], "venue": "IET Control Theory & Applications, vol. 8, no. 3, pp. 193\u2013201, 2013.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "On h\u221e and h2 performance regions of multi-agent systems", "author": ["Z. Li", "Z. Duan", "G. Chen"], "venue": "Automatica, vol. 47, no. 4, pp. 797\u2013803, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed robust h consensus control in directed networks of agents with time-delay", "author": ["P. Lin", "Y. Jia", "L. Li"], "venue": "Systems & Control Letters, vol. 57, no. 8, pp. 643\u2013653, 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Disturbance rejection of multi-agent systems: A reinforcement learning differential game approach", "author": ["Q. Jiao", "H. Modares", "S. Xu", "F.L. Lewis", "K.G. Vamvoudakis"], "venue": "2015 American Control Conference (ACC), pp. 737\u2013742, July 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal linear-consensus algorithms: An lqr perspective", "author": ["Y. Cao", "W. Ren"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, pp. 819\u2013830, June 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "A Survey of Modern Algebra", "author": ["G. Birkhoff", "S. Lane"], "venue": "AKP classics, Taylor & Francis,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1977}, {"title": "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, vol. 55", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Courier Corporation,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1964}, {"title": "Decentralized synchronization of uncertain nonlinear systems with a reputation algorithm", "author": ["J.R. Klotz", "A. Parikh", "T.H. Cheng", "W.E. Dixon"], "venue": "IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1\u20131, 2016.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Resilient asymptotic consensus in robust networks", "author": ["H.J. LeBlanc", "H. Zhang", "X. Koutsoukos", "S. Sundaram"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 31, pp. 766\u2013781, April 2013.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed secure average consensus for linear multi-agent systems under dos attacks", "author": ["Z. Feng", "G. Hu"], "venue": "2017 American Control Conference (ACC), pp. 2261\u20132266, May 2017.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2017}, {"title": "Resilient synchronization in robust networked multi-agent systems", "author": ["H.J. LeBlanc", "X.D. Koutsoukos"], "venue": "Proceedings of the 16th International Conference on Hybrid Systems: Computation and Control, HSCC \u201913, (New York, NY, USA), pp. 21\u201330, ACM, 2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal design for synchronization of cooperative systems: State feedback, observer and output feedback", "author": ["H. Zhang", "F.L. Lewis", "A. Das"], "venue": "IEEE Transactions on Automatic Control, vol. 56, pp. 1948\u20131952, Aug 2011.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1948}, {"title": "Cooperative control of multi-agent systems: optimal and adaptive design approaches", "author": ["F.L. Lewis", "H. Zhang", "K. Hengster-Movric", "A. Das"], "venue": "Springer Science & Business Media,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Resilient distributed control in the presence of misbehaving agents in networked control systems", "author": ["W. Zeng", "M.Y. Chow"], "venue": "IEEE Transactions on Cybernetics, vol. 44, pp. 2038\u20132049, Nov 2014.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear control systems", "author": ["A. Isidori"], "venue": "Springer Science & Business Media,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning", "author": ["H. Modares", "S.P. Nageshrao", "G.A.D. Lopes", "R. Babu\u0161ka", "F.L. Lewis"], "venue": "Automatica, vol. 71, pp. 334\u2013341, 2016.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Stable adaptive neuro-control design via lyapunov function derivative estimation", "author": ["G.A. Rovithakis"], "venue": "Automatica, vol. 37, no. 8, pp. 1213\u2013 1221, 2001.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2001}, {"title": "A distributed control approach to a robust output regulation problem for multi-agent linear systems", "author": ["X. Wang", "Y. Hong", "J. Huang", "Z.P. Jiang"], "venue": "IEEE Transactions on Automatic Control, vol. 55, pp. 2891\u2013 2895, Dec 2010.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Event-triggered projected luenberger observer for linear systems under sparse sensor attacks", "author": ["Y. Shoukry", "P. Tabuada"], "venue": "53rd IEEE Conference on Decision and Control, pp. 3548\u20133553, Dec 2014.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Secure and robust state estimation under sensor attacks, measurement noises, and process disturbances: Observerbased combinatorial approach", "author": ["C. Lee", "H. Shim", "Y. Eun"], "venue": "2015 European Control Conference (ECC), pp. 1872\u20131877, July 2015.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2015}, {"title": "Secure state estimation for cyber physical systems under sensor attacks: A satisfiability modulo theory approach", "author": ["Y. Shoukry", "P. Nuzzo", "A. Puggelli", "A.L. Sangiovanni-Vincentelli", "S.A. Seshia", "P. Tabuada"], "venue": "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1\u20131, 2017.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2017}, {"title": "Event-triggered state observers for sparse sensor noise/attacks", "author": ["Y. Shoukry", "P. Tabuada"], "venue": "IEEE Transactions on Automatic Control, vol. 61, pp. 2079\u20132091, Aug 2016.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to event-triggered and self-triggered control", "author": ["W.P.M.H. Heemels", "K.H. Johansson", "P. Tabuada"], "venue": "2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 3270\u20133285, Dec 2012.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]\u2013[4].", "startOffset": 295, "endOffset": 298}, {"referenceID": 3, "context": "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]\u2013[4].", "startOffset": 299, "endOffset": 302}, {"referenceID": 8, "context": "Reinforcement learning (RL) [5]\u2013[10], inspired by learning", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 15, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 248, "endOffset": 252}, {"referenceID": 18, "context": "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]\u2013[13] and tracking [14]\u2013[17] control problems and recently multi-agent systems [18]\u2013[20].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "RL-based H\u221e control is considered to attenuate the effect of disturbances in [21]\u2013[27], and to mitigate attacks in [28] for single-agent systems.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "Attacks on multi-agent systems have been investigated by several researchers [29]\u2013[39].", "startOffset": 77, "endOffset": 81}, {"referenceID": 37, "context": "Attacks on multi-agent systems have been investigated by several researchers [29]\u2013[39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "Besides, the H\u221e control of multi-agent systems is considered in [40]\u2013[46] to attenuate the effects of disturbances on agents.", "startOffset": 64, "endOffset": 68}, {"referenceID": 44, "context": "Besides, the H\u221e control of multi-agent systems is considered in [40]\u2013[46] to attenuate the effects of disturbances on agents.", "startOffset": 69, "endOffset": 73}, {"referenceID": 45, "context": "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.", "startOffset": 45, "endOffset": 49}, {"referenceID": 47, "context": "1) Cayley-Hamilton Theorem [49]: The matrix exponential eAt with A \u2208 Rn\u00d7n can be written by", "startOffset": 27, "endOffset": 31}, {"referenceID": 48, "context": "2) Binomial Theorem [50]: For a positive integer n, one has", "startOffset": 20, "endOffset": 24}, {"referenceID": 32, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 58, "endOffset": 62}, {"referenceID": 49, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 64, "endOffset": 68}, {"referenceID": 51, "context": "This is in contrast to existing attack mitigation methods [34], [51]\u2013[53] in which agents discard their neighbor information based on the discrepancy between their values.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "This is a commonplace assumption in the literature [32], [54].", "startOffset": 51, "endOffset": 55}, {"referenceID": 52, "context": "This is a commonplace assumption in the literature [32], [54].", "startOffset": 57, "endOffset": 61}, {"referenceID": 53, "context": "Define the local neighborhood state tracking error ei \u2208 Rn for agent i as [55]", "startOffset": 74, "endOffset": 78}, {"referenceID": 53, "context": "[55] shows how c and K can be designed by solving an Algebraic Riccati Equation (ARE) to assure synchronization of all agents to the leader.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "It was shown in [55] that in the absence of the adversarial input \u03c9i(t), if the controller ui(t) in (7) is designed to make the local neighborhood tracking error (6) zero, it guarantees that (8) is satisfied and, therefore, the synchronization problem is solved.", "startOffset": 16, "endOffset": 20}, {"referenceID": 54, "context": "[56] Let \u03a3 be a diagonal matrix with at least one nonzero positive element, and L be the Laplacian matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Existing H\u221e disturbance attenuation techniques for multi-agent systems [40] aim to minimize the effect of the disturbance on the local neighborhood tracking error.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 93, "endOffset": 97}, {"referenceID": 52, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 99, "endOffset": 103}, {"referenceID": 55, "context": "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "[56] Consider the dynamic observer defined in (34a).", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "As stated in Lemma 2, \u03c3 \u2192 0 and therefore, based on converse Lyapunov theorem [58], there exists a smooth positive definite function V(\u03c3) such that", "startOffset": 78, "endOffset": 82}, {"referenceID": 56, "context": "Based on LaSalles invariance principle [58], as t \u2192 \u221e all trajectories of (37) and (43) converge to the largest invariant subset of points where V\u0307(\u03c3, \u00ea) = V\u0307\u03c3(\u03c3) = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "It is shown in [21] that if (55) is satisfied, then A + BKxi is Hurwitz.", "startOffset": 15, "endOffset": 19}, {"referenceID": 57, "context": "The rest of the proof is similar to [60], and therefore, is omitted.", "startOffset": 36, "endOffset": 40}, {"referenceID": 46, "context": "Moreover, designing the control protocol (7) in an optimal manner by minimizing the performance function [48] as", "startOffset": 105, "endOffset": 109}, {"referenceID": 58, "context": "Similar to [61], one can show that L\u0307i 6 0 which indicates that the tracking error (73) converges to zero, if we design the following adaptation laws to update the neural network weights \uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \u1e86ai = \u2212kaiWai + kiS a(ei, xi) \u1e86bi = \u2212kbiWbi + kiS b(ei)ui \u1e86ci = \u2212kciWci \u2212 kiS c(ei)\u1e59i (80)", "startOffset": 11, "endOffset": 15}, {"referenceID": 58, "context": "It is shown in [61] that the control law (81) with updating laws (80) guarantee the uniform ultimate boundedness of the error ei.", "startOffset": 15, "endOffset": 19}, {"referenceID": 58, "context": "Moreover, [61] suggested a resetting method to keep away the value of \u2223\u2223\u2223WT bi S b(ei)\u2223\u2223\u2223 from zero.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "Model-free off-policy RL for solving optimal output regulation In order to find the optimal gain (54) without the requirement of the knowledge of the system dynamics, offpolicy RL algorithm [21] is used in this subsection.", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "\u1e8bi = [ 0 \u22124 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] \u03c9i,", "startOffset": 23, "endOffset": 30}, {"referenceID": 0, "context": "\u1e8bi = [ 0 \u22124 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] \u03c9i,", "startOffset": 36, "endOffset": 43}, {"referenceID": 0, "context": "yi = [1 0] xi , i = 1, .", "startOffset": 5, "endOffset": 10}, {"referenceID": 0, "context": "\u1e8b0 = [ 0 \u22124 1 0 ] x0, y0 = [1 0] x0 (86) Algorithm 1.", "startOffset": 27, "endOffset": 32}, {"referenceID": 0, "context": "C2,4 = [ 1 0 0 ] (90)", "startOffset": 7, "endOffset": 16}, {"referenceID": 0, "context": "S = [ 0 1 \u22121 0 ] , F = [ 1 0 ] (91)", "startOffset": 23, "endOffset": 30}, {"referenceID": 59, "context": "The standard control protocol proposed in [62] is used to solve the synchronization problem for heterogeneous multiagent systems as \uf8f1\uf8f4\uf8f2\uf8f4\uf8f3 ui = Kizi \u017ci = \u1e201izi + \u1e202ieiv (92)", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "5 0 ] , G2 = [ 0 1 ] (94)", "startOffset": 13, "endOffset": 20}, {"referenceID": 60, "context": "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]\u2013[65].", "startOffset": 181, "endOffset": 185}, {"referenceID": 62, "context": "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]\u2013[65].", "startOffset": 186, "endOffset": 190}, {"referenceID": 63, "context": "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.", "startOffset": 32, "endOffset": 36}, {"referenceID": 64, "context": "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.", "startOffset": 38, "endOffset": 42}], "year": 2017, "abstractText": "This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leaderfollower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents. To alleviate this problem, a unified RL-based distributed control frameworks is developed for both homogeneous and heterogeneous multi-agent systems to prevent corrupted sensory data from propagating across the network. To this end, only the leader communicates its actual sensory information and other agents estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input. To further improve resiliency, distributed H\u221e control protocols are designed to attenuate the effect of the adversarial inputs on the compromised agent itself. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H\u221e control problem. No knowledge of the agent\u2019s dynamics is required and it is shown that the proposed RL-based H\u221e control protocol is resilient against adversarial inputs.", "creator": "LaTeX with hyperref package"}}}