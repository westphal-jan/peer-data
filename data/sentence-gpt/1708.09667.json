{"id": "1708.09667", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Video Captioning with Guidance of Multimodal Latent Topics", "abstract": "The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&amp;M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better than those of non-supervised topics. The present paper is based on a theoretical framework of m&amp;M TGM, a non-supervised (neonic) topic in which a subject is associated with a content distribution that represents a content distribution with the content distribution associated with a content distribution, which is represented by a content distribution that is associated with a content distribution that is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a content distribution is associated with a content distribution where a", "histories": [["v1", "Thu, 31 Aug 2017 11:18:28 GMT  (3559kb,D)", "http://arxiv.org/abs/1708.09667v1", "To appear in ACM Multimedia 2017"], ["v2", "Sat, 2 Sep 2017 15:34:44 GMT  (3559kb,D)", "http://arxiv.org/abs/1708.09667v2", "ACM Multimedia 2017"]], "COMMENTS": "To appear in ACM Multimedia 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["shizhe chen", "jia chen", "qin jin", "alexander hauptmann"], "accepted": false, "id": "1708.09667"}, "pdf": {"name": "1708.09667.pdf", "metadata": {"source": "META", "title": "Video Captioning with Guidance of Multimodal Latent Topics", "authors": ["Shizhe Chen", "Jia Chen", "Qin Jin", "Alexander Hauptmann"], "emails": ["cszhe1@ruc.edu.cn", "jiac@cs.cmu.edu", "qjin@ruc.edu.cn", "alex@cs.cmu.edu"], "sections": [{"heading": null, "text": "Keywords Video Captioning; Multimodal; Latent Topics; Multi-task"}, {"heading": "1 Introduction", "text": "Videos have become increasingly popular on the Internet, for example, hundreds of hours of video contents are uploaded on YouTube every minute. It is impossible to watch these overwhelming amounts of videos, therefore, automatic techniques to search and analyze video contents are highly desired. Generating natural language descriptions for video contents (a.k.a. video captioning) is one of such important techniques for this challenge. It can bene t a wide range of applications such as assisting the visually impaired people and improving the quality of online video retrieval.\nAlthough there have been signi cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents. e open-domain videos contain a broad range of topics,\n\u2217corresponding author.\nsuch as sports, music, food and so on, which results in very di erent vocabularies and expression styles to describe video contents across topics. For example, sports terms frequently occur in the sports topic while the typical words in the food topic are about cooking and ingredients. e key contents in descriptions for the sports topic are o en the action verbs, while they are object nouns for the food topic and descriptive adjectives for the people topic. us, being aware of the topic information can dramatically narrow down general sentence distributions and enable the caption model to focus on the discriminative video contents under the topic, such as the detailed actions in the sports topic.\nHowever, most of the existing caption models ignore the topic information and mainly try to maximize the overall likelihood for videos in all topics, which have a tendency to seek the most common mode in training sentences as indicated by Lee et al. [20]. Such models not only are prone to generate plain descriptions without details, but also unable to distinguish confusing concepts in a topic. Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning. First, there are no direct topic representations in common video caption datasets, so how can we construct and predict the latent topics of videos? Second, what is an e ective and e cient way to employ the latent topics to guide the caption model for generating topicoriented descriptions? And third, how to train the topic-guided caption model to achieve optimal captioning performance?\nIn this paper, we bring three innovations to deal with the above challenges. First, we propose a multimodal topic mining approach to discover latent video topics from video-sentence pairs. e multimodal latent topics are more semantically coherent and visually consistent than the expert-de ned or textual latent topics in previous work. We then use the teacher-student learning strategy to predict latent topics with multimodal video features. Second, in order to exploit the topic e ectively, we propose a novel topicaware language decoder, which implicitly functions as an ensemble of topic-speci c decoders for each topic, but is computationally more e cient and requires less training data. e predicted video topic automatically modi es weights in the decoder, enabling to generate topic-oriented video descriptions. ird, to optimize the topic-guided caption performance, we develop a multi-task learning architecture to jointly train the caption system with the topic prediction loss and sentence generation loss in an end-to-end manner. It can strengthen the coupling of the two parts to generate be er latent topics and sentence descriptions simultaneously. e overall model is called M&M TGM (Topic-Guided Model with Multimodal latent topic guidance and Multi-task learning). ar X iv :1\n70 8.\n09 66\n7v 1\n[ cs\n.C V\n] 3\n1 A\nug 2\n01 7\ne main contributions of this paper are as follows: \u2022 We present an uni ed framework M&M TGM for video caption-\ning, which can automatically discover and predict the underlying video topics and exploit the topic guidance to generate be er topic-oriented video descriptions. e topic prediction and topicaware sentence generation can be trained jointly with multi-task learning end-to-end. \u2022 e proposed M&M TGM requires no additional labelling for video topics and can exploit the limited video caption data e ectively. For the latent topic generation, we propose unsupervised multimodal topic mining for topic discovery and teacher-student learning for topic prediction. For the topic-oriented sentence generation, the topic-aware decoder shares data and parameters among di erent topics which is e cient in both computation and data. \u2022 Our proposed model is evaluated on two benchmark video caption datasets: MSR-VTT [42] and Youtube2Text [11]. Both quantitative and qualitative analysis show superior performances of using the multimodal latent topic guidance and multi-task learning strategy. e M&M TGM not only outperforms prior state-of-the-art methods, but also has be er generalization ability. e rest of the paper is organized as follows: Section 2 introduces the related work. Section 3 presents the problem formulation. e solution for M&M TGM is described in Section 4. Section 5 presents experimental results and analysis. Section 6 concludes the paper."}, {"heading": "2 Related Work", "text": "Image Captioning has a racted growing interests recently. Early works are mainly based on rule systems [17, 19] and suffer from generating exible and accurate descriptions. So more researches have been focusing on the encoder-decoder framework [2, 35] for caption generation. e deep convolutional neural networks (CNNs) [36] function as the encoder to transform image contents into dense vectors. en the decoder, typically the Long-short Term Memory [12] (LSTM), is utilized to generate sequential words conditioned on image features [41]. Xu et al. [43] propose the spatial a ention mechanism based on the basic encoder-decoder. You et al. [46] and Gan et al. [9] propose the semantic a ention model and semantic compositional networks (SCN) respectively to exploit detected concepts from the image. Our topic-aware language decoder is inspired by SCN but with di erent aims of producing topic-oriented video descriptions, which is more suitable for video captioning task as shown in Section 5.4.\nVideo Captioning is more challenging compared with image captioning due to the temporal dynamics, multiple modalities, and more diverse contents in videos. Yao et al. [45] propose the temporal a ention mechanism, and Pan et al. [25] utilize the hierarchical LSTM encoder to explore the temporal structures. Most previous works only focus on the visual modality [26, 40], but recently Jin et al. [13, 14] and Ramanishka et al. [29] have shown improvement of multimodal fusion for video captioning. To promote the video captioning research for open-domain videos, several large-scale videos with various topic are collected such as MSR-VTT [42] and TGIF [22]. Jin et al. [13] encode expert-de ned topics with multimodal features, which results in their winning of the MSR-VTT challenge [31]. Dong et al. [8] utilize the textual mined topics to\nlearn interpretable features. Shen et al. [33] train separate language decoders for each expert-de ned topic. Chen et al. [4] explore the guidance from textual minded topics to generate topic-aware sentences. In our work, we address the topic diversity challenge in open-domain videos and propose the novel M&M TGM model to jointly generate the latent video topics and topic-oriented video descriptions with multimodal features in a multi-task framework.\nLatent Topic Mining has been a long-standing problem. e latent Dirichlet allocation (LDA) proposed by Blei [3] is one of the most classic models to automatically inference the latent topics for textual documents. Doersch et al. [7] utilize a discriminative clustering approach to discover the visual topics. For multimodal data, Li et al. [21] apply the association rule mining algorithm on image-caption pairs to discover the multimodal topics. In our work, we take the multimodal topic mining perspective on videosentence pairs by a weighted multimodal clustering method to obtain semantically coherent and visually consistent latent topics."}, {"heading": "3 Problem Formulation", "text": "Our goal is to leverage the underlying video topic information to make the model more pro cient in using vocabularies and expressions within the topic when describing a video.\nSuppose we have a videoV along with a set ofNd textual descriptions Y = {y1, y2, ..., yNd }. Each y \u2208 Y is a sentence with Ns words, denoted as y = {w1,w2, ...,wNs }, where w is a word from vocabulary W. We encode the video contentV into a xed-dimensional video representation x. Conditioned on the multimodal video representation x, a caption decoder aims to generate the sentence y with probability\nPr(y|x) = Ns\u220f t=1 Pr(wt |x,w<t ) (1)\nwhere w<t denotes the sequential words before the time step t . We abbreviate this as conditional sentence probability. We use RNN as decoder and parameterize the probability Pr(wt |x,w<t ) by the recurrent units shared across time steps, which can be expressed as: [\nPr(wt |x,w<t ;\u0398d ) ht\n] = \u03c8d (ht\u22121,wt\u22121;\u0398d ) (2)\nwhere \u03c8d is the recurrent unit function, \u0398d are the parameters of the recurrent unit and ht is the hidden state of the recurrent unit at time step t . We de ne w0 as the start token and h0 as the initialized hidden state. For notation simplicity, we abbreviate eq (2) to Pr(wt |x,w<t ;\u0398d ) = \u03c8d (ht\u22121,wt\u22121;\u0398d ). Substituting this abbreviation to eq (1), we get:\nPr(y|x;\u0398d ) = Ns\u220f t=1 \u03c8d (ht\u22121,wt\u22121;\u0398d ) (3)\nAs mentioned earlier, videos in di erent topics have quite di erent vocabularies and expression styles in their video descriptions, and di erent topics can coexist in one video. Based on this observation, we introduce the latent topic variables {zi }i=1, ...,K , where K is the number of topics. en the conditional sentence probability, Pr(y|x) is actually the marginal probability of the joint sentence\nand topic distribution conditioned on video content: Pr(y |x) = \u2211\nz1, . . .,zK Pr(y, z1, . . . , zK |x) = \u2211\nz1, . . .,zK Pr(y |z1, . . . , zK , x)\ufe38 \ufe37\ufe37 \ufe38\ntopic-aware sentence distribution Pr(z1, . . . , zK |x)\ufe38 \ufe37\ufe37 \ufe38 latent topic distribution\n(4)\nIn the second step of eq (4), we factorize the joint distribution to latent topic distribution and topic-aware sentence distribution.\nWe come to parameterize the probability in eq (4). For latent topic distribution, we parameterize it by topic predictor \u03c8z as follows:\nPr(z1, . . . , zK |x) = \u03c8z (x;\u0398z ) (5)\nwhere \u0398z are parameters in the topic predictor. To parameterize the topic-aware sentence distribution by RNN, we need to introduce the topic-related parameter \u0398\u2032z in addition to \u0398d in eq (3) as now the probability we are modelling is also conditioned on topics z1, . . . , zK :\nPr(y |z1, . . . , zK , x;\u0398d , \u0398\u2032z ) = Ns\u220f t=1 Pr(wt |w<t , x, z1, . . . , zK ;\u0398d , \u0398\u2032z )\n= Ns\u220f t=1 \u03c8d (ht\u22121, wt\u22121, z1, . . . , zK ;\u0398d , \u0398\u2032z ) (6)\nPu ing all these together, we get the parameterization for topicaware conditional sentence probability as:\nPr(y |x) = Pr(y |x;\u0398d , \u0398\u2032z, \u0398z )\n= \u2211\nz1, . . .,zK\n\u03c8z (x;\u0398z ) Ns\u220f t=1 \u03c8d (ht\u22121, wt\u22121, z1, . . . , zK ;\u0398d , \u0398\u2032z ) (7)\nContrasting with conditional sentence probability parameterization in eq (3), we see that we have additional parameters \u0398z for topic prediction and \u0398d ,\u0398\u2032z for topic-aware sentence modelling in the parameterization of topic-aware conditional sentence probability.\ne standard loss function for caption generation task is to maximize the log probability of the conditional sentence probability:\nLcaption (y, y\u0303) = \u2212 Ns\u2211 t=1 \u2211 w\u0303t \u2208W \u03b4wt ,w\u0303t log Pr(w\u0303t |x, w<t ;\u0398d ) (8)\nwhere \u03b4wt ,w\u0303t is the indicator function. We could directly use this loss to train our topic-aware parameterized model in eq (4) as it is a special form of the general conditional sentence probability. However in training, it requires large amount of data for the model to discover latent topics from scratch and to learn topic-aware sentence distribution simultaneously. e training data of caption task is limited, and as the labelling cost of such training data is very high, the amount of the training data is not likely to grow very fast in the near future. To solve this problem, we introduce an auxiliary task, topic prediction, to guide the caption task. In this task, we leverage the existing topic mining approaches to generate a set of topics as the teacher topics z. e details of the topic mining approach are given in Section 4.2. We use the teacher topics to guide the latent topic learning by an additional topic loss function Ltopic (z, z\u0303) on the topic predictor \u03c8t (x;\u0398z ), which penalizes the latent topic predictions that are far away from the teacher z. e details ofdist function in Ltopic (z, z\u0303)will be discussed in section 4.1. e two losses are combined by the trading o hyper-parameter\n\u03bb \u2208 [0, 1): L(y, z, y\u0303, z\u0303) = (1 \u2212 \u03bb)Lcaption (y, y\u0303) + \u03bbLtopic (z, z\u0303) (9)\nLcaption (y, y\u0303) = \u2212 Ns\u2211 t=1 \u2211 w\u0303t \u2208W \u03b4wt ,w\u0303t log Pr(w\u0303t |x, w<t ;\u0398d , \u0398\u2032z, \u0398z )\n(10) Ltopic (z, z\u0303) = dist (z, z\u0303) (11)\nIn this way, we weave the video latent topics into video description generation as guidance."}, {"heading": "4 M&M Topic-Guided Model", "text": "In this section, we present our solutions for the topic-guided video captioning. Figure 1 illustrates the overall framework. Our proposed M&M TGM consists of three components: topic mining, topic predictor and topic-aware decoder, which is optimized by multi-task training. We will rst present the overall multi-task training scheme and then introduce the three modules in details."}, {"heading": "4.1 Multi-task Training Scheme", "text": "It is intractable to integrate the topic-aware conditional sentence probability in eq (7) over the latent topic space. But since the latent topic is highly dependent on the video content, usually the latent topic distributions of the video could be very skew with massive likelihood on the peak point. erefore, it is reasonable that we use the maximum latent topic likelihood to approximate the summation over the latent topic space to gain e ciency without losing much accuracy. e approximation for eq (7) is as follows:\nPr(y|x) = \u2211\nz1, ...,zK Pr(y|z1, . . . , zK , x)Pr(z1, . . . , zK |x) (12)\n\u2248 Pr(y|z1 = z\u03031, . . . , zK = z\u0303K , x)Pr(z1 = z\u03031, . . . , zK = z\u0303K |x)\nwhere z\u03031, . . . , \u02dczK = arg maxz1, ...,zK Pr(z1, . . . , zK |x). Our experiments show such approximation leads us to good results.\ne multi-task training scheme is conducted as follows: 1. We rst pretrain the topic predictor using the topic loss Ltopic , and use it to initialize \u0398z in the M&M TGM (as shown in eq (7)). For the dist in topic loss, we choose two widely used distances: l2-distance l2(z, z\u0303) =\u2016 z \u2212 z\u0303 \u201622 and KL-divergence KL(z, z\u0303) = \u2211Kk=1 z\u0303k lo\u0434(z\u0303k/zk ).\n2. We then pretrain the topic-aware decoder using the caption loss Lcaption (as shown in eq (10)) with the topic guidance from the xed topic predictor in the above step, and use it as a good initialization for \u0398\u2032z and \u0398d in the M&M TGM (as shown in eq (7)).\n3. Finally, based on the parameters initialization in the above two steps, we use the multi-task loss L(y, z, y\u0303, z\u0303) (as shown in eq (9)) to further train both models jointly, which ne-tunes all the parameters \u0398z , \u0398\u2032z and \u0398d to optimize the caption performance."}, {"heading": "4.2 Topic Mining and Prediction", "text": "We rst brie y present two topic structures adopted in previous works, and then propose our multimodal latent topic mining approach and the topic prediction model.\nExpert-de ned Topics: e online video websites o en provide an expert-de ned topic schema as shown in Figure 2(a). Video uploaders can select one of the topics to be er organize their videos. Such topics can re ect the topic variety to some extent, but have the following drawbacks: 1) e user assigned labels are noisy with labelling mistakes; 2) e topic distributions are exclusive which ignores the topic diversity inside the video; and 3) ere might exist di erent semantic meanings and visual appearances within a topic, which is suboptimal as latent topics.\nTextual Latent Topics: e annotated descriptions in video caption datasets provide rich and accurate information about the video content, which can also re ect more appropriate latent topic distributions. us, our previous work [4] mined topics from the annotated video captions on the training set. e main idea is to cluster the video captions and each cluster can represent a latent topic. We use the kernel K-means algorithm [6] for clustering. We group all the groundtruth captions of a video as one data sample. Stopwords are removed and the bag-of-words are used as our text features. e cosine kernel is adopted to generate nonlinear cluster separations. Since a video might contain several topics, we utilize the so assignment according to the distance between samples and clusters to generate topic distributions.\nMultimodal Latent Topics: Although the textual latent topics may be er t with the videos\u2019 underlying topic distributions, the mined topics purely based on textual data still su er from the following problems:\n1) Polysemy phenomena: Words can convey several di erent meanings, which might lead to improper assignment of the video. As shown in Figure 2(b), the most frequent word court can either mean tribunal or sports eld, so the video could mistakenly peak at some sports related topics.\n2) Certain messy clusters: e unsupervised clustering method cannot perfectly generate meaningful latent topics, which could produce some semantically unclear topics with dissimilar visual appearances. is brings great harm for the topic prediction model to learn from those messy topics.\nTo address these issues, we further propose to combine the description texts and visual features to learn the multimodal latent topic representation. e textual features are preprocessed in the same way as in textual latent topics and the visual features are elaborated in Section 5.2. We use the weighted kernel K-means algorithm to fuse the textual and visual features with weights of 1 and 0.2 respectively, because we consider the textual features are more accurate than the visual features to unveil an eligible latent topic structure.\nTopic Predictor: We take the teacher-student perspective [1] to train the topic predictor\u03c8z . e distributions of the above mined latent topics serve as the teacher z to guide the\u03c8z to generate latent topic predictions with the topic prediction loss Ltopic . In this work, we adopt a two-layer perception as the topic predictor based on inputs of multimodal video featuresm1,m2,m3:\n\u03c8z (x;\u0398z ) = so f tmax(W2(\u03c6(W1[m1;m2;m3] + b1) + b2) (13) whereWi ,bi (i = 1, 2) are parameters, [\u00b7] denotes feature concatenation and \u03c6 is the RELU nonlinear function [10]."}, {"heading": "4.3 Topic-aware Decoder", "text": "In this section, we rst brie y introduce the standard LSTM, and then describe our proposed topic-aware decoder based on the LSTM model to employ the topic guidance during sentence generation.\ne LSTM model addresses the vanishing gradients problem in traditional RNN by employing a memory cell and three gates to control the information ow in the network. e formulas of the LSTM cell\u03c8d (ht ,wt ;\u0398d ) at timestep t are given below:\ninput \u0434ate : it = \u03c3 (Wiwwt +Uihhhiddent\u22121 + bi ) (14)\nf or\u0434et \u0434ate : ft = \u03c3 (Wf wwt +Uf hhhiddent\u22121 + bf ) (15)\noutput \u0434ate : ot = \u03c3 (Wowwt +Uohhhiddent\u22121 + bo ) (16)\ncell input : \u0434t = \u03d5(W\u0434wwt +U\u0434hhhiddent\u22121 + b\u0434) (17)\ncell state : hcellt = it \u0434t + ft hcellt\u22121 (18)\ncell output : hhiddent = ot \u03d5(hcellt ) (19)\nwherehhiddent is the hidden state,hcellt is the cell state, \u03c3 is sigmoid function, \u03d5 is tanh function, denotes element-wise production, and \u0398d = {W\u2217w ,U\u2217h ,b\u2217} are parameters.\nRecall that in Section 3, when we use LSTM cell to parameterize Pr(y|z1, . . . , zK , x), we need to change the LSTM cell function to\u03c8d (wt ,ht , z1, . . . , zK ;\u0398d ,\u0398\u2032z ) to employ the topic dependency.\nInspired by Gan et al. [9], we extend each weight matrix of the conventional LSTM to be an ensemble of a set of topic-dependent weight matrices.\nLet us take one of the input gate weight matricesWiw as an example, and transformation for other parameters in LSTM model are alike. We de ne the ensemble 3D weight matrix Wi\u03c4 \u2208 Rnh\u00d7nw\u00d7K , where nh is the number of hidden units and nw is the dimension of input vectors. Wi\u03c4 [k] denotes the k-th slice of Wi\u03c4 , which represents the 2D weight matrix belonging to the LSTM model for the k-th topic. erefore, we explicitly specify K language decoders, which is, for each topic there is a pair of topic-speci c LSTM weights. Given the latent topic z\u0303, we can de ne the mixture topic LSTM weight matrix Wi (z\u0303) \u2208 Rnh\u00d7nw as\nWi (z\u0303) = K\u2211 k=1 z\u0303kWi\u03c4 [k] (20)\nwhere z\u0303k is the k-th topic in z\u0303. When z\u0303 is not the exclusive one-hot topic distribution, the video data is shared among di erent topics. However, the parameters still grow linearly with the number of topics K and no parameters are shared among di erent video topics which can easily result in over- ing. So the 3-way factorization method [16, 24] is used to share parameters. We re-represent Wi\u03c4 in terms of three matrices Wia \u2208 Rnh\u00d7nf , Wib \u2208 Rnf \u00d7K and Wic \u2208 Rnf \u00d7nw , where nf is the number of factors, such that\nWi (z) =Wia \u00b7 dia\u0434(Wib z\u0303) \u00b7Wic (21) Wia and Wic are shared among all topics, while Wib can be viewed as the latent vectors of topics.\nerefore, the transformation from input vectors to input gates is changed from Wiw \u00b7 wt to Wia \u00b7 (Wib z\u0303 Wicwt ). e topic distribution z\u0303 a ects the LSTM parameters associated to the video when decoding, which implicitly works as an ensemble of K topicaware language decoders."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "To validate the e ectiveness, robustness and generalization of our proposed methods, we conduct extensive experiments on two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].\nMSR-VTT: e MSR-VTT corpus [42] is currently the largest open-domain video captioning dataset with a wide variety of video topics. It consists of 10,000 video clips with 20 human annotated captions per clip. Each clip also contains a noisy expert-de ned topic label crawled from YouTube, the distribution of which is shown in Figure 3. Following the standard data split in the MM2016 challenge [31], we use 6,513 videos for training, 497 videos for validation and the remained 2,990 for testing.\nYoutube2Text: e Youtube2Text corpus [11] contains 1970 Youtube video clips with around 40 human annotated sentences per clip. For fair comparison, we crawl the expert-de ned topics from YouTube for the whole dataset as shown in Figure 3, which are also noisy. We adopt the same data splits as provided in Yao et al. [45], with 1,200 videos for training, 100 videos for validation and 670 videos for testing.\nDescription Preprocessing: We convert all descriptions to lower case and remove all punctuations. We add begin-of-sentence\ntag <BOS> and end-of-sentence tag <EOS> to our vocabulary. We preserve words that appear more than twice for MSR-VTT, resulting in a vocabulary size of 10,868, and words that appear more than once for Youtube2Text, resulting in a vocabulary size of 7,245."}, {"heading": "5.2 Implementation Details", "text": "Multi-modality Features: We extract features from image, motion and aural modalities. For image features, we extract activations from the penultimate layer of the inception-resnet [36] pre-trained on the ImageNet, the dimensionality of which is 1,536. For motion features, we extract activations from the last 3D convolution layer of the C3D model [37] pre-trained on the Sports-1M dataset. We perform max-pooling on the spatial dimension (width and height), resulting in 512 dimensional features. For aural features, We extract the Mel-Frequency Cepstral Coe cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively. We simply pad zeros for videos without the sound track.\nTraining Settings: We empirically set the hidden layer of the topic prediction model with 512 units. e dimension of the LSTM hidden size is set to be 512. e output weights to predict the words are the transpose of the input word embedding matrix. We apply dropout with rate of 0.5 on the input and output of LSTM and use ADAM algorithm [15] with learning rate of 10\u22124. Beam search with beam width of 5 is used to generate sentences during testing process.\nEvaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38]."}, {"heading": "5.3 Evaluation of M&M TGM", "text": "In this subsection, we introduce two baselines to show the effectiveness of topic guidance and our multi-task learning for video captioning. e rst baseline, Vanilla, consists of the multimodal encoder and the standard LSTM decoder. is baseline doesn\u2019t involve any topic information. e second baseline is TGM, a trimmed version of our M&M TGM. We discard the multi-task joint optimization step (last step) in section 4.1.\nTo evaluate the e ectiveness of topic guidance, we compare the caption performance of the Vanilla and TGMs using di erent latent topic guidance in Table 1. e numbers of textual and multimodal latent topics are optimized on the validation set (50 topics for MSR-VTT and 5 topics for Youtube2Text). We can see that TGMs outperform Vanilla model consistently on all four metrics\nacross MSR-VTT and Youtube2Text datasets, which demonstrates that exploiting topic information is bene cial to generate video descriptions.\ne rst three rows in the TGM block in Table 1 shows a fair comparison of the impact from di erent predicted latent topics on caption performance. We can see that the automatically mined topics (both textual and multimodal latent topics), outperform expertde ned topics by a large margin on all four metrics and two datasets, such as over 2% absolute gain on the CIDEr score. e multimodal latent topics further achieve be er performance than textual latent topics consistently on multiple metrics across datasets. e improvement is also proved to be signi cant in the Student\u2019s t-test such as p-value of 0.03 on the BLEU@4 score. ese results suggest that the multimodal topic mining approach can discover be er underlying topics as guidance for TGM, and such improvement is prominent and robust for di erent evaluation metrics and datasets.\nSince video uploaders can manually assign expert-de ned topic labels on online websites, we also compare our automatically predicted multimodal latent topics with the user assigned expertde ned topics in the last two rows of Table 1. However, the assigned expert-de ned topics are very noisy, because the annotations are labelled on the entire videos but the caption generation is only applied on the short segments and the users may also make labelling mistakes. For almost all metrics, the performances of our predicted latent topics are superior to the noisy assigned expert-de ned topics on both MSR-VTT and Youtube2Text datasets, which shows that the latent topics learned in an unsupervised approach are competitive or even be er than the assigned expert-de ned topics with noise.\nTo validate the in uence of joint training strategy, we compare the caption performance of TGM and M&M TGM. As shown in Table 2, M&M TGM consistently improves TGM on all datasets and metrics, with signi cance p-value < 0.05 for di erent metrics in Student\u2019s t-test, which proves the bene ts brought by the multi-task learning."}, {"heading": "5.4 Comparison with the State-of-the-art", "text": "Table 3 presents our M&M TGM with several state-of-the-art methods applied on the two video caption datasets. Our method achieves signi cant be er performance than prior works on MSRVTT dataset, for example, the BLEU@4 achieves 7.08% relative improvement than the previous best performance. For Youtube2Text dataset, the METEOR and CIDERr scores improved signi cantly but our performance on BLEU@4 is lower than the SCN-LSTM. e BLEU@4 metric focuses on the syntactic agreement, while METEOR and CIDEr concern more about semantic meanings. erefore, we consider the sentence generated from M&M TGM are more semantically relevant with the video content.\nTo be noted, the semantic concepts used in SCN-LSTM are trained with additional image data because they claim Youtube2Text corpus is too small to train reliable concept classi ers, but our M&M TGM is purely trained on Youtube2Text for latent topic prediction and sentence generation. Hence, we argue that for video captioning, the guidance from latent topics might be superior than detected semantic concepts for the following reasons: 1) videos contain more objects than images but many might be irrelevant to the description; 2) topics contain additional information besides concepts such as actions from motion modality and words from speech modality; and 3) the prediction accuracy is important for the decoder and topics are easier to be classi ed than concepts.\nwinner of the MM16 VTT challenge."}, {"heading": "5.5 Experimental Analysis", "text": "Generalization Ability: To evaluate the generalization ability of our proposed method, we conduct the cross dataset experiment. We train M&M TGM on MSR-VTT dataset and test its performance on the Youtube2Text dataset. Results are presented in Table 4. We can see that the proposed M&M TGM works signi cantly be er than Vanilla model for cross datasets evaluation on all four metrics, which demonstrates that our method not only can improve the caption performance for in-domain videos but also generalize well on videos in the wild.\nTopic Loss Selection: We compare the caption performance using l2-distance or KL-divergence as Ltopic in the TGM model. Table 5 presents the results. e l2-distance consistently surpasses the KL-divergence across datasets and metrics. So unless otherwise speci ed, we use l2-distance as the topic prediction loss. eNumber of Topics K : We also explore di erent number of topics on each dataset and the results are shown in Figure 4. e best number of topics for MSR-VTT is 50 and for Youtube2Text is 5, which shows that the more diverse the dataset is, the more amount of topics is required. eMulti-task Parameter \u03bb: Since caption generation is our main goal, we consider that the weight on topic prediction loss should not surpass the weight on caption generation loss. erefore, we search the best multi-task hyper parameter \u03bb1\u2212\u03bb in range [0.1, 1] with step of 0.1. To save space, we present the sum of all the metrics in Figure 5 and the trends are similar for di erent metrics. e multitask learning with di erent \u03bb > 0 all achieves be er performance\nthan single-task learning (\u03bb = 0), which proves the robustness of our multi-task training with respect to the hyper-parameter."}, {"heading": "5.6 alitative Analysis", "text": "To gain an intuition of the improvement on generated video descriptions from M&M TGM model, we present some video examples with the video description from Vanilla model and M&M TGM on testing set of MSR-VTT.\nIn Figure 6, we can see that M&M TGM can generate more accurate video descriptions than Vanilla model even though they utilize the same multimodal features. In Figure 7, though the descriptions from Vanilla and M&M TGM are both correct, M&M TGM model can produce more detailed information about the video contents. We also observe that M&M TGM employs more unique words with 493 compared to the 391 unique words in Vanilla model.\ne reason behind these quality improvements could be that M&M TGM can narrow down the sentence generation space according to the predicted latent topics. is enables the model to focus on subtler di erences between similar concepts such as the soccer and rugby sports in the upper row of Figure 6, and cover more detailed descriptions under the topic such as making airplane vs. folding paper in the upper row of Figure 7 with more specialized words."}, {"heading": "5.7 Human Interaction in Captioning", "text": "Besides the improvement on the caption performance, our topicguided model can provide an interface for users to be involved in the caption generation. For example, users could simply assign a category tag to the uploaded videos with minimum costs to rene the automatic generated video descriptions. To evaluate the\nperformance boosts with the human enhancement to our topicguided model, we manually re-annotate the expert-de ned topics for Youtub2Text dataset as the clean version and compare its caption performance with the noisy assigned topics. Results are shown in Table 6 which demonstrates that a huge gain can be achieved with the manual correction of the topics in low cost."}, {"heading": "6 Conclusions", "text": "In this paper, we propose a novel topic-guided caption model to address the topic diversity challenge for open-domain video captioning task. e proposed model can predict the latent topics of videos and then generate topic-oriented video descriptions with the topic guidance jointly in an end-to-end manner. We utilize the multimodal topic mining approach to construct video topics automatically and take a teacher-student learning perspective to predict the latent topics purely from video multimodal contents. e topic-aware decoder can exploit the predicted topics to adjust its weights to t the topic-dependent sentence distributions. Our experimental results on two public video caption benchmark datasets show that the proposed model can generate more accurate and detailed descriptions within di erent topics and improves the performance consistently on all metrics on both datasets. Furthermore, we show that our model has very good generalization ability across datasets. e proposed topic-guided caption model can be considered as a generic framework, which could be integrated with other techniques such as temporal a ention or hierarchical video encoder. We will study such integration in the future."}, {"heading": "7 Acknowledgments", "text": "is work is supported by National Key Research and Development Plan under Grant No. 2016YFB1001202."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "In NIPS", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Generating Video Descriptions with Topic Guidance", "author": ["Shizhe Chen", "Jia Chen", "Qin Jin"], "venue": "In ICMR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing 28,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1980}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Inderjit S Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Improving Interpretability of Deep Neural Networks with Semantic Information", "author": ["Yinpeng Dong", "Hang Su", "Jun Zhu", "Bo Zhang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Semantic Compositional Networks for Visual Captioning", "author": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep Sparse Recti\u0080er Neural Networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["Sergio Guadarrama", "Niveda Krishnamoorthy", "Girish Malkarnenkar", "Subhashini Venugopalan", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Describing Videos using Multi-modal Fusion", "author": ["Qin Jin", "Jia Chen", "Shizhe Chen", "Yifan Xiong", "Alexander Hauptmann"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Video description generation using audio and visual cues", "author": ["Qin Jin", "Junwei Liang"], "venue": "In Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C Berg", "Tamara L Berg"], "venue": "In Proceedings of the 24th CVPR. Citeseer", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Meteor universal: Language speci\u0080c translation evaluation for any target language", "author": ["Michael Denkowski Alon Lavie"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Phrase-based Image Captioning", "author": ["R\u00e9mi Lebret", "Pedro H.O. Pinheiro", "Ronan Collobert"], "venue": "In ICML", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles", "author": ["Stefan Lee", "Michael Cogswell", "Viresh Ranjan", "David Crandall", "Dhruv Batra"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Event speci\u0080c multimodal pa\u008aern mining for knowledge base construction", "author": ["Hongzhi Li", "Joseph G Ellis", "Heng Ji", "Shih-Fu Chang"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Tgif: A new dataset and benchmark on animated gif description", "author": ["Yuncheng Li", "Yale Song", "Liangliang Cao", "Joel Tetreault", "Larry Goldberg", "Alejandro Jaimes", "Jiebo Luo"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Unsupervised Learning of Image Transformations", "author": ["R Memisevic", "G Hinton"], "venue": "In CVPR", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Jointly Modeling Embedding and Translation to Bridge Video and Language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "So\u0089ening quantization in bag-ofaudio-words", "author": ["Stephanie Pancoast", "Murat Akbacak"], "venue": "In ICASSP", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Multimodal Video Description", "author": ["Vasili Ramanishka", "Abir Das", "Dong Huk Park", "Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach", "Kate Saenko"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Self-critical Sequence Training for Image Captioning", "author": ["Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Image classi\u0080cation with the \u0080sher vector: \u008ceory and practice", "author": ["Jorge S\u00e1nchez", "Florent Perronnin", "\u008comas Mensink", "Jakob Verbeek"], "venue": "International journal of computer vision 105,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Weakly Supervised Dense Video Captioning", "author": ["Zhiqiang Shen", "Jianguo Li", "Zhou Su", "Minjun Li", "Yurong Chen", "Yu-Gang Jiang", "Xiangyang Xue"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Frame-and segment-level features and candidate pool evaluation for video caption generation", "author": ["Rakshith She\u008ay", "Jorma Laaksonen"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": "In NIPS", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["Christian Szegedy", "Sergey Io\u0082e", "Vincent Vanhoucke", "Alex Alemi"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": "In ICCV", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Je\u0082 Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "Science", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Msr-v\u008a: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Show, a\u008aend and tell: Neural image caption generation with visual a\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Review networks for caption generation", "author": ["Zhilin Yang", "Ye Yuan", "Yuexin Wu", "William W Cohen", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Image captioning with semantic a\u008aention", "author": ["\u008banzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 27, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 37, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 39, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 40, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 42, "context": "Although there have been signi\u0080cant breakthroughs recently in image captioning [9, 30, 41, 43, 44, 46], video captioning remains very challenging due to the diversity and complexity of video contents.", "startOffset": 79, "endOffset": 102}, {"referenceID": 17, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 11, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 29, "context": "Although a few works [8, 13, 33] have exploited the topic information for video description generation, there are still three main challenges in employing the topic information in video captioning.", "startOffset": 21, "endOffset": 32}, {"referenceID": 38, "context": "tion datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "tion datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "Early works are mainly based on rule systems [17, 19] and suffer from generating \u0083exible and accurate descriptions.", "startOffset": 45, "endOffset": 53}, {"referenceID": 16, "context": "Early works are mainly based on rule systems [17, 19] and suffer from generating \u0083exible and accurate descriptions.", "startOffset": 45, "endOffset": 53}, {"referenceID": 1, "context": "So more researches have been focusing on the encoder-decoder framework [2, 35] for caption generation.", "startOffset": 71, "endOffset": 78}, {"referenceID": 31, "context": "So more researches have been focusing on the encoder-decoder framework [2, 35] for caption generation.", "startOffset": 71, "endOffset": 78}, {"referenceID": 32, "context": "\u008ce deep convolutional neural networks (CNNs) [36] function as the encoder to transform image contents into dense vectors.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "\u008cen the decoder, typically the Long-short Term Memory [12] (LSTM), is utilized to generate sequential words conditioned on image features [41].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "\u008cen the decoder, typically the Long-short Term Memory [12] (LSTM), is utilized to generate sequential words conditioned on image features [41].", "startOffset": 138, "endOffset": 142}, {"referenceID": 39, "context": "[43] propose the spatial a\u008aention mechanism based on the basic encoder-decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[46] and Gan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] propose the semantic a\u008aention model and semantic compositional networks (SCN) respectively to exploit detected concepts from the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 41, "context": "[45] propose the temporal a\u008aention mechanism, and Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] utilize the hierarchical LSTM encoder to explore the temporal structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Most previous works only focus on the visual modality [26, 40], but recently Jin et al.", "startOffset": 54, "endOffset": 62}, {"referenceID": 36, "context": "Most previous works only focus on the visual modality [26, 40], but recently Jin et al.", "startOffset": 54, "endOffset": 62}, {"referenceID": 11, "context": "[13, 14] and Ramanishka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[13, 14] and Ramanishka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[29] have shown improvement of multimodal fusion for video captioning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "To promote the video captioning research for open-domain videos, several large-scale videos with various topic are collected such as MSR-VTT [42] and TGIF [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "To promote the video captioning research for open-domain videos, several large-scale videos with various topic are collected such as MSR-VTT [42] and TGIF [22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "[13] encode expert-de\u0080ned topics with multimodal features, which results in their winning of the MSR-VTT challenge [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] utilize the textual mined topics to learn interpretable features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[33] train separate language decoders for each expert-de\u0080ned topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] explore the guidance from textual minded topics to generate topic-aware sentences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "\u008ce latent Dirichlet allocation (LDA) proposed by Blei [3] is one of the most classic models to automatically inference the latent topics for textual documents.", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": "[21] apply the association rule mining algorithm on image-caption pairs to discover the multimodal topics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u008cus, our previous work [4] mined topics from the annotated video captions on the training set.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "We use the kernel K-means algorithm [6] for clustering.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Topic Predictor: We take the teacher-student perspective [1] to train the topic predictor\u03c8z .", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "whereWi ,bi (i = 1, 2) are parameters, [\u00b7] denotes feature concatenation and \u03c6 is the RELU nonlinear function [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "[9], we extend each weight matrix of the conventional LSTM to be an ensemble of a set of topic-dependent weight matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "So the 3-way factorization method [16, 24] is used to share parameters.", "startOffset": 34, "endOffset": 42}, {"referenceID": 38, "context": "To validate the e\u0082ectiveness, robustness and generalization of our proposed methods, we conduct extensive experiments on two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "To validate the e\u0082ectiveness, robustness and generalization of our proposed methods, we conduct extensive experiments on two benchmark video captioning datasets: MSR-VTT [42] and Youtube2Text [11].", "startOffset": 192, "endOffset": 196}, {"referenceID": 38, "context": "MSR-VTT: \u008ce MSR-VTT corpus [42] is currently the largest open-domain video captioning dataset with a wide variety of video topics.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "Youtube2Text: \u008ce Youtube2Text corpus [11] contains 1970 Youtube video clips with around 40 human annotated sentences per clip.", "startOffset": 37, "endOffset": 41}, {"referenceID": 41, "context": "[45], with 1,200 videos for training, 100 videos for validation and 670 videos for testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "For image features, we extract activations from the penultimate layer of the inception-resnet [36] pre-trained on the ImageNet, the dimensionality of which is 1,536.", "startOffset": 94, "endOffset": 98}, {"referenceID": 33, "context": "For motion features, we extract activations from the last 3D convolution layer of the C3D model [37] pre-trained on the Sports-1M dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 77, "endOffset": 80}, {"referenceID": 24, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 108, "endOffset": 112}, {"referenceID": 28, "context": "For aural features, We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [5] and use Bag-of-Audio-Words [27] and Fisher Vector [32] encoding methods to generate video-level features, with dimensionality of 1,024 and 624 respectively.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "5 on the input and output of LSTM and use ADAM algorithm [15] with learning rate of 10\u22124.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "Evaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [28], METEOR [18], ROUGE-L [23] and CIDEr [38].", "startOffset": 147, "endOffset": 151}, {"referenceID": 30, "context": "Aalto [34] 39.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "v2t navigator [13] 40.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "dense caption [33] 41.", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "LSTM-YT [40] 33.", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "S2VT [39] - 29.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "LSTM-I [8] 44.", "startOffset": 7, "endOffset": 10}, {"referenceID": 41, "context": "SA [45] 41.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "LSTM-E [26] 45.", "startOffset": 7, "endOffset": 11}, {"referenceID": 43, "context": "h-RNN decoder [47] 49.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "h-RNN encoder [25] 43.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "SCN-LSTM [9] 50.", "startOffset": 9, "endOffset": 12}], "year": 2017, "abstractText": "\u008ce topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an uni\u0080ed caption framework, M&M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-de\u0080ned topics, the mined multimodal topics are more semantically and visually coherent and can re\u0083ect the topic distribution of videos be\u008aer. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. \u008ce topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. \u008ce entire learning procedure is end-to-end and it optimizes both tasks simultaneously. \u008ce results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the e\u0082ectiveness of our proposed model. M&M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves be\u008aer generalization ability.", "creator": "LaTeX with hyperref package"}}}