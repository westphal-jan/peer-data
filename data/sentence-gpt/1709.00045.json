{"id": "1709.00045", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "On Security and Sparsity of Linear Classifiers for Adversarial Settings", "abstract": "Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 31 Aug 2017 19:11:56 GMT  (150kb,D)", "http://arxiv.org/abs/1709.00045v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["ambra demontis", "paolo russu", "battista biggio", "giorgio fumera", "fabio roli"], "accepted": false, "id": "1709.00045"}, "pdf": {"name": "1709.00045.pdf", "metadata": {"source": "CRF", "title": "On Security and Sparsity of Linear Classifiers for Adversarial Settings", "authors": ["Ambra Demontis", "Paolo Russu", "Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "emails": ["ambra.demontis@diee.unica.it", "paolo.russu@diee.unica.it", "battista.biggio@diee.unica.it", "fumera@diee.unica.it", "roli@diee.unica.it"], "sections": [{"heading": "1 Introduction", "text": "Machine-learning techniques are becoming an essential tool in several application fields such as marketing, economy and medicine. They are increasingly being used also in security-related applications, like spam and malware detection, despite their vulnerability to adversarial attacks, i.e., the deliberate manipulation of training or test data, to subvert their operation; e.g., spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].\nIn this work, we focus on the security of linear classifiers. These classifiers are particularly suited to mobile and embedded systems, as the latter usually demand for strict constraints on storage, processing time and power consumption. Nonetheless, linear classifiers are also a preferred choice as they provide easier-to-interpret decisions (with respect to nonlinear classification methods). For instance, the widely-used SpamAssassin anti-spam filter exploits a linear classifier [7,5].1 Work in the adversarial machine learning literature has already\n1 See also http://spamassassin.apache.org.\nar X\niv :1\n70 9.\n00 04\n5v 1\n[ cs\n.L G\n] 3\n1 A\nug 2\n01 7\ninvestigated the security of linear classifiers to evasion attacks [4,7], suggesting the use of more evenly-distributed feature weights as a mean to improve their security. Such a solution is however based on heuristic criteria, and a clear understanding of the conditions under which it can be effective, or even optimal, is still lacking. Moreover, in mobile and embedded systems, sparse weights are more desirable than evenly-distributed ones, in terms of processing time, memory requirements, and interpretability of decisions.\nIn this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect. 2). This is particularly relevant in adversarial settings as the aforementioned ones, since evasion attacks can be essentially considered a form of noise affecting the non-manipulated, initial data (e.g., malicious code before obfuscation). Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks). We analyze the relationship between the sparsity of the weights of a linear classifier and its security in Sect. 3, where we also propose an octagonal-norm regularizer to better tune the trade-off arising between sparsity and security. In Sect. 4, we empirically evaluate our results on a handwritten digit recognition task, and on real-world application examples including spam filtering and detection of malicious software (malware) in PDF files. We conclude by discussing the main contributions and findings of our work in Sect. 5, while also sketching some promising research directions."}, {"heading": "2 Background", "text": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15]."}, {"heading": "2.1 Attacker\u2019s Model", "text": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.g., evading detection at test time), knowledge of the classifier, and capability of manipulating the input data.\nAttacker\u2019s Goal. Among the possible goals, here we focus on evasion attacks, where the goal is to modify a single malicious sample (e.g., a spam email) to have it misclassified as legitimate (with the largest confidence) by the classifier [8].\nAttacker\u2019s Knowledge. The attacker can have different levels of knowledge of the targeted classifier; she may have limited or perfect knowledge about the training data, the feature set, and the classification algorithm [8,9]. In this work, we focus on perfect-knowledge (worst-case) attacks.\nAttacker\u2019s Capability. In evasion attacks, the attacker is only able to modify malicious instances. Modifying an instance usually has some cost. Moreover, arbitrary modifications to evade the classifier may be ineffective, if the resulting instance loses its malicious nature (e.g., excessive obfuscation of a spam email could make it unreadable for humans). This can be formalized by an applicationdependent constraint. As discussed in [16], two kinds of constraints have been mostly used when modeling real-world adversarial settings, leading one to define sparse (`1) and dense (`2) attacks. The `1-norm yields typically a sparse attack, as it represents the case when the cost depends on the number of modified features. For instance, when instances correspond to text (e.g., the email\u2019s body) and each feature represents the occurrences of a given term in the text, the attacker usually aims to change as few words as possible. The `2-norm yields a dense attack, as it represents the case when the cost of modifying features is proportional to the distance between the original and modified sample in Euclidean space. For example, if instances are images, the attacker may prefer making small changes to many or even all pixels, rather than significantly modifying only few of them. This amounts to (slightly) blurring the image, instead of obtaining a salt-and-pepper noise effect (as the one produced by sparse attacks) [16].\nAttack Strategy. It consists of the procedure for modifying samples, according to the attacker\u2019s goal, knowledge and capability, formalized as an optimization problem. Let us denote the legitimate and malicious class labels respectively with \u22121 and +1, and assume that the classifier\u2019s decision function is f(x) = sign (g(x)), where g(x) = w>x + b \u2208 R is a linear discriminant function with feature weights w \u2208 Rd and bias b \u2208 R, and x is the representation of an instance in a d-dimensional feature space. Given a malicious sample x0, the goal is to find the sample x\u2217 that minimizes the classifier\u2019s discriminant function g(\u00b7) (i.e., that is classified as legitimate with the highest possible confidence) subject to the constraint that x\u2217 lies within a distance dmax from x0:\nx\u2217 = arg min x g(x) (1)\ns.t. d(x,x0) \u2264 dmax , (2)\nwhere the distance measure d(\u00b7, \u00b7) is defined in terms of the cost of data manipulation (e.g., the number of modified words in each spam) [1,2,8,9,12]. For sparse and dense attacks, d(\u00b7, \u00b7) corresponds respectively to the `1 and `2 distance."}, {"heading": "2.2 Robustness and Regularization", "text": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15]. In particular, Xu et al. [13] have considered the following robust optimization problem:\nmin w,b max u1,...,um\u2208U m\u2211 i=1 ( 1\u2212 yi(w>(xi \u2212 ui) + b) ) + , (3)\nwhere (z)+ is equal to z \u2208 R if z > 0 and 0 otherwise, u1, ...,um \u2208 U define a set of bounded perturbations of the training data {xi, yi}mi=1 \u2208 Rm\u00d7{\u22121,+1}m, and the so-called uncertainty set U is defined as U \u2206= { (u1, . . . ,um)| \u2211m i=1 \u2016ui\u2016\u2217 \u2264 c } , being \u2016 \u00b7\u2016\u2217 the dual norm of \u2016 \u00b7\u2016. Typical examples of uncertainty sets according to the above definition include `1 and `2 balls [13,14].\nProblem (3) basically corresponds to minimizing the hinge loss for a two-class classification problem under worst-case, bounded perturbations of the training samples xi, i.e., a typical setting in robust optimization [13,14,15]. Under some mild assumptions easily verified in practice (including non-separability of the training data), the authors have shown that the above problem is equivalent to the following non-robust, regularized optimization problem (cf. Th. 3 in [13]):\nmin w,b c\u2016w\u2016+ m\u2211 i=1 ( 1\u2212 yi(w>xi + b) ) + . (4)\nThis means that, if the `2 norm is chosen as the dual norm characterizing the uncertainty set U , then w is regularized with the `2 norm, and the above problem is equivalent to a standard Support Vector Machine (SVM) [17]. If input data uncertainty is modeled with the `1 norm, instead, the optimal regularizer would be the `\u221e regularizer, and vice-versa.\n2 This notion is clarified in Fig. 1, where we consider different norms to model input data uncertainty against the corresponding SVMs; i.e., the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively."}, {"heading": "3 Security and Sparsity", "text": "We discuss here the main contributions of this work. The result discussed in the previous section, similar to that reported independently in [15], helps un-\n2 Note that the `1 norm is the dual norm of the `\u221e norm, and vice-versa, while the `2 norm is the dual norm of itself.\nderstanding the security properties of linear classifiers in adversarial settings, in terms of the relationship between security and sparsity. In fact, what discussed in the previous section does not only confirm the intuition in [4,7], i.e., that more uniform feature weighting schemes should improve classifier security by enforcing the attacker to manipulate more feature values to evade detection. The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w. If one considers an `1 (sparse) attacker, facing a higher cost when modifying more features, it turns out that the optimal regularizer is given by the `\u221e norm of w, which tends to yield more uniform weights. In particular, the solution provided by `\u221e regularization (in the presence of a strongly-regularized classifier) tends to yield weights which, in absolute value, are all equal to a (small) maximum value. This also implies that `\u221e regularization does not provide a sparse solution.\nFor this reason we propose a novel octagonal (8gon) regularizer,3 given as a linear (convex) combination of `1 and `\u221e regularization:\n\u2016w\u20168gon = (1\u2212 \u03c1)\u2016w\u20161 + \u03c1\u2016w\u2016\u221e (5)\nwhere \u03c1 \u2208 (0, 1) can be increased to trade sparsity for security. Our work does not only aim to clarify the relationships among regularization, sparsity, and adversarial noise. We also aim to quantitatively assess the aforementioned trade-off on real-world application examples, to evaluate whether and to what the extent the choice of a proper regularizer may have a significant impact in practice. Thus, besides proposing a new regularizer and shedding light on uniform feature weighting and classifier security, the other main contribution of the present work is the experimental analysis reported in the next section, in which we consider both dense (`2) and sparse (`1) attacks, and evaluate their impact on SVMs using different regularizers. We further analyze the weight distribution of each classifier to provide a better understanding on how sparsity is related to classifier security under the considered evasion attacks."}, {"heading": "4 Experimental Analysis", "text": "We first consider dense and sparse attacks in the context of handwritten digit recognition, to visually demonstrate their blurring and salt-and-pepper effect on images. We then consider two real-world application examples including spam and PDF malware detection, investigating the behavior of different regularization terms against (sparse) evasion attacks.\nHandwritten Digit Classification. To visually show how evasion attacks work, we perform sparse and dense attacks on the MNIST digit data [21]. Each image is represented by a vector of 784 features, corresponding to its gray-level pixel values. As in [8], we simulate an adversarial classification problem where the digits 8 and 9 correspond to the legitimate and malicious class, respectively.\n3 Note that octagonal regularization has been previously proposed also in [20]. However, differently from our work, the authors have used a pairwise version of the infinity norm, for the purpose of selecting (correlated) groups of features.\nSpam Filtering. This is a well-known application subject to adversarial attacks. Most spam filters include an automatic text classifier that analyzes the email\u2019s body text. In the simplest case Boolean features are used, each representing the presence or absence of a given term. For our experiments we use the TREC 2007 spam track data, consisting of about 25000 legitimate and 50000 spam emails [22]. We extract a dictionary of terms (features) from the first 5000 emails (in chronological order) using the same parsing mechanism of SpamAssassin, and then select the 200 most discriminant features according to the information gain criterion [23]. We simulate a well-known (sparse) evasion attack in which the attacker aims to modify only few terms. Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].\nPDF Malware Detection. Another application that is often targeted by attackers is the detection of malware in PDF files. A PDF file can host different kinds of contents, like Flash and JavaScript. Such third-party applications can be exploited by attacker to execute arbitrary operations. We use a data set made up of about 5500 legitimate and 6000 malicious PDF files. We represent every file using the 114 features that are described in [24]. They consist of the number of occurrences of a predefined set of keywords, where every keyword represents an action performed by one of the objects that are contained into the PDF file (e.g., opening another document that is stored inside the file). An attacker cannot trivially remove keywords from a PDF file without corrupting its functionality. Conversely, she can easily add new keywords by inserting new object\u2019s operations. For this reason, we simulate this attack by only considering feature increments (decrementing a feature value is not allowed). Accordingly, the most convenient strategy to mislead a malware detector (classifier) is thus to insert as many occurrences of a given keyword as possible, which is a sparse attack.4\nWe consider different versions of the SVM classifier obtained by combining the hinge loss with the different regularizers shown in Fig. 2.\n4 Despite no upper bound on the number of injected keywords may be set, we set the maximum value for each keyword to the corresponding one observed during training.\n2-norm SVM (SVM). This is the standard SVM learning algorithm [17]. It finds w and b by solving the following quadratic programming problem:\nmin w,b\n1 2 \u2016w\u201622 + C m\u2211 i=1 (1\u2212 yig(xi))+ , (6)\nwhere g(x) = w>x + b denotes the SVM\u2019s linear discriminant function. Note that `2 regularization does not induce sparsity on w. Infinity-norm SVM (\u221e-norm). In this case, the `\u221e regularizer bounds the weights\u2019 maximum absolute value as \u2016w\u2016\u221e = maxj=1,...,d |wj | [20]:\nmin w,b \u2016w\u2016\u221e + C m\u2211 i=1 (1\u2212 yig(xi))+ . (7)\nAs the standard SVM, this classifier is not sparse; however, the above learning problem can be solved using a simple linear programming approach.\n1-Norm SVM (1-norm). Its learning algorithm is defined as [19]:\nmin w,b \u2016w\u20161 + C m\u2211 i=1 (1\u2212 yig(xi))+ . (8)\nThe `1 regularizer induces sparsity, while retaining convexity and linearity.\nElastic-net SVM (el-net). We use here the elastic-net regularizer [25], combined with the hinge loss to obtain an SVM formulation with tunable sparsity:\nmin w,b\n(1\u2212 \u03bb)\u2016w\u20161 + \u03bb\n2 \u2016w\u201622 + C m\u2211 i=1 (1\u2212 yig(xi))+ . (9)\nThe level of sparsity can be tuned through the trade-off parameter \u03bb \u2208 (0, 1). Octagonal-norm SVM (8gon). This novel SVM is based on our octagonalnorm regularizer, combined with the hinge loss:\nmin w,b (1\u2212 \u03c1)\u2016w\u20161 + \u03c1\u2016w\u2016\u221e + C m\u2211 i=1 (1\u2212 yig(xi))+ . (10)\nThe above optimization problem is linear, and can be solved using state-ofthe-art solvers. The sparsity of w can be increased by decreasing the trade-off parameter \u03c1 \u2208 (0, 1), at the expense of classifier security. Sparsity and Security Measures. We evaluate the degree of sparsity S of a given linear classifier as the fraction of its weights that are equal to zero:\nS = 1\nd |{wj |wj = 0, j = 1, . . . , d}| , (11)\nbeing | \u00b7 | the cardinality of the set of null weights.\nTo evaluate security of linear classifiers, we define a measure E of weight evenness, similarly to [4,7], based on the ratio of the `1 and `\u221e norm:\nE = \u2016w\u20161 d\u2016w\u2016\u221e , (12)\nwhere dividing by the number of features d ensures that E \u2208 [ 1 d , 1 ] , with higher values denoting more evenly-distributed feature weights. In particular, if only a weight is not zero, then E = 1d ; conversely, when all weights are equal to the maximum (in absolute value), E = 1.\nExperimental Setup. We randomly select 500 legitimate and 500 malicious samples from each dataset, and equally subdivide them to create a training and a test set. We optimize the regularization parameter C of each SVM (along with \u03bb and \u03c1 for the Elastic-net and Octagonal SVMs, respectively) through 5-fold cross-validation, maximizing the following objective on the training data:\nAUC + \u03b1E + \u03b2S (13)\nwhere AUC is the area under the ROC curve, and \u03b1 and \u03b2 are parameters defining the trade-off between security and sparsity. We set \u03b1 = \u03b2 = 0.1 for the PDF and digit data, and \u03b1 = 0.2 and \u03b2 = 0.1 for the spam data, to promote more secure solutions in the latter case. These parameters allow us to accept a marginal decrease in classifier security only if it corresponds to much sparser feature weights. After classifier training, we perform evasion attacks on all malicious test samples, and evaluate the corresponding performance as a function of the number of features modified by the attacker. We repeat this procedure five times, and report the average results on the original and modified test data.\nExperimental Results. We consider first PDF malware and spam detection. In these applications, as mentioned before, only sparse evasion attacks make sense, as the attacker aims to minimize the number of modified features. In Fig. 3, we report the AUC at 10% false positive rate for the considered classifiers, against an increasing number of words/keywords changed by the attacker. This experiment\nshows that the most secure classifier under sparse evasion attacks is the Infinitynorm SVM, since its performance degrades more gracefully under attack. This is an expected result, given that, in this case, infinity-norm regularization corresponds to the dual norm of the attacker\u2019s cost/distance function. Notably, the Octagonal SVM yields reasonable security levels while achieving much sparser solutions, as expected (cf. the sparsity values S in the legend of Fig. 3). This experiment really clarifies how much the choice of a proper regularizer can be crucial in real-world adversarial applications.\nBy looking at the values reported in Fig. 3, it may seem that the security measure E does not properly characterize classifier security under attack; e.g., note how Octagonal SVM exhibits lower values of E despite being more secure than SVM on the PDF data. The underlying reason is that the attack implemented on the PDF data only considers feature increments, while E generically considers any kind of manipulation. Accordingly, one should define alternative security measures depending on specific kinds of data manipulation. However, the security measure E allows us to properly tune the trade-off between security and sparsity also in this case and, thus, this issue may be considered negligible.\nFinally, to visually demonstrate the effect of sparse and dense evasion attacks, we report some results on the MNIST handwritten digits. In Fig. 4, we show the\n\u201c9\u201d digit image modified by the attacker to have it misclassified by the classifier as an \u201c8\u201d. These modified digits are obtained by solving Problem (1)-(2) through a simple projected gradient-descent algorithm, as in [8].5 Note how dense attacks only produce a slightly-blurred effect on the image, while sparse attacks create more evident visual artifacts. By comparing the values of g(x) reported in Fig. 4, one may also note that this simple example confirms again that Infinity-norm and Octagonal SVM are more secure against sparse attacks, while SVM and Elastic-net SVM are more secure against dense attacks."}, {"heading": "5 Conclusions and Future Work", "text": "In this work we have shed light on the theoretical and practical implications of sparsity and security in linear classifiers. We have shown on real-world adversarial applications that the choice of a proper regularizer is crucial. In fact, in the presence of sparse attacks, Infinity-norm SVMs can drastically outperform the security of standard SVMs. We believe that this is an important result, as (standard) SVMs are widely used in security tasks without taking the risk of adversarial attacks too much into consideration. Moreover, we propose a new octagonal regularizer that enables trading sparsity for a marginal loss of security under sparse evasion attacks. This is extremely useful in applications where sparsity and computational efficiency at test time are crucial. When dense attacks are instead deemed more likely, the standard SVM may be retained a good compromise. In that case, if sparsity is required, one may trade some level of security for sparsity using the Elastic-net SVM. Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27]."}], "references": [{"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "10th Int\u2019l Conf. Knowl. Disc. Data Mining (KDD), Seattle, WA, USA, ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "11th Int\u2019l Conf. Knowl. Disc. Data Mining (KDD), Chicago, IL, USA, ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "2nd Conf. Email and Anti-Spam (CEAS), Mountain View, CA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "6th Conf. Email and Anti-Spam (CEAS), Mountain View, CA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "LEET \u201908, Berkeley, CA, USA, USENIX Association", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple classifier systems for robust classifier design in adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. Mach. Learn. Cyb. 1(1)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "In Blockeel, H. et al., eds.: ECML-PKDD, Part III, vol. 8190, LNCS, Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 26(4)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition systems under attack: Design issues and research challenges", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "Int\u2019l J. Patt. Rec. Artif. Intell. 28(7)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "4th ACM Workshop Artif. Intell. and Sec. (AISec), Chicago, IL, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Adversarial feature selection against evasion attacks", "author": ["F. Zhang", "P. Chan", "B. Biggio", "D. Yeung", "F. Roli"], "venue": "IEEE Trans. Cyb. 46(3)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "J. Mach. Learn. Res. 10", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimization for Machine Learning", "author": ["S. Sra", "S. Nowozin", "S.J. Wright"], "venue": "The MIT Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A simple geometric interpretation of SVM using stochastic adversaries", "author": ["R. Livni", "K. Crammer", "A. Globerson", "Edmond", "E.i.", "L. Safra"], "venue": "JMLR W&CP - vol. 22 of AISTATS \u201912.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On sparse feature attacks in adversarial learning", "author": ["F. Wang", "W. Liu", "S. Chawla"], "venue": "IEEE Int\u2019l Conf. on Data Mining (ICDM), IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn. 20", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Duality and geometry in SVM classifiers", "author": ["K.P. Bennett", "E.J. Bredensteiner"], "venue": "17th ICML, Morgan Kaufmann Publishers Inc.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "1-norm support vector machines", "author": ["J. Zhu", "S. Rosset", "R. Tibshirani", "T.J. Hastie"], "venue": "In Thrun, S., Saul, L., Sch\u00f6lkopf, B., eds.: NIPS 16, MIT Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR", "author": ["Bondell", "Reich"], "venue": "Biometrics 64", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y LeCun"], "venue": "Int\u2019l Conf. Artif. Neural Networks.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "TREC 2007 spam track overview", "author": ["G.V. Cormack"], "venue": "In Voorhees, E.M., Buckland, L.P., eds.: TREC. Volume Special Publication 500-274., NIST", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surv. 34", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A pattern recognition system for malicious PDF files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "In Perner, P., ed.: Mach. Learn. and Data Mining in Patt. Rec. vol. 7376, LNCS, Springer Berlin Heidelberg", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society, Series B 67(2)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Poisoning attacks against SVMs", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "In Langford, J., Pineau, J., eds.: 29th ICML, Omnipress", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Is feature selection secure against training data poisoning", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "eds.: 32nd ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 2, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 3, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 4, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 5, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 6, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 7, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 8, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 9, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 10, "context": ", spam emails can be manipulated (at test time) to evade a trained anti-spam classifier [1,2,3,4,5,6,7,8,9,10,11,12].", "startOffset": 88, "endOffset": 116}, {"referenceID": 5, "context": "For instance, the widely-used SpamAssassin anti-spam filter exploits a linear classifier [7,5].", "startOffset": 89, "endOffset": 94}, {"referenceID": 4, "context": "For instance, the widely-used SpamAssassin anti-spam filter exploits a linear classifier [7,5].", "startOffset": 89, "endOffset": 94}, {"referenceID": 3, "context": "investigated the security of linear classifiers to evasion attacks [4,7], suggesting the use of more evenly-distributed feature weights as a mean to improve their security.", "startOffset": 67, "endOffset": 72}, {"referenceID": 5, "context": "investigated the security of linear classifiers to evasion attacks [4,7], suggesting the use of more evenly-distributed feature weights as a mean to improve their security.", "startOffset": 67, "endOffset": 72}, {"referenceID": 11, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 12, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 13, "context": "In this work, we shed some light on the security of linear classifiers, leveraging recent findings from [13,14,15] that highlight the relationship between classifier regularization and robust optimization problems in which the input data is potentially corrupted by noise (see Sect.", "startOffset": 104, "endOffset": 114}, {"referenceID": 11, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 12, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 13, "context": "Connecting the work in [13,14,15] to adversarial machine learning aims to help understanding what the optimal regularizer is against different kinds of adversarial noise (attacks).", "startOffset": 23, "endOffset": 33}, {"referenceID": 6, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 7, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 8, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 9, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 72, "endOffset": 83}, {"referenceID": 11, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 12, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 13, "context": "In this section, we summarize the attacker model previously proposed in [8,9,10,11], and the link between regularization and robustness discussed in [13,14,15].", "startOffset": 149, "endOffset": 159}, {"referenceID": 6, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 7, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 8, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 9, "context": "To rigorously analyze possible attacks against machine learning and devise principled countermeasures, a formal model of the attacker has been proposed in [6,8,9,10,11], based on the definition of her goal (e.", "startOffset": 155, "endOffset": 168}, {"referenceID": 6, "context": ", a spam email) to have it misclassified as legitimate (with the largest confidence) by the classifier [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "The attacker can have different levels of knowledge of the targeted classifier; she may have limited or perfect knowledge about the training data, the feature set, and the classification algorithm [8,9].", "startOffset": 197, "endOffset": 202}, {"referenceID": 7, "context": "The attacker can have different levels of knowledge of the targeted classifier; she may have limited or perfect knowledge about the training data, the feature set, and the classification algorithm [8,9].", "startOffset": 197, "endOffset": 202}, {"referenceID": 14, "context": "As discussed in [16], two kinds of constraints have been mostly used when modeling real-world adversarial settings, leading one to define sparse (`1) and dense (`2) attacks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "This amounts to (slightly) blurring the image, instead of obtaining a salt-and-pepper noise effect (as the one produced by sparse attacks) [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 1, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 6, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 7, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 10, "context": ", the number of modified words in each spam) [1,2,8,9,12].", "startOffset": 45, "endOffset": 57}, {"referenceID": 11, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 12, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 13, "context": "The goal of this section is to clarify the connection between regularization and input data uncertainty, leveraging on the recent findings in [13,14,15].", "startOffset": 142, "endOffset": 152}, {"referenceID": 11, "context": "[13] have considered the following robust optimization problem:", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Typical examples of uncertainty sets according to the above definition include `1 and `2 balls [13,14].", "startOffset": 95, "endOffset": 102}, {"referenceID": 12, "context": "Typical examples of uncertainty sets according to the above definition include `1 and `2 balls [13,14].", "startOffset": 95, "endOffset": 102}, {"referenceID": 11, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 12, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 13, "context": ", a typical setting in robust optimization [13,14,15].", "startOffset": 43, "endOffset": 53}, {"referenceID": 11, "context": "3 in [13]):", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "This means that, if the `2 norm is chosen as the dual norm characterizing the uncertainty set U , then w is regularized with the `2 norm, and the above problem is equivalent to a standard Support Vector Machine (SVM) [17].", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": ", the standard SVM [17], the Infinity-norm SVM [18] and the 1-norm SVM [19] against `2, `1 and `\u221e-norm uncertainty models, respectively.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "The result discussed in the previous section, similar to that reported independently in [15], helps un2 Note that the `1 norm is the dual norm of the `\u221e norm, and vice-versa, while the `2 norm is the dual norm of itself.", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "In fact, what discussed in the previous section does not only confirm the intuition in [4,7], i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 5, "context": "In fact, what discussed in the previous section does not only confirm the intuition in [4,7], i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 11, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 12, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 13, "context": "The result in [13,14,15] also clarifies the meaning of uniformity of the feature weights w.", "startOffset": 14, "endOffset": 24}, {"referenceID": 19, "context": "To visually show how evasion attacks work, we perform sparse and dense attacks on the MNIST digit data [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "As in [8], we simulate an adversarial classification problem where the digits 8 and 9 correspond to the legitimate and malicious class, respectively.", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "3 Note that octagonal regularization has been previously proposed also in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "For our experiments we use the TREC 2007 spam track data, consisting of about 25000 legitimate and 50000 spam emails [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "We extract a dictionary of terms (features) from the first 5000 emails (in chronological order) using the same parsing mechanism of SpamAssassin, and then select the 200 most discriminant features according to the information gain criterion [23].", "startOffset": 241, "endOffset": 245}, {"referenceID": 2, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 3, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 6, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 7, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 10, "context": "Adding or removing a term amounts to switching the value of the corresponding Boolean feature [3,4,8,9,12].", "startOffset": 94, "endOffset": 106}, {"referenceID": 22, "context": "We represent every file using the 114 features that are described in [24].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "This is the standard SVM learning algorithm [17].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": ",d |wj | [20]:", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Its learning algorithm is defined as [19]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "We use here the elastic-net regularizer [25], combined with the hinge loss to obtain an SVM formulation with tunable sparsity:", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "To evaluate security of linear classifiers, we define a measure E of weight evenness, similarly to [4,7], based on the ratio of the `1 and `\u221e norm:", "startOffset": 99, "endOffset": 104}, {"referenceID": 5, "context": "To evaluate security of linear classifiers, we define a measure E of weight evenness, similarly to [4,7], based on the ratio of the `1 and `\u221e norm:", "startOffset": 99, "endOffset": 104}, {"referenceID": 6, "context": "These modified digits are obtained by solving Problem (1)-(2) through a simple projected gradient-descent algorithm, as in [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}, {"referenceID": 24, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}, {"referenceID": 25, "context": "Finally, we think that an interesting extension of our work may be to investigate the trade-off between sparsity and security also in the context of classifier poisoning (in which the attacker can contaminate the training data to mislead classifier learning) [9,26,27].", "startOffset": 259, "endOffset": 268}], "year": 2017, "abstractText": "Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks. This can be considered a relevant problem, as linear classifiers have been increasingly used in embedded systems and mobile devices for their low processing time and memory requirements. We exploit recent findings in robust optimization to investigate the link between regularization and security of linear classifiers, depending on the type of attack. We also analyze the relationship between the sparsity of feature weights, which is desirable for reducing processing cost, and the security of linear classifiers. We further propose a novel octagonal regularizer that allows us to achieve a proper trade-off between them. Finally, we empirically show how this regularizer can improve classifier security and sparsity in real-world application examples including spam and malware detection.", "creator": "LaTeX with hyperref package"}}}