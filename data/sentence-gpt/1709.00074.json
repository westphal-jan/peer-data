{"id": "1709.00074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Unsupervised Learning of Semantic Mappings", "abstract": "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. In other words, we will use a set of generic mappings to create multiple mappings with all its functionality, including the standard mapping of each target. Given the potential of using multiple mappings to create a set of generic mappings, we could create multiple mappings using one of the following simple things:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Thu, 31 Aug 2017 20:42:12 GMT  (2131kb,D)", "http://arxiv.org/abs/1709.00074v1", null], ["v2", "Tue, 31 Oct 2017 11:46:56 GMT  (2167kb,D)", "http://arxiv.org/abs/1709.00074v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tomer galanti", "lior wolf"], "accepted": false, "id": "1709.00074"}, "pdf": {"name": "1709.00074.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning of Semantic Mappings", "authors": ["Tomer Galanti", "Lior Wolf"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples. For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN (Zhu et al., 2017) creates the analog zebra image for a new image of a horse and vice versa. These recent methods employ two types of constraints. First, when mapping from one domain to another, the output has to be indistinguishable from the samples of the new domain. This is enforced using GANs (Goodfellow et al., 2014) and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa. The second type of constraints enforces that for every single sample, transforming it to the other domain and back (by a composition of the mappings in the two directions) results in the original sample. This is enforced for each training sample from either domain: every training image of a horse (zebra), which is mapped to a zebra (horse) image and then back to the source domain, should be as similar as possible to the original input image. In another example, taken from DiscoGAN (Kim et al., 2017), a function is learned to map a handbag to a shoe of a similar style. One may wonder why striped bags are not mapped, for example, to shoes with a checkerboard pattern. If every striped pattern in either domain is mapped to a checkerboard pattern in the other and vice-versa, then both the distribution constraints and the circularity constraints might hold. The former could hold since both striped and checkerboard patterned objects would be generated. Circularity could hold since, for example, a striped object would be mapped to a checkerboard object in the other domain and then back to the original striped object. One may claim that the distribution of striped bags is similar to those of striped shoes and that the distribution of checkerboard patterns is also the same in both domains. In this case, the alignment follows from fitting the shapes of the distributions. This explanation is unlikely, since no effort is being made to create handbags and shoes that have the same distributions of these properties, as well as many other properties.\nar X\niv :1\n70 9.\n00 07\n4v 1\n[ cs\n.L G\n] 3\n1 A\nug 2\n01 7"}, {"heading": "2. The Unsupervised Alignment Problem", "text": "The learning algorithm is provided with only two unlabeled datasets: one includes i.i.d samples from the first distribution and the second includes i.i.d samples from the other distribution.\nxi \u2208 XA for i = 1 . . .m where xi i.i.d\u223c DA and XA denotes the space of domain A = (XA, DA) xj \u2208 XB for j = 1 . . . n where xj i.i.d\u223c DB and XB denotes the space of domain B = (XB , DB) (1)\n(all notations are listed in the appendix, see Tab. 1). To semantically tie the two distributions together, our model is based on a generative approach, which is well aligned with the success of GAN-based image generation, e.g., (Radford et al., 2015), in mapping random input vectors into realistic-looking images. Let z \u2208 X be a random vector that is distributed according to the distribution DZ and which we employ to denote the semantic essence of samples in XA and XB . We denote DA = yA \u25e6 DZ and DB = yB \u25e6 DZ , where the functions yA : X \u2192 XA and yB : X \u2192 XB , and f \u25e6D denotes the distribution of f(x), where x \u223c D. It makes sense to assume that both yA and yB are invertible, since given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision (Chen et al., 2016). We denote by yAB = yB \u25e6 y\u22121A , the function that maps the first domain to the second domain. It is semantic in the sense that it goes through the shared semantic space X . The goal of the learner is to fit a function h \u2208 H, for some hypothesis classH that is closest to yAB ,\ninf h\u2208H RDA [h, yAB ], (2)\nwhere RD[f1, f2] = Ex\u223cD`(f1(x), f2(x)), for a loss function ` : R\u00d7 R\u2192 R and a distribution D. It is not clear that such fitting is possible without further information. Assume, for example, that there is a natural order on the samples in XB . A mapping that maps an input sample x \u2208 XA to the sample that is next in order to yAB(x) could be just as feasible. More generally, one can permute the samples in XA by some function \u03a0 that replaces each sample with another sample that has a similar likelihood (the formal definition is given in Sec. 4) and learn h that satisfies h = yAB \u25e6\u03a0. We call this difficulty \u201cthe alignment problem\u201d and our work is dedicated to understanding the plausibility of learning despite this problem. In the cross domain transfer line of work (Taigman et al., 2017), the alignment problem is dealt with by incorporating a fixed pre-trained feature map f and requiring what is called f -constancy, namely that the following risk is small RDA [f, f \u25e6 h], or informally, f(x) = f(h(x)). In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed. Circularity requires the recovery of both yAB and yBA = yA \u25e6 y\u22121B simultaneously. Namely, functions h and h\u2032 are learned jointly by minimizing the risk:\ninf h,h\u2032\u2208H\ndiscC(h \u25e6DA, DB) + discC(h\u2032 \u25e6DB , DA)\n+RDA [h \u2032 \u25e6 h, IdA] +RDB [h \u25e6 h\u2032, IdB ]\n(3)\nwhere discC(D1, D2) = supc1,c2\u2208C |RD1 [c1, c2]\u2212RD2 [c1, c2]| denotes the discrepancy between distributions D1 and D2 that is implemented with a GAN (Ganin et al., 2016). The first term in Eq. 3 ensures that the samples generated by mapping domain A to domain B follow the distribution of samples in domain B. The second term is the analog term for the mapping in the other direction. The last two terms ensure that mapping a sample from one domain to the second and back, results in the original sample. While the circularity constraints, expressed as the last two terms in Eq. 3, are elegant and do not require additional supervision, for every invertible permutation \u03a0 of the samples in domain B (not to be confused with a permutation of the vector elements of the representation of samples in B) we have\n(h\u2032 \u25e6\u03a0\u22121) \u25e6 (\u03a0 \u25e6 h) = h \u25e6 h\u2032 \u2248 IdA, and (\u03a0 \u25e6 h) \u25e6 (h\u2032 \u25e6\u03a0\u22121) = \u03a0 \u25e6 (h\u25e6h\u2032) \u25e6\u03a0\u22121 \u2248 \u03a0 \u25e6 IdB \u25e6\u03a0\u22121 = IdB .\n(4)\nTherefore, every circularity preserving h and h\u2032 gives rise to many possible solutions of the form h\u0303 = h \u25e6\u03a0 and h\u0303\u2032 = \u03a0\u22121 \u25e6 h\u2032. If \u03a0 happens to satisfy DB(x) \u2248 DB(\u03a0(x)), then the discrepancy terms in Eq. 3 also remain largely unchanged. Circularity by itself cannot, therefore, explain the recent success of unsupervised mapping."}, {"heading": "2.1 An Illustrative Example", "text": "Despite the availability of a large number of alternative hypotheses h\u2032 that satisfy the constraints of Eq. 3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig. 1. Consider the domain A of uniformly distributed points (x1, x2)> \u2208 R2, where 0 \u2264 x1 < 1 and x2 = 0.5. Let B be a similar domain, except x2 = 2. We are interested in learning the mapping y2DAB((x1, 0.5)\n>) = (x1, 2)>. We note that there are infinitely many mappings from domain A to B that satisfy the constraints of Eq. 3. However, when we learn the mapping using a neural network with one hidden layer of size 2, and Leaky ReLU activations1 (Maas et al., 2013), y2DAB is one of only two options. In this case h(x) = \u03c3a(Wx+ b), for W \u2208 R2\u00d72,b \u2208 R2 and where \u03c3a is applied per coordinate. The only admissible solutions are of the form Wb =\n( 1 \u22122b1 0 4\u2212 2b2 ) or W \u2032b = ( \u22121 1\u2212 2b1 0 4\u2212 2b2 ) and b = (b1, b2)>, which are identical, for every b, to\ny2DAB or to an alternative y 2D\u2032 AB ((x1, 0.5) >) = (1\u2212 x1, 2)>. Exactly the same situation holds for any pair of line segments in Rd+. Therefore, by restricting the hypothesis space of h, we eliminate all alternative solutions, except two. These two are exactly the two mappings that would commonly be considered \u201cmore semantic\u201d than any other mapping, and can be expressed as the simplest possible mapping through a shared one dimensional space. While this is an extreme example, we are able to show that limiting the complexity of the admissible solutions\n1. \u03c3a(x) = Ind[x < 0]ax+ Ind[x \u2265 0]x, for the indicator function Ind[q] which maps a true value to one, zero otherwise.\neliminates the solutions that are derived from yAB by permuting the samples in the space XA, since such mixing requires added complexity."}, {"heading": "2.2 Informal Statement of the Main Results", "text": "In this work, we show that what separates h \u2248 yAB and h\u0303 \u2248 \u03a0 \u25e6 yAB is the complexity of these functions. Namely, that the complexity of h is much lower and, therefore, it is learnable with much smaller networks than the alternatives. Therefore, in order to learn h and not h\u0303, all that one needs is to use a network that is not \u201ctoo big\u201d. To show this, we develop a new framework for measuring the complexity of composition of functions. Our function complexity framework measures the complexity of a function as the depth of a neural network which implements it, or the shallowest network, if there are multiple such networks. In other words, we use the number of layers of a network as a proxy for the Kolmogorov complexity of functions, using layers in lieu of the primitives of the universal Turing machines, which is natural for studying functions that can be computed by feedforward neural networks. To make it a useful tool, an elaborate theoretical system is developed for this complexity framework. The system is based on well-justified assumptions and is presented in the next section. In Sec. 4, we apply the new framework and study the properties of the low-complexity solutions when learning in an unsupervised manner. We provide a complexity-based definition to the illusive notion of semantics. A semantic mapping is a mapping with the lowest complexity among all permuted versions of it, i.e., it is the most straightforward one in this set. Using this notion, we are able to state our main results informally as: (1) There are only a handful of semantic mappings between two domains. (2) every semantic function between two domains A and B either passes through a shared space Z, or is very different than any composition of a semantic function form A to Z with a semantic function from Z to B. Therefore, if there is a semantic function through Z, one obtains it by learning a minimal network between A and B, or alternatively, but unlikely, learns a completely different function through another shared space Z \u2032. Based on our results, we are able to make concrete predictions. The first one, which is empirically validated in the appendix, states that in contrast to the current common wisdom, one can learn a semantic mapping between two spaces without any matching samples and even without circularity.\nPrediction 1 When learning with a small enough network in an unsupervised way a mapping between domains that share common characteristics, the GAN constraint in the target domain is sufficient to obtain a semantic mapping.\nThe strongest clue that helps identify the semantic mapping from the other mappings is the suitable complexity of the network that is learned. A network with a complexity that is too low cannot replicate the target distribution, when taking inputs in the source domain. A network that has a complexity that is too high, would not learn the semantic mapping. We believe that the success of the recent methods arises from selecting the architecture used in an appropriate way. For example, DiscoGAN (Kim et al., 2017) employs either eight or ten layers, depending on the dataset. We make the following prediction, which is also validated empirically in the appendix:\nPrediction 2 When learning in an unsupervised way a mapping between domains, the size of the network needs to be carefully adjusted.\nThis prediction is also surprising, since in supervised learning, extra depth is not as detrimental, if at all. As far as we know, this is the first time that this clear distinction between supervised and unsupervised learning is made. Although our first prediction states that circularity is not needed, there does seem to be an advantage to using it. We show that using circularity, ambiguity is reduced in a very specific way. Namely, the potential family of permutations \u03a0 that extend the semantic solution h to the alternatives \u03a0 \u25e6 h are constrained to be of lower complexity. Therefore, the argument made following Eq. 4 holds only for a very limited set of possible permutations \u03a0."}, {"heading": "3. A Complexity Measure for Functions", "text": "In order to model the composition of neural networks, we define a complexity measurement that assigns a value based on the number of simple functions that make up a complex function.\nDefinition 1 (Stratified complexity model (SCM)) A stratified complexity model N := SCM[C] is a hypothesis class of functions p : RM \u2192 RM specified by a set of functions C. Every function p in N has an appropriate decomposition:\n\u2022 There are p1, ..., pn \u2208 C such that p = pn \u25e6 pn\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 p1 (if n = 0 then p = Id and if n = 1 then p = p1).\n\u2022 Every function in C is invertible. Informally, a SCM partitions a set of invertible functions into disjoint complexity classes,\nC0 := {Id} Cn := { p = pn \u25e6 ... \u25e6 p1 \u2223\u2223\u2223pn \u25e6 ... \u25e6 p1 is an appropriate decomposition of p } \\ n\u22121\u22c3\ni=0\nCi (5)\nWhen considering simple functions pi that are layers in a neural network, each complexity class contains the functions that are implemented by networks of n layers. In addition, we denote the complexity of a function p:\nC(p) := argn{p \u2208 Cn} (6) If the complexity of a function p equals n, then any appropriate decomposition p = pn \u25e6 ... \u25e6 p1 will be called a minimal decomposition of p. If there is no such n, we denote C(p) =\u221e. According to this measurement, the complexity of a function p is determined by the minimal number of primitive functions required in order to represent it. It is, therefore, necessary to understand the rules that dictate the complexity of a composition of functions, inverse functions, etc. We begin with the simplest relationship that occurs between two composed functions.\nDefinition 2 (Unfused functions) Let N = SCM[C] and let p, q \u2208 N be any two functions. We say that p is unfused in q if C(p \u25e6 q) = C(p) + C(q) and denote p 7 q. Otherwise, we say that p is fused in q and denote p . q. In addition, the function p and q will be called left and right partial functions of p \u25e6 q (resp.). Informally, a function p is unfused in another function q, if the first operations of p do not invert the last processing steps of q. For instance, if we can represent p = g1 \u25e6 g2 and q = g\u221212 \u25e6 g3 such that C(g1) < C(p) and C(g3) < C(q), then (by Lem. 4 below) the two functions are fused. In addition to characterizing the result of composing two functions, we also define a measure for the complexity of transforming one function into the other. The conditional complexity between the functions p and q is the complexity of the function g that satisfies: p = g \u25e6 p. Definition 3 (Conditional complexity) Let N = SCM[C] and let p, q \u2208 N be any two functions. The conditional complexity between p and q is denoted: C(p||q) := C(p \u25e6 q\u22121). In this work, we focus our attention on SCMs that represent the architectures of fully connected neural networks with layers of a fixed size, i.e.,\nDefinition 4 (NN-SCM) A NN-SCM is a SCM N = SCM[C] that satisfies the following conditions:\n\u2022 C = { \u03c3 \u25e6W \u2223\u2223\u2223W \u2208 RM\u00d7M and W is invertible }\n. Here, W denotes both a linear transformation and the associated matrix form.\n\u2022 \u03c3 is a non-linear element-wise activation function. For brevity, we denote N := SCM[\u03c3] to refer to a NN-SCM with the activation function \u03c3. The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used)."}, {"heading": "3.1 Semantic mappings", "text": "The following definition of a semantic mapping is both intuitive and well defined in concrete complexity terms. We say that given two distributions DA and DB , a semantic mapping f : XA \u2192 XB between domains A and B is a mapping that has minimal complexity among the functions h : XA \u2192 XB that satisfy h \u25e6DA \u2248 DB . Consider, again, the example of a line segment in RM (Sec. 2.1) and the semantic space of the [0, 1] \u2282 R interval. The two linear mappings, which map either segment ends to 0 and the other to 1 are semantic, when using f that are ReLU based neural networks. Other mappings to this segment are possible, simply by permuting points on the segment in RM . However, these are much more complicated. In order to measure the distance between h \u25e6DA and DB we will use the discrepancy distance, discD. In this work, we will focus on classes of discriminators D of the form Dm := {u|C(u) \u2264 m} for some m \u2208 N. In addition, for simplicity, we will write discm := discDm .\nDefinition 5 (Semantic mapping) Let N = SCM[C]. Let A = (XA, DA) and B = (XB , DB) be two domains. We define the (m, 0)-semantic complexity between A and B as:\nCm, 0A,B := min i\u2208N\u222a{0} {\u2203h s.t C(h) = i and discm(h \u25e6DA, DB) \u2264 0} (7)\nThe set of (m, 0)-semantic functions between A and B is: H 0(DA, DB ;m) := H 0(DA, DB ;m,C m, 0 A,B ), where H 0(DA, DB ;m, k) := { h \u2223\u2223\u2223C(h) \u2264 k and discm(h \u25e6DA, DB) \u2264 0 } . (8)\nWe note that for any fixed 0 > 0, the sequence {Cm, 0A,B }\u221em=0 is monotonically increasing asm tends to infinity. In addition, we assume that for every two distributions of interest, DI and DJ , and an error rate 0 > 0, there is a function h of finite complexity such that disc\u221e(h \u25e6DI , DJ) \u2264 0. Therefore, the sequence {Cm, 0A,B }\u221em=0 is upper bounded by C(h) for all m \u2208 N \u222a {0}. In particular, there is a minimal value m0 > 0 such that Cm, 0A,B = C m0, 0 A,B for all m \u2265 m0. We denote: E 0A,B := m0 and C 0A,B := Cm0, 0A,B ."}, {"heading": "3.2 Identifiability", "text": "Every neural network implementation gives rise to many alternative implementations by performing simple operations, such as permuting the units of any hidden layer, and then permuting back as part of the linear mapping in the next layer. Therefore, it is first required to identify and address the set of transformations that could be inconsequential to the function which the network computes.\nDefinition 6 (Invariant set) Let N = SCM[\u03c3] be a NN-SCM. The invariant set Invariant(N ) is the set of all \u03c0 : RM \u2192 RM that satisfy the following conditions:\n\u2022 \u03c0 : RM \u2192 RM is an invertible linear transformation.\n\u2022 \u03c3 \u25e6 \u03c0 = \u03c0 \u25e6 \u03c3.\nFunctions in Invariant(N ) are called invariants or invariant functions.\nFor example, for neural networks with the tanh activation function, the set of invariant functions contains the linear transformations that take vectors, permute them and multiply each coordinate by \u00b11. Formally, each \u03c0 = [ 1 \u00b7 et(1), ..., M \u00b7 et(M)]> where ei is the i\u2019th standard basis vector, t is a permutation over [M ] and i \u2208 {\u00b11} (Fefferman and Markel, 1993). Our analysis is made much simpler, if every function has one invariant representation up to a sequence of manipulations using invariant functions that do not change the essence of the processing at each layer.\nDefinition 7 (Identifiability of minimal representation) A NN-SCM N = SCM[\u03c3] obeys identifiability of minimal representation with respect to Invariant(N ), if for all n \u2208 N \u222a {0} and p \u2208 Cn such that there\nare two appropriate decompositions p = pn \u25e6 ... \u25e6 p1 and p = qn \u25e6 ... \u25e6 q1, then there are invariants \u03c01, ..., \u03c0n \u2208 Invariant(N ) such that:\nq1 = \u03c3 \u25e6 (\u03c01 \u25e6W1) and p1 = \u03c3 \u25e6W1 \u22001 < i < n : qi = \u03c3 \u25e6 (\u03c0i \u25e6Wi \u25e6 \u03c0\u22121i\u22121) and pi = \u03c3 \u25e6Wi qn = \u03c3 \u25e6 (Wn \u25e6 \u03c0\u22121n\u22121) and pn = \u03c3 \u25e6Wn\n(9)\nWe consider that since any \u03c0 \u2208 Invariant(N ) commutes with \u03c3, then, an alternative writing could be:\nq1 = \u03c01 \u25e6 p1, \u2200i = 2, ..., n\u2212 1 : qi = \u03c0i \u25e6 pi \u25e6 \u03c0\u22121i\u22121 and qn = pn \u25e6 \u03c0\u22121n\u22121 (10)\nA stronger identifiability condition requires that every non-minimal implementation of a function p goes through the same processing steps as dictated by the layers of the minimal representation, where each of these steps can be mapped to multiple layers in the longer implementation.\nDefinition 8 (Identifiability) Let N = SCM[\u03c3] be a NN-SCM obeying identifiability of minimal representation. We say that N obeys identifiability if for every function p \u2208 N , such that p = pn \u25e6 ... \u25e6 p1 is a minimal decomposition, if p = qm \u25e6 ... \u25e6 q1, then:\n\u2203j1 = 1 < ... < jn+1 = m+ 1, \u03c01, ..., \u03c0n \u2208 Invariant(N ) : qj2:j1 = \u03c01 \u25e6 p1, \u2200i = 2, ..., n\u2212 1 : qji+1:ji = \u03c0i \u25e6 pi \u25e6 \u03c0\u22121i\u22121 and qjn+1:jn = pn \u25e6 \u03c0\u22121n\u22121\n(11)\nHere, if u = un \u25e6 ... \u25e6 u1 then ui+1:j = ui \u25e6 ... \u25e6 uj if j \u2264 i and ui:i = Id.\nIn the context of neural networks, the general question of uniqueness up to invariants, also known as identifiability, is an open question. Nevertheless, several authors have made progress in this area for different neural network architectures. The most notable work has been done by Fefferman and Markel (1993) that proves identifiability for \u03c3 = tanh. Furthermore, the representation is unique up to the invariant functions. Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurkov\u00e1 and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions. As far as we know, there are no recent results continuing this line of work for activation functions such as Leaky ReLU. Uniqueness, which is stronger than identifiability, since it means that even multiple representations with different number of layers do not exist, does not hold for these activation functions. To see this, note that for every M \u00d7M mapping W , the following holds:\n\u03c3 \u25e6W = (\u03c3 \u25e6W ) \u25e6 (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (12)\nwhere \u03c3 is the Leaky ReLU activation function with parameter a. We conjecture that for networks with Leaky ReLU activations identifiability holds, or at least for networks with a fixed number of neurons per layer."}, {"heading": "3.3 Properties of Inverses and Compositions", "text": "It is necessary to study the effect of inversion on the complexity of functions, since, for example, we care about both h\u2032 = \u03a0 \u25e6 h and h = \u03a0\u22121 \u25e6 h\u2032.\nDefinition 9 Let N = SCM[C] be an SCM. We say that N is d-inverse-complexity-preserving (d-ICP for short) if: \u2200p \u2208 C : C(p\u22121) \u2264 d \u00b7C(p). Sometimes, we will omit writing I(N ) and write I instead, when N is obvious from the context.\nAn immediate consequence of the following theorem is that neural networks with Leaky ReLU activations are 3-ICP, see also Lem. 11 in the appendix, which is part of the theorem\u2019s proof (all proofs can be found in the appendix).\nTheorem 1 Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is the Leaky ReLU with parameter a > 0. Then, for any u \u2208 N , |C(u\u22121)\u2212 C(u)| is either 0 or 2. The case C(u\u22121) = C(u) holds, for example, for invertible linear mappings because every linear mapping W can be expressed as\n(\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212W/a) = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212Id/a)\ufe38 \ufe37\ufe37 \ufe38 =Id \u25e6W,\nwhere \u03c3 is the Leaky Relu activation function with a parameter a. Nevertheless, for a generic function u, C(u\u22121) = C(u) + 2, as the following theorem shows.\nTheorem 2 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n such that C(u\u22121n+1:1) = C(un+1:1) + 2 and ui = \u03c3 \u25e6Wi (for i \u2208 [n]) is open and dense in RM\u00d7M\u00d7n. As we discussed earlier, two functions f and g can be either fused or unfused. In the following Theorem, it is shown that a generic decomposition un+1:1 is unfused, i.e, C(un+1:1) = n. In particular, for generic functions f and g are unfused.\nTheorem 3 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n such that C(un+1:1) = n where ui = \u03c3 \u25e6Wi (for i \u2208 [n]) is open and dense in RM\u00d7M\u00d7n.\nIt is still important to cover both fused and unfused functions, since our main results are \u201cfor every\u201d and not \u201cfor general\u201d due to the fact that we study specific functions, and specifically we study relations of the form h\u2032 = \u03a0 \u25e6 h, and look for the h\u2032 with the minimal complexity, for which, C(h\u2032) < C(h). If two functions are known to be unfused then, from definition, the complexity of their composition is exactly the sum of their complexities. It is natural to assume that the same holds for the composition of three unfused functions. However, this is not necessarily the situation when the middle function in this composition has a complexity that is lower than the ICP factor I (see Lem. 13 in the appendix). However, when the function in the middle of the composition has a complexity \u2265 I , the complexity of the composition is the sum of the complexities.\nTheorem 4 LetN = SCM[\u03c3] be a NN-SCM obeying identifiability. Then, for all f, g, h \u2208 N such that f 7 g, g 7 h and I \u2264 C(g), we have:\nC(f \u25e6 g \u25e6 h) = C(f) + C(g) + C(h) (13)"}, {"heading": "4. Learning Semantic Mappings", "text": "In this section, we present the theoretical foundations for unsupervised alignment algorithms. In the unsupervised alignment problem, the algorithms are provided with only two unmatched datasets of samples from the domains A and B and the task is to learn a semantic function between them. The goal of this section is to understand under which constraints one is able to learn the semantic mapping using unsupervised alignment."}, {"heading": "4.1 Counting Semantic Mappings", "text": "Recall that discm is the discrepancy distance for discriminators of complexity up to m. We have discussed the functions \u03a0 which replaces between members in the domain B that have similar probabilities. Formally, these are defined using the discrepancy distance.\nDefinition 10 (Density preserving mapping) Let N = SCM[C] and D a distribution. A (m, 0)-density preserving mapping over D (or an (m, 0)-DPM for short) is a function \u03a0 such that\ndiscm(\u03a0 \u25e6D,D) \u2264 0 (14) We denote (m, 0)-DPMs of complexity k by DPM 0(D;m, k) := { \u03a0 \u2223\u2223discm(\u03a0 \u25e6D,D) \u2264 0 and C(\u03a0) = k } .\nWe would like to bound the number of shared semantic distributions by the number of DPMs. We consider that there are infinitely many DPMs and semantic mappings. For example, if we slightly perturb the weights of a minimal representation of DPM, \u03a0, we obtain a new DPM. Therefore, we define a relation between of functions that reflects whether the two are similar. In this way, we are able to bound the number of different (not-similar) semantic mappings by the number of different DPMs.\nDefinition 11 (Similarity between pairs of distributions or functions) Let N = SCM[C].\n\u2022 Distributions D1 and D2 are (m, 0)-similar and we denote\nD1 \u223c m, 0 D2 \u21d0\u21d2 discm(D1, D2) \u2264 0 (15)\n\u2022 Functions f and g are (D,m, 0)-similar and we denote f D\u223c m, 0 g, if C(f) = C(g) =: n and there are\nminimal decompositions: f = fn+1:1 and g = gn+1:1 such that\n\u2200i \u2208 [n] : fi+1:1 \u25e6D \u223c m, 0 gi+1:1 \u25e6D (16)\nThe defined similarity is reflexive and symmetric, but not transitive. Therefore, there are many different ways to partition the space of functions into disjoint subsets such that in each subset, any two functions are similar. We count the number of functions up to the similarity as the minimal number of subsets required in order to cover the entire space. This idea is presented in the following Def. 12.\nDefinition 12 (Covering numbers) Let (U ,\u223cU ) be a set and a reflexive and symmetric relation. A covering of (U ,\u223cU ), is a tuple (U ,\u2261U ) such that: \u2261U is an equivalence relation and u1 \u2261U u2 =\u21d2 u1 \u223cU u2. The covering number of (U ,\u223cU ) is:\nmin \u2223\u2223U/ \u2261U \u2223\u2223 s.t: the minimum is taken over (U ,\u2261U ) that is a covering of (U ,\u223cU ) (17)\nWe denote the covering number of (U ,\u223cU ) by Covering(U ,\u223cU ). Here, U/ \u2261U is the quotient set of U by \u2261U .\nInformally, the following theorem states that the number of semantic mappings is upper bounded by the square root of the number of DPMs of size 2C 0A,B + 2. This result is useful since DPMs are expected to be rare in real-world domains . When imagining mapping a space to itself, in a way that preserves the distribution, one first considers symmetries. Near-perfect symmetries are rare in natural domains, and when these occur, e.g., (Kim et al., 2017), they form well-understood ambiguities. Another option that can be considered is that of replacing specific samples in domain B with other samples of the same probability. However, these very local discontinuous mappings are of very high complexity, since this complexity is required in order to reduce the modeling error for discontinuous functions. One can also consider replacing larger sub-domains with other sub-domains such that the distribution is preserved. This could be possible, for example, if the distribution within the sub-domains is almost uniform (unlikely), or if it is estimated inaccurately due to the limitations of the training set. Thm. 5 employs the following weak assumption. In Lem. 35 in the Appendix we prove that this assumption holds for the case of a continuous risk if the discriminators have bounded weights.\nAssumption 1 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 \u25e6D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1).\nTheorem 5 (Counting semantic mappings) Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is a Leaky ReLU with parameter a > 0 and assume Assumption 1. Let 0, 1 and 2 < 1 \u2212 2 0 are three positive constants and A = (XA, DA) and B = (XB , DB) are two domains. Assume that m \u2265 k + 2C 0A,B + 5. Then,\nCovering ( H 0(DA, DB ;m),\nDA\u223c k, 1 ) \u2264 lim \u21920 \u221a Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) (19)\nAn interesting variant of this theorem follows from trying to count the number of mappings inH+ 0(DA, DB ;m) := H 0(DA, DB ;m) \u2229 (H 0(DB , DA;m))\u22121, i.e., the set of semantic mappings between A and B such that their inverse is semantic between B and A. This is the case of the circularity constraint, which requires both sides to be semantic. In this case, we obtain the same bound with min{2C 0A,B + 2, 2C 0B,A + 2}. Therefore, the number of two-sided semantic mappings is smaller. To see an example where a function is semantic while its inverse is not, consider the network representation of a linear function W = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212W/a). The inverse function W\u22121 is of the same complexity (2). However, a small perturbation of the network representation, which is still semantic, has an inverse complexity of 4, while the original inverse function W\u22121 is the semantic function and has a complexity of 2. Similar situations of higher complexities can be constructed, for example, by having linear functions as the top two layers of a neural network."}, {"heading": "4.2 Shared Semantic Distributions", "text": "The recovery of an analog in domain B for a sample in domain A, naturally takes place as a two step process: first recovering the properties of the source sample that can be transfered between the domains, and then generating a sample in B that has these properties. Therefore, when discussing analogies, there are elementary compositions that play a major role. For example, drawing semantic analogies (i.e., mapping semantically) between domain A and domain B through domain Z is naturally given by a composition of a mapping from A to Z and a mapping from Z to B.\nDefinition 13 (Z-mappings and shared semantic distributions) Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution.\n\u2022 The set of Z-mappings associated with DZ is denoted by: Z(DA, DZ , DB ;m, 0, 1) := H 1(DZ , DB ;m) \u25e6H 0(DA, DZ ;m) (20)\nIf 0 = 1 we write Z(DA, DZ , DB ;m, 0) for short. \u2022 DZ is a (m, 0, 1, 2)-shared semantic distribution between A and B if for all yB \u2208 H 1(DZ , DB ;m)\nand y\u22121A \u2208 H 0(DA, DZ ;m) we have: yB 7 y\u22121A and, Z(DA, DZ , DB ;m, 0, 1) \u2229H 2(DA, DB ;m) 6= \u2205 (21)\nIf 0 = 1 and 2 = 2 0 we write (m, 0) for short.\nIn general, there are many shared semantic distributions DZ between A and B. For example, DA itself is a shared semantic distribution. In addition, under mild assumptions (see Thm. 40), for every semantic function y \u2208 H 0(DA, DB ;m) and a minimal decomposition y = yn \u25e6 ... \u25e6 y1, for every i \u2264 n, yi+1:1 \u25e6DA is a shared semantic distribution between A and B. For simplicity, when considering a function h = g \u25e6 f \u2208 Z(DA, DZ , DB ;m, 0, 1) such that g \u2208 H 1(DZ , DB ;m) and f \u2208 H 0(DA, DZ ;m) we will simply write h = g \u25e6 f \u2208 Z(DA, DZ , DB ;m, 0, 1). A semantic mapping betweenA andB passes through a sequence of shared semantic distributionsDZ1 , ..., DZn . One may expect that there exists a semantic mapping in the other direction, from B to A, that passes through the same sequence. In the proof of the following theorem, it is shown that for the typical case of C 0+ 1B,A = C 0+ 1A,B + 2 (see Thm. 2), there is a semantic mapping that passes through (\u2212\u03c3\u22121) \u25e6DZn , ..., (\u2212\u03c3\u22121) \u25e6DZ1 . The case C 0+ 1B,A = C 0+ 1 A,B \u2212 2 is similar and we do not cover it in this paper.\nTheorem 6 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ 6= DA, DB is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let k \u2265 max { E 0A,B , E 4 0+4 1 B,A , E 4 0 B,A + C 0+ 1 A,Z + 1 } and m \u2265 k+3C 0+ 1A,B +4. Assume thatC3 0+ 1B,A = C 0+3 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B +2. Then, (\u2212\u03c3\u22121)\u25e6DZ is a (k, 1, 0, 0+ 1)-shared semantic distribution between B and A.\nThe importance of shared semantic distributions comes from the fact that they define the semantic functions that pass through them. Under reasonable assumptions, there is only one semantic function that passes through a given shared semantic distribution. In the following theorem, we study the relationship between a semantic mapping h and a shared semantic distribution DZ and show that h passes through DZ or is very different from any mapping that passes through DZ . The theorem assumes that the classes of the discriminators used in order to differentiate between the domains DA, DB , and DZ have sufficient capacity.\nTheorem 7 (Alignment) Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains. Let k \u2265 max { E2 0A,B , E 0 A,Z , E 0 Z,B } and m \u2265 k + C 0Z,B . Assume that DZ is a (m, 0)-shared semantic distribution between A and B such that C3 0Z,B = C 0 Z,B . Let h \u2208 H2 0(DA, DB ;m). Then, one of the following holds:\n\u2022 h = g \u25e6 f \u2208 Z(DA, DZ , DB ; k, 0, 3 0) such that g 7 f .\n\u2022 For every y \u2208 Z(DA, DZ , DB ;m, 0) we have: C(h||y) \u2265 2C 0Z,B \u2212 9.\nThis theorem requires that DZ is a shared semantic distribution and that h, g, and f are semantic functions. However, this result can be generalized, given a function b, to the domain b \u25e6DZ and the functions g \u25e6 b\u22121, b \u25e6 f as is shown in Lem. 41 and Lem. 43 in the Appendix."}, {"heading": "5. Discussion", "text": "The semantic mapping stands out of all alternative mappings, which allows it to be learned in an unsupervised manner. The usual generalization considerations still hold. For example, if all horse riders are covered in the training set, a shirtless rider in the test set (as demonstrated in (Zhu et al., 2017)) would become striped when converting the horse image to a zebra image. The limitations of unsupervised based learning that are due to symmetry, are also a part of our model. For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in (Kim et al., 2017), is similar in nature to the mapping of x to 1\u2212 x in the simple example given in Sec. 2.1. Such\nsymmetries occur when we can divide yAB into two functions yAB = y2 \u25e6 y1 such that a function \u03c0 in the invariant set is a DPM of y1 \u25e6DA and, therefore, DB \u2248 y2 \u25e6 \u03c0 \u25e6 y1. We base our work on the assumption of identifiability, which constitutes an open question for most activation functions. We hope that there would be a renewed interest in this question, which has been open for decades for networks with more than a single hidden layer and is unexplored for modern activation functions. The stratified complexity model (SCM) is related to structural risk minimization by Vapnik and Chervonenkis (1971), which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity. SCMs are specific to a hypothesis class of a certain recursive form, and the complexity classes are not nested. While we focused on unsupervised learning, the emergence of semantics from learning with a restricted capacity is widely applicable, e.g., to autoencoders, transfer learning, semi-supervised learning and elsewhere. As an extreme example, Sutskever et al. (2015) present empirical evidence that a semantic mapper can be learned, even from very few examples, if the network trained is kept small. We point to a key difference between supervised learning and unsupervised learning. While in the former, deeper networks, which can learn even random labels, work well (Zhang et al., 2017), unsupervised learning requires a careful control of the network capacity."}, {"heading": "6. Conclusion", "text": "The recent success in mapping between two domains in an unsupervised way and without any existing knowledge, other than network hyperparameters is nothing less than extraordinary and has far reaching consequences. As far as we know, nothing in the existing machine learning or cognitive science literature suggests that this would be possible. We provide the necessary machinery for understanding such phenomena by presenting a framework for measuring the complexity of compositions of functions and by providing a concrete definition of semantics. Using the new machinery, we explain how, simply by training networks that are not too complex, the semantic mapping stands out from all other alternative mappings. There are a few results that require further exploration. As one curious example, a surprising result is that all invertible functions can be divided into three classes, depending on the complexity of the inverse, which can be lower by 2, larger by 2, or the same. This can be an artifact of measuring complexity with Leaky ReLU networks, or something that has profound implications."}, {"heading": "Acknowledgements", "text": "This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant ERC CoG 725974). The authors would like to thank Sagie Benaim for helping with the experiments and Moustapha Cisse, L\u00e9on Bottou and Ofir Yakovian for insightful discussions."}, {"heading": "Appendix A. Summary of Notation", "text": "Tab. 1 lists the symbols used in our work."}, {"heading": "Appendix B. Empirical Validation of Prediction 1", "text": "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA \u2192 XB , based only on the GAN constraint on B, is presented as a failing baseline. In (Yi et al., 2017), among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful. However, this goes unnoticed. In order to validate the prediction that a purely GAN based solution is viable, we conducted a series of experiments using the DiscoGAN architecture. We consider image domains A and B, where XA = XB = R3\u00d764\u00d764. In DiscoGAN, the generator is build of: (i) an encoder consisting of convolutional layers with 4\u00d7 4 filters followed by Leaky ReLU activation units and (ii) a decoder consisting of deconvolutional layers with 4 \u00d7 4 filters followed by a ReLU activation units. Sigmoid is used for the output layer. Between 4 to 5 convolutional/deconvolutional layers are used, depending on the domains used in A and B (we match the published code architecture per dataset). The discriminator is similar to the encoder, but has an additional convolutional layer as the first layer and a sigmoid output unit. The first set of experiments considers the CelebA face dataset. Transformations are learned between the subset of images labeled as \u201cman\u201d and those labeled as \u201cwoman\u201d, as well as from blond to black hair and glasses to no eyewear. The results are shown in Fig. 3, 4, and 5, (resp.). It is evident that the output image is highly related to the input images. In the case of mapping handbag to shoes, as seen in Fig. 6, the GAN does not provide a meaningful solution. However, in the case of edges to shoes and vice versa (Fig. 7), the GAN solution is successful."}, {"heading": "Appendix C. Empirical Validation of Prediction 2", "text": "We predict that the selection of the right number of layers is crucial in unsupervised learning. Using fewer layers than needed will not support the modeling of the semantic transformation. In contrast, adding superfluous layers would mean that more and more alternative mappings obscure the semantic transformation. In (Kim et al., 2017), 8 layers are sometimes employed while at other times 10 layers are used (counting both convolution and deconvolution). In our experiment we vary the number of layers and inspect the influence on the results. These experiments were done on the CelebA gender conversion task, where eight layers are employed in the experiments of (Kim et al., 2017). Using the public implementation and adding and removing layers, we obtain the results in Fig. 8,9,10,11. Note that since the encoder and the decoder parts of the learned network are symmetrical, the number of layers is always even. As can be seen, changing the number of layers has a dramatic effect on the results and the best results are obtained at eight layers. The results degrade quickly as one deviates from the optimal value. Using fewer layers, the GAN fails to produce images of the desired class. Adding layers, the semantic alignment is lost, just as expected."}, {"heading": "Appendix D. Assumptions", "text": "Assumption 1 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 \u25e6D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1).\nAssumption 2 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. For all m > 0, the function\nRD[fVm,...,V1 , fWm,...,W1 ] (22)\nis continuous as a function of Vm, ..., V1,Wm, ...,W1."}, {"heading": "Appendix E. Lemmas", "text": "Lemma 1 Let D1 and D2 be two classes of functions and D1, D2 two distributions. Assume that D1 \u25e6 {p} \u2282 D2 then, discD1(p \u25e6D1, p \u25e6D2) \u2264 discD2(D1, D2) (23) In particular, if m \u2265 k + C(p) then,\ndisck(p \u25e6D1, p \u25e6D2) \u2264 discm(D1, D2) (24)\nProof By the definition of discrepancy:\ndiscD1(p \u25e6D1, p \u25e6D2) = sup c1,c2\u2208D1\n\u2223\u2223\u2223Rp\u25e6D1 [c1, c2]\u2212Rp\u25e6D2 [c1, c2] \u2223\u2223\u2223\n= sup c1,c2\u2208D1\n\u2223\u2223\u2223RD1 [c1 \u25e6 p, c2 \u25e6 p]\u2212RD2 [c1 \u25e6 p, c2 \u25e6 p] \u2223\u2223\u2223\n(25)\nSince D1 \u25e6 {p} \u2282 D2 we have:\ndiscD1(p \u25e6D1, p \u25e6D2) = sup c1,c2\u2208D1\n\u2223\u2223\u2223RD1 [c1 \u25e6 p, c2 \u25e6 p]\u2212RD2 [c1 \u25e6 p, c2 \u25e6 p] \u2223\u2223\u2223\n\u2264 sup u1,u2\u2208D2\n\u2223\u2223\u2223RD1 [u1, u2]\u2212RD2 [u1, u2] \u2223\u2223\u2223 = discD2(D1, D2)\n(26)\nThe second inequality is a special case for D1 = Dk and D2 = Dm.\nLemma 2 Let A = (X1, D1) and B = (X2, D2) be two domains and DZ a distribution.\n1. Assume that m \u2265 k + C(p). Then,\ndisck(p \u25e6D1, D3) \u2264 discm(D1, D2) + disck(p \u25e6D2, D3) (27)\n2. Let y1, y2 and y = y2 \u25e6 y\u221211 be three functions and m \u2265 k + C(y2). Then,\ndisck(y \u25e6D1, D2) \u2264 discm(DZ , y\u221211 \u25e6D1) + disck(y2 \u25e6DZ , D2) (28)\n3. Let h be any function and m \u2265 k + C(h\u22121). Then,\ndisck(D1, h\u22121 \u25e6D2) \u2264 discm(h \u25e6D1, D2) (29)"}, {"heading": "Proof", "text": "1. Follows from Lem. 1, since Dk \u25e6 {p} \u2282 Dm, we have:\ndisck(p \u25e6D1, p \u25e6D2) \u2264 discm(D1, D2) (30)\nTherefore, by the triangle inequality,\ndisck(p \u25e6D1, D3) \u2264 disck(p \u25e6D1, p \u25e6D2) + disck(p \u25e6D2, D3) \u2264 discm(D1, D2) + disck(p \u25e6D2, D3)\n(31)\n2. We use Lem. 1 with p :\u2190 y2, D1 :\u2190 Dk, and D2 :\u2190 Dm and Dk \u25e6 {y2} \u2282 D2:\ndisck(y2 \u25e6DZ , y \u25e6D1) = disck(y2 \u25e6DZ , y2 \u25e6 y\u221211 \u25e6D1) \u2264 discm(DZ , y\u221211 \u25e6D1) (32)\nTherefore, by the triangle inequality,\ndisck(y \u25e6D1, D2) \u2264 disck(y2 \u25e6DZ , D2) + disck(y2 \u25e6DZ , y \u25e6D1) \u2264 disck(y2 \u25e6DZ , D2) + discm(DZ , y\u221211 \u25e6D1)\n(33)\n3. Follows immediately from Lem. 1 for p :\u2190 h\u22121 and Dk \u25e6 {h\u22121} \u2282 Dm.\nLemma 3 Let N = SCM[C]. Assume that D1 \u223c m, 1 D2 and D2 \u223c m, 2 D3 then D1 \u223c m, 1+ 2 D3.\nProof We consider that D1 \u223c\nm, 1 D2 =\u21d2 discm(D1, D2) \u2264 1 (34)\nand, D2 \u223c\nm, 2 D3 =\u21d2 discm(D2, D3) \u2264 2 (35)\nTherefore, by the triangle inequality,\ndiscm(D1, D3) \u2264 discm(D1, D2) + discm(D2, D3) \u2264 1 + 2 (36)\nLemma 4 Let N = SCM[C]. In addition, let u, v be any two functions. Then,\nmax{C(u)\u2212 C(v\u22121), C(v)\u2212 C(u\u22121)} \u2264 C(u \u25e6 v) \u2264 C(u) + C(v) (37)\nProof We begin by proving the upper bound. We denote C(u) = n and C(v) = m. Let u = un+1:1 and v = vm+1:1 be minimal decompositions of u and v (resp.). Therefore, we can represent, u\u25e6v = un+1:1\u25e6vm+1:1. In particular, C(u \u25e6 v) \u2264 n+m = C(u) + C(v). The lower bound follows immediately from the upper bound:\nC(u) = C(u \u25e6 v \u25e6 v\u22121) \u2264 C(u \u25e6 v) + C(v\u22121) =\u21d2 C(u)\u2212 C(v\u22121) \u2264 C(u \u25e6 v) (38)\nBy similar considerations, C(v)\u2212 C(u\u22121) \u2264 C(u \u25e6 v).\nLemma 5 LetN = SCM[C]. In addition, let u1, u2, u3 \u2208 N be three functions such that: C(u1 \u25e6 u2 \u25e6 u3) = C(u1) + C(u2) + C(u3). Then, u1 7 u2 and u2 7 u3.\nProof By Lem. 4, C(u1 \u25e6 u2 \u25e6 u3) \u2264 C(u1 \u25e6 u2) + C(u3). In addition, by C(u1 \u25e6 u2 \u25e6 u3) = C(u1) + C(u2) + C(u3), we have: C(u1) + C(u2) \u2264 C(u1 \u25e6 u2). Again by Lem. 4, C(u1 \u25e6 u2) \u2264 C(u1) + C(u2) and conclude that C(u1 \u25e6 u2) = C(u1) + C(u2). The second equation follows by similar arguments.\nLemma 6 Invariant(N ) is closed under inverse and composition, i.e,\n\u03c0 \u2208 Invariant(N ) \u21d0\u21d2 \u03c0\u22121 \u2208 Invariant(N ) (39)\nAnd, \u03c01, \u03c02 \u2208 Invariant(N ) =\u21d2 \u03c01 \u00b7 \u03c02 \u2208 Invariant(N ) (40)\nProof Inverse: Let \u03c0 \u2208 Invariant(N ). Then, by definition, \u03c0 is an invertible linear mapping and \u03c0 \u25e6\u03c3 = \u03c3 \u25e6\u03c0. In particular, \u03c0\u22121 is also an invertible linear mapping and \u03c0\u22121 \u25e6 \u03c3 = \u03c3 \u25e6 \u03c0\u22121. Thus, \u03c0\u22121 \u2208 Invariant(N ). Composition: Let \u03c01, \u03c02 \u2208 Invariant(N ). Then, \u03c0i is an invertible linear mapping and \u03c0i \u25e6 \u03c3 = \u03c3 \u25e6 \u03c0i for i = 1, 2. In particular, \u03c01 \u25e6 \u03c02 is also an invertible linear mapping and \u03c01 \u25e6 \u03c02 \u25e6 \u03c3 = \u03c01 \u25e6 \u03c3 \u25e6 \u03c02 = \u03c3 \u25e6 \u03c01 \u25e6 \u03c02. Thus, \u03c01 \u25e6 \u03c02 \u2208 Invariant(N ).\nRecall the notation introduced in Eq. 11.\nLemma 7 Let N = SCM[\u03c3] obeying identifiability. If p = pn+1:1 = qn+1:1 are two minimal decompositions of p then:\n\u2200i \u2208 [n] : pi+1:1 \u25e6 q\u22121i+1:1 \u2208 Invariant(N ) and pn+1:i \u25e6 q\u22121n+1:i \u2208 Invariant(N ) (41)\nProof First, if i = n then pi+1:1 \u25e6 q\u22121i+1:1 = Id \u2208 Invariant(N ). Otherwise, by minimal identifiability,\nq1 = \u03c01 \u25e6 p1, \u2200i = 2, ..., n\u2212 1 : qi = \u03c0i \u25e6 pi \u25e6 \u03c0\u22121i\u22121 and qn = pn \u25e6 \u03c0\u22121n\u22121 (42)\nIn addition, pi+1:1 = pi \u25e6 pi\u22121 \u25e6 ... \u25e6 p1 qi+1:1 = (\u03c0i \u25e6 pi \u25e6 \u03c0\u22121i\u22121) \u25e6 (\u03c0i\u22121 \u25e6 pi \u25e6 \u03c0\u22121i\u22122) \u25e6 ... \u25e6 (\u03c01 \u25e6 p1)\n(43)\nTherefore, qi+1:1 = \u03c0i \u25e6 pi+1:1 and pi+1:1 \u25e6 q\u22121i+1:1 = \u03c0\u22121i \u2208 Invariant(N ). By similar considerations, pn+1:i \u25e6 q\u22121n+1:i \u2208 Invariant(N ).\nLemma 8 Let N = SCM[\u03c3] obeying identifiability and \u03c3 is a non-linear element-wise activation function. Let p1, p2 and p3 be three functions such that C(pi) = 1 for all i = 1, 2, 3. Then, p2 \u25e6 p1 6= p3.\nProof There are invertible linear mappings, Wi, such that pi = \u03c3 \u25e6Wi for all i = 1, 2, 3. Therefore, if:\np2 \u25e6 p1 = p3 (44)\nthen, (\u03c3 \u25e6W2) \u25e6 (\u03c3 \u25e6W1) = (\u03c3 \u25e6W3) (45)\nwhich is equivalent to: \u03c3 = W\u221212 \u25e6W3 \u25e6W\u221211 (46)\nin contradiction to the assumption that \u03c3 is a non-linear function.\nLemma 9 Let N = SCM[\u03c3], f 6= Id is any function such that C(f) > 0 and \u03c0 \u2208 Invariant(N ). Then, C(\u03c0 \u25e6 f) \u2264 C(f).\nProof Let f = fn+1:1 be a minimal decomposition of f and fi = \u03c3 \u25e6Wi for i \u2208 [n] and Wi are invertible linear mappings. Since \u03c0 \u25e6 \u03c3 = \u03c3 \u25e6 \u03c0, we have:\n\u03c0 \u25e6 f = (\u03c3 \u25e6 (\u03c0 \u25e6Wn)) \u25e6 (\u03c3 \u25e6Wn\u22121) \u25e6 ... \u25e6 (\u03c3 \u25e6W1) (47)\nThis is a decomposition of length n. Therefore, C(\u03c0 \u25e6 f) \u2264 n = C(f).\nLemma 10 Let N = SCM[\u03c3], f 6= Id is any function and W is an invertible linear mapping. Then, C(f \u25e6W ) \u2264 C(f).\nProof Let f = fn+1:1 be a minimal decomposition of f and fi = \u03c3 \u25e6Wi for i \u2208 [n] and Wi are invertible linear mappings. We have:\nf \u25e6W = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W2) \u25e6 (\u03c3 \u25e6W1 \u00b7W ) (48)\nThis is a decomposition of length n. Therefore, C(f \u25e6W ) \u2264 n = C(f)."}, {"heading": "Appendix F. Proofs for the Thms. 1 and 4", "text": ""}, {"heading": "F.1 Properties of inverses", "text": "Lemma 11 Let N = SCM[\u03c3], where \u03c3 is the Leaky ReLU activation function, with parameter a > 0. Then, I(N ) \u2264 3.\nProof We show that the inverse of every function u of complexity 1 can be represented as a decomposition u1 \u25e6 u2 \u25e6 u3 for u1, u2, u3 \u2208 C. It follows from the following simple identities,\n\u03c3\u22121 = \u2212Id \u25e6 \u03c3 \u25e6 \u2212Id/a (49)\nTherefore, W\u22121 \u25e6 \u03c3\u22121 = \u2212W\u22121 \u25e6 \u03c3 \u25e6 \u2212Id/a (50)\nAnd, Id = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (51)\nWe obtain, W\u22121 \u25e6 \u03c3\u22121 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u22121/a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (52)\nTherefore, C(u\u22121) \u2264 3. Finally, let p \u2208 N be a function of complexity n \u2265 1 with minimal decomposition p = pn+1:1.\nTheorem 1 Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is the Leaky ReLU with parameter a > 0. Then, for any u \u2208 N , |C(u\u22121)\u2212 C(u)| is either 0 or 2."}, {"heading": "Proof", "text": "Part 1: In this part, we prove by induction on C(u) = n that:\nu\u22121 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (53)\nWhere u = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1) is a minimal decomposition of u. Case C(u) = 0: Then u = Id, u\u22121 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6 \u2212Id/a) = Id and C(u\u22121) = 0. Case C(u) = 1: Follows immediately from Lem. 11 and Eq. 52. Induction hypothesis: Assume that:\nu\u22121 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (54)\nWhere u = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1) is a minimal decomposition of u. Case C(u) = n+ 1: Let u = un+2:1 = (\u03c3 \u25e6Wn+1) \u25e6 ... \u25e6 (\u03c3 \u25e6W1) be a minimal decomposition of u. We denote v = un+1:1. By the induction hypothesis,\nv\u22121 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (55)\nBy Eq. 50 we can represent: u\u22121n+1 = \u2212W\u22121n+1 \u25e6 \u03c3 \u25e6 \u2212Id/a. We consider that:\nu\u22121 = v\u22121 \u25e6 u\u22121n+1 = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) \u25e6 (W\u22121n+1 \u25e6 \u03c3 \u25e6 \u2212Id/a) = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6W\u22121n+1/a) \u25e6 (\u03c3 \u25e6 \u2212Id/a)\n(56) In particular, C(u\u22121) \u2264 n+ 3 = C(u) + 2 and C(u) = C((u\u22121)\u22121) \u2264 C(u\u22121) + 2. Therefore,\n|C(u\u22121)\u2212 C(u)| \u2264 2 (57)\nPart 2: In this part, we show that |C(u\u22121)\u2212 C(u)| 6= 1 (58)\nAssume by contradiction thatC(u\u22121) = C(u)+1. Therefore, there is a minimal decomposition u\u22121 = pn+2:1. By Part 1, there is a decomposition u\u22121 = qn+3:1. By identifiability,\n\u2203j1 < ... < jn+2 = n+ 3, \u03c01, ..., \u03c0n \u2208 Invariant(N ) : qj2:j1 = \u03c01 \u25e6 p1, \u2200i = 2, ..., n : qji+1:ji = \u03c0i \u25e6 pi \u25e6 \u03c0\u22121i\u22121 and qjn+2:jn+1 = pn+1 \u25e6 \u03c0\u22121n\n(59)\nSince jn+2 = n+3 and ji < ji+1, there is k \u2208 [n+1] such that jk+1\u2212 jk = 2 and for every i \u2208 [n+1]\\{k} we have ji+1 \u2212 ji = 1. In particular, C(qjk+2:jk) = 1 in contradiction to Lem. 8. We conclude that C(u\u22121) 6= C(u) + 1 and C(u) = C((u\u22121)\u22121) 6= C(u\u22121) + 1.\nLemma 12 Let N = SCM[\u03c3] where \u03c3 is the Leaky ReLU activation function, with parameter a > 0. We define a function from one decomposition to another as follows:\nF+(un+1:1) = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (60)\nwhere ui = \u03c3 \u25e6Wi for i \u2208 [n]. Then, for all j \u2208 [n+ 1] we have:\n[F+(un+1:1)]n\u2212j+3:1 \u25e6 u \u25e6D = (\u2212\u03c3\u22121) \u25e6 uj:1 \u25e6D (61)\nProof In the proof of Thm. 1, we built recursively the composition:\nF+(un+1:1) = (\u03c3 \u25e6 \u2212Id) \u25e6 (\u03c3 \u25e6W\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6W\u22121n /a) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (62)\nto invert un+1:1, i.e, F+(un+1:1) = u\u22121n+1:1. Similarly, for all j \u2208 [n+ 1]:\n(\u03c3\u25e6\u2212Id)\u25e6 [F+(un+1:1)]n\u2212j+3:1 = (\u03c3\u25e6\u2212Id)\u25e6(\u03c3\u25e6W\u22121j /a)\u25e6 ...\u25e6(\u03c3\u25e6W\u22121n /a)\u25e6(\u03c3\u25e6\u2212Id/a) = u\u22121n+1:j (63)\nIn particular, for all j \u2208 [n+ 1]:\n[F+(un+1:1)]n\u2212j+3:1 = (\u2212\u03c3\u22121) \u25e6 u\u22121n+1:j (64)\nTherefore, for all j \u2208 [n+ 1]:\n[F+(un+1:1)]n\u2212j+3:1 \u25e6 u \u25e6D = (\u2212\u03c3\u22121) \u25e6 u\u22121n+1:j \u25e6 u \u25e6D = (\u2212\u03c3\u22121) \u25e6 uj:1 \u25e6D (65)"}, {"heading": "F.2 Properties of compositions", "text": "Lemma 13 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Then, there are functions f, g, h \u2208 N such that C(f \u25e6 g \u25e6 h) < C(f) + C(g) + C(h), f 7 g and g 7 h.\nProof Let W 6= Id be any invertible M \u00d7M real matrix. Consider the following functions:\nf = (\u03c3 \u25e6W ), g = (\u03c3 \u25e6 \u2212Id) and h = (\u03c3 \u25e6 \u2212W\u22121/a) (66)\nSince both f and g are functions of complexity 1, the complexity of f \u25e6 g is at most 2. The complexity of f \u25e6 g is not 0, since f \u25e6 g 6= Id. In addition, by Lem. 8, C(f \u25e6 g) 6= 1. Similarly, C(g \u25e6 h) = 2. On the other hand, f \u25e6 g \u25e6 h = \u03c3 and C(\u03c3) = 1.\nLemma 14 Let N = SCM[\u03c3] be a NN-SCM obeying identifiability. Let u \u2208 N be a function such that C(u) = n > 0 and a \u2208 N be a function of complexity 1. Then, there is 0 \u2264 r \u2264 n such that:\nC(ur+1:1 \u25e6 a) = 1 and C(u) = C(u \u25e6 a) + r \u2212 1 (67)\nProof If u 7 a then we take r = 0. Otherwise, u . a. Let u = un+1:1 be a minimal decomposition of u and let u \u25e6 a = vd+1:1 be a minimal decomposition of u \u25e6 a. Therefore,\nu \u25e6 a = un+1:1 \u25e6 a = vd+1:1 (68)\nBy identifiability, there is 0 \u2264 r \u2264 n such that:\nur+1:1 \u25e6 a = \u03c01 \u25e6 v1 (69)\nFor some \u03c01 \u2208 Invariant(N ). In addition,\n\u2203j2 = r + 1 < ... < jd+1 = n+ 1 and \u03c02, ..., \u03c0d\u22121 \u2208 Invariant(N ) : \u2200i = 2, ..., d\u2212 1 : uji+1:ji = \u03c0i \u25e6 vi \u25e6 \u03c0\u22121i\u22121 ujd+1:jd = vn \u25e6 \u03c0\u22121d\u22121\n(70)\nTherefore, \u2200i = 2, ..., d : 1 = C(uji+1:ji) = ji+1 \u2212 ji (71)\nIn particular, \u2200i = 2, ..., d : ji = r + i\u2212 1 and ui = \u03c0i \u25e6 vi \u25e6 \u03c0\u22121i\u22121 (72)\nWe also have, jd+1 = n+ 1. Therefore,\nC(u) = n = jd+1 \u2212 1 = d+ r \u2212 1 = C(u \u25e6 a) + r \u2212 1 (73)\nLemma 15 Let N = SCM[\u03c3] be a NN-SCM obeying identifiability. Let u \u2208 N be a function such that C(u) = n > 0 and a \u2208 N be a function of complexity 1. If u . a, u = b \u25e6 d such that b 7 d and C(a\u22121) = C(d), then, we have d . a or d \u25e6 a \u2208 Invariant(N ).\nProof Let b = bn+1:1 and d = dm+1:1 be minimal decompositions of b and d (resp.). We consider that u = b \u25e6 d = bn+1:1 \u25e6 dm+1:1 is a minimal decomposition of u. We denote u = un+m+1:1 where ui = bi\u2212m for m+ 1 \u2264 i \u2264 n+m and ui = di for i \u2264 m. By Lem. 14 we have:\n\u22030 \u2264 r \u2264 n+m : C(ur+1:1 \u25e6 a) = 1 and C(u) = C(u \u25e6 a) + r \u2212 1 (74)\nIf r \u2264 m: then we have C(d \u25e6 a) = C(um+1:r+1 \u25e6 ur+1:1 \u25e6 a) \u2264 m\u2212 r + C(ur+1:1 \u25e6 a) = m\u2212 r + 1. If r 6= 0, then d . a as desired. On the other hand, if r = 0 then C(u) = C(u \u25e6a) + 1 and u 7 a in contradiction. If r \u2265 m: then there is a function v such that C(v) = 1 and:\nv = br\u2212m+1:1 \u25e6 d \u25e6 a (75)\nAlternatively, v \u25e6 a\u22121 = br\u2212m+1:1 \u25e6 d (76)\nSince b 7 d, by Lem. 5,\nC(br\u2212m+1:1 \u25e6 d) = C(br\u2212m+1:1) + C(d) = C(d) + r \u2212m (77)\nIf r \u2212m > 1: then C(d) + r \u2212m = C(v \u25e6 a\u22121) \u2264 C(a\u22121) + 1 = C(d) + 1 in contradiction to r > 1. If r \u2212 m = 0: then v = d \u25e6 a. Therefore, d . a and C(d \u25e6 a) = 1 (since C(a) = 1, 1 \u2264 C(d) and C(d \u25e6 a) = 1 < C(a) + C(d)). If r \u2212m = 1: then, we have,\nC(v \u25e6 a\u22121) = C(b1 \u25e6 d) = C(b1) + C(d) = 1 + C(d) = 1 + C(a\u22121) = C(v) + C(a\u22121) (78)\nAlternatively, v 7 a\u22121. By Lem. 7, for p = v \u25e6 a\u22121 = b1 \u25e6 d we have: d \u25e6 a \u2208 Invariant(N ). We conclude that d . a or d \u25e6 a \u2208 Invariant(N ).\nLemma 16 (Composition reduction) Let N = SCM[\u03c3] be a NN-SCM obeying identifiability. Let u, v \u2208 N . Then, we can represent u = a \u25e6 b and v = b\u22121 \u25e6 c such that: a 7 c, b\u22121 7 c and\nC(a) + C(b)\u2212 I \u2212 1 \u2264 C(a \u25e6 b) . (79)\nProof Let u = un+1:1 and v = vm+1:1 be minimal decompositions of u and v (resp.). Let u \u25e6 v = zd+1:1 be a minimal decomposition of u \u25e6 v. By identifiability we have:\n\u2203j1 = 1 < ... < jd+1 = n+m+ 1,\u2203\u03c01, ..., \u03c0d\u22121 \u2208 Invariant(N ) [un \u25e6 ... \u25e6 u1 \u25e6 vm \u25e6 ... \u25e6 v1]j2:j1 = \u03c01 \u25e6 z1 \u2200i = 2, ..., d\u2212 1 : [un \u25e6 ... \u25e6 u1 \u25e6 vm \u25e6 ... \u25e6 v1]ji+1:ji = \u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121 [un \u25e6 ... \u25e6 u1 \u25e6 vm \u25e6 ... \u25e6 v1]jd+1:jd = zd \u25e6 \u03c0\u22121d\u22121\n(80)\nHere, [fk \u25e6 ... \u25e6 f1]i:j = fi\u22121 \u25e6 ... \u25e6 fj for 0 \u2264 j < i and ui:i = Id. We consider two options. The first: there is no index k such that jk \u2264 m and m + 1 < jk+1. The second: there is such k.\nCase 1: In this case, for every index i = 2, ..., d\u2212 1:\nvji+1:ji = \u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121 or uji+1\u2212m:ji\u2212m = \u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121 (81)\nAnd for i = 1 and i = d: vj2:j1 = \u03c01 \u25e6 z1 and vjd+1:jd = zd \u25e6 \u03c0\u22121d\u22121 (82)\nIf the first equation holds,\n1 = C(\u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121) = C(vji+1:ji) = ji+1 \u2212 ji (83)\nTherefore, for any i = 1, ..., d: ji+1 = ji + 1. We conclude that:\nC(u \u25e6 v) = d = n+m = C(u) + C(v) and u 7 v (84)\nFinally, we choose a = u, b = Id and c = v. This gives a 7 b, b\u22121 7 c and a 7 c.\nCase 2: Let 1 \u2264 k \u2264 d be the index for which jk \u2264 m and m+ 1 < jk+1. With no loss of generality, we assume that k 6= 1, d. For any index i 6= k such that 1 < i < d, we have,\nvji+1:ji = \u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121 or uji+1\u2212m:ji\u2212m = \u03c0i \u25e6 zi \u25e6 \u03c0\u22121i\u22121 (85)\nAnd if i = 1 or i = d: vj2:j1 = \u03c01 \u25e6 z1 or vjd+1:jd = zd \u25e6 \u03c0\u22121d\u22121 (86)\nAs in Case 1, we conclude that ji+1 = ji + 1. Therefore, we have jk = k and we denote jk+1 = m+ r + 1, i.e,\n[un \u25e6 ... \u25e6 u1 \u25e6 vm \u25e6 ... \u25e6 v1]jk+1:jk = ur+1:1 \u25e6 vm+1:k = \u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121 (87)\nFinally, we choose: b = v\u22121m+1:k, a = u \u25e6 b\u22121 and c = vk:1 (88) It follows immediately that v = b\u22121 \u25e6 c such that b\u22121 7 c. In addition, we have, u = a \u25e6 b and u \u25e6 v = a \u25e6 c. We consider that:\nC(a \u25e6 c) = C(zd+1:1) = C(zd+1:k \u25e6 zk:1) = C(zd+1:k) + C(zk:1) (89)\nBy Lem. 5 we have C(c) = C(vk:1) = k \u2212 1 = C(zk;1). In addition,\na = u \u25e6 b\u22121 = un+1:r+1 \u25e6 ur+1:1 \u25e6 vm+1:k (90)\nBy identifiability, ur+1:1 \u25e6 vm+1:k = \u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121 and un+1:r+1 \u25e6 \u03c0k = zd+1:k+1. Therefore,\na = u \u25e6 b\u22121 = zd+1:k \u25e6 \u03c0\u22121k\u22121 (91)\nThus, by Lem. 10, C(a) \u2264 C(zd+1:k) since zd+1:k 6= Id because k \u2264 d. In particular,\nC(a \u25e6 c) = C(zd+1:k) + C(zk:1) \u2265 C(a) + C(c) (92)\nFinally, by Lem. 4, we have: C(a \u25e6 c) = C(a) + C(c) or a 7 c. Since ur+1:1 = \u03c0\u22121k \u25e6 zk \u25e6 \u03c0k\u22121 \u25e6 b we have:\nC(u) = C(un+1:r+1 \u25e6 \u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121 \u25e6 b) = C(un+1:r+1) + C(\u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121 \u25e6 b) = C(a)\u2212 1 + C(\u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121 \u25e6 b) \u2265 C(a)\u2212 1 + C(b)\u2212 C((\u03c0k \u25e6 zk \u25e6 \u03c0\u22121k\u22121)\u22121) \u2265 C(a)\u2212 1 + C(b)\u2212 C(\u03c0k\u22121 \u25e6 z\u22121k \u25e6 \u03c0\u22121k ) \u2265 C(a)\u2212 1 + C(b)\u2212 I\n(93)\nTheorem 4 LetN = SCM[\u03c3] be a NN-SCM obeying identifiability. Then, for all f, g, h \u2208 N such that f 7 g, g 7 h and I \u2264 C(g), we have:\nC(f \u25e6 g \u25e6 h) = C(f) + C(g) + C(h) (13)\nProof Denote C(f) = i, C(g) = n, C(h) = m. We prove the theorem by induction on C(f) = i. Case i = 0: In this case f = Id and C(f \u25e6 g \u25e6 h) = C(g \u25e6 h) = C(g) + C(h) as desired. Case i = 1: Let\ng = gn+1:1 and h = hm+1:1 (94)\nbe minimal decompositions of g and h (resp.). In addition, we denote: b = gn+1:n\u2212q+1 where q = C(f\u22121) and c = gn\u2212q+1:1 \u25e6 hm+1:1. Assume by contradiction that f . g \u25e6 h. Then, we apply Lem. 15 and conclude that f . b or f \u25e6 b \u2208 Invariant(N ). In addition, since C(b) = C(f\u22121) \u2264 I \u2264 C(g), we decompose c as follows: c = d1 \u25e6 d2, d1 = gn\u2212q+1:1 and d2 = hm+1:1. By Lem. 5, b 7 d1 and d1 7 d2. In addition,\nC(f \u25e6 b \u25e6 d1) = C(f \u25e6 g) = C(f) + C(g) = C(f) + C(b \u25e6 d1) = C(f) + C(b) + C(d1) (95)\nTherefore, by Lem. 5, f 7 b and C(f \u25e6 b \u25e6 d1) > C(d1). This immediately eliminates the option f . b. On the other hand, if f \u25e6 b \u2208 Invariant(N ) then by Lem. 9,\nC(f \u25e6 b \u25e6 d1) \u2264 C(d1) (96)\nin contradiction. Therefore, f 7 g \u25e6 h and\nC(f \u25e6 g \u25e6 h) = C(f) + C(g \u25e6 h) = C(f) + C(g) + C(h) (97)\nInduction hypothesis: Assume that the statement holds for all f such that C(f) = i \u2265 1, g and h that satisfy, i.e., I \u2264 C(g), f 7 g and g 7 h =\u21d2 C(f \u25e6 g \u25e6 h) = C(f) + C(g) + C(h) (98) Induction step: Let f be a function such that C(f) = i + 1 and f 7 g. We denote f = f \u2032 \u25e6 f1 such that f \u2032 7 f1, C(f1) = 1 and C(f \u2032) = i. Then, by Lem. 5, f1 7 g. Therefore, by case i = 1 we have: C(f1 \u25e6 g \u25e6 h) = C(f1) + C(g) + C(h) = C(f1 \u25e6 g) + C(h). Alternatively, f1 \u25e6 g 7 h. In addition, since f 7 g then C(f \u2032 \u25e6 (f1 \u25e6 g)) = C(f) + C(g) = C(f \u2032) + 1 + C(g) = C(f \u2032) + C(f1 \u25e6 g). Alternatively, f \u2032 7 f1 \u25e6 g. Therefore, because I \u2264 C(g) < C(f1 \u25e6 g) by the induction hypothesis we have:\nI \u2264 C(f1 \u25e6 g), f \u2032 7 f1 \u25e6 g and f1 \u25e6 g 7 h =\u21d2 C(f \u25e6 g \u25e6 h) = C(f \u2032 \u25e6 (f1 \u25e6 g) \u25e6 h) = C(f \u2032) + C(f1 \u25e6 g) + C(h)\n= C(f \u2032) + 1 + C(g) + C(h) = C(f) + C(g) + C(h)\n(99)"}, {"heading": "Appendix G. Proofs for Thms. 2 and 3", "text": "Lemma 17 LetN = SCM[C]. LetA = (XA, DA) andB = (XB , DB) be two domains andDZ a distribution. Then, if m \u2265 k + Cm, 1Z,B we have: Ck, 0+ 1A,B \u2264 Cm, 0A,Z + Cm, 1Z,B (100) In particular,\nC 0+ 1A,B \u2264 C 0A,Z + C 1Z,B (101)\nProof Let f \u2208 H 0(DA, DZ ;m) and g \u2208 H 1(DZ , DB ;m). Then, by the first item of Lem. 2, for D1 :\u2190 f \u25e6DA, D2 :\u2190 DZ , D3 :\u2190 DB , p :\u2190 g and m \u2265 k + Cm, 1Z,B , we have:\ndisck(g \u25e6 f \u25e6DA, DB) \u2264 discm(f \u25e6DA, DZ) + discm(g \u25e6DZ , DB) \u2264 0 + 1 (102)\nTherefore, Ck, 0+ 1A,B \u2264 C(f) + C(g) = Cm, 0A,Z + Cm, 1Z,B (103) In order to prove the second inequality, we choose k \u2265 max { E 0+ 1A,B , E 1 Z,B , E 0 A,Z } and obtain:\nC 0+ 1A,B = C k, 0+ 1 A,B \u2264 Cm, 0A,Z + Cm, 1Z,B = C 0A,Z + C 1Z,B (104)\nLemma 18 Let N = SCM[C] and A = (XA, DA) and B = (XB , DB) are two domains. If m \u2265 k \u2265 E 0A,B then H 0(DA, DB ;m) \u2282 H 0(DA, DB ; k).\nProof Let y \u2208 H 0(DA, DB ;m). Since m \u2265 k \u2265 E 0A,B , we have: C(y) \u2264 Cm, 0A,B \u2264 C 0A,B = Ck, 0A,B . In addition, sinceDk \u2282 Dm: disck(y\u25e6DA, DB) \u2264 discm(y\u25e6DA, DB) \u2264 0. In particular, y \u2208 H 0(DA, DB ; k).\nLemma 19 LetN = SCM[C]. LetA = (XA, DA) andB = (XB , DB) be two domains andDZ a distribution. If m \u2265 max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } , then, DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B iff: C 0+ 1A,B = C 1 Z,B + C 0 A,Z (105)"}, {"heading": "Proof", "text": "Part 1: Assume that DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Then, there is a function yAB \u2208 Z(DA, DZ , DB ;m, 0, 1)\u2229H 0+ 1(DA, DB ;m). In addition, there is a decomposition yAB = yB \u25e6 y\u22121A such that yB \u2208 H 1(DZ , DB ;m) and y\u22121A \u2208 H 0(DA, DZ ;m) and yB 7 y\u22121A . Therefore,\nC 0+ 1A,B = C m, 0+ 1 A,B = C(yAB) = C(yB) + C(y \u22121 A ) = C m, 0 A,Z + C m, 1 Z,B = C 0 A,Z + C 1 Z,B (106)\nPart 2: Assume that C 0+ 1A,B = C 1 Z,B + C 0 A,Z . Let t = m + C 1 Z,B , yB \u2208 H 1(DZ , DB ; t) and y\u22121A \u2208 H 0(DA, DZ ; t) and denote yAB = yB \u25e6 y\u22121A . By the second item of Lem. 2, for D1 :\u2190 DA, D2 :\u2190 DB , DZ :\u2190 DZ , p :\u2190 yB and t \u2265 m+ C(yB) we have:\ndiscm(yAB \u25e6DA, DB) \u2264 disct(y\u22121A \u25e6DA, DZ) + disct(yB \u25e6DZ , DB) \u2264 0 + 1 (107)\nTherefore, C 0+ 1A,B \u2264 C(yAB) \u2264 C(yB) + C(y\u22121A ) = C 1Z,B + C 0A,Z = C 0+ 1A,B (108)\nIn particular, yAB \u2208 Z(DA, DZ , DB ;m, 0, 1) \u2229H 0+ 1(DA, DB ;m) and yB 7 y\u22121A . Alternatively, DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution.\nNote: we assumed that H 1(DZ , DB ; t) 6= \u2205 and H 0(DA, DZ ; t) 6= \u2205. It follows from the assumption that for every two distributions of interest in the paper, DI and DJ , and an error rate > 0, there is a function h of finite complexity such that disc\u221e(h \u25e6DI , DJ) \u2264 ."}, {"heading": "G.1 Topological properties of decompositions", "text": "Let un+1:1 be a decomposition such that ui = \u03c3 \u25e6Wi for i \u2208 [n]. We denote:\nS (W ) = { W\u0304 \u2223\u2223 ||W \u2212 W\u0304 ||2 < } (109)\nand, S (W1, ...,Wn) = S (W1)\u00d7 ...\u00d7 S (Wn) (110)\nDefinition 1 (Perturbation of a function) Let N = SCM[\u03c3] be a NN-SCM and a decomposition un+1:1. The set of -perturbations of un+1:1 is:\nS (un+1:1) = {u\u0304n+1:1 \u2223\u2223 u\u0304i = \u03c3 \u25e6 W\u0304i for i \u2208 [n] and (W\u03041, ..., W\u0304n) \u2208 S (W1, ...,Wn)} (111)\nLemma 20 Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0. Let un+1:1 be a decomposition such that ui = \u03c3 \u25e6Wi for i \u2208 [n]. Therefore, we have:\n\u2200x : ||un+1:1(x)||2 \u2264 max{a, 1}n \u25e6 n\u220f\ni=1\n||Wi||2 \u00b7 ||x||2 (112)\nand,\n\u2200x : ||\u03c3\u22121 \u25e6 un+1:1(x)||2 \u2264 max{a, 1}n\u22121 \u25e6 n\u220f\ni=1\n||Wi||2 \u00b7 ||x||2 (113)\nProof We prove the second inequality by induction on n \u2265 1. The first inequality follows immediately from the second and ||\u03c3(x)||2 \u2264 ||a \u00b7 x||2 \u2264 a \u00b7 ||x||2. Case n = 1: We have:\n||\u03c3\u22121 \u25e6 un+1:1(x)||2 = ||W1(x)||2 \u2264 ||W1||2 \u00b7 ||x||2 (114)\nInduction hypothesis: We assume that:\n\u2200x : ||\u03c3\u22121 \u25e6 un+1:1(x)||2 \u2264 max{a, 1}n\u22121 \u25e6 n\u220f\ni=1\n||Wi||2 \u00b7 ||x||2 (115)\nAs mentioned above, an immediate consequence is the following inequality,\n\u2200x : ||un+1:1(x)||2 \u2264 max{a, 1}n \u25e6 n\u220f\ni=1\n||Wi||2 \u00b7 ||x||2 (116)\nCase n+ 1: We have:\n||\u03c3\u22121 \u25e6 un+2:1(x)||2 = ||Wn \u25e6 un+1:1(x)||2 \u2264 ||Wn+1 \u25e6 un+1:1(x)||2 \u2264 ||Wn+1||2 \u00b7 ||un+1:1(x)||2\n(117)\nAnd by the induction hypothesis we have:\n||\u03c3\u22121 \u25e6 un+2:1(x)||2 \u2264 ||Wn+1||2 \u00b7max{a, 1}n \u25e6 n\u220f\ni=1\n||Wi||2 \u00b7 ||x||2\n= max{a, 1}n \u25e6 n+1\u220f\ni=1\n||Wi||2 \u00b7 ||x||2 (118)\nLemma 21 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = n. Then, there is > 0 such that if u\u0304n+1:1 \u2208 S (un+1:1) then: C(u\u0304n+1:1) = n.\nProof We denote un+1:1 = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1). Assume by contradiction that for all > 0, there is u\u0304n+1:1 = (\u03c3 \u25e6 W\u0304n) \u25e6 ... \u25e6 (\u03c3 \u25e6 W\u03041) such that\nC(u\u0304n+1:1) < n and \u2200i \u2208 [n] : ||Wi \u2212 W\u0304i||2 < (119)\nThen, there is a sequence {ukn+1:1}\u221ek=1 such that \u2200i \u2208 [n] : ||Wi \u2212W ki ||2 < /k we have C(ukn+1:1) < n. Here, ukn+1:1 = u k n \u25e6 ... \u25e6 uk1 = (\u03c3 \u25e6W kn ) \u25e6 ... \u25e6 (\u03c3 \u25e6W k1 ). By identifiability, for every k, there are indexes pk, qk \u2264 n such that pk + 2 \u2264 qk and C(ukqk+1:pk) = 1 (120)\nSince the sequence {ukn+1:1}\u221ei=1 is infinitely long, by the pigeonhole principle, there is a tuple (p, q) such that p + 2 \u2264 q and there are infinitely many indexes k that satisfy C(ukq+1:p) = 1. Therefore, with no loss of generality, we can assume that for all k we have C(ukq+1:p) = 1 for a fixed tuple of indexes p, q \u2264 n that satisfy p+ 2 \u2264 q (or replace the original sequence with such a sequence). We denote Uk an invertible linear mapping such that:\nukq+1:p = (\u03c3 \u25e6 Uk) (121)\nIn particular, \u03c3\u22121 \u25e6 ukq+1:p = Uk (122)\nTherefore, \u2200x : ||\u03c3\u22121 \u25e6 ukq+1:p(x)||2 = ||Uk(x)||2 (123)\nBy Lem. 20,\n||\u03c3\u22121 \u25e6 ukq+1:p(x)||2 \u2264 max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n||uki || \u00b7 ||x||2 (124)\nTherefore,\n||Uk||2 \u2264 max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n(||Wi||+ /k) (125)\nAnd also, if the input dimension is M and || \u00b7 ||F is the Frobenius norm, then,\n||Uk||F \u2264M \u00b7max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n(||Wi||+ /k) (126)\nIn particular, the sequence Uk is bounded. Thus, by the Bolzano-Weierstrass theorem, there is a subsequence Ukt that converges to a matrix U (w.r.t the `2 norm). With no loss of generality, we can replace Uk with the sequence Ukt . Alternatively, we can assume that Uk converges to the matrix U (w.r.t the `2 norm). In addition, we have W ki \u2192Wi (w.r.t the `2 norm). Therefore, since for any x, fEn,...,E1(x) (see Tab. 1) is continuous as a function of a the matrices En, ..., E1, we have:\n\u2200x : lim k\u2192\u221e ukq+1:p(x) = uq+1:p(x) (127)\nOn the other hand, since for any x, the function fE(x) is continuous as a function of the matrix E, we have:\nlim k\u2192\u221e ukq+1:p(x) = lim k\u2192\u221e (\u03c3 \u25e6 Uk)(x) = (\u03c3 \u25e6 U)(x) (128)\nFinally, \u2200x : uq+1:p(x) = (\u03c3 \u25e6 U)(x) (129)\nAlternatively, uq+1:p = (\u03c3 \u25e6 U) (130)\nTherefore, C(uq+1:p) \u2264 1. On the other hand, since C(un+1:1) = n, by Lem. 5 we have C(uq+1:p) = q \u2212 p > 1 in contradiction.\nLemma 22 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = 1. Then, for every > 0 there is a u\u0304n\u22121 \u2208 S (un\u22121) such that C(un \u25e6 u\u0304n\u22121 \u25e6 un\u22122:1) > 1.\nProof We assume that C(un+1:1) = 1 and show that for every > 0 there is u\u0304n\u22121 \u2208 S (un\u22121) such that C(un \u25e6 u\u0304n\u22121 \u25e6 un\u22122:1) > 1. Assume by contradiction that there is > 0 such that for all u\u0304n\u22121 \u2208 S (un\u22121) we have C(un \u25e6 u\u0304n\u22121 \u25e6 un\u22122:1) = 1. By Lem. 8, there is no possibility that n = 2. Therefore, n \u2265 3. We denote ui = \u03c3 \u25e6Wi for i \u2208 [n]. Let\n\u2200k = 1, 2 : u\u0304kn+1:1 = un \u25e6 ukn\u22121 \u25e6 un\u22122:1 (131)\nwhere ukn\u22121 = un\u22121 \u25e6 Vk such that:\n\u2200k = 1, 2 : ||Wn\u22121 \u00b7 Vk \u2212Wn\u22121||2 \u2264 ||Wn\u22121||2 \u00b7 ||Vk \u2212 Id||2 < (132)\nand \u03c3\u22121 \u25e6 (Wn\u22121 \u25e6 V1 \u00b7 V \u221212 \u25e6W\u22121n\u22121) \u25e6 \u03c3 is not linear (133)\nThus, \u2200k = 1, 2 : u\u0304kn\u22121 \u2208 S (un\u22121). Therefore,\nun \u25e6 (un\u22121 \u25e6 Vk) \u25e6 un\u22122:1 = \u03c3 \u25e6 Uk (134)\nfor some linear mappings Uk for k = 1, 2. In particular,\n\u03c3 \u25e6Wn \u25e6 \u03c3 \u25e6Wn\u22121 \u00b7 V1 \u00b7 V \u221212 \u00b7W\u22121n\u22121 \u25e6 \u03c3\u22121 \u25e6W\u22121n \u25e6 \u03c3\u22121\n=un \u25e6 u\u22121n\u22121 \u25e6 V1 \u00b7 V \u221212 \u25e6 u\u22121n\u22121 \u25e6 u\u22121n =(un \u25e6 (un\u22121 \u25e6 V1) \u25e6 un\u22122:1) \u25e6 (un \u25e6 (un\u22121 \u25e6 V2) \u25e6 un\u22122:1)\u22121 =(\u03c3 \u25e6 U1) \u25e6 (\u03c3 \u25e6 U2)\u22121 = \u03c3 \u25e6 U1 \u00b7 U\u221212 \u25e6 \u03c3\u22121\n(135)\nAlternatively, \u03c3 \u25e6Wn\u22121 \u00b7 V1 \u00b7 V \u221212 \u00b7W\u22121n\u22121 \u00b7 \u03c3\u22121 = W\u22121n \u00b7 U1 \u00b7 U\u221212 \u00b7Wn (136)\nin contradiction to the assumption that \u03c3 \u25e6Wn\u22121 \u00b7 V1 \u00b7 V \u221212 \u00b7W\u22121n\u22121 \u00b7 \u03c3\u22121 is not linear.\nLemma 23 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = 1. Then, for every > 0 there is an u\u0304n:2 \u2208 S (un:2) such that C(un \u25e6 u\u0304n:2 \u25e6 u1) = n.\nProof We prove this claim by induction on n.\nCase n = 2: By Lem. 8, there is no possibility that C(u3:1) = 1. Therefore, the claim follows immediately.\nCase n = 3: By Lem. 22, there is u\u03044:1 \u2208 S (u4:1) such that C(u\u03044:1) = k > 1. If k = 2, then, by identifiability, C(u\u03044:2) = 1 or C(u\u03043:1) = 1, in contradiction to Lem. 8. Therefore, k = 3 = n.\nCase n = 4: By Lem. 22, there is u\u03045:1 \u2208 S /4(u5:1) such that C(u\u03045:1) = k > 1. If k = 3, then, by identifiability, there is an index i \u2264 n\u2212 1 such that: C(u\u0304i+2:i) = 1 in contradiction to Lem. 8. If k = 2, then, by identifiability, there are three options:\n\u2022 C(u\u03043:1) = 1 and C(u\u03045:3) = 1.\n\u2022 C(u\u03045:2) = 1.\n\u2022 C(u\u03044:1) = 1.\nThe first option is not a possibility, since it contradicts Lem. 8. The second and third options are analogous - we prove them in parallel. By Case n = 3, there is u\u0304\u20324:2 \u2208 S /4(u\u03044:2) such that C(u4 \u25e6 u\u0304\u20324:2) = 3. By Lem. 21, there is 0 > 0 such that for every u\u0304\u2032\u20324:2 \u2208 S 0(u\u0304\u20324:2) we have: C(u4 \u25e6 u\u0304\u20324:2) = 3. We denote \u2032 = min{ /4, 0}. Again, by Case n = 3, there is u\u0304\u2032\u20324:2 \u2208 S \u2032(u\u0304\u20324:2) such that C(u\u0304\u2032\u20324:2 \u25e6 u1) = 3. Therefore, we have:\nC(u\u0304\u2032\u20324:2 \u25e6 u1) = 3 and C(u4 \u25e6 u\u0304\u2032\u20324:2) = 3 (137)\nBy the above, C(u4 \u25e6 u\u0304\u2032\u20324:2 \u25e6 u1) 6= 2, 3. In addition, by Lem. 21 and Lem. 22 there is \u2032\u2032 \u2264 /4 such that there is u\u0304\u2032\u2032\u20324:2 \u2208 S \u2032\u2032(u\u0304\u20324:2) that satisfies:\nC(u4 \u25e6 u\u0304\u2032\u2032\u20324:2 \u25e6 u1) > 1, C(u\u0304\u2032\u2032\u20324:2 \u25e6 u1) = 3 and C(u4 \u25e6 u\u0304\u2032\u2032\u20324:2) = 3 (138)\nTherefore, C(u4 \u25e6 u\u0304\u2032\u2032\u20324:2 \u25e6 u1) 6= 1, 2, 3. We conclude that C(u4 \u25e6 u\u0304\u2032\u2032\u20324:2 \u25e6 u1) = 4. We consider that u\u0304\u2032\u2032\u20324:2 \u2208 S /4(u\u0304\u2032\u20324:2) \u2282 S /2(u\u0304\u20324:2) \u2282 S3 /4(u\u03044:2) \u2282 S (u4:2). Alternatively, we found u\u0304\u2032\u2032\u20324:2 \u2208 S (u4:2) such that C(u4 \u25e6 u\u0304\u2032\u2032\u20324:2 \u25e6 u1) = 4. Induction hypothesis: We assume that for every k \u2264 n and decomposition uk+1:1 such that C(uk+1:1) = 1, for every > 0 there is an u\u0304k:2 \u2208 S (uk:2) such that C(uk \u25e6 u\u0304k:2 \u25e6 u1) = k. Case n+ 1: By the induction hypothesis,\n\u2203u\u0304n:2 \u2208 S /2(un:2) such that C(un \u25e6 u\u0304n:2 \u25e6 u1) = n (139)\nIn addition, by Lem. 21, there is 0 > 0 such that\n\u2200u\u0304\u2032n+1:1 \u2208 S 0(un \u25e6 u\u0304n:2 \u25e6 u1) =\u21d2 C(u\u0304\u2032n+1:1) = n (140)\nIn particular,\n\u2200u\u0304\u2032n+1:2 \u2208 S 0(u\u0304n+1:2) =\u21d2 u\u0304\u2032n+1:2 \u25e6 u1 \u2208 S 0(un \u25e6 u\u0304n:2 \u25e6 u1) =\u21d2 C(u\u0304\u2032n+1:2 \u25e6 u1) = n (141)\nWe denote \u2032 = min{ /2, 0} and obtain: \u2200u\u0304\u2032n+1:2 \u2208 S \u2032(u\u0304n+1:2) =\u21d2 C(u\u0304\u2032n+1:2 \u25e6 u1) = n. In addition, by Case n = 4, there is u\u0304\u2032n+1:2 \u2208 S \u2032(u\u0304n+1:2) such that C(un+1 \u25e6 u\u0304\u2032n+1:n\u22122) = 4. Thus, we have:\nC(u\u0304\u2032n+1:2 \u25e6 u1) = n and C(un+1 \u25e6 u\u0304\u2032n+1:n\u22122) = 4 (142)\nTherefore, by Thm. 4, since un+1 7 u\u0304\u2032n+1:n\u22122 7 u\u0304\u2032n\u22122:1 and C(u\u0304\u2032n+1:n\u22122) = I = 3 we have:\nC(un+1 \u25e6 u\u0304\u2032n+1:2 \u25e6 u1) = n+ 1 (143)\nIn addition, u\u0304\u2032n+1:2 \u2208 S (un+1:2) (144) since u\u0304\u2032n+1:2 \u2208 S \u2032(u\u0304n+1:2) and u\u0304n+1:2 \u2208 S /2(un+1:2) and \u2032 \u2264 /2.\nLemma 24 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition such that C(un+1:1) = n. Then, if for every > 0 there is u\u0304n+1:1 \u2208 S (un+1:1) such that C(u\u0304q+1:p) = 1 then C(uq+1:p) \u2264 1.\nProof Assume that for every > 0, there is u\u0304n+1:1 \u2208 S (un+1:1) such that C(u\u0304q+1:p) = 1. Let {ukn+1:1}\u221ek=1 be a sequence such that ukn+1:1 \u2208 S1/k(un+1:1) and C(ukq+1:p) = 1. We denote, uki = (\u03c3 \u25e6W ki ) for i \u2208 [n] and k \u2208 N. We denote Uk a invertible linear mapping such that:\nukq+1:p = (\u03c3 \u25e6 Uk) (145)\nIn particular, \u03c3\u22121 \u25e6 ukq+1:p = Uk (146)\nTherefore, \u2200x : ||\u03c3\u22121 \u25e6 ukq+1:p(x)||2 = ||Uk(x)||2 (147)\nBy Lem. 20,\n||\u03c3\u22121 \u25e6 ukq+1:p(x)||2 \u2264 max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n||W ki ||2 \u00b7 ||x||2\n\u2264 max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n(||Wi||2 + /k) \u00b7 ||x||2 (148)\nTherefore,\n||Uk||2 \u2264 max{a, 1}q\u2212p+1 \u00b7 q\u220f\ni=p\n(||Wi||+ /k) (149)\nAnd also, if the input dimension is M and || \u00b7 ||F is the Frobenious norm, then,\n||Uk||F \u2264M \u00b7max{a, 1}q\u2212p\u22121 \u00b7 q\u220f\ni=p\n(||Wi||+ /k) (150)\nIn particular, the sequence Uk is bounded. Thus, by the Bolzano-Weierstrass theorem, there is a subsequence Ukt that converges to a matrix U (w.r.t the `2 norm). With no loss of generality, we can replace Uk with the sequence Ukt . Alternatively, we can assume that Uk converges to the matrix U (w.r.t the `2 norm). In addition, we have W ki \u2192Wi (w.r.t the `2 norm). Therefore, since for any x, fEn,...,E1(x) is continuous as a function of a the matrices En, ..., E1, we have:\nlim k\u2192\u221e\nukq+1:p(x) = uq+1:p(x) (151)\nOn the other hand, since for any x, the function fE(x) is continuous as a function of a matrix E, we have:\nlim k\u2192\u221e ukq+1:p(x) = lim k\u2192\u221e (\u03c3 \u25e6 Uk)(x) = (\u03c3 \u25e6 U)(x) (152)\nFinally, \u2200x : uq+1:p(x) = (\u03c3 \u25e6 U)(x) (153)\nAlternatively, uq+1:p = (\u03c3 \u25e6 U) (154)\nTherefore, C(uq+1:p) \u2264 1.\nLemma 25 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 and vk+1:1 be two decompositions such that C(un+1:1) = n and C(vk+1:1) = k. Then, for every > 0, there is u\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S (un:1 \u25e6 vk+1:2) such that: C(un \u25e6 u\u0304n:1 \u25e6 v\u0304k+1:2 \u25e6 v1) = n+ k.\nProof Assume by contradiction that there is > 0 such that:\n\u2200u\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S (un:1 \u25e6 vk+1:2) =\u21d2 C(un \u25e6 u\u0304n:1 \u25e6 v\u0304k+1:2 \u25e6 v1) < n+ k (155)\nIf un+1:1 7 vk+1:1 then we take u\u0304n:1 = un:1 and v\u0304k+1:2 = vk+1:2. Otherwise, by identifiability, there are indexes p \u2264 k and q \u2264 n such that\nC(uq+1:1 \u25e6vk+1:p) = 1 and C(un+1:q+1 \u25e6uq+1:1 \u25e6vk+1:p \u25e6vp:1) = (n\u2212q)+1+(p\u22121) = n\u2212q+p (156)\nBy Lem. 21, there is 0 > 0 such that u\u0304n:1 \u2208 S (un:1) and v\u0304k+1:2 \u2208 S (vk+1:2) then C(un \u25e6 u\u0304n:1) = n and C(v\u0304k+1:2 \u25e6 v1) = k. We denote 1 = min{ 0, }. We assumed that if u\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S (un:1 \u25e6 vk+1:2) then C(un \u25e6 u\u0304n:1 \u25e6 v\u0304k+1:2 \u25e6 v1) < n + k. Thus, by identifiability, for every u\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S 1(un:1 \u25e6 vk+1:2) there are r < s such that:\nC(u\u0304r+1:1\u25e6v\u0304k+1:s) = 1 and C(un\u25e6u\u0304n:r+1\u25e6u\u0304r+1:1\u25e6v\u0304k+1:s\u25e6v\u0304s:2\u25e6v1) = (n\u2212r)+1+(s\u22121) = n\u2212r+s (157)\nThus, by Lem. 22, for every sequence {uin+1:1 \u25e6 vik+1:2}\u221ei=1 such that\nuin+1:1 \u25e6 vik+1:2 \u2208 S 1/i(un:1 \u25e6 vk+1:2) C(uir+1:1 \u25e6 vik+1:s) = 1 where s \u2264 k and r \u2264 n C(un \u25e6 uin:1 \u25e6 vik+1:2 \u25e6 v1) = n\u2212 r + s\n=\u21d2 r = q and s = p\n(158)\nIn particular, there is an 0 < 2 \u2264 1 such that\nu\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S 2(un:1 \u25e6 vk+1:2), C(u\u0304r+1:1 \u25e6 v\u0304k+1:s) = 1 and C(un \u25e6 u\u0304n:1 \u25e6 v\u0304k+1:2 \u25e6 v1) = n\u2212 r + s =\u21d2 r = q and s = p\n(159)\nTherefore, u\u0304n:1 \u25e6 v\u0304k+1:2 \u2208 S 2(un:1 \u25e6 vk+1:2) =\u21d2 C(u\u0304q+1:1 \u25e6 v\u0304k+1:p) = 1 (160)\nIn particular, u\u0304q+1:1 \u25e6 v\u0304k+1:p \u2208 S 2(uq+1:1 \u25e6 vk+1:p) =\u21d2 C(u\u0304q+1:1 \u25e6 v\u0304k+1:p) = 1 (161)\nin contradiction to Lem. 23.\nLemma 26 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 be a decomposition. Then, for every > 0 there is u\u0304n:2 \u2208 S (un:2) such that: C(un \u25e6 u\u0304n:2 \u25e6 u1) = n.\nProof If C(un+1:1) = n then we take u\u0304n+1:1 = un+1:1. Otherwise, we denote C(un+1:1) = k and by identifiability, \u2203j1 = 1 < ... < jk+1 = n+ 1 : C(uji+1:ji) = 1 (162) By Lem. 8, for every i such that ji+1 6= ji + 1, we have: ji + 3 \u2264 ji+1. By Lem. 23, for each i such that ji+1 6= ji + 1\n\u2203u\u0304ji+1\u22121:ji+1 \u2208 S /k(uji+1\u22121:ji+1) such that C(uji+1 \u25e6 u\u0304ji+1\u22121:ji+1 \u25e6 uji) = ji+1 \u2212 ji (163)\nWe denote: \u2200i \u2208 [k] : v1ji = uji and \u2200t \u2208 [n] \\ {j1, ..., jk} : v1t = u\u0304t (164)\nWe denote: \u2200i \u2208 [k + 1] : j1i = ji (165)\nWe have: \u2200i \u2208 [k] : C(v1j1i+1:j1i ) = j 1 i+1 \u2212 j1i\nv1n:2 \u2208 S /k(un:2) v11 = u1\n(166)\nBy Lem. 25, there is a decomposition\nv\u03041j13\u22121:j11+1 \u2208 S /k(v 1 j13\u22121:j11+1) (167)\nsuch that C(v1 j13 \u25e6 v\u03041 j13\u22121:j11+1 \u25e6 v1 j11 ) = j13 \u2212 j11 . Next, we replace v1n+1:1 with v2n+1:1 defined as follows:\n\u2200i \u2265 j13 : v2i = v1i , \u22001 < i < j13 : v2i = v\u03041i and v21 = v11 = u1 (168)\nand we denote: j21 = 1 and \u2200i \u2208 [k] \\ {1} : j2i = j1i+1 (169)\nWe have: \u2200i \u2208 [k \u2212 1] : C(v2j2i+1:j2i ) = j 2 i+1 \u2212 j2i\nv2n:2 \u2208 S2 /k(un:2) v21 = v 1 1 = u1\n(170)\nWe continue the process of replacing each vtn+1:1 with v t+1 n+1:1 for k \u2212 1 times. For each iteration, we have a decomposition that satisfies:\n\u2200i \u2208 [k \u2212 t+ 1] :C(vtjti+1:jti ) = j t i+1 \u2212 jti , jt1 = 1, jtk\u2212t+2 = n+ 1, vt1 = u1,\nvtn = un and v t n:2 \u2208 St /k(un:2)\n(171)\nand obtain that u\u0304n+1:1 = vkn+1:1 satisfies:\nC(u\u0304n+1:1) = n, u\u03041 = u1, u\u0304n = un and u\u0304n:2 \u2208 S (un:2) (172)\nLemma 27 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let un+1:1 such that C(un+1:1) = n. Then, for all > 0 there is u\u0304n+1:1 \u2208 S (un+1:1) such that C(u\u0304\u22121n+1:1) = n+ 2.\nProof We define a one-to-one function between compositions:\nF((\u03c3 \u25e6 Un) \u25e6 ... \u25e6 (\u03c3 \u25e6 U1)) = (\u03c3 \u25e6 U\u221211 /a) \u25e6 ... \u25e6 (\u03c3 \u25e6 U\u22121n /a) (173)\nWe consider that for any decomposition dn+1:1, we have:\nd\u22121n+1:1 = (\u03c3 \u25e6 \u2212Id) \u25e6 F(dn+1:1) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (174)\nLet u = un+1:1 and u\u0304n+1:1 be two decompositions such that ui = \u03c3 \u25e6Wi and u\u0304i = \u03c3 \u25e6 Ui, where Wi and Ui are invertible linear mapping. Since W\u22121 is continuous as a function of W , there is \u2032 > 0 such that\n\u2200i \u2208 [n] : ||W\u22121i /a\u2212 U\u22121i /a|| < \u2032 =\u21d2 \u2200i \u2208 [n] : ||Wi \u2212 Ui|| < (175)\nIn particular, S \u2032(F(un+1:1)) \u2282 { v\u0304n+1:1 \u2223\u2223 F\u22121(v\u0304n+1:1) \u2208 S (un+1:1) }\n(176)\nBy Lem. 26, there is v\u0304n+1:1 \u2208 S \u2032(F(un+1:1)) such that C((\u03c3 \u25e6 \u2212Id) \u25e6 v\u0304n+1:1 \u25e6 (\u03c3 \u25e6 \u2212Id/a)) = n+ 2 (177)\nTherefore, there is a decomposition u\u0304n+1:1 = F\u22121(v\u0304n+1:1) \u2208 S (un+1:1) such that: u\u0304\u22121n+1:1 = (\u03c3 \u25e6 \u2212Id) \u25e6 v\u0304n+1:1 \u25e6 (\u03c3 \u25e6 \u2212Id/a) (178)\nThus, C(u\u0304\u22121n+1:1) = n+ 2.\nTheorem 2 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n such that C(u\u22121n+1:1) = C(un+1:1) + 2 and ui = \u03c3 \u25e6Wi (for i \u2208 [n]) is open and dense in RM\u00d7M\u00d7n. Proof We denote by F the function from Eq. 173, Z = { (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n \u2223\u2223 \u2200i \u2208 [n] : ui = \u03c3 \u25e6Wi, Wi is invertible and C(u\u22121n+1:1) = C(un+1:1) + 2 } (179) and,\nZ \u2032 = { (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n \u2223\u2223 \u2200i \u2208 [n] : Wi is invertible } (180)\nOpenness: Let un+1:1 be a decomposition such that C(u\u22121n+1:1) = n+ 2 and ui = \u03c3 \u25e6Wi. We have: u\u22121n+1:1 = (\u03c3 \u25e6 \u2212Id) \u25e6 F(un+1:1) \u25e6 (\u03c3 \u25e6 \u2212Id/a) (181)\nis a minimal decomposition. By Lem. 21, there is an > 0 such that:\nv\u0304n+1:1 \u2208 S (F(un+1:1)) =\u21d2 C((\u03c3 \u25e6 \u2212Id) \u25e6 v\u0304n+1:1 \u25e6 (\u03c3 \u25e6 \u2212Id/a)) = n+ 2 (182) Since W\u22121 is continuous as a function of W , there is \u2032 > 0 such that\n\u2200i \u2208 [n] : ||Wi \u2212 Ui|| < \u2032 =\u21d2 i \u2208 [n] : ||W\u22121i /a\u2212 U\u22121i /a|| < (183) In particular,\nS \u2032(un+1:1) \u2282 { u\u0304n+1:1 \u2223\u2223 F(u\u0304n+1:1) \u2208 S \u2032(F(un+1:1)) }\n(184)\nTherefore,\nu\u0304n+1:1 \u2208 S \u2032(un+1:1) =\u21d2 C(u\u0304\u22121n+1:1) = C((\u03c3 \u25e6 \u2212Id) \u25e6 F(u\u0304n+1:1) \u25e6 (\u03c3 \u25e6 \u2212Id/a)) = n+ 2 (185) Density: By Lem. 27, the set Z is dense in Z \u2032. In addition, the set Z \u2032 is dense in RM\u00d7M\u00d7n. Since density is a transitive relation, Z is also dense in RM\u00d7M\u00d7n.\nTheorem 3 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. The set of sequences of invertible matrices (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n such that C(un+1:1) = n where ui = \u03c3 \u25e6Wi (for i \u2208 [n]) is open and dense in RM\u00d7M\u00d7n.\nProof We denote:\nZ = { (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n \u2223\u2223 \u2200i \u2208 [n] : ui = \u03c3 \u25e6Wi, Wi is invertible and C(un+1:1) = n } (186)\nand, Z \u2032 = { (W1, ...,Wn) \u2208 RM\u00d7M\u00d7n \u2223\u2223 \u2200i \u2208 [n] : Wi is invertible }\n(187)\nBy Lem. 21 and Lem. 27, the set Z is open and dense in Z \u2032. In addition, Z \u2032 is open and dense in RM\u00d7M\u00d7n. Openness and density are both transitive relations. Therefore, Z is also open and dense in RM\u00d7M\u00d7n."}, {"heading": "Appendix H. Proof of Thm. 5", "text": ""}, {"heading": "H.1 Covering numbers", "text": "Definition 14 (Set embedding) Let (U ,\u223cU ) and (V,\u223cV) be two tuples of sets and symmetric and reflexive relations on them (resp.). A function F : U \u2192 V is an embedding of (U ,\u223cU ) in (V,\u223cV) and we denote (U ,\u223cU ) (V,\u223cV) if:\n\u2200u1, u2 \u2208 U : F (u1) \u223cV F (u2) =\u21d2 u1 \u223cU u2 (188)\nLemma 28 Let (U ,\u223cU ) and (V,\u223cV) be two tuples of sets and equivalence relations on them (resp.). Assume that |U/ U | and |V/ V | are finite. Then,\n(U ,\u223cU ) (V,\u223cV) \u21d0\u21d2 |U/ \u223cU | \u2264 |V/ \u223cV | (189)\nProof Assume that (U ,\u223cU ) (V,\u223cV). Then, there is a function F : U \u2192 V such if F (u1) \u223cV F (u2) then u1 \u223cU u2. We denote n := |U/ \u223cU |. Let d1, ..., dn be n representatives of the n classes in U/ \u223cU , i.e, \u2200i 6= j \u2264 n : di 6\u223cU dj . Then, \u2200i 6= j \u2264 n : F (di) 6\u223cV F (dj). Therefore, we found n representatives of n different equivalence classes of (V,\u223cV). In particular, |U/ \u223cU | \u2264 |V/ \u223cV |. Assume that |U/ \u223cU | \u2264 |V/ \u223cV |. We denote n := |U/ \u223cU |. Let d1, ..., dn be n representatives of the n classes in U/ \u223cU , i.e, \u2200i 6= j \u2264 n : di 6\u223cU dj . Similarly, e1, ..., em are m representatives of the m classes in V/ \u223cV , i.e, \u2200i 6= j \u2264 n : ei 6\u223cV ej . We define a mapping F (u) = F (di) = ei iff u \u223cU di. Thus, for any u1, u2 \u2208 U , if u1 \u223cU u2 then F (u1) = F (u2). In addition, if F (u1) 6\u223cV F (u2) then F (u1) = ei and F (u2) = ej for i 6= j \u2264 n. Thus, u1 \u223cU di and u2 \u223cU dj such that di 6= dj . But, by the way we defined d1, ..., dn we have: di 6= dj implies di 6\u223cU dj . Therefore, u1 \u223cU u2 iff F (u1) \u223cV F (u2). Alternatively, F is an embedding from (U ,\u223cU ) to (V,\u223cV).\nLemma 29 Let (U ,\u223cU ) be a tuples of a set and an equivalence relation on it (resp.). Then,\nCovering(U ,\u223cU ) = |U/ \u223cU | (190)\nProof First, we consider that (U,\u223cU ) is a covering of itself and therefore, Covering(U ,\u223cU ) \u2264 |U/ \u223cU |. Assume by contradiction that Covering(U ,\u223cU ) < |U/ \u223cU |. Thus, there is a covering (U ,\u2261U ) of (U ,\u223cU ) such that |U/ \u2261U | < |U/ \u223cU |. But, by definition u1 \u2261U u2 =\u21d2 u1 \u223cU u2. Thus, if u1, ..., un \u2208 U are n representations of n different equivalence classes in (U ,\u223cU ) then u1, ..., un \u2208 U are also n representations of n different equivalence classes in (U ,\u2261U ). Therefore, |U/ \u223cU | \u2264 |U/ \u2261U | in contradiction to |U/ \u2261U | < |U/ \u223cU |. Finally, we conclude that: Covering(U ,\u223cU ) = |U/ \u223cU |.\nLemma 30 Let (U ,\u223cU ) and (V,\u223cV) be two tuples of sets and reflexive and symmetric relations on them (resp.). If (U ,\u223cU ) (V,\u223cV) then Covering(U ,\u223cU ) \u2264 Covering(V,\u223cV).\nProof Assume that (U ,\u223cU ) (V,\u223cV). Then, by definition, there is an embedding function F : U \u2192 V such that: \u2200u1, u2 \u2208 U : F (u1) \u223cV F (u2) =\u21d2 u1 \u223cU u2 (191) Let (V,\u2261V) be a covering of (V,\u223cV). We define a covering (U ,\u2261U ) of (U ,\u223cU ) as follows:\nu1 \u2261U u2 \u21d0\u21d2 F (u1) \u2261V F (u2) (192)\nPart 1: We would like to prove that (U ,\u2261U ) is a covering of (U ,\u223cU ). It is easy to see that \u2261U is an equivalence relation since \u2261V is an equivalence relation. Next, we would like to prove that u1 \u2261U u2 =\u21d2 u1 \u223cU u2. By the definition of \u2261U :\nu1 \u2261U u2 =\u21d2 F (u1) \u2261V F (u2) (193)\nIn addition, since (V,\u2261V) is a covering of (V,\u223cV):\nF (u1) \u2261V F (u2) =\u21d2 F (u1) \u223cV F (u2) (194)\nFinally, since F is an embedding:\nF (u1) \u223cV F (u2) =\u21d2 u1 \u223cU u2 (195)\nWe conclude: u1 \u2261U u2 =\u21d2 u1 \u223cU u2 (196)\nTherefore, (U ,\u2261U ) is indeed a covering of (U ,\u223cU ). Part 2: We would like to prove that |U/ \u2261U | \u2264 |V/ \u2261V |. Let u1, u2 \u2208 U such that u1 6\u2261U u2. Then, by definition of \u2261U we have: F (u1) 6\u2261V F (u2). Therefore, if we take u1, ..., un \u2208 U representations of n different equivalence classes in (U ,\u2261U ) then, F (u1), ..., F (un) \u2208 V are n representations of n different equivalence classes in (V,\u2261V). In particular, |U/ \u2261U | \u2264 |V/ \u2261V |. Therefore, the covering number of (U ,\u223cU ) is at most the covering number of (V,\u223cV).\nLemma 31 Let (U ,\u22611) and (U ,\u22612) be coverings of (U ,\u223cU ). Then, (U2,\u22611 \u00d7 \u22612) is a covering of (U2,\u223c2U ). Where U2 = U \u00d7 U and the relation \u223c2U is defined as follows:\n(a, b) \u223c2U (c, d) \u21d0\u21d2 a \u223cU c and b \u223cU d (197)\nand \u22611 \u00d7 \u22612 is defined as:\n(a, b) \u22611 \u00d7 \u22612 (c, d) \u21d0\u21d2 a \u22611 c and b \u22612 d (198)\nProof We have to prove that \u22611 \u00d7 \u22612 is an equivalence relation and that (u1, u2) \u22611 \u00d7 \u22612 (v1, v2) =\u21d2 (u1, u2) \u223c2U (v1, v2). Reflexivity: (u1, u2) \u22611 \u00d7 \u22612 (u1, u2) \u21d0\u21d2 u1 \u22611 u1 and u2 \u22611 u2 (199) The RHS is true since \u22611 and \u22612 are reflexive relations. Symmetry: (u1, u2) \u22611 \u00d7 \u22612 (v1, v2) \u21d0\u21d2 u1 \u22611 v1 and u2 \u22612 v2 (200) Since \u22611 and \u22612 are symmetric, we have:\nu1 \u22611 v1 and u2 \u22612 v2 \u21d0\u21d2 v1 \u22611 u1 and v2 \u22612 u2 (201)\nIn addition, (v1, v2) \u22611 \u00d7 \u22612 (u1, u2) \u21d0\u21d2 v1 \u22611 u1 and v2 \u22612 u2 (202)\nTherefore, (u1, u2) \u22611 \u00d7 \u22612 (v1, v2) \u21d0\u21d2 (v1, v2) \u22611 \u00d7 \u22612 (u1, u2) (203)\nTransitivity: follows from similar arguments. Covering:\n(u1, u2) \u22611 \u00d7 \u22612 (v1, v2) \u21d0\u21d2 u1 \u22611 v1 and u2 \u22612 v2 (204)\nSince (U ,\u2261i) is a covering of (U ,\u223cU ), for i = 1, 2, we have: u1 \u22611 v1 and u2 \u22612 v2 =\u21d2 u1 \u223cU v1 and u2 \u223cU v2 (205)\nBy the definition of \u223c2U we have: u1 \u223cU v1 and u2 \u223cU v2 \u21d0\u21d2 (u1, u2) \u223cU (v1, v2) (206)\nTherefore, (u1, u2) \u22611 \u00d7 \u22612 (v1, v2) =\u21d2 (u1, u2) \u223c2U (v1, v2) (207)\nLemma 32 Let (U ,\u223cU ) be a tuple of a set and a reflexive and symmetric relation on it (resp.). Then, Covering(U2,\u223c2U ) = Covering(U ,\u223cU )2 (208)"}, {"heading": "Proof", "text": "(\u2264) : Let \u2261U be an equivalence relation such that (U ,\u2261U ) is a covering of (U ,\u223cU ). By Lem. 31, (U2,\u22612U ) is a covering of (U2,\u223c2U ). In addition,\n|U2/ \u22612U | = |U/ \u2261U |2 (209) Thus, for every covering (U ,\u2261U ) of (U ,\u223cU ) we can construct a covering of (U2,\u223c2U ) of size |U/ \u2261U |2. In particular, Covering(U2,\u223c2U ) \u2264 Covering(U ,\u223cU )2 (210) (\u2265) : Let (U2,\u2261) be a covering of (U2,\u223c2U ). Let \u2261i be the following equivalence relation on U (proving that \u22611, \u22612 are equivalence relations is left for the reader),\nu1 \u22611 v1 \u21d0\u21d2 \u2203u2, v2 \u2208 U s.t (u1, u2) \u2261 (v1, v2) (211) we similarly define \u22612 (w.r.t the second coordinate). We also note that (U ,\u22611) and (U ,\u22612) are coverings of (U ,\u223cU ). That is because,\nu1 \u22611 v1 \u21d0\u21d2 \u2203v1, v2 s.t (u1, v1) \u2261 (u2, v2) =\u21d2 u1 \u223cU u2\n(212)\nLet u1, ..., un are representations of the n different equivalence classes of \u22611 and v1, ..., vk are representations of the k different equivalence classes of \u22612 then: (ui, vj) \u2261 (us, vt) iff ui = us and vj = vt. Otherwise, there are ui, us and vj , vt such that (with no loss of generality), ui 6= us and (ui, vj) \u2261 (us, vt). Since ui 6= us then by the way we defined u1, ..., un we have ui 6\u22611 us. Thus, by definition, there are no v, v\u2032 \u2208 U such that (ui, v) \u2261 (us, v\u2032) in contradiction to (ui, vj) \u2261 (us, vt). Thus, {(ui, vj)}i,j\u2208[n]\u00d7[k] are in different equivalence classes by \u2261. In particular,\n|U2/ \u22611 \u00d7 \u22612 | \u2264 |U/ \u22611 | \u00b7 |U/ \u22612 | \u2264 |U2/ \u22612U | (213) And by Lem. 31, Covering(U ,\u223cU )2 \u2264 |U2/ \u22611 \u00d7 \u22612 | (214) By combining Eq. 213 and 214, Covering(U ,\u223cU )2 \u2264 |U2/ \u22612U | (215) Therefore, Covering(U ,\u223cU )2 \u2264 Covering(U2,\u223c2U ) (216) By taking minimum over the RHS of Eq. 215 over the different possibilities of \u22612U .\nLemma 33 Let (U ,\u223cU ) and (V,\u223cV) be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that U \u2282 V and \u223cU= (\u223cV) \u2223\u2223 U , i.e,\n\u2200u, v \u2208 U : u \u223cU v \u21d0\u21d2 u \u223cV v (217)\nThen, Covering(U ,\u223cU ) \u2264 Covering(V,\u223cV) (218)\nProof Let (V,\u2261V) be a covering of (V,\u223cV). Then, it is easy to see that (U ,\u2261U ) is a covering of (U ,\u223cU ), where \u2261U= (\u2261V) \u2223\u2223 U . In addition, we have: |U/ \u2261U | \u2264 |V/ \u2261V |. Thus, for every covering of (V,\u223cV), we can find a smaller covering for (U ,\u223cU ). In particular, Covering(U ,\u223cU ) \u2264 Covering(V,\u223cV).\nLemma 34 Let (U1,\u223cU1) and (U2,\u223cU2) be two tuples of sets and reflexive and symmetric relations on them (resp.). Assume that U1 \u2229 U2 = \u2205. We define (U ,\u223cU ) as follows:\nU = U1 \u222a U2 u \u223cU v \u21d0\u21d2 \u2203i = 1, 2 : u, v \u2208 Ui and u \u223cUi v\n(219)\nThen, Covering(U ,\u223cU ) \u2264 Covering(U1,\u223cU1) + Covering(U2,\u223cU2) (220)\nProof Let (U1,\u2261U1) and (U2,\u2261U2) be minimal coverings of (U1,\u223cU1) and (U2,\u223cU2) (resp.). We define a covering (U ,\u2261U ) of (U ,\u223cU ) as follows:\nu \u2261U v \u21d0\u21d2 \u2203i = 1, 2 : u, v \u2208 Ui and u \u2261Ui v (221)\nSince both \u2261U1 and \u2261U2 are equivalence relations, \u2261U is also an equivalence relation. In addition,\nu \u2261U v =\u21d2 \u2203i = 1, 2 : u, v \u2208 Ui and u \u2261Ui v =\u21d2 \u2203i = 1, 2 : u, v \u2208 Ui and u \u223cUi v =\u21d2 u \u223cU v\n(222)\nWe also consider that the members U1 and the members of U2 do not share equivalence classes by \u2261U . In addition, the members of U1 are partitioned into equivalence classes by \u2261U1 . Similarly, U2 w.r.t \u2261U2 . Therefore,\nCovering(U ,\u223cU ) \u2264 |U/ \u2261U | = |U1/ \u2261U1 |+ |U2/ \u2261U2 | = Covering(U1,\u223cU1) + Covering(U2,\u223cU2)\n(223)"}, {"heading": "H.2 Perturbations and discrepancy", "text": "Assumption 1 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. For every m > 0 and n > 0, the function discm(fWn,...,W1 \u25e6D1, D2) (18) is continuous as a function of the weights of Wn, ...,W1. Here, fWn,...,W1 = (\u03c3 \u25e6Wn) \u25e6 ... \u25e6 (\u03c3 \u25e6W1).\nAssumption 2 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. For all m > 0, the function\nRD[fVm,...,V1 , fWm,...,W1 ] (22)\nis continuous as a function of Vm, ..., V1,Wm, ...,W1.\nLemma 35 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0 and assume Assumption 2 with D :\u2190 D1. Let discm,E = discCm,E for Cm,E = { um+1:1 \u2223\u2223\u2200i \u2208 [m] : ui = \u03c3 \u25e6Wi s.t: ||Wi|| \u2264 E }\n. For all m > 0, n > 0 and E > 0, the function\ndiscm,E(fWn,...,W1 \u25e6D1, D2) (224)\nis continuous as a function of Wn, ...,W1.\nProof Let Wn, ...,W1 and W kn , ...,W k1 be any invertible matrices in RM\u00d7M such that for all i \u2208 [n], W ki \u2192 Wi. We denote GE = { W \u2208 RM\u00d7M \u2223\u2223 ||W || \u2264 E } . By the triangle inequality,\ndiscm,E(D1, D2) \u2264 discm,E(D1, D3) + discm,E(D3, D2) =\u21d2 discm,E(D1, D2)\u2212 discm,E(D3, D2) \u2264 discm,E(D1, D3)\n(225)\nSimilarly, discm,E(D3, D2) \u2264 discm,E(D1, D3) + discm,E(D1, D2)\n=\u21d2 discm,E(D3, D2)\u2212 discm,E(D1, D2) \u2264 discm,E(D1, D3) (226)\ntherefore, |discm,E(D3, D2)\u2212 discm,E(D1, D2)| \u2264 discm,E(D1, D3) (227)\nIn particular, \u2223\u2223\u2223discm,E(fWn,...,W1 \u25e6D1, D2)\u2212 discm,E(fWkn ,...,Wk1 \u25e6D1, D2)\n\u2223\u2223\u2223 \u2264discm,E(fWn,...,W1 \u25e6D1, fWkn ,...,Wk1 \u25e6D1)\n\u2264 sup c1,c2\u2208Cm,E\n\u2223\u2223\u2223RD1 [c1 \u25e6 fWn,...,W1 , c2 \u25e6 fWn,...,W1 ]\u2212RD1 [c1 \u25e6 fWkn ,...,Wk1 , c2 \u25e6 fWkn ,...,Wk1 ] \u2223\u2223\u2223\n\u2264 sup V1,..,Vm,U1,...,Um\u2208GE\n\u2223\u2223\u2223RD1 [fVm,...,V1 \u25e6 fWkn ,...,Wk1 , fUm,...,U1 \u25e6 fWn,...,W1 ]\n\u2212RD1 [fVm,...,V1 \u25e6 fWkn ,...,Wk1 , fUm,...,U1 \u25e6 fWkn ,...,Wk1 ] \u2223\u2223\u2223\n\u2264 sup V1,..,Vm,U1,...,Um\u2208GE\n\u2223\u2223\u2223RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ]\n\u2212RD1 [fVm,...,V1,Wkn ,...,Wk1 , fUm,...,U1,Wkn ,...,Wk1 ] \u2223\u2223\u2223\n(228)\nAssume by contradiction that the last expression does not converge to 0. Therefore, there is a sequence (V k1 , ..., V k m, U k 1 , ..., U k m) such that V k 1 , .., V k m, U k 1 , ..., U k m \u2208 GE and\n\u2223\u2223\u2223RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ]\n\u2212RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ] \u2223\u2223\u2223 6\u2192 0\n(229)\nSince (V k1 , ..., V k m, U k 1 , ..., U k m) \u2208 G2mE and G2mE is compact in RM\u00d7M\u00d72m, by the Bolzano-Weierstrass theorem, there is a converging subsequence. Alternatively, there is an increasing sequence {kj}\u221ej=1 \u2282 N such that\n(V kj 1 , ..., V kj m , U kj 1 , ..., U kj m )\u2192 (V1, ..., Vm, U1, ..., Um) \u2208 G2mE (230)\nIn particular, (V kjm , ..., V kj 1 ,W kj n , ...,W kj 1 )\u2192 (Vm, ..., V1,Wn, ...,W1)\n(Ukjm , ..., U kj 1 ,W kj n , ...,W kj 1 )\u2192 (Um, ..., U1,Wn, ...,W1)\n(231)\nBy Assumption 2, the function RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ] is a continuous. Therefore, \u2223\u2223\u2223RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ]\n\u2212RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ] \u2223\u2223\u2223\u2192 0\n(232)\nand, \u2223\u2223\u2223RD1 [fVm,...,V1,Wn,...,W1 , fUm,...,U1,Wn,...,W1 ]\n\u2212RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ] \u2223\u2223\u2223\u2192 0\n(233)\nand therefore, by the triangle inequality, \u2223\u2223\u2223RD1 [fV km,...,V k1 ,Wn,...,W1 , fUkm,...,Uk1 ,Wn,...,W1 ] \u2212RD1 [fV km,...,V k1 ,Wkn ,...,Wk1 , fUkm,...,Uk1 ,Wkn ,...,Wk1 ]\u2192 0\n(234)\nin contradiction. Thus, we conclude that:\nlim k\u2192\u221e \u2223\u2223\u2223discm,E(fWn,...,W1 \u25e6D1, D2)\u2212 discm,E(fWkn ,...,Wk1 \u25e6D1, D2) \u2223\u2223\u2223 = 0 (235)\nLemma 36 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. In addition, let f = fn+1:1 and g = gn+1:1 be two decompositions. Assume that discm(g \u25e6DA, DB) \u2264 0 and m \u2265 t + n + 2. Then, there are f\u0304 = f\u0304n+1:1, g\u0304 = g\u0304n+1:1 and q\u0304 = q\u0304n+3:1 such that:\n\u2022 q\u0304n+2:1 = F(g\u0304n+1:1) \u25e6 (\u03c3 \u25e6 \u2212Id/a).\n\u2022 C(f\u0304n+1:1 \u25e6 q\u0304n+3:1) = 2n+ 2.\n\u2022 \u2200j \u2208 [n+ 1] : q\u0304n\u2212j+3:1 \u25e6 g\u0304 \u25e6DA = (\u2212\u03c3\u22121) \u25e6 g\u0304j:1 \u25e6DA.\n\u2022 disct(q\u0304 \u25e6DB , DA) \u2264 0 + .\n\u2022 \u2200j \u2208 [n] : discm(f\u0304j+1:1 \u25e6DA, fj+1:1 \u25e6DA) \u2264 .\n\u2022 \u2200j \u2208 [n] : discm(g\u0304j+1:1 \u25e6DA, gj+1:1 \u25e6DA) \u2264 .\nProof We denote F+ and F are the functions from Eq 60 and Eq. 173 (resp.). Let:\nqn+3:1 = F+(gn+1:1) (236)\nLet q\u0304 = q\u0304n+3:1 = q\u0304n+3:2 \u25e6 (\u03c3 \u25e6 \u2212Id/a) and g\u0304 = F\u22121(q\u0304n+2:2). By the first item of Lem. 2, for D1 :\u2190 DB , D2 :\u2190 g\u0304 \u25e6DA, D3 :\u2190 DA, p :\u2190 q\u0304, m \u2265 t+ n+ 2 \u2265 t+ C(q\u0304), we have:\ndisct(q\u0304 \u25e6DB , DA) \u2264 discm(q\u0304 \u25e6 g\u0304 \u25e6DA, DA) + discm(g\u0304 \u25e6DA, DB) = discm(q\u0304n+2 \u25e6 q\u22121n+2 \u25e6DA, DA) + discm(g\u0304 \u25e6DA, DB) \u2264 discm(q\u0304n+2 \u25e6 q\u22121n+2 \u25e6DA, DA) + discm(g \u25e6DA, DB) + discm(g\u0304 \u25e6DA, g \u25e6DA) \u2264 discm(q\u0304n+2 \u25e6 q\u22121n+2 \u25e6DA, DA) + 0 + discm(g\u0304 \u25e6DA, g \u25e6DA)\n(237) The weights of g\u0304n+1:1 are continuous functions of the weights of q\u0304n+2:2. In addition, for all j \u2208 [n], the functions discm(f\u0304j+1:1\u25e6DA, fj+1:1\u25e6DA), discm(g\u0304j+1:1\u25e6DA, gj+1:1\u25e6DA) and discm(q\u0304n+2\u25e6q\u22121n+2\u25e6DA, DA) are continuous as a function of the weights of f\u0304n+1:1 and g\u0304n+1:1 (resp.). In particular,\n(discm(f\u0304j+1:1 \u25e6DA, fj+1:1 \u25e6DA))nj=1 ||(discm(g\u0304j+1:1 \u25e6DA, gj+1:1 \u25e6DA))nj=1 ||(discm(q\u0304n+2 \u25e6 q\u22121n+2 \u25e6DA, DA))\n(238)\nis a continuous function of the weights of f\u0304n+1:1 \u25e6 q\u0304n+3:1. Here, || is the concatenations operator between tuples, i.e, (x1, ..., xn)||(y1, ..., ym) = (x1, ..., xn, y1, ..., ym). Therefore, for every > 0 there is \u2032 > 0 such that for all f\u0304n:1 \u25e6 q\u0304n+3:2 \u2208 S \u2032(fn:1 \u25e6 qn+3:2) we have:\n\u2200j \u2208 [n] : discm(f\u0304j+1:1 \u25e6DA, fj+1:1 \u25e6DA) \u2264 /2 discm(g\u0304j+1:1 \u25e6DA, gj+1:1 \u25e6DA) \u2264 /2 discm(q\u0304n+2 \u25e6 q\u22121n+2 \u25e6DA, DA) \u2264 /2\n(239)\nWhere q\u0304 = q\u0304n+3:1 = q\u0304n+3:2 \u25e6 (\u03c3 \u25e6 \u2212Id/a) and g\u0304 = F\u22121(q\u0304n+2:2). By Lem. 26, there is f\u0304n+1:1 \u25e6 q\u0304n+3:1 such that:\n\u2022 f\u0304n:1 \u25e6 q\u0304n+3:2 \u2208 S \u2032(fn:1 \u25e6 qn+3:2).\n\u2022 C(f\u0304n+1:1 \u25e6 q\u0304n+3:1) = 2n+ 2.\n\u2022 q\u03041 = q1 = (\u03c3 \u25e6 \u2212Id/a).\n\u2022 f\u0304n = fn.\nFinally, by Lem. 12,\n\u2200j \u2208 [n+ 1] : q\u0304n\u2212j+3:1 \u25e6 g\u0304 \u25e6DA = (\u2212\u03c3\u22121) \u25e6 g\u0304j:1 \u25e6DA (240)\nTherefore, we found f\u0304n+1:1 \u25e6 q\u0304n+3:1 with all the desired properties.\nLemma 37 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Let f D\u223c m+2, g. Then, for every minimal decomposition f = f \u2032n+1:1 there is a minimal decomposition g = g \u2032 n+1:1 such that:\n\u2200i \u2208 [n] : f \u2032i+1:1 \u25e6D \u223c m, g\u2032i:1 \u25e6D (241)\nProof Since f D\u223c m, g there are minimal decompositions f = fn+1:1 and g = gn+1:1 such that:\n\u2200i \u2208 [n] : fi+1:1 \u25e6D \u223c m, gi+1:1 \u25e6D (242)\nBy minimal identifiability, f \u20321 = \u03c01 \u25e6 f1, for all i = 2, ..., n \u2212 1: f \u2032i = \u03c0i \u25e6 fi \u25e6 \u03c0\u22121i\u22121 and f \u2032n = fn \u25e6 \u03c0\u22121n\u22121. Therefore, we define a minimal decomposition for g as follows: g = g\u2032n+1:1 such that g \u2032 1 = \u03c01 \u25e6 g1, for all i = 2, ..., n\u2212 1: g\u2032i = \u03c0i \u25e6 gi \u25e6 \u03c0\u22121i\u22121 and g\u2032n = gn \u25e6 \u03c0\u22121n\u22121. This is a minimal decomposition of g, since each invariant function is an invertible linear mapping and commutes with \u03c3. By Lem. 7 we have:\nf \u2032i+1:1 = \u03c0i \u25e6 fi+1:1 and g\u2032i+1:1 = \u03c0i \u25e6 gi+1:1 (243)\nTherefore, discm(f \u2032n+1:1 \u25e6D, g\u2032n+1:1 \u25e6D) = discm(fn+1:1 \u25e6D, gn+1:1 \u25e6D) \u2264 (244)\nand,\n\u2200i \u2208 [n\u2212 1] : discm(f \u2032i+1:1 \u25e6D, g\u2032i+1:1 \u25e6D) = discm(\u03c0i \u25e6 fi+1:1 \u25e6D,\u03c0i \u25e6 gi+1:1 \u25e6D) (245)\nBy Lem. 1, since C(\u03c0i) = 2 we have:\n\u2200i \u2208 [n\u2212 1] : discm(f \u2032i+1:1 \u25e6D, g\u2032i+1:1 \u25e6D) = discm(\u03c0i \u25e6 fi+1:1 \u25e6D,\u03c0i \u25e6 gi+1:1 \u25e6D) \u2264 discm+2(fi+1:1 \u25e6D, gi+1:1 \u25e6D) \u2264\n(246)\nAlternatively, \u2200i \u2208 [n] : f \u2032i+1:1 \u25e6D \u223c\nm, g\u2032i+1:1 \u25e6D (247)\nLemma 38 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Let h = hn+1:1 be any function such that C(h) = n and q = qn+3:1 such that qn+2:1 = F(hn+1:1) \u25e6 (\u03c3 \u25e6\u2212Id/a) (F is the function in Eq. 173) andC(q) = n+2. Then, for any minimal decomposition q = q\u2032n+3:1 there is a minimal decomposition h = h\u2032n+1:1 such that:\n\u2200j \u2208 [n] \\ {1} :q\u2032n\u2212j+3:1 \u25e6 h \u25e6DA = (\u2212\u03c3\u22121) \u25e6 h\u2032j:1 \u25e6DA (248)\nProof Let q = q\u2032n+3:1 and h = hn+1:1 be minimal decompositions of q and h (resp.). By Lem. 12,\n\u2200j \u2208 [n+ 1] : qn\u2212j+3:1 \u25e6 h \u25e6DA = (\u2212\u03c3\u22121) \u25e6 hj:1 \u25e6DA (249)\nBy minimal identifiability, for decompositions q = q\u2032n+3:1 we have:\n\u2203\u03c01, ..., \u03c0n+1 \u2208 Invariant(N ) : q\u20321 = \u03c01 \u25e6 q1, \u2200j = 2, ..., n+ 1 : q\u2032j = \u03c0j \u25e6 qj \u25e6 \u03c0\u22121j\u22121 and q\u2032n+2 = qn+2 \u25e6 \u03c0\u22121n+1\n(250)\nBy Lem. 7, \u2200j \u2208 [n+ 1] : q\u2032n\u2212j+3:1 = \u03c0n\u2212j+2 \u25e6 qn\u2212j+3:1 (251)\nTherefore, for all j \u2208 [n+ 1] we have:\n(\u03c3 \u25e6 \u2212Id) \u25e6 q\u2032n\u2212j+3:1 \u25e6 h \u25e6DA = (\u03c3 \u25e6 \u2212Id) \u25e6 \u03c0n\u2212j+2 \u25e6 qn\u2212j+3:1 \u25e6 h \u25e6DA (252)\nWe define a decomposition h = h\u2032n+1:1 as follows:\nh\u20321 = \u03c0n \u25e6 h1, \u2200j = 2, ..., n\u2212 1 : h\u2032j = \u03c0n\u2212j+1 \u25e6 hj \u25e6 \u03c0\u22121n\u2212j+2 and h\u2032n = hn \u25e6 \u03c0\u221212 (253)\nIn particular, \u2200j \u2208 [n] \\ {1} : h\u2032j:1 = \u03c0n\u2212j+2 \u25e6 hj:1 (254)\nBut, \u03c0n\u2212j+2 commutes with both \u03c3 and \u2212Id and therefore, for all j \u2208 [n] such that j 6= 1 we have:\n(\u03c3 \u25e6 \u2212Id) \u25e6 q\u2032n\u2212j+3:1 \u25e6 h \u25e6DA = (\u03c3 \u25e6 \u2212Id) \u25e6 \u03c0n\u2212j+2 \u25e6 qn\u2212j+3:1 \u25e6 h \u25e6DA = \u03c0n\u2212j+2 \u25e6 (\u03c3 \u25e6 \u2212Id) \u25e6 qn\u2212j+3:1 \u25e6 h \u25e6DA = \u03c0n\u2212j+2 \u25e6 hj:1 \u25e6DA = h\u2032j:1 \u25e6DA\n(255)\nLemma 39 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Assume that:\nf DA 6\u223c k, 1 f \u2032, f\u0304 DA\u223c k+2, f and f\u0304 \u2032 DA\u223c k+2, f \u2032 (256)\nThen, f\u0304 DA 6\u223c\nk, 1\u22122 f\u0304 \u2032.\nProof Assume by contradiction that f\u0304 DA\u223c k, 1\u22122 f\u0304 \u2032. Then, there are decompositions f\u0304 = f\u0304n+1:1 and f\u0304 \u2032 = f\u0304 \u2032n+1:1 such that:\n\u2200j \u2208 [n] : disck(f\u0304j+1:1 \u25e6DA, f\u0304 \u2032j+1:1 \u25e6DA) \u2264 1 \u2212 2 (257)\nBy Lem. 37, since f\u0304 DA\u223c k+2, f and f\u0304 \u2032 DA\u223c k+2, f \u2032, there are minimal decompositions f = fn+1:1 and f \u2032 = f \u2032n+1:1 such that:\n\u2200j \u2208 [n] : disck(fj+1:1 \u25e6DA, f\u0304j+1:1 \u25e6DA), disck(f \u2032j+1:1 \u25e6DA, f\u0304 \u2032j+1:1 \u25e6DA) \u2264 (258)\nSince f DA 6\u223c k, 1 f \u2032, there is an index i \u2208 [n] such that:\ndisck(fi+1:1 \u25e6DA, f \u2032i+1:1 \u25e6DA) > 1 (259)\nTherefore, by the triangle inequality,\ndisck(fi+1:1 \u25e6DA, f \u2032i+1:1 \u25e6DA) \u2264disck(f\u0304i+1:1 \u25e6DA, f \u2032i+1:1 \u25e6DA) + disck(f\u0304i+1:1 \u25e6DA, fi+1:1 \u25e6DA) \u2264disck(f\u0304i+1:1 \u25e6DA, f\u0304 \u2032i+1:1 \u25e6DA) + disck(f\u0304i+1:1 \u25e6DA, fi+1:1 \u25e6DA)\n+ disck(f\u0304 \u2032i+1:1 \u25e6DA, f \u2032i+1:1 \u25e6DA) \u2264( 1 \u2212 2 ) + + = 1\n(260)\nin contradiction."}, {"heading": "H.3 Proof of Thm. 5", "text": "Theorem 5 (Counting semantic mappings) Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is a Leaky ReLU with parameter a > 0 and assume Assumption 1. Let 0, 1 and 2 < 1 \u2212 2 0 are three positive constants and A = (XA, DA) and B = (XB , DB) are two domains. Assume that m \u2265 k + 2C 0A,B + 5. Then,\nCovering ( H 0(DA, DB ;m),\nDA\u223c k, 1 ) \u2264 lim \u21920 \u221a Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) (19)\nProof We denote by any positive constant such that: < ( 1 \u2212 2 0 \u2212 2)/4 and t := k + C 0A,B + 3. We would like to find an embedding mapping:\nG : (H 0(DA, DB ;m)) 2 \u2192 DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (261)\nPart 1: In this part, we show how to constructG. Let (f, g) \u2208 (H 0(DA, DB ;m))2 and f = fn+1:1 and g = gn+1:1 are decompositions of f and g (resp.). Let F be the function from Eq. 173 and qn+3:1 = F+(gn+1:1). Then, by Lem. 36 there are f\u0304 = f\u0304n+1:1, g\u0304 = g\u0304n+1:1 and q\u0304 = q\u0304n+3:1 such that:\n\u2022 q\u0304n+2:1 = F(g\u0304n+1:1) \u25e6 (\u03c3 \u25e6 \u2212Id/a).\n\u2022 C(f\u0304n+1:1 \u25e6 q\u0304n+3:1) = 2n+ 2.\n\u2022 \u2200j \u2208 [n+ 1] : q\u0304n\u2212j+3:1 \u25e6 g\u0304 \u25e6DA = (\u2212\u03c3\u22121) \u25e6 g\u0304j:1 \u25e6DA.\n\u2022 disct(q\u0304 \u25e6DB , DA) \u2264 0 + .\n\u2022 \u2200j \u2208 [n] : discm(f\u0304j+1:1 \u25e6DA, fj+1:1 \u25e6DA) \u2264 .\n\u2022 \u2200j \u2208 [n] : discm(g\u0304j+1:1 \u25e6DA, gj+1:1 \u25e6DA) \u2264 .\nWe define: G(f, g) = f\u0304n+1:1 \u25e6 q\u0304n+3:1. Part 2: In this part, we show that:\n(f, g) \u2208 (H 0(DA, DB ;m))2 =\u21d2 G(f, g) \u2208 DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (262)\nBy Part 1, C(f\u0304n+1:1 \u25e6 q\u0304n+3:1) = 2n+ 2 = 2C 0A,B + 2. In addition, by the first item of Lem. 2, for D1 :\u2190 q\u0304 \u25e6DB , D2 :\u2190 DA, D3 :\u2190 DB , p :\u2190 f\u0304 , t \u2265 k + C 0A,B we have: disck(f\u0304 \u25e6 q\u0304 \u25e6DB , DB) \u2264 disct(f\u0304 \u25e6DA, DB) + disct(q\u0304 \u25e6DB , DA) (263) Since f \u2208 H 0(DA, DB ;m):\ndisct(f\u0304 \u25e6DA, DB) \u2264 discm(f \u25e6DA, DB) + discm(f\u0304 \u25e6DA, f \u25e6DA) \u2264 0 + (264)\nFinally, disck(f\u0304 \u25e6 q\u0304 \u25e6DB , DB) \u2264 2( 0 + ) (265)\nWe conclude that G(f, g) \u2208 DPM2( 0+ ) ( k, 2C 0A,B + 2 ) (266)\nPart 3: In this part, we show that G is an embedding. It requires showing that\nG(f, g) DB\u223c m, 2 G(f \u2032, g\u2032) =\u21d2 (f, g) ( DA\u223c k, 1 )2 (f \u2032, g\u2032) (267)\nAssume by contradiction that G(f, g) DB\u223c m, 2 G(f \u2032, g\u2032) and that (f, g) DA 6\u223c k, 1 (f \u2032, g\u2032). Then, we have\nf DA 6\u223c k, 1 f \u2032 or g DA 6\u223c k, 1 g\u2032 (268)\nWe denote G(f, g) = f\u0304 \u25e6 q\u0304 and G(f \u2032, g\u2032) = f\u0304 \u2032 \u25e6 q\u0304\u2032 (see Part 1).\nAssume that f DA 6\u223c k, 1 f \u2032: By Lem. 39, f\u0304 DA 6\u223c k, 1\u22122 f\u0304 \u2032. In particular, for every f\u0304 = f\u0304n+1:1 and f\u0304 \u2032 = f\u0304 \u2032n+1:1, there is an index i \u2208 [n] such that:\ndisck(f\u0304i+1:1 \u25e6DA, f\u0304 \u2032i+1:1 \u25e6DA) > 1 \u2212 2 (269)\nAs we showed in Part 2,\ndisct(q\u0304 \u25e6DB , DA), disct(q\u0304\u2032 \u25e6DB , DA) \u2264 0 + (270)\nBy the first item of Lem. 2, forD1 :\u2190 DA,D2 :\u2190 q\u0304\u25e6DB ,D3 :\u2190 f\u0304 \u2032i+1:1\u25e6DA, t \u2265 k+C 0A,B \u2265 k+C(f\u0304i+1:1), we have:\ndisck(f\u0304i+1:1 \u25e6DA, f\u0304 \u2032i+1:1 \u25e6DA) \u2264 disct(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6DA) + disct(q\u0304 \u25e6DB , DA) \u2264 disct(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6DA) + 0 +\n(271)\nAgain, by the first item of Lem. 2, for D1 :\u2190 DA, D2 :\u2190 q\u0304\u2032 \u25e6DB , D3 :\u2190 fi+1:1 \u25e6 q\u0304 \u25e6DB , m \u2265 t+C 0A,B \u2265 t+ C(f\u0304 \u2032i+1:1), we have:\ndisct(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6DA) \u2264discm(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6 q\u0304\u2032 \u25e6DB) + discm(q\u0304\u2032 \u25e6DB , DA) \u2264discm(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6 q\u0304\u2032 \u25e6DB) + 0 +\n(272)\nTherefore, we conclude that:\n1 \u2212 2 0 \u2212 4 < discm(f\u0304i+1:1 \u25e6 q\u0304 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6 q\u0304\u2032 \u25e6DB) (273)\nAlternatively, for any minimal decompositions f\u0304 \u25e6 q\u0304 = f\u0304n+1:1 \u25e6 q\u0304n+3:1 and f\u0304 \u2032 \u25e6 q\u0304\u2032 = f\u0304 \u2032n+1:1 \u25e6 q\u0304\u2032n+3:1 there are right partial functions f\u0304i+1:1 \u25e6 q\u0304n+3:1 and f\u0304 \u2032i+1:1 \u25e6 q\u0304\u2032n+3:1 such that:\n2 \u2264 1 \u2212 2 0 \u2212 4 < discm(f\u0304i+1:1 \u25e6 q\u0304n+3:1 \u25e6DB , f\u0304 \u2032i+1:1 \u25e6 q\u0304\u2032n+3:1 \u25e6DB) (274)\nin contradiction to F (f, g) DB\u223c m, 2 F (f \u2032, g\u2032).\nAssume that g DA 6\u223c k, 1 g\u2032: Let q\u0304 = q\u0304n+3:1 and q\u0304\u2032 = q\u0304\u2032n+3:1 be any two minimal decompositions of q\u0304 and q\u0304\u2032 (resp.). Then, by Lem. 38, there are minimal decompositions g\u0304 = g\u0304n+1:1 and g\u0304\u2032 = g\u0304\u2032n+1:1 such that:\n\u2200j \u2208 [n] \\ {1} :q\u0304n\u2212j+3:1 \u25e6 g\u0304 \u25e6DA = (\u2212\u03c3\u22121) \u25e6 g\u0304j:1 \u25e6DA and:q\u0304\u2032n\u2212j+3:1 \u25e6 g\u0304\u2032 \u25e6DA = (\u2212\u03c3\u22121) \u25e6 g\u0304\u2032j:1 \u25e6DA\n(275)\nBy Lem. 39, since g\u0304 DA\u223c m, g and g\u0304\u2032 DA\u223c m,\ng\u2032, we have: g\u0304 DA 6\u223c\nk, 1\u22122 g\u0304\u2032. In particular, there is an index i \u2208 [n+ 1] such\nthat: disck(g\u0304i:1 \u25e6DA, g\u0304\u2032i:1 \u25e6DA) > 1 \u2212 2 (276)\nand, disck(g\u03041:1 \u25e6DA, g\u0304\u20321:1 \u25e6DA) = 0 \u2264 1 \u2212 2 (277)\nand,\ndisck(g\u0304n+1:1 \u25e6DA, g\u0304\u2032n+1:1 \u25e6DA) = disck(g\u0304 \u25e6DA, g\u0304\u2032 \u25e6DA) \u2264 disck(g\u0304 \u25e6DA, DB) + disck(DB , g\u0304\u2032 \u25e6DA) \u2264 disck(g \u25e6DA, DB) + disck(DB , g\u2032 \u25e6DA) + disck(g\u0304 \u25e6DA, g \u25e6DA) + disck(g\u2032 \u25e6DA, g\u0304\u2032 \u25e6DA) \u2264 2( 0 + ) \u2264 1 \u2212 2 (278)\nTherefore, i 6= n+ 1, 1. Thus, i \u2208 [n] \\ {1} and: disck+1((\u2212\u03c3\u22121) \u25e6 g\u0304i:1 \u25e6DA, (\u2212\u03c3\u22121) \u25e6 g\u0304\u2032i:1 \u25e6DA)\n=disck+1(q\u0304n\u2212i+3:1 \u25e6 g\u0304 \u25e6DA, q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) (279)\nBy Lem. 1, for p :\u2190 (\u03c3 \u25e6 \u2212Id) of complexity 1 we have: 1 \u2212 2 < disck(g\u0304i:1 \u25e6DA, g\u0304\u2032i:1 \u25e6DA)\n\u2264 disck+1((\u2212\u03c3\u22121) \u25e6 g\u0304i:1 \u25e6DA, (\u2212\u03c3\u22121) \u25e6 g\u0304\u2032i:1 \u25e6DA) (280)\nIn addition, by Lem. 2, forD1 :\u2190 g\u0304\u25e6DA,D2 :\u2190 DB ,D3 :\u2190 q\u0304\u2032n\u2212i+3:1\u25e6 g\u0304\u2032\u25e6DA, t \u2265 (k+1)+(C 0A,B+2) \u2265 (k + 1) + C(q\u0304n\u2212i+3:1), we have:\ndisck+1(q\u0304n\u2212i+3:1 \u25e6 g\u0304 \u25e6DA, q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) \u2264 disct(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) + disct(g\u0304 \u25e6DA, DB) \u2264 disct(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) + disct(g \u25e6DA, DB) + disct(g\u0304 \u25e6DA, g \u25e6DA) \u2264 disct(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) + 0 +\n(281)\nAgain, by Lem. 2, for D1 :\u2190 g\u0304\u2032 \u25e6 DA, D2 :\u2190 DB , D3 :\u2190 q\u0304n\u2212i+3:1 \u25e6 DB , m \u2265 t + (C 0A,B + 2) \u2265 t+ C(q\u0304\u2032n\u2212i+3:1), we have:\ndisct(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) \u2264 discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) + discm(g\u0304\u2032 \u25e6DA, DB) \u2264 discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) + discm(g\u2032 \u25e6DA, DB) + discm(g\u0304\u2032 \u25e6DA, g\u2032 \u25e6DA) \u2264 discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) + 0 +\n(282)\nFinally, 1 \u2212 2 < disck(g\u0304i:1 \u25e6DA, g\u0304\u2032i:1 \u25e6DA)\n\u2264 disct(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6 g\u0304\u2032 \u25e6DA) + 0 + \u2264 discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) + 2( 0 + )\n(283)\nIn particular, 2 \u2264 1 \u2212 2 0 \u2212 4 < discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) (284) Alternatively, for any minimal decompositions f\u0304 \u25e6 q\u0304 = f\u0304n+1:1 \u25e6 q\u0304n+3:1 and f\u0304 \u2032 \u25e6 q\u0304\u2032 = f\u0304 \u2032n+1:1 \u25e6 q\u0304\u2032n+3:1 there are right partial functions q\u0304n\u2212i+3:1 and q\u0304\u2032n\u2212i+3:1 such that:\n2 \u2264 1 \u2212 2 0 \u2212 4 < discm(q\u0304n\u2212i+3:1 \u25e6DB , q\u0304\u2032n\u2212i+3:1 \u25e6DB) (285)\nin contradiction to F (f, g) DB\u223c m, 2 F (f \u2032, g\u2032).\nPart 3: Finally, by Lem. 32 and Lem. 30,\nCovering ( H 0(DA, DB ;m),\nDA\u223c k, 1\n)2 = Covering ( (H 0(DA, DB ;m)) 2, ( DA\u223c k, 1 )2)\n\u2264 Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) (286)\nAlternatively, for all 0, 1, 2, such that < ( 1 \u2212 2 0 \u2212 2)/4,\nCovering ( H 0(DA, DB ;m),\nDA\u223c k, 1\n)2 \u2264 Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) (287)\nBy Lem. 33, the function Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) is monotonically decreasing as tends to 0 and is lower bounded by Covering ( DPM2 0 ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) . Thus, the limit\nlim \u21920\nCovering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 ) (288)\nexists and\nCovering ( H 0(DA, DB ;m),\nDA\u223c k, 1\n) \u2264 \u221a\nlim \u21920\nCovering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 )\n= lim \u21920\n\u221a Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 )\n= lim \u21920\n\u221a Covering ( DPM2( 0+ ) ( k, 2C 0A,B + 2 ) , DB\u223c m, 2 )\n(289)\nAppendix I. Proof of Thms. 6 and 7 Lemma 40 Let N = SCM[C], 0, 1, 2 > 0 are three constants, A = (XA, DA) and B = (XB , DB) are two domains and y \u2208 H 0(DA, DB ;m) such that y = yn+1:1 is a minimal decomposition of y. Assume that k \u2265 max { E 0A,B , E 2 0+2 1+2 2 A,B } , m \u2265 k + 2C 0A,B and that C 0A,B = C 0+2 1+2 2A,B . Let DI and DJ be two distributions such that:\ndiscm(yi+1:1 \u25e6DA, DI) \u2264 1 and discm(yj+1:1 \u25e6DA, DJ) \u2264 2 (290)\nThen, yj+1:i+1 \u2208 H 1+ 2(DI , DJ ; k).\nProof Assume by contradiction that yj+1:i+1 /\u2208 H 1+ 2(DI , DJ ; k). Then, exactly one of the following options hold:\n\u2022 There is a function u such that C(u) < C(yj+1:i+1) = j \u2212 i and disck(u \u25e6DI , DJ) \u2264 1 + 2.\n\u2022 disck(yj+1:i+1 \u25e6DI , DJ) > 1 + 2.\nIf the second option holds, then, by the triangle inequality,\n1 + 2 < disck(yj+1:i+1 \u25e6DI , DJ) \u2264 disck(yj+1:i+1 \u25e6DI , yj+1:1 \u25e6DA) + disck(DJ , yj+1:1 \u25e6DA) \u2264 disck(yj+1:i+1 \u25e6DI , yj+1:1 \u25e6DA) + 2\n(291) In addition, by the first part of Lem. 2, forD1 :\u2190 DI ,D2 :\u2190 yi+1:1\u25e6DA,D3 :\u2190 yj+1:1\u25e6DA, p :\u2190 yj+1:i+1 and m \u2265 k + j \u2212 i\ndisck(yj+1:i+1 \u25e6DI , yj+1:1 \u25e6DA) \u2264discm(yj+1:1 \u25e6DA, yj+1:1 \u25e6DA) + discm(yi+1:1 \u25e6DA, DI) \u2264 1\n(292)\nTherefore, 1 + 2 < disck(yj+1:i+1 \u25e6DI , DJ) \u2264 1 + 2 in contradiction. Thus, we conclude that the first option must hold. We denote t = k + C 0A,B . We note that since m \u2265 t \u2265 k \u2265 E 0A,B , by Lem. 18, we have:\ny \u2208 H 0(DA, DB ;m) \u2282 H 0(DA, DB ; t) (293)\nBy the triangle inequality,\ndisct(u \u25e6 yi+1:1 \u25e6DA, yj+1:1 \u25e6DA) \u2264disct(yj+1:1 \u25e6DA, DJ) + disct(u \u25e6 yi+1:1 \u25e6DA, DJ) \u2264disct(u \u25e6 yi+1:1 \u25e6DA, DJ) + 2\n(294)\nBy the first item of Lem. 2; for D1 :\u2190 yi+1:1 \u25e6DA, D2 :\u2190 DI , D3 :\u2190 DJ , p :\u2190 u, and m \u2265 t+ C 0A,B \u2265 t+ C(u), we have:\ndisct(u \u25e6 yi+1:1 \u25e6DA, DJ) \u2264 discm(u \u25e6DI , DJ) + discm(yi+1:1 \u25e6DA, DI) \u2264 2 1 + 2 (295)\nTherefore, disct(u \u25e6 yi+1:1 \u25e6DA, yj+1:1 \u25e6DA) \u2264 2( 1 + 2) (296)\nBy the first item of Lem. 2; for D1 :\u2190 u \u25e6 yi+1:1 \u25e6DA, D2 :\u2190 yj+1:1 \u25e6DA, D3 :\u2190 DB , p :\u2190 yn+1:j+1 and t = k + C 0A,B \u2265 k + C(yn+1:j+1), we have:\ndisck(yn+1:j+1 \u25e6 u \u25e6 yi+1:1 \u25e6DA, DB) \u2264disct(yn+1:j+1 \u25e6 yj+1:1 \u25e6DA, DB) + disct(u \u25e6 yi+1:1 \u25e6DA, yj+1:1 \u25e6DA)\n\u2264disct(y \u25e6DA, DB) + 2( 1 + 2) \u2264 0 + 2( 1 + 2) (297)\nOn the other hand,\nC(yn+1:j+1 \u25e6 u \u25e6 yi+1:1) \u2264 n\u2212 j + C(u) + i < n\u2212 j + (j \u2212 i) + i = n = C(y) (298)\nThus, we found a function g = yn+1:j+1 \u25e6 u \u25e6 yi+1:1 such that C(g) < C(y) and disck(g \u25e6 DA, DB) \u2264 0 + 2( 1 + 2). Since k \u2265 E 0+2 1+2 2A,B we have:\nC 0+2 1+2 2A,B = C k, 0+2 1+2 2 A,B \u2264 C(g) < C(y) = C 0A,B (299)\nin contradiction to C 0+2 1+2 2A,B = C 0 A,B ."}, {"heading": "Proof of Thm. 6", "text": "Theorem 6 Let N = SCM[\u03c3] with \u03c3 that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ 6= DA, DB is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let k \u2265 max { E 0A,B , E 4 0+4 1 B,A , E 4 0 B,A + C 0+ 1 A,Z + 1 } and m \u2265 k+3C 0+ 1A,B +4. Assume thatC3 0+ 1B,A = C 0+3 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B +2. Then, (\u2212\u03c3\u22121)\u25e6DZ is a (k, 1, 0, 0+ 1)-shared semantic distribution between B and A.\nProof Let yAB \u2208 Z(DA, DZ , DB ;m, 0, 1) \u2229 H 0+ 1(DA, DB ;m). Assume that yAB = pn+1:1 is a minimal decomposition of yAB , such that discm(pi+1:1 \u25e6DA, DZ) \u2264 0 and discm(DB , pn+1:i+1 \u25e6DZ) \u2264 1 for some i \u2208 {1, ..., n\u2212 1}. We denote yBA = y\u22121AB , t := m\u2212 C 0+ 1A,B \u2212 2. Part 1: In this part, we show that\nyBA \u2208 H 0+ 1(DB , DA; t) \u2282 H 0+ 1(DB , DA; k) (300)\nBy the third item of Lem. 2, for: D1 :\u2190 DA,D2 :\u2190 DB , h :\u2190 yAB , h\u22121 :\u2190 yBA andm = t+C 0+ 1A,B +2 \u2265 t+ C(yBA), we have:\ndisct(DA, yBA \u25e6DB) \u2264 discm(yAB \u25e6DA, DB) \u2264 0 + 1 (301)\nSince m \u2265 t \u2265 k \u2265 E 0+ 1B,A , we have:\nCt, 0+ 1B,A = C k, 0+ 1 B,A = C 0+ 1 B,A = C 0+ 1 A,B + 2 = C m, 0+ 1 A,B + 2 = n+ 2 (302)\nTherefore, C(yAB) + 2 = C m, 0+ 1 A,B + 2 \u2264 C(yBA) and by Thm. 1, C(yBA) = C(yAB) + 2. In particular,\nyBA \u2208 H 0+ 1(DB , DA; t) \u2282 H 0+ 1(DB , DA; k) (303)\nIn addition, since C(yBA) = C(yAB) + 2, the following is a minimal decomposition of yBA:\nyBA = F+(pn+1:1) (304)\nWhere, F+ is the function defined in Eq. 173 (see the proof of Thm. 1).\nPart 2: In this part, we show that:\ndisct((\u2212\u03c3\u22121) \u25e6DZ , [F+(pn+1:1)]n\u2212i+2:1 \u25e6DB) \u2264 1 and disct(DA, [F+(pn+1:1)]n+3:n\u2212i+2 \u25e6 (\u2212\u03c3\u22121) \u25e6DZ) \u2264 0\n(305)\nBy Eq. 64, for i \u2208 [n], [F+(pn+1:1)]n\u2212i+2:1 = (\u2212\u03c3\u22121) \u25e6 p\u22121n+1:i+1 (306)\nIn particular, for i \u2208 [n], [F+(pn+1:1)]n+3:n\u2212i+2 \u25e6 (\u2212\u03c3\u22121) = p\u22121i+1:1 (307)\nBy Lem. 12, for i \u2208 [n],\n[F+(pn+1:1)]n\u2212i+2:1 \u25e6 yAB \u25e6DA = (\u2212\u03c3\u22121) \u25e6 pi+1:1 \u25e6DA (308)\nBy Lem. 1, for p :\u2190 (\u2212\u03c3\u22121) of complexity 1,\ndisct ( (\u2212\u03c3\u22121) \u25e6DZ , [F+(pn+1:1)]n\u2212i+2:1 \u25e6DB ) = disct ( (\u2212\u03c3\u22121) \u25e6DZ , (\u2212\u03c3\u22121) \u25e6 p\u22121n+1:i+1 \u25e6DB )\n\u2264 disct+1 ( DZ , p \u22121 n+1:i+1 \u25e6DB )\n(309) By the third item of Lem. 1, for h :\u2190 pn+1:i+1 and m = (t+ 1) + C 0+ 1A,B + 2 = (t+ 1) + C(yAB) + 2 = (t+ 1) + C(yBA) \u2265 (t+ 1) + C(p\u22121n+1:i+1) and by Eq. 307 we have:\ndisct ( (\u2212\u03c3\u22121) \u25e6DZ , [F+(pn+1:1)]n\u2212i+2:1 \u25e6DB ) \u2264disct+1 ( DZ , p \u22121 n+1:i+1 \u25e6DB )\n\u2264discm (pn+1:i+1 \u25e6DZ , DB) \u2264 1 (310)\nAgain, by Eq. 307 and by the third item of Lem. 1, for h :\u2190 pi+1:1 andm \u2265 t+C 0+ 1A,B +2 \u2265 t+C(yAB)+2 \u2265 t+ C(p\u22121i+1:1) we have:\ndisct ( [F+(pn+1:1)]n+3:n\u2212i+2 \u25e6 (\u2212\u03c3\u22121) \u25e6DZ , DA ) =disct ( (pi+1:1) \u22121 \u25e6DZ , DA )\n\u2264discm (DZ , pi+1:1 \u25e6DB) \u2264 0 (311)\nPart 3: In this part, we show that:\n[F+(pn+1:1)]n\u2212i+2:1 \u2208 H 1(DB , (\u2212\u03c3\u22121) \u25e6DZ ; k) and [F+(pn+1:1)]n+3:n\u2212i+2 \u2208 H 0((\u2212\u03c3\u22121) \u25e6DZ , DA; k)\n(312)\nThe following conditions hold:\n\u2022 t \u2265 k + 2C 0+ 1A,B + 4 = k + 2C 0+ 1B,A . \u2022 k \u2265 max { E 0+ 1B,A , E 4 0+4 1 B,A } .\n\u2022 C 0+ 1B,A = C3 0+ 1B,A = C 0+3 1B,A .\n\u2022 disct([F+(pn+1:1)]n\u2212i+2:1 \u25e6DB , (\u2212\u03c3\u22121) \u25e6DZ) \u2264 1.\n\u2022 disct([F+(pn+1:1)]n+3:n\u2212i+2 \u25e6 (\u2212\u03c3\u22121) \u25e6DZ , DA) \u2264 0.\n\u2022 yBA = F+(pn+1:1) \u2208 H 0+ 1(DB , DA; t).\nTherefore, by Lem. 40,\n[F+(pn+1:1)]n\u2212i+2:1 \u2208 H 1(DB , (\u2212\u03c3\u22121) \u25e6DZ ; k) (313)\nand [F+(pn+1:1)]n+3:n\u2212i+2 \u2208 H 0((\u2212\u03c3\u22121) \u25e6DZ , DA; k) (314)\nWe conclude that: Z(DB , (\u2212\u03c3\u22121) \u25e6DZ , DA; k, 1, 0) \u2229H 0+ 1(DB , DA; k) (315)\nFinally, let f \u2208 H 1(DB , (\u2212\u03c3\u22121) \u25e6 DZ ; k) and g \u2208 H 0((\u2212\u03c3\u22121) \u25e6 DZ , DB ; k) and s = k \u2212 C 0+ 1Z,B \u2212 1, then:\ndiscs(g \u25e6 f \u25e6DB , DA) \u2264disck(f \u25e6DA, (\u2212\u03c3\u22121) \u25e6DZ) + disck(g \u25e6 (\u2212\u03c3\u22121) \u25e6DZ , DA) \u2264 0 + 1\n(316)\nSince s \u2265 E 0+ 1B,A , we have: C(g \u25e6 f) \u2265 C 0+ 1B,A . Therefore,\nC 0+ 1B,A \u2264 C(g \u25e6 f) \u2264 C(g) + C(f) \u2264 C([F+(pn+1:1)]n+3:n\u2212i+2) + C([F+(pn+1:1)]n\u2212i+2:1) = C(yBA) \u2264 Ck, 0+ 1B,A = C 0+ 1B,A (317)\nIn particular, g 7 f . Alternatively, (\u2212\u03c3\u22121) \u25e6DZ is a (k, 1, 0, 0 + 1)-shared semantic distribution between B and A."}, {"heading": "Proof of Thm. 7", "text": "Definition 15 Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution. We say that DZ is a (m, 0, 1)-shared irreducible distribution between A and B, if for all yB \u2208 H 1(DZ , DB ;m) and y\u22121A \u2208 H 0(DA, DZ ;m) we have: yB 7 y\u22121A . If 0 = 1 we write (m, 0) for short.\nLemma 41 (Alignment) Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains and DZ a distribution. Let k \u2265 max { E 0A,Z , E 0 Z,B } and m \u2265 k + C 0Z,B . Let DZ is a (m, 0)-shared irreducible distribution between A and B. Let h be a function that satisfies:\n\u2022 C(h) \u2264 C 0A,Z + C 0Z,B .\n\u2022 discm(h \u25e6DA, DB) \u2264 1."}, {"heading": "Then, one of the following holds:", "text": "\u2022 h \u2208 H 0+ 1(DZ , DB ; k,C 0Z,B) \u25e6H 0(DA, DZ ;m).\n\u2022 For every y \u2208 Z(DA, DZ , DB ;m, 0) we have: C(h||y) \u2265 2C 0Z,B \u2212 9.\nProof We denote i = C 0A,Z + C 0 Z,B , j = 2C 0 Z,B \u2212 9 and:\nH(DA, DB ; i, j, 0, 1) := { h \u2223\u2223\u2223\u2203y \u2208 Z(DA, DZ , DB ;m, 0)\ndiscm(h \u25e6DA, DB) \u2264 1, C(h) \u2264 i and C(h||y) < j } (318)\nWe would like to show that if C(h) \u2264 C 0A,Z + C 0Z,B and discm(h \u25e6DA, DB) \u2264 1, then exactly one of the following holds:\n\u2022 h \u2208 H 0+ 1(DZ , DB ; k,C 0Z,B) \u25e6H 0(DZ , DB ;m).\n\u2022 h /\u2208 H(DA, DB ; i, j, 0, 1).\nIf the second option holds; then, for every y \u2208 Z(DA, DZ , DB ;m, 0) we have C(h||y) = C ( h \u25e6 y\u22121 ) \u2265 2C 0Z,B \u2212 9. Therefore, we assume that h \u2208 H(DA, DB ; i, j, 0, 1) and prove that the first option holds. We denote by yAB \u2208 Z(DA, DZ , DB ;m, 0) the function, y, that corresponds to h (see Eq. 318). In addition, we denote y\u22121A \u2208 H 0(DA, DZ ;m) and yB \u2208 H 0(DZ , DB ;m) such that yAB = yB \u25e6 y\u22121A and \u03a0 := h \u25e6 y\u22121AB . Since DZ is a (m, 0)-shared irreducible distribution, yB 7 y\u22121A .\nPart 1: We would like to prove that C(\u03a0 \u25e6 yB) \u2264 C(yB). By Thm. 16, there are decompositions \u03a0 = a \u25e6 b and yB = b\u22121 \u25e6 c such that: b\u22121 7 c, a 7 c and C(\u03a0) \u2265 C(a) + C(b)\u2212 I \u2212 1. Case 1: Assume that C(c) \u2265 I . By Thm. 4, for f :\u2190 a, g :\u2190 c, h :\u2190 y\u22121A , we have,\nC(\u03a0 \u25e6 yAB) = C(a) + C(c) + C(y\u22121A ) = C(a \u25e6 c) + C(y\u22121A ) = C(\u03a0 \u25e6 yB) + C(y\u22121A )\n(319)\nIn particular,\nC(\u03a0 \u25e6 yAB)\u2212 C(yAB) = C(\u03a0 \u25e6 yAB)\u2212 C(yAB) \u2265 C(\u03a0 \u25e6 yB)\u2212 C(yB) (320)\nBut, C(h) \u2264 C 0A,Z + C 0Z,B = C(yAB) and therefore, C(\u03a0 \u25e6 yB) \u2264 C(yB). Case 2: Assume that C(c) < I . If C(yB) < C(\u03a0 \u25e6 yB) then we have: \u03a0 \u25e6 yB = a \u25e6 c such that C(a \u25e6 c) = C(a) + C(c) > C(yB) and since C(c) < I we have:\nC(a) \u2265 C(yB) + 1\u2212 C(c) \u2265 C(yB) + 1\u2212 (I \u2212 1) = C(yB)\u2212 I + 2 (321)\nIn addition, C(\u03a0) \u2265 C(a) + C(b)\u2212 I \u2212 1 \u2265 C(yB) + C(b)\u2212 2I + 1 (322) Since C(yB) = C(b\u22121) + C(c), by Thm. 1, we have:\nC(b) \u2265 C(b\u22121)\u2212 2 = C(yB)\u2212 C(c)\u2212 2 \u2265 C(yB)\u2212 I \u2212 1 (323)\nWe conclude that: C(\u03a0) \u2265 2C(yB)\u2212 3I (324)\nSince \u03c3 is a Leaky ReLU with parameter a > 0, we have, I \u2264 3, and, therefore, we can replace every instance of I in the proof with 3. In contradiction to C(\u03a0) < 2C(yB)\u2212 9. In particular, C(\u03a0 \u25e6 yB) \u2264 C(yB). Part 2: In this part, we show that:\ndisck(\u03a0 \u25e6 yB \u25e6DZ , DB) \u2264 0 + 1 (325)\nBy the first item of Lem. 2; forD1 :\u2190 DZ ,D2 :\u2190 y\u22121A \u25e6DA,D3 :\u2190 DB , p :\u2190 \u03a0\u25e6yB andm \u2265 k+C 0Z,B \u2265 k + C(yB) \u2265 k + C(\u03a0 \u25e6 yB), we have:\ndisck(\u03a0 \u25e6 yB \u25e6DZ , DB) \u2264 discm(y\u22121A \u25e6DZ , DB) + discm(\u03a0 \u25e6 yB \u25e6 y\u22121A \u25e6DA, DB) = discm(y\u22121A \u25e6DZ , DB) + discm(h \u25e6DA, DB) \u2264 0 + 1\n(326)\nPart 3: As we mentioned earlier, one can represent h = \u03a0 \u25e6 yAB = (\u03a0 \u25e6 yB) \u25e6 y\u22121A . In Parts 1 and 2, we showed that \u03a0 \u25e6 yB \u2208 H 0+ 1(DZ , DB ; k,C 0Z,B) (327) since C(\u03a0 \u25e6 yB) \u2264 C(yB) = C 0Z,B and disck(\u03a0 \u25e6 yB \u25e6 DZ , DB) \u2264 0 + 1. In addition, by definition, y\u22121A \u2208 H 0(DA, DZ ;m). Therefore, we conclude that:\nh \u2208 H 0+ 1(DZ , DB ; k,C 0Z,B) \u25e6H 0(DA, DZ ;m) (328)\nLemma 42 LetN = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0,A = (XA, DA) andB = (XB , DB) are two domains andDZ is a distribution. m \u2265 max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } and assume that DZ is a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B. Let f \u2208 H 0(DA, DZ ;m) and h = g \u25e6 f is a function such that:\n\u2022 discm(h \u25e6DA, DB) \u2264 0 + 1. \u2022 C(g) \u2264 C 1Z,B ."}, {"heading": "Then, g 7 f .", "text": "Proof Since m \u2265 max { E 0+ 1A,B , E 0 A,Z , E 1 Z,B } and DZ being a (m, 0, 1, 0 + 1)-shared semantic distribution between A and B, by Lem. 19, we have:\nC 1Z,B + C 0 A,Z = C 0+ 1 A,B (329)\nOn the other hand, we assumed that C(g) \u2264 C 1Z,B and that f \u2208 H 0(DA, DZ ;m). In particular,\nC(h) \u2264 C(g) + C(f) \u2264 C 1Z,B + C 0A,Z (330)\nBy the definition of Cm, 0+ 1A,B as the minimal complexity of a function h \u2032 satisfying discm(h\u2032 \u25e6DA, DB) \u2264\n0 + 1, we have: C 0+ 1A,B = C m, 0+ 1 A,B \u2264 C(h) (331)\nTherefore, C(h) = C(g) + C(f) and g 7 f .\nTheorem 7 (Alignment) Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0 and A = (XA, DA) and B = (XB , DB) are two domains. Let k \u2265 max { E2 0A,B , E 0 A,Z , E 0 Z,B } and m \u2265 k + C 0Z,B . Assume that DZ is a (m, 0)-shared semantic distribution between A and B such that C3 0Z,B = C 0 Z,B . Let h \u2208 H2 0(DA, DB ;m). Then, one of the following holds:\n\u2022 h = g \u25e6 f \u2208 Z(DA, DZ , DB ; k, 0, 3 0) such that g 7 f . \u2022 For every y \u2208 Z(DA, DZ , DB ;m, 0) we have: C(h||y) \u2265 2C 0Z,B \u2212 9.\nProof By Lem. 19, we have: C 0A,Z + C 0 Z,B = C 2 0 A,B (332) Therefore, since h \u2208 H2 0(DA, DB ;m) we obtain: \u2022 C(h) \u2264 C 0A,Z + C 0Z,B . \u2022 discm(h \u25e6DA, DB) \u2264 2 0.\nIn addition, DZ is a shared (m, 0)-semantic distribution between A and B. Thus, DZ is a shared (m, 0)- irreducible distribution between A and B. By Lem. 41 for 1 :\u2190 2 0, one of the following holds: \u2022 h \u2208 H3 0(DZ , DB ; k,C 0Z,B) \u25e6H 0(DA, DZ ;m). \u2022 For all y \u2208 Z(DA, DZ , DB ;m, 0) we have: C(h||y) \u2265 2C 0Z,B \u2212 9.\nIf the first option holds, then h = g \u25e6 f such that f \u2208 H 0(DA, DZ ;m) and C(g) \u2264 C 0Z,B . Therefore, by Lem. 42 for 1 :\u2190 0 we have: g 7 f . In addition, by Lem. 18 we have:\nH 0(DA, DZ ;m) \u2282 H 0(DA, DZ ; k) (333)\nand since C3 0Z,B = C 0 Z,B we can replace the first option by:\nh = g \u25e6 f \u2208 H3 0(DZ , DB ; k,C3 0Z,B) \u25e6H 0(DA, DZ ; k) = Z(DA, DZ , DB ; k, 0, 3 0) s.t g 7 f (334)"}, {"heading": "Appendix J. Irreducible distributions", "text": "If yAB = yB \u25e6 y\u22121A is the minimal mapping that passes through a distribution DZ as DA y\u22121A\u2212\u2192 DZ yB\u2212\u2192 DB , then one can replace DZ with a shared irreducible distribution DT , such that yAB = gB \u25e6 g\u22121A , DA g\u22121A\u2212\u2192 DT gB\u2212\u2192 DB and there is a function b such that yA = gA \u25e6 b, yB = gB \u25e6 b. For simplicity we assume that the discriminators are D\u221e. We will omit writing that the discriminators are of complexity \u2264 \u221e, i.e, H (D1, D2) := H (D1, D2;\u221e), Z(D1, D2; ) := Z(D1, D2;\u221e, ), etc\u2019. Lemma 43 Let N = SCM[\u03c3] be a NN-SCM with \u03c3 that is Leaky ReLU with parameter a > 0. Let A = (XA, DA) and B = (XB , DB) be two domains and DZ a distribution. Assume that C 0/2A,Z = C2 0A,Z and C 0Z,B = C 2 0 Z,B . Let yAB = yB \u25e6 y\u22121A \u2208 Z(DA, DZ , DB ; 0/2) such that:\nu \u2208 H2 0(DZ , DB ;C 0Z,B + 4) \u25e6H2 0(DA, DZ) we have: C(yAB) \u2264 C(u) (335) Then, there is a 0-shared irreducible distribution DT such that:\nyAB = gB \u25e6 g\u22121A \u2208 H2 0(DT , DB ;C 0T,B + 4) \u25e6H 0(DA, DT ) such that: gB 7 g\u22121A and \u2203b : yA = gA \u25e6 b, yB = gB \u25e6 b\n(336)\nProof We divide the proof into a few parts.\nPart 1: In this part, we find a distribution DT such that:\nyAB \u2208 H2 0(DT , DB ;C 0T,B + 4) \u25e6H 0(DA, DT ) (337)\nBy Thm. 16, there are decompositions, yB = gB \u25e6 b and y\u22121A = b\u22121 \u25e6 g\u22121A such that gB 7 g\u22121A , b\u22121 7 g\u22121A and C(gB) + C(b)\u2212 4 \u2264 C(gB \u25e6 b). We define DT := g\u22121A \u25e6DA. In particular,\ndisc\u221e(g\u22121A \u25e6DA, DT ) \u2264 0 (338)\nSince b\u22121 7 g\u22121A and C 0/2 A,Z = C 2 0 A,Z , we have:\nC(b\u22121) + C(g\u22121A ) = C(b \u22121 \u25e6 g\u22121A ) = C(y\u22121A ) \u2264 C 0/2 A,Z = C 2 0 A,Z (339)\nIn addition,\ndisc\u221e(b\u22121 \u25e6DT , DZ) = disc\u221e(b\u22121 \u25e6 g\u22121A \u25e6DA, DZ) = disc\u221e(y\u22121A \u25e6DA, DZ) \u2264 0 (340) Thus, C(b\u22121) \u2265 C 0T,Z and by Lem. 17, we have:\nC(b\u22121) + C(g\u22121A ) = C(y \u22121 A ) \u2264 C2 0A,Z \u2264 C 0A,T + C 0T,Z \u2264 C 0A,T + C(b\u22121) (341)\nIn particular, C(g\u22121A ) \u2264 C 0A,T . Therefore, g\u22121A \u2208 H 0(DA, DT ). Thus, C(g\u22121A ) = C 0A,T . Next, we would like to prove that gB \u2208 H 0(DT , DB ;C 0T,B + 4). We consider that:\ndisc\u221e(gB \u25e6DT , DB) = disc\u221e(gB \u25e6 g\u22121A \u25e6DA, DB) = disc\u221e(yAB \u25e6DA, DB) \u2264 0 (342) In particular, C(g\u22121A ) \u2265 C 0T,B . And by the third part of Lem. 2, for p :\u2190 b,\ndisc\u221e(b \u25e6DZ , DT ) \u2264 disc\u221e(DZ , b\u22121 \u25e6DT ) \u2264 0 (343) In particular, C(b) \u2265 C 0Z,T . By Lem. 17,\nC(gB) + C(b)\u2212 4 \u2264 C(gB \u25e6 b) = C(yB) \u2264 C 0Z,B = C2 0Z,B \u2264 C 0T,B + C 0Z,T \u2264 C 0T,B + C(b)\n(344)\nTherefore, C(gB) \u2264 C 0T,B + 4. In particular, yAB \u2208 H2 0(DT , DB ;C 0T,B + 4) \u25e6H 0(DA, DT ) (345)\nPart 2: We would like to prove that:\nZ(DA, DT , DB ; 0) \u2282 H2 0(DZ , DB ;C 0Z,B + 4) \u25e6H2 0(DA, DZ) (346)\nLet uAB = uB \u25e6 u\u22121A \u2208 Z(DA, DT , DB ; 0). Then, uAB = uB \u25e6 u\u22121A such that u\u22121A \u2208 H 0(DA, DT ) and uB \u2208 H 0(DT , DB). By the first item of Lem. 2, for D1 :\u2190 u\u22121A \u25e6DA, D2 :\u2190 DT , D3 :\u2190 DZ we have:\ndisc\u221e(b\u22121 \u25e6 u\u22121A \u25e6DA, DZ) \u2264 disc\u221e(b\u22121 \u25e6DT , DZ) + disc\u221e(DT , u\u22121A \u25e6DA) = disc\u221e(y\u22121A \u25e6DA, DZ) + disc\u221e(DT , u\u22121A \u25e6DA) \u2264 2 0\n(347)\nIn addition,\nC(b\u22121 \u25e6 u\u22121A ) \u2264 C(b\u22121) + C(u\u22121A ) = C(b\u22121) + C(g\u22121A ) = C(b\u22121 \u25e6 g\u22121A ) = C(y\u22121A ) (348) In particular, b\u22121 \u25e6 u\u22121A \u2208 H2 0(DA, DZ) (349) On the other hand, by the first item of Lem. 2, for D1 :\u2190 b \u25e6DZ , D2 :\u2190 DT , D3 :\u2190 DB , p :\u2190 uB ,\ndisc\u221e(uB \u25e6 b \u25e6DZ , DB) \u2264 disc\u221e(uB \u25e6DT , DB) + disc\u221e(b \u25e6DZ , DT ) = disc\u221e(uB \u25e6DT , DB) + disc\u221e(b \u25e6DZ , b \u25e6 y\u22121A \u25e6DA) \u2264 0 + disc\u221e(b \u25e6DZ , b \u25e6 y\u22121A \u25e6DA)\n(350)\nBy Lem. 1, for p :\u2190 b, we have: disc\u221e(b \u25e6DZ , b \u25e6 y\u22121A \u25e6DA) \u2264 disc\u221e(DZ , y\u22121A \u25e6DA) \u2264 0 (351)\nFinally, disc\u221e(uB \u25e6 b \u25e6DZ , DB) \u2264 2 0 (352)\nIn addition, since disc\u221e(gB \u25e6DT , DB) \u2264 0 we have C(gB) \u2265 C 0T,B = C(uB). Therefore, C(uB \u25e6 b) \u2264 C(uB) + C(b) \u2264 C(gB) + C(b) \u2264 C(gB \u25e6 b) + 4 = C(yB) + 4 = C 0Z,B + 4 (353)\nWe conclude that:\nb\u22121 \u25e6 u\u22121A \u2208 H2 0(DA, DT ) and uB \u25e6 b \u2208 H2 0(DT , DB ;C 0Z,B + 4) (354) Therefore,\nuAB \u2208 H2 0(DT , DB ;C 0Z,B + 4) \u25e6H2 0(DA, DT ) (355)\nPart 3: Now, let uAB = uB \u25e6 u\u22121A \u2208 Z(DA, DT , DB ; 0) such that u\u22121A \u2208 H 0(DA, DT ) and uB \u2208 H 0(DT , DB). Therefore,\nC(uB) \u2264 C 0T,B \u2264 C(gB) and C(u\u22121A ) \u2264 C 0A,T = C(g\u22121A ) (356) In addition, by part 2:\nuAB \u2208 H2 0(DT , DB ;C 0Z,B + 4) \u25e6H2 0(DA, DT ) := F (357) Since yAB has a smaller complexity than all of the functions in F , we have C(yAB) \u2264 C(uAB) and:\nC(yAB) \u2264 C(uAB) \u2264 C(uB) + C(u\u22121A ) (358) In addition, since gB 7 g\u22121A and the fact that C(uB) \u2264 C(gB) and C(u\u22121A ) \u2264 C(g\u22121A ), we have:\nC(uB) + C(u \u22121 A ) \u2264 C(gB) + C(g\u22121A ) = C(yAB) \u2264 C(uB \u25e6 u\u22121A ) \u2264 C(uB) + C(u\u22121A ) (359)\nIn particular, uB 7 u\u22121A .\nmaps between DT and DB ."}], "references": [{"title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Uniqueness of weights for neural networks", "author": ["E.D. Sontag F. Albertini", "V. Maillot"], "venue": "Artificial Neural Networks for Speech and Vision,", "citeRegEx": "Albertini and Maillot.,? \\Q1993\\E", "shortCiteRegEx": "Albertini and Maillot.", "year": 1993}, {"title": "Recovering a feed-forward net from its output", "author": ["Charles Fefferman", "Scott Markel"], "venue": "In NIPS,", "citeRegEx": "Fefferman and Markel.,? \\Q1993\\E", "shortCiteRegEx": "Fefferman and Markel.", "year": 1993}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ganin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Learning to discover cross-domain relations with generative adversarial networks", "author": ["Taeksoo Kim", "Moonsu Cha", "Hyunsoo Kim", "Jungkwon Lee", "Jiwon Kim"], "venue": "arXiv preprint arXiv:1703.05192,", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Comparing fixed and variable-width gaussian networks", "author": ["Vera Kurkov\u00e1", "Paul C. Kainen"], "venue": "Neural Networks,", "citeRegEx": "Kurkov\u00e1 and Kainen.,? \\Q2014\\E", "shortCiteRegEx": "Kurkov\u00e1 and Kainen.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng"], "venue": "In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Uniqueness of the weights for minimal feedforward nets with a given input-output map", "author": ["H\u00e9ctor J. Sussmann"], "venue": "Neural Networks,", "citeRegEx": "Sussmann.,? \\Q1992\\E", "shortCiteRegEx": "Sussmann.", "year": 1992}, {"title": "Towards principled unsupervised learning", "author": ["Ilya Sutskever", "Rafal J\u00f3zefowicz", "Karol Gregor", "Danilo Jimenez Rezende", "Timothy P. Lillicrap", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1511.06440,", "citeRegEx": "Sutskever et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Unsupervised cross-domain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Taigman et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2017}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probab. and its Applications,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1971}, {"title": "Existence and uniqueness results for neural network approximations", "author": ["Robert C. Williamson", "Uwe Helmke"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "Williamson and Helmke.,? \\Q1995\\E", "shortCiteRegEx": "Williamson and Helmke.", "year": 1995}, {"title": "Dual learning for machine translation", "author": ["Yingce Xia", "Di He", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma"], "venue": "arXiv preprint arXiv:1611.00179,", "citeRegEx": "Xia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "Dualgan: Unsupervised dual learning for image-to-image translation", "author": ["Zili Yi", "Hao Zhang", "Ping Tan", "Minglun Gong"], "venue": "arXiv preprint arXiv:1704.02510,", "citeRegEx": "Yi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2017}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "venue": "In ICLR,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Unpaired image-to-image translation using cycle-consistent adversarial networkss", "author": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1703.10593,", "citeRegEx": "Zhu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2017}, {"title": "Empirical Validation of Prediction 1 In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping", "author": ["B. Appendix"], "venue": null, "citeRegEx": "Appendix,? \\Q2017\\E", "shortCiteRegEx": "Appendix", "year": 2017}], "referenceMentions": [{"referenceID": 14, "context": "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.", "startOffset": 24, "endOffset": 95}, {"referenceID": 5, "context": "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.", "startOffset": 24, "endOffset": 95}, {"referenceID": 17, "context": "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.", "startOffset": 24, "endOffset": 95}, {"referenceID": 15, "context": "Multiple recent reports (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) convincingly demonstrated that one can learn to map between two domains that are each specified merely by a set of unlabeled examples.", "startOffset": 24, "endOffset": 95}, {"referenceID": 17, "context": "For example, given a set of unlabeled images of horses, and a set of unlabeled images of zebras, CycleGAN (Zhu et al., 2017) creates the analog zebra image for a new image of a horse and vice versa.", "startOffset": 106, "endOffset": 124}, {"referenceID": 4, "context": "This is enforced using GANs (Goodfellow et al., 2014) and is applied at the distribution level: the mapping of horse images to the zebra domain should create images that are indistinguishable from the training images of zebras and vice versa.", "startOffset": 28, "endOffset": 53}, {"referenceID": 5, "context": "In another example, taken from DiscoGAN (Kim et al., 2017), a function is learned to map a handbag to a shoe of a similar style.", "startOffset": 40, "endOffset": 58}, {"referenceID": 8, "context": ", (Radford et al., 2015), in mapping random input vectors into realistic-looking images.", "startOffset": 2, "endOffset": 24}, {"referenceID": 0, "context": "It makes sense to assume that both yA and yB are invertible, since given training samples, one may be expected to be able to recover the underlying properties of the generated samples, even with very weak supervision (Chen et al., 2016).", "startOffset": 217, "endOffset": 236}, {"referenceID": 11, "context": "In the cross domain transfer line of work (Taigman et al., 2017), the alignment problem is dealt with by incorporating a fixed pre-trained feature map f and requiring what is called f -constancy, namely that the following risk is small RDA [f, f \u25e6 h], or informally, f(x) = f(h(x)).", "startOffset": 42, "endOffset": 64}, {"referenceID": 14, "context": "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.", "startOffset": 33, "endOffset": 104}, {"referenceID": 5, "context": "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.", "startOffset": 33, "endOffset": 104}, {"referenceID": 17, "context": "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.", "startOffset": 33, "endOffset": 104}, {"referenceID": 15, "context": "In multiple recent contributions (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) circularity is employed.", "startOffset": 33, "endOffset": 104}, {"referenceID": 3, "context": "where discC(D1, D2) = supc1,c2\u2208C |RD1 [c1, c2]\u2212RD2 [c1, c2]| denotes the discrepancy between distributions D1 and D2 that is implemented with a GAN (Ganin et al., 2016).", "startOffset": 148, "endOffset": 168}, {"referenceID": 14, "context": "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.", "startOffset": 18, "endOffset": 89}, {"referenceID": 5, "context": "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.", "startOffset": 18, "endOffset": 89}, {"referenceID": 17, "context": "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.", "startOffset": 18, "endOffset": 89}, {"referenceID": 15, "context": "3, the methods of (Xia et al., 2016; Kim et al., 2017; Zhu et al., 2017; Yi et al., 2017) enjoy empirical success, Why? In order to illustrate our main thesis, we present a very simple toy example, depicted in Fig.", "startOffset": 18, "endOffset": 89}, {"referenceID": 7, "context": "However, when we learn the mapping using a neural network with one hidden layer of size 2, and Leaky ReLU activations1 (Maas et al., 2013), y AB is one of only two options.", "startOffset": 119, "endOffset": 138}, {"referenceID": 5, "context": "For example, DiscoGAN (Kim et al., 2017) employs either eight or ten layers, depending on the dataset.", "startOffset": 22, "endOffset": 40}, {"referenceID": 5, "context": "The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used).", "startOffset": 86, "endOffset": 122}, {"referenceID": 17, "context": "The NN-SCM with the Leaky ReLU activation function is of a particular interest, since (Kim et al., 2017; Zhu et al., 2017) employ it as the main activation function (plain ReLUs and tanh are also used).", "startOffset": 86, "endOffset": 122}, {"referenceID": 2, "context": ", M \u00b7 et(M)]> where ei is the i\u2019th standard basis vector, t is a permutation over [M ] and i \u2208 {\u00b11} (Fefferman and Markel, 1993).", "startOffset": 100, "endOffset": 128}, {"referenceID": 13, "context": "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurkov\u00e1 and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.", "startOffset": 12, "endOffset": 115}, {"referenceID": 6, "context": "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurkov\u00e1 and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.", "startOffset": 12, "endOffset": 115}, {"referenceID": 9, "context": "Other works (Williamson and Helmke, 1995; F. Albertini and Maillot, 1993; Kurkov\u00e1 and Kainen, 2014; Sussmann, 1992) prove such uniqueness for neural networks with only one hidden layer and various activation functions.", "startOffset": 12, "endOffset": 115}, {"referenceID": 1, "context": "The most notable work has been done by Fefferman and Markel (1993) that proves identifiability for \u03c3 = tanh.", "startOffset": 39, "endOffset": 67}, {"referenceID": 5, "context": ", (Kim et al., 2017), they form well-understood ambiguities.", "startOffset": 2, "endOffset": 20}, {"referenceID": 17, "context": "For example, if all horse riders are covered in the training set, a shirtless rider in the test set (as demonstrated in (Zhu et al., 2017)) would become striped when converting the horse image to a zebra image.", "startOffset": 120, "endOffset": 138}, {"referenceID": 5, "context": "For example, the mapping of cars in one pose to cars in the mirrored pose that sometimes happens in (Kim et al., 2017), is similar in nature to the mapping of x to 1\u2212 x in the simple example given in Sec.", "startOffset": 100, "endOffset": 118}, {"referenceID": 16, "context": "While in the former, deeper networks, which can learn even random labels, work well (Zhang et al., 2017), unsupervised learning requires a careful control of the network capacity.", "startOffset": 84, "endOffset": 104}, {"referenceID": 11, "context": "The stratified complexity model (SCM) is related to structural risk minimization by Vapnik and Chervonenkis (1971), which employs a hierarchy of nested subsets of hypothesis classes in order of increasing complexity.", "startOffset": 84, "endOffset": 115}, {"referenceID": 10, "context": "As an extreme example, Sutskever et al. (2015) present empirical evidence that a semantic mapper can be learned, even from very few examples, if the network trained is kept small.", "startOffset": 23, "endOffset": 47}, {"referenceID": 17, "context": "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA \u2192 XB , based only on the GAN constraint on B, is presented as a failing baseline.", "startOffset": 18, "endOffset": 54}, {"referenceID": 5, "context": "In the literature (Zhu et al., 2017; Kim et al., 2017), learning a mapping h : XA \u2192 XB , based only on the GAN constraint on B, is presented as a failing baseline.", "startOffset": 18, "endOffset": 54}, {"referenceID": 15, "context": "In (Yi et al., 2017), among many non-semantic mappings obtained by the GAN baseline, one can find images of GANs that are successful.", "startOffset": 3, "endOffset": 20}, {"referenceID": 5, "context": "In (Kim et al., 2017), 8 layers are sometimes employed while at other times 10 layers are used (counting both convolution and deconvolution).", "startOffset": 3, "endOffset": 21}, {"referenceID": 5, "context": "These experiments were done on the CelebA gender conversion task, where eight layers are employed in the experiments of (Kim et al., 2017).", "startOffset": 120, "endOffset": 138}], "year": 2017, "abstractText": "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that the target mapping is of lower complexity than all other mappings. The measured complexity is directly related to the depth of the neural networks being learned and the semantic mapping could be captured simply by learning using architectures that are not much bigger than the minimal architecture.", "creator": "LaTeX with hyperref package"}}}