{"id": "1709.00489", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Arc-Standard Spinal Parsing with Stack-LSTMs", "abstract": "We present a neural transition-based parser for spinal trees, a dependency representation of constituent trees. The parser uses Stack-LSTMs that compose constituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves.\n\n\n\nThe following code was adapted from the Python-inspired LSTM library. It was originally published in the July 2012 issue of GDC, and is used as a repository for C-like libraries. The source code is a single file that contains the code. The code is a subset of the original Python syntax for LSTM.\nAs a result of this proposal, C-like libraries are developed as a set of dependency-based parsers. In each language, the dependency representation is provided by a set of dependencies. If a dependency is defined as a non-relational dependency, it will be replaced by a dependency-based representation of the dependency-based parsers.\nAn example\nI have previously mentioned that dependency-based parsers will be very useful in the Python-style context, but this paper is only part of a larger series. I suggest that any further further refinement of the approach may be necessary.\nFurther reading\n[1] https://github.com/python-tensor/sensor/sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-sensor-s", "histories": [["v1", "Fri, 1 Sep 2017 21:38:28 GMT  (41kb)", "http://arxiv.org/abs/1709.00489v1", "IWPT 2017"]], "COMMENTS": "IWPT 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miguel ballesteros", "xavier carreras"], "accepted": false, "id": "1709.00489"}, "pdf": {"name": "1709.00489.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["miguel.ballesteros@ibm.com", "xavier.carreras@naverlabs.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n00 48\n9v 1\n[ cs\n.C L\n] 1\nS ep\n2 01\n7\nparser for spinal trees, a dependency representation of constituent trees. The parser uses Stack-LSTMs that compose constituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves."}, {"heading": "1 Introduction", "text": "There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation.\nIn this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inherent in head-driven models (Collins, 1997) and was used by Carreras et al. (2008) with a higher-order factored model. We extend the Stack-LSTMs by Dyer et al. (2015) from dependency to spinal parsing, by augmenting the composition operations to include constituent information in the form of spines. To parse sen-\ntences, we use the extension by Cross and Huang (2016a) of the arc-standard system for dependency parsing (Nivre, 2004). This parsing system generalizes shift-reduce methods (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.\nIn experiments on the Penn Treebank, we look at how sensitive our method is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements."}, {"heading": "2 Spinal Trees", "text": "In a spinal tree each token is associated with a spine. The spine of a token is a (possibly empty) vertical sequence of non-terminal nodes for which the token is the head word. A spinal dependency is a binary directed relation from a node of the head spine to a dependent spine. In this paper we consider projective spinal trees. Figure 1 shows a constituency tree from the Penn Treebank together with two spinal trees that use alternative head identities: the spinal tree in 1b uses Stanford Dependencies (De Marneffe et al., 2006), while the spinal tree in 1c takes the leftmost word of any constituent as the head. It is direct to map a constituency tree with head annotations to a spinal tree, and to map a spinal tree to a constituency or a dependency tree.\nS \u00b7VP\nADVP\ndeep\nrun\nNP\nPP\nNP\nothereach\nof\nNP\nsuspicionstheir\nAnd\n(a) A constituency tree from the Penn Treebank."}, {"heading": "3 Arc-Standard Spinal Parsing", "text": "We use the transition system by Cross and Huang (2016a), which extends the arc-standard system by Nivre (2004) for constituency parsing in a headdriven way, i.e. spinal parsing. We describe it here for completeness. The parsing state is a tuple \u3008\u03b2, \u03c3, \u03b4\u3009, where \u03b2 is a buffer of input tokens to be processed; \u03c3 is a stack of tokens with partial spines; and \u03b4 is a set of spinal dependencies. The operations are the following:\n\u2022 shift : \u3008i:\u03b2, \u03c3, \u03b4\u3009 \u2192 \u3008\u03b2, \u03c3:i, \u03b4\u3009\nShifts the first token of the buffer i onto the stack, i becomes a base spine consisting of a single token.\n\u2022 node(n) : \u3008\u03b2, \u03c3:s, \u03b4\u3009 \u2192 \u3008\u03b2, \u03c3:s+n, \u03b4\u3009\nAdds a non-terminal node n onto the top element of the stack s, which becomes s+n. At this point, the node n can receive left and right children (by the operations below) until the node is closed (by adding a node above, or by reducing the spine with an arc operation with this spine as dependent). By this\nTransition Buffer \u03b2 Stack \u03c3 New Arc in \u03b4\n[And, their, . . . ] []\nshift [their, suspicions, . . . ] [And] shift [suspicions, of, . . . ] [And, their] shift [of, each, . . . ] [. . . , their, susp.] node(NP) [of, each, . . . ] [. . . , their, susp.+NP1 3 ] left-arc [of, each, . . . ] [And, susp.+NP1 3 ] (NP1 3 ,their) node(NP) [of, each, . . . ] [And, susp.+NP1 3 +NP2 3 ] shift [each, other, . . . ] [. . . , susp.+NP1 3 +NP2 3 , of] node(PP) [each, other, . . . ] [. . . , susp.+NP1 3 +NP2 3 , of+PP1 4 ] shift [other, run, . . . ] [. . . , of+PP1 4 , each] shift [run, deep, . . . ] [. . . , each, other] node(NP) [run, deep, . . . ] [. . . , each, other+NP1 6 ] left-arc [run, deep, . . . ] [. . . , of+PP1 4 , other+NP1 6 ] (NP1 6 , each) right-arc [run, deep, . . . ] [. . . , susp.+NP1 3 +NP2 3 , of+PP1 4 ] (PP1 4 , NP1 6 ) right-arc [run, deep, . . . ] [And, susp.+NP1 3 +NP2 3 ] (NP2 3 , PP1 4 ) . . .\nheaded at token i by:\n1. Shifting the i-th token to the top of the stack.\nBy induction, the left children of i are in the stack and are complete.\n2. Adding a constituency node n to i\u2019s spine. 3. Adding left children to n in head-outwards\norder with left-arc, which are removed from the stack. 4. Adding right children to n in head-outwards\norder with right-arc, which are built recursively. 5. Repeating steps 2-4 for as many nodes in the\nspine of i.\nFigure 2 shows an example of a derivation. The process is initialized with all sentence tokens in the buffer, an empty stack, and an empty set of dependencies. Termination is always attainable and occurs when the buffer is empty and there is a single element in the stack, namely the spine of the full sentence head. This transition system is correct and sound with respect to the class of projective spinal trees, in the same way as the arc-standard system is for projective dependency trees (Nivre, 2008). A derivation has 2n +m steps, where n is the sentence length and m is the number of constituents in the derivation.\nWe note that the system naturally handles constituents of arbitrary arity. In particular, unary productions add one node in the spine without any children. In practice we put a hard bound on the number of consecutive unary productions in a spine1, to ensure that in the early training steps the model does not generate unreasonably long spines. We also note there is a certain degree of non-determinism: left and right arcs (steps 3 and 4) can be mixed as long as the children of a node are added in head-outwards order. At training time, our oracle derivations impose the order above (first left arcs, then right arcs), but the parsing system runs freely. Finally, the system can be easily extended with dependency labels, but we do not use them."}, {"heading": "4 Spinal Stack-LSTMs", "text": "Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embedding for each element in the stack.2. Our model is based on the same architecture, with the addition of the node(n) action. The state of our algorithm presented in Section 3 is represented by the contents of the STACK, the BUFFER and a list with the history of actions with Stack-LSTMs. This state representation is then used to predict the next action to take.\nComposition: when the parser predicts a left-arc() or right-arc(), we compose the vector representation of the head and dependent elements; this is equivalent to what it is presented by Dyer et al. (2015). The\n1Set to 10 in our experiments 2We refer interested readers to (Dyer et al., 2015;\nBallesteros et al., 2017).\nrepresentation is obtained recursively as follows:\nc = tanh (U[h;d] + e) .\nwhere U is a learned parameter matrix, h represents the head spine and d represents the dependent spine (or token, if the dependent is just a single token) ; e is a bias term.\nSimilarly, when the parser predicts a node(n) action, we compose the embedding of the nonterminal symbol that is added (n) with the representation of the element at the top of the stack (s), that might represent a spine or a single terminal symbol. The representation is obtained recursively as follows:\nc = tanh (W[s;n] + b) . (1)\nwhere W is a learned parameter matrix, s represents the token in the stack (and its partial spine, if non-terminals have been added to it) and n represents the non-terminal symbol that we are adding to s; b is a bias term.\nAs shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models."}, {"heading": "5 Related Work", "text": "Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graphbased parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arceager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottomup arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is headdriven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions.\nFinally, dependency parsers have been extended to constituency parsing by encoding the additional structure in the dependency labels, in different ways (Hall et al., 2007; Hall and Nivre, 2008; Ferna\u0301ndez-Gonza\u0301lez and Martins, 2015)."}, {"heading": "6 Experiments", "text": "We experiment with stack-LSTM spinal models trained with different types of head rules. Our goal is to check how the head identities, which define the derivation sequence, interact with the ability of Stack-LSTMs to propagate latent information beyond the local scope of each action. We use the Penn Treebank (Marcus et al., 1993) with standard splits.3\nWe start training four spinal models, varying the\nhead rules that define the spinal derivations:4\n\u2022 Leftmost heads as in Figure 1c. \u2022 Rightmost heads. \u2022 Stanford Dependencies (SD) (De Marneffe et al., 2006), as in Figure\n1b.\n\u2022 Yamada and Matsumoto heads (YM) (Yamada and Matsumoto, 2003).\nTable 1 presents constituency and dependency metrics on the development set. The model using rightmost heads works the best at 91.11 F1, followed by the one using leftmost heads. It is worth\n3We use the the same POS tags as Dyer et al. (2015). 4It is simple to obtain a spinal tree given a constituency tree and a corresponding dependency tree. We assume that the dependency tree is projective and nested within the constituency tree, which holds for the head rules we use.\nto note that the two models using structural head identities (right or left) work better than those using linguistic ones. This suggests that the StackLSTMmodel already finds useful head-child relations in a constituent by parsing from the left (or right) even if there are non-local interactions. In this case, head rules are not useful.\nThe same Table 1 shows two ablation studies. First, we turn off the composition of constituent nodes into the latent derivations (Eq 1). The ablated models, tagged with \u201cno n-comp\u201d, perform from 0.5 to 1 points F1 worse, showing the benefit of adding constituent structure. Then, we check if constituent structure is any useful for dependency parsing metrics. To this end, we emulate a dependency parser using a spinal model by taking standard Stanford dependency trees and adding a dummy constituent for every head with all its children. This model, tagged \u201cSD heads, dummy spines\u201d, is slightly outperformed by the \u201cSD heads\u201d model using true spines, even though the margin is small.\nTables 2 and 3 present results on the test, for constituent and dependency parsing respectively. As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al. (2016) and Kuncoro et al. (2017) are better than the rest, but compared to the rest our parser is at the same level or better. The most similar system is by Ballesteros and Carreras (2015) and our parser significantly improves the performance. Considering dependency parsing, our model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al. (2016), but it slightly improves the parser by Dyer et al. (2015) with static training. The systems that calculate dependencies by transforming phrase-structures with conversion rules and that use generative training are ahead compared to the rest."}, {"heading": "7 Conclusions", "text": "We have presented a neural model based on StackLSTMs for spinal parsing, using a simple extension of arc-standard transition parsing that adds constituent nodes to the dependency derivation. Our experiments suggest that Stack-LSTMs can figure out useful internal structure within constituents, and that the parser might work better without providing linguistically-derived head\nwords. Overall, our spinal neural method is simple, efficient, and very accurate, and might prove useful to model constituent trees with dependency relations."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proceedings of the 54th Annual Meeting of the", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Transition-based spinal parsing", "author": ["Miguel Ballesteros", "Xavier Carreras."], "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics, pages 289\u2013299.", "citeRegEx": "Ballesteros and Carreras.,? 2015", "shortCiteRegEx": "Ballesteros and Carreras.", "year": 2015}, {"title": "Greedy transition-based dependency parsing with stack lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Yoav Goldberg", "Noah Smith."], "venue": "Computational Linguistics 43(2).", "citeRegEx": "Ballesteros et al\\.,? 2017", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2017}, {"title": "Training with exploration improves a greedy stack lstm parser", "author": ["Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Lin-", "citeRegEx": "Dyer and Smith.,? 2016", "shortCiteRegEx": "Dyer and Smith.", "year": 2016}, {"title": "Organizing Committee, chapter TAG, Dynamic Programming, and the Perceptron", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo"], "venue": "Proceedings of the Twelfth Conference on Computational Natural Language Learning,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2331\u20132336.", "citeRegEx": "Choe and Charniak.,? 2016a", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Parsing as language modeling", "author": ["Kook Do Choe", "Eugene Charniak."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 2331\u20132336.", "citeRegEx": "Choe and Charniak.,? 2016b", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Incremental parsing with minimal features using bi-directional lstm", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association", "citeRegEx": "Cross and Huang.,? 2016a", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages", "citeRegEx": "Cross and Huang.,? 2016b", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of LREC. Genoa,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Parsing as reduction", "author": ["Daniel Fern\u00e1ndez-Gonz\u00e1lez", "T. Andr\u00e9 F. Martins."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.,? 2015", "shortCiteRegEx": "Fern\u00e1ndez.Gonz\u00e1lez and Martins.", "year": 2015}, {"title": "Proceedings of the Workshop on Parsing German, Association for Computational Linguistics, chapter A Dependency-Driven Parser for German Dependency and Constituency Representations, pages 47\u201354", "author": ["Johan Hall", "Joakim Nivre"], "venue": null, "citeRegEx": "Hall and Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Hall and Nivre.", "year": 2008}, {"title": "University of Tartu, Estonia, chapter A Hybrid Constituency", "author": ["Johan Hall", "Joakim Nivre", "Jens Nilsson"], "venue": "Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA", "citeRegEx": "Hall et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2007}, {"title": "Empty element recovery by spinal parser operations", "author": ["Katsuhiko Hayashi", "Masaaki Nagata."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association", "citeRegEx": "Hayashi and Nagata.,? 2016", "shortCiteRegEx": "Hayashi and Nagata.", "year": 2016}, {"title": "Shift-reduce spinal tag parsing with dynamic programming", "author": ["Katsuhiko Hayashi", "Jun Suzuki", "Masaaki Nagata."], "venue": "Transactions of the Japanese Society for Artificial Intelligence 31(2).", "citeRegEx": "Hayashi et al\\.,? 2016", "shortCiteRegEx": "Hayashi et al\\.", "year": 2016}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Vol-", "citeRegEx": "Henderson.,? 2003", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "Transactions of the Association for Computational Linguistics 4:313\u2013327. https://transacl.org/ojs/index.php/tacl/article/view/885.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In Proceedings of the 15th Conference", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Distilling an ensemble of greedy dependency parsers into one mst parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Shift-reduce constituent parsing with neural lookahead features", "author": ["Jiangming Liu", "Yue Zhang."], "venue": "Transactions of the Association of Computational Linguistics 5:45\u201358. http://aclanthology.coli.uni-saarland.de/pdf/Q/Q17/Q17-1004.pdf.", "citeRegEx": "Liu and Zhang.,? 2017", "shortCiteRegEx": "Liu and Zhang.", "year": 2017}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary A. Marcinkiewicz."], "venue": "Computational Linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition To-", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre."], "venue": "Computational Linguistics 34(4):513\u2013553.", "citeRegEx": "Nivre.,? 2008", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "Proceedings of the Ninth International Workshop on Parsing Technology. Association for Computational Linguistics, Van-", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the Tenth International Conference on Parsing Technologies. Association for Computational Linguistics,", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "CoRR abs/1412.7449. http://arxiv.org/abs/1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Transition-based neural constituent parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Hiroyasu Yamada", "Yuji Matsumoto."], "venue": "Proceedings of IWPT. volume 3, pages 195\u2013206.", "citeRegEx": "Yamada and Matsumoto.,? 2003", "shortCiteRegEx": "Yamada and Matsumoto.", "year": 2003}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 28, "context": "There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al.", "startOffset": 96, "endOffset": 186}, {"referenceID": 5, "context": "There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al.", "startOffset": 96, "endOffset": 186}, {"referenceID": 11, "context": "There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al.", "startOffset": 96, "endOffset": 186}, {"referenceID": 0, "context": "There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al.", "startOffset": 96, "endOffset": 186}, {"referenceID": 29, "context": ", 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 30, "endOffset": 139}, {"referenceID": 30, "context": ", 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 30, "endOffset": 139}, {"referenceID": 12, "context": ", 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 30, "endOffset": 139}, {"referenceID": 9, "context": ", 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b).", "startOffset": 30, "endOffset": 139}, {"referenceID": 25, "context": "To parse sentences, we use the extension by Cross and Huang (2016a) of the arc-standard system for dependency parsing (Nivre, 2004).", "startOffset": 118, "endOffset": 131}, {"referenceID": 18, "context": "This parsing system generalizes shift-reduce methods (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.", "startOffset": 53, "endOffset": 138}, {"referenceID": 27, "context": "This parsing system generalizes shift-reduce methods (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.", "startOffset": 53, "endOffset": 138}, {"referenceID": 32, "context": "This parsing system generalizes shift-reduce methods (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.", "startOffset": 53, "endOffset": 138}, {"referenceID": 30, "context": "This parsing system generalizes shift-reduce methods (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) to be sensitive to constituent heads, as opposed to, for example, parse a constituent from left to right.", "startOffset": 53, "endOffset": 138}, {"referenceID": 4, "context": "representation is inherent in head-driven models (Collins, 1997) and was used by Carreras et al. (2008) with a higher-order factored model.", "startOffset": 81, "endOffset": 104}, {"referenceID": 4, "context": "representation is inherent in head-driven models (Collins, 1997) and was used by Carreras et al. (2008) with a higher-order factored model. We extend the Stack-LSTMs by Dyer et al. (2015) from dependency to spinal parsing, by augmenting the composition operations to include constituent information in the form of spines.", "startOffset": 81, "endOffset": 188}, {"referenceID": 4, "context": "representation is inherent in head-driven models (Collins, 1997) and was used by Carreras et al. (2008) with a higher-order factored model. We extend the Stack-LSTMs by Dyer et al. (2015) from dependency to spinal parsing, by augmenting the composition operations to include constituent information in the form of spines. To parse sentences, we use the extension by Cross and Huang (2016a) of the arc-standard system for dependency parsing (Nivre, 2004).", "startOffset": 81, "endOffset": 390}, {"referenceID": 10, "context": "In experiments on the Penn Treebank, we look at how sensitive our method is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents.", "startOffset": 292, "endOffset": 355}, {"referenceID": 8, "context": "We use the transition system by Cross and Huang (2016a), which extends the arc-standard system by Nivre (2004) for constituency parsing in a headdriven way, i.", "startOffset": 32, "endOffset": 56}, {"referenceID": 8, "context": "We use the transition system by Cross and Huang (2016a), which extends the arc-standard system by Nivre (2004) for constituency parsing in a headdriven way, i.", "startOffset": 32, "endOffset": 111}, {"referenceID": 26, "context": "This transition system is correct and sound with respect to the class of projective spinal trees, in the same way as the arc-standard system is for projective dependency trees (Nivre, 2008).", "startOffset": 176, "endOffset": 189}, {"referenceID": 19, "context": "(2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embedding for each element in the stack.", "startOffset": 85, "endOffset": 119}, {"referenceID": 11, "context": "pose the vector representation of the head and dependent elements; this is equivalent to what it is presented by Dyer et al. (2015). The", "startOffset": 113, "endOffset": 132}, {"referenceID": 11, "context": "Set to 10 in our experiments We refer interested readers to (Dyer et al., 2015; Ballesteros et al., 2017).", "startOffset": 60, "endOffset": 105}, {"referenceID": 2, "context": "Set to 10 in our experiments We refer interested readers to (Dyer et al., 2015; Ballesteros et al., 2017).", "startOffset": 60, "endOffset": 105}, {"referenceID": 21, "context": "As shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graphbased parser for this representation.", "startOffset": 131, "endOffset": 154}, {"referenceID": 1, "context": "Ballesteros and Carreras (2015) presented an arceager system that labels dependencies with", "startOffset": 0, "endOffset": 32}, {"referenceID": 16, "context": "Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottomup arc-standard system that assigns a full spine", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "(2016) and Hayashi and Nagata (2016) presented a bottomup arc-standard system that assigns a full spine", "startOffset": 11, "endOffset": 37}, {"referenceID": 18, "context": "Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is headdriven.", "startOffset": 62, "endOffset": 147}, {"referenceID": 27, "context": "Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is headdriven.", "startOffset": 62, "endOffset": 147}, {"referenceID": 32, "context": "Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is headdriven.", "startOffset": 62, "endOffset": 147}, {"referenceID": 30, "context": "Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is headdriven.", "startOffset": 62, "endOffset": 147}, {"referenceID": 8, "context": "Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing.", "startOffset": 0, "endOffset": 24}, {"referenceID": 15, "context": "Finally, dependency parsers have been extended to constituency parsing by encoding the additional structure in the dependency labels, in different ways (Hall et al., 2007; Hall and Nivre, 2008; Fern\u00e1ndez-Gonz\u00e1lez and Martins, 2015).", "startOffset": 152, "endOffset": 231}, {"referenceID": 14, "context": "Finally, dependency parsers have been extended to constituency parsing by encoding the additional structure in the dependency labels, in different ways (Hall et al., 2007; Hall and Nivre, 2008; Fern\u00e1ndez-Gonz\u00e1lez and Martins, 2015).", "startOffset": 152, "endOffset": 231}, {"referenceID": 13, "context": "Finally, dependency parsers have been extended to constituency parsing by encoding the additional structure in the dependency labels, in different ways (Hall et al., 2007; Hall and Nivre, 2008; Fern\u00e1ndez-Gonz\u00e1lez and Martins, 2015).", "startOffset": 152, "endOffset": 231}, {"referenceID": 24, "context": "We use the Penn Treebank (Marcus et al., 1993) with standard splits.", "startOffset": 25, "endOffset": 46}, {"referenceID": 31, "context": "\u2022 Yamada and Matsumoto heads (YM) (Yamada and Matsumoto, 2003).", "startOffset": 34, "endOffset": 62}, {"referenceID": 11, "context": "We use the the same POS tags as Dyer et al. (2015). It is simple to obtain a spinal tree given a constituency tree and a corresponding dependency tree.", "startOffset": 32, "endOffset": 51}, {"referenceID": 5, "context": "As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al.", "startOffset": 101, "endOffset": 127}, {"referenceID": 5, "context": "As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al. (2016) and Kuncoro et al.", "startOffset": 101, "endOffset": 147}, {"referenceID": 5, "context": "As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al. (2016) and Kuncoro et al. (2017) are better than the rest, but compared to the rest our parser is at the same level or better.", "startOffset": 101, "endOffset": 173}, {"referenceID": 1, "context": "The most similar system is by Ballesteros and Carreras (2015) and our parser significantly improves the performance.", "startOffset": 30, "endOffset": 62}, {"referenceID": 17, "context": "model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al.", "startOffset": 60, "endOffset": 92}, {"referenceID": 2, "context": "model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al. (2016), but it slightly improves the parser by Dyer et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 2, "context": "model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al. (2016), but it slightly improves the parser by Dyer et al. (2015) with static training.", "startOffset": 96, "endOffset": 181}, {"referenceID": 8, "context": "1 Cross and Huang (2016a) 93.", "startOffset": 2, "endOffset": 26}, {"referenceID": 6, "context": "5 Choe and Charniak (2016a)* (Semi-sup) 95.", "startOffset": 2, "endOffset": 28}, {"referenceID": 6, "context": "5 Choe and Charniak (2016a)* (Semi-sup) 95.9 Kuncoro et al. (2017)* (Generative) 95.", "startOffset": 2, "endOffset": 67}], "year": 2017, "abstractText": "We present a neural transition-based parser for spinal trees, a dependency representation of constituent trees. The parser uses Stack-LSTMs that compose constituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves.", "creator": "LaTeX with hyperref package"}}}