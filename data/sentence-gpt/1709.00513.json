{"id": "1709.00513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Learning Loss for Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student over time. In addition, we focus on the student-teacher strategy for the study of self-confidence, but also on the task of developing better models for the task of learning in a group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 2 Sep 2017 01:03:08 GMT  (356kb,D)", "http://arxiv.org/abs/1709.00513v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["zheng xu", "yen-chang hsu", "jiawei huang"], "accepted": false, "id": "1709.00513"}, "pdf": {"name": "1709.00513.pdf", "metadata": {"source": "META", "title": "Learning Loss for Knowledge Distillation with Conditional Adversarial Networks", "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks (DNNs) achieve massive success in artificial intelligence by substantially improving the stateof-the-art performance in various applications. For one of the core applications in computer vision, large-scale image classification (Russakovsky et al. 2015), the accuracy reached by DNNs has become comparable to humans on several benchmark datasets. The recent progress towards such impressive accomplishment is largely driven by exploring deeper and wider network architectures. Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications. In the meantime, the demands on low cost networks are increasing for applications on mobile devices and autonomous cars.\nDo DNNs really need to be deep and wide? Early theoretical studies (Cybenko 1989; Hornik, Stinchcombe, and White 1989) suggest that shallow networks can approximate arbitrary functions. More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017). Moreover, the overparameterized and redundant networks, which can easily memorize and overfit the training data, surprisingly generalize well in practice (Zhang et al. 2017; Arpit et al. 2017). Various explanations have been investigated, but the secret of deep and wide networks remains an open problem.\nOn the other hand, empirical studies suggest that the performance of shallow networks can be improved by learning from large networks following the student-teacher strategy (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015). In these approaches, the student networks are forced to mimic the output probability distribution of the teacher networks to transfer the knowledge embedded in the soft targets. The intuition is that the dark knowledge (Hinton, Vinyals, and Dean 2015), which contains the relative probabilities of \u201cincorrect\u201d answers provided by deep and wide networks, is informative and representative. For example, an image of a dog may be mistakenly recognized as cat or sheep with small probability, but should be seldom recognized as car; the soft target of output distribution over categories for this image, (0.7, 0.2, 0.1, 0), contains more information than the hard target of one-hot vector, (1, 0, 0, 0).\nIn the previous works, (Ba and Caruana 2014; Urban et al. 2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al. 2015) train a small deep and thin network to replace a shallow and wide network for acceleration, given the best teacher at that time is the shallow and wide VGGNet (Simonyan and Zisserman 2014). Since then, the design of network architecture has advanced. ResNet (He et al. 2016) has significantly deepened the networks by introducing residual connections, and wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) suggest to widen the networks for better performance. It is unclear whether the dark knowledge from the state-of-the-art networks based on residual connections, which are both deep and wide, can help train a shallow and/or thin network for acceleration.\nIn this paper, we focus on the practical approach that can improve the performance of a shallow and thin modern network by learning from the dark knowledge of a deep and wide network. Both the student and teacher networks are convolutional neural networks (CNNs) with residual connections, and the student network is shallow and thin so that it can be much faster than the teacher network during inference. Instead of forcing the output of a student network to exactly mimic the soft targets produced by a teacher\nar X\niv :1\n70 9.\n00 51\n3v 1\n[ cs\n.L G\n] 2\nS ep\n2 01\n7\nnetwork, we introduce conditional adversarial networks to transfer the dark knowledge from teacher to student. We empirically show that the loss learned by the adversarial training has the advantage over the hand-engineered loss in the student-teacher strategy, especially when a relatively small student is used.\nOur learning loss approach is inspired by the recent success of conditional adversarial networks for various imageto-image translation applications (Isola et al. 2017). We show that the generative adversarial nets (GAN) is capable of benefiting a task that is very different from image generation. In the student-teacher strategy, GAN can help preserve the multi-modal nature of the output distribution. Take the soft targets of a dog image over (dog, cat, sheep, car) as an example, both soft targets (0.7, 0.2, 0.1, 0) and (0.8, 0, 0.2, 0) can predict the correct label, dog. In fact, a teacher network trained multiple times would produce different while correct soft targets because of the randomness in the training. Forcing a student network to exactly mimic one of the soft targets (or the average/ensemble of several teacher networks) can be difficult when the student has smaller capacity than the teacher. By introducing the discriminator as in GAN, our learning loss approach transfers the correlation between classes, i.e., the dark knowledge from teacher, and also preserves the multi-modality."}, {"heading": "2 Related work", "text": "Network acceleration has attracted increasing interest because the needs of real-time applications in artificial intelligence are growing . The techniques can be roughly divided into three categories, low precision, pruning and factorization, and knowledge distillation. Low precision methods use limited number of bits to store and operate the network weights, and the extreme case is binary networks that only use 1-bit to represent each number (Rastegari et al. 2016). The acceleration of these methods is sometimes conceptual because the low precision support of common GPU is still limited. Networks can also be directly modified by pruning and factorizing the redundant weights, either as a post-processing after training, or as a fine-tuning stage (Li et al. 2017b; Howard et al. 2017). These methods often assume network weights are sparse and/or low rank, and aim for efficient networks of similar architecture with reduced number of weights.\nKnowledge distillation is a principled approach to train small neural networks for acceleration. We slightly abuse the name knowledge distillation to represent methods that train student networks by transferring knowledge from teacher networks. (Bucilu, Caruana, and Niculescu-Mizil 2006) pioneered this approach for model compression. (Ba and Caruana 2014; Urban et al. 2017) trained shallow but wide student by learning from a deep teacher, which were not primarily designed for acceleration. (Hinton, Vinyals, and Dean 2015) generalized the previous methods by introducing a new metric between the output distribution of teacher and student, as well as a tuning parameter. The variants of knowledge distillation has also been applied to many different tasks, such as semantic segmentation (Ros et al.\n2016), pedestrian detection (Shen et al. 2016), face recognition (Luo et al. 2016), metric learning(Chen, Wang, and Zhang 2017), reinforcement learning (Teh et al. 2017) and for regularization(Sau and Balasubramanian 2016). A recent preprint (Kim and Kim 2017) presented promising preliminary results on CIFAR-10 by learning a small ResNet from a large ResNet. Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017). Our approach is complement to those methods by directly following (Hinton, Vinyals, and Dean 2015) to design a new metric between the output distribution of teacher and student, and adversarial networks are used to learn the metric to replace hand-engineering.\nGenerative adversarial networks (GAN) has been extensively studied over recent years since (Goodfellow et al. 2014). GAN trains two neural networks, the generator and the discriminator, in an adversarial learning process that alternatively updates the two networks. We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images. Unlike the previous works focus on generating and editing images, we target on learning a loss for knowledge distillation, which requires quite different architecture choices for our generator and discriminator."}, {"heading": "3 Learning loss for knowledge distillation", "text": "In this section, we introduce the learning loss approach based on conditional adversarial networks. We start from a recap of modern network architectures (section 3.1), and then describe the dark knowledge that can be transferred from teacher to student networks (section 3.2). The GANbased approach for learning loss is detailed in section 3.3.\n3.1 Neural networks with residual connection\nThe modern neural networks are built by stacking basic components. For computer vision tasks, residual blocks (He\net al. 2016; Zagoruyko and Komodakis 2016) are the basic components to build deep neural networks to achieve stateof-the-art performance. Both student and teacher networks in this paper are based on the residual convolutional blocks shown in Figure 1 (left). The first layer is 16 filters of 3\u00d7 3 convolution, followed by a stack of 6n layers, where n is the number of residual blocks and each block contains two convolution layers equipped with batch normalization (Ioffe and Szegedy 2015), ReLU (Krizhevsky, Sutskever, and Hinton 2012) and dropout (Srivastava et al. 2014). The output feature map is subsampled twice, and the number of filters are doubled when subsampling, as shown in Table 1. The widen factor m is used to increase the number of filters in each residual block. After the last residual block is the global average pooling, and then fully-connected layer and softmax. In the following sections, the architecture of wide residual networks (WRNs) is represented by WRN-d-m, where the total depth is d = 6n + 4. Our teacher network is deep and wide WRN with large d and m, while student network is shallow and thin WRN with small d and m."}, {"heading": "3.2 Knowledge distillation", "text": "The output of neural networks for image classification is a probability distribution over categories. The probability is generated by the softmax layer from logits, which represents the output of the last fully connected layer. The dimension of logits from student and teacher networks are both equal to the number of categories. Rich information is embedded in the output probability of a teacher network, and we can use logits to transfer the knowledge to student network (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015). We review the method in (Hinton, Vinyals, and Dean 2015), which provides a metric between student and teacher logits that generalized previous methods for knowledge distillation.\nThe logits vector generated by pre-trained teacher network for an input image xi, i = 1, . . . , N is represented by ti, where the dimension of vector ti = (t1i , . . . , t C i ) is the number of categories C. We now consider training a student network F to generate student logits F (xi). By introducing a parameter called temperature T , the generalized softmax layer converts logits vector ti to probability distribution qi,\nMT (ti) = qi, where q j i = exp(tji/T )\u2211 k exp(t k i /T ) . (1)\n(Hinton, Vinyals, and Dean 2015) proposed to minimize the Kullback-Leibler divergence between teacher output and\nstudent output,\nLKD(F, T ) = 1\nN N\u2211 i=1 KL(MT (ti)\u2016MT (F (xi))). (2)\nwhere higher temperature T produces softer probability over categories. As detailed in (Hinton, Vinyals, and Dean 2015), the Euclidean distance between teacher and student logits, \u2016ti \u2212 F (xi)\u201622, is a special case of LKD when T is large .\nThe regular softmax for classification uses T = 1. When the image-label pairs {xi, li} are provided, the cross-entropy loss for supervised training of a neural network can be represented as\nLS(F ) = 1\nN N\u2211 i=1 H(li,M1(F (xi))). (3)\nLS is the most frequently used loss for pure supervised learning in image classification. (Hinton, Vinyals, and Dean 2015) minimized the weighted combination of loss LKD and loss LS to train a student network,\nL1(F, T ) = 1\n2 LS(F ) + T 2LKD(F, T ). (4)"}, {"heading": "3.3 Learning loss with adversarial networks", "text": "Overview. The main idea of learning the loss for transferring knowledge from teacher to student is presented in Figure 2. Instead of forcing the student to exactly mimic the teacher by minimizing KL-divergence in L1(F, T ) of Equation (4), the knowledge is transferred from teacher to student through a discriminator in our GAN-based approach.\nThis discriminator is trained to distinguish the output logits of teacher and student, while the student is adversarially trained to fool the discriminator, i.e., output logits similar to the teacher logits so that the discriminator can not distinguish.\nThere are several benefits of the proposed method. First, the learned loss can be effective, as has already been demonstrated for several image to image translation tasks (Isola et al. 2017). Moreover, the GAN-based approach relieves the pain for hand-engineering the loss. Though the parameter tuning and hand-engineering of the loss is replaced by handengineering the discriminator networks in some sense, our empirical study shows that the performance is less sensitive to the discriminator architecture than the temperature parameter in knowledge distillation. The second benefit is closely related to the multi-modality of network output. Let us consider the previous example on classifying a dog image for labels (dog, cat, sheep, car). The relationship of the categories, such as dog looks more like cat than car, can be captured by the discriminator trained from the multi-modal logits of teacher. However, if we look at each individual output, both (0.7, 0.2, 0.1, 0) and (0.8, 0, 0.2, 0) are valid outputs, but it may not be plausible for the student with small capacity to exactly produce either one of the outputs. The student can still benefit from the knowledge transferred from discriminator, which suggests the output should be similar to the two vectors and different from a vector like (0.4, 0.1, 0.1, 0.4).\nDiscriminator update. We now describe the proposed method in a more rigorous way. The student and discriminator in Figure 2 are alternatively updated in the GAN-based approach. Let us first look at the update of the discriminator, which is trained to distinguish teacher and student logits. We use multi-layer perceptron (MLP) as discriminator, which is the stack of residual block shown in Figure 1 (right). The number of nodes in each layer is the same as the dimension of logits, i.e., the number of categories C. Representing the discriminator that predicts binary value \u201cReal/Fake\u201d as D(\u00b7) and fixing the student network F (\u00b7), we can maximize the log-likelihood, which is known as binary cross-entropy loss,\nLA(D,F ) = 1\nN N\u2211 i=1 ( logP (Real|D(ti))+logP (Fake|D(F (xi))) ) .\nThe adversarial loss LA for knowledge distillation, which follows the original GAN (Goodfellow et al. 2014), faces two major issues. First, the adversarial training is difficult. Even if we replace the log-likelihood with advanced techniques such as Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017) and Least Squares GAN (Mao et al. 2016), the training is still slow and unstable in our experiments. Second, the discriminator captures the high-level statistics of teacher and student outputs, but the low-level alignment is missing. The student outputs F (xi) for xi can learn from a completely unrelated teacher sample tj by optimizing LA, which means a dog image can be mapped to a logits vector that predicts cat.\nTo tackle these problems, we modify the discriminator objective to also predict the class labels, inspired by (Chen et al. 2016; Odena, Olah, and Shlens 2017). In this case, the output of discriminator D(\u00b7) is a 2C dimensional vector and\neach entry corresponds to a tuple of {Label, Real/Fake}. We now maximize\nL\u0303GAN (D,F ) = 1\nN N\u2211 i=1 ( logP ({li,Real}|D(ti))\n+ logP ({li, Fake}|D(F (xi))) ) . (5)\nIf we further assume Label and Real/Fake are conditional independent, we can simplify the loss as\nLGAN (D,F ) = 1\n2 (LA(D,F ) + LDS(D,F )), (6)\nwhere LA is the previously defined adversarial loss, LDS is the supervised log-likelihood of discriminator written as\nLDS(D,F ) = 1\nN N\u2211 i=1 ( logP (li|D(ti))+logP (li|D(F (xi))) ) .\nIn (6), the output of discriminator D(\u00b7) is a C + 2 dimensional vector and each entry corresponds to either Label or Real/Fake. In our experiments, optimizing the two forms of GAN-based loss, L\u0303GAN in (5) and LGAN in (6), achieves almost identical performance. Hence we will always use (6) in the following sections because of the simplicity and compactness of discriminator. Note that equation (6) has the same form as the auxiliary classifier GANs (Odena, Olah, and Shlens 2017).\nThe adversarial training becomes much more stable when the discriminator reconstructs category Labels besides Real/Fake. Moreover, the discriminator can provide category-level alignment between outputs of student and teacher. The student outputs of a dog image are more likely to learn from the teacher outputs that predict dogs.\nTo provide instance-level information for the discriminator, we investigate conditional discriminators, in which the input of discriminators are logits concatenate with a conditional vector. We tried the following conditional vectors: image with convolutional embedding; label one-hot vector with embedding; and the extracted teacher logits. The embedding includes several weight layers and outputs a vector that is the same size as the logits. However, it turns out the conditional vectors are easy to be ignored during the training of discriminator. The conditional discriminator does not help in practice and we introduce a more direct instancelevel alignment for training student network below.\nStudent update. We update the student network after updating the discriminator in each iteration. When updating the student network F (\u00b7), we aim to fool the discriminator by fixing discriminator D(\u00b7) and minimizing the adversarial loss LA. In the meantime, the student network is also trained to satisfy the auxiliary classifier of discriminator LDS . Besides the category-level alignment provided by LDS , we introduce instance-level alignment between teacher and student outputs as\nLL1(F ) = 1\nN N\u2211 i=1 |F (xi)\u2212 ti|. (7)\nThe L1 norm alignment has been found helpful in the GANbased approach for image to image translation (Isola et al.\n2017). At last, we combine the learned loss with the supervised loss LS in (3), and minimize the following objective for the student network F (\u00b7),\nL2(D,F ) =LS(F ) + LL1(F )\n+ 1\n2 (LA(D,F )\u2212 LDS(D,F )).\n(8)\nThe sign of LDS is flipped in (6) and (8) because both the discriminator and student are trained to preserve the category-level information.\nThe final loss L2(D,F ) in (8) is a combination of the learned loss for knowledge distillation and the supervised loss for neural network, and may look complicated at the first glance. However, each component of the loss is relatively simple. Moreover, since both student F and discriminator D are learned, there is no explicit parameters to be tuned in the loss function. Our experiments in the next section suggest the performance is reasonably insensitive to the discriminator architecture and the proposed method can replace the hand-engineering loss for knowledge distillation."}, {"heading": "4 Experiments", "text": "We present the experimental results in this section. The implementation details and experimental settings are provided in section 4.1. The benefits of the proposed method for training a small student network with the help of a large teacher network is presented in section 4.2. We then analyze the different components of the proposed methods in section 4.3. The effect of depth and width of the student network is presented in section 4.4, followed by the discussion of trade-off between classification accuracy and inference acceleration in section 4.5. At last, in section 4.6, we show the qualitative visualization on the output distribution for student, teacher, and knowledge distillation."}, {"heading": "4.1 Experimental setting", "text": "We consider three benchmark datasets for image classification: ImageNet32 (Chrabaszcz, Loshchilov, and Hutter 2017), CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton 2009). ImageNet32 is a downsampled version of the ImageNet2012 challenge dataset (Russakovsky et al. 2015), which contains 1.28M training images and 50K validation images for 1K classes; all images are downsampled to 32\u00d732. The CIFAR datasets contain 50K training images and 10K validation images for 10 and 100 classes, respectively. The images are also 32\u00d732. In all the experiments, light data augmentation with horizontal flip, padding and cropping is used for input images as in (He et al. 2016).\nWe use wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) as both student and teacher networks. The residual blocks are shown in Figure 1 (right) and the network architectures are in Table 1. WRN-d-m represents network with d = 6n+ 4 as depth and m as widen factor. The teacher network is always WRN-40-10, while the student network changes depth and width in different experiments. Dropout 0.3 is used for all WRNs. We use stochastic gradient descent (SGD) as optimizer, and set the initial learning rate as 0.1, momentum as 0.9, and weight decay as 1e-4. We\nuse minibatch size 128 and train the WRNs for 200 epochs with learning rate divided by 10 at epoch 80 and 160 for CIFARs; we use minibatch size 256 and train the WRNs for 70 epochs with learning rate divided by 10 at epoch 25 and 50 for Imagenet32.\nWe use multi-layer preceptron (MLP) for the discriminator in the GAN-based approach. The number of nodes in each layer is the same as the length of logits, i.e., number of categories. 3-layer MLP is used for most of the experiments except for section 4.3, in which we study the effect of depth for discriminator. The logits of teacher are generated offline and stored in memory. The logits pass through a batch normalization layer before the MLP. Dropout 0.3 is also used for discriminator.\nThe implementation is in PyTorch. The results below present the median of five random runs."}, {"heading": "4.2 Benefits of learning loss", "text": "We first show the proposed method is good for transferring knowledge from teacher to student. Table 2 presents the error rate of classification on the three benchmark datasets. The teacher is the deep and wide WRN-40-10. The student is much shallower and thinner, WRN-10-4 for CIFARs, and WRN-22-4 for ImageNet32. We choose a larger student network for ImageNet32 because the dataset contains more training samples and categories. More discussion on wisely choosing the student architecture is in section 4.4 and 4.5.\nThe first two rows of Table 2 present the performance of pure supervised learning for student and teacher networks, without knowledge transfer by student-teacher strategy. We compare the GAN-based approach with knowledge distillation proposed in (Hinton, Vinyals, and Dean 2015) and reviewed in section 3.2. We choose the temperature parameter T \u2208 {1, 2, 5, 10} following (Hinton, Vinyals, and Dean 2015). The GAN-based approach is detailed in section 3.3 and no parameter is tuned.\nWe have several observations from Table 2. The deep and wide teacher performs much better than the shallow and thin student by pure supervised learning. The error rate of the small network trained with student-teacher strategy is lower bounded by the teacher performance, as expected. Baseline method (Hinton, Vinyals, and Dean 2015) helps the training of small networks for the two CIFARs, but does not help for ImageNet32. The difference may be because the capacity of the student is too small to learn from knowledge distillation for ImageNet32 that has more samples and categories. The temperature parameter T introduced in (Hinton,\nVinyals, and Dean 2015) is useful. For CIFARs, (Hinton, Vinyals, and Dean 2015) performs better when T is large to some extent, and T = 5 and T = 10 performs similarly. The proposed method improves the performance of small network for all three datasets, and outperforms (Hinton, Vinyals, and Dean 2015)."}, {"heading": "4.3 Analysis of the proposed method", "text": "We discuss the proposed method in more detail in this section. Figure 3 presents the training curve of the small student network, WRN-10-4, on CIFAR-100 dataset. The loss of the discriminator (blue solid line) is gradually decreasing, which suggests the adversarial training steadily makes progress. The error rates of GAN-based method for both training and testing data are decreasing. The testing error rate of GANbased method is consistently better than the pure supervised training of the student model, and looks more stable between epoch 50-100. Surprisingly, the training error rate of pure supervised learning is slightly better than the GAN-based method, which suggests knowledge transfer is more beneficial for generalization.\nNext, we look into different components of the GANbased approach, as shown in Table 3. By optimizing the adversarial loss and the category-level knowledge transfer, the learned loss LGAN performs reasonably well. However, the indirect knowledge provided by LGAN alone is not as good as pure supervised learning LS . Both category-level knowledge transfer by LGAN and instance-level knowledge transfer by LL1 can improve the performance of training\nstudent network. The final approach combines these components and performs the best without parameter tuning.\nFinally, we present the effect of the depth of MLP as discriminator in Table 4. The error rate is relatively insensitive to the depth of discriminator. The error rate slightly decreases as the depth increases when the discriminator is generally shallow. When the discriminator becomes deeper, the error rate increases as the adversarial training becomes unstable. Decreasing the learning rate of discriminator sometimes helps, but it may introduce parameter tuning for the proposed method. The 3-layer MLP works reasonably well and is used for all our experiments to keep the GAN-based method simple."}, {"heading": "4.4 Do WRN need to be deep and wide?", "text": ""}, {"heading": "10-2 0.32 0.14 33.22 32.74 32.1", "text": ""}, {"heading": "10-4 1.22 0.32 28.52 27.16 25.75", "text": ""}, {"heading": "10-6 2.72 0.60 27.27 25.39 24.39", "text": ""}, {"heading": "10-8 4.81 0.82 26.23 24.31 23.38", "text": ""}, {"heading": "10-10 7.49 1.17 26.04 23.49 23.02", "text": ""}, {"heading": "16-4 2.77 0.71 24.73 22.9 22.73", "text": ""}, {"heading": "22-4 4.32 1.07 23.61 22.02 21.66", "text": "(Urban et al. 2017) asked similar question for convolutional neural networks and claimed the network should at least has a few layers of convolutions. We study the modern architecture WRN of residual blocks. Our empirical study suggests that even for the modern architecture WRN, the network has to be deep and wide to some extent.\nTable 5 presents the results of pure supervised learning, knowledge distillation (Hinton, Vinyals, and Dean 2015) and the GAN-based approach for different student networks on CIFAR-100. We first fix the depth of WRN as 10, and change the widen factor from 2 to 10. 10 is the minimum depth for our WRN architecture as the depth has to be 6n+4. We then fix the width as 4, and increase depth from 10 to 34. The parameter size is in millions, and the inference time is in seconds per minibatch of 100 samples on CPU.\nWhen the student is very small, such as WRN-10-2, it is difficult to transfer knowledge from teacher to student because the student is limited by the network capacity. When the student is large, such as WRN-34-4, both knowledge distillation (Hinton, Vinyals, and Dean 2015) and GAN-based approach can improve the performance to almost as good\nas the teacher. The advantage of the proposed method is more obvious for relatively small student such as WRN-104. Increasing depth is more effective than increasing width for WRN. For example, WRN-34-4 has less parameter than WRN-10-10, but achieves lower error rate."}, {"heading": "4.5 Training student for acceleration", "text": "The shallow and thin network is much easier to deploy in practice. We present the trade-off between error rate, inference time and parameter size in Figure 4. The figure is generated from Table 5 by changing the architecture of the student network. Larger student network is more accurate but also slower. For network with similar size, such as WRN10-10 and WRN-34-4, deep network achieves lower error rate, while wide network runs slightly faster. The studentteacher strategy can help improve the classification performance of the student network. When the student network is relatively large, such as WRN-34-4, the student network trained by the GAN-based approach can achieve competitive error rate comparing to the teacher WRN-40-10. WRN-34-4 is 7x smaller and 5x faster than WRN-40-10, and the GANbased approach decreases the absolute error rate by 2.5%.\n(Student) (Ours) (Teacher)"}, {"heading": "4.6 Visualization of distribution", "text": "In the last section of experimental results, we present qualitative visualization for the GAN-based approach. Figure 5 presents the scaled histogram for the prediction of category 85 in CIFAR-100. The histogram is counted on the 10K testing samples, in which 100 samples are from category 85 and labeled as positive (green in figure), and the other 9.9K are labeled as negative (blue in the figure). The histogram is scaled to sum up to one for positive and negative, respectively. The three plots represent the distribution predicted by student network trained by pure supervised learning , the teacher network, and the student network trained by GANbased approach. The histogram in the middle is similar to the histogram in the right, which suggests the GAN-based approach transfers knowledge from teacher to student."}, {"heading": "5 Conclusion and discussion", "text": "We study the student-teacher strategy for network acceleration in this paper. We propose a GAN-based approach to learn the loss for transferring knowledge from teacher to student. We empirically show the the GAN-based approach can improve the training of student network, especially when the student network is shallow and thin. Moreover, we empirically study the effect of capacity for modern network as student and provide guidelines for wisely choosing a student to balance error rate and acceleration. In specific setting, we can train a student that is 7x smaller and 5x faster than teacher without loss of accuracy.\nThe GAN-based approach is stable and easy to implement after applying several advanced techniques in the GAN literature. The current implementation uses the stored logtis from teacher network to save GPU memory and computation. Generating teacher logits on the fly with dropout can be more reliable for the adversarial training. At last, the GANbased approach can be naturally extended for the ensemble of networks as teacher. The logits of multiple teacher networks can be fed into the discriminator for better performance. We will investigate these ideas for future work."}], "references": [{"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "ICML. Arpit, D.; Jastrzebski, S.; Ballas, N.; Krueger, D.; Bengio, E.; Kanwal, M. S.; Maharaj, T.; Fischer, A.; Courville, A.; Bengio, Y.; et al. 2017. A closer look at memorization in deep networks. ICML.", "citeRegEx": "Arjovsky et al\\.,? 2017", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "Ba and Caruana,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana", "year": 2014}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "KDD, 535\u2013541. ACM.", "citeRegEx": "Bucilu et al\\.,? 2006", "shortCiteRegEx": "Bucilu et al\\.", "year": 2006}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "NIPS, 2172\u20132180.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Darkrank: Accelerating deep metric learning via cross sample similarities transfer", "author": ["Y. Chen", "N. Wang", "Z. Zhang"], "venue": "arXiv preprint arXiv:1707.01220.", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "A downsampled variant of imagenet as an alternative to the cifar datasets", "author": ["P. Chrabaszcz", "I. Loshchilov", "F. Hutter"], "venue": "arXiv preprint arXiv:1707.08819.", "citeRegEx": "Chrabaszcz et al\\.,? 2017", "shortCiteRegEx": "Chrabaszcz et al\\.", "year": 2017}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "MCSS 2(4):303\u2013314.", "citeRegEx": "Cybenko,? 1989", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "The power of depth for feedforward neural networks", "author": ["R. Eldan", "O. Shamir"], "venue": "COLT, 907\u2013940.", "citeRegEx": "Eldan and Shamir,? 2016", "shortCiteRegEx": "Eldan and Shamir", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. WardeFarley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS, 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 770\u2013778.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks 2(5):359\u2013366.", "citeRegEx": "Hornik et al\\.,? 1989", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "author": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"], "venue": "arXiv preprint arXiv:1704.04861.", "citeRegEx": "Howard et al\\.,? 2017", "shortCiteRegEx": "Howard et al\\.", "year": 2017}, {"title": "Like what you like: Knowledge distill via neuron selectivity transfer", "author": ["Z. Huang", "N. Wang"], "venue": "arXiv preprint arXiv:1707.01219.", "citeRegEx": "Huang and Wang,? 2017", "shortCiteRegEx": "Huang and Wang", "year": 2017}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML, 448\u2013456.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Image-toimage translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CVPR.", "citeRegEx": "Isola et al\\.,? 2017", "shortCiteRegEx": "Isola et al\\.", "year": 2017}, {"title": "Transferring knowledge to smaller network with class-distance loss", "author": ["S.W. Kim", "Kim", "H.-E."], "venue": "ICLR Workshop.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Training quantized nets: A deeper understanding", "author": ["H. Li", "S. De", "Z. Xu", "C. Studer", "H. Samet", "T. Goldstein"], "venue": "arXiv preprint arXiv:1706.02379.", "citeRegEx": "Li et al\\.,? 2017a", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Pruning filters for efficient convnets", "author": ["H. Li", "A. Kadav", "I. Durdanovic", "H. Samet", "H.P. Graf"], "venue": "ICLR.", "citeRegEx": "Li et al\\.,? 2017b", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Why deep neural networks for function approximation? ICLR", "author": ["S. Liang", "R. Srikant"], "venue": null, "citeRegEx": "Liang and Srikant,? \\Q2017\\E", "shortCiteRegEx": "Liang and Srikant", "year": 2017}, {"title": "Face model compression by distilling knowledge from neurons", "author": ["P. Luo", "Z. Zhu", "Z. Liu", "X. Wang", "X Tang"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2016}, {"title": "Least squares generative adversarial networks", "author": ["X. Mao", "Q. Li", "H. Xie", "R.Y. Lau", "Z. Wang", "S.P. Smolley"], "venue": "arXiv preprint ArXiv:1611.04076.", "citeRegEx": "Mao et al\\.,? 2016", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784.", "citeRegEx": "Mirza and Osindero,? 2014", "shortCiteRegEx": "Mirza and Osindero", "year": 2014}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": "ICML.", "citeRegEx": "Odena et al\\.,? 2017", "shortCiteRegEx": "Odena et al\\.", "year": 2017}, {"title": "XNOR-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ECCV, 525\u2013542. Springer.", "citeRegEx": "Rastegari et al\\.,? 2016", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "ICML.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "ICLR.", "citeRegEx": "Romero et al\\.,? 2015", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Training constrained deconvolutional networks for road scene semantic segmentation", "author": ["G. Ros", "S. Stent", "P.F. Alcantarilla", "T. Watanabe"], "venue": "arXiv preprint arXiv:1604.01545.", "citeRegEx": "Ros et al\\.,? 2016", "shortCiteRegEx": "Ros et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Depth-width tradeoffs in approximating natural functions with neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "ICML, 2979\u2013 2987.", "citeRegEx": "Safran and Shamir,? 2017", "shortCiteRegEx": "Safran and Shamir", "year": 2017}, {"title": "Deep model compression: Distilling knowledge from noisy teachers", "author": ["B.B. Sau", "V.N. Balasubramanian"], "venue": "arXiv preprint arXiv:1610.09650.", "citeRegEx": "Sau and Balasubramanian,? 2016", "shortCiteRegEx": "Sau and Balasubramanian", "year": 2016}, {"title": "In teacher we trust: Learning compressed models for pedestrian detection", "author": ["J. Shen", "N. Vesdapunt", "V.N. Boddeti", "K.M. Kitani"], "venue": "arXiv preprint arXiv:1612.00478.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman,? 2014", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Distral: Robust multitask reinforcement learning", "author": ["Y.W. Teh", "V. Bapst", "W.M. Czarnecki", "J. Quan", "J. Kirkpatrick", "R. Hadsell", "N. Heess", "R. Pascanu"], "venue": "arXiv preprint arXiv:1707.04175.", "citeRegEx": "Teh et al\\.,? 2017", "shortCiteRegEx": "Teh et al\\.", "year": 2017}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485.", "citeRegEx": "Telgarsky,? 2016", "shortCiteRegEx": "Telgarsky", "year": 2016}, {"title": "Do deep convolutional nets really need to be deep and convolutional? ICLR", "author": ["G. Urban", "K.J. Geras", "S.E. Kahou", "O. Aslan", "S. Wang", "R. Caruana", "A. Mohamed", "M. Philipose", "M. Richardson"], "venue": null, "citeRegEx": "Urban et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2017}, {"title": "Deeply-fused nets", "author": ["J. Wang", "Z. Wei", "T. Zhang", "W. Zeng"], "venue": "arXiv preprint arXiv:1605.07716.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["S. Xie", "R. Girshick", "P. Doll\u00e1r", "Z. Tu", "K. He"], "venue": "CVPR.", "citeRegEx": "Xie et al\\.,? 2017", "shortCiteRegEx": "Xie et al\\.", "year": 2017}, {"title": "Stabilizing adversarial nets with prediction methods", "author": ["A. Yadav", "S. Shah", "Z. Xu", "D. Jacobs", "T. Goldstein"], "venue": "arXiv preprint arXiv:1705.07364.", "citeRegEx": "Yadav et al\\.,? 2017", "shortCiteRegEx": "Yadav et al\\.", "year": 2017}, {"title": "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning", "author": ["J. Yim", "D. Joo", "J. Bae", "J. Kim"], "venue": "CVPR.", "citeRegEx": "Yim et al\\.,? 2017", "shortCiteRegEx": "Yim et al\\.", "year": 2017}, {"title": "Learning from multiple teacher networks", "author": ["S. You", "C. Xu", "C. Xu", "D. Tao"], "venue": "KDD, 1285\u20131294. ACM.", "citeRegEx": "You et al\\.,? 2017", "shortCiteRegEx": "You et al\\.", "year": 2017}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146.", "citeRegEx": "Zagoruyko and Komodakis,? 2016", "shortCiteRegEx": "Zagoruyko and Komodakis", "year": 2016}, {"title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "ICLR.", "citeRegEx": "Zagoruyko and Komodakis,? 2017", "shortCiteRegEx": "Zagoruyko and Komodakis", "year": 2017}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "ICLR.", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Rocket launching: A universal and efficient framework for training well-performing light net", "author": ["G. Zhou", "Y. Fan", "R. Cui", "W. Bian", "X. Zhu", "K. Gai"], "venue": "arXiv preprint arXiv:1708.04106.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 30, "context": "For one of the core applications in computer vision, large-scale image classification (Russakovsky et al. 2015), the accuracy reached by DNNs has become comparable to humans on several benchmark datasets.", "startOffset": 86, "endOffset": 111}, {"referenceID": 9, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 44, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 40, "context": "Despite the clear performance boost of modern DNNs (He et al. 2016; Zagoruyko and Komodakis 2016; Xie et al. 2017), the heavy computation and memory cost of these deep and wide networks makes it difficult to directly deploy the trained networks on embedded system for real-time applications.", "startOffset": 51, "endOffset": 114}, {"referenceID": 6, "context": "Do DNNs really need to be deep and wide? Early theoretical studies (Cybenko 1989; Hornik, Stinchcombe, and White 1989) suggest that shallow networks can approximate arbitrary functions.", "startOffset": 67, "endOffset": 118}, {"referenceID": 7, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 37, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 21, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 31, "context": "More recent theorems show depth is indeed beneficial for the expressive capacity of networks (Eldan and Shamir 2016; Telgarsky 2016; Liang and Srikant 2017; Safran and Shamir 2017).", "startOffset": 93, "endOffset": 180}, {"referenceID": 46, "context": "Moreover, the overparameterized and redundant networks, which can easily memorize and overfit the training data, surprisingly generalize well in practice (Zhang et al. 2017; Arpit et al. 2017).", "startOffset": 154, "endOffset": 192}, {"referenceID": 1, "context": "On the other hand, empirical studies suggest that the performance of shallow networks can be improved by learning from large networks following the student-teacher strategy (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 173, "endOffset": 288}, {"referenceID": 38, "context": "On the other hand, empirical studies suggest that the performance of shallow networks can be improved by learning from large networks following the student-teacher strategy (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 173, "endOffset": 288}, {"referenceID": 1, "context": "In the previous works, (Ba and Caruana 2014; Urban et al. 2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al.", "startOffset": 23, "endOffset": 63}, {"referenceID": 38, "context": "In the previous works, (Ba and Caruana 2014; Urban et al. 2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al.", "startOffset": 23, "endOffset": 63}, {"referenceID": 28, "context": "2017) train shallow and wide student networks that potentially have more parameters than the deep teacher networks; (Hinton, Vinyals, and Dean 2015) use ensemble of networks as teacher, and train student network with similar architecture and capacity; particularly, (Romero et al. 2015) train a small deep and thin network to replace a shallow and wide network for acceleration, given the best teacher at that time is the shallow and wide VGGNet (Simonyan and Zisserman 2014).", "startOffset": 266, "endOffset": 286}, {"referenceID": 34, "context": "2015) train a small deep and thin network to replace a shallow and wide network for acceleration, given the best teacher at that time is the shallow and wide VGGNet (Simonyan and Zisserman 2014).", "startOffset": 165, "endOffset": 194}, {"referenceID": 9, "context": "ResNet (He et al. 2016) has significantly deepened the networks by introducing residual connections, and wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) suggest to widen the networks for better performance.", "startOffset": 7, "endOffset": 23}, {"referenceID": 44, "context": "2016) has significantly deepened the networks by introducing residual connections, and wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) suggest to widen the networks for better performance.", "startOffset": 117, "endOffset": 147}, {"referenceID": 15, "context": "Our learning loss approach is inspired by the recent success of conditional adversarial networks for various imageto-image translation applications (Isola et al. 2017).", "startOffset": 148, "endOffset": 167}, {"referenceID": 26, "context": "Low precision methods use limited number of bits to store and operate the network weights, and the extreme case is binary networks that only use 1-bit to represent each number (Rastegari et al. 2016).", "startOffset": 176, "endOffset": 199}, {"referenceID": 20, "context": "Networks can also be directly modified by pruning and factorizing the redundant weights, either as a post-processing after training, or as a fine-tuning stage (Li et al. 2017b; Howard et al. 2017).", "startOffset": 159, "endOffset": 196}, {"referenceID": 12, "context": "Networks can also be directly modified by pruning and factorizing the redundant weights, either as a post-processing after training, or as a fine-tuning stage (Li et al. 2017b; Howard et al. 2017).", "startOffset": 159, "endOffset": 196}, {"referenceID": 1, "context": "(Ba and Caruana 2014; Urban et al. 2017) trained shallow but wide student by learning from a deep teacher, which were not primarily designed for acceleration.", "startOffset": 0, "endOffset": 40}, {"referenceID": 38, "context": "(Ba and Caruana 2014; Urban et al. 2017) trained shallow but wide student by learning from a deep teacher, which were not primarily designed for acceleration.", "startOffset": 0, "endOffset": 40}, {"referenceID": 29, "context": "The variants of knowledge distillation has also been applied to many different tasks, such as semantic segmentation (Ros et al. 2016), pedestrian detection (Shen et al.", "startOffset": 116, "endOffset": 133}, {"referenceID": 33, "context": "2016), pedestrian detection (Shen et al. 2016), face recognition (Luo et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 22, "context": "2016), face recognition (Luo et al. 2016), metric learning(Chen, Wang, and Zhang 2017), reinforcement learning (Teh et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 36, "context": "2016), metric learning(Chen, Wang, and Zhang 2017), reinforcement learning (Teh et al. 2017) and for regularization(Sau and Balasubramanian 2016).", "startOffset": 75, "endOffset": 92}, {"referenceID": 32, "context": "2017) and for regularization(Sau and Balasubramanian 2016).", "startOffset": 28, "endOffset": 58}, {"referenceID": 28, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 39, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 45, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 42, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 13, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 47, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 43, "context": "Another line of research focuses on transferring intermediate features instead of soft targets from teacher to student (Romero et al. 2015; Wang et al. 2016; Zagoruyko and Komodakis 2017; Yim et al. 2017; Huang and Wang 2017; Zhou et al. 2017; You et al. 2017).", "startOffset": 119, "endOffset": 260}, {"referenceID": 8, "context": "Generative adversarial networks (GAN) has been extensively studied over recent years since (Goodfellow et al. 2014).", "startOffset": 91, "endOffset": 115}, {"referenceID": 24, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 15, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 27, "context": "We apply GAN in the conditional setting (Mirza and Osindero 2014; Isola et al. 2017; Reed et al. 2016; Odena, Olah, and Shlens 2017), where the generator is conditioned on input images.", "startOffset": 40, "endOffset": 132}, {"referenceID": 44, "context": "Figure 1: Blocks with residual connection for convolutional neural networks (Zagoruyko and Komodakis 2016) (left) and multi-layer perceptron (right).", "startOffset": 76, "endOffset": 106}, {"referenceID": 14, "context": "The first layer is 16 filters of 3\u00d7 3 convolution, followed by a stack of 6n layers, where n is the number of residual blocks and each block contains two convolution layers equipped with batch normalization (Ioffe and Szegedy 2015), ReLU (Krizhevsky, Sutskever, and Hinton 2012) and dropout (Srivastava et al.", "startOffset": 207, "endOffset": 231}, {"referenceID": 35, "context": "The first layer is 16 filters of 3\u00d7 3 convolution, followed by a stack of 6n layers, where n is the number of residual blocks and each block contains two convolution layers equipped with batch normalization (Ioffe and Szegedy 2015), ReLU (Krizhevsky, Sutskever, and Hinton 2012) and dropout (Srivastava et al. 2014).", "startOffset": 291, "endOffset": 315}, {"referenceID": 44, "context": "Table 1: The architecture of wide residual networks (Zagoruyko and Komodakis 2016).", "startOffset": 52, "endOffset": 82}, {"referenceID": 1, "context": "Rich information is embedded in the output probability of a teacher network, and we can use logits to transfer the knowledge to student network (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 144, "endOffset": 259}, {"referenceID": 38, "context": "Rich information is embedded in the output probability of a teacher network, and we can use logits to transfer the knowledge to student network (Bucilu, Caruana, and Niculescu-Mizil 2006; Ba and Caruana 2014; Urban et al. 2017; Hinton, Vinyals, and Dean 2015).", "startOffset": 144, "endOffset": 259}, {"referenceID": 15, "context": "First, the learned loss can be effective, as has already been demonstrated for several image to image translation tasks (Isola et al. 2017).", "startOffset": 120, "endOffset": 139}, {"referenceID": 8, "context": "The adversarial loss LA for knowledge distillation, which follows the original GAN (Goodfellow et al. 2014), faces two major issues.", "startOffset": 83, "endOffset": 107}, {"referenceID": 23, "context": "Even if we replace the log-likelihood with advanced techniques such as Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017) and Least Squares GAN (Mao et al. 2016), the training is still slow and unstable in our experiments.", "startOffset": 147, "endOffset": 164}, {"referenceID": 3, "context": "To tackle these problems, we modify the discriminator objective to also predict the class labels, inspired by (Chen et al. 2016; Odena, Olah, and Shlens 2017).", "startOffset": 110, "endOffset": 158}, {"referenceID": 17, "context": "We consider three benchmark datasets for image classification: ImageNet32 (Chrabaszcz, Loshchilov, and Hutter 2017), CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton 2009).", "startOffset": 140, "endOffset": 168}, {"referenceID": 30, "context": "ImageNet32 is a downsampled version of the ImageNet2012 challenge dataset (Russakovsky et al. 2015), which contains 1.", "startOffset": 74, "endOffset": 99}, {"referenceID": 9, "context": "In all the experiments, light data augmentation with horizontal flip, padding and cropping is used for input images as in (He et al. 2016).", "startOffset": 122, "endOffset": 138}, {"referenceID": 44, "context": "We use wide residual networks (WRNs) (Zagoruyko and Komodakis 2016) as both student and teacher networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 38, "context": "(Urban et al. 2017) asked similar question for convolutional neural networks and claimed the network should at least has a few layers of convolutions.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "There is an increasing interest on accelerating neural networks for real-time applications. We study the studentteacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student.", "creator": "TeX"}}}