{"id": "1709.01193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Compositional Approaches for Representing Relations Between Words: A Comparative Study", "abstract": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, noun-modifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In the same way, data sparseness is seen as an extremely common feature in language processing tasks such as, grammar analysis and, in some cases, linguistics.\n\n\n\n\nIn a new study, the New York University Press found that one of the more important tasks in our language processing tasks, which has been described as the \u201ctolerance of language processing tasks\u201d is to determine a group of words. A meta-analysis of this work found that there are several areas of the problem of the ability to describe linguistic fluency and, in particular, the ability to predict and interpret a complex linguistic expression.\nOne of these areas of the problem is the degree of linguistic ambiguity.\nThe study was published in a open-access journal on the journal Proceedings of the National Academy of Sciences.", "histories": [["v1", "Mon, 4 Sep 2017 23:30:22 GMT  (1185kb,D)", "http://arxiv.org/abs/1709.01193v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huda hakami", "danushka bollegala"], "accepted": false, "id": "1709.01193"}, "pdf": {"name": "1709.01193.pdf", "metadata": {"source": "CRF", "title": "Compositional Approaches for Representing Relations Between Words: A Comparative Study", "authors": ["Huda Hakami", "Danushka Bollegala"], "emails": ["h.a.hakami@liv.ac.uk", "danushka.bollegala@liv.ac.uk"], "sections": [{"heading": null, "text": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, nounmodifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words. This study aims to compare different operations for creating relation representations from word-level representations. We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy. Moreover, we evaluate the different relation representations in a knowledge base completion task.\nKeywords: Relation representations, Compositional semantics, Semantic relations, Relational similarity."}, {"heading": "1. Introduction", "text": "Different kinds of semantic relations exist between words such as synonymy, antonymy, meronymy, hypernymy, etc. Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3]. To answer analogical questions of the form \u201ca is to b as c is to d \u201d, the relationship between the two words in each\n\u2217Corresponding author Email addresses: h.a.hakami@liv.ac.uk (Huda Hakami),\ndanushka.bollegala@liv.ac.uk (Danushka Bollegala)\nPreprint submitted to Elsevier September 6, 2017\nar X\niv :1\n70 9.\n01 19\n3v 1\n[ cs\n.C L\n] 4\nS ep\npair (a, b) and (c, d) must be identified and compared. For example, (lion, cat) is relationally analogous to (ostrich, bird) because a lion is a large cat as an ostrich is a large bird. In relational information retrieval, given the query a is to b as c is to? we would like to retrieve entities that have a semantic relationship with c similar to that between a and b. For example, given the relational search query Bill Gates is to Microsoft as Steve Jobs is to?, a relational search engine [4] is expected to return the result Apple Inc.\nA popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6]. In a text corpus, relationships between words are categorised by the patterns in which they co-occur, for instance \u201ca is a b\u201d or \u201cb such as a \u201d patterns indicate that a is a hyponym of b. Following the Vector Space Model (VSM) [7], each pair of words is represented using a vector of pattern frequencies where the elements correspond to the number of times the two words in a given pair co-occur with a particular pattern. This representation allows us to measure the relational similarity between two given pairs of words by the cosine of the angle between the corresponding pattern-frequency vectors. We call this approach the holistic approach, because the pairs of words are treated as a whole rather than individually. This method achieved humanlevel performance for measuring relational similarity on Scholastic Aptitude Test multiple-choice word analogy questions. The average score reported for the US college applicants is 57.0%, whereas Latent Relational Analysis (LRA), a state-of-the-art algorithm for measuring relational similarity using the holistic approach, obtained a score of 56.1% [5, 8].\nDespite the holistic method achieving human-level performance, especially for relational similarity prediction tasks, a major drawback of the holistic approach is the data sparseness. Most of the elements in pair-pattern vector space have zero occurrences, because most related words co-occur only with a small fraction of the extracted patterns. Moreover, not every related word pair cooccur even in a large corpus. Therefore the relations that exist between words that co-occur rarely cannot be adequately represented. Another limitation of this approach is its scalability, as we must consider co-occurrences between patterns and all pairs of words. The number of all pair-wise combinations between words grows quadratically with the number of words in the vocabulary. Therefore, it is computationally costly, especially if the vocabulary size is very large (> 106) and new words are continuously proposed because for each new word, we must pair it with existing words in the vocabulary. Furthermore, a continuously increasing set of patterns is required in order to cover the relations that exist between the two words in each of those word-pairs.\nTo overcome the above mentioned issues in the holistic approach, an alternative method that does not rely on pair-pattern co-occurrences is required. Such alternative methods must be able to represent the semantic relations that exist between all possible pairings of words, requiring only semantic representations for the constituent words. In this paper, we call such approaches for representing the relationship between words as compositional approaches, because the way in which the relation representation is composed using the semantic\nrepresentations of the constituent words. Different approaches have been proposed in the NLP community for representing the meaning of individual words based on the distributional hypothesis [9], which states that the meaning of a word can be predicted by the words that co-occur with it in different contexts. Counting-based approaches [10] represent the meaning of a word by a potentially high-dimensional sparse vector, where each dimension corresponds to a particular word that co-occurs with the word under consideration in some context. The values of the dimensions are computed using some word association measure such as the pointwise mutual information or log-likelihood ratio [11].\nPrediction-based approaches have also been used for representing the meanings of words using vectors [12, 13]. Instead of counting the co-occurrences of a target word in its context, Neural Network Language Model (NNLM) [14] uses distributional information in a corpus to maximise the probability of predicting the target word from the surrounding context. This procedure embeds the words into a low-dimensional latent dense vector space model. Mikolov et al. [15] show that the learnt word embeddings using recurrent neural network language model [16] captures linguistic regularities by simply applying vector offset and addition operators. They evaluate the accuracy of the learnt word representation by applying them to solve word analogy questions of the form \u201ca is to b as c is to d \u201d, where d is unknown and it is typically selected from a subset of words from the vocabulary such that vb \u2212 va + vc \u2248 vd (we denote the vector representing the word a as va). Arguably, one of the most popular examples is the following: vking \u2212 vman + vwoman \u2248 vqueen, which describes a gender relationship.\nIn compositional approaches, the meaning of longer lexical units such as phrases or sentences are composed by applying some operators on the semantic representations for individual words. The principle of compositionality states that the meaning of the hole is a function of the meaning of the parts [17]. Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20]. However, the problem of representing the meaning of a sentence differs from our problem, representing the relation between two words, in several important ways. First, a sentence would often contain more than two words, whereas we consider word pairs that always contain exactly two words. Second, a good sentence representation must encode the meaning of the sentence in its entirety, ideally capturing the meanings of salient content words in the sentence. On the other hand, in relation representation, we are not interested in the meanings of individual words, but the relationship between the two words in a word pair. For example, given the word pair (ostrich, bird), the semantics associated with ostrich or bird is not of interest to us. We would like to represent the relation is-a-large that holds between the two words in this example. It is true that most of the compositional operators that have been proposed in prior work on sentence representations such as vector addition or element-wise multiplication could be used to create relation representations for word pairs, but there is no guarantee that the exact same operators that have found to be effective for sentence representation will be accurate for relation\nrepresentation. As we see later in our experiments, vector offset, which does not scale up to sentences turns out to be a better operator for relation representation.\nIn this paper, we explore several compositional approaches for creating representations for the relations between words. In brief, we need a function that takes two vector representations for each word in a given word-pair to generate a vector for the relation that exists between those words. Our contributions in this work can be summarised as follows:\n\u2022 An empirical comparison of the unsupervised compositional operators (offset, concatenation, addition and element-wise multiplication) to represent relations between words.\n\u2022 Investigate the performance of those operators on relational similarity and a relational classification tasks using five different word-analogy benchmark datasets.\n\u2022 Evaluate such operators on a knowledge base completion task.\n\u2022 Understand to what extent the performance of those methods change across different word representation methods including counting-based and predicting-based approaches.\n\u2022 Systematically examine how the performance of different compositional operators are affected by the dimensionality of the word embeddings.\nOur study shows that the offset operator for relational compositionality outperforms other compositional operators on word-analogy datasets. For knowledge base completion, element-wise multiplication shows its ability to capture relations between entity embeddings for a given knowledge graph."}, {"heading": "2. Related work", "text": "Representing the meaning of individual words has received a wide attention in NLP. Different representation methods have been proposed using the distributional semantics of the words in a corpus to obtain a vector space model of semantics where each word is represented in term of its surrounding lexical contexts. The distributional hypothesis is summarised by Firth [21] as follows \u201cYou shall know a word by the company it keeps\u201d, which means that the words that appear in similar contexts share similar meanings. The traditional count-based word representations count the co-occurrences of a word with its neighbouring words in a specific window size. In practice however this method generates high dimensional and sparse vectors [22, 11].\nRecently, instead of counting the occurrences between words and contexts, machine learning techniques have been applied in NLP to directly learn dense words vectors by predicting the occurrence of a word in a given context. For example, skip-gram and continuous bag-of-words models learn vectors that maximise the likelihood of co-occurrence contexts in a corpus [12]. The word representations learnt via prediction-based methods are often referred to as words\nembeddings because the words are represented (embedded) using vectors in some lower-dimensional space. In addition to the fact that the learnt semantic space represents semantically similar words close to each other, Mikolov et al. [15] report that word embeddings capture relational information between words by simple linear offset between words vectors. In their study, they propose an analogical reasoning task to evaluate word embeddings. To answer analogical questions of the form \u201ca is to b as c is to ? \u201d, they subtract the embedding of word b from a and then add the embedding of c. Next, a word in the entire vocabulary set that is the most similar to the generated vector is selected as the answer. They refer to this method for solving analogy as 3CosAdd. Following this work, alternative methods have been proposed and compared with 3CosAdd for analogical reasoning [23, 24, 25]. These prior studies focus on proposing methods for solving word analogy problems given word embeddings but do not consider composing representations for the relations that exist between two words in a word-pair.\nVylomova et al. [26] conduct a study to evaluate how well the offset method encodes relational information between pairs of words. They test the generalisation of the offset method across different types of relations by evaluating the relational vectors generated by the offset method in an unsupervised (clustering) task and a supervised (classification) task. They conclude that information about syntactic and semantic relations are implicitly embedded in the offset vectors, especially under supervised learning. However, they find that the offset method does not capture semantic relations to the same level of accuracy as it captures the syntactic relations.\nMany compositional operators have been proposed for the purpose of representing sentences [19, 27]. For example, Mitchell and Lapata [27] introduce additive and multiplicative models for sentence representations, whereas Nickel et al. [28] proposed circular correlation for relational composition. However, to the best of our knowledge, there exist no work that compares different compositional operators for the purpose of relation representation. To this end, our study aims to systematically evaluate how well the contribution of word embeddings to represent relations between words by comparing different compositional operators under unsupervised settings."}, {"heading": "3. Relation Composition", "text": ""}, {"heading": "3.1. Compositional operators", "text": "Our goal in this paper is to compare different compositional operators for the purpose of composing representations for the relation between two words, given the word embeddings for those two words. In this work, we assume that pre-trained word embeddings are given to us, and our task is to use those word embeddings to compose relation representations. Specifically, given a word-pair (a, b), consisting of two unigrams a and b, represented respectively by their embeddings va,vb \u2208 Rn, we propose and evaluate different compositional op-\nerators/functions that return a vector vr given by (1) that represents the relationship between a and b.\nvr = f(va,vb) (1)\nIn this paper, we limit our study to non-parametric functions f . Parametric functions that require labelled data for computing the optimal values of the parameters for generating relation representations are beyond the scope of this paper.\nWe use the following operators to construct a vector for a given pair of words:\nPairDiff: Pair Difference operator has been used by Mikolov et al. [15] for detecting syntactic and semantic analogies using the offset method. For example, given a pair of words (a, b), they argue that (vb\u2212va) produces a vector that captures the relation that exits between the two words a and b. Under the PairDiff operator, a resultant relation representation vector has the same dimensionality as the input vectors. The PairDiff operator is defined as follows:\nvr = (vb \u2212 va) (2) PairDiff captures the information related to a semantic relation by the direction of the resultant vector. Similar relations have shown to produce parallel vectors in prior work on word embedding learning [13]. Such geometric regularities are useful for NLP tasks such as solving word analogies [15].\nConcat.: The linear concatenation of two n-dimensional vectors va = (a1, . . . , an) >\nand vb = (b1, b2, . . . , bn) > produces a 2n-dimensional vector vr given by,\nvr = (a1, a2, . . . , an, b1, b2, . . . , bn) >.\nvr can then be used as a proxy for the relationship between a and b. Vector concatenation retains the information that exist in both input vectors in the resulting composed vector. In particular, vector concatenation has been found to be effective for combining multiple source embeddings to a single meta embedding [29]. However, one disadvantage of concatenation is that it increases the dimensionality of the relation representation compared to that in the input word embeddings.\nMult: We apply element-wise multiplication between va and vb such that the ith dimension of vr has the value of multiplying the i\nth dimensions of the input vectors. Applying element-wise multiplication generates a vector in which the dimensions common to both words receive non-zero values. Mult operator is defined as follows:\nvr = (va vb) vri = vaivbi\n(3)\nElement-wise multiplication has the effect of selecting dimensions that are common to the embeddings of both words for representing the relationship between those words. Prior work on compositional semantics have\nshown that element-wise multiplication to be an effective method for composing representations for larger lexical units such as phrases or sentences from elementary lexical units such as words [27]. However, element-wise multiplication has an undesirable effect when the embeddings contain negative values. For example, two negative-valued dimensions can generate a positive-valued dimension in the relational representation. If the relations are directional (asymmetric), then such changes of sign can incorrectly indicate opposite/reversed relations between words. For example, Baroni and Zamparelli [19] report that word embeddings created via singular value decomposition performs poorly when composing phrase representations because of this sign-flipping issue. As we see later in Section 5, Mult also suffers from data sparseness because if at least one of the corresponding dimensions in two word embeddings is zero (or numerically close to zero), then the resultant dimension in the composed relational vector becomes zero. Our experimental results suggest that more than negativity, sparseness is problematic for the Mult operator. However, to the best of our knowledge, the accuracy of element-wise multiplication has not been evaluated so far in the task of relation representation.\nAdd: We apply element-wise addition between va and vb such that the i th\ndimension of vr has the value of adding the i th dimensions of the input vectors, given as follows:\nvr = (va + vb) vri = vai + vbi (4)\nElement-wise multiplication and addition have been evaluated in compositional semantics for composing phrase-level or sentence-level representations from word-level representations [30, 27]. In the context of relations, a relationship might materialise between two entities because they share many attributes in common. For example, two people might become friends in social media because they discover they have many common interests. Consequently, elementwise addition and multiplication emphasise such common attributes by adding their values together when composing the corresponding relation representation. In this work, we hypothesise that some relations are formed between entities because they have common attributes. By pairwise addition or multiplication of the attributes of two given words, we are emphasising these common attributes in their relational representation.\nElement-wise operators between word vectors assume that the dimensions of the word representation space are linearly independent. Alternatively, we can consider that the dimensions are cross-correlated and use cross-dimensional operators (i.e. operators that consider ith and jth dimensions for i = j as well as i 6= j) instead of element-wise operators to create relation representations. For this purpose, given a word representation matrix W \u2208 Rm\u00d7n of m words and n dimensions, we create a correlation matrix C \u2208 Rn\u00d7n in which the Cij element is the Pearson correlation coefficient of W:,i and W:,j , (i.e., the i th and the jth\ndimensions for all of the represented words). In our preliminary experiments, for the pre-trained word embeddings we use as inputs, we found that the correlation coefficients between i, j(6= i) dimensions are close to zero, which indicates that the dimensions are indeed uncorrelated. Consequently, for the prediction-based word embeddings we used in this comparative study, we did not obtain any significant improvement in performance by using cross-dimensional operators. Therefore, in the remainder of the paper, we do not consider cross-dimensional operators."}, {"heading": "3.2. Input Word Embeddings", "text": "We consider three widely used prediction-based word embedding methods namely, Continuous Bag-of Words (CBOW), Skip-gram (SG)1[12] and Global Vector Prediction (GloVe)2 [13]. CBOW and SG models the task of learning word embeddings as predicting words that co-occur in a local contextual window. The latent dimensions in the embeddings can be seen as representing various semantic concepts that are useful for representing the meanings of words. However, unlike in counting-based word embeddings, in prediction-based word embeddings the dimensions are not explicitly associated with a particular word or a class of words. In brief, CBOW learn word embeddings by maximising the probability of predicting a target word from the surrounding context words, whereas SG aims to predict surrounding context words given a target word in some context. On the other hand, GloVe learning method considers global co-occurrences over the entire corpus. Specifically, GloVe first builds a co-occurrence matrix between words, and then learns embeddings for the words such that using the inner-product between the corresponding embeddings we can approximate the logarithm of the co-occurrence counts between the words.\nFor consistency of the comparison, we train all word embedding learning methods on the same ukWaC corpus3 which is a web-derived corpus of English consisting of ca. 2 billion words [31]. We lowercase all the text and tokenise using NLTK4. We use the publicly available implementations by the original authors of CBOW, SG, and GloVe for training the word embeddings using the recommended parameters settings. Specifically, the context window is set to 5 words before and after the target word, and words with frequency less than 6 in the corpus are ignored, resulting in a vocabulary containing 1,371,950 unique words. The negative sampling rate in SG is set to 5 words for each co-occurrence. Our vocabulary is restricted to the words that appeared more that 6 times in the corpus, resulting in a vocabulary which includes 1,371,950 unique words. Using each of the word embedding learning methods, we learn 300 dimensional word embeddings.\nIn addition to prediction-based word embeddings described above (i.e. CBOW, SG, and GloVe), we evaluate counting-based word representations for relation\n1https://code.google.com/archive/p/word2vec/ 2http://nlp.stanford.edu/projects/glove/ 3http://wacky.sslmit.unibo.it/doku.php?id=corpora 4http://www.nltk.org/_modules/nltk/tokenize.html\nrepresentation. This method assigns each word with a high-dimensional vector that captures the contexts in which it occurs. We first construct unigram counts from the ukWaC corpus. The co-occurrences between low-frequency words are rare and result in a sparse co-occurrence matrix. To avoid this issue, we consider the most-frequent 50,000 words in the corpus as our vocabulary, and consider co-occurrences between only those words. We found that a vocabulary of 50,000 frequent words to be sufficient for covering all the benchmark datasets used in the evaluations. Moreover, truncating the co-occurrence matrix to the top frequent contexts makes the dimensionality reduction methods computationally inexpensive. Then the word-word co-occurrence statistics are computed from the corpus using windows of size 5 tokens on each side of the target word. We weight the co-occurrences by the inverse of the distance between the two words measured by the number of tokens that appear between the two words. Afterwards, Positive Pointwise Mutual Information (PPMI) is computed from the co-occurrence matrix W \u2208 Rm\u00d7n as follows:\nPPMI(x, y) = max ( 0, log p(x, y)\np(x)p(y)\n) , (5)\nwhere p(x, y) is the joint probability that the two words x and y co-occurring in a given context, whereas p(x) is the marginal probability. We then apply Singular Value Decomposition (SVD) to the PPMI matrix, which factorises W as, W = USV>, where S is the singular values of W. We truncate S keeping only the top 300 singular values to reduce the dimensionality and thus increase the density of words representation. This count-based statistical method for word representations is widely applied in NLP to produce semantic representations for words and documents [32, 11].\nAs an alternative dimensionality reduction method, we use Nonnegative Matrix Factorisation (NMF) in our experiments [33]. Given a matrix W \u2208 Rm\u00d7n, NMF computes the factorisation W = GH, where G \u2208 Rm\u00d7d, and H \u2208 Rd\u00d7n, and G \u2265 0,H \u2265 0 (i.e. G and H contain non-negative elements). By setting d < min(n,m), we can obtain lower d-dimensional embeddings for the rows and columns of W, given respectively by the rows and columns in G and H. Unlike, SVD, the embeddings created using NMF are non-negative. By using non-negative embeddings in our evaluations, we can test the behaviour of the different relation composition operators under nonnegativity constraints."}, {"heading": "4. Evaluation methods", "text": "Prior work that proposed compositional operators such as Mult, Add etc. evaluate their effectiveness on semantic composition tasks. For example, Mitchell and Lapata [27, 18] used a crowd sourced dataset of phrase similarity. First, a phrase is represented by applying a particular compositional operator on the constituent word representations. Next, the similarity between two phrases is computed using some similarity measure such as the cosine similarity between the corresponding phrase representations. Finally, the computed similarity scores are compared against human similarity ratings using some correlation\nmeasure such as the Spearman or Pearson correlation coefficients. If a particular compositional operator produces a higher agreement with human similarity ratings then it is considered superior. However, our task in this paper is to measure similarity between relations and not phrases (or sentences). Therefore, this evaluation protocol is not directly relevant to us. Instead, we use word analogy detection (Section 4.1) and knowledge base completion (Section 4.3) tasks, which are more dependent on better relation representations."}, {"heading": "4.1. Relational similarity prediction", "text": "Given two word pairs (a, b) and (c, d), the task is to measure the similarity between the semantic relations that exist between the two words in each pair. This type of similarity is often referred to as relational similarity in prior work [5]. The task is to measure the degree of relational similarity between two given word-pairs (a, b) and (c, d). We need a method that assigns a high degree of relational similarity if the first pair stands in the same relation as another pair. Two benchmark datasets have been used frequently in prior work for evaluating relational similarity measures are SAT [34] dataset and SemEval 2012-Task25 [35] dataset. Next, we briefly describe the protocol for evaluating relational similarity measures using those datasets.\nThe Scholastic Aptitude Test (SAT) word analogy dataset contains 374 multiple choice questions in which each question contains a word-pair as the stem, and the examinees are required to select the most analogous word-pair from a list of 4 or 5 candidate answer word-pairs. An example is shown in Table 1. We generate relation embeddings for the question and its choice word-pairs using a compositional operator. Next, the cosine similarity (Equation 6) between the relation representation x of the question word-pair (a, b) and the relation representation y of each of the candidate word-pairs (c, d) is computed to select the candidate with the highest similarity score as the correct answer. Cosine similarity between two vectors is defined as follows:\nsim(x,y) = cos(\u03b8) = x>y\n||x|| ||y|| (6)\nThe recorded accuracy is the ratio of the number of questions that are answered correctly to the total number of the questions in the dataset. Because there are five candidate answers out of which only one is correct, random guessing would give a 20% accuracy.\nSemEval 2012 Task-2 covers 10 categories of semantic relations, each with a number of subcategories. In total the dataset has 79 subcategories. Each subcategory (relation) has approximately 41 word pairs and three to four prototypical examples. Example word-pairs from the SemEval dataset are illustrated in Table 2. The task here is to assign a score to each word-pair, which indicates the average of the relational similarity between the given word-pair and prototypical word-pairs in a subcategory.\n5https://sites.google.com/site/semeval2012task2/\nAn alternative approach for measuring the accuracy of a relation embedding method is to apply the relation embedding to complete word analogies. measuring relational similarity could be evaluated in terms of completing an analogy a : b :: c :?. In other words, we must find the fourth (missing) word d from a fixed vocabulary such that the relational similarity between (a, b) and (c, d) is maximised. Equation 7 uses the PairDiff operator for representing the relation between two words, and use cosine similarity to measure the relational similarity between the two word-pairs. Likewise, we can use the other compositional operators Add and Mult to first create a relational embedding and then use cosine similarity to measure relational similarity.\nFor the analogy completion task we use two datasets: MSR [15], and Google analogy [36] datasets. MSR dataset contains 8,000 proportional analogies covering 10 different syntactic relations, whereas the Google contains 19,544 analogical word-pairs covering 9 syntactic and 4 semantic relation types, corresponding to 10,675 syntactic and 8,869 semantic analogies. We restrict the search space for the missing word to the words that appear in a large set of vocabulary consists of 13,609 words in ukWaC, excluding the three words for each question.\nd\u2217 = arg max d\u2208V (cos(vb \u2212 va,vd \u2212 vc)) (7)"}, {"heading": "4.2. Relation classification", "text": "In relation classification, the problem is to classify a given pair of words (w1, w2) to a specific relation r in a predefined set of relations R according to the relation that exists between w1 and w2 . We use the DiffVecs dataset proposed by Vylomova et al. [26] that consists of 12,458 triples \u3008w1, w2, r\u3009, where word w1 and w2 are connected by a relation r. The relation set R includes 15\nrelation types comprising lexical semantic relations, morphosyntactic paradigm relations and morphosemantic relations.6\nWe use the different compositional operators discussed in Section 3.1 to represent each word-pair by a relational embedding. We then perform 1-nearest neighbour (1-NN) classification in this relational embedding space to classify the test word-pairs into the relation types. If the nearest neighbour has the same relation label as the target word-pair, then we consider it to be a correct classification. The classification accuracy is computed as follows:\nAccuracy = correct matches\ntotal number of pairs (8)\nWe experimented using both unnormalised word embeddings as well as `2 normalised word embeddings. We found that `2 normalised word embeddings perform better than the unnormalised version in most configurations. Consequently, we report results obtained only with the `2 normalised word embeddings in the remainder of the paper."}, {"heading": "4.3. Knowledge base completion", "text": "Knowledge graphs such as WordNet and FreeBase that link entities according to numerous relation types that hold between entities are important resources for numerous NLP tasks such as question answering, entity and relation extraction. Automatic knowledge base completion attempts to overcome the incompleteness of such knowledge bases by predicting missing relations in a knowledge base. For instance, given a first entity (also known as the head entity h) and a relation type r, we need to predict a second entity (also known as the tail entity t) such that h and t are related by r.\nTo evaluate the unsupervised compositional operators for the knowledge base completion task, we apply the following procedure. First, we require pretrained entity embeddings as the input to a compositional operator. Translating embeddings (TransE) model is one of the popular methods for learning entity representations from a given knowledge graph [37]. In TransE, if (h, r, t) holds, then the entity embeddings are learnt such that: h + r \u2248 t. We consider two knowledge bases frequently used in prior work on knowledge base completion [37]. Namely, WordNet (WN18) and FreeBase (FB15k). The datasets and the source code that generates entity embeddings are publicly available [38]7.\nTo evaluate the accuracy of a relation composition operator f , we first create a representation ri for each relation type r using the entity pairs(h, t) in the training data by applying f to the embeddings of the two entities h and t as follows:\nr = 1 |R| \u2211\n(h,r,t)\u2208R\nf(h, t) (9)\nHere, R is the set of pairs of entities that are related by ri.\n6https://github.com/ivri/DiffVec 7https://github.com/thunlp/KB2E\nNext, for each test triple (h\u2032, r\u2032, t\u2032), we rank the candidate tail entities t\u2032 according to the cosine similarity between each of the relation embedding r\u2032 of the relation r\u2032 computed using (9), and the result of applying f to the entity embeddings h\u2032 and t\u2032. The cosine similarity score we used to rank candidate tail entities is given by,\ncos(r, f(h\u2032, t\u2032)). (10)\nWe rank all tail entities in all test entity pairs according to (10) and select the top-ranked entity as the correct completion. This process is repeatedly applied for predicting the head entities for each test triple as well.\nIf the correct tail (or head) entity (according to the original test tuple) can be accurately predicted using the relation embeddings created by applying a particular compositional operator, then we can conclude that operator to be accurately capturing the relational information. Following prior work on knowledge base completion, we use two measures for evaluating the predicted tail (or head) entities: Mean Rank and Hits@10. Mean rank is the average rank assigned to the correct tail (or head) entity in the ranked listed of candidate entities according to (10). A lower mean rank is better because the correct candidate is ranked at the top by the compositional operator under evaluation. Hits@10 is the proportion of correct entities that have been ranked among the top 10 candidates. It is noteworthy that our purpose here is not to propose state-of-the-art knowledge base completion methods. We are using knowledge base completion simply as an evaluation task to compare different compositional operators, whereas the prior works in knowledge base completion learn entity and relation embeddings that can accurately predict the missing relations in a knowledge base."}, {"heading": "5. Experimental results", "text": ""}, {"heading": "5.1. Performance of Relational Similarity Task", "text": "In Table 3, we compare the four compositional operators (PairDiff, Concat, Add and Mult) described in Section 3.1 for the four different word representation models as described in Section 3.2. We observe that PairDiff achieves the best results compared with other operators for all the evaluated datasets and all the word representation methods. PairDiff is significantly better than Add or Mult for all embeddings (both prediction- and counting-based) in MSR, Google and DiffVec datasets according to Clopper-Pearson confidence intervals (p < 0.05). SAT is the smallest dataset among all, so we were unable to see any significant differences on SAT.\nAnalogy completion in Google and MSR datasets are considered as an open vocabulary task because to answer a question of the form \u201ca is to b as c is to ?\u201d, we must consider all the words in the corpus as candidates, which is an open vocabulary, not limited to the words that appear in the benchmark datasets as in SAT or SemEval datasets. Therefore, applying PairDiff to each pair (a, b) and (c, d) will retrieve candidates d that have relations with c similar to the relation between a and b, but not necessary similar to the word c. For\ninstance, the top 3 ranked candidates for a question \u201cman is to woman as king is to ?\u201d are women, pregnant and maternity. We notice that the top ranked candidates indicate feminine entities. This explains the performance of PairDiff on MSR and Google datasets, which is lower compared with other datasets. Similar observations have been made by Levy et al. [23]. Moreover, the open vocabulary task (Google and MSR) is harder than the closed vocabulary task (SAT, SemEval and DiffVecs) as the number of incorrect candidates is much larger in the open vocabulary setting. This means that the probability of accidentally retrieving a noisy negative candidate as the correct answer is higher than in the closed vocabulary task.\nMult is performing slightly worse with NMF compared to other embeddings. Recall that NMF produces non-negative embeddings and Mult is performing an elementwise multiplication operation on the two input word embeddings to create the embeddings for their relation. If the negativity was the only issue with Mult operator as previously suggested by [19], then Mult should have performed better with NMF. We hypothesise the issue here is sparsity in the relation representations. To test our hypothesis empirically we conduct the following experiment.\nFirst, we randomly select 140 word-pairs from the Google dataset and apply different compositional operators to create relation embeddings for each wordpair using 300 dimensional CBOW word embeddings as the input. Next, we measure the average sparsity of the set of relational embeddings created by each\noperator. We define sparsity at a particular cut-off level for a d dimensional vector as the percentage of elements with absolute value less than or equal to out of d. Formally, sparsity is given by (11).\nsparsity = 1\nd d\u2211 i=1 I[|xi|\u2264 ] (11)\nHere, I is the indicator function which returns 1 if the expression evaluated is true, or 0 otherwise. Our definition of sparsity is a generalisation of the `0 norm that counts the number of non-zero elements in a vector. However, in practice, exact zeros will be rare and we need a more sensitive measure of sparsity, such as the one given in (11). Average sparsity is computed by dividing the sum of sparsity values given by (11) for the set of word-pairs by the number of word-pairs in the set (i.e. 140).\nFigure 1 shows the average sparsity values for different operators under different levels. Figure 1 shows that Mult operator generates sparse vectors for relations compared to other operators under all values. Considering that Mult is performing a conjunction over the two input word embeddings, even if at least one embedding has a nearly zero dimension, after elementwise multiplication we are likely to be left with nearly zero dimensions in the relation embedding. Such sparse representations become problematic when measuring cosine similarity between relation embeddings, which leads to poor performances in word analogy tasks."}, {"heading": "5.2. Effect of Dimensionality", "text": "The dimensionality of the relational embeddings produced by the compositional operators presented in Section 3.1 depends on the dimensionality of\nthe input word embeddings. For example, Mult, Add, and PairDiff operators produce relational embeddings with the same dimensionality as the input word embeddings, whereas the Concat operator produce relational embeddings twice the dimensionality of the input word embedding. A natural questions therefore is that how does the performance of the relational embeddings vary with the dimensionality of the input word embedding. To study the relationship between the dimensionality of the input word embedding and the composed relational embedding we conduct the following experiment.\nWe first train word embeddings of different dimensionalities using the ukWaC corpus. We keep all the other parameters of the word embedding learning method fixed except for the dimensionality of the word embeddings learnt. Because CBOW turned out be the single best word embedding learning method according to the results in Table 3, we use CBOW as the preffered word embedding learning method in this analysis. Figure 2 shows the performance of the different compositional operators on the benchmark datasets using CBOW input word embeddings with dimensionalities in the range 50-800.\nAs seen from Figure 2, PairDiff outperforms all other operators across all dimensionalities. The best results on SemEval and DiffVecs datasets are reported by PairDiff with 200 dimensions. Performance saturates when the dimensionality is increased beyond this point. On the other hand, SAT shows different trend. On SAT, the performance of PairDiff continuously increases with the dimensionality of the input word embeddings. On the other hand, in MSR and Google datasets we see a different trend where the performance of PairDiff decreases while that of Mult increases with the dimensionality of the input word embedding.\nTo understand the above-described trends first note that the dimensions in word embeddings are providing almost complementary information related to the semantics of a word.8 Adding more dimensions to the word embedding can be seen as a way of representing richer semantic information. However, increasing the dimensionality also increases the number of parameters that we must learn. Prediction-based word embedding learning methods first randomly initialise all the parameters and then update them such that the co-occurrences between words can be accurately predicted in a given context window. However, the training dataset, which in our case is the ukWaC corpus, is fixed. Therefore, we will have more parameters than we could reliably estimate using the data we have, resulting in some overfitted noisy dimensions as we increase the dimensionality of the word embeddings learnt.\nOne hypothesis for explaining the seemingly contradictory behaviour with PairDiff and Mult operators is as follows. When we increase the dimensionality of the input word embeddings, there will be some noisy dimensions in the input word embeddings. PairDiff operators amplifies the noise in the sense that the resultant offset vector will retain noisy high dimensions that appear in both word\n8As described in Section 3.1, Pearson correlation coefficients between different dimensions in word embeddings are small, showing that different dimensions are uncorrelated.\nembeddings. On the other hand, Mult operator can be seen as a low-pass filter where we shutdown dimensions that have small (or zero) valued dimensions in at least one of the two embeddings via the element-wise multiplication of corresponding dimensions. Therefore, Mult will be robust against the noise that exist in the higher dimensions of the word embeddings than the PairDiff operator.\nTo empirically test this hypothesis we compute the `2 norm of (va \u2212 vb) and (va vb) for word embeddings of different dimensionalities and compute the average over 140 randomly selected word-pairs. As shown in Figure 3, the norm of PairDiff relation embedding is increasing with dimensionality, whereas norm of the relation embedding generated by Mult decreases. This proves our hypothesis that Mult is filers out the noise in high dimensional word embeddings better than PairDiff."}, {"heading": "5.3. Performance on Knowledge Base Completion Task", "text": "Table 4 displays the performance on the compositional operators for the knowledge base completion task on the two knowledge graphs WN18 and FB15k, where low mean rank and high Hits@10 indicates better performance. As can be seen from the Table, Mult operator yields the lowest mean rank and the highest Hits@10 accuracy among other operators for the both knowledge bases.\nGiven that PairDiff was the best operator for relational similarity tasks, it is surprising that Mult operator outperforms PairDiff in both WN18 and FB15k datasets. Recall that knowledge base completion is the task where given that (h, t) pair is related by a relation r (as provided in the train set) we need to assess how likely that (h\u2032, t\u2032) is related by r. In our evaluation process, this task is answered by measuring the inner-product between f(h, t) and f(h\u2032, t\u2032), where f is a compositional function that represents the relationship between h and t. In the case of Mult operator, we have: similarity-score = (h t)>(h\u2032 t\u2032), this indicates that if a dimension is not common across all four entities it does not contribute to the overall similarity score. This can be seen as a strict way of estimating relational similarity between train and test pairs because a particular dimension must be on in all four words involved in an analogy.\nOn the other hand, PairDiff operator scores test entity pairs by (h\u2212t)>(h\u2032\u2212 t\u2032). Here, (h\u2032, t\u2032) is an entity pair in the test dataset with the target relation r, and we are interested in finding, for example, candidate tail entities t\u2032 that\nhas r with a given head entity h\u2032. This score can be further expanded as (h\u2212 t)>h\u2032 \u2212 (h\u2212 t)>t\u2032. The first term is fixed given the training dataset and the head entity, and the rank of the tail entity is determined purely based on (h\u2212 t)>t\u2032, where the head entity h\u2032 does not participate in. This is problematic because entities t\u2032 that are similar to t and dissimilar to h will be simply ranked at the top irrespective of the relation t\u2032 has with h\u2032. Indeed in Table 4 we see that mean rank for PairDiff is significantly higher compared to that of Mult. This suggests that many irrelevant tail (or head) entities are ranked ahead of the correct entity for each test tuple. On the other hand, in relational similarity task, the two pairs between which we must measure similarity are fixed and this issue is not\nIf a relation is asymmetric such as hypernym and hyponym as in WN18, addition model will be insensitive to the directionality of such relations compared to PairDiff which explains the better performance of PairDiff over Add."}, {"heading": "5.4. Evaluating the Asymmetry of the PairDiff Operator", "text": "Relations between words can be categorised as either being symmetric or asymmetric. If two words a and b are related by a symmetric relation r, then b is also related to a with the same relation r. Examples of symmetric relations include synonyms and antonyms. On the other hand, if a is related to b by an asymmetric relation, then b might not be necessarily related to a with the same relation r. Examples of asymmetric relations include hypernyms and meronyms. As discussed in Section 5.1, PairDiff operator outperforms Add and Mult operators. Unlike Mult and Add, which are commutative operators, PairDiff is a non-commutative operator. Therefore, PairDiff should be able to detect the direction of a relation.\nTo test the ability of PairDiff to detect the direction of a relation, we set up the following experiment. Using a set of word-pairs where there is a common directional relation r between the two words in each word pair as training data, we use PairDiff to represent the relationship between two words in a word-pair, given the word embeddings for those two words. Next, we swap the two words in each word-pair and apply the same procedure to create relation embeddings for the reversed relation r\u2032 in each word-pair. We model the task of predicting whether a given word-pair contains the original relation r or its reversed version r\u2032 as a binary classification task. Specifically, we train a binary support vector machine with a linear kernel with the cost parameter set to 1 using held-out\ndata. If the trained binary classifier can correctly predict the direction of a relation in a word-pair, then we can conclude that the relation embedding for that word-pair accurately captures the information about the direction of the relation that exists between the two words in the word-pair. We can repeat this experiment with symmetric as well as asymmetric relation types r and compare the performances of the trained classifiers to understand how well the directionality in asymmetric relations is preserved in the PairDiff embeddings.\nFor the asymmetric relation types we use all relation types in the DIFFVECS because this dataset contains only asymmetric relation types. For symmetric relation types we use two popular symmetric semantic relations namely, synonymy9 and antonymy10. We report five-fold cross-validation accuracies with each relation type in Figure 4. If the classifier reports a high classification accuracy for asymmetric relations than symmetric relations, then it indicates that the relation embedding can encode the directional information in a relation. From Figure 4 we see that, overall, the accuracies for the two symmetric relation types is lower than that for the asymmetric relation types. This result indicates that PairDiff can correctly detect the direction in the asymmetric relation types.\n9http://saifmohammad.com/WebDocs/LC-data/syns.txt 10http://saifmohammad.com/WebDocs/LC-data/opps.txt"}, {"heading": "6. Discussion and conclusion", "text": "This work evaluated the contribution of word embeddings for representing relations between pairs of words. Specifically, we considered several compositional operators such as PairDiff, Mult, Add, and Concat for creating a representation (embedding) for the relation that exist between two words, given their word embeddings as the input. We used different pre-trained word embeddings and evaluated the performance of the operators on two tasks: relational similarity measurement and knowledge base completion. We observed that PairDiff to be the best operator for relational similarity measurement task, whereas Mult operator to be the best for knowledge base completion task. We then studied the effect of dimensionality on the performance of these two operators and showed that the sparsity of the input embeddings is affecting the Mult operator, and not the negativity of the input word embedding dimensions as speculated in prior work. Our analysis in this paper was limited to unsupervised operators in the sense that there are no parameters in the operators that can be (or must be) learnt from training data. This raises the question whether we can learn better compositional operators from labelled data to further improve the performance of the compositional approaches for relation representation, which we plan to explore in our future work."}], "references": [{"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "in: Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Using relational similarity between word pairs for latent relational search on the web", "author": ["N.T. Duc", "D. Bollegala", "M. Ishizuka"], "venue": "in: IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Corpus-based learning of analogies and semantic relations", "author": ["P.D. Turney", "M.L. Littman"], "venue": "Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Cross-language latent relational search: Mapping knowledge across languages", "author": ["N.T. Duc", "D. Bollegala", "M. Ishizuka"], "venue": "in: Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Www sits the sat: Measuring relational similarity on the web", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka"], "venue": "in: ECAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Aritificial Intelligence Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Measuring semantic similarity by latent relational analysis, arXiv preprint", "author": ["P.D. Turney"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Distributional structure, The Philosophy of Linguistics", "author": ["Z. Harris"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of artificial intelligence research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Efficient estimation of word representation in vector space", "author": ["T. Mikolov", "K. Chen", "J. Dean"], "venue": "in: Proceedings of International Conference on Learning Representations,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Glove: global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in: Proceedings of Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "in: HLT-NAACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in: Interspeech,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "in: EMNLP\u201910,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A regression model of adjective-noun compositionality in distributional semantics, in: ACL\u201910", "author": ["E. Guevara"], "venue": "Workshop on Geometrical Models of Natural Language Semantics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Issues in evaluating semantic spaces using word analogies", "author": ["T. Linzen"], "venue": "arXiv preprint arXiv:1606.07736", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Word embeddings, analogies, and machine learning: Beyond king-man+ woman= queen", "author": ["A. Drozd", "A. Gladkova", "S. Matsuoka"], "venue": "in: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning, arXiv preprint", "author": ["E. Vylomova", "L. Rimmel", "T. Cohn", "T. Baldwin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "in: ACL-HLT\u201908,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Holographic embeddings of knowledge graphs", "author": ["M. Nickel", "L. Rosasco", "T. Poggio"], "venue": "in: Proc. of AAAI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning meta-embeddings by using ensembles of embedding sets", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "in: Proc. of ACL,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Language models based on semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english, in: Proceedings of the 4th Web as Corpus Workshop (WAC-4", "author": ["A. Ferraresi", "E. Zanchetta", "M. Baroni", "S. Bernardini"], "venue": "Can we beat Google,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Vector space models of lexical meaning, Handbook of Contemporary Semantic Theory, The (2015) 493\u2013522", "author": ["S. Clark"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Algorithms for non-negative matrix factorization, in: Advances in neural information processing", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Combining independent modules to solve multiple-choice synonym and analogy problems", "author": ["P. Turney", "M.L. Littman", "J. Bigham", "V. Shnayder"], "venue": "in: Proceedings of the Recent Advances in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "in: NIPS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "in: AAAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "Identifying the semantic relations between words (or entities) is important for various Natural Language Processing (NLP) tasks such as knowledge base completion [1], relational information retrieval [2] and analogical reasoning [3].", "startOffset": 229, "endOffset": 232}, {"referenceID": 3, "context": "For example, given the relational search query Bill Gates is to Microsoft as Steve Jobs is to?, a relational search engine [4] is expected to return the result Apple Inc.", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 4, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 5, "context": "A popular approach for representing the relations that exist between pairs of words is to extract the lexical patterns in which the pairs of words co-occur in some context [3, 5, 6].", "startOffset": 172, "endOffset": 181}, {"referenceID": 6, "context": "Following the Vector Space Model (VSM) [7], each pair of words is represented using a vector of pattern frequencies where the elements correspond to the number of times the two words in a given pair co-occur with a particular pattern.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "1% [5, 8].", "startOffset": 3, "endOffset": 9}, {"referenceID": 7, "context": "1% [5, 8].", "startOffset": 3, "endOffset": 9}, {"referenceID": 8, "context": "Different approaches have been proposed in the NLP community for representing the meaning of individual words based on the distributional hypothesis [9], which states that the meaning of a word can be predicted by the words that co-occur with it in different contexts.", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Counting-based approaches [10] represent the meaning of a word by a potentially high-dimensional sparse vector, where each dimension corresponds to a particular word that co-occurs with the word under consideration in some context.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "The values of the dimensions are computed using some word association measure such as the pointwise mutual information or log-likelihood ratio [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "Prediction-based approaches have also been used for representing the meanings of words using vectors [12, 13].", "startOffset": 101, "endOffset": 109}, {"referenceID": 12, "context": "Prediction-based approaches have also been used for representing the meanings of words using vectors [12, 13].", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "[15] show that the learnt word embeddings using recurrent neural network language model [16] captures linguistic regularities by simply applying vector offset and addition operators.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] show that the learnt word embeddings using recurrent neural network language model [16] captures linguistic regularities by simply applying vector offset and addition operators.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 16, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 17, "context": "Over the years, researchers in compositional semantics have applied different compositional approaches to extend the meaning of individual words to larger linguistic units [18, 19, 20].", "startOffset": 172, "endOffset": 184}, {"referenceID": 18, "context": "In practice however this method generates high dimensional and sparse vectors [22, 11].", "startOffset": 78, "endOffset": 86}, {"referenceID": 10, "context": "In practice however this method generates high dimensional and sparse vectors [22, 11].", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "For example, skip-gram and continuous bag-of-words models learn vectors that maximise the likelihood of co-occurrence contexts in a corpus [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "[15] report that word embeddings capture relational information between words by simple linear offset between words vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Following this work, alternative methods have been proposed and compared with 3CosAdd for analogical reasoning [23, 24, 25].", "startOffset": 111, "endOffset": 123}, {"referenceID": 20, "context": "Following this work, alternative methods have been proposed and compared with 3CosAdd for analogical reasoning [23, 24, 25].", "startOffset": 111, "endOffset": 123}, {"referenceID": 21, "context": "[26] conduct a study to evaluate how well the offset method encodes relational information between pairs of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Many compositional operators have been proposed for the purpose of representing sentences [19, 27].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "Many compositional operators have been proposed for the purpose of representing sentences [19, 27].", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "For example, Mitchell and Lapata [27] introduce additive and multiplicative models for sentence representations, whereas Nickel et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "[28] proposed circular correlation for relational composition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] for detecting syntactic and semantic analogies using the offset method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Similar relations have shown to produce parallel vectors in prior work on word embedding learning [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Such geometric regularities are useful for NLP tasks such as solving word analogies [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "In particular, vector concatenation has been found to be effective for combining multiple source embeddings to a single meta embedding [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 22, "context": "shown that element-wise multiplication to be an effective method for composing representations for larger lexical units such as phrases or sentences from elementary lexical units such as words [27].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "For example, Baroni and Zamparelli [19] report that word embeddings created via singular value decomposition performs poorly when composing phrase representations because of this sign-flipping issue.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "Element-wise multiplication and addition have been evaluated in compositional semantics for composing phrase-level or sentence-level representations from word-level representations [30, 27].", "startOffset": 181, "endOffset": 189}, {"referenceID": 22, "context": "Element-wise multiplication and addition have been evaluated in compositional semantics for composing phrase-level or sentence-level representations from word-level representations [30, 27].", "startOffset": 181, "endOffset": 189}, {"referenceID": 11, "context": "We consider three widely used prediction-based word embedding methods namely, Continuous Bag-of Words (CBOW), Skip-gram (SG)[12] and Global Vector Prediction (GloVe) [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "We consider three widely used prediction-based word embedding methods namely, Continuous Bag-of Words (CBOW), Skip-gram (SG)[12] and Global Vector Prediction (GloVe) [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 26, "context": "2 billion words [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "This count-based statistical method for word representations is widely applied in NLP to produce semantic representations for words and documents [32, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 10, "context": "This count-based statistical method for word representations is widely applied in NLP to produce semantic representations for words and documents [32, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 28, "context": "As an alternative dimensionality reduction method, we use Nonnegative Matrix Factorisation (NMF) in our experiments [33].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "For example, Mitchell and Lapata [27, 18] used a crowd sourced dataset of phrase similarity.", "startOffset": 33, "endOffset": 41}, {"referenceID": 15, "context": "For example, Mitchell and Lapata [27, 18] used a crowd sourced dataset of phrase similarity.", "startOffset": 33, "endOffset": 41}, {"referenceID": 4, "context": "This type of similarity is often referred to as relational similarity in prior work [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 29, "context": "Two benchmark datasets have been used frequently in prior work for evaluating relational similarity measures are SAT [34] dataset and SemEval 2012-Task2 [35] dataset.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "For the analogy completion task we use two datasets: MSR [15], and Google analogy [36] datasets.", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "For the analogy completion task we use two datasets: MSR [15], and Google analogy [36] datasets.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "[26] that consists of 12,458 triples \u3008w1, w2, r\u3009, where word w1 and w2 are connected by a relation r.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The datasets and the source code that generates entity embeddings are publicly available [38].", "startOffset": 89, "endOffset": 93}, {"referenceID": 16, "context": "If the negativity was the only issue with Mult operator as previously suggested by [19], then Mult should have performed better with NMF.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "Identifying the relations that exist between words (or entities) is important for various natural language processing tasks such as, relational search, nounmodifier classification and analogy detection. A popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus, and assign each word-pair a vector of pattern frequencies. Despite the simplicity of this approach, it suffers from data sparseness, information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus. In contrast, a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words. This study aims to compare different operations for creating relation representations from word-level representations. We investigate the performance of the compositional methods by measuring the relational similarities using several benchmark datasets for word analogy. Moreover, we evaluate the different relation representations in a knowledge base completion task.", "creator": "LaTeX with hyperref package"}}}