{"id": "1709.02535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Mirror Descent Search and Acceleration", "abstract": "In recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learn- ing algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster. Third, the reinforcement learning algorithm generates large blocks of reward and reduces the variance between it and the reward. Finally, the algorithm reduces the variance between the reward and the reward. Finally, the learning algorithm generates large blocks of reward for the participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the learning algorithm generates large blocks of reward for participants. Finally, the algorithm creates large blocks of reward for participants. Finally, the algorithm generates large blocks of reward for participants. Finally, the algorithm generates large blocks of reward for participants. Finally, the learning algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Lastly, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates large blocks of reward for participants. Finally, the neural algorithm generates", "histories": [["v1", "Fri, 8 Sep 2017 04:43:48 GMT  (692kb)", "http://arxiv.org/abs/1709.02535v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["megumi miyashita", "shiro yano", "toshiyuki kondo"], "accepted": false, "id": "1709.02535"}, "pdf": {"name": "1709.02535.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Megumi Miyashita", "Shiro Yano", "Toshiyuki Kondo"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n02 53\n5v 1\n[ cs\n.L G\n] 8\nS ep\n2 01\nIn recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learning algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.\nKeywords: Reinforcement Learning, Mirror Descent"}, {"heading": "1. Introduction", "text": "In recent years, as stated in [1], attention has focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input x\u2217 \u2208 X that optimizes the output f (x) : a \u2192 R represented by an unknown function. Because the objective function is unknown, we solve the black box optimization problem without gradient information. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. Based on this, the solution to the black box optimization problem can be used as a solution for reinforcement learning.\nIn this research, we propose a reinforcement learning algorithm based on the mirror descent (MD) method [2]. MD is general optimization algorithm that employs a Bregman\n\u2729The research was partially supported by JSPS KAKENHI (Grant numbers JP26120005, JP16H03219, and JP17K12737).\nPreprint submitted to Robotics and Autonomous Systems September 11, 2017\ndivergence alternative to the Euclidean distance, which is the metric of gradient descent. The derivation for this is detailed in Section 2. We call our proposed method Mirror Descent Search (MDS). In addition, MDS is expected to generalize some existing reinforcement learning algorithms. This research shows (1) that the extension method in the MD can be applied to reinforcement learning, and (2) that the relationship between existing reinforcement learning algorithms can be clarified."}, {"heading": "1.1. Related works", "text": "In this section, we describe previous research and its relation to the research in this paper.\nRelative Entropy Policy Search (REPS) [? ] and its derivation method focuses on information loss during policy searches. This information loss is the relative entropy of the data distribution generated from the observation data distribution. The new policy and is set as the upper limit value. This is equivalent to defining the upper limit value of the Kullback\u2013Librar (KL) divergence between each distribution. Episode-based REPS [4] is a derivation method formalized by considering the upper-level policy. Although the equations for episode-based REPS and the proposed method are similar, our method can naturally consider distance metrics other than the KL divergence. Consequently, we can apply extension methods in MD.\nIn [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization. Specifically, [1] explains the history of black box optimization and reinforcement learning, and proposes PIBB. PIBB refers to Policy Improvement with Path Integrals (PI2) [6, 7]. It is considered a black box optimization method, where PIBB is derived on the basis of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8], a black box optimization algorithm. Through a comparison with PI2, the authors discuss the connection between reinforcement learning and black box optimization. We further discuss the connection between PI2 and the proposed method in the Appendix, below.\nPrevious studies proposed solutions to reinforcement learning based on MD. Indeed, [9] is strongly associated with our research insofar as the authors propose a method based on MD. However, the details are different. We adopted the exponentiated gradient method (EG) using KL divergence as a regularization term. By contrast, [9] argues that using the Minkowski distance with the Euclidean distance is preferable to KL divergence, because it offers flexibility when updating the gradient."}, {"heading": "2. Methods: MDS, G-MDS, AMDS, G-AMDS", "text": ""}, {"heading": "2.1. Derivation of proposed algorithm: MDS and G-MDS", "text": ""}, {"heading": "2.1.1. MDS", "text": "A reinforcement learning algorithm is an algorithm aimed at obtaining an optimal policy to maximize reward (i.e., by minimizing cost). Consider the problem of minimizing the objective function J (\u03b8). Rather than dealing with policy parameters \u03b8 \u2208 \u0398 directly, we\nconsider the probability function p (\u03b8). Therefore, we search the following domain:\nP =\n{\np (\u03b8) = [p (\u03b81) , \u00b7 \u00b7 \u00b7 , p (\u03b8M)] \u2208 R M\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2200jp (\u03b8j) \u2265 0, M \u2211\nj=1\np (\u03b8j) = 1\n}\n. (1)\nThe decision variable is p (\u03b8), and the objective function is the expectation of the cost J (\u03b8).\nJ = M \u2211\nj=1\np (\u03b8j)J (\u03b8j) (2)\nTherefore, the optimal generative probability is\np\u2217(\u03b8) = arg min p(\u03b8)\u2208P J . (3)\nNext, we consider obtaining the optimal policy by updating p (\u03b8). As a means for updating p (\u03b8), we use MD, given as follows:\n\u03b2t = arg min \u03b2\u2208B {\u3008gt, \u03b2\u3009+ \u03b7tB\u03c6 (\u03b2||\u03b2t\u22121)} . (4)\nThe parameter \u03b2t in (4) is the probability distribution pt (\u03b8) of the policy parameter \u03b8 at update step t. Thus,\npt (\u03b8) := \u03b2t (5)\nSubstituting the above equation into Eq. (4), we obtain the following:\npt (\u03b8) = arg min p(\u03b8)\u2208P {\u3008gt, p (\u03b8)\u3009+ \u03b7tB\u03c6 (p (\u03b8) ||pt\u22121 (\u03b8))} (6)\nwhere B\u03c6 is the Bregman divergence, which has an arbitrarily smooth convex function \u03c6 and is defined as B\u03c6 (x, x\n\u2032) = \u03c6 (x)\u2212 \u03c6 (x\u2032)\u2212 \u3008\u2207\u03c6 (x\u2032) , x\u2212 x\u2032\u3009. The domain of the decision variable is found on the simplex P. We can select the Bregman divergence as the KL divergence \u03c6 (xt) = \u2211N\nj=1 xt,j log (xt,j), but we can also use the Euclidean distances assumed on the simplex [10]. Moreover, we can select a different Bregman divergence, as discussed in [10, 11]. Note that gt in Eq. (6) is the gradient of the objective function \u2207p(\u03b8)J . We derive this as follows:\n\u2207p(\u03b8)J =\n[\n\u2202J\n\u2202p (\u03b81) , \u00b7 \u00b7 \u00b7 ,\n\u2202J\n\u2202p (\u03b8j) , \u00b7 \u00b7 \u00b7 ,\n\u2202J\n\u2202p (\u03b8M )\n]\n= [J (\u03b81) , \u00b7 \u00b7 \u00b7 , J (\u03b8j) , \u00b7 \u00b7 \u00b7 , J (\u03b8M)]\n= J (\u03b8) . (7)\nThat is, \u2207p(\u03b8)J is a value obtained without using derivatives of J . From the above, pt (\u03b8) can be updated using Eq. (6) and learning can proceed.\nIn a typical reinforcement learning problem, we employ the expected cumulative reward\u2014 derived with Eq. (8)\u2014as the objective function J (\u03b8):\nJ (\u03b8j) =\n\u222b\nT\np ( \u03c4 \u03b8j ) r ( \u03c4 \u03b8j ) d\u03c4 (8)\nwhere the trajectory generated from the policy parameter \u03b8j is \u03c4 \u03b8j \u2208 T , the generating probability of the trajectory \u03c4 \u03b8j is p ( \u03c4 \u03b8j ) , and the reward in the trajectory \u03c4 \u03b8j is r (\n\u03c4 \u03b8j i\n)\n.\nWe can approximate this using a Monte Carlo integral, as follows:\nJ (\u03b8j) \u2243 1\nN\nN \u2211\ni=1\nr (\n\u03c4 \u03b8j i\n)\n(9)\n\u221d N \u2211\ni=1\nr (\n\u03c4 \u03b8j i\n)\n(10)\nIn order to solve this problem, we must generate trajectories of N kinds for M-type policy parameters, and make M \u00d7N attempts for one update. Here, we use the concept of online learning.\nConsidering this as online learning, the gradient of the objective function is derived as follows:.\n\u2207p(\u03b8)J \u2243 [ r ( \u03c4 \u03b81i ) , \u00b7 \u00b7 \u00b7 , r ( \u03c4 \u03b8j i ) , \u00b7 \u00b7 \u00b7 , r ( \u03c4 \u03b8Mi )] = r ( \u03c4\u03b8i )\n(11)\nwhere r ( \u03c4\u03b8i )\nis a vector of the cumulative reward before calculating the expected value. Thus, \u2207p(\u03b8)J \u2243 r ( \u03c4\u03b8i )\ncan be used as the gradient of MD gt. Because this derived algorithm is a policy search based on MD, it is called MDS."}, {"heading": "2.2. G-MDS", "text": "For the experiment, we considered a case where the Bregman divergence B\u03c6 in Eq. (6)\nis the KL divergence. That is, in B\u03c6, \u03c6 is \u03c6 (xt) = \u2211N j=1 xt,j log (xt,j) ( x \u2208 RN , xt,j > 0 )\n. Then, it can be rewritten as follows:\npt (\u03b8i) = exp (\u2212\u03b7tgt,i) pt\u22121 (\u03b8i)\n\u2211N j=1 exp (\u2212\u03b7tgt,j) pt\u22121 (\u03b8j) (12)\nIn this paper, we considered pt (\u03b8i) as the Gaussian distribution of the average \u00b5t\u22121 and the variance \u03a3\u01ebt\u22121,i, where \u03b8i is generated accordingly:\npt (\u03b8i) = N ( \u03b8 | \u00b5t\u22121,\u03a3\u01ebt\u22121,i )\n(13)\nHere, we consider the average \u00b5t of the Gaussian distribution. From Eq. (12), \u00b5t can be calculated as follows:\n\u00b5t = N \u2211\ni=1\n\u03b8ipt (\u03b8i) = Ept\u22121 [\u03b8i exp (\u2212\u03b7tgt,i)]\nEpt\u22121 [exp (\u2212\u03b7tgt,j)] (14)\nBy using the Monte Carlo integral for Eq. (14), the average value \u00b5t can be estimated as \u00b5\u0302t when N is sufficiently large:\n\u00b5\u0302t = 1 N \u2211N i=1 \u03b8i exp (\u2212\u03b7tgt,i) 1 N \u2211N j=1 exp (\u2212\u03b7tgt,j) (15)\nFurthermore, from Eq. (13), \u03b8i for \u01ebt,i \u223c N ( 0,\u03a3\u01ebt,i )\nduring the update step t can be expressed as follows:\n\u03b8i = \u00b5t\u22121 + \u01ebt\u22121,i (16)\nSubstituting this into Eq. (15), we have the following:\n\u00b5\u0302t =\n\u2211N\ni=1 (\u00b5\u0302t\u22121 + \u01ebt\u22121,i) exp (\u2212\u03b7tgt,i) \u2211N\nj=1 exp (\u2212\u03b7tgt,j)\n= \u00b5\u0302t\u22121 +\nN \u2211\ni=1\n(\nexp (\u2212\u03b7tgt,i) \u01ebt\u22121,i \u2211N\ni=j exp (\u2212\u03b7tgt,j)\n)\n. (17)\nBecause this derived algorithm is an instance of MDS that assumes that the policy follows a Gaussian distribution, it is called G-MDS."}, {"heading": "2.3. Derivation of the proposed algorithm: AMDS and G-AMDS", "text": ""}, {"heading": "2.3.1. AMDS", "text": "Next, the accelerated mirror descent (AMD) method [11] is applied to the proposed method. AMD is an accelerated method that generalizes Nesterov\u2019s accelerated gradient such that it can be applied to MD. Here, Eq. (6) with AMD yields the following equations:\npt (\u03b8) = \u03bbt\u22121p z\u0303 t\u22121 (\u03b8) + (1\u2212 \u03bbt\u22121) p x\u0303 t\u22121 (\u03b8) ,with\u03bbt\u22121 =\nr\nr + t (18)\npz\u0303t (\u03b8) = arg min pz\u0303(\u03b8)\u2208P\nts\nr\n{\n\u3008gt, p z\u0303 (\u03b8)\u3009+B\u03c6\n( pz\u0303 (\u03b8) ||pz\u0303t\u22121 (\u03b8) )}\n(19)\npx\u0303t (\u03b8) = arg min px\u0303(\u03b8)\u2208P \u03b3s { \u3008gt, p x\u0303 (\u03b8)\u3009+R ( px\u0303 (\u03b8) ||px\u0303t\u22121 (\u03b8) )}\n(20)\nwhere R (x, x\u2032) = B\u03c9 (x, x \u2032), which represents the Bregman divergence of the arbitrarily smooth convex function \u03c9 (x). We here explain the parameters for AMD. First, Eqs. (18)\u2013(20) consist of parallel MDs. Therefore, it seems that \u03bb defines the mixture ratio of Eqs. (20) and (19). In addition, \u03bb is initially close to 1, such that AMD applies Eq. (20). As \u03bb comes close to 0, AMD applies Eq. (19).\nFurthermore, consider Eq. (19), where ts r corresponds to a reciprocal learning rate. This increases as the number of updates increases. Therefore, it can be said that the learning rate decreases as the number of updates increases. This is equivalent to a simulated annealing operation. Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.\nNext, we describe the implementation of AMDS in detail. AMDS proceeds as follows. By repeating the following series of flows, AMD can be treated as reinforcement learning:\n1. Sample from the continuous distribution pt (\u03b8); 2. Calculate the discrete distribution px\u0303t\u22121 (\u03b8) and p z\u0303 t\u22121 (\u03b8) from the continuous distribu-\ntion pt (\u03b8), using the obtained samples;\n3. Evaluate objective values for each obtained samples as inputs; 4. Calculate the discrete distribution px\u0303t (\u03b8) and p z\u0303 t (\u03b8) based on Eqs. (19) and (20); 5. Perform fitting for the discrete distributions px\u0303t (\u03b8) and p z\u0303 t as the continuous distribu-\ntion (e.g., with a Gaussian distribution);\n6. Calculate the continuous distribution pt (\u03b8) for the next sampling."}, {"heading": "2.3.2. G-AMDS", "text": "We derive the same procedure as G-MDS when using KL divergence. Let the Bregman divergence B\u03c6 from Eq. (19) be the KL distance, and let R = B\u03c9 in Eq. (20) be \u03c9 (x) = \u01eb \u2211n\ni=1 (xi + \u01eb) log (xi + \u01eb) ( x \u2208 RN , xt,j > 0 )\n. Accordingly, this method is referred to as GAMDS. Furthermore, the result cannot be calculated analytically. Indeed, it is known that an efficient and numerical calculation is available.\nFinally, we approximate the distributions px\u0303 (\u03b8) and pz\u0303 (\u03b8) with a Gaussian distribution."}, {"heading": "3. Results", "text": ""}, {"heading": "3.1. 2DOF Via-point task", "text": "We performed a 2DOF Via-point task to evaluate the proposed method. The agent is represented as a point on the x\u2013y plane. This agent learns to pass through the point (0.5,\n0.2) at 250 ms. Before learning this, an initial trajectory from (0, 0) to (1, 1) is generated. The reward function is as follows:\nrt = 5000f 2 t + 0.5\u03b8 T\u03b8 (21)\n\u2206r250ms = 10000000000 ( (0.5\u2212 x250ms) 2 + (0.2\u2212 y250ms) 2) (22)\nHere, DMP [12] is used for the parameterization of the policy, and the agent is seeking a policy for each x-axis and y-axis.\nThe parameter settings are as follows: 1000 updates, 15 rollouts, and 10 basis functions."}, {"heading": "3.2. Experimental Results", "text": "In this section, we describe the experimental results. We summarize the results for GMDS and G-AMDS in Fig. Figure 2. In the figure, the thin line represents a standard deviation of 1. Table Table 1 shows the average and the variance at convergence. The variance \u03a3\u01eb for each search noise shall be 1.0.\nFrom the above, we confirm that G-AMDS learns at a faster rate than G-MDS. Therefore, it is effective to apply the proposed extension for MD to reinforcement learning."}, {"heading": "4. Conclusions", "text": "In this study, we proposed MDS. We explained the theoretical derivations of MDS, G-MDS, AMDS, and G-AMDS. According to the experimental results, learning progressed faster with the proposed G-AMDS. Moreover, based on the fact that AMD is a generalization of Nesterov\u2019s acceleration method, we expect that the acceleration will be effective for an objective function with a saddle point."}, {"heading": "Acknowledgment", "text": "The research was supported by JSPS KAKENHI (Grant numbers JP26120005, JP16H03219, and JP17K12737).\nAppendix A. Relationship between G-MDS and PI2 algorithm\nWe here demonstrate that the algorithm is equivalent to PI2. The symbols are replaced as follows:\nPt\u22121,i = exp (\u2212\u03b7tgt,i)\n\u2211N j=1 exp (\u2212\u03b7tgt,j) (A.1)\nMt\u22121,i = 1 (A.2)\nThat is, Eq. (17) becomes similar to Eq. (A.3):\n\u00b5\u0302t = \u00b5\u0302t\u22121 + N \u2211\ni=1\n(Pt\u22121,iMt\u22121,i\u01ebt\u22121,i) (A.3)\nThe PI2 algorithm is equivalent to Eq. (A.3), provided that we employ DMP [12] as the policy function and ignore Mt\u22121,i. The algorithm here can be written such that it resembles that in Table Table A.2. There are some differences between PI2 and the algorithms obtained here, however. For instance, with PI2, the decision variable is updated sequentially using the provisional cumulative reward at each point during one trial, whereas the G-MDS uses only cumulative rewards. Moreover, G-MDS is an algorithm with fewer procedures. Finally, PI2 assumes a Gaussian distribution, whereas G-MDS can be generalized as MDS, which calculate similar algorithms using arbitrary probability distributions."}], "references": [{"title": "Policy improvement methods: Between black-box optimization and episodic reinforcement learning", "author": ["F. Stulp", "O. Sigaud"], "venue": "34 pages ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Y", "author": ["J. Peters", "K. Mulling"], "venue": "Altun, Relative entropy policy search., in: AAAI, Atlanta", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical relative entropy policy search", "author": ["C. Daniel", "G. Neumann", "J.R. Peters"], "venue": "in: International Conference on Artificial Intelligence and Statistics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "ROCK*- Efficient black-box optimization for policy learning", "author": ["J. Hwangbo", "C. Gehring", "H. Sommer", "R. Siegwart", "J. Buchli"], "venue": "in: 2014 IEEE-RAS International Conference on Humanoid Robots, IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "Journal of Machine Learning Research 11 (Nov) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning of motor skills in high dimensions: A path integral approach", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "in: Robotics and Automation (ICRA), 2010 IEEE International Conference on, IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Completely derandomized self-adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary computation 9 (2) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient bregman projections onto the simplex", "author": ["W. Krichene", "S. Krichene", "A. Bayen"], "venue": "in: Decision and Control (CDC), 2015 IEEE 54th Annual Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerated mirror descent in continuous and discrete time", "author": ["W. Krichene", "A. Bayen", "P.L. Bartlett"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning attractor landscapes for learning motor primitives", "author": ["A.J. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "Tech. rep. ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Introduction In recent years, as stated in [1], attention has focused on the relationship between black box optimization and reinforcement learning.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Episode-based REPS [4] is a derivation method formalized by considering the upper-level policy.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.", "startOffset": 3, "endOffset": 9}, {"referenceID": 3, "context": "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.", "startOffset": 3, "endOffset": 9}, {"referenceID": 0, "context": "Specifically, [1] explains the history of black box optimization and reinforcement learning, and proposes PI.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 5, "context": "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "It is considered a black box optimization method, where PI is derived on the basis of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8], a black box optimization algorithm.", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "We can select the Bregman divergence as the KL divergence \u03c6 (xt) = \u2211N j=1 xt,j log (xt,j), but we can also use the Euclidean distances assumed on the simplex [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 7, "context": "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 8, "context": "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].", "startOffset": 72, "endOffset": 80}, {"referenceID": 8, "context": "AMDS Next, the accelerated mirror descent (AMD) method [11] is applied to the proposed method.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.", "startOffset": 30, "endOffset": 36}, {"referenceID": 5, "context": "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.", "startOffset": 30, "endOffset": 36}, {"referenceID": 9, "context": "2\u2212 y250ms) 2) (22) Here, DMP [12] is used for the parameterization of the policy, and the agent is seeking a policy for each x-axis and y-axis.", "startOffset": 29, "endOffset": 33}], "year": 2017, "abstractText": "In recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learning algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.", "creator": "LaTeX with hyperref package"}}}