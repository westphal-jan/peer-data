{"id": "1709.02753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Privacy Loss in Apple's Implementation of Differential Privacy on MacOS 10.12", "abstract": "In June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.\n\n\n\nApple has used differential privacy in its products, including Android and iOS, for tracking users' personal information and for tracking user profiles and browsing history.\nSince 2014, Apple has used algorithms developed by the FBI to analyze and analyze Apple's data collection. Apple has also taken advantage of algorithms developed by the FBI to track users' personal information and for tracking user profiles and browsing history.\nIn June 2016, Apple launched a new strategy for collecting customer data on customers' devices. Users must be provided their customer details to be able to access their information.\nApple has also set up a special privacy unit for user information, to monitor customers' usage. The company's privacy unit is a separate unit, called an i3-Sensors Device (i3).\nIn June, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.\nIn June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The company's privacy unit is a separate unit, called an i3-Sensors Device (i3).\nIn June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The company's privacy unit is a separate unit, called an i3-Sensors Device (i3).\nIn June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The company's privacy unit is a separate unit, called an i3-Sensors Device (i3).\nIn June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The company's privacy unit is a separate unit, called an", "histories": [["v1", "Fri, 8 Sep 2017 16:00:14 GMT  (2569kb,D)", "http://arxiv.org/abs/1709.02753v1", null], ["v2", "Mon, 11 Sep 2017 05:18:29 GMT  (2758kb,D)", "http://arxiv.org/abs/1709.02753v2", null]], "reviews": [], "SUBJECTS": "cs.CR cs.CY cs.LG", "authors": ["jun tang", "aleksandra korolova", "xiaolong bai", "xueqiang wang", "xiaofeng wang"], "accepted": false, "id": "1709.02753"}, "pdf": {"name": "1709.02753.pdf", "metadata": {"source": "CRF", "title": "Privacy Loss in Apple\u2019s Implementation of Differential Privacy on MacOS 10.12", "authors": ["Jun Tang", "Aleksandra Korolova", "Xiaolong Bai", "Xueqiang Wang", "Xiaofeng Wang"], "emails": ["juntang@usc.edu", "korolova@usc.edu", "bxl12@mails.tsinghua.edu.cn", "xw48@indiana.edu", "xw7@indiana.edu"], "sections": [{"heading": null, "text": "In this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple\u2019s set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple\u2019s servers.\nWe find that although Apple\u2019s deployment ensures that the (differential) privacy loss per each datum submitted to its servers is 1 or 2, the overall privacy loss permitted by the system is significantly higher, as high as 16 per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints [21]. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications.\nWe applaud Apple\u2019s deployment of differential privacy for its bold demonstration of feasibility of innovation while guaranteeing rigorous privacy. However, we argue that in order to claim the benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the daily, weekly, and device lifetime privacy loss permitted.\nACM Reference Format: Jun Tang, Aleksandra Korolova, Xiaolong Bai, XueqiangWang, and Xiaofeng Wang. 2017. Privacy Loss in Apple\u2019s Implementation of Differential Privacy on MacOS 10.12.\nPreprint, August 13, 2017, 2017. ."}, {"heading": "1 INTRODUCTION", "text": "Differential privacy [7] has been widely recognized as the leading statistical data privacy definition by the academic community [6, 11]. Thus, as one of the first large-scale commercial deployments of differential privacy (preceded only by Google\u2019s RAPPOR [10]), Apple\u2019s deployment is of significant interest to privacy theoreticians and practitioners alike. Furthermore, since Apple may be perceived as competing on privacy with other consumer companies, understanding the actual privacy protections afforded by the deployment of differential privacy in its desktop and mobile OSes may be of interest to consumers and consumer advocate groups [16].\nHowever, Apple\u2019s publicly-facing communications about its deployment of differential privacy have been extremely limited: neither its developer documents [1, 2, 21, 22, 24] nor interstitials prompting the users to opt-in to differentially private data collection (Figures 8 and 9) provide details of the technology, except to say what data types it may be applied to. Two aspects of the deployment are crucial to understanding its privacy merits: the algorithms or processes used to ensure differential privacy of the data being sent and the privacy parameters being used by those algorithms. Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.\nBoth EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1. As far as we are aware, we are the first to systematically study privacy budget use in Apple\u2019s deployment of differential privacy."}, {"heading": "1.1 The (Differential) Privacy Budget", "text": "One of the core distinctions of differential privacy (DP) from colloquial notions of privacy is that the definition provides a way to quantify the privacy risk incurred whenever a differentially private algorithm is deployed. Typically called privacy budget or privacy loss and denoted by \u03f5 , it quantitatively measures by how much the risk to an individual privacy may increase due to that individual\u2019s data inclusion in the inputs to the algorithm. The higher the value of \u03f5 , the less privacy protection is provided by the algorithm; in particular, the increase in privacy risks is proportional to exp(\u03f5). Although the choice of \u03f5 is typically treated as a social choice by\n1Apple\u2019s only public comments on the privacy budget are \u201cRestrict the number of submissions made during a period. No identifiers. Periodically delete donations from server\" [21].\nar X\niv :1\n70 9.\n02 75\n3v 1\n[ cs\n.C R\n] 8\nS ep\n2 01\n7\nthe theoretical computer scientists [6], it is of crucial importance in practical deployments, as the meaning of a privacy risk of exp(1) vs exp(50) is radically different.\nIn practice, an individual\u2019s data contribution is rarely limited to one datum. Whenever multiple data are submitted with differential privacy, the overall differential privacy loss incurred by that individual is viewed as bounded by the sum of the privacy losses of each of the submissions, due to what is known as composition theorems [9, 15]. Hence, understanding the privacy implications of a deployed system such as Apple\u2019s, requires not only understanding the privacy loss incurred per datum submitted, but also how many datums may be submitted per time period or over a lifetime of a user\u2019s device. In fact, the need to understand the total privacy loss of differential privacy deployments has prompted Dwork and Mulligan to propose an \u201cEpsilon Registry\" [8]."}, {"heading": "1.2 Our Findings", "text": "We find that although the privacy loss per datum is strictly limited to privacy budgets typically used in the literature, the daily privacy loss permitted by the implementation exceeds values typically considered acceptable by the theoretical community [12], and the overall privacy loss per device may be unbounded (Section 4)."}, {"heading": "2 OVERVIEW", "text": ""}, {"heading": "2.1 System Components", "text": "We start by listing the components of the DP system on Mac OS we have identified:\n\u2022 The differential privacy framework, located at /System/ Library/PrivateFrameworks/DifferentialPrivacy.framework. The framework contains code implementing differential privacy, which we will decompile with Hopper Disassembler. In particular, it contains code responsible for per-datum privatization and for periodic functions that manage the privacy budget, updates of the database for privatized data, and creation of report files to be submitted to Apple servers. \u2022 The com.apple.dprivacyd daemon handling differential privacy, located at /usr/libexec/dprivacyd. We will study it using code tracing with LLDB. \u2022 A database, located at /private/var/db/DifferentialPrivacy, which contains several tables of privatized records and a table related to available budget per record type. Anyone with sudo privileges can open the database using sqlite3. We will study its contents (Section 3.1) and the changes to them due to usage of features that are supposed to trigger differentially private data collection and over time. \u2022 Configuration files (Figures 4, 5, 6 and 7) with extension .plist, located at /System/Library/DifferentialPrivacy/Configuration/. The four files, which can be inspected by anyone but are difficult to change, specify numerous parameters that configure the actions of the DP framework, such as the per datum privacy parameter, privacy budget increase rate, etc. We will study the effects of each of these parameters by changing them and observing their effects during code execution with LLDB and through the resulting report files produced. \u2022 Report files (Figure 11) with extensions .dpsub and .json.anon, located at /Library/Logs/DiagnosticReports/ and\n/private/var/db/DifferentialPrivacy/Reports/. These files contain privatized data and are the ones transmitted to Apple\u2019s servers. They can be openedwith a text editor, and the .dpsub files can also be inspected through the MacOS Console under System Reports. We will study when they get created, their contents, and when they get deleted through observations and experiments. \u2022 The MacOS Console (Figure 20), which contains messages mentioning differential privacy, either in the library or process name. The messages are timestamped and easily readable, and are thus useful in noting certain system actions."}, {"heading": "2.2 System Organization and Data Pipeline", "text": "The dprivacy (com.apple.dprivacyd) daemon runs the system responsible for implementation of differential privacy. Once a user opts-in to differentially private data collection in the MacOS Security & Privacy Settings (Figure 8), the dprivacy daemon is enabled and the database that will be supporting relevant data storage and management is created in /var/db/DifferentialPrivacy. Furthermore, there\u2019s a message visible on Console: \u201cdprivacyd: accepting work now\".\nPer Apple\u2019s original announcement [1, 21, 23], the use of DP is focused on four applications: new words, emojis, deeplinks, and lookup hints in Notes, with iCloud data added as an additional application in early 2017 [2], and further types of data collection such as health data introduced in mid-2017 [24]. We observed how to reliably trigger DP-related activity when entering new words and emojis2; thus, our conclusions will be based on experiments with those applications.\nWhenever a user enters an emoji or a previously unseen new word in Notes, the relevant datum is perturbed using a differentially private algorithm and its privatized version and some metadata are added to a corresponding database table.\nA ReportGenerator task (Figure 10) is run periodically, at which point some records from the database are selected and written to report files (Figure 11), which are then transmitted to Apple\u2019s servers. The table rows corresponding to the selected records are \u201cmarked as submitted\" and eventually deleted from the database by a task.\nThere are several other periodicmaintenance tasks, whose effects are: to delete records from the database (even those that weren\u2019t submitted) and to delete report files from disk. These periodic tasks are accompanied bymessages observable on the Console (Figure 20)."}, {"heading": "2.3 Study Questions", "text": "In order to understand the privacy loss in Apple\u2019s implementation of differential privacy, we need to understand the following aspects of the system:\n(1) What are the privacy parameters used in order to achieve privatization before the privatized datum gets entered into the database? This will let us understand per datum privacy.\n2To reliably trigger DP application to emojis, the user needs to call out the emoji keyboard by pressing \"ctrl-cmd space\", then click an emoji (or select an emoji with the arrow key and press \"Enter\"). For new words, the user can type an incorrectly spelled word in the Notes app and then ignore the spelling suggestion by pressing \u2019esc\u2019.\n(2) How frequently are records selected for inclusion in a report? How many records can be included in one report? How frequently are the reports created and submitted? This will let us understand the rate of privacy loss. (3) Is the total privacy loss that a particular user can incur limited? (4) How easy is it to alter the performance of the system, e.g., change parameters responsible for each datum\u2019s privatization, change the number of records selected for inclusion into a report or the frequency of report generation?\nWe discovered that the answers to questions (1) \u2013 (3) (see Section 4) depend on the parameters specified in the configuration files and their use by the framework to establish the available privacy budget. We describe our findings and observations regarding the database tables, configuration files, and the functionality related to report generation and privacy budget maintenance next. We will discuss (4) in Section 5.2."}, {"heading": "3 SYSTEM\u2019S DETAILS", "text": ""}, {"heading": "3.1 The Database", "text": "The ZOBHRECORD (See Figure 1) and ZCMSRECORD tables in the database store the perturbed data, with the former dedicated to the privatized emoji records, and the latter \u2013 to the privatized new words records. Every emoji typed by a user gets privatized and stored in the ZOBHRECORD table. In contrast, only words that haven\u2019t been previously typed are privatized and stored in the ZCMSRECORD table.\nA notable table is ZPRIVACYBUDGETRECORD, whose schema and example contents are shown in Figure 2. The table contains 7 entries, one for each of the applications (NewWords, Emoji, AppDeepLink, Search, and health) and two for helper functions (default and testBudget). The ZBALANCE column contains the integer value of the currently available privacy budget for each application."}, {"heading": "3.2 Configuration Files", "text": "There are four configuration files for theDP daemon: com.apple.dprivacyd. {keynames, keyproperties, algorithmparameters, budgetproperties}.plist. See Figures 4, 5, 6 and 7 for snippets of the configuration files, Figure 3 for a schematic relationship between keys in them, and Tables 1 and 2 for a snippet of their values.\n3.2.1 KeyName\u2192 PropertiesName. keynames.plist contains a mapping of KeyNames to PropertiesNames. KeyNames describe the possible data types, e.g., com.apple.keyboard.NewWords.en_US \u2013 a new word in English using the US keyboard3, com.apple.keyboard.NewWords.en_GB \u2013 a new word in English using the Great Britain keyboard, com.apple.keyboard.NewWords.ru_RU \u2013 a new word in Russian, com.apple.keyboard.Emoji.fr_FR.EmojiKeyboard \u2013 an emoji in French, com.apple.parsec.AppDeepLink \u2013 a deeplink. In MacOS 10.12.3 keynames.plist contains 160 distinct KeyNames.\nEach KeyName is assigned one of 13 possible PropertiesName. For example, KeyName com.apple.keyboard.NewWords.en_US has a PropertiesNameNewWords, as do com.apple.keyboard.NewWords.en_GB and com.apple.keyboard.NewWords.ru_RU; KeyName com.apple.parsec.AppDeepLink has a PropertiesName DeepLinks; KeyName com.apple.keyboard.Emoji.fr_FR.EmojiKeyboard has a PropertiesName TermFrequency, as do com.apple.keyboard.Emoji.ru_RU.EmojiKeyboard and com.apple.keyboard.Emoji.en_US.EmojiKeyboard.\n3.2.2 Determining the Per-Datum Privacy Loss: KeyName \u2192 PropertiesName\u2192 PrivatizationAlgorithm, PrivacyParameter. For each of the 13 possible PropertiesName values, the keyproperties.plist file specifies a PrivatizationAlgorithm and PrivacyParameter. For example, for PropertiesName=HealthDataTypes: PrivatizationAlgorithm=OneBitHistogram and PrivacyParameter=1; for PropertiesName=NewWords: PrivatizationAlgorithm= CountMedianSketch and PrivacyParameter=2; for PropertiesName= TermFrequency: PrivatizationAlgorithm=OneBitHistogram and PrivacyParameter=1.\nalgorithmparameters.plist specifies additional parameters of the privatization algorithm.\n3.2.3 Determining a Budget for Particular Data Types: KeyName \u2192 PropertiesName\u2192BudgetKeyName. Furthermore, for each of the 13 possible PropertiesName values, the keyproperties.plist file specifies a BudgetKeyName. For example, for PropertiesName=LocalWords: BudgetKeyName=com.apple.keyboard.NewWords; for PropertiesName=NewWords: BudgetKeyName=com.apple.keyboard.NewWords; for PropertiesName= DeepLinks: BudgetKeyName= com. apple. parsec. AppDeepLink; for PropertiesName=TermFrequency: BudgetKeyName=com.apple.keyboard.Emoji. In particular, this implies that all new words, regardless of language, have the same BudgetKeyName=com.apple. keyboard. NewWords, all emojis \u2013 the same BudgetKeyName=com. apple.keyboard.Emoji, etc. There are a total of 7 distinct BudgetKeyNames, which is consistent with what we saw in the ZPRIVACYBUDGETRECORD table in the database (Figure 2).\n3We are not certain whether the second identifier encodes a keyboard preference or a region preference, or both.\n3.2.4 Budget Properties. The budgetproperties.plist file specifies two quantities for each BudgetKeyName: SessionSeconds and SessionAmount (see Figure 7 for a snippet and Table 2 for the values).\nSessionSeconds = 86400 for all BudgetKeyNames except com.apple.health, for whom SessionSeconds = 604800. These correspond to number of seconds in a day and in a week, respectively.\nSessionAmount values range from 1 to 10, depending on BudgetKeyName (see Table 2)."}, {"heading": "4 PRIVACY LOSS FINDINGS", "text": "We now answer the questions posed in Section 2.3."}, {"heading": "4.1 Each Datum\u2019s Privatization", "text": "PrivacyParameter of the KeyName specifies the privacy parameter used for privatization of the datumof thatKeyName prior to its addition to the database.\nFor example, for an emoji in French, Russian, or English, the privatization algorithm that will be run is OneBitHistogram with PrivacyParameter 1. For a new word in English using the US keyboard or a new word in Russian, the privatization algorithm that will be run is CountMedianSketch with PrivacyParameter 2, etc. See Table 1 for the PrivacyParameter values used for the various datum types in Mac OS 10.12.3.\nIt is difficult to understand and verify correctness of the privatization algorithms when one only has access to the binary code. We expect that the algorithms implement the ideas described in the patents [17\u201319]. What we do verify using LLDB and by changing the PrivacyParameter in the configuration file and observing the effects, is that the privacy parameter epsilon used for emoji and new words privatization is what one would expect based on the values in the configuration file (see Section A.1.1)."}, {"heading": "4.2 Report Generation and Privacy Budget Management over Time", "text": "SessionAmount specifies how many records belonging to a particular KeyName can be included in a report file. SessionAmount also specifies the increase to the available budget balance for each BudgetKeyName that happens every SessionSeconds.\n4.2.1 Number of Records per Report. Specifically, Apple keeps track of the privacy budget balance (ZBALANCE) available for each of the 7 BudgetKeyNames in the ZPRIVACYBUDGETRECORD database table. The available budget balance together with the SessionAmount is used to decide how many records to include in each report file. Specifically, every 18 hours, the daemon runs a ReportGenerator task. It selects records from the database tables to be included in the report according to the following:\n\u2022 Atmostmin(SessionAmount, 40)4 records per KeyNamemay be selected. \u2022 The total number of records belonging to the same BudgetKeyName selected may not exceed the privacy budget balance for that BudgetKeyName currently available as per the ZPRIVACYBUDGETRECORD table in the database. When the number of records of a particular BudgetKeyName available in the database tables exceeds the available budget balance, then the subset of records whose total number does not exceed the available budget balance are chosen at random while taking into account SubmissionPriority.\nEach record selected is \u201cmarked as submitted\" in the corresponding table in the database, and for each record submitted the corresponding privacy budget balance is decreased by one.\nThe report files created contain the creation time (or creation time adjusted forward by 7 hours) in the file name, and are placed in the folder /Library/Logs/DiagnosticReports/ or /private/var/db/ DifferentialPrivacy/Reports/. Recordswith KeyNames that have TermFrequency, NewWords or LocalWords as their PropertiesName are included in reports in the first folder; records with KeyNames that have Search as PropertiesName \u2013 in the second folder.\n4.2.2 Budget Increase. A periodic task PrivacyBudgetMaintenance increases the ZBALANCE value in the ZPRIVACYBUDGETRECORD table for each BudgetKeyName by its corresponding SessionAmount every SessionSeconds (see Section A.1.2 for experimental and code evidence supporting this claim). Thus, for all BudgetKeyNames except health and default, the available privacy budget balance is increased by SessionAmount every 24 hours5. Due to OS sleep, which is common on MacOS, in practice the daemon increases the privacy budget balance by the SessionAmount multiplied by the number of days that have elapsed since the last budget update (a time which is kept track of in the database table).\nWhen a user opts-in to differentially private data collection, the budgets for each BudgetKeyName are initialized with their corresponding SessionAmount.\n4.2.3 Total Privacy Loss Permitted. Consider an example: a user opts in to DP, then types one or several emoji every day for t days. Since for emoji PrivacyParameter=1, every emoji will be put into database after being privatized with privacy loss of 1. Since for emoji the SessionAmount=1, the privacy budget balance will be increased by one every day, and so every day one privatized emoji will be included in a report sent to Apple\u2019s servers. After t days, by composition theorems [9, 15], the privacy loss incurred will be 1 \u00b7 1 \u00b7 t = t . 4The number 40 is hard-coded in the binary code (Figure 12). 5The default and AppDeepLink BudgetKeyNames are the exceptions to this; the former likely due to its role as a default value for abuse scenarios (Section 5.2) and the latter due to DeepLink functionality not being present in MacOS 10.12.\nConsider another example: a user opts in to DP, but doesn\u2019t use any emoji for 20 days. The available privacy budget balance for emoji will be 20 after that time. Then the user types two emoji each in 10 different languages supported by differential privacy in one day. Each of the 20 emojis will be put into the database after being privatized with privacy loss of 1. In the first day, 10 different emoji, one from each language (since SessionAmount = 1 for each emoji KeyName and available privacy budget balance is 20), will be included in the report, for a privacy loss of 10. The next day, the remaining 10 different emoji will be included in the report, for an additional privacy loss of 10.\nIn other words, the privacy loss for a particular application permitted by the implementation is PrivacyParameter \u00b7 SessionAmount every SessionSeconds. The privacy loss that is not realized during particular time period if that application is not used remains available for future use. Thus, for the applications of NewWords, AppDeepLink, Search, and Emoji, whose respective PrivacyParameters are: 2, 1, 1, 1, and SessionAmounts are: 2, 10, 1, 1, and SessionSeconds is 86,400 in MacOS 10.12.3, the overall daily privacy loss permitted is 16. Moreover, since unused privacy budget balance rolls over for subsequent use, the overall privacy loss of a device for the four initially announced applications by Apple may reach 16 times the number of days since the user of the device has opted in to DP. A caveat to these findings is that DeepLink functionality does not appear to be implemented on MacOS yet, so the actual privacy loss on Mac OS 10.12.3 is currently as large as 6 per day and on iOS 10.1.1 \u2013 as large as 14 per day (Section 5.4) for the four initially announced applications."}, {"heading": "5 DISCUSSION", "text": ""}, {"heading": "5.1 Report File and Database Maintenance", "text": "Besides the periodic tasks of ReportGenerator and PrivacyBudgetMaintenance, whose actions have already been described in Section 4.2, the following 3 periodic tasks are responsible for database and report file management (Figure 10):\n\u2022 StorageCulling (every 24 hours): deletes records that have been submitted and records with the mismatched version number from the database. \u2022 StorageMaintenance (every 12 hours): deletes records to limit database size and deletes records added to the database more than two weeks before the current date. \u2022 ReportFilesMaintenance (every 24 hours): removes report files older than a month6 from disk."}, {"heading": "5.2 Ease of Altering System\u2019s Performance", "text": "We have observed several precautions that are implemented by Apple in order to make it difficult to abuse the implementation:\n\u2022 The configuration files are difficult to change, as such a change on Mac OS requires turning off Apple\u2019s System Integrity Protection, which is not trivial. We have not found a way to change the configuration files on iOS. \u2022 Even if one succeeds in changing the configuration files, whenever a PrivacyParameter in a configuration file is set\n6Concluded based on observation and dynamic code analysis, as we could not find this in the framework code.\nto a value higher than epsilonMax, a constant value equal to 2 which is hard-coded in the code of the framework (Figure 13), the PrivacyParameter used is defaulted to 1 at runtime and the corresponding record\u2019s Submission Priority is set to 99999, effectively ensuring it does not get included in report files. Furthermore, SessionAmount is defaulted to at most 40 at runtime (Section 4.2). \u2022 Time measures used by the daemon, such as the number of seconds in 18 hours, in a day, in 7 days, are hardcoded in the code (Section A.1.3). That may be the reason why we have not succeeded in accelerating report file generation or privacy budget increase by changing the SessionSeconds in the configuration file or changing the computer\u2019s clock.\nOn the other hand, anyone with root permissions can alter the privacy budget balance in the database, thereby artificially increasing the privacy loss."}, {"heading": "5.3 Configuration Differences between MacOS versions", "text": "We observed that Apple made changed to configuration files from MacOS 10.12.1 to 10.12.3 (see Table 3). The main distinctions are that in MacOS 10.12.3:\n\u2022 SessionAmount for com.apple.keyboard.NewWords increases from 1 to 2, resulting in a higher daily privacy loss. \u2022 BudgetName com.apple.health and PropertiesName LocalWords are introduced, signaling new applications for DP. \u2022 SubmissionPriority is adopted, signaling new protections against abuse. \u2022 Health-related PropertiesName and Budget are introduced, signaling that health-related data will also be included in DP data collection."}, {"heading": "5.4 A Note on iOS", "text": "Studying the iOS DP implementation is significantly more difficult. From our observations of the Console messages when the iPhone is connected to a computer running MacOS and of the reports (observable under Settings\u2192 Privacy\u2192 Analytics\u2192 Analytics Data), the iOS implementation follows the same principles as the MacOS one. The configuration files for iOS 10.1.1 we obtained from a jailbroken phone were identical to those of MacOS 10.12.1. The distinctions we found relate to iOS reports containing more metrics\n(in particular, we were not able to trigger DeepLink functionality on MacOS while such records abound on iOS), and to faster report file deletion from the phone than from the computer (7 vs 30 days)."}, {"heading": "6 CONCLUSIONS AND FUTUREWORK", "text": "We applaud Apple for its deployment of differential privacy in the local privacy model and for the many safeguards put in place to make it difficult to abuse. However, we believe the deployment has several significant shortcomings.\n(1) The privacy loss permitted by the system is not explained anywhere and takes significant effort to reverse-engineer. This is contrary to one of the main conceptual advantages of differential privacy \u2013 that a user can make an informed choice whether to opt-in to differentially private data collection based on the quantifiable knowledge of risk announced by the data collector. Furthermore, the lack of transparency on privacy loss opens the door for intentional or un-intentional abuse by Apple itself, e.g., by unilaterally changing either the per-datum privacy loss or the rate of privacy loss in a time period or by introducing additional BudgetKeyName(s), Apple may significantly weaken the privacy guarantees provided without anyone\u2019s knowledge or consent. (2) The privacy loss of 16 per day permitted by the system is significantly higher than what is commonly considered reasonable in academic literature. Furthermore, since the permitted privacy loss balance is replenished every day, over a course of time the total privacy loss per device becomes larger by orders of magnitude. (3) Due to the way the database and report files are structured, the implementation leaks what features of MacOS a user is using and in what language and, possibly, with what geographic and keyboard preference, both to Apple and to anyone who has access to the database. Furthermore, because only new words are privatized and added to the relevant database tables, one can potentially test whether a particular non-dictionary word has been ever used by the owner of the device by observing whether typing it triggers changes to the DP database.\nWe call for Apple to make its implementation of privacy-preserving algorithms public and to make the rate of privacy loss fully transparent and tunable by the user.\n6.0.1 Future Work. Apple does not transmit any user or device identifiers along with reports [21]. It would be worthwhile to investigate the effect that such (or other ways of) decoupling of data source from data aggregator can play in mitigating the implications of the (theoretically) infinitely increasing privacy loss [14].\nIt has been observed that properly implementing algorithms claiming to preserve differential privacy is non-trivial in practice [4, 5, 13]. It would be worthwhile to develop further techniques for verifying correctness of claimed DP implementations.\nFinally, the question of how to intuitively interpret and convey the privacy guarantees and limitations of differential privacy at various privacy loss levels to the public, remains open."}, {"heading": "A APPENDIX A.1 Code Support for Findings", "text": "The code convention for Objective-C, which Hopper disassembles the framework code to, is as follows. In an Objective-C function name, the leading - means this is an instance method that can only be accessed by an instance of the class, while + indicates the method is a class method and can be accessed anytime by simply referencing the class. The following [] includes both the function name and argument name, and arguments are separated by : (an example can be found in Figure 21).\nA.1.1 Checking PrivacyParameter corresponds to the privacy parameter used in a datum\u2019s privatization. For newwords, we observed that the function _DPCMSSample initWith uses PrivacyParameter to create the _DPBiasedCoin (see Figure 15 for Hopper code and Figure 16 for our interpretation of it), and using runtime LLDB we observed that the value is 1.0/(exp(PrivacyParameter ) + 1.0) (Figure 19). Analogously, we analyzed the emoji randomization code (Figure 21).\nA.1.2 Checking that SessionAmount controls the daily budget increase value. In the -[_DPPrivacyBudgetProperties initWithDictionary:] function (Figure 17), the SessionAmount is assigned to intervalBudgetValue. In the _DPPrivacyBudget updateAllBudgetsIn function (Figure 18), we can see the value used to multiply with the number of days since ZLASTUPDATE is r14, and r14 = (r15 intervalBudgetValue) interValue). So we can conclude that SessionAmount is used as the daily budget increase value.\nA.1.3 Hard-coded values. In addition to epsilonMAX and _kSecondsInOneDay (Figure 13), the following constants are also hardoced in the framework code: _kSecondsIn3Day, _kSecondsIn7Day, _kSecondsIn14Day, _kSecondsIn12Hours, _kSecondsIn18Hours, _kSecondsIn24Hours (Figure 14).\nA.2 Aspects of the System that we don\u2019t Understand\nSeveral details of the implementation\u2019s behavior were puzzling: \u2022 The privacy budget balance sometimes changes dramatically. We observed this phenomenon twice over the course of 6 months. In both cases, the privacy budget increased and was set to ZLASTU PDAT E\u2212ZCREAT IONDATE86400 . We don\u2019t know the reason for this; one possibility is that the Apple server triggered this change remotely.\n\u2022 The daemon occasionally automatically stops data collection and un-checks the boxes under Settings\u2192 Security & Privacy (Figure 8) indicating opt-in to Analytics Sharing (and hence, differential privacy). Over the course of our experiments in the last 6 months, we observed this effect several times. We were not able to reliably reproduce it. In particular, our experiments of setting the privacy budget balance to zero or a negative number in the database or entering thousands of emoji within a short period of time did not trigger opt-out. \u2022 When PrivacyParameter > epsilonMAX (Section 5.2), the NewWords record will be inserted into the ZOBHRECORD\ntable, even though it is typically inserted into the ZCMSRECORD table. \u2022 We don\u2019t understand the role of testBudget, one of the 7 BudgetKeyNames.\nA.3 Figures\nFigure 13: epsilonMAX (equals to 2 if interpreted as a double type number) and number of seconds in one day are hardcoded in the code."}], "references": [{"title": "Comment: Differential privacy and data collection is still not clearly defined as optin on iOS", "author": ["Greg Barbosa"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Proving Differential Privacy in Hoare Logic", "author": ["Gilles Barthe", "Marco Gaboardi", "Emilio Jes\u00fas Gallego Arias", "Justin Hsu", "C\u00e9sar Kunz", "Pierre-Yves Strub"], "venue": "In Proceedings of the 2014 IEEE 27th Computer Security Foundations Symposium (CSF)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Proving Differential Privacy via Probabilistic Couplings", "author": ["Gilles Barthe", "Marco Gaboardi", "Benjamin Gr\u00e9goire", "Justin Hsu", "Pierre-Yves Strub"], "venue": "In Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "A firm foundation for private data analysis", "author": ["Cynthia Dwork"], "venue": "Commun. ACM 54,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference (TCC)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Privacy in Information-Rich Intelligent Infrastructure. ArXiv e-prints (June 2017)", "author": ["C. Dwork", "G.J. Pappas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science 9,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response", "author": ["\u00dalfar Erlingsson", "Vasyl Pihur", "Aleksandra Korolova"], "venue": "In Proceedings of the ACM SIGSAC Conference on Computer and Communications Security (CCS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Differential privacy: An economic method for choosing epsilon", "author": ["Justin Hsu", "Marco Gaboardi", "Andreas Haeberlen", "Sanjeev Khanna", "Arjun Narayan", "Benjamin C Pierce", "Aaron Roth"], "venue": "IEEE Computer Security Foundations Symposium (CSF)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "On Significance of the Least Significant Bits for Differential Privacy", "author": ["Ilya Mironov"], "venue": "In Proceedings of the 2012 ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Differential Privacy: A Primer for a Non-technical Audience (Preliminary Version)", "author": ["Kobbi Nissim", "Thomas Steinke", "Alexandra Wood", "Micah Altman", "Aaron Bembenek", "Mark Bun", "Marco Gaboardi", "David O\u2019Brien", "Salil Vadhan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "https: //www.google.com/patents/US9594741 US Patent 9,594,741", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudiger", "V.R. Sridhar", "D. Davidson"], "venue": "Learning new words. (March", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2017}, {"title": "https: //www.google.com/patents/US9645998 US Patent 9,645,998", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudiger", "V.R. Sridhar", "D. Davidson"], "venue": "Learning new words. (May", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Emoji frequency detection and deep link frequency", "author": ["A.G. Thakurta", "A.H. Vyrros", "U.S. Vaishampayan", "G. Kapoor", "J. Freudinger", "V.V. Prakash", "A. Legendre", "S. Duplinsky"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Differential Privacy: From Theory to Deployment", "author": ["Abhradeep Guha Thakurta"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}], "referenceMentions": [{"referenceID": 11, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice.", "startOffset": 25, "endOffset": 32}, {"referenceID": 4, "context": "Differential privacy [7] has been widely recognized as the leading statistical data privacy definition by the academic community [6, 11].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "Differential privacy [7] has been widely recognized as the leading statistical data privacy definition by the academic community [6, 11].", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "Thus, as one of the first large-scale commercial deployments of differential privacy (preceded only by Google\u2019s RAPPOR [10]), Apple\u2019s deployment is of significant interest to privacy theoreticians and practitioners alike.", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 12, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 13, "context": "Although one can speculate about the algorithms deployed based on the recent patents [17\u201319], the question of parameters used to govern permitted privacy loss remains open and is our primary focus.", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 5, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 14, "context": "Both EFF and academics have called for Apple to detail its privacy budget use [3, 8, 16, 20], to no avail1.", "startOffset": 78, "endOffset": 92}, {"referenceID": 3, "context": "the theoretical computer scientists [6], it is of crucial importance in practical deployments, as the meaning of a privacy risk of exp(1) vs exp(50) is radically different.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Whenever multiple data are submitted with differential privacy, the overall differential privacy loss incurred by that individual is viewed as bounded by the sum of the privacy losses of each of the submissions, due to what is known as composition theorems [9, 15].", "startOffset": 257, "endOffset": 264}, {"referenceID": 10, "context": "Whenever multiple data are submitted with differential privacy, the overall differential privacy loss incurred by that individual is viewed as bounded by the sum of the privacy losses of each of the submissions, due to what is known as composition theorems [9, 15].", "startOffset": 257, "endOffset": 264}, {"referenceID": 5, "context": "In fact, the need to understand the total privacy loss of differential privacy deployments has prompted Dwork and Mulligan to propose an \u201cEpsilon Registry\" [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "We find that although the privacy loss per datum is strictly limited to privacy budgets typically used in the literature, the daily privacy loss permitted by the implementation exceeds values typically considered acceptable by the theoretical community [12], and the overall privacy loss per device may be unbounded (Section 4).", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 12, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 13, "context": "We expect that the algorithms implement the ideas described in the patents [17\u201319].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "After t days, by composition theorems [9, 15], the privacy loss incurred will be 1 \u00b7 1 \u00b7 t = t .", "startOffset": 38, "endOffset": 45}, {"referenceID": 10, "context": "After t days, by composition theorems [9, 15], the privacy loss incurred will be 1 \u00b7 1 \u00b7 t = t .", "startOffset": 38, "endOffset": 45}], "year": 2017, "abstractText": "In June 2016, Apple made a bold announcement that it will deploy local differential privacy for some of their user data collection in order to ensure privacy of user data, even from Apple [21, 23]. The details of Apple\u2019s approach remained sparse. Although several patents [17\u201319] have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy. In this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple\u2019s set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple\u2019s servers. We find that although Apple\u2019s deployment ensures that the (differential) privacy loss per each datum submitted to its servers is 1 or 2, the overall privacy loss permitted by the system is significantly higher, as high as 16 per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints [21]. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications. We applaud Apple\u2019s deployment of differential privacy for its bold demonstration of feasibility of innovation while guaranteeing rigorous privacy. However, we argue that in order to claim the benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the daily, weekly, and device lifetime privacy loss permitted. ACM Reference Format: Jun Tang, Aleksandra Korolova, Xiaolong Bai, XueqiangWang, and Xiaofeng Wang. 2017. Privacy Loss in Apple\u2019s Implementation of Differential Privacy", "creator": "LaTeX with hyperref package"}}}