{"id": "1709.03010", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "Steering Output Style and Topic in Neural Response Generation", "abstract": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling, the neural generation process itself requires a level of training (we consider it the \"experience\") and an overall degree of competence (we consider it the \"competition-level\" of training), with all of the features described here. We assume that any potential problem in which the neural generator is trained has the right degree of competence and that any potential problem will be addressed as well. We propose a method that will help solve this problem.", "histories": [["v1", "Sat, 9 Sep 2017 22:03:11 GMT  (1187kb,AD)", "http://arxiv.org/abs/1709.03010v1", "EMNLP 2017 camera-ready version"]], "COMMENTS": "EMNLP 2017 camera-ready version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["di wang", "nebojsa jojic", "chris brockett", "eric nyberg"], "accepted": true, "id": "1709.03010"}, "pdf": {"name": "1709.03010.pdf", "metadata": {"source": "CRF", "title": "Steering Output Style and Topic in Neural Response Generation", "authors": ["Di Wang", "Nebojsa Jojic", "Chris Brockett", "Eric Nyberg"], "emails": ["diwang@cs.cmu.edu,", "jojic@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al., 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like\npatterns of conversation, the agents need to have recognizable (and tunable) style, just as individual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an extreme case, an agent may be micro-managed by a human user who uses the neural model to enforce grammar and style (e.g., a level of politeness, or a type of humor), while driving the content directly (e.g., by expressing In this manner, the neural model becomes an authoring tool, rather than an independent chat-bot. On the other hand, in fully automated agent systems, the agent may be influenced by a knowledge database, or some other artificial information system, while running in a pre-set style or a style deemed best based on the course of the conversation.\nOne obstacle to achieving this with neural language generation models is that the sentence representation is distributed across all coordinates of\nar X\niv :1\n70 9.\n03 01\n0v 1\n[ cs\n.C L\n] 9\nS ep\n2 01\n7\nthe embedding vector in a way that is hard to disentangle, and thus control. In order to gain insight into the full distribution of what a decoder might produce given the prompt sentence as input, the model has to be heavily (and sometimes cleverly) sampled. The second problem is that neural models only become highly functional after training with very large amounts of data, while the strongly recognizable style usually must be defined by a relatively tiny corpus of examples (e.g., all Seinfeld episodes, or all popular song lyrics).\nIn this paper, we address the challenge of how to enforce the decoder models to mimic a specific language style with only thousands of target sentences, as well as generating specific content in that style. We developed and experimented with several training and decoding procedures to allow the model to adapt to target language style and follow additional content guidance. Our experiments, conducted on an open-domain corpus of Twitter conversations and small persona corpora, show that our methods are capable of responding to queries in a transferred style without significant loss of relevance, and can respond within a specific topic as restricted by a human. Some examples of \u2018scenting\u2019 the base conversation model with particular styles are shown in Table 1. More can be found in the Supplementary Material."}, {"heading": "2 Related Work", "text": "Recurrent neural network based encoder-decoder models have been applied to machine translation and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015). As an extension, the attention mechanism enables the decoder to revisit the input sequence\u2019s hidden states and dynamically collects information needed for each decoding step. Specifically, our conversation model is established based on a combination of the models of (Bahdanau et al., 2014) and (Luong et al., 2015) that we found to be effective. In section 3, we describe the attention-based neural encoder-decoder model we used in detail.\nThis work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have\nused neural encoder-decoder models to directly generate responses in an end-to-end fashion without relying on SMT phrase tables(Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).\nLi et al. (2016) defined a \u201cpersona\u201d as the character that an artificial agent, as actor, plays or performs during conversational interactions. Their dataset requires user identification for all speakers in the training set, while our methods treat the base data (millions of twitter conversations) as unlabeled, and the target persona is defined simply by a relatively small sample of their speech. In this sense, the persona can be any set of text data. In our experiments, for example, we used a generic Star Wars character that was based on the entire set of Star Wars scripts (in addition to 46 million base conversations from Twitter). This provides us with a system that can talk about almost anything, being able to respond to most prompts, but in a recognizable Star Wars style. Other possibilities include training (styling) on famous personalities, or certain types of poetry, or song lyrics, or even mixing styles by providing two or more datasets for styling. Thus our targets are highly recognizable styles, and use of these for emphasis (or caricature) by human puppeteers who can choose from multiple options and guide neural models in a direction they like. We expect that these tools might not only be useful in conversational systems, but could also be popular in social media for text authoring that goes well beyond spelling/grammar auto correction."}, {"heading": "3 Neural Encoder-Decoder Background", "text": "In general, neural encoder-decoder models aim at generating a target sequence Y = ( y1, . . . , yTy ) given a source sequenceX = (x1, . . . , xTx). Each word in both source and target sentences, xt or yt, belongs to the source vocabulary Vx, and the target vocabulary Vy respectively.\nFirst, an encoder converts the source sequence X into a set of context vectors C = {h1,h2, . . . ,hTx}, whose size varies with regard to the length of the source passage. This context representation is generated using a multi-layered recurrent neural network (RNN). The encoder RNN reads the source passage from the first token until the last one, where hi = \u03a8 (hi\u22121,Ex [xt]) . Here Ex \u2208 R|Vx|\u00d7d is an embedding matrix containing vector representations of words, and \u03a8 is\na recurrent activation unit that we employ in the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).\nThe decoder, which is also implemented as an RNN, generates one word at a time, based on the context vector set returned by the encoder. The decoder\u2019s hidden state h\u0304t is a fixed-length continuous vector that is updated in the same way as encoder. At each time step t in the decoder, a time-dependent attentional context vector ct is computed based on the current hidden state of the decoder h\u0304t and the whole context set C.\nDecoding starts by computing the contentbased score of each context vector as: et,i = h\u0304>t Wahi. This relevance score measures how helpful the i-th context vector of the source sequence is in predicting next word based on the decoder\u2019s current hidden state h\u0304>t . Relevance scores are further normalized by the softmax function: \u03b1t,i =\nexp(et,i)\u2211Tx j=1 exp(et,j) , and we call \u03b1t,i the at-\ntention weight. The time-dependent context vector ct is then the weighted sum of the context vectors with their attention weights from above: ct = \u2211Tx i=1 \u03b1t,ihi.\nWith the context vector ct and the hidden state ht, we then combine the information from both vectors to produce an attentional hidden state as follow: zt = tanh(Wc[ct;ht]). The probability distribution for the next target symbol is computed by p(yt = k|y\u0303<t, X) \u221d exp(Wszt + bt)."}, {"heading": "4 Decoding with Selective Sampling", "text": "The standard objective function for neural encoder-decoder models is the log-likelihood of target T given source S, which at test time yields the statistical decision problem:\nT\u0302 = arg max T\n{ log p(T |S)}. (1)\nHowever, as discussed in (Li et al., 2015; Shao et al., 2017), simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as \u201cI am not sure\u201d. In section 7.3, we present a ranking experiment in which we verify that an RNN-based neural decoder provides a poor approximation of the above conditional probability, and instead biases towards the target language model p(T ). Fortunately, the backward model p(S|T ) empirically perform much better than p(T |S) on the relevance ranking task. Therefore, we directly apply Bayes\u2019\nrule to Equation 1, as in statistical machine translation (Brown et al., 1993), and use:\nT\u0302 = arg max T\n{ log p(S|T ) + log p(T )}. (2)\nSince p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Maximum Mutual Information (MMI) objective function in (Li et al., 2015).\nThe challenge now is to develop an effective search algorithm for a target words sequence that maximize the product in Equation 2. Here, we follow a similar process as in (Wen et al., 2015) which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above. However, as also observed by (Shao et al., 2017), step-by-step naive sampling can accumulate errors as the sequence gets longer.\nTo reduce language errors of stochastic sampling, we introduce a sample selector to choose the next token among N stochastically sampled tokens based on the predicted output word distributions. The sample selector, which is a multilayer perceptron in our experiments, takes the following features: 1) the log-probability of current sample word in p(wt|S); 2) the entropy of current predicted word distribution,\u2211\nwt P (wt|S) logP (wt|S) for all wt in the vocabulary; 3) the log-probability of current sample word in p(wt|\u2205), which we found effective in ranking task. The selector outputs a binary variable that indicates whether the current sample should be accepted or rejected.\nAt test time, if none of theN sampled tokens are above the classification threshold, we choose the highest scored token. If there are more than 1 acceptable samples amongN stochastically sampled tokens, we randomly choose one among them. Ideally, this permits us to safely inject diversity while maintaining language fluency. We also use the sample acceptor\u2019s probabilities as the language model score P (T ) for objective in equation 2.\nAs regards directly integrating beam-search, we found (a) that beam-search often produces a set of similar top-N candidates, and (b) that decoding with only the objective p(Y |X) can easily lead to irrelevant candidates. (See section 7.3) Therefore, we use the selective-sampling method to generate candidates for all our experiments; this (a) samples stochastically then (b) selects using a learned objective from data. The sample-then-select ap-\nproach encourages more diversity (v.s. MMI\u2019s beam-search) while still maintain language fluency (v.s. naive-sampling)."}, {"heading": "5 Output style restriction using a small \u2018scenting\u2019 dataset", "text": "In this section, we propose three simple yet effective methods of influencing the language style of the output in the neural encoder-decoder framework. Our language style restricting setup assumes that there is a large open-domain parallel corpus that provides training for context-response relevance, and a smaller monologue speaker corpus that reflects the language characteristics of the target speaker. We will refer to this smaller set as a \u2018scenting\u2019 dataset, since it hints at, or insinuates, the characteristics of the target speaker.\n5.1 Rank: Search in the Target Corpus\nOur first approach to scenting is to simply use the all sentences in the target speaker\u2019s corpus as generation candidates, ranked by the objective (2) for a given prompt. Since these sentences are naturally-occurring instead of generated wordby-word, we can safely assume p(T ) is constant (and high), and so the objective only requires sorting the sentences based on the backward model p(S|T ).\nRNN-based ranking methods are among the most effective methods for retrieving relevant responses (Wang and Nyberg, 2015, 2016). Thus this approach is a very strong baseline. Its limitation is also obvious: by limiting all possible responses to a fixed finite set of sentences, this method cannot provide a good response if such a response is not already in the scenting dataset.\n5.2 Multiply: Mixing the base model and the target language model during generation\nIn our second method we use both the vanilla encoder-decoder model trained on open-domain corpus and the target domain language model trained on the corpus while decoding output sentence. The idea is to use a speaker\u2019s language model, which is also RNN-based in our experiments, to restrict the open-domain encoderdecoder model\u2019s step-by-step word prediction. Similar ideas have been tested in domain adaptation for statistical machine translation (Koehn and Schroeder, 2007), where both in-domain and open-domain translation tables were used as can-\ndidates for generating target sentence. Because open-domain encoder-decoder models are trained with various kinds of language patterns and topics, choosing a sequence that satisfies both models may produce relevant responses that are also in the target language style. We found that a straightforward way of achieving this is to multiply the two models\u2019 distributions p1(t|S)\u03bb1p2(t)\u03bb2 at each point and then re-normalize before sampling. The weights can be tuned either by the perplexity on the validation set, or through manually controlling the trade-off between style restriction and answer accuracy.\n5.3 Finetune: Over-training on Target Corpus with Pseudo Context\nFine-tuning is a widely used in the neural network community to achieve transfer learning. This strategy permits us to train the neural encoder-decoder on a larger general parallel corpus, and then use the learned parameters to initialize the training of a styled model. Most of the time, however, the target speaker\u2019s corpus will lack training data in parallel form. For example, if we train on song lyrics or movie scripts, or political speeches, the data will not be in a question-answer form. To make encoder-decoder overtraining possible, we treat every sentence in the scenting corpus as a target sentence T generated a pseudo context from the backward model p(S|T ) trained on the opendomain corpus. Over-training on such pairs imparts the scenting dataset\u2019s language characteristics, while retaining the generality of the original model. We also found that the previous sentence in the styled corpus (i.e., previous sentence in the speech) provides helpful context for the current sentence, analogous with a question-answer link. Thus we use both pseudo context and the previous sentence as possible sources S to fine-tune the in-domain decoder. To avoid overfitting, we stop overtraining when the perplexity on the in-domain validation set starts to increase. A corresponding sample acceptor is also trained for the fine-tuned model: we found it helpful to initialize this from the open-domain model\u2019s sample acceptor."}, {"heading": "6 Restricting the Output Topic", "text": "We further introduce a topic restricting method for neural decoders based on the Counting Grid (Jojic and Perina, 2011) model, by treating language guidance as a topic embedding. Our model exten-\nsion provides information about the output topic in the form of an additional topic embedding vector to the neural net at each time step.\n6.1 CG: Counting Grids The basic counting grid \u03c0k is a set of distributions on the d-dimensional toroidal discrete grid E indexed by k. The grids in this paper are bidimensional and typically from (Ex = 32) \u00d7 (Ey = 32) to (Ex = 64) \u00d7 (Ey = 64) in size. The index z indexes a particular word in the vocabulary z = [1 . . . Z]. Thus, \u03c0i(z) is the probability of the word z at the d-dimensional discrete location i, and \u2211 z \u03c0i(z) = 1 at every location on the grid. The model generates bags of words, each represented by a list of words w = {wn}Nn=1 with each word wn taking an integer value between 1 and Z. The modeling assumption in the basic CG model is that each bag is generated from the distributions in a single window W of a preset size, e.g., (Wx = 5)\u00d7 (Wy = 5). A bag can be generated by first picking a window at a d-dimensional location `, denoted as W`, then generating each of the N words by sampling a location kn for a particular micro-topic \u03c0kn uniformly within the window, and sampling from that micro-topic.\nBecause the conditional distribution p(kn|`) is a preset uniform distribution over the grid locations inside the window placed at location `, the variable kn can be summed out (Jojic and Perina, 2011), and the generation can directly use the grouped histograms\nh`(z) = 1 |W| \u2211 j\u2208W` \u03c0j(z), (3)\nwhere |W| is the area of the window, e.g. 25 when 5\u00d75 windows are used. In other words, the position of the window ` in the grid is a latent variable given which we can write the probability of the bag as\nP (w|`) = \u220f\nwn\u2208w\nh`(wn) = \u220f\nwn\u2208w\n( 1 |W| \u00b7 \u2211 j\u2208W` \u03c0j(wn) ) (4)\nAs the grid is toroidal, a window can start at any position and there is as many h distributions as there are \u03c0 distributions. The former will have a considerably higher entropy as they are averages of many \u03c0 distributions. Although the basic CG model is essentially a simple mixture assuming the existence of a single source (one window) for all the features in one bag, it can have a very large\nnumber of (highly related) choices h to choose from. Topic models (Blei et al., 2003; Lafferty and Blei, 2006), on the other hand, are admixtures that capture word co-occurrence statistics by using a much smaller number of topics that can be more freely combined to explain a single document (and this makes it harder to visualize the topics and pinpoint the right combination of topics to use in influencing the output).\nIn a well-fit CG model, each data point tends to have a rather peaky posterior location distribution because the model is a mixture. The CG model can be learned efficiently using the EM algorithm because the inference of the hidden variables, as well as updates of \u03c0 and h can be performed using summed area tables (Crow, 1984), and are thus considerably faster than most of the sophisticated sampling procedures used to train other topic models. The use of overlapping windows helps both in controlling the capacity of the model and in organizing topics on the grid automatically: Two overlapping windows have only slightly different h distributions, making CGs especially useful in visualization applications where the grid is shown in terms of the most likely words in the component distributions \u03c0 (Perina et al., 2014).1\nHaving trained the grid on some corpus (in our case a sample of the base model\u2019s corpus), the mapping of either a source S and/or target T sentence can be obtained by treating the sentences as bags of words. By appending one or both of these mappings to the decoder\u2019s embedding of the target T , the end-to-end encoder-decoder learning can be performed in a scenario where the decoder is expected to get an additional hint through a CG mapping. In our experiments, we only used the embedding of the target T as the decoder hint, and we appended the full posterior distribution over CG locations to the encoder\u2019s embedding. At test time, we only have the S and need to generate T without knowing where it may map in the counting grid. We considered two ways of providing a mapping: \u2022 The user provides a hint sentence H (could\nbe just a few words in any order), and the CG mapping of the user\u2019s hint, i.e. the full posterior distribution p(`|H), is used in the decoding. The posterior probabilities over 32\u00d7 32 grid locations are unwrapped into a vector\n1(Chen et al., 2017) have recently proposed using LDA for topic modeling in Sequence-To-Sequence response generation models. We believe that the CG embedding used here will prove easier to apply and interpret through visualization.\nwith a size of |L| = 1024, and then concatenated with the word embedding as the input at each time-step. That acts to expand the user\u2019s hint into a sentence with similar content (and style if the model is also styled). \u2022 The CG is scanned and a variety of mappings\nare tested as inputs to provide a diverse set of possible answers. In our experiments, instead of scanning over all 1024 possible locations in the grid, we retrieved several possible answers using information retrieval (ranking of the data samples in the training set based on the source S and picking the top ten). Then the CG mapping p(`|H) of these retrieved hints is used to decode several samples from each.\nAs an example, Figure 1 shows a portion of a CG trained on randomly chosen 800k tweets from the twitter corpus. In each cell of the grid, we show the top words in the distribution \u03c0j(z) over words (z) in that location (j). (Each cell has a distribution over the entire vocabulary). As a response to \u201cI am hungry,\u201d using two highlighted areas as hints, we can generate either a set of empathic responses, such as \u2018Me too,\u2019 or food suggestions, such as \u2018Let\u2019s have cake.\u2019 It will also be evident\nthat some areas of the grid may produce less sensical answers. These can later be pruned by likelihood criteria or by user selection."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 Datasets", "text": "Yahoo! Answer Dataset. We use the Comprehensive Questions and Answers dataset2 to train and validate the performances of different decoding setups with ranking experiments described in section 7.3. This dataset contains 4.4 million Yahoo! Answers questions and the user-selected best answers. Unlike the conversational datasets, such as the Twitter dataset described below, it contains more relevant and specific responses for each question, which leads to less ambiguity in ranking.\nTwitter Conversation Dataset. We trained our base encoder-decoder models on the Twitter Conversation Triple Dataset described in (Sordoni et al., 2015), which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month\n2http://webscope.sandbox.yahoo.com/ catalog.php?datatype=l\nperiod from June through August 2012. For the purposes of our experiments, we split the triples into context-message and message-response pairs yielding 46M source-target pairs. For tuning and evaluation, we used the development dataset of size 200K conversation pairs and the test dataset of 5K examples. The corpus is preprocessed using a Twitter specific tokenizer (O\u2019Connor et al., 2010). The vocabulary size is limited to 50,000 excluding the special boundary symbol and the unknown word tag.\nScenting datasets. A variety of persona characters have been trained and tested, including Hillary Clinton, Donald Trump, John F. Kennedy, Richard Nixon, singer-songwriters, stand-up comedians, and a generic Star Wars character. In experiments, we evaluated on a diverse set of representative target speakers:\nJFK. We mainly tested our models on John F. Kennedy\u2019s speeches collected from American Presidency Project3, which contains 6474 training and 719 validation sentences.\nStar Wars. Movie subtitles of three Star Wars movies are also tested4. They are extracted from Cornell Movie-Dialogs Corpus (DanescuNiculescu-Mizil and Lee, 2011), and have 495 training and 54 validation sentences.\nSinger-Songwriter. We also evaluated our approach on a lyric corpus from a collective of singers: Coldplay, Linkin Park, and Green Day. The lyric dataset is collected from mldb.org and has 9182 training and 1020 validation lines.\nDebate Chat Contexts. We designed testing questionnaires with 64 chat contexts spanning a range of topics in politic, science, and technology: the sort of questions we might ask in an entertaining political debate.5 To test the model\u2019s ability to control output topic in section 7.4.3, we also created one hint per question."}, {"heading": "7.2 Network Setup and Implementation", "text": "Our encoder and decoder RNNs contains twolayer stacked LSTMs. Each LSTM layer has a memory size of 500. The network weights are randomly initialized using a uniform distribution (\u22120.08, 0.08), and are trained with the ADAM optimizer (Kingma and Ba, 2014), with\n3http://www.presidency.ucsb.edu/ 4 Koncel-Kedziorski et al. (2016) also uses Star Wars\nscripts to test theme rewriting of algebra word problems. 5See the Supplementary material.\nan initial learning rate of 0.002. Gradients were clipped so their norm does not exceed 5. Each mini-batch contains 200 answers and their questions. The words of input sentences were first converted to 300-dimensional vector representations learned from the RNN based language modeling tool word2vec (Mikolov et al., 2013). The beginning and end of each passage are also padded with a special boundary symbol. During decoding, our model generates 500 candidate samples in parallel, then ranks them. As these are processed in batches on GPU, generation is very efficient. We also experimented incorporating an information retrieval (IR) module to automatically collect topic hints for CG-based decoder. Specifically, a full-text index of twitter corpus is built using solr6, and the top 10 searched results based on the source sentence are be used to generate posterior CG distributions as hints."}, {"heading": "7.3 Validating the Decoding Setup with Ranking", "text": "We performed a ranking evaluation applying different decoding setups on the Yahoo! Answers dataset. Here we wanted to test the relevance judgment capacities of different setups, and validate the necessity of the new decoding method discussed in section 4. Yahoo! Answers question is used as source S, and its answer is treated as target T . Each test question is associated with one true answer and 19 random answers from the test set. MRR (Mean Reciprocal Rank) and P@1 (precision of top1) were then used as evaluation metrics.\nTable 2 shows the answer ranking evaluation results: the forward model P (T |S), by itself is close to the performance of random selection in distinguishing true answer from wrong answers. This implies that a naive beam search over only the forward model may generate irrelevant outputs. One hypothesis was that P (T |S) is biased toward P (T ), and performance indeed improves after normalizing by P (T ). However, it is difficult to directly decode with objective P (T |S)/P (T |\u2205), because this objective removes the influence of the target-side language model. Decoding only according to this function will thus result in only low-frequency words and ungrammatical sentences, behavior also noted by (Li et al., 2015; Shao et al., 2017).\n6https://lucene.apache.org/solr/"}, {"heading": "7.4 Human Evaluations", "text": ""}, {"heading": "7.4.1 Systems", "text": "We tested 10 different system configurations to evaluate the overall output quality, and their abilities of influencing output language style and topic: \u2022 vanilla-sampling each word in the target. \u2022 selective-sampling as described in section 4;\nall the following systems are using it as well. \u2022 cg-ir uses IR results to create counting grid\ntopic hints (sections 6.1 and 7.2). \u2022 rank uses proposals from the full JFK corpus\nas in section 5.1. \u2022 multiply with a JFK language model as in\nsection 5.2. \u2022 finetune with JFK dataset as in section 5.3. \u2022 finetune-cg-ir uses IR results as topic hints\nfor fine-tuned JFK. \u2022 finetune-cg-topic forced to use the given\ntopic hint for fine-tuned JFK. \u2022 singer-songwriter fine-tuned cg-topic. \u2022 starwars fine-tuned cg-topic."}, {"heading": "7.4.2 Evaluation Setup", "text": "Owing to the low consistency between automatic metrics and human perception on conversational tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT). Workers were selected based on their AMT prior approval rate (>95%). Each questionnaire was presented to 3 different workers. We evaluated our proposed models on the 64 debate chat contexts. Each of the evaluated methods generated 3 samples for every chat context. To ensure calibrated ratings between systems, we show the human judges all system outputs (randomly ordered) for each particular test case at the same time. For each chat context, we conducted three kinds of assessments:\nQuality Assessment Workers were provided with the following guidelines: \u201cGiven the chat\ncontext, a chat-bot needs to continue the conversation. Rate the potential answers based on your own preference on a scale of 1 to 5 (the highest):\u201d\n\u2022 5-Excellent: \u201cVery appropriate response, and coherent with the chat context.\u201d \u2022 4-Good: \u201cCoherent with the chat context.\u201d \u2022 3-Fair: \u201cInterpretable and related. It is OK\nfor you to receive this chat response.\u201d \u2022 2-Poor: \u201cInterpretable, but not related.\u201d \u2022 1-Bad: \u201cNot interpretable.\u201d\nIn this test, the outputs of all 10 systems evaluated are then provided to worker together for a total of 30 responses. In total, we gathered 64 \u00b7 30 \u00b7 3 = 5760 ratings for quality assessments, and 47 different workers participated.\nStyle Assessment. We provided following instructions: \u201cWhich candidate responses are likely to have come from or are related to [Persona Name]?\u201d. Checkboxes were provided for the responses from style-influenced systems and from selective-sampling as a baseline.\nTopic Assessment. The instruction was: \u201cWhich candidate answers to the chat context above are similar or related to the following answer: \u2018[a hint topic provided by us]\u2019?\u201d. This was also a checkbox questionnaire. Candidates are from both style- and topic-influenced systems (fine-tuned cg-topic), and from selective-sampling as a baseline."}, {"heading": "7.4.3 Results", "text": "Overall Quality. We conducted mean opinion score (MOS) tests for overall quality assessment of generated responses with questionnaires described above. Table 3 shows the MOS results with standard error. It can be seen that all the systems based on selective sampling are significantly better than vanilla sampling baseline. When restricting output\u2019s style and/or topic, the MOS score results of most systems do not decline significantly except singer-songwriter, which attempts to generate lyrics-like outputs in response to to political debate questions, resulting in uninterpretable strings.\nOur rank method uses p(S|T ) to pick the answer from the original persona corpus, and is thus as good at styling as the person themselves. Because most of our testing questionnaire is political, the rank was indeed often able to find related answers in the dataset (JFK). Also, unlike generation-based approaches, rank has oraclelevel language fluency and it is expected to have quality score of at least 2 (\u201cInterpretable, but not related\u201d). Overall, however, the quality score of rank is still lower than other approaches. Note that a hybrid system can actually chose between rank and the decoder\u2019s outputs based on likelihood, as shown in the example of bJFk-bNixon debate in the supplemental material.\nInfluencing the Style. Table 3 also shows the likelihood of being labeled as JFK for different methods. It is encouraging that finetune based approaches have similar chances as the rank system which retrieves sentences directly from JFK corpus, and are significantly better than the selectivesampling baseline.\nInfluencing both Style and Topic. Table 4 summarizes the results in terms of style (the fraction of answers labeled as in-style for the target persona), and topic (the percentage of answers picked as related to the human-provided topic hint text). We used the last three of the ten listed systems, which are both styled and use specific topic hints to generate answers. These results demonstrate that it is indeed possible to provide simple prompts to a styled model and drive their answers in a desired direction while picking up the style of the persona. It also shows that the style of some characters is harder to recreate than others. For example, workers are more likely to label baseline results as lyrics from a singer-songwriter than lines from Star Wars movies, which might be because lyrics often take significant freedom with structure and grammar. We also found that it is harder for Star Wars and Singer-Songwriter bots to follow topic hints than it is for the John F. Kennedy model, largely because the political debate questions we used overlap less with the topics found in the scenting datasets for those two personas."}, {"heading": "8 Conclusions", "text": "In this study we investigated the possibility of steering the style and content in the output of a neural encoder-decoder model7. We showed that acquisition of highly recognizable styles of famous personalities, characters, or professionals, is achievable, and that it is even possible to allow users to influence the topic direction of conversations. The tools described in the paper are not only useful in conversational systems (e.g., chatbots), but can also be useful as authoring tools in social media. In the latter case, the social media users might use neural models as consultants to help with crafting responses to any post the user is reading. The AMT tests show that these models do indeed provide increased recognizability of the style, without sacrificing quality or relevance."}, {"heading": "Acknowledgments", "text": "We thank Shashank Srivastava, Donald Brinkman, Michel Galley, and Bill Dolan for useful discussions and encouragement. Di Wang is supported by the Tencent Fellowship and Yahoo! Fellowship, to which we gratefully acknowledge.\n7The code and testing data are available at https://github.com/digo/ steering-response-style-and-topic"}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Latent Dirichlet Allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "J. Mach. Learn. Res. 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The Mathematics of Statistical Machine Translation: Parameter Estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Comput. Linguist. 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Topic aware neural response generation", "author": ["Xing Chen", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, Califor-", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Summed-area Tables for Texture Mapping", "author": ["Franklin C Crow."], "venue": "Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques. New York, NY, USA, pages 207\u2013212.", "citeRegEx": "Crow.,? 1984", "shortCiteRegEx": "Crow.", "year": 1984}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs", "author": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee."], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational", "citeRegEx": "Danescu.Niculescu.Mizil and Lee.,? 2011", "shortCiteRegEx": "Danescu.Niculescu.Mizil and Lee.", "year": 2011}, {"title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Multidimensional Counting Grids: Inferring Word Order from Disordered Bags of Words", "author": ["Nebojsa Jojic", "Alessandro Perina."], "venue": "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence. AUAI Press, Arlington, Vir-", "citeRegEx": "Jojic and Perina.,? 2011", "shortCiteRegEx": "Jojic and Perina.", "year": 2011}, {"title": "Controlling Output Length in Neural Encoder-Decoders", "author": ["Yuta Kikuchi", "Graham Neubig", "Ryohei Sasano", "Hiroya Takamura", "Manabu Okumura."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP", "citeRegEx": "Kikuchi et al\\.,? 2016", "shortCiteRegEx": "Kikuchi et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Experiments in Domain Adaptation for Statistical Machine Translation", "author": ["Philipp Koehn", "Josh Schroeder."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA,", "citeRegEx": "Koehn and Schroeder.,? 2007", "shortCiteRegEx": "Koehn and Schroeder.", "year": 2007}, {"title": "A themerewriting approach for generating algebra word problems", "author": ["Rik Koncel-Kedziorski", "Ioannis Konstas", "Luke Zettlemoyer", "Hannaneh Hajishirzi."], "venue": "CoRR abs/1610.06210.", "citeRegEx": "Koncel.Kedziorski et al\\.,? 2016", "shortCiteRegEx": "Koncel.Kedziorski et al\\.", "year": 2016}, {"title": "Correlated Topic Models", "author": ["John D Lafferty", "David M Blei."], "venue": "Y Weiss, P B Sch\u00f6lkopf, and J C Platt, editors, Advances in Neural Information Processing Systems 18, MIT Press, pages 147\u2013154.", "citeRegEx": "Lafferty and Blei.,? 2006", "shortCiteRegEx": "Lafferty and Blei.", "year": 2006}, {"title": "A Persona-Based Neural Conversation Model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv page 10.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A DiversityPromoting Objective Function for Neural Conversation Models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "William B. Dolan."], "venue": "Arxiv pages 110\u2013119.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lis-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "TweetMotif : Exploratory search and topic summarization for Twitter", "author": ["B O\u2019Connor", "M Krieger", "D Ahn"], "venue": "4th International AAAI Conference on Weblogs and Social Media", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Skim-reading thousands of documents in one minute: Data indexing and visualization for multifarious search", "author": ["Alessandro Perina", "Dongwoo Kim", "Andrzej Turski", "Nebojsa Jojic."], "venue": "Workshop on Interactive Data Exploration and Analytics (IDEA\u201914)", "citeRegEx": "Perina et al\\.,? 2014", "shortCiteRegEx": "Perina et al\\.", "year": 2014}, {"title": "Data-driven Response Generation in Social Media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Stroudsburg,", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A Neural Attention Model for Abstractive Sentence Summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portu-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural Responding Machine for Short-Text Conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "CoRR abs/1503.0.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Generating Long and Diverse Responses with Neural Conversation Models", "author": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil"], "venue": null, "citeRegEx": "Shao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2017}, {"title": "A Neural Network Approach to ContextSensitive Generation of Conversational Responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "William B. Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Evaluating Evaluation Methods for Generation in the Presence of Variation", "author": ["Amanda Stent", "Matthew Marge", "Mohit Singhai."], "venue": "Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing.", "citeRegEx": "Stent et al\\.,? 2005", "shortCiteRegEx": "Stent et al\\.", "year": 2005}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. MIT Press, Cambridge, MA, USA, pages", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A Neural Conversational Model", "author": ["Orioi Vinyals", "Quoc V. Le."], "venue": "ICML Deep Learning Workshop 2015 37.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "author": ["Di Wang", "Eric Nyberg."], "venue": "Annual Meeting of the Association for Computational Linguistics. pages 707\u2013 712.", "citeRegEx": "Wang and Nyberg.,? 2015", "shortCiteRegEx": "Wang and Nyberg.", "year": 2015}, {"title": "CMU OAQA at TREC 2016 LiveQA: An Attentional Neural Encoder-Decoder Approach for Answer Ranking", "author": ["Di Wang", "Eric Nyberg."], "venue": "Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016, Gaithersburg, Maryland,", "citeRegEx": "Wang and Nyberg.,? 2016", "shortCiteRegEx": "Wang and Nyberg.", "year": 2016}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Peihao Su", "David Vandyke", "Steve J. Young."], "venue": "Proceedings of the 2015 Conference on Em-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044 .", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 4, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 0, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 18, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 33, "context": ", 2016), image captioning (Xu et al., 2015), summarization (Rush et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 23, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 7, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 10, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 29, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 26, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 24, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 25, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 16, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 0, "context": "and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 46, "endOffset": 89}, {"referenceID": 18, "context": "and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 46, "endOffset": 89}, {"referenceID": 0, "context": "Specifically, our conversation model is established based on a combination of the models of (Bahdanau et al., 2014) and (Luong et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 18, "context": ", 2014) and (Luong et al., 2015) that we found to be effective.", "startOffset": 12, "endOffset": 32}, {"referenceID": 22, "context": "This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem.", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": ", 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem.", "startOffset": 12, "endOffset": 34}, {"referenceID": 22, "context": "(2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history.", "startOffset": 16, "endOffset": 37}, {"referenceID": 22, "context": "This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al.", "startOffset": 53, "endOffset": 232}, {"referenceID": 8, "context": "Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 30, "endOffset": 64}, {"referenceID": 16, "context": "However, as discussed in (Li et al., 2015; Shao et al., 2017), simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as \u201cI am not sure\u201d.", "startOffset": 25, "endOffset": 61}, {"referenceID": 25, "context": "However, as discussed in (Li et al., 2015; Shao et al., 2017), simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as \u201cI am not sure\u201d.", "startOffset": 25, "endOffset": 61}, {"referenceID": 2, "context": "Therefore, we directly apply Bayes\u2019 rule to Equation 1, as in statistical machine translation (Brown et al., 1993), and use:", "startOffset": 94, "endOffset": 114}, {"referenceID": 16, "context": "Since p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Maximum Mutual Information (MMI) objective function in (Li et al., 2015).", "startOffset": 153, "endOffset": 170}, {"referenceID": 32, "context": "Here, we follow a similar process as in (Wen et al., 2015) which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above.", "startOffset": 40, "endOffset": 58}, {"referenceID": 25, "context": "However, as also observed by (Shao et al., 2017), step-by-step naive sampling can accumulate errors as the sequence gets longer.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": "Similar ideas have been tested in domain adaptation for statistical machine translation (Koehn and Schroeder, 2007), where both in-domain and", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "We further introduce a topic restricting method for neural decoders based on the Counting Grid (Jojic and Perina, 2011) model, by treating language", "startOffset": 95, "endOffset": 119}, {"referenceID": 9, "context": "Because the conditional distribution p(kn|`) is a preset uniform distribution over the grid locations inside the window placed at location `, the variable kn can be summed out (Jojic and Perina, 2011), and the generation can directly use the grouped histograms", "startOffset": 176, "endOffset": 200}, {"referenceID": 5, "context": "The CG model can be learned efficiently using the EM algorithm because the inference of the hidden variables, as well as updates of \u03c0 and h can be performed using summed area tables (Crow, 1984), and are thus", "startOffset": 182, "endOffset": 194}, {"referenceID": 21, "context": "overlapping windows have only slightly different h distributions, making CGs especially useful in visualization applications where the grid is shown in terms of the most likely words in the component distributions \u03c0 (Perina et al., 2014).", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "(Chen et al., 2017) have recently proposed using LDA for topic modeling in Sequence-To-Sequence response generation models.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "We trained our base encoder-decoder models on the Twitter Conversation Triple Dataset described in (Sordoni et al., 2015), which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month", "startOffset": 99, "endOffset": 121}, {"referenceID": 20, "context": "Twitter specific tokenizer (O\u2019Connor et al., 2010).", "startOffset": 27, "endOffset": 50}, {"referenceID": 11, "context": "08), and are trained with the ADAM optimizer (Kingma and Ba, 2014), with", "startOffset": 45, "endOffset": 66}, {"referenceID": 13, "context": "edu/ 4 Koncel-Kedziorski et al. (2016) also uses Star Wars scripts to test theme rewriting of algebra word problems.", "startOffset": 7, "endOffset": 39}, {"referenceID": 19, "context": "tool word2vec (Mikolov et al., 2013).", "startOffset": 14, "endOffset": 36}, {"referenceID": 16, "context": "Decoding only according to this function will thus result in only low-frequency words and ungrammatical sentences, behavior also noted by (Li et al., 2015; Shao et al., 2017).", "startOffset": 138, "endOffset": 174}, {"referenceID": 25, "context": "Decoding only according to this function will thus result in only low-frequency words and ungrammatical sentences, behavior also noted by (Li et al., 2015; Shao et al., 2017).", "startOffset": 138, "endOffset": 174}, {"referenceID": 17, "context": "metrics and human perception on conversational tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT).", "startOffset": 53, "endOffset": 91}, {"referenceID": 27, "context": "metrics and human perception on conversational tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT).", "startOffset": 53, "endOffset": 91}], "year": 2017, "abstractText": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoderdecoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selectivesampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks.", "creator": "LaTeX with hyperref package"}}}