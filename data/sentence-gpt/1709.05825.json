{"id": "1709.05825", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Relational Marginal Problems: Theory and Estimation", "abstract": "In the propositional setting, the marginal problem is to find a (maximum-entropy) distribution that has some given marginals. We study this problem in a relational setting and make the following contributions. First, we compare two different notions of relational marginals. Second, we show a duality between the resulting relational marginal problems and the maximum likelihood estimation of the parameters of relational models, which generalizes a well-known duality from the propositional setting. Third, by exploiting the relational marginal formulation, we present a statistically sound method to learn the parameters of relational models that will be applied in settings where the number of constants differs between the training and test data. Furthermore, based on a relational generalization of marginal polytopes, we characterize cases where the standard estimators based on feature's number of true groundings needs to be adjusted and we quantitatively characterize the consequences of these adjustments. Fourth, we prove bounds on expected errors of the estimated parameters, which allows us to lower-bound, among other things, the effective sample size of relational training data. Finally, we demonstrate the inverse relationship between the minimum- and maximum-entropy estimates of posterior models. Finally, we demonstrate the inverse relationship between the mean and minimum-entropy estimates of posterior models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 18 Sep 2017 09:10:27 GMT  (32kb)", "http://arxiv.org/abs/1709.05825v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ondrej kuzelka", "yuyi wang", "jesse davis", "steven schockaert"], "accepted": false, "id": "1709.05825"}, "pdf": {"name": "1709.05825.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yuyi Wang", "Jesse Davis", "Steven Schockaert"], "emails": ["KuzelkaO@cardiff.ac.uk", "yuwang@ethz.ch", "jesse.davis@cs.kuleuven.be", "SchockaertS1@cardiff.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 9.\n05 82\n5v 1\n[ cs\n.A I]\n1 8\nSe p\n20 17"}, {"heading": "Introduction", "text": "Statistical Relational Learning (SRL, Getoor and Taskar, eds., 2007) is concerned with learning probabilistic models of relational data. Many popular SRL frameworks, such as Markov Logic Networks (MLNs, Richardson and Domingos 2006), use weighted logical formulas to encode statistical regularities that hold for the considered problem. Typically, the maximum (pseudo-)likelihood weights of the formulas are estimated from training data, which is usually a single large example (e.g. a social network). This is problematic for two reasons. First, the weights that are learned from this single training example are in general not optimal for examples of different sizes. This turns out to be a fundamental problem, which can not simply be solved by rescaling the weights (Shalizi and Rinaldo 2013). Second, without making further assumptions, it is difficult to provide any statistical guarantees about the learned weights. In this paper, we approach parameter estimation in SRL from a novel direction, by introducing the notion of a relational marginal problem. In the propositional\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ncase (Wainwright and Jordan 2008), marginal problems entail finding a maximum-entropy distribution which has the givenmarginal probabilities. A well-known property of such problems is that they are dual to the maximum-likelihood estimation of the parameters of an undirected graphical model (where \u201cdual\u201d is in the sense of convex optimization). In relational marginal problems, we are similarly looking for a maximum-entropy distribution which satisfies some given statistics \u2013 relational marginals. However, we also need to define what these relational marginals are. Thus, first, we describe two different types of relational marginals, which differ in the kinds of statistics that are provided. The first type is based on relational marginal distributions (Kuz\u030celka, Davis, and Schockaert 2017) and the second is based on Halpern-style random substitution semantics (Bacchus et al. 1992). Second, for both types of statistics, we establish a relational counterpart of the duality between maximum-likelihood estimation and max-entropy marginal problems. Interestingly, for the latter model, the corresponding dual is MLNs.\nThird, the relational marginal perspective allows us to learn parameters for domains that have different sizes (i.e., number of constants) than the training data. The basic idea to achieve this is simple. We assume that the training data is a sample of the data that we want to model. For example, imagine trying to model all of Facebook based on a sampled subset of Facebook users along with all relations among them. Assuming the sample is a large enough and was obtained in a suitable way (which is not always the case in practice \u2013 we discuss this issue later), the parameters of the marginals estimated from the sample should be close to the respective parameters for the whole network. Then, instead of using a model learned by optimizing the likelihood on the training data, we use a model obtained as a solution of the corresponding relational marginal problemwith a domain of the required size. We may end up with estimated parameters for which the relational marginal problem has no solution. Therefore, we propose a method for adjusting the estimated parameters that enables a solution and characterize its effect on the estimates. Then we also the relational marginal polytopes, which allows us to provide conditions under which the unbiased unadjusted estimate will be valid (\u201crealizable\u201d) for domains of any size.\nIn addition, the relational marginal view of the param-\neter learning problem, can be thought of as consisting of two decoupled problems: estimation of the parameters of marginals and optimization to obtain the max-entropy distributions. Thus, to better understand parameter learning from relational data, it is important to characterize how accurate the estimates are. Assuming that all subsamples of the data being modeled are sampled with the same probability, we derive bounds on expected error, that is, the expected difference between the parameters obtained from the subsample and parameters that could be theoretically computed if the whole dataset were accessible (e.g. the whole Facebook). From this, we can also obtain lower-bounds on the effective sample size for relational data. The paper is structured around addressing the following four questions about relational marginal problems:\n1. What should the relational marginals be? (Section Two Types of Relational Marginals)\n2. What are the max-entropy distributions with the given relational marginals, and how can we find them? (Section Max-Entropy Models)\n3. When are relational marginal problems realizable, and how can we adjust them when they are not? How can we adjust learning to account for differences in domain sizes? (Section Realizability)\n4. How accurate are the parameter estimates of relational marginals, and what are the links with realizability? (Sections Relational Marginal Polytopes and Estimation)\nProofs of all propositions stated in the main text and details of the duality derivations are in the appendix located in the supplementary material."}, {"heading": "Preliminaries", "text": "This paper considers a function-free first-order logic language L, which is built from a set of constants Const, variables Var and predicates Rel = \u22c3 i Reli, where Reli contains the predicates of arity i. We assume an untyped language and use the domain size to refer to the |Const|. For a1, ..., ak \u2208 Const\u222aVar andR \u2208 Relk, we callR(a1, ..., ak) an atom. If a1, .., ak \u2208 Const, this atom is called ground. A literal is an atom or its negation. A clause is a disjunction of literals. We use vars(\u03b1) to denote the variables that appear in a formula \u03b1. The formula \u03b10 is called a grounding of \u03b1 if \u03b10 can be obtained by replacing each variable in \u03b1 with a constant from Const. A formula is called closed if all variables are bound by a quantifier. A possible world \u03c9 is defined as a set of ground atoms. The satisfaction relation |= is defined in the usual way. A substitution is a mapping from variables to terms. An injective substitution is a substitution which does not map any two variables to the same term. As commonly done in statistical relational learning, we use the unique names assumption, meaning that c1 6= c2 whenever c1 and c2 are different constants. A first-order universally quantified formula \u03b1 is said to be proper if \u03b1\u03d1 is trivially false whenever \u03d1 is not injective. For instance, the formula \u2200X,Y : fr(X,Y ) is not proper whereas the formula \u2200X,Y : fr(X,Y ) \u2227X 6= Y is proper. In what follows we sometimes omit writing the\nuniversal quantifiers explicitly for universally quantified formulas and write simply, e.g. fr(X,Y ) \u2227X 6= Y . A (global) example is a pair (A, C), with C a set of constants and A a set of ground atoms which only use constants from C. Let \u03a5 = (A, C) be an example and S \u2286 C. The fragment \u03a5\u3008S\u3009 = (B,S) is defined as the restriction of \u03a5 to the constants in S, i.e. B is the set of all atoms fromAwhich only contain constants from S. Two examples \u03a51 = (A1, C1) and \u03a52 = (A2, C2) are isomorphic, denoted as \u03a51\u2248\u03a52, if there exists a bijection \u03c3 : C1 \u2192 C2 such that \u03c3(A1) = A2, where \u03c3 is extended to ground atoms in the usual way.When C is a set of constants and\u03a60 a set of closed formulas, \u03a0(C,\u03a60) denotes the set of all \u03a5 = (A, C) such that \u03a5 |= \u03a60 (we can think of \u03a60 as a set of constraints)."}, {"heading": "Two Types of Relational Marginals", "text": "Typically, parameters for a statistical relational model are estimated from a single example of a relational structure that consists of a large set of ground atoms A. Intuitively, the goal is to learn a probability distribution of such relational structure. The challenge is how to estimate the distribution from a single example. One solution is based on the assumption that the relational structure has repeated regularities. Then, statistics about these regularities can be computed for small substructures of the train example and used to construct a distribution over large relational structures. Thus, the next issue is how to construct the fragments and compute statistics on them. Next, we discuss two possible ways to do so, which we will refer to as Model A and Model B."}, {"heading": "Model A", "text": "The first approach to constructing fragments is from (Kuz\u030celka, Davis, and Schockaert 2017). It repeatedly samples subsets S \u2286 C of the constants from the given example \u03a5 = (A, C) and then builds one training example \u03a5\u3008S\u3009 for each S. However, the training examples must consider isomorphic classes of constants to account for the fact that each fragment will contain different constants. This is accomplished by using the notion of local examples.\nDefinition 1 (Local example). Let k \u2208 N. A local example of width k is a pair \u03c9 = (A, {1, ..., k}), where A is a set of ground atoms that contain only constants from the set {1, 2, . . . , k}. For an example \u03a5 = (A, C) and S \u2286 C, we write \u03a5[S] for the set of all local examples of width |S| which are isomorphic to \u03a5\u3008S\u3009. To distinguish local examples from global examples, we will denote them using lower case Greek letters such as \u03c9 instead of upper case letters such as \u03a5.\nExample 1. For \u03a5 = ({fr(alice, bob), fr(bob, alice), fr(bob, eve), fr(eve, bob), sm(alice)}, {alice, bob, eve}), we have: \u03a5\u3008{alice, bob}\u3009= ({sm(alice), fr(alice, bob), fr(bob, alice)}, {alice, bob}), \u03a5[{alice, bob}]= {({fr(1, 2), fr(2, 1), sm(1)}, {1, 2}), {({fr(2, 1), fr(1, 2), sm(2)}, {1, 2})}. This leads to a natural definition of a probability distribution over local examples of width k.\nDefinition 2 (Relational marginal distribution of a global example). Let \u03a5 = (A, C) be an example and k \u2208 N. The relational marginal distribution of \u03a5 of width k is a distribution P\u03a5,k over local examples, where P\u03a5,k(\u03c9) is defined as the probability that \u03c9 is sampled by the following process: (i) Uniformly sample a subset S of k constants from C. (ii) Uniformly sample a local example \u03c9 from the set \u03a5[S]. For a closed formula \u03b1 without constants, we also define, its probability: P\u03a5,k(\u03b1) = \u2211 \u03c9:\u03c9|=\u03b1 P\u03a5,k(\u03c9).\nWe will call a pair (\u03b1, p), where \u03b1 is a constant-free closed formula and p \u2208 [0; 1], a relational marginal constraint. We may also interpret the probability P\u03a5,k(\u03b1) of a closed constant-free formula \u03b1 as the probability that \u03b1 is true in a restriction \u03a5\u3008S\u3009 of \u03a5 to a randomly sampled subset S of k constants from\u03a5. Thus, if we are only interested in the probabilities of closed constant-free formulas, we do not have to refer to local examples. Local examples are important because relational marginal distributions defined using them are themselves probability distributions on possible worlds, which is both nice conceptually and convenient (as it means that we can model relational marginals of Model A using any standard propositional probabilistic model).\nGlobal examplesmay also be assumed to be sampled from some distribution and we define the corresponding marginal distributions induced by such distributions accordingly. When P (\u03a5) is a distribution over finite global examples from a possibly countably infinite set \u2126, then the marginal distribution of width k is a distribution Pk over local examples where Pk(\u03c9) is defined as Pk(\u03c9) = \u2211 \u03a5\u2208\u2126 P (\u03a5) \u00b7 P\u03a5,k(\u03c9). For a closed formula \u03b1 without constants, we also analogically define: Pk(\u03b1) = \u2211 \u03c9:\u03c9|=\u03b1 Pk(\u03c9). In other words, a relational marginal distribution is a mixture of (possibly countably many) relational marginal distributions of global examples.\nProposition 2. Let P (\u03a5) be a distribution on domain size n and k \u2264 n be an integer. Let \u2126\u03b1 = {\u03a5\u3008S\u3009 : S \u2286 C, |S| = k,\u03a5\u3008S\u3009 |= \u03b1} where \u03a5 = (A, C) is sampled according to the distribution P (\u03a5). Then, for a closed formula \u03b1, p\u0302\u03b1 = |\u2126\u03b1| \u00b7 (\nn k\n)\u22121 is an unbiased estimate of Pk(\u03b1)\nfor Model A."}, {"heading": "Model B", "text": "The second approach is to consider random substitutions, which is in the spirit of existing works (Bacchus et al. 1992; Schulte et al. 2014). Here, the statistics that we collect about \u03a5 are defined as follows.\nDefinition 3 (Probability of formulas under Model B). Let \u03a5 = (A, C) be a global example and \u03b1 be a universally quantified formula. Let P\u03d1 be a uniform distribution on injective substitutions from the set \u0398\u03b1 = {\u03d1|\u03d1 : vars(\u03b1) \u2192 C and \u03d1 is injective}. Then the probability Q(\u03b1) of the formula \u03b1 under model B is defined as\nQ\u03a5(\u03b1) = \u2211\n\u03d1\u2208\u0398\u03b1\n1(\u03a5 |= \u03b1\u03d1)P\u03d1(\u03d1) = 1 |\u0398| \u2211\n\u03d1\u2208\u0398\u03b1\n1(\u03a5 |= \u03b1\u03d1).\nJust like for Model A, we extend the definition of the probability of formulas straightforwardly to the case where \u03a5 is not fixed but sampled from some distribution over a countable set \u2126: Q(\u03b1) = \u2211 \u03a5\u2208\u2126Q\u03a5(\u03b1) \u00b7 P (\u03a5). Example 3. Let \u03a5 be as in Example 1. Let\n\u03b1 = \u2200A,B : \u00acfr(A,B) \u2228 sm(B), \u03b2 = \u2200A,B : \u00acfr(A,B) \u2228 sm(A) \u2228 sm(B).\nAssuming that the relation fr(., .), \u201cfriends\u201d, is symmetric, the formula \u03b1 is classically true if all people who are friends with someone who smokes, and the formula \u03b2 is classically true if for every pair of people A and B who are friends, at least one of them smokes. Computing the respective probabilities, we get P\u03a5,2(\u03b1) = 1 3 , P\u03a5,2(\u03b2) = 2 3 , Q\u03a5(\u03b1) = 1 2 , Q\u03a5(\u03b2) = 2 3 , which illustrates that in general the \u201cmarginal\u201d probabilities given by the two models will differ. The first model might be slightly easier to interpret as the marginal probabilities P (\u03b3) correspond to fraction of the width-k fragments of \u03a5 in which \u03b3 is true as a classical logic formula.\nWe note that the straightforward analogue of Proposition 2 also holds for Model B."}, {"heading": "Max-Entropy Models", "text": "In this section we show how to compute models of given relational marginals under Model A and Model B.\nDefinition 4 (Model of relational marginals). Let us have a set of pairs \u03a6 = {(\u03b11, \u03b81), . . . , (\u03b1h, \u03b8h)} of relational marginals, where \u03b1i is a closed formula and \u03b8i \u2208 [0; 1]. Let \u03a60 be a set of hard rules. We say that a probability distribution P (\u03a5) over worlds satisfying the hard rules from \u03a60 is a width-k model of \u03a6 iff Pk(\u03b1i) = \u03b8i (Qk(\u03b1i) = \u03b8i, respectively) for all (\u03b1i, \u03b8i) \u2208 \u03a6. We will use standard duality arguments from convex optimization (Boyd and Vandenberghe 2004), essentially following (Singh and Vishnoi 2014). The results will in both cases be exponential-family models and the one for Model B will turn out to be equivalent to MLNs."}, {"heading": "Model A", "text": "Let C be a set of constants, A be the set of all atoms over C based on some given first-order language and let Ck denote the set of all k-element subsets of C. Next, let \u03a6 be a set of relational marginals and \u03a60 be a set of hard constraints, i.e. formulas \u03b1 such that if \u03a5 6|= \u03b1 then P (\u03a5) = 0. We assume that there exists at least one distribution P \u2032 which is a model of \u03a6 and which satisfies the following positivity condition: P \u2032(\u03a5) > 0 for all \u03a5 satisfying the hard constraints (i.e. those for which \u03a5 |= \u03a60).\nsup {P\u03a5:\u03a5\u2208\u03a0(C,\u03a60)}\n\u2211\n\u03a5\u2208\u03a0n(C,\u03a60)\nP\u03a5 log 1\nP\u03a5 s.t. (1)\n\u2200(\u03b1i, \u03b8i) \u2208 \u03a6 : 1\n|Ck| \u2211\nS\u2208Ck\n\u2211\n\u03a5\u2208\u03a0(C,\u03a60)\n1(\u03a5\u3008S\u3009 |= \u03b1i) \u00b7 P\u03a5 = \u03b8i (2)\n\u2200\u03a5 \u2208 \u03a0n(C,\u03a60) : P\u03a5 \u2265 0, \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\nP\u03a5 = 1 (3)\nNext we define #k(\u03b1,\u03a5) = |{S \u2208 Ck : \u03a5\u3008S\u3009 |= \u03b1}|. That is #k(\u03b1,\u03a5) is the number of sets S \u2208 Ck such that the formula \u03b1 is classically true in the restriction of \u03a5 to constants in S. We show in the appendix that the distribution, if it exists, that is a solution to the optimization problem has the following form:\nP\u03a5 = 1\nZ exp\n( \u2211\n\u03b1i\u2208\u03a6\n\u03bbi #k(\u03b1i,\u03a5)\n|Ck|\n) . (4)\nThe Lagrangian dual problem of the maximum entropy problem is to maximize (where \u03bbi \u2208 R): \u2211\n\u03b1i\u2208\u03a6\n\u03bbi\u03b8i \u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 \u03bbi #k(\u03b1i,\u03a5) |Ck| (5)\nDue to the positivity assumption, Slater\u2019s condition (Boyd and Vandenberghe 2004) is satisfied and strong duality holds. Consequently, instead of solving the original problem, which has an intractable number of constraints and variables (one variable for each world \u03a5 \u2208 \u03a0(C,\u03a60)), we can solve the dual problem, which only has |\u03a6| variables. On the other hand, the optimization criterion of the dual problem may still be computationally hard to solve as it requires weighted counting over worlds in \u03a0(C,\u03a60). However, in many restricted, but non-trivial, cases, we can exploit lifted weighted model counting techniques in the same way as they were used for maximum-likelihood estimation in (Van Haaren et al. 2016). Let us perform a change of variableswi := \u03bbi/|Cki |. This gives us\nP (\u03a5) = 1\nZ exp\n( \u2211\n\u03b1i\u2208\u03a6\nwi \u00b7#k(\u03b1i,\u03a5) )\n(6)\nfor the distribution and\u2211\n\u03b1i\u2208\u03a6\nwi\u03b8i|Cki | \u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 wi\u00b7#k(\u03b1i,\u03a5) (7)\nfor the optimization criterion of the dual problem. Assuming that the marginals were estimated from a global example \u03a5\u0302 \u2208 \u03a0(C,\u03a60) (note that here the domain C is the same as the domain of the global examples over which the distribution is defined) and that they still satisfy the positivity assumption, we can also rewrite (7) as\n\u2211\n\u03b1i\u2208\u03a6\nwi \u00b7#k(\u03b1i, \u03a5\u0302)\u2212log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 wi\u00b7#k(\u03b1i,\u03a5) (8)\nIt is straightforward to check that this is the same\nas directly optimizing the log-likelihood of \u03a5\u0302. Thus, here we have a relational analogue of the well-known duality of maximum likelihood and maximum entropy (Wainwright and Jordan 2008). Note that it is important for the duality of maximum likelihood and maximum entropy\nthat both the \u03a5\u0302, from which we estimated the parameters, and the global examples over which the distribution is computed have the same domain size. The Section Realizability will address cases where the domain sizes of the training and testing data differ."}, {"heading": "Model B", "text": "Like for Model A, we can construct a convex optimization problem to obtain a maximum-entropy distribution with the given relational marginals under Model B. This problem\u2019s optimization criterion is the same as (1). To obtain the constraints enforcing the marginals, we can replace in (2) the summation over subsets of constants in C by a summation over substitutions from\u0398\u03b1i , where\u0398\u03b1i is defined as in Definition 3, which gives us the following set of constraints for all (\u03b1i, \u03b8i) \u2208 \u03a6:\n\u2211\n\u03d1\u2208\u0398\u03b1i\n1 |\u0398\u03b1i | \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\n1(\u03a5 |= \u03b1i\u03d1) \u00b7 P\u03a5 = \u03b8i (9)\nUsing basically the same reasoning as for Model A, we arrive at the following form of the probability distribution\nP (\u03a5) = 1\nZ exp\n( \u2211\n\u03b1i\u2208\u03a6\nwi \u00b7 n(\u03b1i,\u03a5) )\n(10)\nwhere n(\u03b1i,\u03a5) is the number of groundings \u03b1i\u03d1 of the formula \u03b1i, and all \u03d1\u2019s are injective, which are true in \u03a5. This is distribution is identical to the one for MLNs which only contain proper formulas1 (because of the injectivity requirement in the definition of Model B). The only difference to the distribution of Model A is the use of n(\u03b1i,\u03a5) instead of #k(\u03b1i,\u03a5). The dual optimization criterion for Model B then becomes to maximize\n\u2211\n\u03b1i\u2208\u03a6\nwi\u03b8i|\u0398\u03b1i |\u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 wi\u00b7n(\u03b1i,\u03a5) (11)\nwhich can be rewritten, when \u03b8i\u2019s are estimated from some \u03a5\u0302 = (A\u0302, C\u0302) with |C\u0302| = |C\u0302|, as \u2211\u03b1i\u2208\u03a6wi \u00b7 n(\u03b1i, \u03a5\u0302) \u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60) e \u2211 \u03b1i\u2208\u03a6 wi\u00b7n(\u03b1i,\u03a5) which is the same as\nlog-likelihood of \u03a5\u0302 w.r.t. the MLN given by (11), again assuming that all the formulas in the MLN are proper. Thus,\nwhen the size of the domain of the training example \u03a5\u0302 is the same as the cardinality of the domain of the modeled distribution, the max-entropy relational marginal problem in Model B is the same as maximum-likelihood estimation in MLNs."}, {"heading": "Realizability", "text": "Not all relational marginals have a model for a given domain size (or even any model at all). This is a problem if we want to estimate relational marginals on some given global exam-\nple \u03a5\u0302 and then use them to obtain a distribution on global examples that have a different domain size. The duality between maximum-likelihood estimation and max-entropy relational marginal problems discussed in the previous section only holds when the training data and the distribution that we want to model have the same domain size. In this section, we will show how to obtain relational marginals for any domain size. Accomplishing this requires\n1It has been shown (Poole et al. 2012) that the setting with proper formulas is equivalent in expressivity to general MLNs.\nreplacing the consistency of relational marginal estimators with a weaker notion. However, we can still bound the difference between the unbiased and the adjusted estimates. More importantly, the difference will tend to zero as the domain size of the examples from which the respective marginals were estimated increases. First, it is easy to see that if a model exists for a given set of marginals and domain size, then there is also such a model for smaller domain sizes, as the next proposition asserts.2\nProposition 4. For Model A, if there is a width-k model of \u03a6 on domain of size n then there is also a width-k model of \u03a6 on domain of size m for any m satisfying n \u2265 m \u2265 k. For Model B, if there is a model of \u03a6 on domain of size n then there is also a model of \u03a6 on domain of size m for any m satisfying n \u2265 m \u2265 max\u03b1\u2208\u03a6 |vars(\u03b1)|. Next, we give an example of a relational marginal distributions that does not have a model for arbitrary domain cardinalities.\nExample 5. Let L consist of predicate symbols e/2, r/1, g/1, b/1 and \u03a5 = (A, C) be a global example where\nA = {e(v1, v2), e(v2, v1), e(v2, v3), e(v3, v2), e(v1, v3), e(v3, v1), r(v1), g(v2), b(v3)}, C = {v1, v2, v3}.\nLet k = 2 be the width of local examples. And let F (X1, X2) def = X1 6= X2 \u2227 \u00ace(X1, X1) \u2227 e(X1, X2) \u2227 e(X2, X1) \u2227 \u00ace(X2, X2). Then we can estimate, for instance, the following marginals from \u03a5 under Model A:\nP [\u2203X1, X2 : F (X1, X2) \u2227 r(X1) \u2227 \u00acg(X1) \u2227 \u00acb(X1)\u2227\n\u2227\u00acr(X2) \u2227 g(X2) \u2227 \u00acb(X2)] = 1\n3 P [\u2203X1, X2 : F (X1, X2) \u2227 r(X1) \u2227 \u00acg(X1) \u2227 \u00acb(X1)\u2227\n\u2227\u00acr(X2) \u2227 \u00acg(X2) \u2227 b(X2)] = 1\n3 P [\u2203X1, X2 : F (X1, X2) \u2227 \u00acr(X1) \u2227 g(X1) \u2227 \u00acb(X1)\u2227\n\u2227\u00acr(X2) \u2227 \u00acg(X2) \u2227 b(X2)] = 1\n3\n(which can also be rewritten as probabilities of clauses by negating the existentially quantified conjunctions). The global example \u03a5 can be imagined as a complete directed graph (without self-loops) on 3 vertices v1, v2, v3 where each of the vertices is colored by one of the \u201ccolors\u201d r, g, b. We claim that no distribution on global examples satisfies the above marginal probabilities for domain size greater than 3. This can be shown as follows. Using the intuitive view of \u03a5 as a colored directed graph, the distribution on global examples of domain size e.g. 4 would be a distribution on graphs with 4 vertices. Such graphs would have to contain either two vertices not connected by an edge or two vertices connected by an edge but labeled with the same color or a vertex with no color. However, two such vertices\n2Proposition 4 is one of the results which would not hold for Model B if we did not require injectivity of the randomly sampled substitutions in the definition of Model B.\nwould correspond to a local example which would otherwise have zero probability under the marginals estimated from\u03a5, which are shown above. While the above reasoning is for Model A, a similar argument can be used to show the same issues for Model B.\nOne of the consequences of the above example is that the unbiased estimates of relational marginals from Proposition 2 cannot always be used for defining distributions of arbitrary domain sizes (we will show later in the paper conditions under which such unbiased estimates do exist, using the concept of relational marginal polytopes). In order to construct distributions for arbitrary domain sizes, which have relational marginals that are close to the relational marginals given by some global example \u03a5, we will rely on the following construction which we call expansion of global example.\nDefinition 5 (Expansion of a global example). Let \u03a5 = (A, C) be a global example where C = {c1, c2, . . . , cn}, and let l be a positive integer. Then the l-level expansion of \u03a5 is a global example \u03a5\u2032 = (A\u2032, C\u2032) given by: A\u2032 = {a\u03b8 : a \u2208 A, \u03b8 \u2208 \u0398}, C\u2032 = {c1, c2, . . . , cn, cn+1, . . . , cl\u00b7n}. Here, constants ci, cj are said to be congruent if i \u2261 j mod n. Here, cn+1, . . . , cl\u00b7n are some arbitrary new constants and \u0398 is a set of all substitutions \u03b8 which satisfy the requirement that c\u03b8 is congruent with c for each c \u2208 C. Next we illustrate the notion of expansion of examples. Example 6. Let \u03a5 = (A, C) be given by A = {e(c1, c2), e(c2, c3)}, C = {c1, c2, c3}. Then the 2-level expansion of \u03a5 is \u03a5\u2032 = (A\u2032, C\u2032) where A = {e(c1, c2), e(c2, c3), e(c4, c5), e(c5, c6), e(c1, c5), e(c2, c6), e(c4, c2), e(c5, c3)}, C = {c1, c2, c3, c4, c5, c6}. The width-2 marginal probabilities on \u03a5 and \u03a5\u2032 are:\nP\u03a5,2(\u03c91) = 1\n3 P\u03a5\u2032,2(\u03c91) =\n7\n15 \u03c91 = ({}, {1, 2})\nP\u03a5,2(\u03c92) = 1\n3 P\u03a5\u2032,2(\u03c92) =\n4\n15 \u03c92 = ({e(1, 2)}, {1, 2})\nP\u03a5,2(\u03c93) = 1\n3 P\u03a5\u2032,2(\u03c93) =\n4\n15 \u03c93 = ({e(2, 1)}, {1, 2})\nP\u03a5,2(\u03c9) = 0 P\u03a5\u2032,2(\u03c9) = 0 \u03c9 6\u2208 {\u03c91, \u03c92, \u03c93} The differences between the marginal probabilities given by \u03a5 and \u03a5\u2032 are at most 215 in this case, which is quite high. However, it follows from what we show in turn that this is mostly because of the small size of \u03a5. For larger global examples, the difference between the marginals obtained from them and from their expansions will tend to be smaller.\nImportantly, it is possible to bound the difference of the parameters obtained on expansions of global examples and the unbiased estimates obtained on the original examples. Proposition 7. Let \u03a5 = (A, C) be a global example and\u03a5\u2032 its l-level expansion and let n = |A| and k be the width of local examples. Then for any formula \u03b1:\n|P\u03a5,k(\u03b1) \u2212 P\u03a5\u2032,k(\u03b1)| \u2264 1 \u2212 ( n\u2212 k + 1\nn\n)k\u22121\nfor Model A, and\n|Q\u03a5(\u03b1)\u2212Q\u03a5\u2032(\u03b1)| \u2264 1\u2212 ( n\u2212 |vars(\u03b1)| + 1\nn\n)|vars(\u03b1)|\u22121\nfor Model B.\nNote that the difference between the true and modeled probabilities of a fixed formula \u03b1 decays as O ( 1 n ) .\nThe techniques described in this section have the following limitation. If we have a set of hard rules \u03a60 which are satisfied by a given \u03a5, these rules may not be satisfied in an expansion of \u03a5. This is not just a limitation of our method though. There are cases where it is not possible to extend a given \u03a5 while satisfying the constraints (this is because we allow the use of equality in the formulas and because we use the unique names assumption). However, if the example \u03a5 is large enough and it satisfies the hard rules, then the number of violations of these rules will be small, which follows again from Proposition 7.\nIn fact, it seems to be a desirable property that formulas \u03b1 satisfying P\u03a5,k(\u03b1) = 1 do not have to be treated as completely hard rules but as rules that \u201cmostly\u201d hold if they are learned from \u03a5, since it may be that they are not really rules that should always hold. Yet, if we actually took them as hard rules we would be forced to assign probability 0 to any example that violates them. It is possible to use the idea of expansions to obtain a distribution in which any formula \u03b1 has nonzero probability and all properties of expansions are still preserved (such as those from Proposition 7). This can be achieved by randomly sampling additional atoms containing only congruent constants and adding them to the respective expansion. If we use a sufficiently high level of the expansion (at least k for Model A and at least max\u03b1\u2208\u03a6 |vars(\u03b1)| for Model B) then the probability of any formula will be nonzero and not equal to one w.r.t. the distribution induced by the expansions with the sampled atoms."}, {"heading": "Relational Marginal Polytopes", "text": "In this section, we define another important concept called relational marginal polytope, which will be used in the next section where we deal with estimation errors.\nDefinition 6 (Relational marginal polytope for Model A). Let k,m \u2208 N and \u03a6 = {\u03b11, . . . , \u03b1l} be a set of formulas and \u03a60 be a set of hard rules. Let C = {1, . . . ,m} and Ck be the set of size-k subsets of C. Then, for Model A, the relational marginal polytope of\u03a6 of width k and cardinality m w.r.t. the hard rules from \u03a60 is the convex hull of the set {(\n#k(\u03b11,\u03a5)\n|Ck| , . . . ,\n#k(\u03b1l,\u03a5)\n|Ck|\n)\u2223\u2223\u2223\u2223\u03a5 \u2208 \u03a0(C,\u03a60) } .\nLet \u0398\u03b1i be the set of all injective substitutions from variables of \u03b1i to constants from C. Then, for Model B, the relational marginal polytope of \u03a6 of cardinality m w.r.t. the hard rules from \u03a60 is the convex hull of the set\n{( n(\u03b11,\u03a5)\n|\u0398\u03b11 | , . . . ,\nn(\u03b1l,\u03a5)\n|\u0398\u03b1l |\n)\u2223\u2223\u2223\u2223\u03a5 \u2208 \u03a0(C,\u03a60) } .\nAny realizable set of relational marginals for Model A and Model B naturally corresponds to a point in the respective polytope. In the remainder of this paper, we only consider the cases when the relational marginal polytope is full-dimensional, that is, it does not live in a lower dimensional subspace which could happen if some of the relational marginals that define it were linearly dependent. We will also need the concept of \u03b7-interior of a relational marginal. We say that a point y is in the \u03b7-interior of a relational marginal polytope P if P contains a ball of radius \u03b7 with center in y. Using Proposition 4 and Proposition 7, we can show the following both for Model A and Model B.\nProposition 8. Let \u03b8 be a vector representing the values of a set of relational marginals given by formulas from a set \u03a6 = {\u03b11, . . . , \u03b1l}. Let k be the width of the relational marginals of Model A or k = max\u03b1i |vars(\u03b1i)| for Model B. Let the set of hard rules \u03a60 be empty. If \u03b8 is in\nthe ( \u03b7 + \u221a l ( 1\u2212 ( m\u2212k+1\nm\n)k\u22121)) -interior of the relational\nmarginal polytope of \u03a6 of domain-size m then it is also in the \u03b7-interior of the relational marginal polytope of \u03a6 for any domain size m\u2032."}, {"heading": "Estimation", "text": "In this section, we present error bounds for the estimation of relational marginals. We start by defining the learning setting. Clearly, we need some assumptions on the training and test data and their relationship (otherwise one could always come up with a setting in which the error can be arbitrarily large). In order to stay close to realistic settings we assume that there is some large global example \u2135 = (A\u2135, C\u2135) that is not available and that represents the ground truth. That is what we essentially want to estimate for a given formula \u03b1 is P\u2135,k(\u03b1), but we do not have access to whole \u2135. Imagine for instance that \u2135 is the human gene regulatory network or Facebook. We assume that there is a process that samples size-m subsets of C\u2135 uniformly and that we have access to one such sample C\u03a5 and also to the respective induced \u03a5 = \u2135\u3008C\u03a5\u3009. So, for a given formula \u03b1, we need to estimate P\u2135,k(\u03b1) using the available example \u03a5 and the estimate needs to be realizable (otherwise the optimization problem would have no solution and the duality would also not hold). This is a reasonably realistic setting3 as in practice we also do not have a distribution over different Facebooks but there is one Facebook and we want to model it based on a sample that is available to us. We now provide theoretical upper bounds for the expected error of the estimates of P\u2135,k(\u03b1) assuming the just described learning setting. However, we first need to describe the estimators. Based on the results from the previous sections, the estimator works as follows. Given a global example \u03a5 = (A\u03a5, C\u03a5) and an integer n, which is the size of the domain of the modelled distribution (e.g. n can be size of \u2135\u2019s domain if it is known), we construct the l-level expansion \u03a5(l) of \u03a5, where l = \u2308n/|C\u03a5|\u2309, and we use it to estimate the parameters as P\u0302\u03b1 = P\u03a5(l),k(\u03b1) for Model A and\n3What might differ in realistic settings is the sampling process. We briefly discuss this in section Conclusions.\nas Q\u0302\u03b1 = Q\u03a5(l)(\u03b1) for Model B. The following proposition introduces an upper bound for the expected error of the estimated parameters.\nProposition 9. Let m and n be positive integers, \u03b1 a closed formula and let k be the width of local examples. Let \u2135 = (A\u2135, C\u2135) be a global example, C\u03a5 be sampled uniformly among all size-m subsets of C\u2135 and \u03a5 = \u2135\u3008C\u03a5\u3009. Let A\u0302\u2135 = P\u2135,k(\u03b1). Let B\u0302\u03a5 be an estimate computed from the l-level expansion of C\u03a5. Then\nE [\u2223\u2223\u2223A\u0302\u2135 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ] \u2264 1\u2212 ( m\u2212 k + 1\nm\n)k\u22121 + \u221a 1 + 2 log 2\n4\u230am/k\u230b .\nIn the case of model B, the same upper bound holds if we choose k = |vars(\u03b1)| and A\u0302\u2135 = Q\u2135(\u03b1). The proof of this proposition is based on a deviation bound\nfor a randomized estimator of B\u0302\u03a5, which we prove in the appendix located in the supplemantary material. It is possible to improve the estimation in some cases. If the vector corresponding to the marginals \u03a6 estimated from \u03a5 is guaranteed to be in the ( \u03b7 + \u221a l ( 1\u2212 ( m\u2212k+1\nm\n)k\u22121)) -\ninterior of the relational marginal polytope of domain of size m = |C\u03a5| for some \u03b7 > 0, where k is the width of local examples for Model A (or max\u03b1\u2208\u03a6 |vars(\u03b1)| for Model B), then, by Proposition 8, we can estimate the parameters directly from \u03a5 without constructing its expansion. We then have the following improved bound:\nE [\u2223\u2223\u2223A\u0302\u2135 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ] \u2264 \u221a (1 + 2 log 2)/(4\u230am/k\u230b).\nIt is interesting to note that the lower-bound on effective sample size obtained from these bounds is \u230am/k\u230b, which is also the maximum number of non-overlapping size-k subsets of C\u03a5. A consequence for learning the parameters of models such as MLNs (which corresponds to relational marginal problems inModel B) is that this bound is inversely proportional to the number of variables in the used formulas, which also suggests an explanation for why learning with longer formulas is difficult."}, {"heading": "Related Work", "text": "The relational marginals from Model A were recently introduced (Kuz\u030celka, Davis, and Schockaert 2017). However, they were only studied in a possibilistic setting, which differs substantially from the probabilistic maximum-entropy setting that we considered. The idea of using random substitutions (Model B) goes back to (Bacchus et al. 1992) who, however, only considered unary predicates. Schulte et al. (2014) used the random substitutions semantics to define a relational Bayesian network model for population statistics. However, their model is, not based on any underlying ground model, and it is unclear whether the distributions are always realizable by a ground model. In the more restricted setting of exponential random graph models (ERGMs, Chatterjee and Diakonis 2013), a formally similar duality, based on densities of graph homomorphisms, has previously been established. To the best of our knowledge, however, such a duality has never been established in\nan SRL setting. In fact, even for ERGMs this duality has not yet been exploited for estimating parameters for models of different domain sizes, which is one of the key contributions of our work. Certain statistical properties of learning have been already studied for SRL models. Xiang and Neville (2011) studied consistency but postulated rather strong assumptions4, as a result of which their results are not comparable with ours. Their approach also differs in that it only considers distributions of labels conditioned on the underlying graph structure. It is interesting that a statistical estimation problem equivalent to the estimation of parameters in Model A has also been studied in the literature. In (Nandi and Sen 1963), the variance of an estimator, equivalent to the unbiased estimator for Model A, was given. However, we are not aware of any work showing a deviation bound in the same setting, which was needed to establish the bound on expected error in Proposition 9. Interestingly, the effective sample size m/k stemming from the work (Nandi and Sen 1963) for variance of the estimator is almost the same as the effective sample size \u230am/k\u230b stemming from our deviation and expected-error bounds. This actually suggests that our bounds are rather tight. There were many works on Ustatistics (Hoeffding 1948) which are related as well but they rely on assumptions that are generally not realistic for SRL and, in particular, are not applicable to our setting; the work (Nandi and Sen 1963) is an exception."}, {"heading": "Conclusions", "text": "In this paper, we have introduced and studied relational marginal problems. Interestingly, this perspective enables learning a model that is applicable to data sets whose domain sizes differ from that of the training data. We established a relational counterpart of the classical duality between maximum-likelihood and max-entropy marginal problems. Then, we showed how to estimate and adjust parameters of the marginals in order to guarantee their realizability. We also complemented these results by providing bounds on the expected errors of the estimates in a reasonable learning setting. We believe that due to the simplicity and transparency of the learning setting that we introduced, this setting could play a similar role for SRL as the standard i.i.d. statistical learning setting plays for learning in propositional domains (Vapnik 1995). That is, as an idealized setting that is suitable for theoretical study, but that is not too far from settings that one encounters in reality. Still, it would be possible to extend the learning setting to make it more realistic. In particular, the sampling process that creates the training examples could be replaced by another sampling process that would take into account the structure of the relational data. That would probably make estimation of parameters and derivation of error bounds significantly more complex, and hence arguably less illuminating, which is why we leave it for future work.\n4The assumptions used in their work were weak dependency and bounded degree of graph nodes."}, {"heading": "Appendix: Duality", "text": "In this section we prove the duality results from the main text. First we recall the setting and notations. Let C be a set of constants, A the set of all atoms over C based on some given first-order language and let Ck denote the set of all k-element subsets of C. Next, let \u03a6 be a set of relational marginals and \u03a60 be a set of hard constraints, i.e. formulas \u03b1 such that if \u03a5 6|= \u03b1 then P (\u03a5) = 0. Let us also assume that there exists at least one distribution P \u2032 which is a model of \u03a6 and which satisfies the positivity condition: P \u2032(\u03a5) > 0 for all \u03a5 satisfying the hard constraints. The optimization problem is then given by:\nsup {P\u03a5:\u03a5\u2208\u03a0(C,\u03a60)}\n\u2211\n\u03a5\u2208\u03a0n(C,\u03a60)\nP\u03a5 log 1\nP\u03a5 s.t. (12)\n\u2200(\u03b1i, \u03b8i) \u2208 \u03a6 : 1\n|Ck| \u2211\nS\u2208Ck\n\u2211\n\u03a5\u2208\u03a0(C,\u03a60)\n1(\u03a5\u3008S\u3009 |= \u03b1i) \u00b7 P\u03a5 = \u03b8i (13)\n\u2200\u03a5 \u2208 \u03a0n(C,\u03a60) : P\u03a5 \u2265 0, \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\nP\u03a5 = 1 (14)\nChanging maximization of entropy to minimization of negative entropy, we obtain a convex optimization problem. Next, we construct the Lagrangian L(P, \u03bb, z) (here, P denotes the vector of all P\u03a5 \u2208 \u03a0(C,\u03a60), \u03bb the vector of all \u03bbi\u2019s, and we will ignore the non-negativity constraints for the moment as it will turn out that they are enforced implicitly):\nL(P, \u03bb, z) = \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\nP\u03a5 \u00b7 logP\u03a5\u2212\n\u2212 1|Ck| \u2211\n\u03b1i\u2208\u03a6\n\u2211\nS\u2208Ck\n\u2211\n\u03a5\u2208\u03a0(C,\u03a60)\n\u03bbi \u00b7 1(\u03a5\u3008S\u3009 |= \u03b1i) \u00b7 P\u03a5+\n+ \u2211\n\u03b1i\u2208\u03a6\n\u03bbi \u00b7 \u03b8i + z \u2212 z \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\nP\u03a5 (15)\nTo find the stationary points of L(P, \u03bb, z) w.r.t. P , we take the partial derivatives of the Lagrangian w.r.t. P\u03a5j and set them equal to zero:\nlogP\u03a5j \u2212 1\n|Ck|\n\u2211\n\u03b1i\u2208\u03a6\n\u2211\nS\u2208Ck\n\u03bbi \u00b71(\u03a5j\u3008S\u3009 |= \u03b1i)\u2212 z\u2212 1 = 0\n(16)\nNext we define #k(\u03b1,\u03a5) = |{S \u2208 Ck : \u03a5\u3008S\u3009 |= \u03b1}|. That is #k(\u03b1,\u03a5) is a function counting the number of sets S \u2208 Ck such that the formula \u03b1 is true in the restriction of \u03a5 to constants in S (i.e. those for which \u03a5\u3008S\u3009 |= \u03b1). This allows us to rewrite (16) as follows:\nlogP\u03a5j \u2212 \u2211\n\u03b1i\u2208\u03a6\n\u03bbi #k(\u03b1i,\u03a5j)\n|Ck| \u2212 z \u2212 1 = 0 (17)\nFrom this, we get:\nP\u03a5j = exp\n( z + 1 + \u2211\n\u03b1i\u2208\u03a6\n\u03bbi #k(\u03b1i,\u03a5j)\n|Ck|\n) (18)\nSince we have \u2211\n\u03a5\u2208\u03a0(C,\u03a60) P\u03a5 = 1, exp (z + 1) is a nor-\nmalization constant. By further algebraic manipulations, combining (15) and (18) we obtain the Lagrangian dual problem (where \u03bbi \u2208 R) is to maximize:\n\u2211\n\u03b1i\u2208\u03a6\n\u03bbi\u03b8i \u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 \u03bbi #k(\u03b1i,\u03a5j) |Ck| (19)\nDue to the positivity assumption, Slater\u2019s condition (Boyd and Vandenberghe 2004) is satisfied and strong duality holds.\nRemark 10. The same reasoning can be also applied when we have multiple sets of relational marginals \u03a61, \u03a62, . . . , \u03a6n (\u03a6 = \u03a61 \u222a \u00b7 \u00b7 \u00b7 \u222a\u03a6n) containing relational marginals of widths 1, 2, . . . , n, respectively. The only difference is that the denominators |Ck| will vary according to widths (ki) of the respective marginals (\u03b1i) and we will denote the respective sets of subsets of C by Cki . Let us perform a change of variables wi := \u03bbi/|Cki | and denote the normalization constant of the distribution as Z . This gives us\nP (\u03a5) = 1\nZ exp\n( \u2211\n\u03b1i\u2208\u03a6\nwi \u00b7#k(\u03b1i,\u03a5) )\n(20)\nfor the distribution and \u2211\n\u03b1i\u2208\u03a6\nwi\u03b8i|Cki |\u2212log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 wi\u00b7#k(\u03b1i,\u03a5) (21)\nfor the optimization criterion of the dual problem. Assuming that the marginals were estimated from a global example \u03a5\u0302 \u2208 \u03a0(C,\u03a60) (note that here the domain C is the same as the domain of the global examples over which the distribution is defined) and that they still satisfy the positivity assumption, we can also rewrite (21) as\n\u2211\n\u03b1i\u2208\u03a6\nwi \u00b7 #k(\u03b1i, \u03a5\u0302) \u2212 log \u2211\n\u03a5\u2208\u03a0(C,\u03a60)\ne \u2211 \u03b1i\u2208\u03a6 wi\u00b7#k(\u03b1i,\u03a5)\n(22)\nThe reasoning is completely analogical for Model B. The only difference is that #k(\u03b1,\u03a5) is replaced by n(\u03b1,\u03a5) which counts the number of true groundings of \u03b1 in \u03a5."}, {"heading": "Appendix: Proofs", "text": "In this section we give proofs of the propositions from the main text. Proposition 2. Let P (\u03a5) be a distribution on domain size n and k \u2264 n be an integer. Let \u2126\u03b1 = {\u03a5\u3008S\u3009 : S \u2286 C, |S| = k,\u03a5\u3008S\u3009 |= \u03b1} where \u03a5 = (A, C) is sampled according to the distribution P (\u03a5). Then, for a closed formula \u03b1,\np\u0302\u03b1 = |\u2126\u03b1| \u00b7 (\nn k\n)\u22121\nis an unbiased estimate of Pk(\u03b1).\nProof. We have:\nE[p\u0302\u03b1] = \u2211\n\u03a5=(A,C)\u2208\u2126\nP (\u03a5) \u00b7 (\nn k\n)\u22121 \u00b7\n\u00b7 |{S : S \u2286 C, |S| = k,\u03a5\u3008S\u3009}| = \u2211\n\u03a5\u2208\u2126\nP (\u03a5) \u00b7 P\u03a5,k(\u03b1) =\n= \u2211\n\u03c9:\u03c9|=\u03b1\n\u2211\n\u03a5\u2208\u2126\nP (\u03a5) \u00b7 P\u03a5,k(\u03c9) = P (\u03b1).\nProposition 4. For Model A, if there is a width-k model of\u03a6 on domain of size n then there is also a width-k model of \u03a6 on domain of size m for any m satisfying n \u2265 m \u2265 k. For Model B, if there is a model of \u03a6 on domain of size n then there is also a model of \u03a6 on domain of size m for any m satisfying n \u2265 m \u2265 max\u03b1\u2208\u03a6 |vars(\u03b1)|.\nProof. Let P (.) be a model of \u03a6 on domain of size n. Let P \u2032(.) be a distribution given by the following process: (i) sample \u03a5 = (A, C) according to P (.), (ii) sample a subset C\u2032 from C uniformly among all subsets of C of cardinality m, and (iii) set \u0393 := \u03a5\u3008C\u2032\u3009. We claim that P \u2032(.) is also a width-k model of M. To show that this is true it is enough to demonstrate that Pk(\u03c9) = P \u2032 k(\u03c9) for all width-k local examples \u03c9, where Pk(.) and P \u2032 k(.) are relational marginal distributions induced by P (.) and P \u2032(.), respectively. This is straightforward to show. It suffices to notice that Pk(\u03c9) does not change when we modify the relational marginal sampling process so that it first samples \u03a5 = (A, C), then it samples uniformly a subset C\u2032 of C of cardinalitym, then it samples uniformly a subset S of C\u2032 and then a local example from\u03a5[S]. But this process is equivalent to sampling local examples from the global examples sampled according to the distribution P \u2032(.). Hence, Pk(\u03c9) = P \u2032 k(\u03c9).\nThe argument is completely analogical for Model B5.\nIt is worth noting here why the Proposition 4 would not hold for Model B if we did not require the sampled substitutions of Model B to be injective. In such a case, the distribution of local examples \u03c9 would not be the same if we first sampled a subset C\u2032 of size m from C. For instance, for a formula \u03b1 with two variables A and B, the proportion of samples mapping A and B to the same constant would be 1 n for sampling directly from C but 1m when first sampling C\u2032 from C and then sampling the substitutions for C\u2032. So the two distributions would be no longer equivalent.\nLemma 1. Let \u03a5 = (A, C) be a global example and \u03a5\u2032 = (A\u2032, C\u2032) its l-level expansion. Let n = |C|. Then there exists a distribution \u03bb(.) such that P\u03a5\u2032,k(\u03c9) = (1\u2212 \u03b3)P\u03a5,k(\u03c9) + \u03b3\u03bb(\u03c9). Here \u03b3 is the probability that a randomly sampled subset of k constants contains at least two congruent con-\n5Here, the condition n \u2265 m \u2265 max\u03b1\u2208\u03a6 |vars(\u03b1)| is here so that we could actually sample injective substitutions.\nstants, i.e.\n\u03b3 = 1\u2212\n( n k ) \u00b7 lk\n( n \u00b7 l k )\nProof. We prove this proposition for Model A; the arguments for Model B are essentially the same and therefore we omit them. For notational convenience, let us define an indicator function (S) which equals 1 when S contains two congruent constants and is 0 otherwise. We decompose the probability P\u03a5\u2032,k(\u03c9) as:\nP\u03a5\u2032,k(\u03c9) = PS [S = \u03c9| (S) = 1] \u00b7 PS [ (S) = 1]+ + PS [S = \u03c9| (S) = 0] \u00b7 PS [ (S) = 0]\nwherePS [.] denotes probability under sampling of S according to Model A. Using elementary combinatorial reasoning, we have PS [ (S) = 1] = \u03b3, PS [ (S) = 0] = 1\u2212 \u03b3 and we can set \u03bb(\u03c9) = PS [S = \u03c9| (S) = 1]. It remains to analyze PS [S = \u03c9| (S) = 0]. First, we observe that this is a distribution on local examples \u201cinduced\u201d by a uniform distribution over size-k subsets of C\u2032 which do not contain congruent constants. The same distribution can also be obtained as follows. First, pick uniformly at random a size-k subset S \u2032 of C and then assign, again uniformly and independently at random, to each of the constants in S \u2032 a constant sampled from the constants congruent to it in C\u2032. Thenwemay notice that it follows from the construction of expansions that the isomorphism class of the induced local example \u03c9 to be sampled is already determined when we sample the set S \u2032 and it is equal to \u03a5[S \u2032]. It follows that PS [S = \u03c9| (S) = 0] = P\u03a5,k(\u03c9) which finishes the proof.\nLemma 2. Let P and \u03bb be distributions over \u2126 and \u03b1 be a closed and constant-free formula. Then\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\n\u03c9\u2208\u2126:\u03c9|=\u03b1\nP (\u03c9)\u2212 \u03bb(\u03c9) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 1\nProof. If \u2211\n\u03c9\u2208\u2126:\u03c9|=\u03b1 P (\u03c9)\u2212 \u03bb(\u03c9) \u2265 0 then we have \u2211\n\u03c9\u2208\u2126:\u03c9|=\u03b1\nP (\u03c9)\u2212 \u03bb(\u03c9) \u2264 1\u2212 \u2211\n\u03c9\u2208\u2126:\u03c9|=\u03b1\n\u03bb(\u03c9) \u2264 1\nwhere the first and second inequality follow from the fact that P (\u03c9) and \u03bb(\u03c9) most sum up to 1 over \u2126 and that they are positive. We can reason completely analogically for\u2211\n\u03c9\u2208\u2126:\u03c9|=\u03b1 P (\u03c9)\u2212\u03bb(\u03c9) \u2264 0. The correctness of the lemma follows easily.\nProposition 7. Let \u03a5 = (A, C) be a global example and \u03a5\u2032 its l-level expansion and let n = |A| and k be the width of local examples. Then for any formula \u03b1:\n|P\u03a5,k(\u03b1) \u2212 P\u03a5\u2032,k(\u03b1)| \u2264 1 \u2212 ( n\u2212 k + 1\nn\n)k\u22121\nfor Model A, and\n|Q\u03a5(\u03b1)\u2212Q\u03a5\u2032(\u03b1)| \u2264 1\u2212 ( n\u2212 |vars(\u03b1)| + 1\nn\n)|vars(\u03b1)|\u22121\nfor Model B.\nProof. We first prove the proposition for Model A. Let \u03b3 be as in Lemma 1. We have |P\u03a5,k(\u03b1)\u2212 P\u03a5\u2032,k(\u03b1)| =\n\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\n\u03c9:\u03c9|=\u03b1\n(P\u03a5,k(\u03c9)\u2212 (1\u2212 \u03b3) \u00b7 P\u03a5,k(\u03c9)\u2212 \u03b3 \u00b7 \u03bb(\u03c9)) \u2223\u2223\u2223\u2223\u2223\u2223\nwhere \u03bb(.) is some suitable probability distribution. This can be rewritten as\n|P\u03a5,k(\u03b1)\u2212P\u03a5\u2032,k(\u03b1)| = \u03b3 \u00b7 \u2223\u2223\u2223\u2223\u2223\u2223 \u2211\n\u03c9:\u03c9|=\u03b1\n(P\u03a5,k(\u03c9)\u2212 \u03bb(\u03c9)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2264 \u03b3 = 1\u2212 l k \u00b7 n \u00b7 (n\u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n\u2212 k + 1)\nn \u00b7 l \u00b7 (n \u00b7 l \u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n \u00b7 l\u2212 k + 1)\nwhere the first inequality follows from Lemma 2. Next, we can verify that\n1\u2212 l k \u00b7 n \u00b7 (n\u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n\u2212 k + 1)\nn \u00b7 l \u00b7 (n \u00b7 l \u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n \u00b7 l \u2212 k + 1) =\n= 1\u2212 n \u00b7 (n\u2212 1) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n\u2212 k + 1) n \u00b7 (n\u2212 1l ) \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (n\u2212 k+1l )\nis non-decreasing with l. Taking the limit l \u2192 \u221e therefore preserves the inequality and we then get\n|P\u03a5,k(\u03b1)\u2212P\u03a5\u2032,k(\u03b1)| \u2264 1\u2212 n\u2212 1 n \u00b7 n\u2212 2 n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 n\u2212 k + 1 n .\nNow, to prove the proposition also for Model B, we proceed as follows. It is easy to verify that Q\u03a5(\u03b1) is equivalently given by the following process. First, we uniformly sample a local example \u03c9 of width k = |vars(\u03b1)| and then we uniformly sample a substitution \u03d1 from the set \u0398k of injective substitutions from vars(\u03b1) to the set {1, 2, . . . , k}. Q\u03a5(\u03b1) is then also equal to the probability that \u03c9 |= \u03b1\u03d1. We can then exploit the arguments used to prove the result for Model A. We have |Q\u03a5(\u03b1) \u2212Q\u03a5\u2032(\u03b1)| =\n= \u2223\u2223\u2223\u2223\u2223\u2223 1 |\u0398k| \u2211\n\u03d1\u2208\u0398k\n\u2211\n\u03c9:\u03c9|=\u03b1\u03d1\nP\u03a5,k(\u03c9)\u2212 P\u03a5\u2032,k(\u03c9) \u2223\u2223\u2223\u2223\u2223\u2223\n\u2264 1\u2212 ( n\u2212 k + 1\nn\n)k\u22121\n= 1\u2212 ( n\u2212 |vars(\u03b1)|+ 1\nn\n)|vars(\u03b1)|\u22121\nwhere the inequality follows from the same algebraic manipulations that we performed above for Model A.\nProposition 8. Let \u03b8 be a vector representing the values of a set of relational marginals given by formulas from a set \u03a6 = {\u03b11, . . . , \u03b1l}. Let k be the width of the relational marginals of Model A or k = max\u03b1i |vars(\u03b1i)| for Model B. Let the set of hard rules \u03a60 be empty. If \u03b8 is in\nthe ( \u03b7 + \u221a l ( 1\u2212 ( m\u2212k+1\nm\n)k\u22121)) -interior of the relational\nmarginal polytope of \u03a6 of domain-size m then it is also in the \u03b7-interior of the relational marginal polytope of \u03a6 for any domain size m\u2032.\nProof. Let us denote by S(\u03a5i) the vector of values of the relational marginals corresponding to the formulas \u03b11, . . . , \u03b1l computed from \u03a5i. Let B be a point on the boundary of the relational marginal polytope. It follows from the definitions that we can write B as a linear combination6 B = a1 \u00b7 S(\u03a51) + \u00b7 \u00b7 \u00b7 + al \u00b7 S(\u03a5l) where all ai\u2019s are positive and a1 + \u00b7 \u00b7 \u00b7 + al = 1 for some suitable \u03a51, . . . ,\u03a5l on domain of size m. Let \u03a5\u20321, . . . ,\u03a5 \u2032 l be the expansions of the respective \u03a51, . . . ,\u03a5l (we pick the level of these expansions sufficient to make the cardinality of their domains greater than the desired cardinality m\u2032) and let B\u2032 = a1 \u00b7 S(\u03a5\u20321) + \u00b7 \u00b7 \u00b7 + al \u00b7 S(\u03a5\u2032l). For notational convenience, let us denote\n\u03be = 1\u2212 ( n\u2212 k + 1\nn\n)k\u22121 .\nWith this notation we have that \u03b8 is in the (\u03b7 + \u03be)-interior of the marginal polytope of domain of size cardinality n. Then, using Proposition 7, we have (for both Model A and Model B):\n\u2016S(\u03a5i)\u2212 S(\u03a5\u2032i)\u20162 \u2264 \u03be \u221a l\nwhere we note that l is the number of formulas in \u03a6. Using triangle inequality, it follows that\n\u2016B \u2212B\u2032\u20162 = = \u2016a1(S(\u03a51)\u2212 S(\u03a5\u20321)) + \u00b7 \u00b7 \u00b7+ al(S(\u03a5l)\u2212 S(\u03a5\u2032l))\u20162 \u2264 a1\u2016S(\u03a51)\u2212 S(\u03a5\u20321)\u20162 + \u00b7 \u00b7 \u00b7+ al\u2016S(\u03a5l)\u2212 S(\u03a5\u2032l)\u20162\n\u2264 (a1 + \u00b7 \u00b7 \u00b7+ al) \u00b7 \u03be \u00b7 \u221a l = \u03be \u221a l\nIntuitively this means that any point on the boundary of the polytope for domain size m will have a point on the boundary or inside the polytope for domain sizem\u2032 in distance not greater than \u03be. Since the point \u03b8 is in the (\u03b7 + \u03be)-interior of the polytope for domain size m, it follows that it must also be in the \u03b7-interior of the polytope for domain size m\u2032 (for anym\u2032), which is what we needed to show.\nNext we prove Proposition 9 using a series of lemmas.We first state and prove the lemmas for Model A and after that, separately, also their analogoues for Model B.\nLemma 3 (Model A Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants and A\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135|\n6In particular, this follows from the fact that relational marginal polytope is a convex hull of a finite set of points in l-dimensional space where the points are given by S(\u03a5) for \u03a5 \u2208 \u03a0(C, \u2205).\nand 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let X = (S1,S2, . . . ,S\u230an\nk \u230b) be a vector of subsets of C\u2135, each\nsampled uniformly and independently of the others from all size-k subsets of C\u2135. Next let Y = (S \u20321,S \u20322, . . . ,S \u2032\u230an\nk \u230b) be a\nvector sampled by the following process:\n1. Let I \u2032 = {1, 2, . . . , |C\u2135|}. 2. Sample subsets I \u20321, . . . , I \u2032\u230an k \u230b of size k uniformly from I \u2032.\n3. Sample an injective function g : \u22c3\nI\u2032 i \u2286I\u2032 I \u2032i \u2192 C\u03a5 uni-\nformly from all such functions.\n4. Let S \u2032i = g(I \u2032i) for all 0 \u2264 i \u2264 \u230ank \u230b. ThenX andY have the same distribution.\nProof. (Sketch) Let h be a bijection from I to C\u2135 sampled uniformly from the set of all such bijections. If we sample I1, . . . , I\u230an\nk \u230b of size k uniformly from I then the dis-\ntribution of (h(I1), . . . , h(I\u230an k \u230b)) will be the same as that of X. In fact, most of h is actually irrelevant and we can replace sampling h by uniformly sampling an injective function g : \u22c3 Ii \u2192 C\u2135 after we sample the sets Ii. Finally, it is easy to realize that the function g can also be sampled as follows. First, sample a subset C of C\u2135 of size n uniformly and then sample uniformly an injective function g such that Range(g) \u2286 C. The reason why we can do this is because |\u22c3 Ii| \u2264 n. This finishes the proof of the lemma as this is actually the same as the process of samplingY.\nLemma 4 (Model A Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants andA\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let Y be sampled as in Lemma 3 (i.e. Y is sampled only using \u03a5 and not directly \u2135). Let \u03b1 be a closed and constant-free formula. Let\nA\u0302\u03a5 = 1 \u230ank \u230b \u2211\nS\u2032 i \u2208Y\n1(\u03a5\u3008S \u2032i\u3009 |= \u03b1)\nand let A\u2135 = P\u2135,k(\u03b1). Then we have\nP [\u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 \u2265 \u01eb ] \u2264 2 exp ( \u22122 \u230an k \u230b \u01eb2 ) .\nProof. We define an estimator on the set X which is sampled as in Lemma 3\nA\u0302 = 1\u230a n k\n\u230b \u2211\nSi\u2208X\n1(\u2135\u3008Si\u3009 |= \u03b1).\nBy Lemma 3, we know that\nP [\u2223\u2223\u2223A\u0302\u2212A\u2135 \u2223\u2223\u2223 \u2265 \u01eb ] = P [\u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 \u2265 \u01eb ] ,\nso we only need to show\nP [\u2223\u2223\u2223A\u0302\u2212A\u2135 \u2223\u2223\u2223 \u2265 \u01eb ] \u2264 2 exp ( \u22122 \u230an k \u230b \u01eb2 ) .\nSince Si inX are independent, the inequality above follows immediately from the fact that E [ A\u0302 ] = A\u2135 (which follows E [1(\u2135\u3008Si\u3009 |= \u03b1)] = A\u2135 and the linearity property of expectation) and the Chernoff-Hoeffding theorem.\nLemma 5 (Model A Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants andA\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let Y be sampled as in Lemma 3 (i.e. Y is sampled only using \u03a5 and not directly \u2135). Let \u03b1 be a closed and constant-free formula. Let\nA\u0302\u03a5 = 1 \u230ank \u230b \u2211\nS\u2032 i \u2208Y\n1(\u03a5\u3008S \u2032i\u3009 |= \u03b1)\nand let A\u2135 = P\u2135,k(\u03b1). Then we have\nE [\u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/k\u230b .\nProof. Let Z := \u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 . First, notice that\nE [Z] \u2264 \u221a E [Z2],\nso we now try to bound E [ Z2 ] . To bound E [ Z2 ] , note that for any u > 0,\nE [ Z2 ] =\n\u222b \u221e\n0\nP [ Z2 \u2265 t ] dt\n=\n\u222b \u221e\n0\nP [ Z \u2265 \u221a t ] dt\n\u2264 u+ \u222b \u221e\nu\nP [ Z \u2265 \u221a t ] dt\n\u2264 u+ 2 \u222b \u221e\nu\nexp ( \u22122 \u230an k \u230b t ) dt\n= u+ exp ( \u22122 \u230an k \u230b u ) / \u230an k \u230b .\nSince u was arbitrary, we may choose it to minimize the obtained bound. The optimal choice is u = log 22\u230an/k\u230b , which yields E [ Z2 ] \u2264 1+2 log 24\u230an/k\u230b .\nLemma 6 (Model A Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants andA\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let \u03b1 be a closed and constant-free formula and let Ck denote all size-k subsets of C\u03a5. Let\nA\u0303\u03a5 = ( n k )\u22121 \u2211\nS\u2208Ck\n1(\u03a5\u3008S\u3009 |= \u03b1)\nand let A\u2135 = P\u2135,k(\u03b1). Then we have\nE [\u2223\u2223\u2223A\u0303\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/k\u230b .\nProof. First we define an auxiliary estimator A\u0303 (q) \u03a5 . Let Yq be a vector of n \u00b7 q size-k subsets of C\u03a5 where the subsets\nof C\u03a5 are sampled in the same way as the elements of the vectorY in Lemma 3. Let us define\nA\u0303 (q) \u03a5 =\n1 q \u00b7 n \u2211\nS\u2208Yq\n1(\u03a5\u3008S\u3009 |= \u03b1).\nUsing the triangle inequality and Lemma 5, we can show that\nE [\u2223\u2223\u2223A\u0303(q)\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/k\u230b .\nWe have (again using triangle inequality)\nE [\u2223\u2223\u2223A\u0303\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] = E [\u2223\u2223\u2223A\u0303\u03a5 \u2212 A\u0303(q)\u03a5 + A\u0303 (q) \u03a5 \u2212A\u2135 \u2223\u2223\u2223 ]\n\u2264 E [\u2223\u2223\u2223A\u0303\u03a5 \u2212 A\u0303(q)\u03a5 \u2223\u2223\u2223 ] + E [\u2223\u2223\u2223A\u0303(q)\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ]\n\u2264 E [\u2223\u2223\u2223A\u0303\u03a5 \u2212 A\u0303(q)\u03a5 \u2223\u2223\u2223 ] +\n\u221a 1 + 2 log 2\n4\u230an/k\u230b .\nIt follows from the strong law of large numbers (which holds for any\u03a5) and Proposition 2 that P [limq\u2192\u221e A\u0303 (q) \u03a5 = A\u0303\u03a5] = 1. Since q was arbitrary, we can use this to conclude\nE [\u2223\u2223\u2223A\u0303\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/k\u230b .\nNext we state the analogues of the above lemmas also for Model B (for the first reading, we recommend skipping these lemmas and proceeding directly to the proof of Proposition 9). We omit the proofs as they are completely analogical to their respective counterparts for Model A (we just replace sampling of subsets by sampling of injective substitutions).\nLemma 7 (Model B Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants and A\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let X = (\u03d11, \u03d12, . . . , \u03d1\u230an\nk \u230b) be a vector of injective substi-\ntutions from a given size-k set of variables V to C\u2135, each sampled uniformly and independently of the others. Next let Y = (\u03d1\u20321, \u03d1 \u2032 2, . . . , \u03d1 \u2032 \u230an\nk \u230b) be a vector sampled by the follow-\ning process:\n1. Let I \u2032 = {1, 2, . . . , |C\u2135|}. 2. Sample ordered subsets I \u20321, . . . , I \u2032\u230an\nk \u230b of size k uniformly\nfrom I \u2032. 3. Sample an injective function g :\n\u22c3 I\u2032 i \u2286I\u2032 I \u2032i \u2192 C\u03a5 uni-\nformly from all such functions.\n4. Let \u03d1\u2032i = {Vj 7\u2192 cj|Vj \u2208 V and (c1, . . . , c|V|) = g(I \u2032i)} for all 0 \u2264 i \u2264 \u230ank \u230b, i.e. \u03d1\u2032i is an injective substitution from V to C\u03a5.\nThenX andY have the same distribution.\nLemma 8 (Model B Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants andA\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all sizen subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let \u03b1 be a closed and constant-free formula. LetY be sampled as in Lemma 7 (i.e. Y is sampled only using\u03a5 and not directly \u2135), where we set V = vars(\u03b1). Let\nA\u0302\u03a5 = 1 \u230ank \u230b \u2211\n\u03d1\u2032 i \u2208Y\n1(\u03a5 |= \u03b1\u03d1\u2032i)\nand let A\u2135 = Q\u2135(\u03b1). Then we have\nP [\u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 \u2265 \u01eb ] \u2264 2 exp ( \u22122 \u230an k \u230b \u01eb2 ) .\nLemma 9 (Model B Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants andA\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let \u03a5 = \u2135\u3008C\u03a5\u3009. Let Y be sampled as in Lemma 7 (i.e.Y is sampled only using\u03a5 and not directly \u2135), where we set V = vars(\u03b1). Let \u03b1 be a closed and constantfree formula. Let\nA\u0302\u03a5 = 1 \u230ank \u230b \u2211\n\u03d1\u2032 i \u2208Y\n1(\u03a5 |= \u03b1\u03d1\u2032i)\nand let A\u2135 = Q\u2135(\u03b1). Then we have\nE [\u2223\u2223\u2223A\u0302\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/k\u230b .\nLemma 10 (Model B Case). Let \u2135 = (A\u2135, C\u2135), where C\u2135 is a set of constants and A\u2135 is a set of ground atoms involving only constants from C\u2135 and let 0 \u2264 n \u2264 |C\u2135| and 0 \u2264 k \u2264 n be integers. Let C\u03a5 be sampled uniformly from all size-n subsets of C\u2135 and let\u03a5 = \u2135\u3008C\u03a5\u3009. Let \u03b1 be a closed and constant-free formula and \u0398\u03b1 denote all injective substitutions from vars(\u03b1) to C\u03a5. Let\nA\u0303\u03a5 = 1 |\u0398\u03b1| \u2211\n\u03d1\u2208\u0398\u03b1\n1(\u03a5 |= \u03b1\u03d1)\nand let A\u2135 = Q\u2135(\u03b1). Then we have\nE [\u2223\u2223\u2223A\u0303\u03a5 \u2212A\u2135 \u2223\u2223\u2223 ] \u2264 \u221a 1 + 2 log 2\n4\u230an/|vars(\u03b1)|\u230b .\nNow we have all the ingredients needed to prove Proposition 9. Proposition 9 Let m and n be positive integers, \u03b1 a closed formula and let k be the width of local examples. Let \u2135 = (A\u2135, C\u2135) be a global example, C\u03a5 be sampled uniformly among all size-m subsets of C\u2135 and \u03a5 = \u2135\u3008C\u03a5\u3009. Let A\u0302\u2135 = P\u2135,k(\u03b1). Let B\u0302\u03a5 be an estimate computed from the l-level expansion of C\u03a5. Then\nE [\u2223\u2223\u2223A\u0302\u2135 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ] \u2264 1\u2212 ( m\u2212 k + 1\nm\n)k\u22121 + \u221a 1 + 2 log 2\n4\u230am/k\u230b .\nIn the case of model B, the same upper bound holds if we choose k = |vars(\u03b1)| and A\u0302\u2135 = Q\u2135(\u03b1).\nProof. Let A\u0302\u03a5 = P\u03a5,k(\u03b1) (A\u0302\u03a5 = Q\u03a5(\u03b1), respectively). Then we have\nE [\u2223\u2223\u2223A\u0302\u2135 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ] = E [\u2223\u2223\u2223A\u0302\u2135 \u2212 A\u0302\u03a5 + A\u0302\u03a5 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ]\n\u2264 E [\u2223\u2223\u2223A\u0302\u03a5 \u2212 B\u0302\u03a5 \u2223\u2223\u2223 ] + E [\u2223\u2223\u2223A\u0302\u2135 \u2212 A\u0302\u03a5 \u2223\u2223\u2223 ]\n\u2264 1\u2212 ( m\u2212 k + 1\nm\n)k\u22121 + \u221a 1 + 2 log 2\n4\u230am/k\u230b\nwhere the last inequality follows from Proposition 7 and Lemma 6 for Model A, and Lemma 10 for Model B, respectively."}], "references": [{"title": "1992", "author": ["F. Bacchus", "A.J. Grove", "D. Koller", "J. Y Halpern"], "venue": "From statistics to beliefs. In Proceedings of the 10th National Conference on Artificial Intelligence. San Jose, CA, July 12-16,", "citeRegEx": "Bacchus et al. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Vandenberghe", "author": ["S. Boyd"], "venue": "L.", "citeRegEx": "Boyd and Vandenberghe 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Diaconis", "author": ["S. Chatterjee"], "venue": "P.", "citeRegEx": "Chatterjee and Diaconis 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Taskar", "author": ["L. Getoor"], "venue": "B.", "citeRegEx": "Getoor and Taskar 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Induction of interpretable possibilistic logic theories from relational data", "author": ["Davis Ku\u017eelka", "O. Schockaert 2017] Ku\u017eelka", "J. Davis", "S. Schockaert"], "venue": "In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Ku\u017eelka et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ku\u017eelka et al\\.", "year": 2017}, {"title": "and Sen", "author": ["H. Nandi"], "venue": "P.", "citeRegEx": "Nandi and Sen 1963", "shortCiteRegEx": null, "year": 1963}, {"title": "Aggregation and population growth: The relational logistic regression and markov logic cases", "author": ["Poole"], "venue": "In 2nd International Workshop on Statistical Relational AI (StarAI", "citeRegEx": "Poole,? \\Q2012\\E", "shortCiteRegEx": "Poole", "year": 2012}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A", "author": ["O. Schulte", "H. Khosravi", "Kirkpatrick"], "venue": "E.; Gao, T.; and Zhu, Y.", "citeRegEx": "Schulte et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Rinaldo", "author": ["C.R. Shalizi"], "venue": "A.", "citeRegEx": "Shalizi and Rinaldo 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "N", "author": ["M. Singh", "Vishnoi"], "venue": "K.", "citeRegEx": "Singh and Vishnoi 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lifted generative learning of markov logic networks. Machine Learning 103(1):27\u201355", "author": ["Van Haaren"], "venue": null, "citeRegEx": "Haaren,? \\Q2016\\E", "shortCiteRegEx": "Haaren", "year": 2016}, {"title": "V", "author": ["Vapnik"], "venue": "N.", "citeRegEx": "Vapnik 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "M", "author": ["M.J. Wainwright", "Jordan"], "venue": "I.", "citeRegEx": "Wainwright and Jordan 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "and Neville", "author": ["R. Xiang"], "venue": "J.", "citeRegEx": "Xiang and Neville 2011", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "In the propositional setting, the marginal problem is to find a (maximum-entropy) distribution that has some given marginals. We study this problem in a relational setting and make the following contributions. First, we compare two different notions of relational marginals. Second, we show a duality between the resulting relational marginal problems and the maximum likelihood estimation of the parameters of relational models, which generalizes a well-known duality from the propositional setting. Third, by exploiting the relational marginal formulation, we present a statistically sound method to learn the parameters of relational models that will be applied in settings where the number of constants differs between the training and test data. Furthermore, based on a relational generalization ofmarginal polytopes, we characterize cases where the standard estimators based on feature\u2019s number of true groundings needs to be adjusted and we quantitatively characterize the consequences of these adjustments. Fourth, we prove bounds on expected errors of the estimated parameters, which allows us to lower-bound, among other things, the effective sample size of relational training data.", "creator": "LaTeX with hyperref package"}}}