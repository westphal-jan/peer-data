{"id": "1709.05861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "abstract": "This paper reports the analysis of audio and visual features in predicting the emotion dimensions under the seventh Audio/Visual Emotion Subchallenge (AVEC 2017). For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event (https://www.facebook.com/LAD_WOMAN_SWALL) in order to generate these samples using the standard, a set of standard RNN filters with a number of filters that are based on the type of audio data we presented. The resulting audio quality (SGR) of the event was shown as measured on the VSCs in the first video, with a fixed SGR range of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of SGR of", "histories": [["v1", "Mon, 18 Sep 2017 11:01:43 GMT  (313kb,D)", "http://arxiv.org/abs/1709.05861v1", "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385"], ["v2", "Tue, 24 Oct 2017 12:08:09 GMT  (313kb,D)", "http://arxiv.org/abs/1709.05861v2", "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385"]], "COMMENTS": "4 pages, 3 figures,arXiv:1605.06778,arXiv:1512.03385", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["narotam singh", "nittin singh", "abhinav dhall indian institute of technology ropar)"], "accepted": false, "id": "1709.05861"}, "pdf": {"name": "1709.05861.pdf", "metadata": {"source": "CRF", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "authors": ["Narotam Singh", "Nittin Singh", "Abhinav Dhall"], "emails": ["*2015csb1065@iitrpr.ac.in,", "2015csb1067@iitrpr.ac.in,", "abhinav@iitrpr.ac.in"], "sections": [{"heading": null, "text": "Keywords\u2014HOG, SIFT, GMM, Fisher, Neural Networks.\nI. INTRODUCTION\nEmotions are one of the most important and complex aspects of human consciousness. Researchers have been trying to classify or measure human emotions, as this research can lead to a significant improvement in human-computer interaction. Many studies have defined emotions as discrete categories. In discrete emotion theory, humans are thought to have some basic emotions. These basic emotions include anger, disgust, fear, happiness, sadness and surprise [4]. While some other studies represent emotions using continuous dimensional models. Dimensional models of emotion attempt to represent emotions as a particular continuous region in 2-D or 3-D continuum. Harold Schlosberg in his study defined the three dimensions of emotion: pleasantness-unpleasantness, attentionrejection, level of activation [24]. Dimensional analysis of emotions suggest that each dimension/factor is related to different regions in our brain, and are processed independently. Most studies incorporate two dimensions that are arousal and valence. Valence as used in psychology of emotions represents pleasantness-unpleasantness of an emotion, while Arousal signifies the level of activation. Another dimension is liking which points the wanting of a particular experience [12]. Recently the dimensional models are being widely used for research purposes, thanks to their practicality and ease of representing complex emotions. The basic motivation behind this challenge is the observation that a lot of human emotions can be judged by the physical gestures of human face while interacting with other people. [3] Psychologists have shown that various facial activities like eye movements, lip movement, wrinkles around eye-corners etc. have special semantic meanings in emotional\n* \u2020 Third year undergraduates - Equal Contribution\ncontext, and conversely by detecting these facial signs one can judge the emotional state of a person. Various studies have shown that it is in fact how humans understand others emotions. This paper implements our work in the challenge and our main focus is on the visual and audio features. We have extracted the visual features in the form of HOG features, deep visual features and fisher vector representation of SIFT [13] features and the audio features are used as it is provided by the organizers of the challenge. Then we have trained models using Neural Networks."}, {"heading": "II. RELATED WORKS", "text": "Affective Computing Analysis has been attracting many researchers since it was first introduced into modern computing by Rosalind Picard in 1995 [20]. Since then researchers have been introducing new techniques to predict human emotions. Regarding the extraction of features using audio signals and from human speech, feature sets such that LPC [18] have been used. MFCC [22] features have also been used by some getting better results. Teams which participated in AVEC 2016 competition [21] also used physiological features like the heart rate (HR), ECG signals, the skin conductance response (SCR), the skin conductance level (SCL), etc. Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1]. HOG [17] and LBP (Local binary patterns) features can also be used in combination like in [29], where they complemented each other. LBP tend to reduce the noise of hog noisy edges. LSTM (Long Short Term Memory) is a type of Recurrent neural Network proposed in 1997 which was used for affect recognition in [15]. This network was also used by teams in the AVEC 2015 competition [9]. Talking of neural networks, deep neural networks like Deep Convolutional Neural Network in [10] used transductive learning approach for emotion recognition along with the hypergraphs. Multimodal techniques are nowadays and are being used widely in research areas [28]. Researchers dealing with the different types of modalities very often want to get better results by combining their results. Many such techniques exist, one of them being Markov fusion Network [14]. MFN combines classifier outputs of different modalities with respect to its temporal relationship. Researchers also created apps and softwares for sentiment prediction that analyses facial expressions, gestures, speech, etc. One such tool is Kairos Emotions API1 which is available online.\n1https://www.kairos.com/emotion-analysis-api\nar X\niv :1\n70 9.\n05 86\n1v 1\n[ cs\n.C V\n] 1\n8 Se\np 20\n17"}, {"heading": "III. DATASET", "text": "This challenge is based on a novel database of humanhuman interactions recorded \u2018in-the-wild\u2019: Automatic Sentiment Analysis in the Wild (SEWA2) [25] data set. The dataset contains the video data, audio data and manual transcriptions of the speech of the corresponding subjects for training, development and testing with emotion dimensions values : arousal, valence and liking. Among the audio data are the audio files of the subjects, 23 LLDs from the eGeMAPSv0.1a feature set, 88 acoustic features from the eGeMAPSv0.1a feature set and Bag-of-audio-words representation (BoAW) of the LLDs for all the subjects. Among the video data are the video files of the subjects, video features (pixel coordinates of facial landmark points and head orientation) : normalised and unnormalised, and Bag-of-video-words representation of the normalised video features."}, {"heading": "IV. FEATURES", "text": ""}, {"heading": "A. Audio Features", "text": "The BoAW representation (generated by openXBOW [16]) of 23 LLDs from eGeMAPSv0.1a [5] feature set extracted using openSmile [6] are provided in the sub challenge dataset. We used BoAW with block size 6 seconds as it is for the training process using neural network.\nB. Visual Features\nBefore extracting any visual feature, we detected the human faces from the frames of the videos and applied the affine transformation using the facial landmark coordinates provided in the dataset to get the vertical faces. Faces are detected using the Viola-Jones face detection [27] algorithm and then cropped. All the visual features further discussed are extracted from these affine-warped faces. Most of the visual features are trained after normalisation process except in the cases where it increased the error. Images are resized and converted to gray scale as per need depending on the model requirements.\n1) Histogram of Gradients (HOG) features: Faces are resized to the default window size of HOG [17] (64x128) so as to get a single feature from the frames. Then trained on the neural network after normalisation.\n2) SIFT-Fisher Vector representation with GMM: Bag of words model is a quite successful approach of simplifying the representation of text/documents, used in Natural Language Processing. In the context of computer vision it has proved to be a very effective representation of an image [7]. We have used the BOW representation with GMM. GMM is a generative and rather complex clustering model as compared to simpler models such as k-means. We used GMM model for final cluster formation as it takes into account the distance of a data point from the centroid and also the weight of a particular cluster, which is otherwise ignored in simple k-means. We first calculated the SIFT [13] descriptors for each frame and only the descriptors having highest response value were chosen (top 50 descriptors). Then we clustered our entire data into 32 clusters3 using K-means algorithm. After dividing the data into 32 clusters, we calculated mean, covariance matrix and\n2https://sewaproject.eu/ 3The cluster number value 32 was chosen empirically.\nweights for each cluster. Weights are calculated as the fraction of data points allotted to that cluster. After that these values are used as the initial values of means, covariance matrices and weights in the EM (Expectation-Maximization) algorithm for the generation of the GMM.\na) Fisher Vector Extraction: Following the fisher vector approach in [2], we took the previously calculated SIFT features of each frame and got the Fisher vector encodings [11] of these sift features based on the pre-calculated GMM model. The resultant fisher vector has dimensions 128x32x2. Dimension reduction technique Principal Component Analysis (PCA) is used to convert the fisher encoding into final 4508 dimensions (99 percent variance). Thus a final fisher vector is obtained for each frame of each video.\n3) Deep Visual Features: An output of a particular layer of pretrained models : VGG-Face [19] and ResNet-50-dag [8] are used as features. These models are deep convolutional neural networks obtained from MatConvNet toolbox [26]. Output of 35th layer of VGG-Face and output of 174th layer of ResNet50-dag [8] are used as features of the frames of each video clip. These are activation layer (ReLU) and a fully connected layer in their respective models."}, {"heading": "V. TRAINING", "text": "Three different models for arousal,valence and liking are trained for different modalities. We used Neural Networks for this regression task instead of LibSVM because it was very time consuming for such a large dataset. More or less the number of layers used in the network is the same in all the models. ReLU (rectified linear unit) activation function is used between layers except the last layer where tanh function is used to get the final output. We observed that 4-5 layers were sufficient and increasing the layers more than this was computationally expensive as well as it did not improve the results."}, {"heading": "VI. RESULTS", "text": "For each emotion dimension, the models trained on the Train Set, are used for the prediction on the Development (Dev) Set. Then CCC is calculated for each model. Here \u03c1 is the correlation coefficient between two variables x and y, and\n\u03c32x, \u03c3 2 y is the variance of x and y respectively while \u00b5x, \u00b5y are respective means. Here CCC is the Concordance Correlation Coefficient which is calculated as:\nCCC = 2\u03c1\u03c3x\u03c3y\n\u03c32x + \u03c3 2 y + (\u00b5x \u2212 \u00b5y)2"}, {"heading": "VII. MULTIMODAL FUSION", "text": "Now we can perfom multimodal fusion on these features to get final fused results.\nFeature Arousal Valence Liking\nTest-Multimodal 0.361 0.437 -0.001"}, {"heading": "VIII. CONCLUSION", "text": "This report summarizes our second year internship project which proved very informative as beginners and this was our first time handling such large datasets. Although the field of emotion recognition is not in very advanced state, still the rate of growth ensures a quite bright future. From our project we can say that visual features such as HOG, SIFT, etc. are capable of capturing the important facial information. We observed that the HOG features and VGG-Face features performed quite well as compared to the others.We\nwere expecting better results from the SIFT-GMM features but the results obtained are not very encouraging. However the overall results have shown that these visual features are quite dependable, for most of the cases and can be used as a way for emotion detection. Moreover, it was observed that the multi-modal fusion provides even better results by giving suitable weights to the respective features. Further we have observed that the weights varied from model to model, which suggests that the success of detecting a particular emotion is dependent on the type of feature we are extracting, and simpler models can have more impact as compared to complex models."}, {"heading": "ACKNOWLEDGMENT", "text": "We are greatly thankful to our teacher and the entire AVEC 2017 organising team, who gave us the opportunity to participate in this informative event. This project has helped us in gaining more knowledge and insight regarding the field of computer vision as well as machine learning and some basic knowledge of deep learning."}], "references": [{"title": "Representing shape with a spatial pyramid kernel", "author": ["A.Z.A. BOSCH", "X. MUNOZ"], "venue": "Conference on Image and Video Retrieval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A temporally piece-wise fisher vector approach for depression analysis", "author": ["A. DHALL", "R. GOECKE"], "venue": "In 2015 International Conference on Affective Computing and Intelligent Interaction (ACII),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An argument for basic emotions", "author": ["P. EKMAN"], "venue": "Cognition and emotion,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1106}, {"title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing", "author": ["F. EYBEN", "K.R. SCHERER", "B.W. SCHULLER", "J. SUNDBERG", "E. ANDR", "C. BUSSO", "L.Y. DEVILLERS", "J. EPPS", "P. LAUKKA", "S.S. NARAYANAN", "K.P. TRUONG"], "venue": "IEEE Transactions on Affective Computing 7,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor", "author": ["F. EYBEN", "F. WENINGER", "F.G.B. S"], "venue": "In Proc. ACM Multimedia (MM) (Barcelona,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A visual bag of words method for interactive qualitative localization and mapping", "author": ["D. FILLIAT"], "venue": "International Conference on Robotics and Automation, Italy", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. HE", "D. JIANG", "L. YANG", "E. PEI", "P. WU", "H. SAHLI"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Deep learning driven hypergraph representation for image-based emotion recognition", "author": ["HUANG Y", "LU"], "venue": "In Proceedings of the International Conference on Multimodal Interaction (Tokyo, Japan,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Image classifcation with the fisher vector: Theory and practice", "author": ["JORGE SANCHEZ", "FLORENT PERRONNIN", "T.M.J. V"], "venue": "International Journal of Computer Vision", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deap : a database for emotion analysis using physiological signals", "author": ["KOELSTRA", "E.A. SANDER"], "venue": "IEEE Transactions on Affective Computing vol.3", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. LOWE"], "venue": "Int. J. Comput. Vision,Hingham,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Multi-modal fusion based on classication using rejection option and markov fusion networks", "author": ["M. GLODEK", "G.P.M. SCHELS", "F. SCHWENKER"], "venue": "In Proc. of ICPR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "LSTMmodeling of continuous emotions in an audiovisual aect recognition framework", "author": ["M. WOLLMER", "M. KAISER", "F.E.B. S", "G. RIGOLL"], "venue": "Image and Vision Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit", "author": ["B.W.S. MAXIMILIAN SCHMITT"], "venue": "arXiv preprint arXiv:1605.06778", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Histograms of oriented gradients for human detection", "author": ["B.T. NAVNEET DALAL"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Emotion recognition in speech using neural networks", "author": ["J. NICHOLSON", "K. TAKAHASHI", "R. NAKATSU"], "venue": "Neural Computing & Applications 9,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Deep face recognition", "author": ["O.M. PARKHI", "A. VEDALDI", "A. ZISSERMAN"], "venue": "In Proceedings of the British Machine Vision Conference (BMVC)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multimodal emotion recognition for avec 2016 challenge", "author": ["F. POVOLNY", "P. MATEJKA", "M. HRADIS", "A. POPKOV\u00c1", "L. OTRUSINA", "P. SMRZ", "I. WOOD", "C. ROBIN", "L. LAMEL"], "venue": "In Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge (New York, NY,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Avec 2017\u2013real-life depression, and affect recognition workshop and challenge", "author": ["F. RINGEVAL", "B. SCHULLER", "M. VALSTAR", "J. GRATCH", "R. COWIE", "S. SCHERER", "S. MOZGAI", "N. CUMMINS", "M. SCHMITT", "M. PANTIC"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Multimodal sentiment analysis in the wild: Ethical considerations on data collection, annotation, and exploitation", "author": ["B. SCHULLER", "GANASCIA", "J.-G", "L. DEVILLERS"], "venue": "In Actes du Workshop on Ethics In Corpus Collection,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. VEDALDI", "K. LENC"], "venue": "In Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Robust real-time object detection", "author": ["P. VIOLA", "M. JONES"], "venue": "International Journal of Computer Vision", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "An hog-lbp human detector with partial occlusion handling", "author": ["WANG T.H. X", "YAN"], "venue": "International Conference on Computer Vision", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "These basic emotions include anger, disgust, fear, happiness, sadness and surprise [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "Another dimension is liking which points the wanting of a particular experience [12].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "We have extracted the visual features in the form of HOG features, deep visual features and fisher vector representation of SIFT [13] features and the audio features are used as it is provided by the organizers of the challenge.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "Regarding the extraction of features using audio signals and from human speech, feature sets such that LPC [18] have been used.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Teams which participated in AVEC 2016 competition [21] also used physiological features like the heart rate (HR), ECG signals, the skin conductance response (SCR), the skin conductance level (SCL), etc.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "Apart from the visual features like HOG [17] and SIFT [13] features, PHOG are also successful in human detection [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 15, "context": "HOG [17] and LBP (Local binary patterns) features can also be used in combination like in [29], where they complemented each other.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "HOG [17] and LBP (Local binary patterns) features can also be used in combination like in [29], where they complemented each other.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "LSTM (Long Short Term Memory) is a type of Recurrent neural Network proposed in 1997 which was used for affect recognition in [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "This network was also used by teams in the AVEC 2015 competition [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Talking of neural networks, deep neural networks like Deep Convolutional Neural Network in [10] used transductive learning approach for emotion recognition along with the hypergraphs.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Many such techniques exist, one of them being Markov fusion Network [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "This challenge is based on a novel database of humanhuman interactions recorded \u2018in-the-wild\u2019: Automatic Sentiment Analysis in the Wild (SEWA2) [25] data set.", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "The BoAW representation (generated by openXBOW [16]) of 23 LLDs from eGeMAPSv0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "1a [5] feature set extracted using openSmile [6] are provided in the sub challenge dataset.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "1a [5] feature set extracted using openSmile [6] are provided in the sub challenge dataset.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "Faces are detected using the Viola-Jones face detection [27] algorithm and then cropped.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "1) Histogram of Gradients (HOG) features: Faces are resized to the default window size of HOG [17] (64x128) so as to get a single feature from the frames.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "In the context of computer vision it has proved to be a very effective representation of an image [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "We first calculated the SIFT [13] descriptors for each frame and only the descriptors having highest response value were chosen (top 50 descriptors).", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "a) Fisher Vector Extraction: Following the fisher vector approach in [2], we took the previously calculated SIFT features of each frame and got the Fisher vector encodings [11] of these sift features based on the pre-calculated GMM model.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "a) Fisher Vector Extraction: Following the fisher vector approach in [2], we took the previously calculated SIFT features of each frame and got the Fisher vector encodings [11] of these sift features based on the pre-calculated GMM model.", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "3) Deep Visual Features: An output of a particular layer of pretrained models : VGG-Face [19] and ResNet-50-dag [8] are used as features.", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "3) Deep Visual Features: An output of a particular layer of pretrained models : VGG-Face [19] and ResNet-50-dag [8] are used as features.", "startOffset": 112, "endOffset": 115}, {"referenceID": 21, "context": "These models are deep convolutional neural networks obtained from MatConvNet toolbox [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Output of 35th layer of VGG-Face and output of 174th layer of ResNet50-dag [8] are used as features of the frames of each video clip.", "startOffset": 75, "endOffset": 78}, {"referenceID": 19, "context": "TABLE I: AVEC Baseline [23] : Concordance Correlation Coefficient Table for Development (Dev) Set and Test Set.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "This paper reports the analysis of audio and visual features in predicting the emotion dimensions under the seventh Audio/Visual Emotion Subchallenge (AVEC 2017). For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event. Then we trained fully connected neural network regression model on the dataset for all these different modalities. We applied multimodal fusion on the output models to get the Concordance correlation coefficient on Development set as well as Test set. Keywords\u2014HOG, SIFT, GMM, Fisher, Neural Networks.", "creator": "LaTeX with hyperref package"}}}