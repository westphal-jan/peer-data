{"id": "0807.1997", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jul-2008", "title": "Multi-instance learning by treating instances as non-I.I.D. samples", "abstract": "multi - instance learning attempts to learn from a training set consisting randomly labeled bags each containing many unknown entities. previous studies typically treat the instances in the bags as all substantially identically distributed. however, the instances in a bag are partially independent, and therefore a better performance can be expected if the instances are treated in an non - i. i. d. way that provides the relations against instances. in this paper, we propose a cheap yet effective multi - instance learning method, which regards each bag as a graph and uses a specific function to distinguish the graphs by considering the features of the nodes and well as the features of their edges that convey some relations among sessions. the complexity of the proposed method is validated by experiments.", "histories": [["v1", "Sat, 12 Jul 2008 20:19:18 GMT  (413kb)", "http://arxiv.org/abs/0807.1997v1", null], ["v2", "Wed, 15 Apr 2009 17:22:40 GMT  (441kb)", "http://arxiv.org/abs/0807.1997v2", null], ["v3", "Fri, 17 Apr 2009 08:43:03 GMT  (436kb)", "http://arxiv.org/abs/0807.1997v3", "ICML, 2009"], ["v4", "Wed, 13 May 2009 16:22:00 GMT  (453kb)", "http://arxiv.org/abs/0807.1997v4", "ICML, 2009"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["zhi-hua zhou", "yu-yin sun", "yu-feng li"], "accepted": true, "id": "0807.1997"}, "pdf": {"name": "0807.1997.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yu-Feng Li"], "emails": ["liyf}@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :0\n80 7.\n19 97\nv1 [\ncs .L\nG ]\n1 2\nJu l 2"}, {"heading": "1 Introduction", "text": "In multi-instance learning [14], the training set consists of many bags of instances. It is known that a bag is positive if it contains at least one positive instance; otherwise it is a negative bag. However, although the labels of the training bags are known, the labels of the instances in the bags are unknown. The goal is to generate a learner to classify unseen bags. Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.\nIn our opinion, the usefulness of multi-instance learning mainly lies in the fact that many real objects have inherent structures, and by adopting the multiinstance representation we are able to represent such objects more naturally and capture more information than simply using the flat single-instance representation. For example, suppose we can partition an image into several parts. In contrast to representing the whole image as a single-instance, if we represent each part as an instance, then the partition information is captured by the multi-instance representation; and if the partition is meaningful (e.g., each part corresponds to a region of saliency), then the additional information captured by the multi-instance representation is helpful to make the learning task easier to deal with.\nIt is obviously not a good idea to apply multi-instance learning techniques everywhere since if the single-instance representation is sufficient, using multiinstance representation just gilds the lily. Even on tasks where the objects have inherent structure, we should keep in mind that the power of multi-instance representation exists in its ability of capturing some structure information. However, previous studies on multi-instance learning typically treat the instances in the bags as independently and identically distributed [41], which neglects the fact that the relations among the instances convey important structure information. Considering the above image task again, treating the different image parts as inter-correlated samples is evidently more meaningful than treating them as unrelated samples. Zhou and Xu [41] showed that if the instances were treated as i.i.d. samples, multi-instance learning is just a special case of semi-supervised learning [46] and thus the advantages of multi-instance representation could not be exploited well. Actually, the instances in a bag are rarely independent, and a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances.\nIn this paper, we propose a multi-instance learning method which does not treat the instances as i.i.d. samples. Our basic idea is to regard each bag as an entity which is to be processed as a whole, and regard instances as inter-correlated components of the entity. Technically, we map every bag to an undirected graph where the nodes correspond to the instances while the edges convey the affinity of pairs of instances. Then, we design a specific graph kernel for distinguishing the positive and negative bags. Experiments show that our proposed method, MIGraph, achieves a better performance comparing with existing methods that treat the instances as i.i.d. samples.\nThe rest of this paper is organized as follows. We briefly review related work in Section 2. Then, we propose the MIGraph method in Section 3 and report on our experiments in Section 4. Finally, we conclude the paper in Section 5."}, {"heading": "2 Related Work", "text": "Many multi-instance learning algorithms have been developed during the past decade. It is difficult to list all existing methods here. To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc. Most algorithms work by adapting single-instance supervised learning algorithms to the multi-instance representation through shifting the focuses of the algorithms from the discrimination on the instances to the discrimination on the bags [42]. Recently there are also some proposal on adapting the multi-instance representation to single-instance algorithms by representation transformation [45].\nKernel methods for multi-instance learning have been studied by many researchers. Ga\u0308rtner et al. [17] defined the MI-Kernel by regarding each multi-\ninstance bag as a set of feature vectors and then applying set kernel directly. Andrews et al. [2] proposed mi-SVM and MI-SVM. The mi-SVM tries to identify a maximal margin hyperplane for the instances with subject to the constraints that at least one instance of each positive bag locates in the positive half-space while all instances of negative bags locate in the negative half-space. The MISVM tries to identify a maximal margin hyperplane for the bags by regarding the margin of the \u201cmost positive instance\u201d in a bag as the margin of that bag. Cheung and Kwok [11] argued that the sign instead of the value of the margin of the most positive instance is important. They defined a loss function which allows both the bags and instances to participate in the optimization process directly, and used the well-formed constrained concave-convex procedure (CCCP) to perform the optimization. Later, they [20] designed marginalized multi-instance kernels by considering that the contribution of different instances can be different. Chen and Wang [10] proposed the DD-SVM method which employs Diverse Density [22] to learn a set of instance prototypes and then maps the bags to a feature space based on the instance prototypes. Zhou and Xu [41] proposed the MissSVM method by regarding instances of negative bags as labeled examples while those of positive bags as unlabeled examples with positive constraints.\nIn addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28]. The main difference between standard multi-instance learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, and a bag is positive if it has an instance satisfies this concept; while in generalized multi-instance learning [35, 28] there are multiple concepts, and a bag is positive only when all concepts are satisfied (i.e., the bag contains instances from every concept). Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported. In this paper we mainly work on standard multi-instance learning [14] and will show that our method is also applicable to multi-instance regression. Actually it is also possible to extend our proposal to other variants of multi-instance learning."}, {"heading": "3 The MIGraph Method", "text": "In this section we propose the MIGraph method. Before presenting the details, we give the formal definition of multi-instance learning as following. Let X denote the instance space. Given a data set {(X1, y1), \u00b7 \u00b7 \u00b7 , (Xi, yi), \u00b7 \u00b7 \u00b7 , (XN , yN)}, where Xi = {xi1, \u00b7 \u00b7 \u00b7 ,xij , \u00b7 \u00b7 \u00b7 ,xi,ni} \u2286 X is called as a bag and yi \u2208 Y = {\u22121,+1} is the label of Xi, the goal is to generate a learner to classify unseen bags. Here xij \u2208 X is an instance [xij1, \u00b7 \u00b7 \u00b7 , xijl , \u00b7 \u00b7 \u00b7 , xijd] \u2032 , xijl is the value of xij at the lth attribute, N is the number of training bags, ni is the number of instances in Xi, and d is the number of attributes. If there exists a g \u2208 {1, \u00b7 \u00b7 \u00b7 , ni} such that xig is a positive instance, then Xi is a positive bag and thus yi = +1; otherwise yi = \u22121. Yet the concrete value of the index g is unknown.\nWe first explain our intuition of the MIgraph method. Here, we use the three example images shown in Fig. 1 for illustration. For simplicity, we show six marked patches in each figure, and assume that each image corresponds to a bag, each patch corresponds to an instance in the bag, and the marked patches with the same color are very similar (real cases are of course more complicated, but the essentials are similar as the illustration). If the instances were treated as independent samples then Fig. 1 can be abstracted as Fig. 2, which is the typical way taken by previous multi-instance learning studies, and obviously the three bags are similar to each other since they contain identical number of very similar instances. However, if we consider the relations among the instances, we can find that in the first two bags the blue marks are very close to each other while in the third bag the blue marks scatters among orange marks, and thus the first two bags should be more similar than the third bag. In this case, Fig. 1 can be abstracted by Fig. 3. It is evident that the abstraction in Fig. 3 is more desirable than that in Fig. 2. Here the essential is, the relation structures of bags\nbelonging to same class are relatively more similar, while that of bags belonging to different classes are relatively more dissimilar.\nNow we describe the MIGraph method. The first step is to construct a graph for each bag. Inspired by [32] which shows that \u01eb-graph is helpful for discovering the underlying manifold structure of data, here we establish an \u01eb-graph for every bag. The process is quite straightforward. For a bag Xi, we regard every instance of it as a node. Then, we compute the distance of every pair of nodes, e.g., xiu and xiv. If the distance between xiu and xiv is smaller than a pre-set threshold \u01eb, then an edge is established between these two nodes, where the weight of the edge expresses the affinity of the two nodes (in experiments we use the normalized reciprocal of non-zero distance as the affinity value). Many distance measures can be used to compute the distances. According to the manifold property [32], i.e., a small local area is approximately an Euclidean space, we use Euclidean distance to establish the \u01eb-graph. If categorical attributes are involved, we use VDM (Value Difference Metric) [30] as a complement.\nAfter mapping the training bags to a set of graphs, we can have several options to build a classifier. For example, we can build a k-nearest neighbor classifier that employs graph edit distance [23], or we can design a graph kernel [16] to capture the similarity among graphs and then solve classification problems by kernel machines such as SVM. The proposed MIGraph method takes the second way, and the idea of our graph kernel is illustrated in Fig. 4.\nBriefly, in order to measure the similarity between the two multi-instance bags shown in the left part of Fig. 4, we use a node kernel, knode, to take into account the information conveyed by the nodes, use an edge kernel, kedge, to take into account the information conveyed by the edges, and aggregate them to obtain the final graph kernel, kgraph. Formally, we define kgraph as following.\nDefinition 1. Given two multi-instance bags X1 and X2 which are presented as graphs Gh({xhj} nh j=1, {ehj} mh j=1), h = 1, 2, where nh and mh are the number of\nnodes and edges in Gh, respectively. kgraph is defined as\nkgraph(X1, X2) =\nn1 \u2211\ni=1\nn2 \u2211\nj=1\nknode(x1i,x2j) +\nm1 \u2211\ni=1\nm2 \u2211\nj=1\nkedge(e1i, e2j), (1)\nwhere knode and kedge are positive semidefinite kernels.\nTo avoid numerical problem, kgraph is normalized to\nkgraph(X1, X2) = kgraph(X1, X2) \u221a\nkgraph(X1, X1) \u221a kgraph(X2, X2) . (2)\nThe knode and kedge can be defined in many ways. In this paper we simply define knode using Gaussian RBF kernel as\nknode(x1i,x2j) = exp(\u2212\u03b3||x1i \u2212 x2j || 2), (3)\nand so the first part of Eq. 1 is exactly the MI-Kernel using Gaussian RBF kernel [17]. kedge is also defined in a form as similar as Eq. 3, except that x1i and x2j are replaced by e1i and e2j , respectively.\nHere a key is how to define the feature vector describing an edge. In this paper, for the edge connecting the nodes xiu and xiv of the bag Xi, we define it as [du, pu, dv, pv]\n\u2032, where du is the degree of the node xiu, that is, the number of edges connecting xiu with other nodes. Note that it has been normalized through dividing it by the total number of edges in the graph corresponding to Xi. dv is the degree of the node xiv, which is defined similarly. pu is defined as pu = wuv/ \u2211\nwu,\u2217, where the numerator is the weight of the edge connecting xiu to xiv; wu,\u2217 is the weight of the edge connecting xiu to any nodes in Xi, thus the denominator is the sum of all the weights connecting with xiu. It is evident that pu conveys information on how important (or unimportant) the connection with the node xiv is for the node xiu. pv is defined similarly for the node xiv. The intuition here is that, edges are similar if properties of their ending nodes (e.g., high-degree nodes or low-degree nodes) are similar.\nIt is obvious that the kgraph defined in Eq. 1 is a positive definite kernel, the computational complexity of kgraph(X1, X2) is O(n1n2+m1m2), and the kgraph can be used for any kinds of graphs. Clearly, the kgraph satisfies the four major properties that should be considered for a graph kernel definition [7]: the kernel should be a good measure of similarity for graphs; the computational complexity should be in polynomial time; the kernel must be positive definite; it should be somewhat general which will not limited in small subsets of graphs.\nActually, we have tried to directly apply some existing graph kernels but unfortunately failed, which will be reported in Section 4.5, and so we decide to design our own graph kernel for multi-instance learning. Our above design is very simple, but in the next section we can see that the proposed MIGraph method is quite effective."}, {"heading": "4 Experiments", "text": "We evaluate the performance of the proposed MIGraph method on three tasks, including two multi-instance classification tasks and a multi-instance regression task."}, {"heading": "4.1 Drug Activity Prediction", "text": "Drug activity prediction is one of the earliest applications of multi-instance learning [14], where the Musk data is a real-world benchmark that has been studied by many researchers. Here a bag corresponds to a molecule and the instances correspond to the alternative low-energy shapes of that molecule. Each instance is a 166-dimensional feature vector. For each molecule, if at least one of its lowenergy shapes could tightly bind to the target area of some larger molecules, the molecule is qualified to make a certain drug and it is regarded as a positive bag; otherwise it is a negative bag.\nThere are two data sets, i.e. Musk1 and Musk2, both publicly available at the UCI machine learning repository [5]. Musk1 contains 47 positive bags and 45 negative bags, and the number of instances contained in each bag ranges from 2 to 40 (5.17 in average). Musk2 contains 39 positive bags and 63 negative bags, and the number of instances contained in each bag ranges from 1 to 1,044 (64.49 in average).\nWe compare MIGraph with MI-Kernel [17] via 10 times 10-fold cross validation (i.e., we repeat 10-fold cross validation for ten times with different random data partitions). Both methods use Gaussian RBF Kernel and the parameters are determined through cross validation on training sets. The results are shown in Table 11. Table 1 also shows the performance of many other multi-instance learning methods. Note that these are the best results reported in literature and since they were obtained using different experimental configurations, these results are only for reference instead of a rigorous comparison.\nIt can be observed from Table 1 that the performance of MIGraph is better than many state-of-the-art multi-instance learning methods. The most important is the comparison between MIGraph andMI-Kernel, because their only difference is that MI-Kernel treats the instances as i.i.d. samples while MIGraph doe not. Table 1 shows that MIGraph is apparently better than MI-Kernel. This suggests that in contrast to treating the instances as i.i.d. samples, treating them in an non-i.i.d. way that exploits the relations among instances is a better choice."}, {"heading": "4.2 Image Categorization", "text": "Image categorization is one of the most successful applications of multi-instance learning. In this experiment we use the COREL data set described in [10, 9]. There are twenty image categories each containing 100 images, and thus 2,000\n1 Note that the performance of MI-Kernel in our implementation is better than that reported in [17].\nimages in total. Each image is regarded as a bag, and the ROIs (Region of Interests) in the image are regarded as instances described by nine features. We used the processed data 2 such that all the bags and instances are as same as those used in [10, 9]. Table 2 summarizes the details of this data set.\n2 http://www.cs.olemiss.edu/\u223cychen/ddsvm.html\nWe use the same experimental routine as that described in [9]. In detail, the original data set is used as two data sets. The first one (i.e. 1000-Image) contains the first ten categorizes in Table 2 while the second (i.e. 2000-Image) uses all the categorizes. On each data set, we randomly partition the images within each category in half, and use one subset for training while the other for testing. The experiment is repeated for five times with five random splits, and the average results are recorded. One-against-one strategy is employed by both MIGraph and MI-Kernel for this multi-class task. The parameters of both MIGraph and MI-Kernel are determined through cross validation on training sets. The overall accuracy as well as 95% confidence intervals are reported in Table 3. For reference, the table also shows the best results of some multiinstance learning methods reported in literature.\nTable 3 shows that MIGraph is among the best performing methods on this task. In particular, it is significantly better than MI-Kernel, which suggests that it is beneficial to treat the instances in an non-i.i.d. way. For a more detailed comparison, we present the confusion matrices of MIGraph and MI-Kernel on 1000-Image in Figs. 5 and 6, respectively, where each row lists the average percentages of images in a specific category classified to each of the 10 categories. Therefore, the numbers on the diagonal show the classification accuracy for each category and off-diagonal entries indicate classification errors.\nFigs. 5 and 6 show that on most categories the performance of MIGraph is better than or comparable to that of MI-Kernel. It is impressive that the accuracy of MIGraph is about 17% (66.0%\u221255.4%55.4% ) higher than MI-Kernel on Category 1 (i.e. Beach) and about 12.7% (85.2%\u221275.6%75.6% ) higher on Category 5 (i.e. Elephants). On Category 0 (i.e. African people and villages), however, MIGraph is much worse than MI-Kernel. This might owe to the fact that structure information of examples belonging to such a complicated concept is too difficult to be captured by the simple edge kernel used in MIGraph, while using incorrect structure information is worse than conservatively treating the instances as i.i.d. samples.\nIt can be observed from Figs. 5 and 6 that for both MIGraph and MI-Kernel, the largest errors occur between Category 1 (i.e. Beach) and Category 8 (i.e. Mountains and glaciers). This phenomenon has also appeared in previous stud-\nies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean."}, {"heading": "4.3 Multi-Instance Regression", "text": "Four multi-instance regression data sets described in Amar et al. [1] are used in this experiment. The data sets are named as LJ-r.f.s where r is the number of relevant features, f is the number of features, and s is the number of different scale factors used for the relevant features that indicate the importance of the features. The suffix S indicates that the data set uses only labels that are not near 1/2. Details of the data sets can be found in [1].\nLeave-one-out test is performed on each data set, where the parameters of both MIGraph and MI-Kernel are determined by cross validation on training sets. The mean squared losses are reported in Table 4. For reference, the table also shows the best results of some multi-instance learning methods reported in literature, where Diverse Density is abbreviated as DD.\nTable 4 shows that MIGraph also works well on those multi-instance regression data sets. In particular, it is apparently better than MI-Kernel, which suggests that treating the instances in an non-i.i.d. way that exploits the relations among instances is also beneficial for multi-instance regression."}, {"heading": "4.4 A Further Study", "text": "We further study the knode, kedge and kgraph in Eq. 1 in the first experiment. The average number of support vectors of these kernels, and the number of their shared support vectors are shown in Table 5.\nTable 5 discloses that knode and kedge are quite different, and their contribution to kgraph are also different. It has been shown in [18] that if two kernels contribute different information and the performance of the two kernels are equally good, the upper bound of generalization error of a combined kernel will be lower. It has also been shown in [21] that in practice, even when the kernels are not equally well, a combined kernel can be a better choice. This explains why MIGraph is superior to MI-Kernel."}, {"heading": "4.5 Using Existing Graph Kernels", "text": "We have also tried to apply a famous graph kernel, i.e., all-paths kernel [7], for our purpose. It has been proved [7] that determining all paths is NP-hard. Considering the computational complexity, we use two of its variants.\nDefinition 2. Given two multi-instance bags X1 and X2 which are presented as graphs Gh({xhj} nh j=1, {ehj} mh j=1), h = 1, 2, where nh and mh are the number of nodes and edges in Gh, respectively. knp and knc are defined as\nknp(X1, X2) = \u2211n1\ni=1\n\u2211n2\nj=1 knode(x1i,x2j) + kpath (G1, G2) (4)\nknc(X1, X2) = \u2211n1\ni=1\n\u2211n2\nj=1 knode(x1i,x2j) + kcircle (G1, G2) (5)\nwhere both knp and knc are positive semidefinite kernels.\nTo avoid numerical problem, knp and knc are also normalized by Eq. 2. Before defining kpath and kcircle, we give the definitions of path and circle at first.\nDefinition 3. Given a graph Gh({xhj} nh j=1, {ehj} mh j=1), s = (es1 , \u00b7 \u00b7 \u00b7 , esl) is called a sequence of edges from es1 to esl , where esi \u2208 {ehj} mh j=1, esi 6= esj , 1 \u2264 i < j \u2264 l, and l is the length of the sequence. Each edge esi connects two nodes, i.e., (xsi1 ,xsi2). The sequence s satisfies that xsi2 = xsj1 , 1 \u2264 i = j \u2212 1 \u2264 l \u2212 1. If xs11 = xsl2 , s is called a circle; otherwise s is called a path.\nDefinition 4. Given two graphs Gh, h = 1, 2. Let P(Gh) and C(Gh) denote the set of paths and circles extracted from Gh, respectively. The kernels kpath and kcircle are defined as\nkpath(G1, G2) = \u2211\np1\u2208P(G1)\n\u2211\np2\u2208P(G2) kp(p1, p2) (6)\nkcircle(G1, G2) = \u2211\nc1\u2208C(G1)\n\u2211\nc2\u2208C(G2) kc(c1, c2) (7)\nwhere the definitions of kp and kc are as same as the marginalized kernel [19], i.e.,\nkp(p, p \u2032) =\n{\n0 (l 6= l\u2032) k(xp11 ,xp\u2032 11 ) \u220fl i=1 k(epi , ep\u2032i)k(xpi2 ,xp\u2032i2) (l = l \u2032)\n(8)\nkc(c, c \u2032) =\n{\n0 (l 6= l\u2032) k(xc11 ,xc\u2032 11 ) \u220fl i=1 k(eci , ec\u2032i)k(xci2 ,xc\u2032i2) (l = l \u2032)\n(9)\nSimilar as in Eq. 3, the kernel k in both Eqs. 8 and 9 is also Gaussian RBF kernel. We use non-divergent DFS method (i.e., to avoid visiting a node for several times in a path) to search paths and circles, and set the maximal length to be considered for the paths/circles as 3. It can be seen in the following that even in such a simple setting, knp and knc could not work out Musk2 in a reasonable time due to the large computational cost.\nThe performance of knp and knc are summarized in Table 6, where the experimental configurations are as same as that used in the previous sections. For\nreference, Table 6 also includes the results of MIGraph and MI-Kernel. OnMusk2 since there are too many edges in the graphs, knp and knc did not finish 10-fold cross validation after running for 72 hours (MI-Kernel and MIGraph require only about 0.5 hour) on a machine with two 3GHz CPUs and 4GB RAM, and so we terminated them.\nIt can be observed from Table 6 that both knp and knc are never better than MIGraph, and in contrast to MIGraph which is always better than MI-Kernel, knp and knc are only better than MI-Kernel on 1000-image. Considering that knp and knc have also tried to exploit structure information, the above observations suggest that we must be careful when exploiting structure information since using structure information inappropriately may be worse than treating the instances as i.i.d. samples conservatively."}, {"heading": "5 Conclusion", "text": "Previous studies on multi-instance learning typically treat the instances in the bags as i.i.d. samples, which neglects the fact that the instances are rarely independent and the relations among the instances convey important structure information. In this paper, we propose the MIGraph method which treats the instances in an non-i.i.d. way that exploits the relations among instances. Experiments show that MIGraph is simple yet effective, which is among the best performing methods on some multi-instance classification and regression tasks.\nAn interesting future issue is to design a better graph kernel to capture more useful structure information of multi-instance bags. Another future issue is to design a better graph construction process that suits multi-instance learning well. Applying graph edit distance or metric learning methods to the graphs corresponding to multi-instance bags is also worth trying.\nIt is noteworthy that the success of MIGraph also suggests that it is possible to improve other multi-instance learning methods by incorporating mechanisms to exploit the relations among instances, which opens a promising future direction. Moreover, it is also possible to extend our proposal to other settings such\nas generalized multi-instance learning, multi-instance semi-supervised learning, multi-instance active learning, multi-instance multi-label learning, etc."}], "references": [{"title": "Multiple-instance learning of real-valued data", "author": ["R.A. Amar", "D.R. Dooly", "S.A. Goldman", "Q. Zhang"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "On learning from multi-instance examples: Empirical evaluation of a theoretical approach", "author": ["P. Auer"], "venue": "In Proceedings of the 14th International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A boosting approach to multiple instance learning", "author": ["P. Auer", "R. Ortner"], "venue": "In Proceedings of the 15th European Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "UCI repository of machine learning databases", "author": ["C. Blake", "E. Keogh", "C.J. Merz"], "venue": "[http://www.ics.uci.edu/\u223cmlearn/MLRepository.html], Department of Information and Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Multi-instance tree learning", "author": ["H. Blockeel", "D. Page", "A. Srinivasan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.-P. Kriegel"], "venue": "In Proceedings of the 5th IEEE International Conference on Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": "In Proceeding of the 24th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "MILES: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1931}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "J.Z. Wang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "A regularization framework for multiple-instance learning", "author": ["P.-M. Cheung", "J.T. Kwok"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "A framework for learning rules from multiple instance data", "author": ["Y. Chevaleyre", "J.-D. Zucker"], "venue": "In Proceedings of the 12th European Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Bray", "C. Dance", "L. Fan"], "venue": "In Proceedings of the ECCV\u201904 Workshop on Statistical Learning in Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Solving the multipleinstance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnappuram", "R.B. Rao"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "A survey of kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "SIGKDD Explorations,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Multi-instance kernels", "author": ["T. G\u00e4rtner", "P.A. Flach", "A. Kowalczyk", "A.J. Smola"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Composite kernels for hypertext categorisation", "author": ["T. Joachims", "N. Cristianini", "J. Shawe-Taylor"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Marginalized kernels between labeled graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Marginalized multi-instance kernels", "author": ["J.T. Kwok", "P.-M. Cheung"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Laerning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-P\u00e9rez"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "A quadratic programming approach to the graph edit distance problem", "author": ["M. Neuhaus", "H. Bunke"], "venue": "In Proceedings of the 6th International Workshop on Graph-based Representations in Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "MISSL: Multiple-instance semi-supervised learning", "author": ["R. Rahmani", "S.A. Goldman"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Supervised versus multiple instance learning: An empirical comparison", "author": ["S. Ray", "M. Craven"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Multiple instance regression", "author": ["S. Ray", "D. Page"], "venue": "In Proceedings of the 18th International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Learning single and multiple instance decision trees for computer security applications", "author": ["G. Ruffo"], "venue": "PhD thesis, Department of Computer Science,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "On generalized multiple-instance learning", "author": ["S.D. Scott", "J. Zhang", "J. Brown"], "venue": "Technical Report UNL-CSE-2003-5, Department of Computer Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Multiple-instance active learning", "author": ["B. Settles", "M. Craven", "S. Ray"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Toward memory-based reasoning", "author": ["C. Stanfill", "D. Waltz"], "venue": "Communications of the ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1986}, {"title": "SVM-based generalized multiple-instance learning via approximate box counting", "author": ["Q. Tao", "S. Scott", "N.V. Vinodchandran", "T.T. Osugi"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "J. Platt", "C. Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Solving the multi-instance problem: A lazy learning approach", "author": ["J. Wang", "J.-D. Zucker"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "A two-level learning method for generalized multi-instance problem", "author": ["N. Weidmann", "E. Frank", "B. Pfahringer"], "venue": "In Proceedings of the 14th European Conference on Machine Learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2004}, {"title": "Image database retrieval with multiple-instance learning techniques", "author": ["C. Yang", "T. Lozano-P\u00e9rez"], "venue": "In Proceedings of the 16th International Conference on Data Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Multiple-instance pruning for learning efficient cascade detectors", "author": ["C. Zhang", "P. Viola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Adapting RBF neural networks to multi-instance learning", "author": ["M. Zhang", "Z. Zhou"], "venue": "Neural Processing Letters,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Content-based image retrieval using multiple-instance learning", "author": ["Q. Zhang", "W. Yu", "S.A. Goldman", "J.E. Fritts"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "On the relation between multi-instance learning and semi-supervised learning", "author": ["Z.-H. Zhou", "J.-M. Xu"], "venue": "In Proceeding of the 24th International Conference on Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Ensembles of multi-instance learners", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "In Proceedings of the 14th European Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "Neural networks for multi-instance learning", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "In Proceedings of the International Conference on Intelligent Information Technology,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Solving multi-instance problems with classifier ensemble based on constructive clustering", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Knowledge and Information Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "In multi-instance learning [14], the training set consists of many bags of instances.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 94, "endOffset": 101}, {"referenceID": 9, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 94, "endOffset": 101}, {"referenceID": 36, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 39, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 1, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 149, "endOffset": 156}, {"referenceID": 28, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 149, "endOffset": 156}, {"referenceID": 26, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 32, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 197, "endOffset": 205}, {"referenceID": 37, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 197, "endOffset": 205}, {"referenceID": 14, "context": "Multi-instance learning has been found useful in diverse domains such as image categorization [9, 10], image retrieval [37, 40], text categorization [2, 29], computer security [27], face detection [33, 38], computer-aided medical diagnosis [15], etc.", "startOffset": 240, "endOffset": 244}, {"referenceID": 40, "context": "However, previous studies on multi-instance learning typically treat the instances in the bags as independently and identically distributed [41], which neglects the fact that the relations among the instances convey important structure information.", "startOffset": 140, "endOffset": 144}, {"referenceID": 40, "context": "Zhou and Xu [41] showed that if the instances were treated as i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "samples, multi-instance learning is just a special case of semi-supervised learning [46] and thus the advantages of multi-instance representation could not be exploited well.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 31, "endOffset": 35}, {"referenceID": 33, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 96, "endOffset": 100}, {"referenceID": 26, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 147, "endOffset": 150}, {"referenceID": 42, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 185, "endOffset": 189}, {"referenceID": 38, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 242, "endOffset": 246}, {"referenceID": 41, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 280, "endOffset": 284}, {"referenceID": 35, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 297, "endOffset": 301}, {"referenceID": 3, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 318, "endOffset": 321}, {"referenceID": 24, "context": "To name a few, Diverse Density [22], k-nearest neighbor algorithm Citation-kNN and Bayesian-kNN [34], decision tree algorithms RELIC [27] and MITI [6], neural network algorithms BP-MIP [43] and RBF-MIP [39], rule learning algorithm RIPPER-MI [12], ensemble algorithms MI-Ensemble [42], MIBoosting [36] and MILBoosting [4], logistic regression algorithm MI-LR [25], etc.", "startOffset": 359, "endOffset": 363}, {"referenceID": 41, "context": "Most algorithms work by adapting single-instance supervised learning algorithms to the multi-instance representation through shifting the focuses of the algorithms from the discrimination on the instances to the discrimination on the bags [42].", "startOffset": 239, "endOffset": 243}, {"referenceID": 44, "context": "Recently there are also some proposal on adapting the multi-instance representation to single-instance algorithms by representation transformation [45].", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "[17] defined the MI-Kernel by regarding each multi-", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proposed mi-SVM and MI-SVM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Cheung and Kwok [11] argued that the sign instead of the value of the margin of the most positive instance is important.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Later, they [20] designed marginalized multi-instance kernels by considering that the contribution of different instances can be different.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "Chen and Wang [10] proposed the DD-SVM method which employs Diverse Density [22] to learn a set of instance prototypes and then maps the bags to a feature space based on the instance prototypes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Chen and Wang [10] proposed the DD-SVM method which employs Diverse Density [22] to learn a set of instance prototypes and then maps the bags to a feature space based on the instance prototypes.", "startOffset": 76, "endOffset": 80}, {"referenceID": 40, "context": "Zhou and Xu [41] proposed the MissSVM method by regarding instances of negative bags as labeled examples while those of positive bags as unlabeled examples with positive constraints.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 25, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 79, "endOffset": 86}, {"referenceID": 34, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 27, "context": "In addition to classification, multi-instance regression has also been studied [1, 26], and different versions of generalized multi-instance learning have been defined [35, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 34, "context": "The main difference between standard multi-instance learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, and a bag is positive if it has an instance satisfies this concept; while in generalized multi-instance learning [35, 28] there are multiple concepts, and a bag is positive only when all concepts are satisfied (i.", "startOffset": 285, "endOffset": 293}, {"referenceID": 27, "context": "The main difference between standard multi-instance learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, and a bag is positive if it has an instance satisfies this concept; while in generalized multi-instance learning [35, 28] there are multiple concepts, and a bag is positive only when all concepts are satisfied (i.", "startOffset": 285, "endOffset": 293}, {"referenceID": 23, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 98, "endOffset": 102}, {"referenceID": 43, "context": "Recently, research on multiinstance semi-supervised learning [24], multi-instance active learning [29] and multi-instance multi-label learning [44] have also been reported.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "In this paper we mainly work on standard multi-instance learning [14] and will show that our method is also applicable to multi-instance regression.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Inspired by [32] which shows that \u01eb-graph is helpful for discovering the underlying manifold structure of data, here we establish an \u01eb-graph for every bag.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "According to the manifold property [32], i.", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "If categorical attributes are involved, we use VDM (Value Difference Metric) [30] as a complement.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "For example, we can build a k-nearest neighbor classifier that employs graph edit distance [23], or we can design a graph kernel [16] to capture the similarity among graphs and then solve classification problems by kernel machines such as SVM.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "For example, we can build a k-nearest neighbor classifier that employs graph edit distance [23], or we can design a graph kernel [16] to capture the similarity among graphs and then solve classification problems by kernel machines such as SVM.", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "1 is exactly the MI-Kernel using Gaussian RBF kernel [17].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Clearly, the kgraph satisfies the four major properties that should be considered for a graph kernel definition [7]: the kernel should be a good measure of similarity for graphs; the computational complexity should be in polynomial time; the kernel must be positive definite; it should be somewhat general which will not limited in small subsets of graphs.", "startOffset": 112, "endOffset": 115}, {"referenceID": 13, "context": "Drug activity prediction is one of the earliest applications of multi-instance learning [14], where the Musk data is a real-world benchmark that has been studied by many researchers.", "startOffset": 88, "endOffset": 92}, {"referenceID": 4, "context": "Musk1 and Musk2, both publicly available at the UCI machine learning repository [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 16, "context": "We compare MIGraph with MI-Kernel [17] via 10 times 10-fold cross validation (i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In this experiment we use the COREL data set described in [10, 9].", "startOffset": 58, "endOffset": 65}, {"referenceID": 8, "context": "In this experiment we use the COREL data set described in [10, 9].", "startOffset": 58, "endOffset": 65}, {"referenceID": 16, "context": "1 Note that the performance of MI-Kernel in our implementation is better than that reported in [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 30, "context": "k\u2227 [31] 82.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "3 kemp non-transduction [31] 88.", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "2 mi-SVM [2] 87.", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "6 MI-SVM [2] 77.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "3 DD-SVM [10] 85.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "3 MissSVM [41] 87.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "0 MILES [9] 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "7 Diverse Density [22] 88.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "5 RELIC [27] 83.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "3 MITI [6] 83.", "startOffset": 7, "endOffset": 10}, {"referenceID": 33, "context": "2 Citation-kNN [34] 92.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "3 RIPPER-MI [12] 88.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "sbMIL [8] 91.", "startOffset": 6, "endOffset": 9}, {"referenceID": 35, "context": "7 MIBoosting [36] 87.", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "0 MILBoosting [4] 92.", "startOffset": 14, "endOffset": 17}, {"referenceID": 24, "context": "MI-LR [25] 86.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "0 MULTINST [3] 76.", "startOffset": 11, "endOffset": 14}, {"referenceID": 13, "context": "0 Iterated-discrim APR [14] 92.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "We used the processed data 2 such that all the bags and instances are as same as those used in [10, 9].", "startOffset": 95, "endOffset": 102}, {"referenceID": 8, "context": "We used the processed data 2 such that all the bags and instances are as same as those used in [10, 9].", "startOffset": 95, "endOffset": 102}, {"referenceID": 1, "context": "MI-SVM [2, 10] 74.", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "MI-SVM [2, 10] 74.", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "5 DD-SVM [10] 81.", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "4 MissSVM [41] 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "2 kmeans-SVM [13] 69.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "7 MILES [9] 82.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "We use the same experimental routine as that described in [9].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 8, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 40, "context": "ies [10, 9, 41], which owes to the fact that many images of these two categories contain semantically related and visually similar regions such as those corresponding to mountain, river, lake and ocean.", "startOffset": 4, "endOffset": 15}, {"referenceID": 0, "context": "[1] are used in this experiment.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Details of the data sets can be found in [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 21, "context": "DD [22, 1] 0.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "DD [22, 1] 0.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "Citation-kNN [1] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 42, "context": "0025 BP-MIP [43, 39] 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 38, "context": "0025 BP-MIP [43, 39] 0.", "startOffset": 12, "endOffset": 20}, {"referenceID": 38, "context": "0752 RBF-MIP [39] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "It has been shown in [18] that if two kernels contribute different information and the performance of the two kernels are equally good, the upper bound of generalization error of a combined kernel will be lower.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "It has also been shown in [21] that in practice, even when the kernels are not equally well, a combined kernel can be a better choice.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": ", all-paths kernel [7], for our purpose.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "It has been proved [7] that determining all paths is NP-hard.", "startOffset": 19, "endOffset": 22}, {"referenceID": 18, "context": "where the definitions of kp and kc are as same as the marginalized kernel [19], i.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "Multi-instance learning attempts to learn from a training set consisting of labeled bags each containing many unlabeled instances. Previous studies typically treat the instances in the bags as independently and identically distributed. However, the instances in a bag are rarely independent, and therefore a better performance can be expected if the instances are treated in an non-i.i.d. way that exploits the relations among instances. In this paper, we propose a simple yet effective multiinstance learning method, which regards each bag as a graph and uses a specific kernel to distinguish the graphs by considering the features of the nodes as well as the features of the edges that convey some relations among instances. The effectiveness of the proposed method is validated by experiments.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}