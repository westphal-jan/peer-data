{"id": "0909.4588", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2009", "title": "Discrete MDL Predicts in Total Variation", "abstract": "the data description length ( stat ) principle selects the model that has the shortest code source data plus model. we show that : a countable class of models, mdl predictions are close to the true distribution in a standardized condition. the result is completely general. no independence, equality, stationarity, identifiability, therefore other assumption on the model class need to be made. more abruptly, we show that for any countable class of models, the distributions selected by mdl ( or map ) asymptotically predict ( merge with ) the true measure in the class in total variation distance. implications for non - objective. f. d. applications like time - biased forecasting, discriminative learning, and reinforcement learning are neglected.", "histories": [["v1", "Fri, 25 Sep 2009 02:57:17 GMT  (18kb)", "http://arxiv.org/abs/0909.4588v1", "15 LaTeX pages"]], "COMMENTS": "15 LaTeX pages", "reviews": [], "SUBJECTS": "math.PR cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "authors": ["marcus hutter"], "accepted": true, "id": "0909.4588"}, "pdf": {"name": "0909.4588.pdf", "metadata": {"source": "CRF", "title": "Discrete MDL Predicts in Total Variation", "authors": ["Marcus Hutter"], "emails": ["RSISE@ANU", "SML@NICTA", "marcus@hutter1.net"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 9.\n45 88\nv1 [\nm at\nh. PR\n] 2\nContents"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Facts, Insights, Problems 4", "text": ""}, {"heading": "3 Notation and Main Result 6", "text": ""}, {"heading": "4 Proof for Finite Model Class 8", "text": ""}, {"heading": "5 Proof for Countable Model Class 10", "text": ""}, {"heading": "6 Implications 12", "text": "7 Variations 14 References 14\nKeywords\nminimum description length; countable model class; total variation distance; sequence prediction; discriminative learning; reinforcement learning."}, {"heading": "1 Introduction", "text": "The minimum description length (MDL) principle recommends to use, among competing models, the one that allows to compress the data+model most [Gru\u030807]. The better the compression, the more regularity has been detected, hence the better will\npredictions be. The MDL principle can be regarded as a formalization of Ockham\u2019s razor, which says to select the simplest model consistent with the data.\nMultistep lookahead sequential prediction. We consider sequential prediction problems, i.e. having observed sequence x \u2261 (x1,x2,...,x\u2113) \u2261 x1:\u2113, predict z \u2261 (x\u2113+1,...,x\u2113+h)\u2261 x\u2113+1:\u2113+h, then observe x\u2113+1 \u2208X for \u2113\u2261 \u2113(x) = 0,1,2,.... Classical prediction is concerned with h= 1, multi-step lookahead with 1<h<\u221e, and total prediction with h=\u221e. In this paper we consider the last, hardest case. An infamous problem in this category is the Black raven paradox [Mah04, Hut07]: Having observed \u2113 black ravens, what is the likelihood that all ravens are black. A more computer science problem is (infinite horizon) reinforcement learning, where predicting the infinite future is necessary for evaluating a policy. See Section 6 for these and other applications.\nDiscrete MDL and Bayes. Let M= {Q1,Q2,...} be a countable class of models=theories=hypotheses=probabilities over sequences X\u221e, sorted w.r.t. to their complexity=codelength K(Qi)=2log2i (say), containing the unknown true sampling distribution P . Our main result will be for arbitrary measurable spaces X , but to keep things simple in the introduction, let us illustrate MDL for finite X .\nIn this case, we define Qi(x) as the Qi-probability of data sequence x\u2208X \u2113. It is possible to code x in logP (x)\u22121 bits, e.g. by using Huffman coding. Since x is sampled from P , this code is optimal (shortest among all prefix codes). Since we do not know P , we could select the Q\u2208M that leads to the shortest code on the observed data x. In order to be able to reconstruct x from the code we need to know which Q has been chosen, so we also need to code Q, which takes K(Q) bits. Hence x can be coded in minQ\u2208M{\u2212logQ(x)+K(Q)} bits. MDL selects as model the minimizer\nMDLx := arg min Q\u2208M {\u2212 logQ(x) +K(Q)}\nGiven x, the true predictive probability of z is P (z|x)=P (xz)/P (x). Since P is unknown we use MDLx(z|x) :=MDLx(xz)/MDLx(x) as a substitute. Our main concern is how close is the latter to the former. We can measure the distance between two predictive distributions by\ndh(P,Q|x) = \u2211\nz\u2208Xh\n\u2223\u2223P (z|x)\u2212Q(z|x) \u2223\u2223 (1)\nfor h<\u221e and d\u221e=limh\u2192\u221edh=sup{d1,d2,...}. It is easy to see that dh is monotone increasing and that d\u221e is twice the total variation distance (tvd) defined in (3).\nMDL is closely related to Bayesian prediction, so a comparison to existing results for Bayes is interesting. Bayesians use Bayes(z|x) for prediction, where Bayes(x) :=\u2211 Q\u2208MQ(x)wQ is the Bayesian mixture with prior weights wQ > 0 \u2200Q \u2208 M and\u2211 Q\u2208MwQ=1. A natural choice is wQ\u221d2 \u2212K(Q).\nResults. The following results can be shown\n\u2211\u221e \u2113=0 E[dh(P,MDL\nx|x1:\u2113)] \u2264 21 h\u00b72 K(P ),\u2211\u221e\n\u2113=0E[dh(P,Bayes|x1:\u2113)] \u2264 h\u00b7lnw \u22121 P ,\nd\u221e(P,MDL x|x) \u2192 0 d\u221e(P,Bayes|x) \u2192 0\n{ almost surely\nfor \u2113(x)\u2192\u221e (2)\nwhere the expectation E is w.r.t. P [\u00b7|x]. The left statements for h < \u221e imply dh \u2192 0 almost surely, including some form of convergence rate. For Bayes it has been proven in [Hut03]; for MDL the proof in [PH05] can be adapted. As far as asymptotics is concerned, the right results d\u221e\u2192 0 are much stronger, and require more sophisticated proof techniques. For Bayes, the result follows from [BD62]. The proof for MDL is the primary novel contribution of this paper; more precisely for arbitrary measurable X in total variation distance. Another general consistency result is presented in [Gru\u030807, Thm.5.1]. Consistency is shown (only) in probability and the predictive implications of the result are unclear. A stronger almost sure result is alluded to, but the given reference to [BC91] contains only results for i.i.d. sequences which do not generalize to arbitrary classes. So existing results for discrete MDL are far less satisfactory than the elegant Bayesian prediction in tvd.\nMotivation. The results above hold for completely arbitrary countable model classes M. No independence, ergodicity, stationarity, identifiability, or other assumption need to be made.\nThe bulk of previous results for MDL are for continuous model classes [Gru\u030807]. Much has been shown for classes of independent identically distributed (i.i.d.) random variables [BC91, Gru\u030807]. Many results naturally generalize to stationaryergodic sequences like (kth-order) Markov. For instance, asymptotic consistency has been shown in [Bar85]. There are many applications violating these assumptions, some of them are presented below and in Section 6.\nOne can often hear the exaggerated claim that (e.g. unlike Bayes) MDL can be used even if the true distribution P is not in M. Indeed, it can be used, but the question is wether this is any good. There are some results supporting this claim, e.g. if P is in the closure of M, but similar results exist for Bayes. Essentially P needs to be at least close to some Q\u2208M for MDL to work, and there are interesting environments that are not even close to being stationary-ergodic or i.i.d.\nNon-i.i.d. data is pervasive [AHRU09]; it includes all time-series prediction problems like weather forecasting and stock market prediction [CBL06]. Indeed, these are also perfect examples of non-ergodic processes. Too much green house gases, a massive volcanic eruption, an asteroid impact, or another world war could change the climate/economy irreversibly. Life is also not ergodic; one inattentive second in a car can have irreversible consequences. Also stationarity is easily violated in multi-agent scenarios: An environment which itself contains a learning agent is nonstationary (during the relevant learning phase). Extensive games and multi-agent reinforcement learning are classical examples [WR04].\nOften it is assumed that the true distribution can be uniquely identified asymptotically. For non-ergodic environments, asymptotic distinguishability can depend\non the realized observations, which prevent a prior reduction or partitioning of M. Even if principally possible, it can be practically burdensome to do so, e.g. in the presence of approximate symmetries. Indeed this problem is the primary reason for considering predictive MDL. MDL might never identify the true distribution, but our main result shows that the sequentially selected models become predictively indistinguishable.\nThe countability of M is the severest restriction of our result. Nevertheless the countable case is useful. A semi-parametric problem class \u22c3\u221e d=1Md with Md={Q\u03b8,d :\u03b8\u2208IR d} (say) can be reduced to a countable class M={Pd} for which our result holds, where Pd is a Bayes or NML or other estimate of Md [Gru\u030807]. Alternatively, \u22c3 dMd could be reduced to a countable class by considering only computable parameters \u03b8. Essentially all interesting model classes contain such a countable topologically dense subset. Under certain circumstances MDL still works for the non-computable parameters [Gru\u030807]. Alternatively one may simply reject non-computable parameters on philosophical grounds [Hut05]. Finally, the techniques for the countable case might aid proving general results for continuous M, possibly along the lines of [Rya09].\nContents. The paper is organized as follows: In Section 2 we provide some insights how MDL and Bayes work in restricted settings, what breaks down for general countable M, and how to circumvent the problems. The formal development starts with Section 3, which introduces notation and our main result. The proof for finite M is presented in Section 4 and for denumerable M in Section 5. In Section 6 we show how the result can be applied to sequence prediction, classification and regression, discriminative learning, and reinforcement learning. Section 7 discusses some MDL variations."}, {"heading": "2 Facts, Insights, Problems", "text": "Before starting with the formal development, we describe how MDL and Bayes work in some restricted settings, what breaks down for general countable M, and how to circumvent the problems. For deterministic environments, MDL reduces to learning by elimination, and the four results in (2) can easily be understood. Consistency of MDL for i.i.d. (and stationary-ergodic) sources is also intelligible. For general M, MDL may no longer converge to the true model. We have to give up the idea of model identification, and concentrate on predictive performance.\nDeterministic MDL = elimination learning. For a countable class M = {Q1,Q2,...} of deterministic theories=models=hypotheses=sequences, sorted w.r.t. to their complexity=codelength K(Qi) = 2log2i (say) it is easy to see why MDL works: Each Q is a model for one infinite sequence xQ1:\u221e, i.e. Q(x\nQ)=1. Given the true observations x\u2261xP1:\u2113 so far, MDL selects the simplest Q consistent with x P 1:\u2113 and for h=1 predicts xQ\u2113+1. This (and potentially other) Q becomes (forever) inconsistent if and only if the prediction was wrong. Assume the true model is P =Qm. Since\nelimination occurs in order of increasing index i, and Qm never makes any error, MDL makes at most m\u22121 prediction errors. Indeed, what we have described is just classical Gold style learning by elimination. For 1<h<\u221e, the prediction xQ\u2113+1:\u2113+h may be wrong only on xQ\u2113+h, which causes h wrong predictions before the error is revealed. (Note that at time \u2113 only xP\u2113 is revealed.) Hence the total number of errors is bounded by h\u00b7(m\u22121). The bound is for instance attained on the class consisting of Qi=1\nih0\u221e, and the true sequence switches from 1 to 0 after having observed m\u00b7h ones. For h=\u221e, a wrong prediction gets eventually revealed. Hence each wrong Qi (i<m) gets eventually eliminated, i.e. P gets eventually selected. So for h=\u221e we can (still/only) show that the number of errors is finite. No bound on the number of errors in terms ofm only is possible. For instance, forM={Q1=1 \u221e,Q2=P=1 n0\u221e}, it takes n time steps to reveal that prediction 1\u221e is wrong, and n can be chosen arbitrarily large.\nDeterministic Bayes = majority learning. Bayesian learning is at the same time, closely related to and very different from MDL. Bayes predicts with a wQweighted average of the models (rather than with a single one). For a deterministic class, Bayes is similar to prediction by majority: Consider the models consistent with the true observation xP1:\u2113, having total weight W , and take the weighted majority prediction (this is the Bayes-optimal decision under 0-1 loss, Bayesian prediction would randomize). For h=1, making a wrong prediction means thatQ\u2019s contributing to at least half of the total weight W get eliminated. Since P =Qm never gets eliminated, we have wP \u2264W \u22642\n\u2212#Errors, hence the number of errors is bounded by log2w \u22121 P . For probabilistic Bayesian prediction proper, it is also easy to see that the expected number of errors is bounded by lnw\u22121P . One can show that these bounds are essentially sharp. (e.g. for Qi defined as the digits after the comma of the binary expansion of (i\u22121)/2n for i=1..m and m=2n\u22121.) With the same reasoning as in the MDL case, for h>1 we have to multiply the bound by h; and for h=\u221e we get correct prediction eventually, but no explicit bound anymore.\nComparison of deterministic\u2194probabilistic and MDL\u2194Bayes. The flavor of results carries over to some extent to the probabilistic case. On a very abstract level even the line of reasoning carries over, although this is deeply buried in the sophisticated mathematical analysis of the latter. So the special deterministic case illustrates the more complex probabilistic case. For instance for h=1 and wi\u221d1/i\n2, we see that \u201cBayes\u201d makes only 2log2m errors, while MDL can make up to the m errors. This carries over to the probabilistic case. Also the multiplier h for 1<h<\u221e and the lack of an explicit bound for h=\u221e carries over. Cf. the bounds in (2). The reader is invited to reveal other relations not explicitly mentioned here. The differences are as follows: In the probabilistic case, the true P can in general not be identified anymore. Further, while the Bayesian bound trivially follows from the 1/2-century old classical merging of opinions result [BD62], the corresponding MDL bound we prove in this paper is more difficult to obtain.\nConsistency of MDL for stationary-ergodic sources. For an i.i.d. class M,\nthe law of large numbers applied to the random variables Zt := log[P (xt)/Q(xt)] implies 1\n\u2113 \u2211\u2113 t=1Zt \u2192KL(P ||Q) := \u2211 x1 P (x1)log[P (x1)/Q(x1)] with P -probability 1.\nEither the Kullback-Leibler (KL) divergence is zero, which is the case if and only if P =Q, or logP (x1:\u2113)\u2212logQ(x1:\u2113)\u2261 \u2211\u2113 t=1Z\u2113\u223cKL(P ||Q)\u2113\u2192\u221e, i.e. asymptotically MDL does not select Q. For countable M, a refinement of this argument shows that MDL eventually selects P [BC91]. This reasoning can be extended to stationaryergodic M, but essentially not beyond. To see where the limitation comes from, we present some troubling examples.\nTrouble makers. For instance, let P be a Bernoulli(\u03b80) process, but let the Qprobability that xt=1 be \u03b8t, i.e. time-dependent (still assuming independence). For a suitably converging but \u201coscillating\u201d (i.e. infinitely often larger and smaller than its limit) sequence \u03b8t\u2192 \u03b80 one can show that log[P (x1:t)/Q(x1:t)] converges to but oscillates around K(Q)\u2212K(P ) w.p.1, i.e. there are non-stationary distributions for which MDL does not converge (not even to a wrong distribution).\nOne idea to solve this problem is to partition M, where two distributions are in the same partition if and only if they are asymptotically indistinguishable (like P and Q above), and then ask MDL to only identify a partition. This approach cannot succeed generally, whatever particular criterion is used, for the following reason: Let P (x1)>0 \u2200x1. For x1=1, let P and Q be asymptotically indistinguishable, e.g. P =Q on the remainder of the sequence. For x1=0, let P and Q be asymptotically distinguishable distributions, e.g. different Bernoullis. This shows that for nonergodic sources like this one, asymptotic distinguishability depends on the drawn sequence. The first observation can lead to totally different futures.\nPredictive MDL avoids trouble. The Bayesian posterior does not need to converge to a single (true or other) distribution, in order for prediction to work. We can do something similar for MDL. At each time we still select a single distribution, but give up the idea of identifying a single distribution asymptotically. We just measure predictive success, and accept infinite oscillations. That\u2019s the approach taken in this paper."}, {"heading": "3 Notation and Main Result", "text": "The formal development starts with this section. We need probability measures and filters for infinite sequences, conditional probabilities and densities, the total variation distance, and the concept of merging (of opinions), in order to formally state our main result.\nMeasures on sequences. Let (\u2126 :=X\u221e,F ,P ) be the space of infinite sequences with natural filtration and product \u03c3-field F and probability measure P . Let \u03c9\u2208\u2126 be an infinite sequence sampled from the true measure P . Except when mentioned otherwise, all probability statements and expectations refer to P , e.g. almost surely (a.s.) and with probability 1 (w.p.1) are short for with P -probability 1 (w.P .p.1). Let x=x1:\u2113=\u03c91:\u2113 be the first \u2113 symbols of \u03c9.\nFor countable X , the probability that an infinite sequence starts with x is P (x):= P [{x}\u00d7X\u221e]. The conditional distribution of an event A given x is P [A|x] :=P [A\u2229 ({x}\u00d7X\u221e)]/P (x), which exists w.p.1. For other probability measures Q on \u2126, we define Q(x) and Q[A|x] analogously. General X are considered at the end of this section.\nConvergence in total variation. P is said to be absolutely continuous relative to Q, written\nP \u226a Q :\u21d4 [Q[A] = 0 implies P [A] = 0 for all A \u2208 F ]\nP and Q are said to be mutually singular, written P\u22a5Q, iff there exists an A\u2208F for which P [A]=1 and Q[A]=0. The total variation distance (tvd) between Q and P given x is defined as\nd(P,Q|x) := sup A\u2208F\n\u2223\u2223Q[A|x]\u2212 P [A|x] \u2223\u2223 (3)\nQ is said to predict P in tvd (or merge with P ) if d(P,Q|x) \u2192 0 for \u2113(x) \u2192 \u221e with P -probability 1. Note that this in particular implies, but is stronger than one-step predictive on- and off-sequence convergence Q(x\u2113+1=a\u2113+1|x1:\u2113)\u2212P (x\u2113+1= a\u2113+1|x1:\u2113)\u21920 for any a, not necessarily equal \u03c9 [KL94]. The famous Blackwell and Dubins convergence result [BD62] states that if P is absolutely continuous relative to Q, then (and only then [KL94]) Q merges with P :\nIf P \u226a Q then d(P,Q|x) \u2192 0 w.p.1 for \u2113(x) \u2192 \u221e\nBayesian prediction. This result can immediately be utilized for Bayesian prediction. Let M :={Q1,Q2,Q3,...} be a countable (finite or infinite) class of probability measures, and Bayes[A] := \u2211 Q\u2208MQ[A]wQ with wQ > 0 \u2200Q and \u2211 Q\u2208MwQ = 1. If the model assumption P \u2208M holds, then obviously P\u226aBayes, hence Bayes merges with P , i.e. d(P,Bayes|x)\u2192 0 w.p.1 for all P \u2208M. Unlike many other Bayesian convergence and consistency theorems, no (independence, ergodicity, stationarity, identifiability, or other) assumption on the model class M need to be made. Good convergence rates for the weaker dh<\u221e distances have also been shown [Hut03]. The analogous result for MDL is as follows:\nTheorem 1 (MDL predictions) Let M be a countable class of probability measures on X\u221e containing the unknown true sampling distribution P . No (independence, ergodicity, stationarity, identifiability, or other) assumptions need to be made on M. Let\nMDLx := arg min Q\u2208M\n{\u2212 logQ(x) +K(Q)} with \u2211\nQ\u2208M\n2\u2212K(Q) < \u221e\nbe the measure selected by MDL at time \u2113 given x\u2208X \u2113. Then the predictive distributions MDLx[\u00b7|x] converge to P [\u00b7|x] in the sense that\nd(P,MDLx|x) \u2261 sup A\u2208F\n\u2223\u2223MDLx[A|x]\u2212 P [A|x] \u2223\u2223 \u2192 0 for \u2113(x) \u2192 \u221e w.p.1\nK(Q) is usually interpreted and defined as the length of some prefix code for Q, in which case \u2211 Q2 \u2212K(Q) \u2264 1. If K(Q) := log2w \u22121 Q is chosen as complexity, by Bayes rule Pr(Q|x) = Q(x)wQ/Bayes(x), the maximum a posteriori estimate MAPx :=argmaxQ\u2208M{Pr(Q|x)}\u2261MDL\nx. Hence the theorem also applies to MAP. The proof of the theorem is surprisingly subtle and complex compared to the analogous Bayesian case. One reason is that MDLx(x) is not a measure on X\u221e.\nArbitrary X . For arbitrary X , definitions are more subtle. The casual reader satisfied with countable X can skip this paragraph. We can consider even more generally xt\u2208Xt [BD62]. Let Bt be a \u03c3-field of subsets of Xt for t=1,2,3,.... Let F\u2113 be the \u03c3-field for X \u2113 :=X1\u00d7...\u00d7X\u2113 generated by (i.e. the smallest \u03c3-field containing) B1\u00d7...\u00d7B\u2113 for \u2113\u2264\u221e. Let (\u2126 :=X\n\u221e,F =F\u221e,P ) be a probability space. Let P\u2113 be the marginal distribution on (X \u2113,F\u2113), i.e. P\u2113[A] :=P [A\u00d7X\u2113+1\u00d7X\u2113+2\u00d7...] for A\u2208F\u2113. The predictive distribution P \u2113[A|x1:\u2113] is (a version of) the conditional distribution of the future \u201cx\u2113+1:\u221e\u201d given past x1:\u2113, implicitly defined by \u222b P \u2113[A|x1:\u2113]dP\u2113(x1:\u2113):=P [A] \u2200A\u2208F . Similarly define Q\u2113 and Q \u2113 for the other Q\u2208M. See [Doo53] for details.\nLet M be a measure on \u2126 such that Q is absolutely continuous (see below) relative to M for all Q\u2208M. For instance M [\u00b7] =Bayes[\u00b7] has this property. Now define the density (Radon-Nikodym derivative) Q\u2113(x1:\u2113) (round brackets) of measure Q\u2113[\u00b7] (square brackets) relative to M\u2113[\u00b7]. It is important to note that all essential quantities, in particular MDLx, are independent of the particular choice of M . We therefore plainly speak of the Q-density or even Q-probability of x.\nFor countable X and counting measure M , Q\u2113[A|x] and Q\u2113(x) coincide with Q[A|x] and Q(x) above. In the following, we drop the sup&superscripts \u2113, since they will always be clear from the argument. Note that by Carathodory\u2019s extension theorem, {Q(x) :x\u2208X \u2217} uniquely defines Q[A] \u2200A\u2208F ."}, {"heading": "4 Proof for Finite Model Class", "text": "We first prove Theorem 1 for finite model classes M. For this we need the following Definition and Lemma:\nDefinition 2 (Relations between Q and P ) For any probability measures Q and P , let\n\u2022 Qr+Qs=Q be the Lebesgue decomposition of Q relative to P into an absolutely continuous non-negative measure Qr\u226aP and a singular non-negative measure Qs\u22a5P . \u2022 g(\u03c9) := dQr/dP = lim\u2113\u2192\u221e[Q(x1:\u2113)/P (x1:\u2113)] be (a version of) the RadonNikodym derivative, i.e. Qr[A]= \u222b A g dP . \u2022 \u2126\u25e6 := {\u03c9 :Q(x1:\u2113)/P (x1:\u2113)\u21920} \u2261 {\u03c9 :g(\u03c9)=0}. \u2022 ~\u2126 := {\u03c9 :d(P,Q|x)\u21920 for \u2113(x)\u2192\u221e}.\nIt is well-known that the Lebesgue decomposition exists and is unique. The representation of the Radon-Nikodym derivative as a limit of local densities can e.g. be found in [Doo53, VII\u00a78]: Z\nr/s \u2113 (\u03c9) :=Q r/s(x1:\u2113)/P (x1:\u2113) for \u2113=1,2,3,... constitute two martingale sequences, which converge w.p.1. Qr\u226aP implies that the limit Zr\u221e is the Radon-Nikodym derivative dQr/dP . (Indeed, Doob\u2019s martingale convergence theorem can be used to prove the Radon-Nikodym theorem.) Qs\u22a5P implies Zr\u221e=0 w.p.1. So g is uniquely defined and finite w.p.1.\nLemma 3 (Generalized merging of opinions) For any Q and P , the following holds:\n(i) P\u226aQ if and only if P [\u2126\u25e6]=0 (ii) P [\u2126\u25e6]=0 implies P [~\u2126]=1 [(i)+[BD62]] (iii) P [\u2126\u25e6\u222a~\u2126]=1 [generalizes (ii)]\n(i) says that Q(x)/P (x) converges almost surely to a strictly positive value if and only if P is absolutely continuous relative to Q, (ii) says that an almost sure positive limit of Q(x)/P (x) implies that Q merges with P . (iii) says that even if P 6\u226aQ, we still have d(P,Q|x)\u2192 0 on almost every sequence that has a positive limit of Q(x)/P (x).\nProof. Recall Definition 2. (i\u21d0) Assume P [\u2126\u25e6]=0: P [A]>0 implies Q[A]\u2265Qr[A]= \u222b A g dP >0, since g>0 a.s. by assumption P [\u2126\u25e6]=0. Therefore P\u226aQ. (i\u21d2) Assume P \u226a Q: Choose a B for which P [B] = 1 and Qs[B] = 0. Now\nQr[\u2126\u25e6]= \u222b \u2126\u25e6 g dP =0 implies 0\u2264Q[B\u2229\u2126\u25e6]\u2264Qs[B]+Qr[\u2126\u25e6]= 0+0. By P \u226aQ this implies P [B\u2229\u2126\u25e6]=0, hence P [\u2126\u25e6]=0. (ii) That P \u226aQ implies P [~\u2126] = 1 is Blackwell-Dubins\u2019 celebrated result. The result now follows from (i). (iii) generalizes [BD62]. For P [\u2126\u25e6]= 0 it reduces to (ii). The case P [\u2126\u25e6]= 1 is trivial. Therefore we can assume 0<P [\u2126\u25e6]<1. Consider measure P \u2032[A] :=P [A|B] conditioned on B :=\u2126\\\u2126\u25e6.\nAssume Q[A] = 0. Using \u222b \u2126\u25e6 g dP = 0, we get 0 =Qr[A] = \u222b A g dP = \u222b A\\\u2126\u25e6 g dP . Since g > 0 outside \u2126\u25e6, this implies P [A\\\u2126\u25e6] = 0. So P \u2032[A] = P [A\u2229B]/P [B] = P [A\\\u2126\u25e6]/P [B]=0. Hence P \u2032\u226aQ. Now (ii) implies d(P \u2032,Q|x)\u21920 with P \u2032 probability 1. Since P \u2032\u226aP we also get d(P \u2032,P |x)\u21920 w.P \u2032.p.1.\nTogether this implies 0 \u2264 d(P,Q|x) \u2264 d(P \u2032,P |x)+d(P \u2032,Q|x) \u2192 0 w.P \u2032.p.1, i.e. P \u2032[~\u2126]=1. The claim now follows from\nP [\u2126\u25e6 \u222a ~\u2126] = P \u2032[\u2126\u25e6 \u222a ~\u2126]P [\u2126 \\ \u2126\u25e6] + P [\u2126\u25e6 \u222a ~\u2126|\u2126\u25e6]P [\u2126\u25e6]\n= 1 \u00b7 P [\u2126 \\ \u2126\u25e6] + 1 \u00b7 P [\u2126\u25e6] = P [\u2126] = 1\nThe intuition behind the proof of Theorem 1 is as follows. MDL will asymptotically not select Q for which Q(x)/P (x)\u21920. Hence for those Q potentially selected by MDL, we have \u03c9 6\u2208\u2126\u25e6, hence \u03c9\u2208~\u2126, for which d(P,Q|x)\u21920 (a.s.). The technical\ndifficulties are for finite M that the eligible Q depend on the sequence \u03c9, and for infinite M to deal with non-uniformly converging d, i.e. to infer d(P,MDLx|x)\u21920.\nProof of Theorem 1 for finite M. Recall Definition 2, and let gQ,\u2126 \u25e6 Q,~\u2126Q refer to some Q\u2208M\u2261{Q1,...,Qm}. The set of sequences \u03c9 for which some gQ for some Q\u2208M is undefined has P -measure zero, and hence can be ignored. Fix some sequence \u03c9\u2208\u2126 for which gQ(\u03c9) is defined for all Q\u2208M, and let M\u03c9 :={Q\u2208M :gQ(\u03c9)=0}.\nMDLx := arg min Q\u2208M LQ(x), where LQ(x) := \u2212 logQ(x) +K(Q).\nConsider the difference\nLQ(x)\u2212 LP (x) = \u2212 log Q(x)\nP (x) +K(Q)\u2212K(P )\n\u2113\u2192\u221e \u2212\u2192 \u2212 log gQ(\u03c9) +K(Q)\u2212K(P )\nFor Q\u2208M\u03c9, the r.h.s. is +\u221e, hence\n\u2200Q\u2208M\u03c9 \u2203\u2113Q\u2200\u2113>\u2113Q : LQ(x) > LP (x)\nSince M is finite, this implies\n\u2200\u2113>\u21130 \u2200Q\u2208M\u03c9 : LQ(x) > LP (x), where \u21130 := max{\u2113Q : Q \u2208 M\u03c9} < \u221e\nTherefore, since P \u2208M, we have MDLx 6\u2208M\u03c9 \u2200\u2113 > \u21130, so we can safely ignore all Q \u2208M\u03c9 and focus on Q \u2208 M\u03c9 := M\\M\u03c9. Let \u21261 := \u22c2 Q\u2208M\u03c9\n(\u2126\u25e6Q\u222a~\u2126Q). Since P [\u21261]=1 by Lemma 3(iii), we can also assume \u03c9\u2208\u21261.\nQ \u2208 M\u03c9 \u21d2 gQ(\u03c9) > 0 \u21d2 \u03c9 6\u2208 \u2126 \u25e6 Q \u21d2 \u03c9 \u2208 ~\u2126Q \u21d2 d(P,Q|x) \u2192 0\nThis implies d(P,MDLx|x) \u2264 sup Q\u2208M\u03c9 d(P,Q|x) \u2192 0\nwhere the inequality holds for \u2113> \u21130 and the limit holds, since M is finite. Since the set of \u03c9 excluded in our considerations has measure zero, d(P,MDLx|x)\u2192 0 w.p.1, which proves the theorem for finite M."}, {"heading": "5 Proof for Countable Model Class", "text": "The proof in the previous Section crucially exploited finiteness of M. We want to prove that the probability that MDL asymptotically selects \u201ccomplex\u201d Q is small. The following Lemma establishes that the probability that MDL selects a specific complex Q infinitely often is small.\nLemma 4 (MDL avoids complex probability measures Q) For any Q and P we have P [Q(x)/P (x)\u2265c infinitly often]\u22641/c.\nProof. P [\u2200\u21130\u2203\u2113>\u21130 : Q(x)\nP (x) \u2265 c]\n(a) = P [ lim\n\u2113\u2192\u221e\nQ(x) P (x) \u2265 c] \u2264\n(b) \u2264 1\nc E[lim \u2113\nQ(x) P (x) ] (c) = 1 c E[lim\n\u2113\nQ(x) P (x) ]\n(d) \u2264 1\nc lim \u2113 E[\nQ(x) P (x) ] (e) \u2261 1 c\n(a) is true by definition of the limit superior lim, (b) is Markov\u2019s inequality, (c) exploits the fact that the limit of Q(x)/P (x) exists w.p.1, (d) uses Fatou\u2019s lemma, and (e) is obvious.\nFor sufficiently complex Q, Lemma 4 implies that LQ(x)>LP (x) for most x. Since convergence is non-uniform in Q, we cannot apply the Lemma to all (infinitely many) complex Q directly, but need to lump them into one Q\u0304.\nProof of Theorem 1 for countable M. Let the Q\u2208M={Q1,Q2,...} be ordered somehow, e.g. in increasing order of complexity K(Q), and P =Qn. Choose some (large) m\u2265n and let M\u0303 := {Qm+1,Qm+2,...} be the set of \u201ccomplex\u201d Q. We show that the probability that MDL selects infinitely often complex Q is small:\nP [MDLx \u2208 M\u0303 infinitely often]\n\u2261 P [\u2200\u21130\u2203\u2113>\u21130 : MDL x \u2208 M\u0303]\n\u2264 P [\u2200\u21130\u2203\u2113>\u21130 \u2227Q \u2208 M\u0303 : LQ(x) \u2264 LP (x)]\n= P [\u2200\u21130\u2203\u2113>\u21130 : sup i>m\nQi(x) P (x) 2K(P )\u2212K(Qi) \u2265 1] (a) \u2264 P [\u2200\u21130\u2203\u2113>\u21130 : Q\u0304(x) P (x) \u03b4 2K(P ) \u2265 1] (b) \u2264 \u03b4 2K(P ) (c) \u2264 \u03b5\nThe first three relations follow immediately from the definition of the various quantities. Bound (a) is the crucial \u201clumping\u201d step. First we bound\nsup i>m\nQi(x) P (x) 2\u2212K(Qi) \u2264\n\u221e\u2211\ni=m+1\nQi(x) P (x) 2\u2212K(Qi) = \u03b4 Q\u0304(x) P (x) ,\n\u03b4 := \u2211\ni>m\n2\u2212K(Qi) < \u221e, Q\u0304(x) := 1\n\u03b4\n\u2211\ni>m\nQi(x)2 \u2212K(Qi),\nWhile MDL\u00b7[\u00b7] is not a (single) measure on \u2126 and hence difficult to deal with, Q\u0304 is a proper probability measure on \u2126. In a sense, this step reduces MDL to Bayes. Now we apply Lemma 4 in (b) to the (single) measure Q\u0304. The bound (c) holds for sufficiently large m=m\u03b5(P ), since \u03b4\u2192 0 for m\u2192\u221e. This shows that for the sequence of MDL estimates\n{MDLx1:\u2113 :\u2113 > \u21130} \u2286 {Q1, ..., Qm} with probability at least 1\u2212 \u03b5\nHence the already proven Theorem 1 for finite M implies that d(P,MDLx|x)\u2192 0 with probability at least 1\u2212\u03b5. Since convergence holds for every \u03b5 > 0, it holds w.p.1."}, {"heading": "6 Implications", "text": "Due to its generality, Theorem 1 can be applied to many problem classes. We illustrate some immediate implications of Theorem 1 for time-series forecasting, classification, regression, discriminative learning, and reinforcement learning.\nTime-series forecasting. Classical online sequence prediction is concerned with predicting x\u2113+1 from (non-i.i.d.) sequence x1:\u2113 for \u2113=1,2,3,.... Forecasting farther into the future is possible by predicting x\u2113+1:\u2113+h for some h>0. One can show that 0\u2264 d1 \u2264 dh \u2264 dh+1 \u2264 d\u221e = 2d\u2264 2, see (1) and (3). Hence Theorem 1 implies good asymptotic (multi-step) predictions. Offline learning is concerned with training a predictor on x1:\u2113 for fixed \u2113 in-house, and then selling and using the predictor on x\u2113+1:\u221e without further learning. Theorem 1 shows that for enough training data, predictions \u201cpost-learning\u201d will be good.\nClassification and Regression. In classification (discrete X ) and regression (continuous X ), a sample is a set of pairs D = {(y1,x1),...,(y\u2113,x\u2113)}, and a functional relationship x\u0307= f(y\u0307)+noise, i.e. a conditional probability P (x\u0307|y\u0307) shall be learned. For reasons apparent below, we have swapped the usual role of x\u0307 and y\u0307. The dots indicate x\u0307\u2208X and y\u0307\u2208Y), while x=x1:\u2113 \u2208X \u2113 and y= y1:\u2113\u2208Y \u2113. If we assume that also y\u0307 follows some distribution, and start with a countable model class M of joint distributions Q(x\u0307,y\u0307) which contains the true joint distribution P (x\u0307,y\u0307), our main result implies that MDLD[(x\u0307,y\u0307)|D] converges to the true distribution P (x\u0307,y\u0307). Indeed since/if samples are assumed i.i.d., we don\u2019t need to invoke our general result.\nDiscriminative learning. Instead of learning a generative [Jeb03] joint distribution P (x\u0307,y\u0307), which requires model assumptions on the input y\u0307, we can discriminatively [LSS07] learn P (\u00b7|y\u0307) directly without any assumption on y (not even i.i.d). We can simply treat y1:\u221e as an oracle to all Q, define M\n\u2032 = {Q\u2032} with Q\u2032(x) := Q(x|y1:\u221e), and apply our main result to M\n\u2032, leading to MDL\u2032x[A|x]\u2192P \u2032[A|x], i.e. MDLx|y1:\u221e[A|x,y1:\u221e]\u2192P [A|x,y1:\u221e]. This not yet useful since y1:\u221e is never known completely. If x1,x2,... are conditionally independent, we can write\nQ(x|y) =\n\u2113\u220f\nt=1\nQ(xt|yt) = \u2211\nx\u2113+1:m\nm\u220f\nt=1\nQ(xt|yt) = \u2211\nx\u2113+1:m\nQ(x1:m|y1:m) = Q(x1:\u2113|y1:m)\nTaking the limit m\u2192\u221e we get Q(x|y) =Q(x|y1:\u221e). This is a generic property satisfied for all causal processes, that a future yt for t > l does not influence past observations x1:\u2113. Hence for a class of conditionally independent distributions, we get MDLx|y[A|x,y]\u2192P [A|x,y]. Since the x given y are not identically distributed, classical MDL consistency results for i.i.d. or stationary-ergodic sources do not apply. The following corollary formalizes our findings:\nCorollary 5 (Discriminative MDL) Let M \u220b P be a class of discriminative causal distributions Q[\u00b7|y1:\u221e], i.e. Q(x|y1:\u221e) =Q(x|y), where x= x1:\u2113 and y = y1:\u2113.\nRegression and classification are typical examples. Further assume M is countable. Let MDLx|y := argminQ\u2208M{\u2212logQ(x|y)+K(Q)} be the discriminative MDL measure (at time \u2113 given x,y). Then supA \u2223\u2223MDLx|y[A|x,y]\u2212P [A|x,y] \u2223\u2223\u21920 for \u2113(x)\u2192\u221e, P [\u00b7|y1:\u221e] almost surely, for every sequence y1:\u221e.\nFor finite Y and conditionally independent x, the intuitive reason how this can work is as follows: If y\u0307 appears in y1:\u221e only finitely often, it plays asymptotically no role; if it appears infinitely often, then P (\u00b7|y\u0307) can be learned. For infinite Y and deterministic M, the result is also intelligible: Every y\u0307 might appear only once, but probing enough function values xt=f(yt) allows to identify the function.\nReinforcement learning (RL). In the agent framework [RN03], an agent interacts with an environment in cycles. At time t, an agent chooses an action yt based on past experience x<t\u2261(x1,...,xt\u22121) and past actions y<t with probability \u03c0(yt|x<ty<t) (say). This leads to a new perception xt with probability \u00b5(xt|x<ty1:t) (say). Then cycle t+1 starts. Let P (xy)= \u220f\u2113 t=1\u00b5(xt|x<ty1:t)\u03c0(yt|x<ty<t) be the joint interaction probability. We make no (Markov, stationarity, ergodicity) assumption on \u00b5 and \u03c0. They may be POMDPs or beyond.\nCorollary 6 (Single-agent MDL) For a fixed policy=agent \u03c0, and a class of environments {\u03bd1,\u03bd2,...} \u220b \u00b5, let M= {Qi} with Qi(x|y) = \u220f\u2113 t=1\u03bdi(xt|x<ty1:t). Then d(P [\u00b7|y],MDLx|y)\u21920 with joint P -probability 1.\nThe corollary follows immediately from the previous corollary and the facts that the Qi are causal and that with P [\u00b7|y1:\u221e]-probability 1 \u2200y1:\u221e implies w.P .p.1 jointly in x and y.\nIn reinforcement learning [SB98], the perception xt := (ot,rt) consists of some regular observation ot and a reward rt\u2208[0,1]. Goal is to find a policy which maximizes accrued reward in the long run. The previous corollary implies\nCorollary 7 (Fixed-policy MDL value function convergence) Let VP [xy] := EP [\u00b7|xy][r\u2113+1+\u03b3r\u2113+2+\u03b3\n2r\u2113+3+...] be the future \u03b3-discounted P -expected reward sum (true value of \u03c0), and similarly VQi[xy] for Qi. Then the MDL value converges to the true value, i.e. V\nMDL x|y [xy]\u2212VP [xy]\u21920, w.P .p.1. for any policy \u03c0.\nProof. The corollary follows from the general inequality\n\u2223\u2223EP [f ]\u2212EQ[f ] \u2223\u2223 \u2264 sup |f | \u00b7 sup\nA\n\u2223\u2223P [A]\u2212Q[A] \u2223\u2223\nby inserting f :=r\u2113+1+\u03b3r\u2113+2+\u03b3 2r\u2113+3+... and P =P [\u00b7|xy] and Q=MDL x|y[\u00b7|xy], and using 0\u2264f\u22641/(1\u2212\u03b3) and Corollary 6.\nSince the value function probes the infinite future, we really made use of our convergence result in total variation. Corollary 7 shows that MDL approximates\nthe true value asymptotically arbitrarily well. The result is weaker than it may appear. Following the policy that maximizes the estimated (MDL) value is often not a good idea, since the policy does not explore properly [Hut05]. Nevertheless, it is a reassuring non-trivial result."}, {"heading": "7 Variations", "text": "MDL is more a general principle for model selection than a uniquely defined procedure. For instance, there are crude and refined MDL [Gru\u030807], the related MML principle [Wal05], a static, a dynamic, and a hybrid way of using MDL for prediction [PH05], and other variations. For our setup, we could have defined multi-step lookahead prediction as a product of single-step predictions:\nMDLI(x1:\u2113) := \u2113\u220f\nt=1\nMDLx<t(xt|x<t), MDLI(z|x) = MDLI(xz)/MDLI(x)\nwhich is a more incremental MDL version. Both, MDLx and MDLI are \u2018static\u2019 in the sense of [PH05], and each allows for a dynamic and a hybrid version. Due to its incremental nature, MDLI likely has better predictive properties than MDLx, and conveniently defines a single measure over X\u221e, but inconveniently is 6\u2208M. One reason for using MDL is that it can be computationally simpler than Bayes. E.g. if M is a class of MDPs, then MDLx is still an MDP and hence tractable, but MDLI like Bayes are a nightmare to deal with.\nAcknowledgements. My thanks go to Peter Sunehag for useful discussions."}], "references": [{"title": "Learning from non-IID data: Theory, Algorithms and Practice (LNIDD\u201909)", "author": ["M.-R. Amini", "A. Habrard", "L. Ralaivola", "N. Usunier", "editors"], "venue": "Bled, Slovenia,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Logically Smooth Density Estimation", "author": ["A.R. Barron"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Barron.,? \\Q1985\\E", "shortCiteRegEx": "Barron.", "year": 1985}, {"title": "Minimum complexity density estimation", "author": ["A.R. Barron", "T.M. Cover"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Barron and Cover.,? \\Q1991\\E", "shortCiteRegEx": "Barron and Cover.", "year": 1991}, {"title": "Merging of opinions with increasing information", "author": ["D. Blackwell", "L. Dubins"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Blackwell and Dubins.,? \\Q1962\\E", "shortCiteRegEx": "Blackwell and Dubins.", "year": 1962}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "The Minimum Description Length Principle", "author": ["P.D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2007}, {"title": "Convergence and loss bounds for Bayesian sequence prediction", "author": ["M. Hutter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hutter.,? \\Q2003\\E", "shortCiteRegEx": "Hutter.", "year": 2003}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Hutter.", "year": 2005}, {"title": "On universal prediction and Bayesian confirmation", "author": ["M. Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Machine Learning: Discriminative and Generative", "author": ["T. Jebara"], "venue": null, "citeRegEx": "Jebara.,? \\Q2003\\E", "shortCiteRegEx": "Jebara.", "year": 2003}, {"title": "Weak and strong merging of opinions", "author": ["E. Kalai", "E. Lehrer"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "Kalai and Lehrer.,? \\Q1994\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1994}, {"title": "Discriminative learning can succeed where generative learning fails", "author": ["P. Long", "R. Servedio", "H.U. Simon"], "venue": "Information Processing Letters,", "citeRegEx": "Long et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Long et al\\.", "year": 2007}, {"title": "Probability captures the logic of scientific confirmation", "author": ["P. Maher"], "venue": "Contemporary Debates in Philosophy of Science,", "citeRegEx": "Maher.,? \\Q2004\\E", "shortCiteRegEx": "Maher.", "year": 2004}, {"title": "Asymptotics of discrete MDL for online prediction", "author": ["J. Poland", "M. Hutter"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Poland and Hutter.,? \\Q2005\\E", "shortCiteRegEx": "Poland and Hutter.", "year": 2005}, {"title": "Artificial Intelligence. A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2003\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2003}, {"title": "Characterizing predictable classes of processes", "author": ["D. Ryabko"], "venue": "In Proc. 25th Conference on Uncertainty in Artificial Intelligence (UAI\u201909),", "citeRegEx": "Ryabko.,? \\Q2009\\E", "shortCiteRegEx": "Ryabko.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Statistical and Inductive Inference by Minimum Message Length", "author": ["C.S. Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2005\\E", "shortCiteRegEx": "Wallace.", "year": 2005}, {"title": "Best-response multiagent learning in nonstationary environments", "author": ["M. Weinberg", "J.S. Rosenschein"], "venue": "In Proc. 3rd International Joint Conf. on Autonomous Agents & Multi Agent Systems", "citeRegEx": "Weinberg and Rosenschein.,? \\Q2004\\E", "shortCiteRegEx": "Weinberg and Rosenschein.", "year": 2004}], "referenceMentions": [], "year": 2013, "abstractText": "The Minimum Description Length (MDL) principle selects the model that has the shortest code for data plus model. We show that for a countable class of models, MDL predictions are close to the true distribution in a strong sense. The result is completely general. No independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made. More formally, we show that for any countable class of models, the distributions selected by MDL (or MAP) asymptotically predict (merge with) the true measure in the class in total variation distance. Implications for non-i.i.d. domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.", "creator": "LaTeX with hyperref package"}}}