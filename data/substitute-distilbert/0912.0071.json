{"id": "0912.0071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2009", "title": "Differentially Private Empirical Risk Minimization", "abstract": "this paper addresses the points at practical privacy - preserving machine learning : how to remove patterns in massive, real - world databases of sensitive personal sites, while maintaining the privacy of individuals. chaudhuri and monteleoni ( 2008 ) recently provided privacy - preserving techniques for learning linear separators via regularized logistic regression. explaining the goal of handling generic databases domains may not be linearly bound, we approach privacy - preserving support vector machine algorithms.", "histories": [["v1", "Tue, 1 Dec 2009 04:35:44 GMT  (47kb)", "http://arxiv.org/abs/0912.0071v1", "Submitted to AISTATS 2010"], ["v2", "Mon, 22 Feb 2010 19:33:44 GMT  (41kb)", "http://arxiv.org/abs/0912.0071v2", "Under review"], ["v3", "Tue, 23 Feb 2010 05:26:22 GMT  (75kb)", "http://arxiv.org/abs/0912.0071v3", "Under review"], ["v4", "Wed, 2 Jun 2010 23:16:36 GMT  (217kb)", "http://arxiv.org/abs/0912.0071v4", "40 pages, 7 figures, under review"], ["v5", "Wed, 16 Feb 2011 22:35:55 GMT  (218kb)", "http://arxiv.org/abs/0912.0071v5", "40 pages, 7 figures, accepted to the Journal of Machine Learning Research"]], "COMMENTS": "Submitted to AISTATS 2010", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR cs.DB", "authors": ["kamalika chaudhuri", "claire monteleoni", "anand d sarwate"], "accepted": false, "id": "0912.0071"}, "pdf": {"name": "0912.0071.pdf", "metadata": {"source": "CRF", "title": "Differentially Private Support Vector Machines", "authors": ["Anand Sarwate"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :0\n91 2.\n00 71\nv1 [\ncs .L\nG ]\nThis paper addresses the problem of practical privacy-preserving machine learning: how to detect patterns in massive, real-world databases of sensitive personal information, while maintaining the privacy of individuals. Chaudhuri and Monteleoni (2008) recently provided privacy-preserving techniques for learning linear separators via regularized logistic regression. With the goal of handling large databases that may not be linearly separable, we provide privacy-preserving support vector machine algorithms.\nWe address general challenges left open by past work, such as how to release a kernel classifier without releasing any of the training data, and how to tune algorithm parameters in a privacy-preserving manner. We provide general, efficient algorithms for linear and nonlinear kernel SVMs, which guarantee \u01eb-differential privacy, a very strong privacy definition due to Dwork et al. (2006). We also provide learning generalization guarantees. Empirical evaluations reveal promising performance on real and simulated data sets."}, {"heading": "1 Introduction", "text": "An emerging problem at the interface of statistics and machine learning, is how to learn from statistical databases that contain sensitive information. Statisticians for health insurance or credit-card companies, and epidemiologists studying disease risk, are faced with the task of performing data analysis on medical or financial records of private individuals. Machine learning algorithms applied to the data of many individuals can discover novel population-wide patterns but the results of such algorithms may reveal certain individuals\u2019 sensitive information, thereby violating their privacy. This is a growing concern, due to the massive increase in personal information stored in electronic databases. Our goal is to provide learning algorithms whose outputs are provably guaranteed to preserve privacy.\nThe support vector machine (SVM) algorithm is one of the most widely used machine learning algorithms in practice. In this paper we develop privacy-preserving\n1\nSVM algorithms for linear and nonlinear kernels. We also develop a method for privacy-preserving parameter-tuning for general machine learning algorithms.\nOur work is inspired by the approach of Chaudhuri and Monteleoni [Chaudhuri and Monteleoni, 2008]. They provide algorithms for regularized logistic regression that are privacy-preserving with respect to the \u01eb-differential privacy model [Dwork et al., 2006], a strong, cryptographically-motivated definition of privacy that has recently received a significant amount of research attention for its robustness to known attacks. The privacy-preserving technique proposed by [Chaudhuri and Monteleoni, 2008], for machine learning algorithms involving convex optimization, is to solve a perturbed optimization problem. For the case of regularized logistic regression, they provide a differentially private algorithm, with learning performance guarantees. However, their analysis does not apply directly to the more widely used support vector machine algorithm. In this paper we take a step towards more practical solutions for privacy-preserving machine learning by studying SVMs. Our main contributions are the following:1\n\u2022 We provide the first differentially private SVM algorithm. This involves applying ideas from [Chaudhuri and Monteleoni, 2008] to develop a privacy-preserving linear SVM algorithm, and also entails the following challenges.\n\u2022 We pose the general question: is it possible to learn a private version of a kernel classifier? That is, for kernel methods such as SVMs with nonlinear kernel functions, the optimal classifier is a linear combination of kernel functions centered at the training points. This form is inherently non-private because it reveals the training data. We answer this question in the affirmative for SVMs by adapting a random projection method due to Rahimi and Recht [Rahimi and Recht, 2007, Rahimi and Recht, 2008b] to develop privacy-preserving kernel-SVM algorithms, and we give generalization guarantees.\n\u2022 We provide a privacy-preserving tuning algorithm that is applicable to general machine learning algorithms. Our technique makes use of a randomized selection procedure [McSherry and Talwar, 2007], and is the first privacy-preserving tuning algorithm.2\n\u2022 Finally we provide experimental results on real and synthetic data that illustrate the tradeoffs between privacy, generalization error, and training size.\nIt is important to note that imposing privacy constraints on a machine learning algorithm generally comes with a cost for learning performance. This is the case in earlier privacy methods, such as random-sampling in which only a fraction of the data set is used, and k-anonymity [Sweeney, 2002], in which the data set is clustered, and only the cluster centers are released, so that only a 1/k fraction of the data set is used. In general, for classification, the error rate increases as the privacy requirements are made more stringent. Our learning performance guarantees formalize this \u201cprice of\n1Full proofs are omitted from the paper due to space constraints, and provided in the supplementary materials.\n2To our knowledge.\n2\nprivacy,\u201d and we demonstrate empirically that our methods better manage the tradeoff between privacy and learning performance than in previous work."}, {"heading": "1.1 Related work", "text": "There has been a significant amount of literature on privacy-preserving data-mining [Agrawal and Srikant, 2000, Evfimievski et al., 2003, Sweeney, 2002, Machanavajjhala et al., 2006], which work with privacy models other than differential privacy. However, many of the models used in these works have been shown to be susceptible to attacks when the adversary has some reasonable amount of prior knowledge [Ganta et al., 2008]. In particular, [Mangasarian et al., 2008] consider the problem of privacy-preserving SVM classification when separate agents have to share private data, and provide a solution that uses random kernels. However, their work does not provide any formal privacy guarantee.\nDifferential privacy, the formal privacy definition used in our paper, was proposed by the seminal work of Dwork et. al [Dwork et al., 2006], and has been used since in numerous works on privacy \u2013 see, for example, [McSherry and Talwar, 2007, Nissim et al., 2007, Barak et al., 2007, Chaudhuri and Monteleoni, 2008, Machanavajjhala et al., 2008]. Unlike many other privacy definitions, such as those mentioned above, differential privacy has been shown to be resistant to side-information attacks [Ganta et al., 2008]. Literature on learning with differential privacy include the work of [Kasiviswanathan et al., 2008, Blum et al., 2008], which present a general method for PAC-learning a concept class, and [Dwork and Lei, 2009], which present an algorithm for privacy-preserving regression using ideas from robust statistics. However, these algorithms have running time exponential in the VC-dimension of the concept class and data dimension, respectively.\nIn the machine learning literature, the work most related to ours is that of [Chaudhuri and Monteleoni, 2008]. First they show that one can extend the sensitivity method, a technique due to [Dwork et al., 2006], to classification algorithms. The sensitivity method makes an algorithm differentially private by adding noise to its output. The noise protects the privacy of the training data, but increases the prediction error. Chaudhuri and Monteleoni then show that by adding noise to the objective function of regularized logistic regression, the same level of privacy can be guaranteed with lower error than the sensitivity method. Their method also works for other strongly-convex optimization problems as long as the objective function is differentiable, and the effect of one person\u2019s private value on the gradient is low. Their method does not directly apply to SVMs.\nAnother line of work [Zhan and Matwin, 2007,Laur et al., 2006] considers computing privacy-preserving SVMs in the Secure Multiparty Communication setting \u2013 when the sensitive data is split across multiple hostile databases, and the goal is to design a distributed protocol to learn a classifier. In contrast, our work deals with a setting where the algorithm has access to the entire dataset.\n3"}, {"heading": "2 Problem statement", "text": "In this paper we develop privacy-preserving algorithms that learn a classifier from labeled examples. Our algorithms takes as input a database D = {(x(i), y(i)) : i = 1, 2, . . . , n} of n data-label pairs. The samples are x(i) \u2208 X \u2282 Rd and the labels are y(i) \u2208 {\u22121,+1}. We will use \u2016x\u20162, \u2016x\u2016\u221e, and \u2016x\u2016H to denote the \u21132-norm, \u2113\u221e-norm, and norm in a Hilbert space H, respectively. We will assume throughout that X is the unit ball so that \u2016x(i)\u20162 \u2264 1. For an integer K we will use [K] to denote the set {1, 2, . . . ,K}. In this paper we are interested in support vector machines (SVMs), which are a class of algorithms that construct a classifier f : X \u2192 R which makes a prediction sgn(f(x)) on a point x. The standard SVM algorithm computes the classifier that minimizes the regularized empirical loss:\nJ(f) = 1\nn\nn \u2211\ni=1\n\u2113(f(x(i)), y(i)) + \u039b\n2 \u2016f\u20162H , (1)\nwhere \u2113(\u00b7, \u00b7) is a loss function and \u2016\u00b7\u2016H is the norm of f in the reproducing kernel Hilbert space (RKHS) H generated by a kernel function k(a, b). The loss function \u2113(t, y) could technically be anything, but for support vector machines is usually set to the hinge loss. In this paper we will use the Huber loss, which is an approximation to the hinge loss [Chapelle, 2007] that depends on a small parameter h:\n\u2113h(t, y) =\n\n\n 0 if yt > 1 + h 1 4h (1 + h\u2212 yt)2 if |1\u2212 yt| \u2264 h 1\u2212 yt if yt < 1\u2212 h\n(2)\nAs h \u2192 0 this loss function approaches the hinge loss. The reason for choosing the Huber loss is that it is differentiable, which is important for the proofs below (see the remark after Theorem 1).\nPrivacy Model. We are interested in producing a classifier in a manner that preserves the privacy of individual entries of the database D that is used in the training. The notion of privacy we use is the \u01eb-differential privacy model, developed by Dwork et al. [Dwork et al., 2006, Dwork, 2006]. In this model, the training algorithm is randomized. Let M denote the training algorithm and let M(D) be the random variable that is the classifier and let \u00b5 denote the density of M. The algorithm M for producing a classifier is said to preserve \u01eb-differential privacy if the likelihood that M produces a classifier f on database D is close to the likelihood of it producing f on any database D\u2032 that differs from D in one entry. That is, any single entry of the database does not affect the results of the algorithm by much; dually, this means that an adversary cannot gain any extra information about any single entry of the database by observing the output of the algorithm.\nDefinition 1. An algorithm M provides \u01eb-differential privacy if for any two databases D and D\u2032 that differ in a single entry and for any f ,\n\u2223 \u2223 \u2223 \u2223 \u2223 log \u00b5 ( M(D) = f \u2223 \u2223 D ) \u00b5 ( M(D) = f \u2223 \u2223 D\u2032 ) \u2223 \u2223 \u2223 \u2223 \u2223 \u2264 \u01eb . (3)\n4\nNote that an SVM solution is not intrinsically private. This is because, an SVM solution is a linear combination of support vectors which are selected training samples. If a support vector changes, the SVM solution will also change. Regularization helps by bounding the L2 norm of the change, but does not completely solve the problem, as the direction of a low-norm classifier can still change when a single training sample changes.\nPerformance Measures. In this paper we construct privacy-preserving SVM training algorithms. The first question is how to approximate the SVM while guaranteeing \u01ebp-differential privacy. The second question is how much we pay in terms of generalization performance. The measure of performance for a classifier f is the expected loss over a data distribution P on X \u00d7 {\u22121,+1}:\nL(f) = EP [\u2113h(f(x), y)] . (4)\nAssuming the databaseD is drawn iid according toP , we are interested in how large the number of entries n has to be in order to guarantee a bounded gapL(fpriv)\u2212L(f) \u2264 \u01ebg. In particular, we are interested in how the number of samples n behaves as a function of the privacy level \u01ebp, generalization loss \u01ebg, and dimension d."}, {"heading": "3 Linear support vector machines", "text": "The simplest case of a support vector machine classifier is a linear SVM, in which the classifier f(x) takes the inner product of the data point x with a fixed vector f . In this setting the \u2016f\u2016H in (1) becomes the Euclidean norm \u2016f\u20162. Previous work has shown that an efficient way of guaranteeing privacy in logistic regression is to solve a perturbed objective function [Chaudhuri and Monteleoni, 2008]. We show how to provide a privacy-preserving Huber SVM by taking a similar approach; in order to guarantee \u01ebp-differential privacy, we minimize the following objective function:\nJpriv(f) = 1\nn\nn \u2211\ni=1\n\u2113h(f T x(i)), y(i)) +\n\u039b 2 \u2016f\u201622 + 1 n b T f , (5)\nwhere b has density\nG(b) = 1\nZ e\u2212(\u03b2/2)\u2016b\u2016 , (6)\nZ is a normalizing constant, and \u03b2 = \u01ebp. Recall that \u2113h here is the Huber loss. Algorithm 1 is our privacy-preserving algorithm for the case of linear SVMs based on (5).\nSimilar to logistic regression, the sensitivity method of [Dwork et al., 2006] can be applied to privacy-preserving SVMs as well. The sensitivity method is a general procedure for computing privacy-preserving approximations to functions. The idea is to add noise to the output of a function proportional to its sensitivity (1). For SVMs, the sensitivity is at most 2/(n\u039b). To achieve \u01ebp-differential privacy, the sensitivity method outputs c + argminJ(f) where c has density given by (6) with \u03b2 = n\u01ebp\u039b. For more details on this analysis, see [Chaudhuri and Monteleoni, 2008].\n5\nAlgorithm 1 Privacy-preserving linear SVM classifier training input Data {Zi}, parameters \u01ebp, \u039b. output Linear classifier fpriv.\nDraw a vector b according to (6). Compute fpriv = argminJpriv(f), defined in (5)."}, {"heading": "3.1 Privacy guarantees", "text": "Theorem 1. Given data {(x(i), y(i)) : i \u2208 [n]}, the output fpriv of Algorithm 1 guarantees \u01ebp-differential privacy.\nWe use the Huber loss instead of the hinge loss because the proof of Theorem 1 depends on the loss function being differentiable. To see why a differential loss function matters for our algorithm, suppose that \u2113(t, y) = (1\u2212ty)+ and consider the case where d = 1. Fix two databases D and D\u2032 which have only one entry each. That is, suppose D = (+1, 1) and D\u2032 has (\u22121, 1), and suppose f\u2217 = 1 was given by the algorithm. Then under database D we have that\nJpriv(f) = (1 \u2212 f)+ + \u039b\n2 f 2 + bf (7)\nis minimized by f = f\u2217 = 1. If we set b to be any number in the interval [\u2212\u039b,\u2212\u039b+1], the subgradient set of Jpriv at f = 1 contains 0; this means that for this entire range of values of b, f = 1 is the minimizer. Now let us look at D\u2032, for which\nJpriv(f) = (1 + f) + +\n\u039b 2 f 2 + bf . (8)\nAt f = 1 the gradient exists and by setting it equal to 0 we get b = \u2212(\u039b + 1), which means that there is only one value of b for which f = 1 is the minimizer. Therefore from the log likelihood ratio in the definition of \u01eb-differential privacy, the likelihood of D is far greater than that of D\u2032, so this protocol cannot be \u01eb-differentially private."}, {"heading": "3.2 Generalization bounds", "text": "To provide learning guarantees we assume the data {x(i), y(i))} consists of independent and identically distributed samples from a distribution P (X, Y ).\nLemma 1. Given a regularized SVM with regularization parameter \u039b, let f0 be the classifier that minimizes J in (1) and fpriv be the classifier that minimizes Jpriv in (5). Then\nPP\n( J(fpriv) \u2264 Jpriv(f0) + 8d2 log2(d/\u03b4)\n\u039bn2\u01eb2p\n)\n\u2265 1\u2212 \u03b4 ,\nwhere the probability is over iid sampling from P .\nProof. The proof is identical to Lemma 4 in [Chaudhuri and Monteleoni, 2008].\n6\nTheorem 2. Let f\u2217 be a classifier with expected loss L(f\u2217) and norm \u2016f\u2217\u20162. Then for\nn = \u2126\n(\nmax\n{\n\u2016f\u2217\u201622 \u01eb2g , \u2016f\u2217\u20162 d log(d/\u03b4) \u01ebp\u01ebg\n})\n, (9)\nthe classifier fpriv produced by Algorithm 1 satisfies\nP (L (fpriv) \u2264 L(f\u2217) + \u01ebg) \u2265 1\u2212 \u03b4 . (10)\nThe algorithm for linear SVMs is a natural modification of the algorithm presented in [Chaudhuri and Monteleoni, 2008], and the analysis is quite similar. The central idea is to perturb the objective function of a convex optimization program rather than the output of the optimization. Indeed, this algorithm can be generalized to general finite dimensional convex optimization programs."}, {"heading": "4 Nonlinear kernels", "text": "In this section, we show how to use SVMs to generate nonlinear decision boundaries for classification in a privacy-preserving way. Such boundaries are typically found by using a nonlinear kernel function k(x,x\u2032). The kernel function is associated with a reproducing kernel Hilbert space (RKHS) in which the classifier lies. The representer theorem [Kimeldorf and Wahba, 1970] shows that the regularized empirical risk in (1) is minimized by a function f(x) that is given by a linear combination of kernel functions centered at the data points:\nf \u2217(x) =\nn \u2211\ni=1\naik(x(i),x) . (11)\nThis elegant result is appealing from a theoretical standpoint. To use it in practice, one releases the values ai corresponding to the f that minimizes the empirical risk, along with the data points x(i); the user classifies a new x by evaluating the function in Equation 11.\nOur first contribution in this section is to observe that this release process is problematic from a privacy point of view. This is because it directly releases the private values x(i) of individuals in the training set. Thus, even if the classifier is computed in a privacy-preserving way, any classifier released by this process will inherently violate privacy. Our second contribution is to provide a solution that avoids this problem; we do this by using the approximation method of Rahimi and Recht [Rahimi and Recht, 2007, Rahimi and Recht, 2008b], which uses random projections to approximate the kernel function in a higher-dimensional space."}, {"heading": "4.1 The algorithm", "text": "Our algorithm for privacy-preserving SVM classification using general kernels is given in Algorithm 2. Here we will describe the algorithm for the Gaussian kernel k(x,x\u2032) = exp(\u2212\u03b3 \u2016x \u2212 x\u2032\u201622), but we can approximate many different kernels of interest [Rahimi\n7\nand Recht, 2008b]. For j \u2208 [D] we draw a sample \u03b8j = (\u03c9j , \u03c8) in Rd \u00d7 R according to the distribution p(\u03b8) given by N (0, 2\u03b3Id) \u00d7 Uniform[\u2212\u03c0, \u03c0]. We then map each vector x(i) \u2208 Rd to a vector v(i) \u2208 RD by evaluating the functions \u03c6(x(i); \u03b8j) = cos(\u03c9 T j x(i) + \u03c8) for j \u2208 [D]. Finally, we use Algorithm 1 to train a privacy-preserving classifier for the projected data {v(i)}. In order to compute the classifier\u2019s label on a new point x, the classifier first projects x into a vector v via the map \u03c6(x; \u03b8j) and then uses the linear classifier given by Algorithm 1.\nAlgorithm 2 Private SVM classification for nonlinear kernels input Data {(xi, yi) : i \u2208 [n]}, positive definite kernel function k(\u00b7, \u00b7), p(\u03b8), parame-\nters \u01ebp, \u039b, D output Classifier fpriv and pre-filter {\u03b8j : j \u2208 [D]}.\nDraw {\u03b8j : j = 1, 2, . . . , D} iid according to p(\u03b8). Set v(i) = \u221a\n2/D[\u03c6(x(i); \u03b81) \u00b7 \u00b7 \u00b7\u03c6(x(i); \u03b8D)]T for each i. Run Algorithm 1 with data {(v(i), y(i))} and parameters \u01ebp, \u039b."}, {"heading": "4.2 Privacy guarantees", "text": "Because the workhorse of Algorithm 2 is Algorithm 1, and the points {\u03b8j : j \u2208 [D]} are independent of the data, the privacy guarantees for Algorithm 2 follow trivially from Theorem 1.\nTheorem 3. Given data {(x(i), y(i)) : i = 1, 2, . . . , n}with (x(i), y(i)) and \u2016x(i)\u2016 \u2264 1, the outputs (fpriv, {\u03b8j : j \u2208 [D]}) of Algorithm 2 guarantee \u01ebp-differential privacy."}, {"heading": "4.3 Generalization bounds", "text": "In this section we prove bounds on the generalization performance of the privacy preserving classifier produced by Algorithm 2. We will compare this generalization performance against arbitrary classifiers f\u2217 whose \u201cnorm\u201d is bounded in some sense. That is, given an f\u2217 with some properties, we will choose regularization parameter \u039b, dimension D, and number of samples n so that the classifier fpriv has expected loss close to that of f\u2217. In particular, our results will say that as long as n is greater than some quantity, then L(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg.\nOur first generalization result is the simplest, since it assumes a strong condition that gives easy guarantees on the projections. We would like our privatized classifier to be competitive against a classifier f\u2217 such that\nf \u2217(x) =\n\u222b\n\u0398\na\u2217(\u03b8)\u03c6(x; \u03b8)p(\u03b8)d\u03b8 , (12)\nand |a\u2217(\u03b8)| \u2264 C (see [Rahimi and Recht, 2008b]).\n8\nLemma 2. Let f\u2217 be a classifier such that |a\u2217(\u03b8)| \u2264 C, where a\u2217(\u03b8) is given by (12). Then for any distribution P , if\nn = \u2126\n(\nC2 \u221a log(1/\u03b4)\n\u01ebp\u01eb2g \u00b7 log C log(1/\u03b4) \u01ebg\u03b4\n)\n, (13)\nthen \u039b and D can be chosen such that\nP (L(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg) \u2265 1\u2212 4\u03b4 . (14)\nThe proof uses generalization results from [Rahimi and Recht, 2008b] with results on perturbed optimization from [Sridharan et al., 2008, Shalev-Shwartz and Srebro, 2008]. We can adapt the proof procedure to show that we can be competitive against any classifier f\u2217 with a given bound on \u2016f\u2217\u2016\u221e. It can be shown that for some constant \u03b6 that |a\u2217(\u03b8)| \u2264 Vol(X )\u03b6 \u2016f\u2217\u2016\u221e. Then we can set this as C in (13) to obtain our next result.\nTheorem 4. Let f\u2217 be a classifier with norm \u2016f\u2217\u2016\u221e. Then for any distribution P , if\nn = \u2126\n(\n\u2016f\u2217\u20162\u221e \u03b62(Vol(X ))2 \u221a log(1/\u03b4)\n\u01ebp\u01eb2g (15)\n\u00b7 log \u2016f \u2217\u2016\u221e Vol(X )\u03b6 log(1/\u03b4)\n\u01ebg\u03b4\u0393( d 2 + 1)\n)\n, (16)\nthen \u039b and D can be chosen such that P (L(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg) \u2265 1\u2212 4\u03b4 .\nFinally, we derive a generalization result with respect to an optimal classifier which has bounded \u2016f\u2217\u2016H.\nTheorem 5. Let f\u2217 be a classifier with norm \u2016f\u2217\u2016H. Then for any distribution P , if\nn = \u2126\n(\n\u2016f\u2217\u20164H \u03b62(Vol(X ))2 \u221a log(1/\u03b4)\n\u01ebp\u01eb4g (17)\n\u00b7 log \u2016f \u2217\u2016H Vol(X )\u03b6 log(1/\u03b4)\n\u01ebg\u03b4\u0393( d 2 + 1)\n)\n, (18)\nthen \u039b and D can be chosen such that P (L(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg) \u2265 1\u2212 4\u03b4 ."}, {"heading": "5 Privacy-preserving tuning", "text": "We now present a privacy-preserving parameter tuning technique that is generally applicable to other machine learning algorithms, in addition to SVMs. In practice, one typically tunes SVM\u2019s regularization parameter as follows: using data held out for validation, learn an SVM for multiple values of \u039b, and select the one which provides the best empirical performance. However, even though the output of Algorithm 1 preserves\n9\n\u01ebp-differential privacy for a fixed \u039b, using a \u039b which itself is a function of the private data may break \u01ebp-differential privacy guarantees. That is, if the procedure that provides \u039b is not private, then from the output of our algorithm, an adversary may gain some knowledge about the value of \u039b, allowing her to infer something about private values in the database.\nWe suggest two ways of resolving this issue. First, if we have access to some publicly available data from the same distribution, then we can use this as a holdout set to tune \u039b. This \u039b can be subsequently used to train a classifier on the private data. Since the value of \u039b does not depend on the values in the private data set, this procedure will still preserve the privacy of individuals in the private data.\nIf no such public data is available, then we need a differentially private tuning procedure. We provide such a procedure below. The main idea is to train for different values of \u039b on separate datasets, so that the total training procedure still maintains \u01ebp-differential privacy. We then select a \u039b using a randomized privacy-preserving comparison procedure [McSherry and Talwar, 2007]. The last step is needed to guarantee \u01ebp-differential privacy for individuals in the validation set.\nAlgorithm 3 Private-preserving parameter tuning input Database D, parameters {\u039b1, . . . ,\u039bm}, \u01ebp. output Parameter fpriv.\nDivide D into m+ 1 equal portions D1, . . . ,Dm+1, each of size |D|m+1 . For each i = 1, 2, . . . ,m, apply a privacy-preserving learning algorithm (e.g. Algorithm 1) on Di with parameter \u039bi and \u01ebp to get output fi. Evaluate zi, the number of mistakes made by fi on Dm+1. Set fpriv = fi with probability\nqi = e\u2212\u01ebzi/2\n\u2211m i=1 e\n\u2212\u01ebzi/2 . (19)\nWe note that the list of potential \u039b values input to this procedure should not be a function of the private dataset. It can also be shown that the empirical error on Dm+1 of the classifier output by this procedure is close to the empirical error of the best classifier in the set {f1, . . . , fm} on Dm+1, provided |D| is high enough. Theorem 6. The output of the tuning procedure of Algorithm 3 is \u01ebp-differentially private.\nThe following theorem shows that the empirical error on DK+1 of the classifier output by the tuning procedure is close to the empirical error of the best classifier in the set {f1, . . . , fK}. The proof of this Theorem follows from Lemma 7 of [McSherry and Talwar, 2007].\nTheorem 7. Let zmin = mini zi, and let z be the number of mistakes made on Dm+1 by the classifier output by our tuning procedure. Then, with probability 1\u2212 \u03b4,\nz \u2264 zmin + 1\n\u01ebp log(m/\u03b4) . (20)\n10"}, {"heading": "6 Experiments", "text": "We validated our theoretical results in three different scenarios. We tested the private linear SVM Algorithm 1 on two different real data sets, the Adult dataset from the UCI Machine Learning Repository [Asuncion and Newman, 2007], and the KDDCup99 dataset [Hettich and Bay, 1999]. We validated the theoretical results for our kernel SVM method (Algorithm 2) using the Gaussian kernel on a toy problem using synthetic data. The convex optimization was done using a standard convex optimization library [Okazaki, 2009].\nWe chose the moderately-sized Adult dataset because demographic data is relevant for privacy; it contains demographic information about approximately 47, 000 individuals, and the classification task was to predict whether the annual income of an individual is below or above $50,000, based on variables such as age, sex, occupation, and education. The average fraction of positive labels was 0.2478. In the larger KDDCup99 dataset [Hettich and Bay, 1999] the task was to predict whether a network connection is a denial-of-service attack, based on several attributes. The dataset includes about 5,000,000 instances, subsampled down to about 500,000 instances, with a 19% positive labels, and was large enough to use our privacy-preserving tuning Algorithm 3 to tune the regularization parameter.3 In the experiments with real data, we compared Algorithm 1 with non-private (Huber) SVM, the privacy-preserving logistic regression classifier of [Chaudhuri and Monteleoni, 2008], and the Sensitivity-method, the straightforward application of the method of [Dwork et al., 2006], applied to Huber SVM. On the real data sets, each value is an average over 10-fold cross-validation, and and for the private algorithms, each value is additionally averaged over 50 random choices of b.\nImposing privacy requirements necessarily degrades classifier performance, and Figure 1 shows the classification error versus privacy level in (a) for the Adult data set. As the privacy parameter \u01ebp increases, the error gap between the private (solid) and the non-private (dashed) methods shrink. For Adult Algorithm 1 strictly outperforms the logistic regression and sensitivity methods. The Adult dataset was not sufficiently large to allow us to do the privacy-preserving tuning of Algorithm 3, so instead we ran experiments over a matrix of \u039b and \u01eb values. The best error rate achieved by the non-private algorithm on this data set is 15.362% (\u039b = 10\u22126), and the best for the Algorithm 1 is 17.62% (\u039b = 10\u22123), for \u01ebp = 0.2. The performance loss is approximately 2.3% and increases as \u01ebp decreases. For a data set of fixed size, these results illustrate the tradeoff between generalization \u01ebg and privacy \u01ebp.\nOur experiments suggest that for stronger privacy guarantees, the privacy-preserving SVM may be a better choice than logistic regression. For KDDCup99 the error versus \u01ebp curves for a random subset of size 60, 000 points are shown in Figure 1 (b). We see that the when \u039b is tuned for all algorithms for 0.05-differential privacy, Algorithm 1 (with \u039b = 10\u22122) performs only slightly more than 1.5% worse than non-private SVM (with \u039b = 10\u22126), whereas the sensitivity-method is 7.5% worse (with \u039b = 10\u22121).4 For small \u01ebp, Algorithm 1 is superior to logistic regression, but the curves cross as the\n3When using privacy-preserving tuning, we used 7 values of \u039b, so 87.5% of the data was used for tuning. 4The tuned values for \u01ebp = 0.1 are \u039b = 10\u22123 for Algorithm 1, and \u039b = 10\u22122 for the Sensitivity-\nmethod.\n11\nprivacy requirement is weakened and \u01ebp increases. We tested the Gaussian kernel version5 of Algorithm 2 on 240, 000 data points in R 5 sampled from a \u201cnested balls\u201d distribution with mass 0.45 uniformly in the ball of radius 0.1 with label +1, mass 0.45 uniformly in the annulus from 0.2 to 0.5 with label \u22121, and mass 0.1 uniformly in the annulus from 0.1 to 0.2 with equiprobable labels \u00b11. Since the sensitivity method cannot be used for nonlinear kernels, we compared Algorithm 2 against a kernel SVM with hinge loss using SVMlight [Joachims, 1999] and a non-private linear SVM using the projections of [Rahimi and Recht, 2008b]. We averaged over 5 random projections and 50 drawings of b.\nFigure 2 gives tabular results. Loss in generalization performance is dominated by the cost of privacy; e.g. in (b), the linear SVM on random projections has error very close to that of the kernel SVM. The degradation due to privacy is about double, but the error rate, 11.4% is still reasonable.\nLearning curves. Figure 1(d)-(f) presents learning curves for KDDCup99 at \u01ebp = 0.05, \u01ebp = 0.005, and the kernel simulation at \u01ebp = 0.1.6 For KDDCup99 for each data set size N we did non-private tuning of Huber-SVM, and the private tuning procedure in Algorithm 3 for the private SVMs7. For the kernels, we compare to Huber SVM with the random projection kernel method. These experiments show that to achieve small generalization error requires large data sets, and decreasing \u01ebp requires even more data. In summary, our differentially private SVM makes significant reductions in error compared to the sensitivity-method, the only other differentially private SVM."}, {"heading": "7 Proofs", "text": "This section contains supplementary material and the proofs for our main results.\nRandom projections and kernel approximation\nA kernel function k(\u00b7, \u00b7) is associated to a reproducing kernel Hilbert Space (RHKS) H of functions from X \u2192 R. We assume that the kernel function is bounded and that we can write it as an integral over a space \u0398 with finite measure p(\u03b8) as follows:\nk(x,x\u2032) =\n\u222b\n\u0398\n\u03c6(x; \u03b8)\u03c6(x\u2032; \u03b8)p(\u03b8)d\u03b8 , (21)\nwhere the feature function \u03c6(x; \u03b8) can be thought of as the mapping of x into the higher dimensional space whose dimensions are indexed by \u03b8 \u2208 \u0398. We will assume that p(\u03b8) is normalized so that it is a probability density over \u0398. We will assume that the functions \u03c6(x; \u03b8) are bounded:\n|\u03c6(x; \u03b8)| \u2264 \u03b6 \u2200x \u2208 X , \u2200\u03b8 \u2208 \u0398 (22)\n15\nArmed with this representation, we can write functions in H in the following way:\nf(x) =\n\u222b\n\u0398\na(\u03b8)\u03c6(x; \u03b8)p(\u03b8)d\u03b8 . (23)\nThis representation can arise quite naturally, as shown by Rahimi and Recht [Rahimi and Recht, 2007]. For example for kernel functions k(x,x\u2032) = k(x\u2212 x\u2032), Fourier features of the form \u03c6(x; (\u03c9, \u03c8)) = cos(\u03c9Tx + \u03c8) can be used, where \u03b8 = (\u03c9, \u03c8). The density p(\u03b8) can be interpreted a Fourier transform of the kernel function. Other kernels can be handled using different feature functions (see [Rahimi and Recht, 2008a] for more examples). For a constant C, we can define a class of functions\nFp(C) = { f(x) = \u222b\n\u0398\na(\u03b8)\u03c6(x; \u03b8)p(\u03b8)d\u03b8 \u2223 \u2223 \u2223|a(\u03b8)| < C } , (24)\nRahimi and Recht [Rahimi and Recht, 2008a, Proposition 4.1] showed that the class of functions Fp(\u221e) is dense in the RHKS H and that the completion of Fp(\u221e) is equal to H.\nWe need to show that bounded classifiers f induced bounded functions a(\u03b8). Given a function f in an RKHS, we can write the evaluation functional as an inner product (this is the reproducing kernel property) and substitute the representation of the kernel function in (21):\nf(x) =\n\u222b\nX\nf(x\u2032)k(x,x\u2032)dx\u2032 (25)\n=\n\u222b\nX\nf(x\u2032)\n\u222b\n\u0398\n\u03c6(x; \u03b8)\u03c6(x\u2032; \u03b8)p(\u03b8)d\u03b8dx\u2032 (26)\n=\n\u222b\n\u0398\n(\u222b\nX\nf(x\u2032)\u03c6(x\u2032; \u03b8)dx\u2032 ) \u03c6(x; \u03b8)p(\u03b8)d\u03b8 (27)\nThus we have\na(\u03b8) =\n\u222b\nX\nf(x\u2032)\u03c6(x\u2032; \u03b8)dx\u2032 (28)\n|a(\u03b8)| \u2264 Vol(X ) \u00b7 \u03b6 \u00b7 \u2016f\u2016\u221e . (29)\nThis shows that a(\u03b8) is bounded uniformly over \u0398 when f(x) is bounded uniformly overX . The volume of the unit ball decreases for large d, and in particular [Ball, 1997]:\nVol(X ) = \u03c0 d/2\n\u0393(d2 + 1) . (30)\nFor large d this is ( \u221a\n2\u03c0e d ) d by Stirling\u2019s formula. Furthermore, we have\n\u2016f\u20162H = \u222b\n\u0398\na(\u03b8)2p(\u03b8)d\u03b8 . (31)\n16\nProof of Theorem 1\nProof. The proof follows the argument of [Chaudhuri and Monteleoni, 2008]. Consider two data sets differing in only one point:\nD = {(x(i), y(i)) : i \u2208 [n]} (32) D\u2032 = {(x(i), y(i)) : i 6= j} \u222a {(x\u2032(j), y\u2032(j))} , (33)\nNow suppose that the algorithm outputs the same classifier for both data sets, so fpriv(\u00b7;D) = fpriv(\u00b7;D\u2032) = f . The only randomness in the process is the realization of the noise added to the objective. Let u and u\u2032 denote the realization of b for D and D\u2032, respectively.\nSince f minimizes the objective, the gradient of the objective is 0 at f for both u and u\u2032:\n\u2207Jpriv(f |D) = 1\nn\n\u2211\ni6=j\n\u2202 \u2202t \u2113h(y(i), t)\n\u2223 \u2223 \u2223\nt=fT x(i) x(i) +\n\u2202\n\u2202t\n1 n \u2113h(y(j), t)\n\u2223 \u2223 \u2223\nt=fT x(j) x(j) + \u039bf +\n1 n u\n(34)\n\u2207Jpriv(f |D\u2032) = 1\nn\n\u2211\ni6=j\n\u2202 \u2202t \u2113h(y(i), t)\n\u2223 \u2223 \u2223\nt=fT x(i) x(i) +\n\u2202\n\u2202t\n1 n \u2113h(y \u2032(j), t) \u2223 \u2223 \u2223 t=fT x\u2032(j) x \u2032(j) + \u039bf + 1 n u \u2032 .\n(35)\nBy canceling common terms we obtain:\n\u2202 \u2202t \u2113h(y(j), t)\n\u2223 \u2223 \u2223\nt=f\u2217T x(j) x(j) + u =\n\u2202 \u2202t \u2113h(y \u2032(j), t) \u2223 \u2223 \u2223 t=f\u2217T x\u2032(j) x \u2032(j) + u\u2032 (36)\nThe derivative of the Huber loss is:\n\u2202 \u2202t \u2113(y, t) =\n\n\n 0 if yt > 1 + h \u2212y 2h (1 + h\u2212 yt) if |1\u2212 yt| \u2264 h \u2212y if yt < 1\u2212 h\n(37)\nThe derivative is always in the interval [\u22121, 1], so\n\u2016u \u2212 u\u2032\u2016 \u2264 2 , (38)\nand therefore \u2016u\u2032\u2016 is within 2 of \u2016u\u2016, or \u2016u\u2016 \u2212 2 \u2264 \u2016u\u2032\u2016 \u2264 \u2016u\u2016+ 2. Now, the log likelihood ratio for seeing the classifier f given the two data sets D and D\u2032 is \u2223\n\u2223 \u2223 \u2223 log P (fpriv(\u00b7;D) = f |D) P (fpriv(\u00b7;D\u2032) = f |D\u2032)\n\u2223 \u2223 \u2223 \u2223 = log G(u)\nG(u\u2032) (39)\n= log exp(\u2212(\u01ebp/2)(\u2016u\u2016 \u2212 \u2016u\u2032\u2016)) (40) \u2208 [\u2212\u01ebp, \u01ebp] . (41)\nTherefore the algorithm preserves \u01ebp-differential privacy.\n17\nProof of Theorem 2\nLet L(f) be the expected loss for classifier f :\nL(f) = EP [ \u2113h(Y, f T X + \u00b5) ] , (42)\nand let J\u0304 be the regularized true risk (expected loss):\nJ\u0304(f) = L(f) + \u039b\n2 \u2016f\u20162 . (43)\nProof. Let frtr = argminf J\u0304(f) be the classifier that minimizes the regularized true risk, f0 be the classifier that minimizes the non-private SVM objective J(f) and fpriv be the classifier that minimizes Jpriv(f). Then we can write:\nJ\u0304(fpriv) = J\u0304(f \u2217) + (J\u0304(fpriv)\u2212 J\u0304(frtr)) + (J\u0304(frtr)\u2212 J\u0304(f\u2217)) .\nFrom this we can see that (c.f. [Shalev-Shwartz and Srebro, 2008]):\nL(fpriv) = L(f \u2217) + (J\u0304(fpriv)\u2212 J\u0304(frtr)) + (J\u0304(frtr)\u2212 J\u0304(f\u2217)) +\n\u039b 2 \u2016f\u2217\u20162 \u2212 \u039b 2 \u2016fpriv\u20162 .\n(44)\nNow from Lemma 1, with probability at least 1\u2212 \u03b4,\nJ(fpriv)\u2212 J(f0) \u2264 8d2 log2(d/\u03b4)\n\u039bn2\u01eb2p . (45)\nNow using [Sridharan et al., 2008, Corollary 2], we can bound the term (J\u0304(fpriv) \u2212 J\u0304(frtr)) by twice the gap in the regularized empirical risk difference (45) plus an additional term. That is, with probability 1\u2212 \u03b4:\nJ\u0304(fpriv)\u2212 J\u0304(frtr) \u2264 16d2 log2(d/\u03b4)\n\u039bn2\u01eb2p +O\n(\n1\n\u039bn\n)\n. (46)\nSince frtr minimizes J\u0304(f), we know J\u0304(frtr) \u2264 J\u0304(f\u2217), so\nL(fpriv) \u2264 L(f\u2217) + \u039b 2 \u2016f\u2217\u20162 + 16d\n2 log2(d/\u03b4)\n\u039bn2\u01eb2p +O\n(\n1\n\u039bn\n)\n.\nThe last thing to do is set \u039b and the bound on the number of samples. If \u039b = \u01ebg/ \u2016f0\u20162 then \u039b2 \u2016f\u2217\u2016 2 < \u01ebg/2. For a suitably large constant C, if n > C \u2016f0\u20162 /\u01eb2g, then the O(1/\u039bn) term can be upper bounded by \u01ebg/4. Similarly, if n > C \u2016f0\u2016 d log(d/\u03b4)/\u01ebp\u01ebg then the penultimate term is upper bounded by \u01ebg/4. Thus the total loss can be bounded:\nL(fpriv) \u2264 L(f0) + \u01ebg . (47)\n18\nProof of Theorem 3\nProof. The pre-filter values {\u03b8j : j \u2208 [D]} are iid and independent of the data, so with some abuse of notation,\nP\n(\nf = f\u0302 , {\u03b8j : j \u2208 [D]} = {\u03b8\u0302j : j \u2208 [D]} \u2223 \u2223 {(x(i), y(i))} )\n= P ( f = f\u0302 , \u2223 \u2223 {(x(i), y(i))}, {\u03b8j : j \u2208 [D]} = {\u03b8\u0302j : j \u2208 [D]} )\nD \u220f\nj=1\np(\u03b8\u0302j)\n(48)\nSince this holds for all {\u03b8j : j \u2208 [D]}, the terms \u220fD\nj=1 p(\u03b8\u0302j) cancel in the log likelihood ratio in (3). Since Algorithm 1 guarantees \u01ebp-differential privacy by Theorem 1, the outputs of Algorithm 2 also guarantee \u01ebp-differential privacy.\nProof of Lemma 2\nProof. The assumptions guarantee that f\u2217 \u2208 Fp(C) as defined in (24), so [Rahimi and Recht, 2008b, Theorem 1] shows that with probability 1 \u2212 2\u03b4 there exists an f0 \u2208 RD such that\nL(f0) \u2264 L(f\u2217) +O ( (\n1\u221a n + 1\u221a D\n)\nC\n\u221a\nlog 1\n\u03b4\n)\n, (49)\nWe will choose D later to make this loss small. Furthermore, f0 is guaranteed to have \u2016f0\u2016\u221e \u2264 C/D, so\n\u2016f0\u201622 \u2264 C2\nD . (50)\nNow given such an f0 we must show that fpriv will have true risk close to that of f0 as long as there are enough data points. This can be shown using the techniques in [Shalev-Shwartz and Srebro, 2008]. Let\nJ\u0304(f) = L(f) + \u039b\n2 \u2016f\u201622 (51)\nfrtr = argmin f\u2208RD J\u0304(f) , (52)\nwhere frtr minimizes the regularized true risk. Then\nJ\u0304(fpriv) = J\u0304(f0) + (J\u0304(fpriv)\u2212 J\u0304(frtr)) + (J\u0304(frtr)\u2212 J\u0304(f0)) . (53)\nNow, since J\u0304(\u00b7) is minimized by frtr, the last term is negative and we can disregard it. Then we have\nL(fpriv)\u2212 L(f0) \u2264 (J\u0304(fpriv)\u2212 J\u0304(frtr)) + \u039b\n2 \u2016f0\u201622 \u2212\n\u039b 2 \u2016fpriv\u201622 (54)\n19\nFrom Lemma 1, with probability at least 1\u2212 \u03b4:\nJ(fpriv)\u2212 J (\nargmin f J(f)\n) \u2264 8D 2 log2(D/\u03b4)\n\u039bn2\u01eb2p (55)\nNow using [Sridharan et al., 2008, Corollary 2], we can bound the term (J\u0304(fpriv) \u2212 J\u0304(frtr)) by twice the gap in the regularized empirical risk difference (55) plus an additional term. That is, with probability 1\u2212 \u03b4:\nJ\u0304(fpriv)\u2212 J\u0304(frtr) \u2264 2(J(fpriv)\u2212 J(frtr)) +O ( log(1/\u03b4)\n\u039bn\n)\n(56)\n\u2264 16D 2 log2(D/\u03b4)\n\u039bn2\u01eb2p +O\n(\nlog(1/\u03b4)\n\u039bn\n)\n(57)\nPlugging (57) into (54), discarding the negative term involving \u2016fpriv\u201622 and setting \u039b = \u01ebg/ \u2016f0\u20162 gives\nL(fpriv)\u2212 L(f0) \u2264 16 \u2016f0\u201622 D2 log2(D/\u03b4)\nn2\u01eb2p\u01ebg +O\n(\n\u2016f0\u201622 log 1\u03b4 n\u01ebg\n)\n+ \u01ebg 2 . (58)\nNow we have, using (49) and (58), that with probability 1\u2212 4\u03b4: L(fpriv)\u2212 L(f\u2217) \u2264 (L(fpriv)\u2212 L(f0)) + (L(f0)\u2212 L(f\u2217)) (59)\n\u2264 16 \u2016f0\u2016 2 2 D 2 log2(D/\u03b4)\nn2\u01eb2p\u01ebg +O\n(\n\u2016f0\u201622 log(1/\u03b4) n\u01ebg\n)\n+ \u01ebg 2\n+O\n(\n(\n1\u221a n + 1\u221a D\n)\nC\n\u221a\nlog 1\n\u03b4\n)\n, (60)\nNow, from (50) we can substitute:\nL(fpriv)\u2212 L(f\u2217) \u2264 16C2D log2(D/\u03b4)\nn2\u01eb2p\u01ebg +O\n(\nC2 log(1/\u03b4)\nDn\u01ebg\n)\n+ \u01ebg 2\n+O\n(\n(\n1\u221a n + 1\u221a D\n)\nC\n\u221a\nlog 1\n\u03b4\n)\n. (61)\nTo set the parameters, we will choose D < n so that\nL(fpriv)\u2212 L(f\u2217) \u2264 16C2D log2(D/\u03b4)\nn2\u01eb2p\u01ebg +O\n(\nC2 log(1/\u03b4)\nDn\u01ebg\n)\n+ \u01ebg 2 +O\n(\nC \u221a\nlog(1/\u03b4)\u221a D\n)\n.\nAgain focusing on the last term, we set D = O(C2 log(1/\u03b4)/\u01eb2g) to make this last term \u01ebg/6:\nL(fpriv)\u2212 L(f\u2217) \u2264 16C2D log2(D/\u03b4)\nn2\u01eb2p\u01ebg +O\n(\nC2 log(1/\u03b4)\nDn\u01ebg\n)\n+ 2\u01ebg 3\n(62)\n\u2264 O\n\n\nC4 log 1\u03b4 log 2 C2 log(1/\u03b4)\n\u01eb2g\u03b4\nn2\u01eb2p\u01eb 3 g\n\n+O ( \u01ebg n ) + 2\u01ebg 3 . (63)\n20\nNow we set\nn = \u2126\n(\nC2 \u221a log(1/\u03b4)\n\u01ebp\u01eb2g \u00b7 log C log(1/\u03b4) \u01ebg\u03b4\n)\n(64)\nto get\nL(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg . (65)\nProof of Theorem 4\nProof. Substituting C = Vol(X )\u03b6 \u2016f\u2217\u2016\u221e in (13) from Lemma 2 and using (30), we get the result.\nProof of Theorem 5\nLet f\u2217 be a classifier with norm \u2016f\u2217\u20162H and expected loss L(f\u2217). Now consider\nfrtr = argmin f L(f) + \u039brtr 2 \u2016f\u20162H . (66)\nWe will first need a bound on \u2016frtr\u2016\u221e in order to use our previous sample complexity results. Note that since frtr is a minimizer, we can take the derivative of the regularized expected loss and set it to 0 to get:\nfrtr(x \u2032) =\n1\n\u039brtr\n\u2202\n\u2202f L(f) (67)\n= \u22121 \u039brtr ( \u2202 \u2202f \u222b\nX\n\u2113(f(x\u2032), y)dP (x, y)\n)\n(68)\n= \u22121 \u039brtr (\u222b\nX\n(\n\u2202\n\u2202f(x\u2032) \u2113(f(x), y)\n) \u00b7 ( \u2202\n\u2202f(x\u2032) f(x)\n)\ndP (x, y)\n)\n, (69)\nwhere P (x, y) is a distribution on pairs (x, y). Now, using the representer theorem:\n\u2202\n\u2202f(x\u2032) f(x) = k(x\u2032,x) . (70)\nAnd the kernel function is bounded. Furthermore, the derivative of the loss is always upper bounded by 1, so the integrand can be upper bounded by a constant. Since P (x, y) is a probability distribution, we have for all x\u2032:\n|frtr(x\u2032)| = O ( 1\n\u039brtr\n)\n. (71)\nNow we set \u039brtr = \u01ebg/ \u2016f\u2217\u20162H to get\n\u2016frtr\u2016\u221e = O ( \u2016f\u2217\u20162H \u01ebg ) . (72)\n21\nWe now have two cases to consider, depending on whether L(f\u2217) < L(frtr) or L(f\u2217) > L(frtr).\nCase 1: Suppose that L(f\u2217) < L(frtr). Then by the definition of frtr,\nL(frtr) + \u01ebg 2\n\u00b7 \u2016frtr\u2016 2 H\n\u2016f\u2217\u20162H \u2264 L(f\u2217) + \u01ebg 2 . (73)\nSince \u01ebg2 \u00b7 \u2016frtr\u2016\n2 H \u2016f\u2217\u20162 H \u2265 0, we have\nL(frtr)\u2212 L(f\u2217) \u2264 \u01ebg 2 . (74)\nCase 2: Suppose that L(f\u2217) > L(frtr). Then the regularized classifier has better generalization performance than the original, so we have trivially that\nL(frtr)\u2212 L(f\u2217) \u2264 \u01ebg 2 . (75)\nTherefore in both cases we have a bound on \u2016frtr\u2016\u221e and a generalization gap of \u01ebg/2. We can now apply our previous generalization bound to get that for\nn = \u2126\n(\n\u2016f\u2217\u20164H \u03b62(Vol(X ))2 \u221a log(1/\u03b4) \u01ebp\u01eb4g \u00b7 log \u2016f \u2217\u20162H \u03b6 Vol(X ) log(1/\u03b4) \u01eb2g\u03b4\u0393( d 2 + 1)\n)\n, (76)\nwe have\nP (L(fpriv)\u2212 L(f\u2217) \u2264 \u01ebg) \u2265 1\u2212 4\u03b4 . (77)\n7.0.1 Proof of Theorem 6\nProof. Let A(D) denote the output of the tuning algorithm, when the input is database D, and let D = D\u0304 \u222a {(x(n), y(n))} and D\u2032 = D\u0304 \u222a {(x\u2032(n), y\u2032(n))} be two databases which differ in the private value of one person. Moreover, let F = (f1, . . . , fm) denote the classifiers computed in Step 2 of the tuning algorithm. Note that, for any f and F,\nP (A(D) = f) = \u222b\nF\nP (A(D) = f |F )P (F|D) d\u00b5(F) (78)\n= P (A(D) = f |FmDm+1) \u00b7 \u220f\ni\n\u222b\nfi\nP (fi|Di) d\u00b5(fi) (79)\nwhere \u00b5(F) is the uniform measure on F, and \u00b5(fi) is the uniform measure on fi. Here, the last line follows from the fact that for each i, the random choices made in execution i in Step 2, as well as the random choices in Step 3, are independent.\nIf (x(n), y(n)) \u2208 Dj , for j = 1, . . . ,m, then, by from Theorem 1,\nP (fj |Dj) \u2264 e\u01ebpP ( fk|D\u2032j )\n(80)\nP (fi|Di) = P(fi|D\u2032i) i 6= j (81)\n22\nMoreover, P (A(D) = f |F,Dm+1) is also the same for the two databases. Therefore\nP (A(D) = f) \u2264 e\u01ebpP (A(D\u2032) = f) . (82)\nOn the other hand, if (x(n), y(n)) \u2208 Dm+1 then \u01ebp-differential privacy follows from Theorem 6 of [McSherry and Talwar, 2007]."}], "references": [{"title": "Privacy-preserving data mining", "author": ["Agrawal", "Srikant", "R. 2000] Agrawal", "R. Srikant"], "venue": "SIGMOD Rec.,", "citeRegEx": "Agrawal et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2000}, {"title": "Privacy, accuracy, and consistency too: a holistic solution to contingency table release", "author": ["Barak et al", "B. 2007] Barak", "K. Chaudhuri", "C. Dwork", "S. Kale", "F. McSherry", "K. Talwar"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "A learning theory approach to non-interactive database privacy", "author": ["A. Blum", "K. Ligett", "A. Roth"], "venue": null, "citeRegEx": "Blum et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2008}, {"title": "Privacy-preserving logistic regression", "author": ["Chaudhuri", "Monteleoni", "K. 2008] Chaudhuri", "C. Monteleoni"], "venue": "In Twenty-Second Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Chaudhuri et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2008}, {"title": "Differential privacy and robust statistics", "author": ["Dwork", "Lei", "C. 2009] Dwork", "J. Lei"], "venue": null, "citeRegEx": "Dwork et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Dwork et al", "C. 2006] Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In 3rd IACR Theory of Cryptography Conference,,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Limiting privacy breaches in privacy preserving data mining", "author": ["Evfimievski et al", "A. 2003] Evfimievski", "J. Gehrke", "R. Srikant"], "venue": "In PODS,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Composition attacks and auxiliary information in data privacy", "author": ["Ganta et al", "S.R. 2008] Ganta", "S.P. Kasiviswanathan", "A. Smith"], "venue": "In KDD,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "The UCI KDD Archive", "author": ["Hettich", "Bay", "S. 1999] Hettich", "S. Bay"], "venue": null, "citeRegEx": "Hettich et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hettich et al\\.", "year": 1999}, {"title": "What can we learn privately", "author": ["Kasiviswanathan et al", "S.A. 2008] Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "In Proc. of Foundations of Computer Science", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "A correspondence between Bayesian estimation on stochastic processes and smoothing by splines", "author": ["Kimeldorf", "Wahba", "G. 1970] Kimeldorf", "G. Wahba"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Kimeldorf et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Kimeldorf et al\\.", "year": 1970}, {"title": "Cryptographically private support vector machines", "author": ["Laur et al", "S. 2006] Laur", "H. Lipmaa", "T. Mielik\u00e4inen"], "venue": "In KDD,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "l-diversity: Privacy beyond k-anonymity", "author": ["Machanavajjhala et al", "A. 2006] Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam"], "venue": "In Proc. of ICDE", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Privacy: Theory meets practice on the map", "author": ["Machanavajjhala et al", "A. 2008] Machanavajjhala", "D. Kifer", "J.M. Abowd", "J. Gehrke", "L. Vilhuber"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Privacy-preserving classification of vertically partitioned data via random kernels", "author": ["Mangasarian et al", "O.L. 2008] Mangasarian", "E.W. Wild", "G. Fung"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Mechanism design via differential privacy", "author": ["McSherry", "Talwar", "F. 2007] McSherry", "K. Talwar"], "venue": "In FOCS,", "citeRegEx": "McSherry et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McSherry et al\\.", "year": 2007}, {"title": "Smooth sensitivity and sampling in private data analysis", "author": ["Nissim et al", "K. 2007] Nissim", "S. Raskhodnikova", "A. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Recht", "A. 2007] Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Uniform approximation of functions with random bases", "author": ["Rahimi", "Recht", "A. 2008a] Rahimi", "B. Recht"], "venue": "In Proceedings of the 46th Allerton Conference on Communication,", "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "Weighted sums of random kitchen sinks : Replacing minimization with randomization in learning", "author": ["Rahimi", "Recht", "A. 2008b] Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2008}, {"title": "SVM optimization : Inverse dependence on training set size", "author": ["Shalev-Shwartz", "Srebro", "S. 2008] Shalev-Shwartz", "N. Srebro"], "venue": null, "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2008}, {"title": "Fast rates for regularized objectives", "author": ["Sridharan et al", "K. 2008] Sridharan", "N. Srebro", "S. Shalev-Shwartz"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Privacy-preserving support vector machine classification", "author": ["Zhan", "Matwin", "J.Z. 2007] Zhan", "S. Matwin"], "venue": "IJIIDS,", "citeRegEx": "Zhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhan et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "We provide general, efficient algorithms for linear and nonlinear kernel SVMs, which guarantee \u01eb-differential privacy, a very strong privacy definition due to Dwork et al. (2006). We also provide learning generalization guarantees.", "startOffset": 159, "endOffset": 179}], "year": 2017, "abstractText": "This paper addresses the problem of practical privacy-preserving machine learning: how to detect patterns in massive, real-world databases of sensitive personal information, while maintaining the privacy of individuals. Chaudhuri and Monteleoni (2008) recently provided privacy-preserving techniques for learning linear separators via regularized logistic regression. With the goal of handling large databases that may not be linearly separable, we provide privacy-preserving support vector machine algorithms. We address general challenges left open by past work, such as how to release a kernel classifier without releasing any of the training data, and how to tune algorithm parameters in a privacy-preserving manner. We provide general, efficient algorithms for linear and nonlinear kernel SVMs, which guarantee \u01eb-differential privacy, a very strong privacy definition due to Dwork et al. (2006). We also provide learning generalization guarantees. Empirical evaluations reveal promising performance on real and simulated data sets.", "creator": "LaTeX with hyperref package"}}}