{"id": "1002.3086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2010", "title": "Convergence of Bayesian Control Rule", "abstract": "recently, new approaches to decay control cannot arise to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. in particular, it has been shown that minimizing the expected rate from the causal temporal - memory dependencies of the true state leads to a new promising stochastic measure rule called the regression control rule. this work sees the convergence of the bayesian control rule under two sufficient assumptions : boundedness, which is an ergodicity condition ; and dependency, which is an instantiation of the sure - thing principle.", "histories": [["v1", "Tue, 16 Feb 2010 14:14:59 GMT  (76kb)", "http://arxiv.org/abs/1002.3086v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["pedro a ortega", "daniel a braun"], "accepted": false, "id": "1002.3086"}, "pdf": {"name": "1002.3086.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["peortega@dcc.uchile.cl", "dab54@cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 2.\n30 86\nv1 [\ncs .A\nI] 1\n6 Fe\nb 20\n10\nKeywords: Adaptive behavior, Intervention calculus, Bayesian control, Kullback-Leibler-divergence"}, {"heading": "1. Introduction", "text": "When the behavior of a plant under any control signal is fully known, then the designer can choose a controller that produces the desired dynamics. Instances of this problem include hitting a target with a cannon under known weather conditions, solving a maze having its map and controlling a robotic arm in a manufacturing plant. However, when the behavior of the plant is unknown, then the designer faces the problem of adaptive control. For example, shooting the cannon lacking the appropriate measurement equipment, finding the way out of an unknown maze and designing an autonomous robot for Martian exploration. Adaptive control turns out to be far more difficult than its nonadaptive counterpart. Even when the plant dynamics is known to belong to a particular class for which optimal controllers are available, constructing the corresponding optimal adaptive controller is in general\nCopyright 2010 by the authors.\nintractable even for simple toy problems (Duff, 2002). Thus, virtually all of the effort of the research community is centered around the development of tractable approximations.\nRecently, new formulations of the adaptive control problem that are based on the minimization of a relative entropy criterion have attracted the interest of the control and reinforcement learning community. For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009). A similar approach minimizes the deviation of the causal input/outputrelationship of a Bayesian mixture of controllers from the true controller, obtaining an explicit solution called the Bayesian control rule (Ortega & Braun, 2010). This control rule is particularly interesting because it leads to stochastic controllers that infer the optimal controller on-line by combining the plant-specific controllers, implicitly using the uncertainty of the dynamics to trade-off exploration versus exploitation.\nAlthough the Bayesian control rule constitutes a promising approach to adaptive control, there are currently no proofs that guarantee its convergence to the desired policy. The aim of this paper is to develop a set of sufficient conditions of convergence and then to provide a proof. The analysis is limited to the simple case of controllers having a finite amount of modes of operation. Special care has been taken to illustrate the motivation behind the concepts."}, {"heading": "2. Preliminaries", "text": "The exposition is restricted to the case of discrete time with discrete stochastic observations and control signals. Let O and A be two finite sets of symbols, where the former is the set of inputs (observations) and the second the set of outputs (actions). Actions and observations at time t are denoted as at \u2208 A and ot \u2208 O re-\nspectively, and the shorthand a\u2264t := a1, a2, . . . , at and the like are used to simplify the notation of strings. Symbols are underlined to glue them together as in ao\u2264t = a1, o1, a2, o2, . . . , at, ot. It is assumed that the interaction between the controller and the plant proceeds in cycles t = 1, 2, . . . where in cycle t the controller issues action at and the plant responds with an observation ot.\nA controller is defined as a probability distribution P over the input/output (I/O) stream, and it is fully characterized by the conditional probabilities\nP (at|ao<t) and P (ot|ao<tat)\nrepresenting the probabilities of emitting action at and collecting observation ot given the respective I/O history. Similarly, a plant is defined as a probability distribution Q characterized by the conditional probabilities Q(ot|ao<tat) representing the probabilities of emitting observation ot given the I/O history.\nIf the plant is known, i.e. if the conditional probabilities Q(ot|ao<tat) are known, then the designer can build a suitable controller by equating the observation streams as P (ot|ao<tat) = Q(ot|ao<tat) and by defining action probabilities P (at|ao<t) such that the resulting distribution P maximizes a desired utility criterion. In this case P is said to be tailored to Q. In many situations the conditional probabilities P (at|ao<t) will be deterministic, but there are cases (e.g. in repeated games) where the designer might prefer stochastic policies instead.\nIf the plant is unknown then one faces an adaptive control problem. Assume we know that the plant Qm is going to be drawn randomly from a set Q := {Qm}m\u2208M of possible plants indexed by M. Assume further we have available a set of controllers P := {Pm}m\u2208M, where each Pm is tailored to Qm. How can we now construct a controller P such that its behavior is as close as possible to the tailored controller Pm under any realization of Qm \u2208 Q?"}, {"heading": "3. Bayesian Control Rule", "text": "A na\u0308\u0131ve approach would be to minimize the relative entropy of the controller P with respect to the true controller Pm, averaged over all possible values of m. However, this is syntactically incorrect. The important observation made in Ortega & Braun (2010) is that we do not want to minimize the deviation of P from Pm, but the deviation of the causal I/O dependencies in P from the causal I/O dependencies in Pm.\nIntuitively speaking, one does not want to predict actions and observations, but to predict the observations (effect) given actions (causes). More specifically, they propose to minimize a set of (causal) divergences C defined by\nC := lim sup t\u2192\u221e\n\u2211\nm\nP (m)\nt \u2211\n\u03c4=1\nC\u03c4 (1)\nwhere\nC\u03c4 := \u2211\no<\u03c4\nPm(a\u0302o<\u03c4 )C\u03c4 (a\u0302o<\u03c4 )\nC\u03c4 (h) := \u2211\na\u03c4\n\u2211\no\u03c4\nPm(ao\u03c4 |h) log Pm(ao\u03c4 |h) P (ao\u03c4 |h) ,\nand where P (m) is the prior probability of m \u2208 M, a\u0302\u03c4 denotes an intervened (not observed) action at time \u03c4 , and a\u03021, a\u03022, a\u03023, . . . is an arbitrary sequence of intervened actions that gives rise to a particular instantiation of C.\nIn Ortega & Braun (2010), it is shown that the controller P that minimizes C in Equation (1) for any sequence of intervened actions is given by the conditional probabilities\nP (at|a\u0302o<\u03c4 ) := \u2211\nm\nPm(at|ao<\u03c4 )P (m|a\u0302o<\u03c4 )\nP (ot|a\u0302o<\u03c4 ) := \u2211\nm\nPm(ot|ao<\u03c4a\u03c4 )P (m|a\u0302o<\u03c4 ) (2)\nwhere\nP (m|a\u0302o\u2264t) := Pm(ot|ao<tat)P (m|a\u0302o<t) \u2211\nm\u2032 Pm\u2032(ot|ao<tat)P (m\u2032|ao<t) . (3)\nEquations (2) and (3) constitute the Bayesian control rule. This result is obtained by using properties of interventions using causal calculus (Pearl, 2000). It is worth to point out that the resulting controller is fully defined in terms of its constituent controllers in P . It is customary to use the notation\nP (at|m, ao<t) := Pm(at|ao<t) P (ot|m, ao<tat) := Pm(ot|ao<tat),\nthat is, treating the different controllers as \u201chypotheses\u201d of a Bayesian model. In the context of the Bayesian control rule, these \u201cI/O hypotheses\u201d are called operation modes. Note that the resulting control law is in general stochastic."}, {"heading": "4. Policy Diagrams", "text": "A policy diagram is a useful informal tool to analyze the effect of control policies on plants. Figure 1, illustrates an example. One can imagine a plant as a\ncollection of states connected by transitions labeled by I/O symbols. For instance, Figure 1 highlights a state s where taking action a \u2208 A and collecting observation o \u2208 O leads to state s\u2032. In a policy diagram, one abstracts away from the underlying details of the plant\u2019s dynamics, representing sets of states and transitions as enclosed areas similar to a Venn diagram. Choosing a particular policy in a plant amounts to partially controlling the transitions taken in the state space, thereby choosing a subset of the plant\u2019s dynamics. Accordingly, a policy is represented by a subset in state space (enclosed by a directed curve) as illustrated in Figure 1.\nPolicy diagrams are especially useful to analyze the effect of policies on different hypotheses about the plant\u2019s dynamics. A controller that is endowed with a set of operation modes M can be seen as having hypotheses about the plant\u2019s underlying dynamics, given by the observation models P (ot|m, ao<tat), and associated policies, given by the action models P (at|m, ao<t), for all m \u2208 M. For the sake of simplifying the interpretation of policy diagrams, we will assume1 the existence of a state space S and a function T : (A \u00d7 O) \u2192 S mapping I/O histories into states. With this assumption, policies and hypotheses can be seen as conditional probabilities\nP (at|m, s) := P (at|m, ao<t) and P (ot|m, s, at) := P (ot|m, ao<tat)\nrespectively, defining transition probabilities\nP (s\u2032|m, s) = \u2211\nS\u2032\nP (aot|m, s)\nfor a Markov chain in the state space, where s = T (ao<t) and S \u2032 contains the transitions aot such that T (ao\u2264t) = s \u2032."}, {"heading": "5. Divergence Processes", "text": "One of the obvious questions to ask oneself with respect to the Bayesian control rule is whether it con-\n1Note however that no such assumptions are made to obtain the results of this paper.\nverges to the right control law or not. That is, whether P (at|a\u0302ot) \u2192 P (at|m\u2217, ao<t) as t \u2192 \u221e when m\u2217 is the true operation mode, i.e. the operation mode such that P (at|m\u2217, ao<t) = Q(at|ao<t). As will be obvious from the discussion in the rest of this paper, this is in general not true.\nAs it is easily seen from Equation 2, showing convergence amounts to show that the posterior distribution P (m|a\u0302o<t) concentrates its probability mass on a subset of operation modesM\u2217 having essentially the same output stream as m\u2217,\n\u2211\nm\u2208M\nP (at|m, ao<t)P (m|a\u0302o<t)\n\u2248 \u2211\nm\u2208M\u2217\nP (at|m\u2217, ao<t)P (m|a\u0302o<t)\n\u2248 P (at|m\u2217, ao<t).\nHence, understanding the asymptotic behavior of the posterior probabilities\nP (m|a\u0302o\u2264t)\nis the main goal of this paper. In particular, one wants to understand under what conditions these quantities converge to zero. The posterior can be rewritten as\nP (m|a\u0302o\u2264t) = P (a\u0302o\u2264t|m)P (m) \u2211\nm\u2032\u2208M P (a\u0302o\u2264t|m\u2032)P (m\u2032)\n= P (m)\n\u220ft \u03c4=1 P (o\u03c4 |m, ao<\u03c4a\u03c4 )\n\u2211 m\u2032\u2208M P (m \u2032) \u220ft \u03c4=1 P (o\u03c4 |m\u2032, ao<\u03c4a\u03c4 ) .\nIf all the summands but the one with index m\u2217 are dropped from the denominator, one obtains the bound\nP (m|a\u0302o\u2264t) \u2264 ln P (m)\nP (m\u2217)\nt \u220f\n\u03c4=1\nP (o\u03c4 |ao<\u03c4a\u03c4 |m) P (o\u03c4 |ao<\u03c4a\u03c4 |m\u2217) ,\nwhich is valid for all m\u2217 \u2208 M. From this inequality, it is seen that it is convenient to analyze the behavior of the stochastic process\ndt(m \u2217\u2016m) :=\nt \u2211\n\u03c4=1\nln P (o\u03c4 |m\u2217, ao<\u03c4a\u03c4 ) P (o\u03c4 |m, ao<\u03c4a\u03c4 )\nwhich is the divergence process of m from the reference m\u2217. Indeed, if dt(m \u2217\u2016m) \u2192 \u221e as t \u2192 \u221e, then\nlim t\u2192\u221e\nP (m)\nP (m\u2217)\nt \u220f\n\u03c4=1\nP (o\u03c4 |ao<\u03c4a\u03c4 |m) P (o\u03c4 |ao<\u03c4a\u03c4 |m\u2217)\n= lim t\u2192\u221e\nP (m)\nP (m\u2217) \u00b7 e\u2212dt(m\u2217\u2016m) = 0,\nand thus clearly P (m|a\u0302o\u2264t) \u2192 0. Figure 2 illustrates simultaneous realizations of the divergence processes of a controller. Intuitively speaking, these processes provide lower bounds on accumulators of surprise value measured in information units.\nA divergence process is a random walk, i.e. whose value at time t depends on the whole history up to time t\u2212 1. What makes them cumbersome to characterize is the fact that their statistical properties depend on the particular policy that is applied; hence, a given divergence process can have different growth rates depending on the policy (Figure 3). Indeed, the behavior of a divergence process might depend critically on the distribution over actions that is used. For example, it can happen that a divergence process stays stable under one policy, but diverges under another. In the context of the Bayesian control rule this problem is further aggravated, because in each time step, the policy to apply is determined stochastically. More specifically, if m\u2217 is the true operation mode, then dt(m\n\u2217\u2016m) is a random variable that depends on the realization ao\u2264t which is drawn from\nt \u220f\n\u03c4=1\nP (a\u03c4 |m\u03c4 , ao\u2264\u03c4 )P (o\u03c4 |m\u2217, ao\u2264\u03c4a\u03c4 ),\nwhere the m1,m2, . . . ,mt are drawn themselves from P (m1), P (m2|a\u0302o1), . . . , P (mt|a\u0302o<t). To deal with the heterogeneous nature of divergence processes, one can introduce a temporal decomposition that demultiplexes the original process into many sub-processes belonging to unique policies. Let Nt := {1, 2, . . . , t} be the set of time steps up to time t. Let T \u2282 Nt, and let m,m\u2032 \u2208 M. Define a sub-divergence of dt(m\u2016m) as a random variable\ng(m\u2032; T ) := \u2211\n\u03c4\u2208T\nln P (o\u03c4 |m\u2217, ao<\u03c4a\u03c4 ) P (o\u03c4 |m, ao<\u03c4a\u03c4 )\ndrawn from\nPmm\u2032({ao\u03c4}\u03c4\u2208T |{ao\u03c4}\u03c4\u2208T \u2201) := ( \u220f\n\u03c4\u2208T\nP (a\u03c4 |m, ao<\u03c4 ) )( \u220f\n\u03c4\u2208T\nP (o\u03c4 |m\u2032, ao<\u03c4a\u03c4 ) ) ,\nwhere T \u2201 := Nt \\ T and where {ao\u03c4}\u03c4\u2208T \u2201 are given conditions that are kept constant. In this definition, m\u2032 plays the role of the policy that is used to sample the actions in the time steps T . Clearly, any realization of the divergence process dt(m\n\u2217\u2016m) can be decomposed into a sum of sub-divergences, i.e.\ndt(m \u2217\u2016m) =\n\u2211\nm\u2032\ng(m\u2032; Tm\u2032), (4)\nwhere {Tm}m\u2208M forms a partition of Nt. Figure 4 shows an example decomposition.\nThe averages of sub-divergences will play an important ro\u0302le in the analysis. Define the average over all realizations of g(m\u2032; T ) as\nG(m\u2032, T ) := \u2211\n(ao \u03c4 )\u03c4\u2208T\nPmm\u2032({ao\u03c4}\u03c4\u2208T |{ao\u03c4}\u03c4\u2208T \u2201)g(m\u2032; T ).\nNotice that for any \u03c4 \u2208 Nt,\nG(m\u2032; {\u03c4}) = \u2211\nao \u03c4\nP (a\u03c4 |m\u2032, ao<\u03c4 )P (o\u03c4 |m\u2217, ao<\u03c4a\u03c4 )\n\u00b7 ln P (o\u03c4 |m \u2217, ao<\u03c4a\u03c4 )\nP (o\u03c4 |m, ao<\u03c4a\u03c4 ) \u2265 0,\nbecause of Gibbs\u2019 inequality. In particular,\nG(m\u2217; {\u03c4}) = 0.\nClearly, this holds as well for any T \u2282 Nt: \u2200m\u2032 G(m\u2032; T ) \u2265 0,\nG(m\u2217; T ) = 0. (5)"}, {"heading": "6. Boundedness", "text": "In general, a divergence process is very complex: virtually all the classes of distributions that are of interest in control go well beyond i.i.d. and stationary processes. This increased complexity can jeopardize the analytic tractability of the divergence process, i.e. such that no predictions about its asymptotic behavior can be made anymore. More specifically, if the growth rates of the divergence processes vary too much from realization to realization, then the posterior distribution over operation modes can vary qualitatively between realizations. Hence, one needs to impose a stability requirement akin to ergodicity to limit the class of possible divergence-processes to a class that is analytically tractable. In the light of this insight, the following property is introduced.\nA divergence process dt(m \u2217\u2016m) is said to be bounded in M iff for any \u03b4 > 0, there is a C \u2265 0, such that for all m\u2032 \u2208 M, all t and all T \u2282 Nt\n\u2223 \u2223 \u2223 g(m\u2032; T )\u2212G(m\u2032; T ) \u2223 \u2223 \u2223 \u2264 C\nwith probability \u2265 1\u2212 \u03b4. Figure 5 illustrates this property. Boundedness is the key property that is going to be used to construct the results of this paper. The first important result is that the posterior probability of the true operation mode is bounded from below.\nTheorem 1. Let the set of operation modes of a controller be such that for all m \u2208 M the divergence process dt(m\n\u2217\u2016m) is bounded. Then, for any \u03b4 > 0, there is a \u03bb > 0, such that for all t \u2208 N,\nP (m\u2217|a\u0302o\u2264t) \u2265 \u03bb\n|M| with probability \u2265 1\u2212 \u03b4.\nProof. As has been pointed out in (4), a particular realization of the divergence process dt(m\n\u2217\u2016m) can be decomposed as\ndt(m \u2217\u2016m) =\n\u2211\nm\u2032\ngm(m \u2032; Tm\u2032),\nwhere the gm(m \u2032; Tm\u2032) are sub-divergences of dt(m \u2217\u2016m) and the Tm\u2032 form a partition of Nt. However, since dt(m \u2217\u2016m) is bounded in M, one has for all \u03b4\u2032 > 0, there is a C(m) \u2265 0, such that for all m\u2032 \u2208 M, all t \u2208 Nt and all T \u2282 Nt, the inequality\n\u2223 \u2223 \u2223 gm(m \u2032; Tm\u2032)\u2212Gm(m\u2032; Tm\u2032) \u2223 \u2223 \u2223 \u2264 C(m)\nholds with probability \u2265 1\u2212 \u03b4\u2032. However, due to (5),\nGm(m \u2032; Tm\u2032) \u2265 0\nfor all m\u2032 \u2208 M. Thus,\ngm(m \u2032; Tm\u2032) \u2265 \u2212C(m).\nIf all the previous inequalities hold simultaneously then the divergence process can be bounded as well. That is, the inequality\ndt(m \u2217\u2016m) \u2265 \u2212MC(m) (6)\nholds with probability \u2265 (1 \u2212 \u03b4\u2032)M where M := |M|. Choose\n\u03b2(m) := max{0, ln P (m)P (m\u2217)}.\nSince 0 \u2265 ln P (m)P (m\u2217)\u2212\u03b2(m), it can be added to the right hand side of (6). Using the definition of dt(m\n\u2217\u2016m), taking the exponential and rearranging the terms one obtains\nP (m\u2217) t \u220f\n\u03c4=1\nP (o\u03c4 |m\u2217, ao<\u03c4a\u03c4 )\n\u2265 e\u2212\u03b1(m)P (m) t \u220f\n\u03c4=1\nP (o\u03c4 |m\u2217, ao<\u03c4a\u03c4 )\nwhere \u03b1(m) := MC(m) + \u03b2(m) \u2265 0. Identifying the posterior probabilities of m\u2217 and m by dividing both sides by the normalizing constant yields the inequality\nP (m\u2217|a\u0302o\u2264t) \u2265 e\u2212\u03b1(m)P (m|a\u0302o\u2264t).\nThis inequality holds simultaneously for all m \u2208 M with probability \u2265 (1 \u2212 \u03b4\u2032)M2 and in particular for \u03bb := minm{e\u2212\u03b1(m)}, that is,\nP (m\u2217|a\u0302o\u2264t) \u2265 \u03bbP (m|a\u0302o\u2264t).\nBut since this is valid for any m \u2208 M, and because maxm{P (m|a\u0302o\u2264t)} \u2265 1M , one gets\nP (m\u2217|a\u0302o\u2264t) \u2265 \u03bb\nM ,\nwith probability \u2265 1\u2212 \u03b4 for arbitrary \u03b4 > 0 related to \u03b4\u2032 through the equation \u03b4\u2032 := 1\u2212 M2 \u221a 1\u2212 \u03b4."}, {"heading": "7. Core", "text": "If one wants to identify the operation modes whose posterior probabilities vanish, then it is not enough to characterize them as those whose hypothesis does not match the true hypothesis. Figure 6 illustrates this problem. Here, three hypotheses along with their associated policies are shown. H1 and H2 share the prediction made for region A but differ in region B. Hypothesis H3 differs everywhere from the others. Assume H1 is true. As long as we apply policy P2, hypothesis H3 will make wrong predictions and thus its divergence process will diverge as expected. However, no evidence against H2 will be accumulated. It is only when we apply policy P1 for long enough time that the controller will eventually enter region B and hence accumulate counter-evidence for H2.\nBut what does \u201clong enough\u201d mean? If P1 is executed only for a short period, then the controller risks not visiting the disambiguating region. But unfortunately, neither the right policy nor the right length of the period to run it are known beforehand. Hence, the controller needs a clever time-allocating strategy to test\nall policies for all finite time intervals. This motivates following definition.\nThe core of an operation mode m\u2217, denoted as [m\u2217], is the subset of M containing operation modes behaving like m\u2217 under its policy. More formally, an operation mode m /\u2208 [m\u2217] (i.e. is not in the core) iff for any C \u2265 0, \u03b4, \u03be > 0, there is a t0 \u2208 N, such that for all t \u2265 t0, G(m\u2217; T ) \u2265 C with probability \u2265 1 \u2212 \u03b4, where G(m\u2217; T ) is a subdivergence of dt(m\n\u2217\u2016m), and Pr{\u03c4 \u2208 T } \u2265 \u03be for all \u03c4 \u2208 Nt. In other words, if the controller was to apply m\u2217\u2019s policy in each time step with probability at least \u03be, and under this strategy the expected sub-divergence G(m\u2217; T ) of dt(m\u2217\u2016m) grows unboundedly, then m is not in the core of m\u2217. Note that demanding a strictly positive probability of execution in each time step guarantees that controller will run m\u2217 for all possible finite time-intervals. As the following theorem shows, the posterior probabilities of the operation modes that are not in the core vanish almost surely.\nTheorem 2. Let the set of operation modes of a controller be such that for all m \u2208 M the divergence process dt(m\n\u2217\u2016m) is bounded. Then, if m /\u2208 [m\u2217], then P (m|a\u0302o\u2264t) \u2192 0 as t \u2192 \u221e almost surely.\nProof. The divergence process dt(m \u2217\u2016m) can be decomposed into a sum of sub-divergences (see Equation 4)\ndt(m \u2217\u2016m) =\n\u2211\nm\u2032\ng(m\u2032; Tm\u2032). (7)\nFurthermore, for every m\u2032 \u2208 M, one has that for all \u03b4 > 0, there is a C \u2265 0, such that for all t \u2208 N and for all T \u2282 Nt\n\u2223 \u2223 \u2223 g(m\u2032; T )\u2212G(m\u2032; T ) \u2223 \u2223 \u2223 \u2264 C(m)\nwith probability \u2265 1\u2212 \u03b4\u2032. Applying this bound to the summands in (7) yields the lower bound\n\u2211\nm\u2032\ng(m\u2032; Tm\u2032) \u2265 \u2211\nm\u2032\n( G(m\u2032; Tm\u2032)\u2212 C(m) )\nwhich holds with probability \u2265 (1\u2212\u03b4\u2032)M , where M := |M|. Due to Inequality 5, one has that for allm\u2032 6= m\u2217, G(m\u2032; Tm\u2032) \u2265 0. Hence,\n\u2211\nm\u2032\n( G(m\u2032; Tm\u2032)\u2212 C(m) ) \u2265 G(m\u2217; Tm\u2217)\u2212MC\nwhere C := maxm{C(m)}. The members of the set Tm\u2217 are determined stochastically; more specifically,\nthe ith member is included into Tm\u2217 with probability P (m\u2217|a\u0302o\u2264i). But since m /\u2208 [m\u2217], one has that G(m\u2217; Tm\u2217) \u2192 \u221e as t \u2192 \u221e with probability \u2265 1\u2212 \u03b4\u2032 for arbitrarily chosen \u03b4\u2032 > 0. This implies that\nlim t\u2192\u221e\ndt(m \u2217\u2016m) \u2265 lim t\u2192\u221e G(m\u2217; Tm\u2217)\u2212MC \u0580 \u221e\nwith probability \u2265 1\u2212 \u03b4, where \u03b4 > 0 is arbitrary and related to \u03b4\u2032 as \u03b4 = 1\u2212 (1\u2212 \u03b4\u2032)M+1. Using this result in the upper bound for posterior probabilities yields the final result\n0 \u2264 lim t\u2192\u221e P (m|a\u0302o\u2264t) \u2264 limt\u2192\u221e P (m) P (m\u2217) e\u2212dt(m \u2217\u2016m) = 0."}, {"heading": "8. Consistency", "text": "Even if an operation mode m is in the core of m\u2217, i.e. given that m is essentially indistinguishable from m\u2217 under m\u2217\u2019s control, it can still happen that m\u2217 and m have different policies. Figure 7 shows an example of this. The hypotheses H1 and H2 share region A but differ in region B. In addition, both operation modes have their policies P1 and P2 respectively confined to region A. Note that both operation modes are in the core of each other. However, their policies are different. This means that it is unclear whether multiplexing the policies in time will ever disambiguate the two hypotheses. This is undesirable, as it could impede the convergence to the right control law.\nThus, it is clear that one needs to impose further restrictions on the mapping of hypotheses into policies. With respect to Figure 7, one can make the following observations:\n1. Both operation modes have policies that select subsets of region A. Therefore, the dynamics in A are preferred over the dynamics in B.\n2. Knowing that the dynamics in A are preferred over the dynamics in B allows to drop region B from the analysis when choosing a policy.\n3. Since both hypotheses agree in regionA, they have to choose the same policy in order to be consistent\nin their selection criterion.\nThis motivates the following definition. An operation mode m is said to be consistent with m\u2217 iff m \u2208 [m\u2217] implies that for all \u03b5 < 0, there is a t0, such that for all t \u2265 t0 and all ao<tat,\n\u2223 \u2223 \u2223 P (at|m\u2217, ao\u2264t)\u2212 P (at|m\u2217, ao\u2264t) \u2223 \u2223 \u2223 < \u03b5.\nIn other words, if m is in the core of m\u2217, then m\u2019s policy has to converge to m\u2217\u2019s policy. Intuitively, this property parallels the well-known sure-thing principle of expected utility theory (Savage, 1954). The following theorem shows that consistency is a sufficient condition for convergence to the right control law.\nTheorem 3. Let the set of operation modes of a controller be such that: for all m \u2208 M the divergence process dt(m\n\u2217\u2016m) is bounded; and for all m,m\u2032 \u2208 M, m is consistent with m\u2032. Then,\nP (at|a\u0302o\u2264t) \u2192 P (at|m\u2217, ao\u2264t) almost surely as t \u2192 \u221e.\nProof. We will use the abbreviations pm(t) := P (at|m, a\u0302o\u2264t) and wm(t) := P (m|ao\u2264t). Decompose P (at|a\u0302o\u2264t) as\nP (at|a\u0302o\u2264t) = \u2211\nm/\u2208[m\u2217]\npm(t)wm(t) + \u2211\nm\u2208[m\u2217]\npm(t)wm(t).\n(8)\nThe first sum on the right-hand side is lower-bounded by zero and upper-bounded by\n\u2211\nm/\u2208[m\u2217]\npm(t)wm(t) \u2264 \u2211\nm/\u2208[m\u2217]\nwm(t)\nbecause pm(t) \u2264 1. Due to Theorem 2, wm(t) \u2192 0 as t \u2192 \u221e almost surely. Given \u03b5\u2032 > 0 and \u03b4\u2032 > 0, let t0(m) be the time such that for all t \u2265 t0(m), wm(t) < \u03b5\n\u2032. Choosing t0 := maxm{t0(m)}, the previous inequality holds for all m and t \u2265 t0 simultaneously with probability \u2265 (1\u2212 \u03b4\u2032)M . Hence,\n\u2211\nm/\u2208[m\u2217]\npm(t)wm(t) \u2264 \u2211\nm/\u2208[m\u2217]\nwm(t) < M\u03b5 \u2032. (9)\nTo bound the second sum in (8) one proceeds as follows. For every member m \u2208 [m\u2217], one has that pm(t) \u2192 pm\u2217(t) as t \u2192 \u221e. Hence, following a similar construction as above, one can choose t\u20320 such that for all t \u2265 t\u20320 and m \u2208 [m\u2217], the inequalities\n\u2223 \u2223 \u2223 pm(t)\u2212 pm\u2217(t) \u2223 \u2223 \u2223 < \u03b5\u2032\nhold simultaneously for the precision \u03b5\u2032 > 0. Applying this to the first sum yields the bounds\n\u2211\nm\u2208[m\u2217]\n( pm\u2217(t)\u2212 \u03b5\u2032 ) wm(t)\n\u2264 \u2211\nm\u2208[m\u2217]\npm(t)wm(t)\n\u2264 \u2211\nm\u2208[m\u2217]\n( pm\u2217(t) + \u03b5 \u2032 ) wm(t).\nHere ( pm\u2217(t) \u00b1 \u03b5\u2032 )\nare multiplicative constants that can be placed in front of the sum. Note that\n1 \u2265 \u2211\nm\u2208[m\u2217]\nwm(t) = 1\u2212 \u2211\nm/\u2208[m\u2217]\nwm(t) > 1\u2212 \u03b5.\nDiligently using of the above inequalities allows simplifying the lower and upper bounds respectively:\n( pm\u2217(t)\u2212 \u03b5\u2032 )\n\u2211\nm\u2208[m\u2217]\nwm(t) > pm\u2217(t)(1 \u2212 \u03b5\u2032)\u2212 \u03b5\u2032\n\u2265 pm\u2217(t)\u2212 2\u03b5\u2032, (\npm\u2217(t) + \u03b5 \u2032 )\n\u2211\nm\u2208[m\u2217]\nwm(t) \u2264 pm\u2217(t) + \u03b5\u2032\n< pm\u2217(t) + 2\u03b5 \u2032.\n(10)\nCombining the inequalities (9) and (10) in (8) yields the final result:\n\u2223 \u2223 \u2223 P (at|a\u0302o\u2264t)\u2212 pm\u2217(t) \u2223 \u2223 \u2223 < 3\u03b5\u2032 = \u03b5,\nwhich holds with probability\u2265 1\u2212\u03b4 for arbitrary \u03b4 > 0 related to \u03b4\u2032 as \u03b4\u2032 = 1 \u2212 M \u221a 1\u2212 \u03b4 and arbitrary precision \u03b5."}, {"heading": "9. Summary and Conclusions", "text": "The Bayesian control rule constitutes a promising approach to adaptive control based on the minimization of the relative entropy of the causal I/O distribution of a mixture controller from the true controller. In this work, a proof of convergence of the Bayesian control rule to the true controller is provided.\nAnalyzing the asymptotic behavior of a controllerplant dynamics could be perceived as a difficult problem that involves the consideration of domain-specific assumptions. Here it is shown that this is not the case: the asymptotic analysis can be recast as the study of concurrent divergence processes that determine the evolution of the posterior probabilities over operation modes, thus abstracting away from the details of the classes of I/O distributions. In particular,\nif the set of operation modes is finite, then two extra assumptions are sufficient to prove convergence. The first one, boundedness, imposes the stability of divergence processes under the partial influence of the policies contained within the set of operation modes. This condition can be regarded as an ergodicity assumption. The second one, consistency, requires that if a hypothesis makes the same predictions as another hypothesis within its most relevant subset of dynamics, then both hypotheses share the same policy. This relevance is formalized as the core of an operation mode.\nThe concepts and proof strategies developed in this work are appealing due to their intuitive interpretation and formal simplicity. Most importantly, they strengthen the intuition about potential pitfalls that arise in the context of controller design. The approach presented in this work can also be considered as a guide for possible extensions to infinite sets of operation modes. For example, one can think of partitioning a continuous space of operation modes into \u201cessentially different\u201d regions where representative operation modes subsume their neighborhoods (Gru\u0308nwald, 2007).\nFinally, convergence proofs play a crucial ro\u0302le in the mathematical justification of any new theory of control. Hopefully, this proof will contribute to establish relative entropy control theories as solid alternative formulations to the problem of adaptive control."}], "references": [{"title": "Optimal learning: computational procedures for bayes-adaptive markov decision processes", "author": ["M.O. Duff"], "venue": "PhD thesis,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "The Minimum Description Length Principle", "author": ["P. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald", "year": 2007}, {"title": "Optimal control as a graphical model inference problem", "author": ["B. Kappen", "V. Gomez", "M. Opper"], "venue": null, "citeRegEx": "Kappen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2009}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "In Proceedings of the third conference on general artificial intelligence,", "citeRegEx": "Ortega and Braun,? \\Q2010\\E", "shortCiteRegEx": "Ortega and Braun", "year": 2010}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "The Foundations of Statistics", "author": ["L.J. Savage"], "venue": null, "citeRegEx": "Savage,? \\Q1954\\E", "shortCiteRegEx": "Savage", "year": 1954}, {"title": "Linearly solvable markov decision problems", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2006\\E", "shortCiteRegEx": "Todorov", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["E. Todorov"], "venue": "Proceedings of the National Academy of Sciences U.S.A.,", "citeRegEx": "Todorov,? \\Q2009\\E", "shortCiteRegEx": "Todorov", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "intractable even for simple toy problems (Duff, 2002).", "startOffset": 41, "endOffset": 53}, {"referenceID": 6, "context": "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).", "startOffset": 255, "endOffset": 297}, {"referenceID": 2, "context": "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).", "startOffset": 255, "endOffset": 297}, {"referenceID": 4, "context": "This result is obtained by using properties of interventions using causal calculus (Pearl, 2000).", "startOffset": 83, "endOffset": 96}, {"referenceID": 5, "context": "Intuitively, this property parallels the well-known sure-thing principle of expected utility theory (Savage, 1954).", "startOffset": 100, "endOffset": 114}], "year": 2010, "abstractText": "Recently, new approaches to adaptive control have sought to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. In particular, it has been shown that minimizing the expected deviation from the causal input-output dependencies of the true plant leads to a new promising stochastic control rule called the Bayesian control rule. This work proves the convergence of the Bayesian control rule under two sufficient assumptions: boundedness, which is an ergodicity condition; and consistency, which is an instantiation of the surething principle.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}