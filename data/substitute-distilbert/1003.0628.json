{"id": "1003.0628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", "abstract": "homogeneous documents are complex high dimensional objects. to effectively visualize such data it is important to reduce its dimensionality and visualize termed low dimensional embedding as a 2 - d or 3 - d scatter plot. alongside this paper we incorporated dimensionality optimization methods that draw upon domain knowledge in order to achieve a reasonably low dimensional embedding and visualization of documents. we demonstrated the use involves geometries specified manually by an expert, geometries derived automatically from corpus tables, and geometries computed from linguistic resources.", "histories": [["v1", "Tue, 2 Mar 2010 16:52:32 GMT  (156kb)", "http://arxiv.org/abs/1003.0628v1", "13 pages, 15 figures"]], "COMMENTS": "13 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yi mao", "krishnakumar balasubramanian", "guy lebanon"], "accepted": false, "id": "1003.0628"}, "pdf": {"name": "1003.0628.pdf", "metadata": {"source": "CRF", "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", "authors": ["Yi Mao", "Krishnakumar Balasubramanian"], "emails": ["yi.mao@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 3.\n06 28\nv1 [\ncs .C\nL ]"}, {"heading": "1 Introduction", "text": "Visual document analysis systems such as IN-SPIRE have demonstrated their applicability in managing large text corpora, identifying topics within a document and quickly identifying a set of relevant documents by visual exploration. The success of such systems depends on several factors with the most important one being the quality of the dimensionality reduction. This is obvious as visual exploration can be made possible only when the dimensionality reduction preserves the structure of the original space, i.e., documents that convey similar topics are mapped to nearby regions in the low dimensional 2D or 3D space.\nStandard dimensionality reduction methods such as principal component analysis (PCA), locally linear embedding (LLE) [19], or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors such as bag of words or tf vectors. An obvious drawback of such an approach is that such methods ignore the textual nature of documents and instead consider the vocabulary words V = {v1, . . . , vn} as abstract orthogonal dimensions that are unrelated to each other. In this paper we introduce a general technique for incorporating domain knowledge into dimensionality reduction for text documents. In contrast to several recent alternatives, our technique is completely unsupervised and does not require any labeled data.\nWe focus on the following type of non-Euclidean geometry where the distance between document x and y is defined as\ndT (x, y) = \u221a (x\u2212 y)\u22a4T (x\u2212 y). (1)\n\u2217To whom correspondence should be addressed. Email: yi.mao@cc.gatech.edu\nHere T \u2208 Rn\u00d7n is a symmetric positive semidefinite matrix, and we assume that documents x, y are represented as term-frequency (tf) column vectors. Since T can always be written as H\u22a4H for some matrix H \u2208 Rm\u00d7n where m \u2264 n, an equivalent but sometimes more intuitive interpretation of (1) is to compose the mapping x 7\u2192 Hx with the Euclidean geometry\ndT (x, y) = dI(Hx,Hy) = \u2016Hx\u2212Hy\u20162. (2)\nWe can view T as encoding the semantic similarity between pairs of words. When H is a square matrix, it smoothes the tf vector x by mapping observed words to unobserved related words. Alternatively, if m, the number of rows of H , equals to the number of existing topics, the mapping can be viewed as describing a document as a mixture of such topics. Therefore, the geometry realized by (1) or (2) may be used to derive novel dimensionality reduction methods that are customized to text in general and to specific text domains in particular. The main challenge is to obtain the matrices H or T that describe the relationship among vocabulary words appropriately.\nWe consider obtaining H or T using three general types of domain knowledge. The first corresponds to manual specification of the semantic relationship among words. The second corresponds to analyzing the relationship between different words using corpus statistics. The third corresponds to knowledge obtained from linguistic resources. In some cases, T might be easier be obtain than H . Whether to specify H directly or indirectly through T depends on the knowledge type and is discussed in detail in Section 4.\nWe investigate the performance of the proposed dimensionality reduction methods for three text domains: sentiment visualization for movie reviews, topic visualization for newsgroup discussion articles, and visual exploration of ACL papers. In each of these domains we compare several different domain dependent geometries and show that they outperform popular state-of-the-art techniques. Generally speaking, we observe that geometries obtained from corpus statistics are superior to manually constructed geometries and to geometries derived from standard linguistic resources such as Word-Net. We also demonstrate effective ways to combine different types of domain knowledge and show how such combinations significantly outperform any of the domain knowledge types in isolation. All the techniques mentioned in this paper are unsupervised, making use of labels only for evaluation purposes."}, {"heading": "2 Related Work", "text": "Despite having a long history, dimensionality reduction is still an active research area. Broadly speaking, dimensionality reduction methods may be classified to projective or manifold based [3]. The first projects data onto a linear subspace (e.g., PCA and canonical correlation analysis) while the second traces a low dimensional nonlinear manifold on which data lies (e.g., multidimensional scaling, isomap, Laplacian eigenmaps, LLE and t-SNE). The use of dimensionality reduction for text documents is surveyed by [21] who also describe current homeland security applications.\nDimensionality reduction is closely related to metric learning. [23] is one of the earliest papers that focus on learning metrics of the form (1). In particular they try to learn matrix T in an supervised way by expressing relationships between pairs of samples. Representative paper on unsupervised metric learning for text documents is [14] which learns a metric on the simplex based on the geometric volume of the data.\nWe focus in this paper on visualizing a corpus of text documents using a 2-D scatter plot. While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist. It is conceivable that the techniques developed in this paper may be ported to enhance these alternative visualization methods as well."}, {"heading": "3 Non-Euclidean Geometries", "text": "Dimensionality reduction methods often assume, either explicitly or implicitly, Euclidean geometry. For example, PCA minimizes the reconstruction error for a family of Euclidean projections. LLE uses the Euclidean geometry as a local metric. t-SNE is based on a neighborhood structure, determined again by the Euclidean geometry. The generic nature of the Euclidean geometry makes it somewhat unsuitable for visualizing text documents as the relationship between words conflicts with Euclidean orthogonality. We consider in this paper several alternative geometries of the form (1) or (2) which are more suited for text and compare their effectiveness in visualizing documents.\nAs mentioned in Section 1 H smoothes the tf vector x by mapping the observed words into observed and non-observed (but related) words. Decomposing H = R \u00d7 D into a product of a Markov morphism1 R \u2208 Rn\u00d7n and a non-negative diagonal matrix D \u2208 Rn\u00d7n, we see that the matrix H plays two roles: blending related vocabulary words (realized by R) and emphasizing some words over others (realized by D). The j-th column of R stochastically smoothes word wj into related words wi where the amount of smoothing is determined by Rij . Intuitively Rij is high if wi, wj are similar and 0 if they are unrelated. The role of the matrix D is to emphasize some words over others. For example, Dii values corresponding to content words may be higher than values corresponding to stop words or less important words.\nIt is instructive to examine the matrices R and D in the case where the vocabulary words cluster according to some meaningful way. Figure 1 gives an example where vocabulary words form two clusters. The matrix R may become block-diagonal with non-zero elements occupying diagonal blocks representing within-cluster word blending, i.e., words within each cluster are interchangeable to some degree. The diagonal matrix D represents the importance of different clusters. The word clusters are formed with respect to the visualization task at hand. For example, in the case of visualizing the sentiment content of reviews we may have word clusters labeled as \u201cpositive sentiment words\u201d, \u201cnegative sentiment words\u201d and \u201cobjective words\u201d. In general, the matrices R,D may be defined based on the language or may be specific to document domain and visualization purpose. It is reasonable to expect that the words emphasized for visualizing topics in news stories might be different than the words emphasized for visualizing writing styles or sentiment content.\nThe above discussion remains valid when H \u2208 Rm\u00d7n for m being the number of topics in the set of documents. In fact, the j-th column of R now stochastically maps word j to related topics i.\n1a non-negative matrix whose columns sum to 1 [4]\nApplying the geometry (1) or (2) to dimensionality reduction is easily accomplished by first mapping documents x 7\u2192 Hx and proceeding with standard dimensionality reduction techniques such as PCA or t-SNE. The resulting dimensionality reduction is Euclidean in the transformed space but non-Euclidean in the original space.\nIn many cases, the vocabulary contains tens of thousands of words or more making the specification of the matrices R,D a complicated and error prone task. We describe in the next section several techniques for specifying R,D in practice. Note, even if in some cases R,D are obtained indirectly by decomposing T into H\u22a4H , the discussion of the role of R,D is still of importance as the matrices can be used to come up word clusters whose quality may be evaluated manually based on the visualization task at hand."}, {"heading": "4 Domain Knowledge", "text": "We consider four different techniques for obtaining the transformation matrix H . Each technique approaches in one of two ways: (1) separately obtain the column stochastic matrix R which blends different words and the diagonal matrix D which determines the importance of each word; (2) estimate the semantic similarity matrix T and decompose it as H\u22a4H . To ensure that H is a non-negative matrix for it to be interpretable, non-negativity matrix factorization techniques such as the one in [7] may be applied."}, {"heading": "Method A: Manual Specification", "text": "In this method, an expert user manually specifies the matrices (R,D) based on his assessment of the relationship among the vocabulary words. More specifically, the user first constructs a hierarchical word clustering that may depend on the current text domain, and then specifies the matrices (R,D) with respect to the cluster membership of the vocabulary.\nDenoting the clusters by C1, . . . , Cr (a partition of {v1, . . . , vn}), the user specifies R by setting the values\nRij \u221d\n{\n\u03c1a, i = j, vi \u2208 Ca \u03c1ab, i 6= j, vi \u2208 Ca, vj \u2208 Cb (3)\nappropriately. The values \u03c1a and \u03c1aa together determine the blending of words from the same cluster. The value \u03c1ab, a 6= b captures the semantic similarity between two clusters. That value may be either computed manually for each pair of clusters or automatically from the clustering hierarchy (for example \u03c1ab can be the minimal number of tree edges traversed to move from a to b). The matrix R is then normalized appropriately to form a column stochastic matrix. The matrix D is specified by setting the values\nDii = da, vi \u2208 Ca (4)\nwhere da may indicate the importance of word cluster Ca to the current visualization task. We emphasize that as with the rest of the methods in this paper, the manual specification is done without access to labeled data.\nSince manual clustering assumes some form of human intervention, it is reasonable to also consider cases where the user specifies (R,D) in an interactive manner. That is, the expert specifies an initial clustering of words and (R,D), views the resulting visualization and adjusts his selection interactively until he is satisfied."}, {"heading": "Method B: Contextual Diffusion", "text": "An alternative technique which performs substantially better is to consider a transformation based on the similarity between the contextual distributions of the vocabulary words. The contextual distribution of word v is defined as\nqv(w) = p(w appears in x|v appears in x) (5)\nwhere x is a randomly drawn document. In other words qv is the distribution governing the words appearing in the context of word v.\nA natural similarity measure between distributions is the Fisher diffusion kernel proposed by [13]. Applied to contextual distributions as in [6] we arrive at the following similarity matrix (where c > 0)\nT (u, v) = exp\n(\n\u2212c arccos2\n(\n\u2211\nw\n\u221a\nqu(w)qv(w)\n))\n.\nIntuitively, the word u will be translated or diffused into v depending on the geometric diffusion between the distributions of likely contexts.\nWe use the following formula to estimate the contextual distribution from a corpus of documents\nqw(u) = \u2211\nx\u2032\np(u, x\u2032|w) = \u2211\nx\u2032\np(u|x\u2032, w)p(x\u2032|w)\n= \u2211\nx\u2032\ntf(u, x\u2032) tf(w, x\u2032) \u2211\nx\u2032\u2032 tf(w, x \u2032\u2032)\n(6)\n=\n(\n1 \u2211\nx\u2032 tf(w, x \u2032)\n)\n(\n\u2211\nx\u2032\ntf(u, x\u2032)tf(w, x\u2032)\n)\nwhere tf(w, x) is the number of times word w appears in document x. The contextual distribution qw or the diffusion matrix T above may be computed in an unsupervised manner without need for labels.\nMethod C: Web n-Grams\nThe contextual distribution method above may be computed based on a large collection of text documents such as the Reuters RCV1 dataset. The estimation accuracy of the contextual distribution increases with the number of documents which may not be as large as required. An alternative is to estimate the contextual distributions qv from the entire n-gram content of the web. Taking advantage of the publicly available Google n-gram dataset2 we can leverage the massive size of the web to construct the similarity matrix T . More specifically, we compute the contextual distribution by altering (6) to account for the proportion of times two words appear together within the n-grams (we used n = 3 in our experiments)."}, {"heading": "Method D: Word-Net", "text": "The last method we consider uses Word-Net, a standard linguistic resource, to specify the matrix T in (1). This is similar to manual specification (method A) in that it builds on expert knowledge rather than corpus\n2The Google n-gram dataset contains n-gram counts (n \u2264 5) obtained from Google based on processing over a trillion words of running text.\nstatistics. In contrast to method A, however, Word-Net is a carefully built resource containing more accurate and comprehensive linguistic information such as synonyms, hyponyms and holonyms. On the other hand, its generality puts it at a disadvantage as method A may be used to construct a geometry suited to a specific text domain.\nWe follow [2] who compare five similarity measures between words based on Word-Net. In our experiments we use Jiang and Conrath\u2019s measure [11] (see also [12])\nTc1,c2 = log p(c1)p(c2)\n2p(lcs(c1, c2))\nas it was shown to outperform the others. Above, lcs stands for the lowest common subsumer, that is, the lowest node in the hierarchy that subsumes (is a hypernym of) both c1 and c2. The quantity p(c) is the probability that a randomly selected word in a corpus is an instance of the synonym set that contains word c."}, {"heading": "Convex Combinations", "text": "In addition to methods A-D which constitute \u201cpure methods\u201d we also consider convex combinations\nH\u2217 = \u2211\ni\n\u03b1iHi \u03b1i \u2265 0, \u2211\ni\n\u03b1i = 1 (7)\nwhere Hi are matrices from methods A-D, and \u03b1 is a non-negative weight vector which sums to 1. Equation 7 allows to combine heterogeneous types of domain knowledge (manually specified such as method A and D and automatically derived such as methods B and C). Doing so leverages their diverse nature and potentially achieving higher performance than each of the methods A-D on its own."}, {"heading": "5 Experiments", "text": "We evaluated methods A-D and the convex combination method by experimenting on two datasets from different domains. The first is the Cornell sentiment scale dataset of movie reviews [17]. The visualization in this case focuses on the sentiment quantity [18]. For simplicity, we only kept documents having sentiment level 1 (very bad) and 4 (very good). Preprocessing included lower-casing, stop words removal, stemming, and selecting the most frequent 2000 words. Alternative preprocessing is possible but should not modify the results much as we focus on comparing alternatives rather than measuring absolute performance. The second text dataset is 20 newsgroups. It consists of newsgroup articles from 20 distinct newsgroups and is meant to demonstrate topic visualization.\nTo measure the dimensionality reduction quality, we display the data as a scatter plot with different data groups (topics, sentiments) displayed with different markers and colors. Our quantitative evaluation is based on the fact that documents belonging to different groups (topics, sentiments) should be spatially separated in the 2-D space. Specifically, we used the following indices to evaluate different reduction methods and geometries.\n(i) The weighted intra-inter measure is a standard clustering quality index that is invariant to non-singular linear transformations of the embedded data. It equals to trS\u22121T SW where SW is the within-cluster scatter matrix, ST = SW +SB is the total scatter matrix, and SB is the between-cluster scatter matrix [8].\n(ii) The Davies Bouldin index is an alternative to (i) that is similarly based on the ratio of within-cluster scatter to between-cluster scatter [5].\n(iii) Classification error rate of a k-NN classifier that applies to data groups in the 2-D embedded space. Despite the fact that we are not interested in classification per se (otherwise we would classify in the original high dimensional space), it is an intuitive and interpretable measure of cluster separation.\n(iv) An alternative to (iii) is to project the embedded data onto a line which is the direction returned by applying Fisher\u2019s linear discriminant analysis [8] to the embedded data. The projected data from each group is fitted to a Gaussian whose separation is used as a proxy for visualization quality. In particular, we summarize the separation of the two Gaussians by measuring the overlap area. While (iii) corresponds to the performance of a k-NN classifier, method (iv) corresponds to the performance of Fisher\u2019s LDA classifier.\nNote that the above methods (i)-(iv) make use of labeled information to evaluate visualization quality. The labeled data, however, is not used during the dimensionality reduction stages justifying their unsupervised behavior.\nThe manual specification of domain knowledge (method A) for the 20 newsgroups domain used matrices R,D that were specified interactively based on the (manually obtained) word clustering in Figure 2. In the case of sentiment data the manual specification consisted of partitioning words into positive, negative or neutral sentiment based on the General Inquirer resource3. The matrix H was completed by assigning large weights (Dii) for negative and positive words and small weights (Dii) to neutral words.\nThe contextual diffusion (method B) was computed from a large external corpus (Reuters RCV1) for the newsgroups domain. For the sentiment domain we used movie reviews authored by other critics. Google n-gram (method C) provided a truly massive scale resource for estimating the contextual diffusion. In the case of Word-Net (method D) we used Ted Pedersen\u2019s implementation of Jiang and Conrath\u2019s similarity\n3http://www.wjh.harvard.edu/\u223cinquirer/\nmeasure4. Note, for method C and D, the resulting matrix H is not domain specific but rather represents general semantic relationships between words.\nIn our experiments below we focused on two dimensionality reduction methods: PCA and t-SNE. PCA is a well known classical method while t-SNE [22] is a recently proposed technique shown to outperform LLE, CCA, MVU, Isomap, and Laplacian eigenmaps. Indeed it is currently considered state-of-the-art for dimensionality reduction for visualization purposes.\nFigures 3 displays qualitative and quantitative evaluation of PCA and t-SNE for the sentiment and newsgroup domains with standard H = I geometry (left column), manual specification (middle column) and contextual diffusion (right column). Generally, we conclude that in both the newsgroup domain and the sentiment domain and both qualitatively and quantitatively (using the numbers in the top two rows), methods A and B perform better than using the original geometry H = I with method B outperforming method A.\nTables 3-1 display two evaluation measures for different types of domain knowledge (see previous section). Table 3 corresponds to the sentiment domain where we conducted separate experiment for four movie critics. Table 1 corresponds to the newsgroup domain where two tasks were considered. The first involving three newsgroups (classes comp.sys.mac.hardware, rec.sports.hockey and talk.politics.mideast) and the second involving four newsgroups (rec.autos, rec.motocycles, rec.sports.baseball and rec.sports.hockey). We conclude from these two figures that the contextual diffusion, Google n-gram, and Word-Net generally outperform the original H = I matrix. The best method varies from task to task but the contextual diffusion and Google n-gram seem to have the strongest performance overall.\nWe also examined convex combinations\n\u03b11HA + \u03b12HB + \u03b13HC + \u03b14HD (8)\nwith \u2211\n\u03b1i = 1 and \u03b1i \u2265 0. Table 2 displays three evaluation measures, the weighted intra-inter measure (i), the Davies-Bouldin index (ii), and the k-NN classifier (k = 5) accuracy on the embedded documents (iii). The beginning of the section provides more information on these measures. The first four rows correspond to the \u201cpure\u201d methods A,B,C,D. The bottom row correspond to a convex combination found by minimizing the unsupervised evaluation measure (ii). Note that the convex combination found also outperforms A, B, C, and D on measure (i) and more impressively on measure (iii) which is a supervised measure that uses labeled data (the search for the optimal combination was done based on (ii) which does not require labeled data).\n4http://wn-similarity.sourceforge.net/"}, {"heading": "H = I 0.6404 0.7465 0.8481 0.8496 0.6559 0.6821 0.6680 0.7410", "text": "We conclude that combining heterogeneous domain knowledge may improve the quality of dimensionality reduction for visualization, and that the search for an improved convex combination may be accomplished without the use of labeled data.\nFinally, we demonstrate the effect of linguistic geometries on a new dataset that consists of all oral papers appearing in ACL 2001 \u2013 2009. For the purpose of manual specification, we obtain 1545 unique words from paper titles, and assign each word relatedness scores for each of the following clusters: morphology/phonology, syntax/parsing, semantics, discourse/dialogue, generation/summarization, machine translation, retrieval/categorization and machine learning. The score takes value from 0 to 2, where 2 represents the most relevant. The score information is then used to generate the transformation matrix R. We also assign each word an importance value ranging from 0 to 3 (larger the value, more important the word). This information is used to generate the diagonal matrix D. Figure 4 shows the projection of all 2009 papers using t-SNE (papers from 2001 to 2008 are used to estimate contextual diffusion). The manual specification improves over no domain knowledge by separating documents into two clusters. By examining the document id, we find that all papers appearing in the smaller cluster correspond to either machine translation or multilingual tasks. Interestingly, the contextual diffusion results in a one-dimensional manifold."}, {"heading": "6 Discussion", "text": "In this paper we introduce several ways of incorporating domain knowledge into dimensionality reduction for visualization of text documents. The novel methods of manual specification, contextual diffusion, Google n-grams, and Word-Net all outperform in general the original assumption H = I . We emphasize that the baseline H = I is the one currently in use in most text visualization systems. The two reduction methods of PCA and t-SNE represent a popular classical technique and a recently proposed technique that outperforms other recent competitors (LLE, Isomap, MVU, CCA, Laplacian eigenmaps).\nOur experiments demonstrate that different domain knowledge methods perform best in different situations. As a generalization, however, the contextual diffusion and Google n-gram methods had the strongest performance. We also demonstrate how combining different types of domain knowledge provides increased effectiveness and that such combinations may be found without the use of labeled data."}], "references": [{"title": "A", "author": ["D. Blei"], "venue": "Ng, , and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Semantic distance in wordnet: An experimental", "author": ["A. Budanitsky", "G. Hirst"], "venue": "application-oriented evaluation of five measures. In NAACL Workshop on WordNet and other Lexical Resources", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Dimension reduction: A guided tour", "author": ["C. Burges"], "venue": "Technical Report MSR-TR-2009-2013, Microsoft Research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical Decision Rules and Optimal Inference", "author": ["N.N. \u010cencov"], "venue": "American Mathematical Society", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1982}, {"title": "A cluster separation measure", "author": ["D.L. Davies", "D.W. Bouldin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 1(4):224\u2013227", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Statistical translation", "author": ["J. Dillon", "Y. Mao", "G. Lebanon", "J. Zhang"], "venue": "heat kernels, and expected distances. In Proc. of the 23rd Conference on Uncertainty in Artificial Intelligence, pages 93\u2013100. AUAI Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["C. Ding", "X. He", "H.D. Simon"], "venue": "Proc. SIAM Data Mining Conf, pages 606\u2013610", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley New York", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Themeriver: Visualizing thematic changes in large document collections", "author": ["S. Havre", "E. Hetzler", "P. Whitney", "L. Nowell"], "venue": "IEEE Transactions on Visualization and Computer Graphics, 8(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "TextTiling: Segmenting text into multi-paragraph subtopic passages", "author": ["M.A. Hearst"], "venue": "Computational Linguistics, 23(1):33\u201364", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "International Conference Research on Computational Linguistics (ROCLING X)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech and Language Processing", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Prentice Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion kernels on statistical manifolds", "author": ["J. Lafferty", "G. Lebanon"], "venue": "Journal of Machine Learning Research, 6:129\u2013163", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):497\u2013508", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequential document visualization", "author": ["Y. Mao", "J. Dillon", "G. Lebanon"], "venue": "IEEE Transactions on Visualization and Computer Graphics, 13(6):1208\u20131215", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "TextArc: Showing word frequency and distribution in text", "author": ["W.B. Paley"], "venue": "IEEE Symposium on Information Visualization Poster Compendium", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "A sentimental eduction: sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proc. of the Association of Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:2323\u20132326", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "InfoCrystal: A visual tool for information retrieval", "author": ["A. Spoerri"], "venue": "Proc. of IEEE Visualization", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "editors", "author": ["J.J. Thomas", "K.A. Cook"], "venue": "Illuminating the Path: The Research and Development Agenda for Visual Analytics. IEEE Computer Society", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Distance metric learning with applications to clustering with side information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russel"], "venue": "Advances in Neural Information Processing Systems, 15", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Standard dimensionality reduction methods such as principal component analysis (PCA), locally linear embedding (LLE) [19], or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors such as bag of words or tf vectors.", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "Standard dimensionality reduction methods such as principal component analysis (PCA), locally linear embedding (LLE) [19], or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors such as bag of words or tf vectors.", "startOffset": 178, "endOffset": 182}, {"referenceID": 2, "context": "Broadly speaking, dimensionality reduction methods may be classified to projective or manifold based [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "The use of dimensionality reduction for text documents is surveyed by [21] who also describe current homeland security applications.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "[23] is one of the earliest papers that focus on learning metrics of the form (1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Representative paper on unsupervised metric learning for text documents is [14] which learns a metric on the simplex based on the geometric volume of the data.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 117, "endOffset": 120}, {"referenceID": 15, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "a non-negative matrix whose columns sum to 1 [4] 3", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "To ensure that H is a non-negative matrix for it to be interpretable, non-negativity matrix factorization techniques such as the one in [7] may be applied.", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "A natural similarity measure between distributions is the Fisher diffusion kernel proposed by [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Applied to contextual distributions as in [6] we arrive at the following similarity matrix (where c > 0) T (u, v) = exp (", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "We follow [2] who compare five similarity measures between words based on Word-Net.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "In our experiments we use Jiang and Conrath\u2019s measure [11] (see also [12]) Tc1,c2 = log p(c1)p(c2) 2p(lcs(c1, c2)) as it was shown to outperform the others.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "In our experiments we use Jiang and Conrath\u2019s measure [11] (see also [12]) Tc1,c2 = log p(c1)p(c2) 2p(lcs(c1, c2)) as it was shown to outperform the others.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "The first is the Cornell sentiment scale dataset of movie reviews [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The visualization in this case focuses on the sentiment quantity [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "It equals to trS T SW where SW is the within-cluster scatter matrix, ST = SW +SB is the total scatter matrix, and SB is the between-cluster scatter matrix [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "(ii) The Davies Bouldin index is an alternative to (i) that is similarly based on the ratio of within-cluster scatter to between-cluster scatter [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "(iv) An alternative to (iii) is to project the embedded data onto a line which is the direction returned by applying Fisher\u2019s linear discriminant analysis [8] to the embedded data.", "startOffset": 155, "endOffset": 158}, {"referenceID": 21, "context": "PCA is a well known classical method while t-SNE [22] is a recently proposed technique shown to outperform LLE, CCA, MVU, Isomap, and Laplacian eigenmaps.", "startOffset": 49, "endOffset": 53}], "year": 2013, "abstractText": "Text documents are complex high dimensional objects. To effectively visualize such data it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents. We consider the use of geometries specified manually by an expert, geometries derived automatically from corpus statistics, and geometries computed from linguistic resources.", "creator": "LaTeX with hyperref package"}}}