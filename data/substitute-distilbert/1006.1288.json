{"id": "1006.1288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2010", "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian approach", "abstract": "the paper addresses the problem of performing a static model parameterized by a five - rank positive semidefinite matrix. the focus lives on the nonlinear nature bordering the frame space and on scalability to high - dimensional problems. the later developments rely on kinetic theory of gradient descent algorithms unique to the riemannian geometry that underlies the set of fixed - rank positive semidefinite transformations. in contrast with recent contributions in the literature, no restrictions are imposed on the range space of the learned matrix. the reconstruction algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. we apply similarly proposed algorithms to the idea of learning a distance function parameterized by a positive semidefinite matrix. good transparency is observed on classical benchmarks.", "histories": [["v1", "Mon, 7 Jun 2010 16:20:02 GMT  (173kb,D)", "http://arxiv.org/abs/1006.1288v1", null], ["v2", "Mon, 31 Jan 2011 09:59:44 GMT  (361kb)", "http://arxiv.org/abs/1006.1288v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gilles meyer", "silvere bonnabel", "rodolphe sepulchre"], "accepted": false, "id": "1006.1288"}, "pdf": {"name": "1006.1288.pdf", "metadata": {"source": "CRF", "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian approach", "authors": ["Gilles Meyer", "Rodolphe Sepulchre"], "emails": ["g.meyer@ulg.ac.be", "silvere.bonnabel@mines-paristech.fr", "r.sepulchre@ulg.ac.be"], "sections": [{"heading": null, "text": "Keywords: linear regression, positive semidefinite matrices, low-rank approximation, Riemannian geometry, gradient-based learning"}, {"heading": "1. Introduction", "text": "A fundamental problem of machine learning is the learning of a distance between data samples. When the distance can be written as a quadratic form (either in the data space (Mahalanobis distance) or in a kernel feature space (kernel distance)), the learning problem is a regression problem on the set of positive definite matrices. The regression problem is turned into the minimization of the prediction error, leading to an optimization framework and gradient-based algorithms.\nThe present paper focuses on the nonlinear nature of the search space. The classical framework of gradient-based learning can be generalized provided that the nonlinear search space is equipped with a proper Riemannian geometry. Adopting this general framework,\nar X\niv :1\n00 6.\n12 88\nv1 [\ncs .L\nwe design novel learning algorithms on the space of fixed-rank positive semidefinite matrices, denoted by S+(r, d), where d is the dimension of the matrix, and r is its rank. Learning a parametric model in S+(r, d) amounts to jointly learn a r-dimensional subspace and a quadratic distance in this subspace.\nThe framework is motivated by low-rank learning in large-scale applications. If the data space is of dimension d, the goal is to maintain a linear computational complexity O(d). In contrast to the classical approach of first reducing the dimension of the data and then learning a distance in the reduced space, there is an obvious conceptual advantage to perform the two tasks simultaneously. If this objective can be achieved without increasing the numerical cost of the algorithm, the advantage becomes also practical.\nOur approach makes use of two quotient geometries of the set S+(r, d) that have been recently studied by Journe\u0301e et al. (2010) and Bonnabel and Sepulchre (2009). Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005).\nNot surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d.\nThe use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning.\nMore recently, algorithmic work has focused on scalability in terms of dimensionality and data set size. A natural extension of the previous work on positive definite matrices is thus to consider low-rank positive semidefinite matrices. Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005). This is a significant complexity reduction as the approximation rank r is typically very small compared to the dimension of the problem d.\nExtending the work of Tsuda et al. (2005), Kulis et al. (2009) recently considered the learning of positive semidefinite matrices. The authors consider Bregman divergence measures that enjoy convexity properties and lead to updates that preserve the rank as well as the positive semidefinite property. However, these divergence-based algorithms intrinsically constrain the learning algorithm to a fixed range space. A practical limitation of this approach is that the subspace of the learned matrix is fixed beforehand by the initial condition of the algorithm.\nThe approach proposed in the present paper is in a sense more classical (we just perform a line-search in a Riemannian manifold) but we show how to interpret Bregman divergence\nbased algorithms in our framework. This is potentially a contribution of independent interest since a general convergence theory exists for line-search algorithms on Riemannian manifolds. The generality of the proposed framework is of course motivated by the nonconvex nature of the rank constraint.\nThe paper is organized as follows. Section 2 presents the general optimization framework of Riemannian learning. This framework is then applied to the learning of subspaces (Section 3), positive definite matrices (Section 4) and fixed-rank positive semidefinite matrices (Section 5). The novel proposed algorithms are presented in Section 6. Section 7 discusses the relationship to existing work as well as extensions of the proposed approach. Applications are presented in Section 8 and experimental results are presented in Section 9."}, {"heading": "2. Linear regression on Riemannian spaces", "text": "We consider the following standard regression problem. Given\n(i) data points X, in a linear data space X = Rd\u00d7d,\n(ii) observations y, in a linear output space Y = R, (or Rd),\n(iii) a regression model y\u0302 = y\u0302W(X) parameterized by a matrix W in a search space W,\n(iv) a quadratic loss function `(y\u0302, y) = 12(y\u0302 \u2212 y) 2,\nfind the optimal fit W\u2217 that minimizes the expected cost\nF (W) = EX,y{`(y\u0302, y)} = \u222b `(y\u0302, y) dP (X, y), (1)\nwhere `(y\u0302, y) penalizes the discrepancy between observations and predictions, and P (X, y) is the (unknown) joint probability distribution over data and observation pairs. Although our main interest will be in the scalar model\ny\u0302 = Tr(WX),\nthe theory applies equally to vector data points x \u2208 Rd, y\u0302 = Tr(WxxT ) = xTWx, to a regression model parameterized by a vector w \u2208 Rd, y\u0302 = wTx, or to a vector output space y\u0302 = Wx.\nAs it is generally not possible to compute F (W) explicitly, batch learning algorithms minimize instead the empirical cost\nfn(W) = 1\n2n n\u2211 i=1 (y\u0302i \u2212 yi)2, (2)\nwhich is the average loss computed over a finite number of samples {(Xi, yi)}ni=1. Online learning algorithms (Bottou, 2004) consider possibly infinite sets of samples {(Xt, yt)}t\u22651, received one at a time. At time t, the online learning algorithm minimizes the instantaneous cost\nft(W) = 1\n2 (y\u0302t \u2212 yt)2.\nIn the sequel, we only present online versions of algorithms to shorten the exposition. The single necessary change to convert an online algorithm into its batch counterpart is to perform, at each iteration, the minimization of the empirical cost fn instead of the minimization of the instantaneous cost ft. In the sequel, we denote by f the cost function that is minimized at each iteration.\nOur focus will be on nonlinear search spacesW. We only requireW to have the structure of a Riemannian matrix manifold. Following Absil et al. (2008), an abstract gradient descent algorithm can then be derived based on the update formula\nWt+1 = RWt(\u2212st gradf(Wt)). (3)\nThe gradient gradf(Wt) is an element of the tangent space TWtW. The scalar st > 0 is the step size. The retraction RWt is a mapping from the tangent space TWtW to the Riemannian manifold. Under mild conditions on the retraction R, the classical convergence theory of line-search algorithms in linear spaces generalizes to Riemannian manifolds (see Absil et al., 2008, Chapter 4).\nObserve that the standard (online) learning algorithm for linear regression in Rd,\nwt+1 = wt \u2212 st(wTt xt \u2212 yt)xt, (4)\ncan be interpreted as a particular case of (3) for the linear model y\u0302 = wTx in the linear search space W = Rd. The Euclidean metric turns Rd in a (flat) Riemannian manifold. For a scalar function f : Rd \u2192 R of w, the gradient satisfies\nDf(w)[\u03b4] = \u03b4T gradf(w),\nwhere Df(w)[\u03b4] is the directional derivative of f in the direction \u03b4, and the natural retraction\nRwt(\u2212st gradf(wt)) = wt \u2212 st gradf(wt)\ninduces a line-search along \u201cstraight lines\u201d which are geodesics (that is paths of shortest length) in linear spaces. With f(w) = 12(w\nTx\u2212 y)2, one arrives at (4). This example illustrates that the main ingredients to obtain a concrete algorithm are convenient formulas for the gradient and for the retraction mapping. This paper provides such formulas for three examples of nonlinear matrix search spaces: the Grassmann manifold (Section 3), the cone of positive definite matrices (Section 4), and the set of fixed-rank positive semidefinite matrices (Section 5). Each of those sets will be equipped with quotient Riemannian geometries that provide convenient formulas for the gradient and for the retractions. Line-search algorithms in quotient Riemannian spaces are discussed in detail in the book of Absil et al. (2008). For the readers convenience, basic concepts and notations are also provided in Appendix A."}, {"heading": "3. Linear regression on the Grassmann manifold", "text": "As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework. Let X = Y = Rd, and consider the linear model\ny\u0302 = UUTx\nwith U \u2208 St(r, d) = {U \u2208 Rr\u00d7d s.t. UTU = I}, the Stiefel manifold of r-dimensional orthonormal bases in Rd. The quadratic loss is then\nf(U) = `(y\u0302,x) = 1\n2 \u2016y\u0302 \u2212 x\u201622 =\n1 2 \u2016UUTx\u2212 x\u201622. (5)\nBecause the cost (5) is invariant by orthogonal transformation U 7\u2192 UO, O \u2208 O(r), where O(r) = St(r, r) is the orthogonal group, the search space is in fact a set of equivalence classes\n[U] = {UO s.t. O \u2208 O(r)}.\nThis set is denoted by St(r, d)/O(r). It is a quotient representation of the set of rdimensional subspaces in Rd, that is, the Grassmann manifold Gr(r, d). The quotient geometries of Gr(r, d) have been well studied (Edelman et al., 1998; Absil et al., 2004). The metric\ng[U](\u03be[U], \u03b6[U]) , g\u0304U(\u03be\u0304U, \u03b6\u0304U)\nis induced by the standard metric in Rd\u00d7r,\ng\u0304U(\u22061,\u22062) = Tr(\u2206 T 1 \u22062),\nwhich is invariant along the fibers, that is, equivalence classes. Tangent vectors \u03be[U] at [U] are represented by horizontal tangent vectors \u03be\u0304U at U:\n\u03be\u0304U = \u03a0U\u2206 = (I\u2212UUT )\u2206, \u2206 \u2208 Rd\u00d7r.\nTherefore, the gradient admits the simple horizontal representation\ngradf(U) = \u03a0U gradf\u0304(U), (6)\nwhere gradf\u0304(U) is defined by the identity\nDf\u0304(U)[\u2206] = g\u0304U(\u2206, gradf\u0304(U)).\nA standard retraction in Gr(r, d) is the exponential mapping, that induces a line-search along geodesics. The exponential map has the closed-form expression\nExpU(\u03be\u0304U) = UV cos(\u03a3)V T + Z sin(\u03a3)VT\nwhich is obtained from a singular value decomposition of the horizontal vector \u03be\u0304U = Z\u03a3V T . Following Absil et al. (2004), an alternative convenient retraction in Gr(r, d) is given by\nRU(s\u03be\u0304U) = [U + s\u03be\u0304U] = qf(U + s\u03be\u0304U), (7)\nwhere qf(\u00b7) extracts the orthogonal factor of the QR-decomposition of its argument. With the formulas (6) and (7) applied to the cost function (5), the abstract update (3) becomes Ut+1 = qf(Ut + st(I\u2212UtUt)xtxTt Ut),\nwhich is Oja\u2019s update for subspace tracking (Oja, 1992)."}, {"heading": "4. Linear regression on the cone of positive definite matrices", "text": "The learning of a full-rank positive definite matrix is recast as follows. Let X = Rd\u00d7d and Y = R, and consider the model\ny\u0302 = Tr(WX),\nwith W \u2208 S+(d) = {W \u2208 Rd\u00d7d s.t. W = WT 0}. Since W is symmetric, only the symmetric part of X will contribute to the trace. The previous model is thus equivalent to\ny\u0302 = Tr(WSym(X)),\nwhere Sym(\u00b7) extract the symmetric part of its argument, that is, Sym(B) = (BT + B)/2. The quadratic loss is\nf(W) = `(y\u0302, y) = 1\n2 (Tr(WSym(X))\u2212 y)2.\nThe quotient geometries of S+(d) are rooted in the matrix factorization\nW = GGT , G \u2208 GL(d),\nwhere GL(d) is the set of all invertible d\u00d7d matrices. Because the factorization is invariant by rotation, G 7\u2192 GO, O \u2208 O(d), the search space is once again identified to the quotient\nS+(d) ' GL(d)/O(d),\nwhich represents the set of equivalence classes\n[G] = {GO s.t. O \u2208 O(d)}.\nWe will equip this quotient with two meaningful Riemannian metrics.\n4.1 A flat metric on S+(d)\nThe metric on the quotient GL(d)/O(d):\ng[G](\u03be[G], \u03b6[G]) , g\u0304G(\u03be\u0304G, \u03b6\u0304G),\nis induced by the standard metric in Rd\u00d7d,\ng\u0304G(\u22061,\u22062) = Tr(\u2206 T 1 \u22062), (8)\nwhich is invariant by rotation along the set of equivalence classes. As a consequence, it induces a metric g[G] on S+(d). With this geometry, a tangent vector \u03be[G] at [G] is represented by a horizontal tangent vector \u03be\u0304G at G by\n\u03be\u0304G = Sym(\u2206)G, \u2206 \u2208 Rd\u00d7d.\nThe horizontal gradient of\nf(G) = `(y\u0302, y) = 1\n2 (Tr(GGTSym(X))\u2212 y)2, (9)\nis the unique horizontal vector gradf(G) that satisfies\nDf(G)[\u2206] = g\u0304G(\u2206, gradf(G)).\nElementary computations yield\ngradf(G) = 2(y\u0302 \u2212 y)Sym(X)G.\nSince the metric is flat, geodesics are straight lines and the exponential mapping is\nExpG(\u03be\u0304G) = [G + \u03be\u0304G] = G + \u03be\u0304G.\nThose formulas applied to the cost (9) turns the abstract update (3) into the simple formula\nGt+1 = Gt \u2212 2st(y\u0302t \u2212 yt)Sym(Xt)Gt (10)\nfor an online gradient algorithm and\nGt+1 = Gt \u2212 2st 1\nn n\u2211 i=1 (y\u0302i \u2212 yi)Sym(Xi)Gt (11)\nfor a batch gradient algorithm.\n4.2 The affine-invariant metric on S+(d)\nBecause S+(d) ' GL(d)/O(d) is the quotient of two Lie groups, its (reductive) geometric structure can be further exploited (Faraut and Koranyi, 1994). Indeed the group GL(d) has a natural action on S+(d) via the transformation W 7\u2192 AWAT for any A \u2208 GL(d). The affine-invariant metric admits interesting invariance properties to these transformations. To build such a metric, the metric at identity\ngI(\u03beI, \u03b6I) = Tr(\u03be T I \u03b6I)\ncan be extended to the entire space to satisfy the invariance property\ngI(\u03beI, \u03b6I) = gW(W 1 2 \u03beIW 1 2 ,W 1 2 \u03b6IW 1 2 ) = gW(\u03beW, \u03b6W).\nThe resulting metric on S+(d) is defined by\ngW(\u03beW, \u03b6W) = Tr(\u03beWW \u22121\u03b6WW \u22121). (12)\nThe affine-invariant geometry of S+(d) has been well studied, in particular in the context of information geometry (Smith, 2005). Indeed, any positive definite matrix W \u2208 S+(d) can be identified to the multivariate normal distribution N (0,W). Using such a metric allows to endow the space of parameters S+(d) with a distance that reflects the proximity of the probability distributions. The Riemannian metric thus distorts the Euclidean distances between positive definite matrices in order to reflect the amount of information between the two associated probability distributions. If \u03beW is a tangent vector to W \u2208 S+(d), we have the following approximation for the Kullback-Leibler divergence (up to third order terms)\nDKL(W||W + \u03beW) \u2248 \u03beTWJW\u03beW = gW(\u03beW, \u03b6W)\nwhere JW is the Fisher information matrix at W. This approximation coincides with the Riemannian distance that is induced by the affine-invariant metric (12) (Smith, 2005). With this geometry, tangent vectors \u03beW are expressed as\n\u03beW = W 1 2 Sym(\u2206)W 1 2 , \u2206 \u2208 Rd\u00d7d.\nThe gradient gradf(W) is given by\nDf(W)[\u2206] = gW(\u2206, gradf(W)).\nApplying this formula to (4) yields\ngradf(W) = (y\u0302 \u2212 y)WSym(X)W. (13)\nThe exponential mapping has the closed-form expression\nExpW(\u03beW) = W 1 2 exp(W\u2212 1 2 \u03beWW \u2212 1 2 )W 1 2 . (14)\nIts first-order approximation provides the convenient retraction\nRW(s\u03beW) = W \u2212 s\u03beW. (15)\nThe formulas (13) and (14) applied to the cost (4) turn the abstract update (3) into\nWt+1 = W 1 2 t exp(\u2212st(y\u0302t \u2212 yt)W 1 2 t Sym(Xt)W 1 2 t )W 1 2 t .\nWith the alternative retraction (15), the update becomes\nWt+1 = Wt \u2212 st(y\u0302t \u2212 yt)WtSym(Xt)Wt,\nwhich is the update of Davis et al. (2007) based on the LogDet divergence (see Section 7.1).\n4.3 The log-Euclidean metric on S+(d)\nFor the sake of completeness, we briefly review a third Riemannian geometry of S+(d), that exploits the property W = exp(S), S = ST \u2208 Rd\u00d7d. The matrix exponential thus provides a global diffeomorphism between S+(d) and the linear space of d\u00d7d symmetric matrices. This geometry is studied in detail in the paper (Arsigny et al., 2007). The cost function\n`(y\u0302, y) = f(S) = 1\n2 (Tr(exp(S)Sym(X))\u2212 y)2\nthus defines a cost function in the linear space of symmetric matrices. The gradient of this cost function is given by gradf(S) = (y\u0302t \u2212 yt)Sym(Xt), and the retraction is\nRS(s\u03beS) = exp(log W + s\u03beS)\nThe corresponding gradient descent update is\nWt+1 = exp(log Wt \u2212 st(y\u0302t \u2212 yt)Sym(Xt)),\nwhich is the update of Tsuda et al. (2005) based on the von Neumann divergence."}, {"heading": "5. Linear regression on fixed-rank positive semidefinite matrices", "text": "We now present the proposed generalizations to fixed-rank positive semidefinite matrices."}, {"heading": "5.1 Linear regression with a flat geometry", "text": "The generalization of the results of Section 4.1 to the set S+(r, d) is a straightforward consequence of the factorization\nW = GGT , G \u2208 Rd\u00d7r\u2217 ,\nwhere Rd\u00d7r\u2217 = {G \u2208 Rd\u00d7r s.t. rank(G) = r}. The flat quotient geometry of S+(d) ' GL(d)/O(d) is generalized to the quotient geometry of S+(r, d) ' Rd\u00d7r\u2217 /O(r) by a mere adaptation of matrix dimension, leading to the updates (10) and (11) for matrices Gt \u2208 Rd\u00d7r. The quotient geometry of S+(r, d) ' Rd\u00d7r\u2217 /O(r) is studied by Journe\u0301e et al. (2010)."}, {"heading": "5.2 Linear regression with a polar geometry", "text": "In contrast to the flat geometry, the affine-invariant geometry of S+(d) ' GL(d)/O(d) does not generalize directly to S+(r, d) ' Rd\u00d7r\u2217 /O(r) because Rd\u00d7r\u2217 is not a group. A partial generalization is however possible by considering the polar matrix factorization\nG = UR, U \u2208 St(r, d), R \u2208 S+(r).\nIt is obtained from the singular value decomposition of G = Z\u03a3VT as U = ZVT and R = V\u03a3VT (Golub and Van Loan, 1996). This gives a polar parametrization of S+(r, d)\nW = UR2UT .\nThis development leads to the quotient representation\nS+(r, d) ' (St(r, d)\u00d7 S+(r))/O(r), (16)\nbased on the invariance of W to the transformation (U,R2) 7\u2192 (UO,OTR2O), O \u2208 O(r). It thus describes the set of equivalence classes\n[(U,R2)] = {(UO,OTR2O) s.t. O \u2208 O(r)}.\nThe cost function is now given by\nf(U,R2) = `(y\u0302, y) = 1\n2 (Tr(UR2UTSym(X))\u2212 y)2. (17)\nThe Riemannian geometry of (16) has been recently studied (Bonnabel and Sepulchre, 2009). A tangent vector \u03be[W] = (\u03beU, \u03beR2)[U,R2] at [U,R\n2] is described by a horizontal tangent vector \u03be\u0304W = (\u03be\u0304U, \u03be\u0304R2)(U,R2) at (U,R 2) by\n\u03be\u0304U = \u03a0U\u2206, \u2206 \u2208 Rd\u00d7r, \u03be\u0304R2 = RSym(\u03a8)R, \u03a8 \u2208 Rr\u00d7r.\nThe metric\ng[W](\u03be[W], \u03b6[W]) , g\u0304W(\u03be\u0304W, \u03b6\u0304W)\n= 1\n\u03bb g\u0304U(\u03be\u0304U, \u03b6\u0304U) +\n1\n1\u2212 \u03bb g\u0304R2(\u03be\u0304R2 , \u03b6\u0304R2), (18)\nwhere \u03bb \u2208 (0, 1), is induced by the metric of St(r, d) and the affine-invariant metric of S+(r),\ng\u0304U(\u22061,\u22062) = Tr(\u2206 T 1 \u22062), g\u0304R2(\u03a81,\u03a82) = Tr(\u03a81R \u22122\u03a82R \u22122).\nIt is invariant along the set of equivalence classes and thus induces a quotient Riemannian structure on S+(r, d). A retraction is provided by decoupled retractions on U and R 2,\nRU(s\u03be\u0304U) = qf(U + s\u03be\u0304U) (19)\nRR2(s\u03be\u0304R2) = R exp(sR \u22121\u03be\u0304R2R \u22121)R. (20)\nOne should observe that this retraction is not the exponential mapping of S+(r, d). The geodesics do not appear to have a closed form in this geometry, see the paper of Bonnabel and Sepulchre (2009) for details. Combining the gradient of (17) with the retractions (19) and (20) gives\nUt+1 = qf ( Ut \u2212 2\u03bbst(y\u0302t \u2212 yt)(I\u2212UtUTt )Sym(Xt)UtR2t ) ,\nR2t+1 = Rt exp ( \u2212(1\u2212 \u03bb)st(y\u0302t \u2212 yt)RtUTt Sym(Xt)UtRt ) Rt.\nA factorization Rt+1R T t+1 of R 2 t+1 is obtained thanks to the property of matrix exponential, exp(A) 1 2 = exp(12A). Updating Rt+1 instead of R 2 t+1 is thus more efficient from a computational point of view, since it avoids the computation of a square root a each iteration. This yields the online gradient descent algorithm\nUt+1 = qf ( Ut \u2212 2\u03bbst(y\u0302t \u2212 yt)(I\u2212UtUTt )Sym(Xt)UtR2t ) ,\nRt+1 = Rt exp\n( \u22121\n2 (1\u2212 \u03bb)st(y\u0302t \u2212 yt)RtUTt Sym(Xt)UtRt\n) ,\n(21)\nand the batch gradient descent algorithm\nUt+1 = qf ( Ut \u2212 2\u03bbst 1\nn n\u2211 i=1 (y\u0302i \u2212 yi)(I\u2212UtUTt )Sym(Xi)UtR2t\n) ,\nRt+1 = Rt exp\n( \u22121\n2 (1\u2212 \u03bb)st\n1\nn n\u2211 i=1 (y\u0302i \u2212 yi)RtUTt Sym(Xi)UtRt\n) .\n(22)"}, {"heading": "6. Algorithms", "text": "This section documents implementation details of the proposed algorithms. Generic pseudocodes are provided in Figure 1 and Table 1 summarizes computational complexities."}, {"heading": "6.1 From subspace learning to distance learning", "text": "The update expressions (22) and (21) show that \u03bb, the tuning parameter of the Riemannian metric (18), acts as a weighting factor on the search direction. A proper tuning of this parameter allows to place more emphasis either on the learning of the subspace U or on the distance in that subspace R2. In the case \u03bb = 1, the algorithm only perform subspace learning. Conversely, in the case \u03bb = 0, the algorithm learns a distance for a fixed range space (see Section 7.1). Intermediate values of \u03bb continuously interpolate between the subspace learning problem and the distance learning problem at fixed range space.\nA proper tuning of \u03bb is of interest when a good estimate of the subspace is available (for instance a subspace given by a proper dimension reduction technique) or when too few observations are available to jointly estimate the subspace and the distance within that subspace. In the latter case, one has the choice to favor either subspace or distance learning.\nExperimental results of Section 9 recommend the value \u03bb = 0.5 as the default setting."}, {"heading": "6.2 Invariance properties", "text": "A nice property of the proposed algorithms is their invariance with respect to rotations W 7\u2192 OTWO, \u2200O \u2208 O(d). This invariance comes from the fact that the chosen metrics are invariant to rotations. A practical consequence is that a rotation of the input matrix X 7\u2192 OXOT (for instance a whitening transformation of the vectors x 7\u2192 Ox if X = xxT ) will not affect the behavior of the algorithms.\nBesides being invariant to rotations, algorithms (21) and (22) are invariant with respect to scalings W 7\u2192 \u00b52W with \u00b52 \u2208 R+. Consequently, a scaling of the input matrix X 7\u2192 \u00b5X with \u00b5 \u2208 R will not affect the behavior of these algorithms."}, {"heading": "6.3 Mini-batch extension of online algorithms", "text": "We consider a mini-batch extension of stochastic gradient algorithms. It consists in performing each gradient step with respect to p \u2265 1 examples at a time instead of a single one. This is a classical speedup and stabilization heuristic for stochastic gradient algorithms. In the particular case p = 1, one recovers plain stochastic gradient descent. Given p samples (Xt,1, yt,1), ..., (Xt,p, yt,p), received at time t, the abstract update (3) becomes\nWt+1 = RWt ( \u2212st 1\np p\u2211 i=1 gradW`(y\u0302i, yi)\n) ."}, {"heading": "6.4 Strategies for choosing the step size", "text": "We here present strategies for choosing the step size in both the batch and online cases."}, {"heading": "6.4.1 Batch algorithms", "text": "For batch algorithms, classical backtracking methods exist (see Nocedal and Wright, 2006). In this paper, we use the Armijo step sA defined at each iteration by the condition\nf(RWt(\u2212sA gradf(Wt))) \u2264 f(Wt) + c\u2016gradf(Wt)\u20162Wt , (23)\nwhere Wt \u2208 S+(r, d) is the current iterate, c \u2208 (0, 1), f is the empirical cost (2) and RW is the chosen retraction. In this paper, we choose the particular value c = 0.5 and repetitively divide by 2 a specified maximum step size smax until condition (23) is satisfied for the considered iteration. In order to reduce the dependence on smax in a particular problem, it is chosen inversely proportional to the norm of the gradient at each iteration,\nsmax = s0\n\u2016gradf(Wt)\u2016Wt .\nA typical value of s0 = 100 showed satisfactory results for all the considered problems."}, {"heading": "6.4.2 Online algorithms", "text": "For online algorithms, the choice of the step size is more involved. In this paper, the step size schedule st is chosen as\nst = s \u00b5\u0302grad \u00d7 nt0 nt0 + t , (24)\nwhere s > 0, n is the number of considered learning samples, \u00b5\u0302grad is an estimate of the average gradient norm \u2016gradf(Wt)\u2016Wt , and t0 > 0 controls the annealing rate of st. During a pre-training phase of our online algorithms, we select a small subset of learning samples and try the values 2k with k = \u22123, ..., 3 for both s and t0. The values of s and t0 that provide the best decay of the cost function are selected to process the complete set of learning samples."}, {"heading": "6.5 Stopping criterion", "text": "Batch algorithms are stopped when the value or the relative change of the empirical cost f is small enough, or when the relative change in the parameter variation is small enough,\nf(Wt+1) \u2264 tol, or f(Wt+1)\u2212 f(Wt)\nf(Wt) \u2264 tol, or \u2016Gt+1 \u2212Gt\u2016F \u2016Gt\u2016F \u2264 tol. (25)\nWe found tol = 10 \u22125 to be a good trade-off between accuracy and convergence time.\nOnline algorithms are run for a fixed number of epochs (number of passes through the set of learning samples). Typically, a few epochs are sufficient to attain satisfactory results."}, {"heading": "6.6 Convergence", "text": "Gradient descent algorithms on matrix manifolds share the well-characterized convergence properties of their analog in Rd. Batch algorithms converge linearly to a local minimum of the empirical cost that depends on the initial condition. Online algorithms converge asymptotically to a local minimum of the expected loss. They intrinsically have a much slower convergence rate than batch algorithms, but they generally decrease faster the expected loss in the large-scale regime (Bottou and Bousquet, 2007). The main idea is that, given a training set of samples, an inaccurate solution may indeed have the same or a lower expected cost than a well-optimized one.\nWhen learning a matrix W \u2208 S+(d), the problem is convex and the proposed algorithms converge toward a global minimum of the cost function, regardless of the initial condition. When learning a low-rank matrix W \u2208 S+(r, d), with r < d, the proposed algorithms converge to a local minimum of the cost function.\nFor batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008).\nFor online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof).\nWhen the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly. However, we empirically observed that the algorithm always converges to a local minimum of the cost function. Numerical simulations thus suggest that the results of Bottou (1998) on stochastic gradient descent extend to the gradient descent algorithm based on polar parameterization. This issue is left as a topic for future research."}, {"heading": "7. Discussion", "text": "This section presents connections with existing works and extensions of the regression model."}, {"heading": "7.1 Closeness-based approaches", "text": "A standard derivation of learning algorithms is as follows (Kivinen and Warmuth, 1997). The (online) update at time t is viewed as an (approximate) solution of\nWt+1 = arg min W\u2208W D(W,Wt) + st `(y\u0302, yt), (26)\nwhere D is a well-chosen measure of closeness between elements of W and st is a tradeoff parameter that controls the balance between the conservative term D(W,Wt) and the innovation (or data fitting) term `(y\u0302, yt). One solves (26) by solving the algebraic equation\ngrad D(W,Wt) = \u2212st grad `(y\u0302t+1, yt), (27)\nwhich is a first-order (necessary) optimality condition. If the search space W is a Riemannian manifold and if the closeness measure D(W,Wt) is the Riemannian distance, the solution of (27) is\nWt+1 = ExpWt(\u2212st grad `(y\u0302t+1, yt)).\nBecause y\u0302t+1 must be evaluated in Wt+1, this update equation is implicit. However, y\u0302t+1 is generally replaced by y\u0302t (which is equal to y\u0302t+1 up to first order terms in st), which gives the update (3) where the exponential mapping is chosen as a retraction.\nBregman divergences have been popular closeness measures for D(W,Wt) because they render the optimization of (26) convex. Bregman divergences on the cone of positive definite matrices include the von Neumann divergence\nDvN (W,Wt) = Tr(W log W \u2212W log Wt \u2212W + Wt),\nand the LogDet divergence\nDld(W,Wt) = Tr(WW \u22121 t )\u2212 log det(WW \u22121 t )\u2212 d.\nWe have shown in Section 4 that the resulting updates can be interpreted as line-search updates for the log-Euclidean metric and the affine-invariant metric of S+(d) and for specific choices of the retraction mapping.\nLikewise, the algorithm (10) can be recast in the framework (26) by considering the closeness\nDflat(W,Wt) = \u2016G\u2212Gt\u20162F ,\nwhere W = GGT and Wt = GtG T t . Algorithm (21) can be recast in the framework (26) by considering the closeness\nDpol(W,Wt) = \u03bb r\u2211 i=1 \u03b82i + (1\u2212 \u03bb) \u2016 log R\u22121t R2R \u22121 t \u20162F .\nwhere the \u03b8i\u2019s are the principal angles between the subspaces spanned by W and Wt (Golub and Van Loan, 1996), and the second term is the affine-invariant distance of S+(d) between matrices R2 and R2t involved in the polar representation of W and Wt.\nObviously, these closeness measures are no longer convex due to the rank constraint. However they reduce to the popular divergences in the full-rank case, up to second order terms. In particular, when \u03bb = 1, the subspace is fixed and one recovers the setup of learning low-rank matrices of fixed range space (Kulis et al., 2009). Thus the algorithms introduced in the present paper can be viewed as generalizations of the ones presented in (Kulis et al., 2009). The authors of (Kulis et al., 2009) present the issue of adapting the range space as an open research question. Each of the proposed algorithms provides an efficient workaround for this problem at the expense of the (potential) introduction of local minima."}, {"heading": "7.2 Handling inequalities", "text": "Inequalities y\u0302 \u2264 y or y\u0302 \u2265 y can be considered by treating them as equalities when they are not satisfied. This is equivalent to the minimization of the continuously differentiable cost function\nf(W) = `(y\u0302, y) = 1\n2 max(0, \u03c1(y\u0302 \u2212 y))2,\nwhere \u03c1 = +1 if y\u0302 \u2264 y is required and \u03c1 = \u22121 if y\u0302 \u2265 y is required."}, {"heading": "7.3 Kernelizing the regression model", "text": "In this paper, we have not considered the kernelized model\ny\u0302 = Tr(W\u03c6(x)\u03c6(x)T ),\nwhose predictions can be extended to new input data \u03c6(x) in the feature space F induced by the nonlinear mapping \u03c6 : x \u2208 X 7\u2192 \u03c6(x) \u2208 F . This is potentially a useful extension of the regression model that could be investigated in the light of recent theoretical results in this area (for example Chatpatanasiri et al., 2008)."}, {"heading": "7.4 Connection with multidimensional scaling algorithms", "text": "Given a set of m dissimilarity measures D = {\u03b4ij}m between n data objects, multidimensional scaling algorithms search for a r-dimensional embedding of the data objects into an Euclidean space representation G \u2208 Rn\u00d7r (Cox and Cox, 2001; Borg and Groenen, 2005). Each row g of G is the coordinates of a data object in a Euclidean space of dimension r.\nMultidimensional scaling algorithms based on gradient descent are equivalent to algorithms (11) and (10) when X = (ei \u2212 ej)(ei \u2212 ej)T , where ei is the i-th unit vector (see Section 8.1), and when the multidimensional scaling reduction criterion is the SSTRESS\nSSTRESS(G) = \u2211\n(i,j)\u2208D\n(\u2016gi \u2212 gj\u201622 \u2212 \u03b4ij)2.\nVectors gi and gj are the rows i and j of G. Gradient descent is a popular technique in the context of multidimensional scaling algorithms. A stochastic gradient descent approach for minimizing the SSTRESS has also been proposed by Matsuda and Yamaguchi (2001). A potential area of future work is the application of the proposed online algorithm (10) for adapting a batch solution to slight modifications of the dissimilarities over time. This approach has a much smaller computational cost than recomputing the offline solution at every time step. It further allows to keep the coordinate representation coherent over time since the solution do not brutally jumps from a local minimum to another."}, {"heading": "8. Applications", "text": "The choice of an appropriate distance measure is a central issue for many distance-based classification and clustering algorithms such as nearest neighbor classifiers, support vector machines or k-means. Because this choice is highly problem-dependent, numerous methods have been proposed to learn a distance function directly from data. In this section, we present two important distance learning applications that are compatible with the considered regression model and review some relevant literature on the subject."}, {"heading": "8.1 Kernel learning", "text": "In kernel-based methods (Shawe-Taylor and Cristianini, 2004), the data samples x1, ...,xn are first transformed by a nonlinear mapping \u03c6 : x \u2208 X 7\u2192 \u03c6(x) \u2208 F , where F is a new feature space that is expected to facilitate pattern detection into the data.\nThe kernel function is then defined as the dot product between any two samples in F ,\n\u03ba(xi,xj) = \u03c6(xi) \u00b7 \u03c6(xj).\nIn practice, the kernel function is represented by a positive semidefinite matrix K \u2208 Rn\u00d7n whose entries are defined as Kij = \u03c6(xi) \u00b7 \u03c6(xj). This inner product information is used solely to compute the relevant quantities needed by the algorithms based on the kernel. For instance, a distance is implicitly defined by any kernel function as the Euclidean distance between the samples in the new feature space\nd\u03c6(xi,xj) = \u2016\u03c6(xi)\u2212 \u03c6(xj)\u20162 = \u03ba(xi,xi) + \u03ba(xj ,xj)\u2212 2\u03ba(xi,xj),\nwhich can be evaluated using only the elements of the kernel matrix by the formula\nd\u03c6(xi,xj) = Kii + Kjj \u2212 2Kij = Tr ( K(ei \u2212 ej)(ei \u2212 ej)T ) ,\nwhich fits into the considered regression model.\nLearning a kernel consists in computing the kernel (or Gram) matrix from scratch or improving a existing kernel matrix based on side-information (in a semi-supervised setting for instance). Data samples and class labels are generally exploited by means of equality or inequality constraints involving pairwise distances or inner products.\nMost of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009). In that setting, the total number of considered samples is known in advance and determines the size of the learned matrix. Recently, algorithms have been proposed to learn a kernel function that can be extended to new points (Chatpatanasiri et al., 2008; Jain et al., 2009). In this paper, we only consider the kernel learning problem in the transductive setting.\nWhen low-rank matrices are considered, kernel learning algorithms can be regarded as dimensionality reduction methods. Very popular unsupervised algorithms in that context are kernel principal component analysis (Scho\u0308lkopf et al., 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005). Other kernel learning techniques include the maximum variance unfolding algorithm (Weinberger et al., 2004) and its semisupervised version (Song et al., 2007), and the kernel spectral regression framework (Cai et al., 2007) which encompasses many reduction criterion (for example, linear discriminant analysis (LDA), locality preserving projection (LPP), neighborhood preserving embedding (NPE)). See the survey of (Yang, 2006) for a more complete state-of-the-art in this area.\nSince our algorithms are able to compute a low-rank kernel matrix from data, they can be used for unsupervised or semi-supervised dimensionality reduction, depending whether or not the class labels are exploited through the imposed constraints."}, {"heading": "8.2 Mahalanobis distance learning", "text": "Mahalanobis distances generalize the usual Euclidean distance as it allows to transform the data with an arbitrary rotation and scaling before computing the distance. Let xi,xj \u2208 Rd be two data samples, the (squared) Mahalanobis distance between these two samples is parametrized by a positive definite matrix A \u2208 Rd\u00d7d and writes as\ndA(xi,xj) = (xi \u2212 xj)TA (xi \u2212 xj). (28)\nIn the particular case of A being equal to the identity matrix, the standard Euclidean distance is obtained. A frequently used matrix is A = \u03a3\u22121, the inverse of the sample covariance matrix. For centered data features, computing this Mahalanobis distance is equivalent to perform a whitening of the data before computing the Euclidean distance.\nFor low-rank Mahalanobis matrices, computing the distance1 is equivalent to first perform a linear data reduction step before computing the Euclidean distance on the reduced data. Learning a low-rank Mahalanobis matrix can thus be seen as learning a linear projector that is used for dimension reduction.\nIn contrast to kernel functions, Mahalanobis distances easily generalize to new data samples since the sole knowledge of A determines the distance function.\nIn recent years, Mahalanobis distance learning algorithms have been the subject of many contributions that cannot be all enumerated here. We review a few of them, most relevant for the present paper. The first proposed methods have been based on successive projections onto a set of large margin constraints (Xing et al., 2002; Shalev-Shwartz et al., 2004). The method proposed by Globerson and Roweis (2005) seeks a Mahalanobis matrix that maximizes the between classes distance while forcing to zero the within classes distance. A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009). Bregman projection based methods minimize a particular Bregman divergence under distance constraints. Both batch (Davis et al., 2007) and online (Jain et al., 2008) formulations have been proposed for learning full-rank matrices. Low-rank matrices have also been considered with Bregman divergences but only when the range space of the matrix is fixed in the first place (Davis and Dhillon, 2008; Kulis et al., 2009)."}, {"heading": "9. Experiments", "text": "In this section, we illustrate the potential of the proposed algorithms on several benchmark experiments. First, the proposed algorithms are evaluated on toy data. Then, they are compared to state-of-the-art kernel learning and Mahalanobis distance learning algorithms on real datasets. Overall, the experiments support that a joint estimation of a subspace and low-dimensional distance in that subspace is a major advantage of the proposed algorithms over methods that estimate the matrix for a subspace that is fixed beforehand.\nTable 2 summarizes the different datasets that have been considered. As a normalization step, the data features are centered and rescaled to unit standard deviation.\n1. In the low-rank case, one should rigorously refer to (28) as a pseudo-distance, since it is possible to have dA(xi,xj) = 0 with xi 6= xj . This is the case when (xi \u2212 xj) lies in the null space of A.\nThe implementation of the proposed algorithms2 as well as the experiments of this paper are performed with Matlab. The implementations of algorithms MVU3, KSR4, LMNN5 and ITML6 have been rendered publicly available by Weinberger et al. (2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al. (2007) respectively. Algorithms POLA (ShalevShwartz et al., 2004), LogDet-KL (Kulis et al., 2009) and LEGO (Jain et al., 2008) have been implemented on our own."}, {"heading": "9.1 Toy data", "text": "In this section, the proposed algorithms are evaluated on synthetic regression problems. The data vectors x1, ...,xn \u2208 Rd and the target matrix W\u2217 \u2208 S+(r, d) are generated with entries drawn from a standard Gaussian distribution N (0, 1). Observations follow\nyi = (x T i W \u2217xi)(1 + \u03bdi), i = 1, ..., n, (29)\nwhere \u03bdi is drawn fromN (0, 0.01). A multiplicative noise model is preferred over an additive one to easily control that observations remain nonnegative after the superposition of noise.\nLearning the subspace versus fixing the subspace up front. As an illustrative example, we show the difference between two approaches for fitting the data to observations when a target model W\u2217 \u2208 S+(3, 3) is approximated with a parameter W \u2208 S+(2, 3).\nA naive approach to tackle that problem is to first project the data xi \u2208 R3 on a subspace of reduced dimension and then to compute a full-rank model based on the projected data. Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data. However, in general, there is no reason why the subspace spanned by the top principal components should coincide with the subspace that is defined by the target model. Therefore, a more appropriate approach\n2. The source code is available from http://www.montefiore.ulg.ac.be/~meyer 3. http://www.cse.wustl.edu/~kilian/Downloads/MVU.html 4. http://www.cs.uiuc.edu/homes/dengcai2/SR/index.html 5. http://www.cse.wustl.edu/~kilian/Downloads/LMNN.html 6. http://www.cs.utexas.edu/users/pjain/itml/\nconsists in learning jointly the subspace and a distance in that subspace that best fits the data to observations within that subspace.\nTo compare the two approaches, we generate a set of learning samples {(xi, yi)}200i=1, with xi \u2208 R3 and yi that follows (29). The target model is\nW\u2217 = U\u0303\u039bU\u0303T (30)\nwhere U\u0303 is a random 3\u00d73 orthogonal matrix and \u039b is a diagonal matrix with two dominant values \u039b11,\u039b22 \u039b33 > 0 (for this specific example, \u039b11 = 4,\u039b22 = 3 and \u039b33 = 0.01). Observations yi are thus nearly generated by a rank-2 model, such that W\n\u2217 should be well approximated with W \u2208 S+(2, 3) that minimizes the train error.\nResults are presented in Figure 2. The top plot shows that the learned subspace (which identifies with the target subspace) is indeed very different from the subspace spanned by the top two principal components. Moreover, the bottom plots clearly demonstrate that the fit is much better when the subspace and the distance in that subspace are learned jointly. For visualization purpose, the two dimensional model is represented by the ellipse\nE = {x\u0303i \u2208 R2 : x\u0303Ti R2x\u0303i = 1}, where x\u0303i = UTxi\u221a yi ,\nand (U,R2) are computed with algorithm (22), either in the setting \u03bb = 0 that fixes the subspace to the PCA subspace (left) or in the setting \u03bb = 0.5 that simultaneously learned U and B (right). A perfect fit is obtained when the points x\u0303i are all located on E , which is the locus of points where y\u0302i = yi.\nInfluence of the parameter \u03bb on the algorithm based on the polar geometry. In theory, the parameter \u03bb should not influence the algorithm since it has no effect on the first-order optimality conditions except for its two extreme values \u03bb = 0 and \u03bb = 1. In practice however, a sensitivity to this parameter is observed due to the finite tolerance of the stopping criterion: the looser the tolerance, the more sensitive to \u03bb.\nTo investigate the sensitivity to \u03bb, we try to recover a target parameter W\u2217 \u2208 S+(5, 10) using pairs (xi, yi) generated according to (29). We generate 10 random regression problems\nwith 1000 samples partitioned into 500 learning samples and 500 test samples. We compute the mean test error and the mean convergence time as a function of \u03bb for different values of tol. The results are presented in Figure 3. As tol decrease, the test error becomes insensitive to \u03bb, but an influence is observed on the convergence time of the algorithm.\nIn view of these results, we recommend the value 0.5 as the default setting for \u03bb. Unless specified otherwise, we therefore use this particular value for all experiments in this paper.\nOnline versus Batch. This experiment shows that when a large amount of sample is available (80, 000 training samples and 20, 000 test samples for learning a parameter W\u2217 in S+(10, 50)), online algorithms minimize the test error more rapidly than batch ones. It further shows that the mini-batch extension allows to improve significantly the performance compared to the plain stochastic gradient descent setting (p = 1). We observe that the minibatch size p = 32 generally gives good results. Figure 4 report the test error as a function of the learning time, that is, the time after each iteration for batch algorithm and the time after each epoch for online algorithms. For the algorithm based on the polar geometry, the mini-batch extension is strongly recommended to amortize the larger cost of each update."}, {"heading": "9.2 Kernel learning", "text": "In this section, the proposed algorithms are applied to the problem of learning a kernel matrix from pairwise distance constraints between data samples. As mentioned earlier, we only consider this problem in the transductive setting, that is, all samples x1, ...xn are available up front and the learned kernel do not generalize to new samples."}, {"heading": "9.2.1 Experimental setup", "text": "After transformation of the data with the kernel map x 7\u2192 \u03c6(x), the purpose is to compute a fixed-rank kernel matrix based on a limited amount of pairwise distances in the kernel feature space and on some information about class labels.\nDistance constraints are generated as y\u0302ij \u2264 yij(1 \u2212 \u03b1) for identically labeled samples and y\u0302ij \u2265 yij(1 + \u03b1) for differentially labeled samples, where \u03b1 \u2265 0 is a scaling factor, yij = \u2016\u03c6(xi)\u2212 \u03c6(xj)\u20162 and y\u0302ij = Tr(W(ei \u2212 ej)(ei \u2212 ej)T ) = (ei \u2212 ej)TW(ei \u2212 ej).\nWe investigate both the influence of the amount of side-information provided, the influence of the approximation rank and the computational time required by the algorithms.\nTo quantify the performance of the learned kernel matrix, we perform either a classification or a clustering of the samples based on the learned kernel. For classification, we compute the test set accuracy of a k-nearest neighbor classifier (k = 5) using a two-fold cross-validation protocol (results are averaged over 10 random splits). For clustering, we use the K-means algorithm with the number of clusters equal to the number of classes in the problem. To overcome K-means local minima, 10 runs are performed in order to select the result that has lead to the smaller value of the K-means objective. The quality of the clustering is measured by the normalized mutual information (NMI) shared between the random variables of cluster indicators C and true class labels T (Strehl et al., 2000),\nNMI = 2 I(C;T )\n(H(C) +H(T )) ,\nwhere I(X1;X2) = H(X1) \u2212 H(X1|X2) is the mutual information between the random variables X1 and X2, H(X1) is the Shannon entropy of X1, and H(X1|X2) is the conditional entropy of X1 given X2. This score ranges from 0 to 1, the larger the score, the better the clustering quality."}, {"heading": "9.2.2 Compared methods", "text": "We compare the following methods:\n1. Batch algorithms (11) and (22), adapted to handle inequalities (see Section 7.2),\n2. The kernel learning algorithm LogDet-KL (Kulis et al., 2009) which learn kernel matrices of fixed range space for a given set of distance constraints.\n3. The kernel spectral regression (KSR) algorithm of Cai et al. (2007) using a similarity matrix N constructed as follows. Let N be the adjacency matrix of a 5-NN graph based on the initial kernel. We modify N according to the set of available constraints: Nij = 1 if samples xi and xj belong to the same class (must-link constraint), Nij = 0 if samples xi and xj do not belong to the same class (cannot-link constraint).\n4. The Maximum Variance Unfolding (MVU) algorithm (Weinberger et al., 2004),\n5. The Kernel PCA algorithm (Scho\u0308lkopf et al., 1998).\nThe last two algorithms are unsupervised techniques that are provided as baselines."}, {"heading": "9.2.3 Results", "text": "The first experiment is reproduced from Tsuda et al. (2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. All algorithms start from the identity matrix that do not encode any domain information. Figure 5 (left) reports the k-NN classification accuracy as a function of the number of distance constraints provided. In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. The convergence time of the algorithm based on the polar geometry is however much faster (0.15 seconds versus 58 seconds for LogDet-KL when learning 1000 constraints). The algorithm based on the flat geometry has inferior performance when too few constraints are provided. This is because in the kernel learning setting, updates of this algorithm only involve the rows and columns that correspond to the set of points for which constraints are provided. It may thus result in a partial update of the kernel matrix entries. This issue disappears as the number of provided constraints increases.\nThe second experiment is reproduced from Kulis et al. (2009). It aims at improving an existing low-rank kernel using limited information about class labels. A rank-16 kernel matrix is computed for clustering a database of 300 handwritten digits randomly sampled from the 3, 8 and 9 digits of the Digits dataset (since we could not find out the specific samples that have been selected by Kulis et al. (2009), we made our own samples selection). The distance constraints are randomly sampled from a linear kernel on the input data K = XXT and \u03b1 = 0.25. The results are presented in Figure 5 (right). The figure\nshows that KSR, LogDet-KL and the algorithm based on the polar geometry with \u03bb = 0 perform similarly. These methods are however outperformed by the proposed algorithms (flat geometry and polar geometry with \u03bb = 0.5) when the number of constraints is large enough. This experiment also enlighten the flexibility of the polar geometry, which allows to fix the subspace in situations where too few constraints are available.\nFinally, we tackle the kernel learning problem on a larger data set. We use the test set of the USPS dataset7, which contains 2007 samples of handwritten zip code digits. The data are first transformed using the kernel map \u03ba(xi,xj) = exp(\u2212\u03b3\u2016xi \u2212 xj\u201622) with \u03b3 = 0.001 and we further center the data in the kernel feature space. Pairwise distance constraints are randomly sampled from that kernel matrix with \u03b1 = 0.5. Except KSR that has its own initialization procedure, algorithms start from the kernel matrix provided by kernel PCA.\nFigure 6 (left) shows the clustering performance as a function of the number of constraints provided when the approximation rank is fixed to r = 25. Figure 6 (right) reports the clustering performance as a function of the approximation rank when the number of constraints provided is fixed to 100K. When the number of provided constraints is large enough, the proposed algorithms perform as good as KSR and outperform the LogDet-KL method that learn a kernel of fixed-range space. Average computational times for learning a rank-6 kernel from 100K constraints are 0.57 seconds for KSR, 3.25 seconds for the algorithm based on the flat geometry, 46.78 seconds for LogDet-KL and 47.30 seconds for the algorithm based on the polar geometry. In comparison, the SDP-based MVU algorithm takes 676.60 seconds to converge.\n7. The ZIP code data set from http://www-stat-class.stanford.edu/~tibs/ElemStatLearn/data.html."}, {"heading": "9.3 Mahalanobis distance learning", "text": "In this section, we tackle the problem of learning from data a Mahalanobis distance for supervised classification and compare our methods to state-of-the-art Mahalanobis metric learning algorithms."}, {"heading": "9.3.1 Experimental setup", "text": "For the considered problem, the purpose is to learn the parameter of a Mahalanobis distance dW(xi,xj) = (xi \u2212 xj)TW(xi \u2212 xj), such that the distance satisfies as much as possible a given set of constraints. As in the paper of Davis et al. (2007), we generate the constraints from the learning set of samples as dW(xi,xj) \u2264 l for same-class pairs and dW(xi,xj) \u2265 u for different-class pairs. The scalars u and l estimate the 95th and 5th percentiles of the distribution of Mahalanobis distances parameterized by a chosen baseline W0. The performance of the learned distance is then quantified by the test error rate of a k-nearest neighbor classifier based on the learned distance. All experiments use the setting k = 5, breaking ties arbitrarily. Unless for the Isolet data set for which a specific train/test partition is provided, error rates are computed using two-fold cross validation. Results are averaged over 10 random partitions."}, {"heading": "9.3.2 Compared methods", "text": "We compare the following distance learning algorithms:\n1. Batch algorithms (11) and (22),\n2. ITML (Davis et al., 2007),\n3. LMNN (Weinberger and Saul, 2009),\n4. Online algorithms (10) and (21),\n5. LEGO (Jain et al., 2008),\n6. POLA (Shalev-Shwartz et al., 2004).\nWhen some methods require the tuning of an hyper-parameter, this is performed by a twofold cross-validation procedure. The slack parameter of ITML as well as the step size of POLA are selected in the range of values 10k with k = \u22123, ..., 3. The step size of LEGO is selected in this same range of value for the UCI datasets, and in the range of value 10k with k = \u221210, ...,\u22125 for the larger data sets Isolet and Prostate."}, {"heading": "9.3.3 Results", "text": "Reproducing a classical benchmark experiment from Kulis et al. (2009), we demonstrate that the proposed batch algorithms compete with state-of-the-art full-rank Mahalanobis distance learning algorithms on several UCI datasets (Figure 7). Except POLA and LMNN which do not learn from provided pairwise constraints, all algorithms process 40c(c \u2212 1) constraints, where c is the number of classes in the data. We choose the Euclidean distance (W0 = I) as the baseline distance for initializing the algorithms. Figure 7 reports the\nresults. The two proposed algorithms compete favorably with the other full-rank distance learning techniques, achieving the minimal average error for 4 of the 5 considered data sets.\nWe finally evaluate the proposed algorithms on higher-dimensional data sets in the lowrank regime (Figure 8). The distance constraints are generated as in the full-rank case, but the initial baseline matrix is now computed as W0 = G0G T 0 , where G0\u2019s columns are the top principal directions of the data. For the Isolet data set, 100K constraints are generated, and 10K constraints are generated for the Prostate data set. For scalability reasons, algorithms LEGO, LMNN and ITML must proceed in two steps: the data are first projected onto the top principal directions and then a full-rank distance is learned within the subspace spanned by these top principal directions. In contrast, our algorithms are initialized with the top principal direction, but they operate on the data in their original feature space. Overall, the proposed algorithms achieve much better performance than the methods that first reduce the data. This is particularly striking when the rank is very small compared to problem size. The performance gap reduces as the rank increases, but for high-dimensional problems, one is usually interested in efficient low-rank approximations that gives satisfactory results."}, {"heading": "10. Conclusion", "text": "In this paper, we propose gradient descent algorithms to learn a regression model parameterized by a fixed-rank positive semidefinite matrix. The rich Riemannian geometry of the set of fixed-rank PSD matrices is exploited through a geometric optimization approach.\nThe resulting algorithms overcome the main difficulties encountered by the previously proposed methods as they scale to high-dimensional problems, and they naturally enforce\nthe rank constraint as well as the positive definite property while leaving the range space of the matrix free to evolve during optimization.\nWe apply the proposed algorithms to the problem of learning a distance function from data, when the distance is parameterized by a fixed-rank positive semidefinite matrix. The good performance of the proposed algorithms is illustrated over several benchmarks."}, {"heading": "Acknowledgments", "text": "This paper presents research results of the Belgian Network DYSCO (Dynamical Systems, Control, and Optimization), funded by the Interuniversity Attraction Poles Programme, initiated by the Belgian State, Science Policy Office. The scientific responsibility rests with its authors. Gilles Meyer is supported as an FRS-FNRS research fellow (Belgian Fund for Scientific Research)."}, {"heading": "Appendix A. Line-search algorithms on matrix manifolds", "text": "This appendix summarizes the exposition of Absil et al. (2008, chap. 3 and 4).\nRestrictions on the search space are generally encoded into optimization algorithms by means of particular constraints or penalties expressed as a function of the search variable. However, when the search space is endowed with a particular manifold structure, it is possible to design an exploration strategy that is consistent with the geometry of the problem and that appropriately turns the problem into an unconstrained optimization problem. This approach is the purpose of optimization algorithms defined on matrix manifolds.\nInformally, a manifold W is a space endowed with a differentiable structure. One usually make the distinction between embedded submanifolds (subsets of larger manifolds) and\nquotient manifolds (manifolds described by a set of equivalence classes). An intuitive example of embedded submanifold is the sphere embedded in Rd. A typical example of quotient manifold is the set of r-dimensional subspaces in Rd, viewed as a collection of r-dimensional orthogonal frames that cannot be superposed by a rotation. The rotational variants of a given frame thus define an equivalence class (denoted using square brackets [\u00b7]), which is identified as a single point on the quotient manifold.\nTo develop line-search algorithms, the notion of gradient of a scalar cost function needs to be extended to manifolds. For that purpose, the manifold W is endowed with a metric gW(\u03beW, \u03b6W), which is an inner product defined between elements \u03beW, \u03b6W of the tangent space TWW at W. The metric induces a norm on the tangent space TWW at W:\n\u2016\u03beW\u2016W = \u221a gW(\u03beW, \u03beW)\nThe gradient of a smooth scalar function f : W \u2192 R at W \u2208 W is the only element gradf(W) \u2208 TWW that satisfies\nDf(W)[\u2206] = gW(\u2206, gradf(W)), \u2200\u2206 \u2208 TWW, (31)\nwhere \u2206 is a matrix representation of a \u201cgeometric\u201d tangent vectors \u03be, and where\nDf(W)[\u2206] = lim t\u21920 f(W + t\u2206)\u2212 f(W) t ,\nis the standard directional derivative of f at W in the direction \u2206. For quotient manifoldsW =W/ \u223c, whereW is the total space and \u223c is the equivalence relation that defines the quotient, the tangent space T[W]W at [W] is sufficiently described by the directions that do not induce any displacement in the set of equivalence classes [W]. This is achieved by restricting the tangent space at [W] to horizontal vectors \u03be\u0304W \u2208 TWW at W that are orthogonal to the equivalence classe [W]. Provided that the metric g\u0304W in\nthe total space is invariant along the equivalence classes, it defines a metric in the quotient space\ng[W](\u03be[W], \u03b6[W]) , g\u0304W(\u03be\u0304W, \u03b6\u0304W).\nThe horizontal gradient gradf(W) is obtained by projecting the gradient gradf\u0304(W) in the total space onto the set of horizontal vectors \u03be\u0304W at W.\nNatural displacements at W in a direction \u03beW on the manifold are performed by following geodesics (paths of shortest length on the manifold) starting from W and tangent to \u03beW. This is performed by means of the exponential mapping\nWt+1 = ExpWt(st\u03beWt), (32)\nwhich induces a line-search algorithm along geodesics.\nA more general update formula is obtained if we relax the constraint of moving along geodesics. The retraction mapping\nWt+1 = RWt(st\u03beWt),\nlocally approximates the exponential mapping. It provides an attractive alternative to the exponential mapping in the design of optimization algorithms on manifolds, as it reduces the computational complexity of the update while retaining the essential properties that ensure convergence results. When \u03beWt coincide with \u2212gradf(Wt) a gradient descent algorithm on the manifold is obtained. Figure 9 pictures a gradient descent update on W.\nAppendix B. Convergence proof of algorithm (10)\nBottou (1998) reviews the mathematical tools required to prove almost sure convergence, that is asymptotic convergence with probability one, of stochastic gradient algorithms. Adapting the results presented for vectors w in Rd to matrices G of Rd\u00d7r, we have that the following five assumptions guarantee that a stochastic gradient algorithm converges:\n(A1) F (G) = EX,y{`(y\u0302, y)} \u2265 0 is three times differentiable with bounded derivatives,\n(A2) \u2211\u221e\nt=1 s 2 t <\u221e and \u2211\u221e t=1 st =\u221e,\n(A3) EX,y{\u2016gradf(G)\u20162F } \u2264 k1 + k2\u2016G\u20162F , where f(G) = `(y\u0302, y),\n(A4) \u2203h1 > 0, inf \u2016G\u20162F>h1 Tr(GT gradf(G)) > 0,\n(A5) \u2203h2 > h1, \u2200(X, y) \u2208 X \u00d7 Y, sup \u2016G\u20162F<h2 \u2016gradf(G)\u2016F \u2264 k3,\nwhere \u2016 \u00b7 \u2016F is the Frobenius norm. For bounded input matrices X and observations y, algorithm (10) minimizing the quadratic cost (9) fulfills the five assumptions (A1) to (A5). Hence, the following convergence result applies.\nProposition 1 The stochastic gradient descent algorithm (10) equipped with the step size schedule (24) converge almost surely to the set of stationary points of F (G).\nProof The proof is completed in two steps. First, it is shown that the stochastic sequence\nut = max(h2, \u2016Gt\u20162F ),\ndefines a Lyapunov process (always positive and decreasing on average) which converges almost surely to h2. This implies that Gt is almost surely confined within distance \u221a h2 from the origin and provides almost sure bounds on all continuous functions of Gt.\nSecond, the Lyapunov process vt = F (Gt) \u2265 0 is proved to converge almost surely. Convergence of F (Gt) is then used to show that wt = grad F (Gt) tends to zero almost surely. Technical details are provided in the paper of Bottou (1998).\nIn practice, saddle points and local maxima are unstable solutions while convergence to asymptotic plateaus is excluded by (A4). As a result, almost sure convergence to a local minimum of the expected cost is obtained."}], "references": [{"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Appl. Math.,", "citeRegEx": "Absil et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2004}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2008}, {"title": "On learning rotations", "author": ["R. Arora"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Arora.,? \\Q2009\\E", "shortCiteRegEx": "Arora.", "year": 2009}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Arsigny et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2007}, {"title": "University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Asuncion", "D.J. Newman"], "venue": "UCI machine learning repository. http://www.ics.uci.edu/~mlearn/MLRepository.html,", "citeRegEx": "Asuncion and Newman.,? \\Q2007\\E", "shortCiteRegEx": "Asuncion and Newman.", "year": 2007}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F. Bach", "M.I. Jordan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning (ICML),", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Riemannian metric and geometric mean for positive semidefinite matrices of fixed rank", "author": ["S. Bonnabel", "R. Sepulchre"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Bonnabel and Sepulchre.,? \\Q2009\\E", "shortCiteRegEx": "Bonnabel and Sepulchre.", "year": 2009}, {"title": "Modern Multidimensional Scaling: Theory and Applications", "author": ["I. Borg", "P. Groenen"], "venue": null, "citeRegEx": "Borg and Groenen.,? \\Q2005\\E", "shortCiteRegEx": "Borg and Groenen.", "year": 2005}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning,", "citeRegEx": "Bottou.,? \\Q2004\\E", "shortCiteRegEx": "Bottou.", "year": 2004}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou and Bousquet.,? \\Q2007\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2007}, {"title": "Efficient kernel discriminant analysis via spectral regression", "author": ["D. Cai", "X. He", "J. Han"], "venue": "In Proceedings of the International Conference on Data Mining (ICDM07),", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "On kernelization of supervised mahalanobis distance learners", "author": ["R. Chatpatanasiri", "T. Korsrilabutr", "P. Tangchanachaianan", "B. Kijsirikul"], "venue": "arXiv,", "citeRegEx": "Chatpatanasiri et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chatpatanasiri et al\\.", "year": 2008}, {"title": "Multidimensional Scaling", "author": ["T.F. Cox", "M.A.A. Cox"], "venue": null, "citeRegEx": "Cox and Cox.,? \\Q2001\\E", "shortCiteRegEx": "Cox and Cox.", "year": 2001}, {"title": "Online tracking of linear subspaces", "author": ["K. Crammer"], "venue": "In Proceedings of 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Crammer.,? \\Q2006\\E", "shortCiteRegEx": "Crammer.", "year": 2006}, {"title": "Structured metric learning for high dimensional problems", "author": ["J.V. Davis", "I.S. Dhillon"], "venue": "In Proceedings of the 14th ACM SIGKDD conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Davis and Dhillon.,? \\Q2008\\E", "shortCiteRegEx": "Davis and Dhillon.", "year": 2008}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Edelman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Edelman et al\\.", "year": 1998}, {"title": "Analysis on Symmetric Cones", "author": ["J. Faraut", "A. Koranyi"], "venue": null, "citeRegEx": "Faraut and Koranyi.,? \\Q1994\\E", "shortCiteRegEx": "Faraut and Koranyi.", "year": 1994}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg", "N. Cristianini", "J. Shawe-taylor", "B. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fine et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fine et al\\.", "year": 2001}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Globerson and Roweis.,? \\Q2005\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2005}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Metric and kernel learning using a linear transformation", "author": ["P. Jain", "B. Kulis", "J.V. Davis", "I.S. Dhillon"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2009}, {"title": "Low-rank optimization for semidefinite convex problems", "author": ["M. Journ\u00e9e", "F. Bach", "P.-A. Absil", "R. Sepulchre"], "venue": "SIAM Journal on Matrix Analysis and Applications (in press),", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Journal of Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Low-rank kernel learning with bregman matrix divergences", "author": ["B. Kulis", "M. Sustik", "I.S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "Learning with idealized kernels", "author": ["J. Kwok", "I. Tsang"], "venue": "Proceedings of the 20th International Conference on Machine learning (ICML),", "citeRegEx": "Kwok and Tsang.,? \\Q2003\\E", "shortCiteRegEx": "Kwok and Tsang.", "year": 2003}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Global mapping analysis: Stochastic gradient algorithm in sstress and classical mds stress", "author": ["Y. Matsuda", "K. Yamaguchi"], "venue": "In Proceedings of International Conference on Neural Information Processing,", "citeRegEx": "Matsuda and Yamaguchi.,? \\Q2001\\E", "shortCiteRegEx": "Matsuda and Yamaguchi.", "year": 2001}, {"title": "Numerical Optimization, Second Edition", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Principal components, minor components, and linear neural networks", "author": ["E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Oja.,? \\Q1992\\E", "shortCiteRegEx": "Oja.", "year": 1992}, {"title": "Serum proteomic patterns for detection of prostate cancer", "author": ["E.F. Petricoin", "D.K. Ornstein", "C.P. Paweletz", "A.M. Ardekani", "P.S. Hackett", "B.A. Hitt", "A. Velassco", "C. Trucco", "L. Wiegand", "K. Wood", "C.B. Simone", "P.J. Levine", "W.M. Linehan", "M.R. Emmert-Buck", "S.M. Steinberg", "E.C Kohn", "L.A. Liotta"], "venue": "Journal of the National Cancer Institute,", "citeRegEx": "Petricoin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Petricoin et al\\.", "year": 2002}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer.,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer.", "year": 1999}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A. Ng"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Covariance, subspace, and intrinsic cram\u00e9r-rao bounds", "author": ["S.T. Smith"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Smith.,? \\Q2005\\E", "shortCiteRegEx": "Smith.", "year": 2005}, {"title": "Colored maximum variance unfolding", "author": ["L. Song", "A. Smola", "K. Borgwardt", "A. Gretton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "Impact of similarity measures on web-page clustering", "author": ["A. Strehl", "J. Ghosh", "R. Mooney"], "venue": "In Workshop on Artificial Intelligence for Web Search (AAAI),", "citeRegEx": "Strehl et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2000}, {"title": "Large margin component analysis", "author": ["L. Torresani", "K. Lee"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Torresani and Lee.,? \\Q2006\\E", "shortCiteRegEx": "Torresani and Lee.", "year": 2006}, {"title": "Matrix exponentiated gradient updates for on-line learning and bregman projection", "author": ["K. Tsuda", "G. Ratsch", "M. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsuda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsuda et al\\.", "year": 2005}, {"title": "Winnowing subspaces", "author": ["M. Warmuth"], "venue": "Proceedings of the 24th international conference on Machine learning (ICML),", "citeRegEx": "Warmuth.,? \\Q2007\\E", "shortCiteRegEx": "Warmuth.", "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["K. Weinberger", "F. Sha", "L. Saul"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Distance metric learning: A comprehensive survey", "author": ["L. Yang"], "venue": "Technical report, Michigan State University,", "citeRegEx": "Yang.,? \\Q2006\\E", "shortCiteRegEx": "Yang.", "year": 2006}, {"title": "SimpleNPKL: simple non-parametric kernel learning", "author": ["J. Zhuang", "I. Tsang", "S. Hoi"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Zhuang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhuang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration.", "startOffset": 83, "endOffset": 103}, {"referenceID": 47, "context": "This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005).", "startOffset": 247, "endOffset": 294}, {"referenceID": 20, "context": "This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005).", "startOffset": 247, "endOffset": 294}, {"referenceID": 14, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 81, "endOffset": 111}, {"referenceID": 44, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 131, "endOffset": 144}, {"referenceID": 43, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005).", "startOffset": 177, "endOffset": 197}, {"referenceID": 35, "context": "(2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices.", "startOffset": 117, "endOffset": 144}, {"referenceID": 19, "context": "Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005).", "startOffset": 189, "endOffset": 231}, {"referenceID": 5, "context": "Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005).", "startOffset": 189, "endOffset": 231}, {"referenceID": 16, "context": "Our approach makes use of two quotient geometries of the set S+(r, d) that have been recently studied by Journ\u00e9e et al. (2010) and Bonnabel and Sepulchre (2009).", "startOffset": 105, "endOffset": 127}, {"referenceID": 2, "context": "(2010) and Bonnabel and Sepulchre (2009). Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al.", "startOffset": 11, "endOffset": 41}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices.", "startOffset": 84, "endOffset": 1123}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning.", "startOffset": 84, "endOffset": 1385}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning. More recently, algorithmic work has focused on scalability in terms of dimensionality and data set size. A natural extension of the previous work on positive definite matrices is thus to consider low-rank positive semidefinite matrices. Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005). This is a significant complexity reduction as the approximation rank r is typically very small compared to the dimension of the problem d. Extending the work of Tsuda et al. (2005), Kulis et al.", "startOffset": 84, "endOffset": 2085}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning. More recently, algorithmic work has focused on scalability in terms of dimensionality and data set size. A natural extension of the previous work on positive definite matrices is thus to consider low-rank positive semidefinite matrices. Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005). This is a significant complexity reduction as the approximation rank r is typically very small compared to the dimension of the problem d. Extending the work of Tsuda et al. (2005), Kulis et al. (2009) recently considered the learning of positive semidefinite matrices.", "startOffset": 84, "endOffset": 2106}, {"referenceID": 9, "context": "Online learning algorithms (Bottou, 2004) consider possibly infinite sets of samples {(Xt, yt)}t\u22651, received one at a time.", "startOffset": 27, "endOffset": 41}, {"referenceID": 0, "context": "Following Absil et al. (2008), an abstract gradient descent algorithm can then be derived based on the update formula", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Line-search algorithms in quotient Riemannian spaces are discussed in detail in the book of Absil et al. (2008). For the readers convenience, basic concepts and notations are also provided in Appendix A.", "startOffset": 92, "endOffset": 112}, {"referenceID": 33, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 14, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 44, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 17, "context": "The quotient geometries of Gr(r, d) have been well studied (Edelman et al., 1998; Absil et al., 2004).", "startOffset": 59, "endOffset": 101}, {"referenceID": 0, "context": "The quotient geometries of Gr(r, d) have been well studied (Edelman et al., 1998; Absil et al., 2004).", "startOffset": 59, "endOffset": 101}, {"referenceID": 0, "context": "Following Absil et al. (2004), an alternative convenient retraction in Gr(r, d) is given by", "startOffset": 10, "endOffset": 30}, {"referenceID": 33, "context": "With the formulas (6) and (7) applied to the cost function (5), the abstract update (3) becomes Ut+1 = qf(Ut + st(I\u2212UtUt)xtxt Ut), which is Oja\u2019s update for subspace tracking (Oja, 1992).", "startOffset": 175, "endOffset": 186}, {"referenceID": 18, "context": "2 The affine-invariant metric on S+(d) Because S+(d) ' GL(d)/O(d) is the quotient of two Lie groups, its (reductive) geometric structure can be further exploited (Faraut and Koranyi, 1994).", "startOffset": 162, "endOffset": 188}, {"referenceID": 39, "context": "The affine-invariant geometry of S+(d) has been well studied, in particular in the context of information geometry (Smith, 2005).", "startOffset": 115, "endOffset": 128}, {"referenceID": 39, "context": "This approximation coincides with the Riemannian distance that is induced by the affine-invariant metric (12) (Smith, 2005).", "startOffset": 110, "endOffset": 123}, {"referenceID": 16, "context": "With the alternative retraction (15), the update becomes Wt+1 = Wt \u2212 st(\u0177t \u2212 yt)WtSym(Xt)Wt, which is the update of Davis et al. (2007) based on the LogDet divergence (see Section 7.", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "This geometry is studied in detail in the paper (Arsigny et al., 2007).", "startOffset": 48, "endOffset": 70}, {"referenceID": 43, "context": "The gradient of this cost function is given by gradf(S) = (\u0177t \u2212 yt)Sym(Xt), and the retraction is RS(s\u03beS) = exp(log W + s\u03beS) The corresponding gradient descent update is Wt+1 = exp(log Wt \u2212 st(\u0177t \u2212 yt)Sym(Xt)), which is the update of Tsuda et al. (2005) based on the von Neumann divergence.", "startOffset": 234, "endOffset": 254}, {"referenceID": 25, "context": "The quotient geometry of S+(r, d) ' Rd\u00d7r \u2217 /O(r) is studied by Journ\u00e9e et al. (2010).", "startOffset": 63, "endOffset": 85}, {"referenceID": 6, "context": "The Riemannian geometry of (16) has been recently studied (Bonnabel and Sepulchre, 2009).", "startOffset": 58, "endOffset": 88}, {"referenceID": 6, "context": "The geodesics do not appear to have a closed form in this geometry, see the paper of Bonnabel and Sepulchre (2009) for details.", "startOffset": 85, "endOffset": 115}, {"referenceID": 10, "context": "They intrinsically have a much slower convergence rate than batch algorithms, but they generally decrease faster the expected loss in the large-scale regime (Bottou and Bousquet, 2007).", "startOffset": 157, "endOffset": 184}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof).", "startOffset": 143, "endOffset": 445}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof). When the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly.", "startOffset": 143, "endOffset": 602}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof). When the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly. However, we empirically observed that the algorithm always converges to a local minimum of the cost function. Numerical simulations thus suggest that the results of Bottou (1998) on stochastic gradient descent extend to the gradient descent algorithm based on polar parameterization.", "startOffset": 143, "endOffset": 804}, {"referenceID": 26, "context": "1 Closeness-based approaches A standard derivation of learning algorithms is as follows (Kivinen and Warmuth, 1997).", "startOffset": 88, "endOffset": 115}, {"referenceID": 27, "context": "In particular, when \u03bb = 1, the subspace is fixed and one recovers the setup of learning low-rank matrices of fixed range space (Kulis et al., 2009).", "startOffset": 127, "endOffset": 147}, {"referenceID": 27, "context": "Thus the algorithms introduced in the present paper can be viewed as generalizations of the ones presented in (Kulis et al., 2009).", "startOffset": 110, "endOffset": 130}, {"referenceID": 27, "context": "The authors of (Kulis et al., 2009) present the issue of adapting the range space as an open research question.", "startOffset": 15, "endOffset": 35}, {"referenceID": 13, "context": "4 Connection with multidimensional scaling algorithms Given a set of m dissimilarity measures D = {\u03b4ij} between n data objects, multidimensional scaling algorithms search for a r-dimensional embedding of the data objects into an Euclidean space representation G \u2208 Rn\u00d7r (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 269, "endOffset": 312}, {"referenceID": 7, "context": "4 Connection with multidimensional scaling algorithms Given a set of m dissimilarity measures D = {\u03b4ij} between n data objects, multidimensional scaling algorithms search for a r-dimensional embedding of the data objects into an Euclidean space representation G \u2208 Rn\u00d7r (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 269, "endOffset": 312}, {"referenceID": 31, "context": "A stochastic gradient descent approach for minimizing the SSTRESS has also been proposed by Matsuda and Yamaguchi (2001). A potential area of future work is the application of the proposed online algorithm (10) for adapting a batch solution to slight modifications of the dissimilarities over time.", "startOffset": 92, "endOffset": 121}, {"referenceID": 38, "context": "1 Kernel learning In kernel-based methods (Shawe-Taylor and Cristianini, 2004), the data samples x1, .", "startOffset": 42, "endOffset": 78}, {"referenceID": 28, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 29, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 43, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 49, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 27, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 12, "context": "Recently, algorithms have been proposed to learn a kernel function that can be extended to new points (Chatpatanasiri et al., 2008; Jain et al., 2009).", "startOffset": 102, "endOffset": 150}, {"referenceID": 24, "context": "Recently, algorithms have been proposed to learn a kernel function that can be extended to new points (Chatpatanasiri et al., 2008; Jain et al., 2009).", "startOffset": 102, "endOffset": 150}, {"referenceID": 36, "context": "Very popular unsupervised algorithms in that context are kernel principal component analysis (Sch\u00f6lkopf et al., 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 93, "endOffset": 117}, {"referenceID": 13, "context": ", 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 37, "endOffset": 80}, {"referenceID": 7, "context": ", 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 37, "endOffset": 80}, {"referenceID": 46, "context": "Other kernel learning techniques include the maximum variance unfolding algorithm (Weinberger et al., 2004) and its semisupervised version (Song et al.", "startOffset": 82, "endOffset": 107}, {"referenceID": 40, "context": ", 2004) and its semisupervised version (Song et al., 2007), and the kernel spectral regression framework (Cai et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 11, "context": ", 2007), and the kernel spectral regression framework (Cai et al., 2007) which encompasses many reduction criterion (for example, linear discriminant analysis (LDA), locality preserving projection (LPP), neighborhood preserving embedding (NPE)).", "startOffset": 54, "endOffset": 72}, {"referenceID": 48, "context": "See the survey of (Yang, 2006) for a more complete state-of-the-art in this area.", "startOffset": 18, "endOffset": 30}, {"referenceID": 47, "context": "The first proposed methods have been based on successive projections onto a set of large margin constraints (Xing et al., 2002; Shalev-Shwartz et al., 2004).", "startOffset": 108, "endOffset": 156}, {"referenceID": 37, "context": "The first proposed methods have been based on successive projections onto a set of large margin constraints (Xing et al., 2002; Shalev-Shwartz et al., 2004).", "startOffset": 108, "endOffset": 156}, {"referenceID": 21, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 42, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 45, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 16, "context": "Both batch (Davis et al., 2007) and online (Jain et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 23, "context": ", 2007) and online (Jain et al., 2008) formulations have been proposed for learning full-rank matrices.", "startOffset": 19, "endOffset": 38}, {"referenceID": 15, "context": "Low-rank matrices have also been considered with Bregman divergences but only when the range space of the matrix is fixed in the first place (Davis and Dhillon, 2008; Kulis et al., 2009).", "startOffset": 141, "endOffset": 186}, {"referenceID": 27, "context": "Low-rank matrices have also been considered with Bregman divergences but only when the range space of the matrix is fixed in the first place (Davis and Dhillon, 2008; Kulis et al., 2009).", "startOffset": 141, "endOffset": 186}, {"referenceID": 18, "context": "The method proposed by Globerson and Roweis (2005) seeks a Mahalanobis matrix that maximizes the between classes distance while forcing to zero the within classes distance.", "startOffset": 23, "endOffset": 51}, {"referenceID": 40, "context": "Data Set Samples Features Classes Reference GyrB 52 3 Tsuda et al. (2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 50}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 92}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 139}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 188}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 228}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 273}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al.", "startOffset": 23, "endOffset": 311}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al.", "startOffset": 23, "endOffset": 358}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al. (2002)", "startOffset": 23, "endOffset": 404}, {"referenceID": 27, "context": ", 2004), LogDet-KL (Kulis et al., 2009) and LEGO (Jain et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 23, "context": ", 2009) and LEGO (Jain et al., 2008) have been implemented on our own.", "startOffset": 17, "endOffset": 36}, {"referenceID": 40, "context": "The implementations of algorithms MVU3, KSR4, LMNN5 and ITML6 have been rendered publicly available by Weinberger et al. (2004), Cai et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al.", "startOffset": 8, "endOffset": 54}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al. (2007) respectively.", "startOffset": 8, "endOffset": 78}, {"referenceID": 15, "context": "Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data.", "startOffset": 93, "endOffset": 145}, {"referenceID": 45, "context": "Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data.", "startOffset": 93, "endOffset": 145}, {"referenceID": 41, "context": "The quality of the clustering is measured by the normalized mutual information (NMI) shared between the random variables of cluster indicators C and true class labels T (Strehl et al., 2000),", "startOffset": 169, "endOffset": 190}, {"referenceID": 27, "context": "The kernel learning algorithm LogDet-KL (Kulis et al., 2009) which learn kernel matrices of fixed range space for a given set of distance constraints.", "startOffset": 40, "endOffset": 60}, {"referenceID": 11, "context": "The kernel spectral regression (KSR) algorithm of Cai et al. (2007) using a similarity matrix N constructed as follows.", "startOffset": 50, "endOffset": 68}, {"referenceID": 46, "context": "The Maximum Variance Unfolding (MVU) algorithm (Weinberger et al., 2004),", "startOffset": 47, "endOffset": 72}, {"referenceID": 36, "context": "The Kernel PCA algorithm (Sch\u00f6lkopf et al., 1998).", "startOffset": 25, "endOffset": 49}, {"referenceID": 42, "context": "The first experiment is reproduced from Tsuda et al. (2005) and Kulis et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only.", "startOffset": 11, "endOffset": 31}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment.", "startOffset": 11, "endOffset": 508}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. All algorithms start from the identity matrix that do not encode any domain information. Figure 5 (left) reports the k-NN classification accuracy as a function of the number of distance constraints provided. In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. The convergence time of the algorithm based on the polar geometry is however much faster (0.15 seconds versus 58 seconds for LogDet-KL when learning 1000 constraints). The algorithm based on the flat geometry has inferior performance when too few constraints are provided. This is because in the kernel learning setting, updates of this algorithm only involve the rows and columns that correspond to the set of points for which constraints are provided. It may thus result in a partial update of the kernel matrix entries. This issue disappears as the number of provided constraints increases. The second experiment is reproduced from Kulis et al. (2009). It aims at improving an existing low-rank kernel using limited information about class labels.", "startOffset": 11, "endOffset": 1508}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. All algorithms start from the identity matrix that do not encode any domain information. Figure 5 (left) reports the k-NN classification accuracy as a function of the number of distance constraints provided. In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. The convergence time of the algorithm based on the polar geometry is however much faster (0.15 seconds versus 58 seconds for LogDet-KL when learning 1000 constraints). The algorithm based on the flat geometry has inferior performance when too few constraints are provided. This is because in the kernel learning setting, updates of this algorithm only involve the rows and columns that correspond to the set of points for which constraints are provided. It may thus result in a partial update of the kernel matrix entries. This issue disappears as the number of provided constraints increases. The second experiment is reproduced from Kulis et al. (2009). It aims at improving an existing low-rank kernel using limited information about class labels. A rank-16 kernel matrix is computed for clustering a database of 300 handwritten digits randomly sampled from the 3, 8 and 9 digits of the Digits dataset (since we could not find out the specific samples that have been selected by Kulis et al. (2009), we made our own samples selection).", "startOffset": 11, "endOffset": 1855}, {"referenceID": 16, "context": "As in the paper of Davis et al. (2007), we generate the constraints from the learning set of samples as dW(xi,xj) \u2264 l for same-class pairs and dW(xi,xj) \u2265 u for different-class pairs.", "startOffset": 19, "endOffset": 39}, {"referenceID": 16, "context": "ITML (Davis et al., 2007),", "startOffset": 5, "endOffset": 25}, {"referenceID": 45, "context": "LMNN (Weinberger and Saul, 2009),", "startOffset": 5, "endOffset": 32}, {"referenceID": 23, "context": "LEGO (Jain et al., 2008),", "startOffset": 5, "endOffset": 24}, {"referenceID": 37, "context": "POLA (Shalev-Shwartz et al., 2004).", "startOffset": 5, "endOffset": 34}, {"referenceID": 27, "context": "3 Results Reproducing a classical benchmark experiment from Kulis et al. (2009), we demonstrate that the proposed batch algorithms compete with state-of-the-art full-rank Mahalanobis distance learning algorithms on several UCI datasets (Figure 7).", "startOffset": 60, "endOffset": 80}, {"referenceID": 8, "context": "Convergence proof of algorithm (10) Bottou (1998) reviews the mathematical tools required to prove almost sure convergence, that is asymptotic convergence with probability one, of stochastic gradient algorithms.", "startOffset": 36, "endOffset": 50}, {"referenceID": 8, "context": "Technical details are provided in the paper of Bottou (1998).", "startOffset": 47, "endOffset": 61}], "year": 2017, "abstractText": "The paper addresses the problem of learning a regression model parameterized by a fixedrank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.", "creator": "LaTeX with hyperref package"}}}