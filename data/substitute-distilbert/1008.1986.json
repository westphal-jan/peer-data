{"id": "1008.1986", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Aug-2010", "title": "For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia", "abstract": "researcher report on work in progress on extracting lexical collections ( e. g., \" collaborate \" - & gt ; \" work together \" ), focusing on utilizing edit histories in simple english wikipedia for this task. we consider two main approaches : ( 1 ) deriving simplification probabilities via an aggregation model that allowed for sufficient mixture of different operations, and ( 2 ) using metadata to focus on edits that function more likely to manage simplification operations. humans find different methods might outperform a reasonable baseline and yield many high - quality lexical descriptions not suitable in an independently - created manually prepared registry.", "histories": [["v1", "Wed, 11 Aug 2010 20:01:59 GMT  (15kb)", "http://arxiv.org/abs/1008.1986v1", "4 pp; data available atthis http URL"]], "COMMENTS": "4 pp; data available atthis http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mark yatskar", "bo pang", "cristian danescu-niculescu-mizil", "lillian lee"], "accepted": true, "id": "1008.1986"}, "pdf": {"name": "1008.1986.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mark Yatskar", "Bo Pang", "Cristian Danescu-Niculescu-Mizil"], "emails": ["my89@cornell.edu,", "bopang@yahoo-inc.com,", "cristian@cs.cornell.edu,", "llee@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 8.\n19 86\nv1 [\ncs .C\nL ]\n1 1\nA ug\n2 01\n0\nPublished at: NAACL 2010 (short paper)"}, {"heading": "1 Introduction", "text": "Nothing is more simple than greatness; indeed, to be simple is to be great. \u2014Emerson, Literary Ethics\nStyle is an important aspect of information presentation; indeed, different contexts call for different styles. Here, we consider an important dimension of style, namely, simplicity. Systems that can rewrite text into simpler versions promise to make information available to a broader audience, such as non-native speakers, children, laypeople, and so on.\nOne major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting. The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW. Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications.\nRelated work and related problems Previous work usually involves general syntactic-level trans-\n1http://simple.wikipedia.org\nformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., \u201ccollaborate\u201d \u2192 \u201cwork together\u201d), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.\nSimplification is strongly related to but distinct from paraphrasing and machine translation (MT). While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing). Simplification can also be considered to be a form of MT in which the two \u201clanguages\u201d in question are highly related. However, note that ComplexEW and SimpleEW do not together constitute a clean parallel corpus, but rather an extremely noisy comparable corpus. For example, Complex/Simple same-topic document pairs are often written completely independently of each other, and even when it is possible to get good sentence alignments between them, the sentence pairs may reflect operations other than simplification, such as corrections, additions, or edit spam.\nOur work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, \u201ceggcorns\u201d3 [7] and entailments [8]."}, {"heading": "2 Method", "text": "As mentioned above, a key idea in our work is to utilize SimpleEW edits. The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (\u201cfixes\u201d). We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (\u00a72.1), and the use of metadata to filter out undesirable revisions (\u00a72.2).\n2One exception [5] changes verb tense and replaces pronouns. Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].\n3A type of lexical corruption, e.g., \u201cacorn\u201d\u2192\u201ceggcorn\u201d."}, {"heading": "2.1 Edit model", "text": "We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., \u201cCat\u201d) and a sequence ~dk of article versions caused by successive edits. For a given lexical item or phrase A, we write A \u2208 ~dk if there is any version in ~dk that contains A. From each ~dk we extract a collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit instances, repeats allowed, where ek,i = A \u2192 a means that phrase A in one version was changed to a in the next, A 6= a; e.g., \u201cstands for\u201d \u2192 \u201cis the same as\u201d. (We defer detailed description of how we extract lexical edit instances from data to \u00a73.1.) We denote the collection of ~dk in ComplexEW and SimpleEW as C and S, respectively.\nThere are at least four possible edit operations: fix (o1), simplify (o2), no-op (o3), or spam (o4). However, for this initial work we assume P (o4) = 0.4\nLet P (oi | A) be the probability that oi is applied to A, and P (a | A, oi) be the probability of A \u2192 a given that the operation is oi. The key quantities of interest are P (o2 | A) in S, which is the probability that A should be simplified, and P (a | A, o2), which yields proper simplifications of A. We start with an equation that models the probability that a phrase A is edited into a:\nP (a | A) = \u2211\noi\u2208\u2126\nP (oi | A)P (a | A, oi), (1)\nwhere \u2126 is the set of edit operations. This involves the desired parameters, which we solve for by estimating the others from data, as described next.\nEstimation Note that P (a | A, o3) = 0 if A 6= a. Thus, if we have estimates for o1-related probabilities, we can derive o2-related probabilities via Equation 1. To begin with, we make the working assumption that occurrences of simplification in ComplexEW are negligible in comparison to fixes. Since we are also currently ignoring edit spam, we thus assume that only o1 edits occur in ComplexEW.5\nLet fC(A) be the fraction of ~dk in C containing A in which A is modified:\nfC(A) = |{~dk\u2208C|\u2203a,i such that ek,i=A\u2192a}|\n|{~dk\u2208C|A\u2208~dk}| .\n4Spam/vandalism detection is a direction for future work. 5This assumption also provides useful constraints to EM, which we plan to apply in the future, by reducing the number of parameter settings yielding the same likelihood.\nWe similarly define fS(A) on ~dk in S. Note that we count topics (version sequences), not individual versions: if A appears at some point and is not edited until 50 revisions later, we should not conclude that A is unlikely to be rewritten; for example, the intervening revisions could all be minor additions, or part of an edit war.\nIf we assume that the probability of any particular fix operation being applied in SimpleEW is proportional to that in ComplexEW\u2014 e.g., the SimpleEW fix rate might be dampened because already-edited ComplexEW articles are copied over \u2014 we have6\nP\u0302 (o1 | A) = \u03b1fC(A)\nwhere 0 \u2264 \u03b1 \u2264 1. Note that in SimpleEW,\nP (o1 \u2228 o2 | A) = P (o1 | A) + P (o2 | A),\nwhere P (o1 \u2228 o2 | A) is the probability that A is changed to a different word in SimpleEW, which we estimate as P\u0302 (o1 \u2228 o2 | A) = fS(A). We then set\nP\u0302(o2 | A) = max (0, fS(A)\u2212 \u03b1fC(A)).\nNext, under our working assumption, we estimate the probability of A being changed to a as a fix by the proportion of ComplexEW edit instances that rewrite A to a:\nP\u0302 (a | A, o1) = |{(k, i) pairs | ek,i = A \u2192 a \u2227 ~dk \u2208 C}|\u2211 a\u2032 |{(k, i) pairs | ek,i = A \u2192 a \u2032 \u2227 ~dk \u2208 C}| .\nA natural estimate for the conditional probability of A being rewritten to a under any operation type is based on observations of A \u2192 a in SimpleEW, since that is the corpus wherein both operations are assumed to occur:\nP\u0302 (a | A) = |{(k, i) pairs | ek,i = A \u2192 a \u2227 ~dk \u2208 S}|\u2211 a\u2032 |{(k, i) pairs | ek,i = A \u2192 a \u2032 \u2227 ~dk \u2208 S}| .\nThus, from (1) we get that for A 6= a:\nP\u0302(a | A,o2) = P\u0302(a | A)\u2212 P\u0302(o1 | A)P\u0302(a | A,o1)\nP\u0302(o2 | A) ."}, {"heading": "2.2 Metadata-based methods", "text": "Wiki editors have the option of associating a comment with each revision, and such comments sometimes indicate the intent of the revision. We therefore sought to use comments to identify \u201ctrusted\u201d\n6Throughout, \u201chats\u201d denote estimates.\nrevisions wherein the extracted lexical edit instances (see \u00a73.1) would be likely to be simplifications.\nLet ~rk = (r1k, . . . , r i k, . . .) be the sequence of revisions for the kth article in SimpleEW, where rik is the set of lexical edit instances (A \u2192 a) extracted from the ith modification of the document. Let cik be the comment that accompanies rik, and conversely, let R(Set) = {rik|c i k \u2208 Set}.\nWe start with a seed set of trusted comments, Seed. To initialize it, we manually inspected a small sample of the 700K+ SimpleEW revisions that bear comments, and found that comments containing a word matching the regular expression *simpl* (e.g, \u201csimplify\u201d) seem promising. We thus set Seed := { \u2217 simpl\u2217} (abusing notation).\nThe SIMPL method Given a set of trusted revisions TRev (in our case TRev = R(Seed)), we score each A \u2192 a \u2208 TRev by the point-wise mutual information (PMI) between A and a.7 We write RANK(TRev) to denote the PMI-based ranking of A \u2192 a \u2208 TRev, and use SIMPL to denote our most basic ranking method, RANK(R(Seed)).\nTwo ideas for bootstrapping We also considered bootstrapping as a way to be able to utilize revisions whose comments are not in the initial Seed set.\nOur first idea was to iteratively expand the set of trusted comments to include those that most often accompany already highly ranked simplifications. Unfortunately, our initial implementations involved many parameters (upper and lower commentfrequency thresholds, number of highly ranked simplifications to consider, number of comments to add per iteration), making it relatively difficult to tune; we thus omit its results.\nOur second idea was to iteratively expand the set of trusted revisions, adding those that contain already highly ranked simplifications. While our initial implementation had fewer parameters than the method sketched above, it tended to terminate quickly, so that not many new simplifications were found; so, again, we do not report results here.\nAn important direction for future work is to differentially weight the edit instances within a revision, as opposed to placing equal trust in all of them; this\n7PMI seemed to outperform raw frequency and conditional probability.\ncould prevent our bootstrapping methods from giving common fixes (e.g., \u201ca\u201d \u2192 \u201cthe\u201d) high scores.\n3 Evaluation8"}, {"heading": "3.1 Data", "text": "We obtained the revision histories of both SimpleEW (November 2009 snapshot) and ComplexEW (January 2008 snapshot). In total, \u223c1.5M revisions for 81733 SimpleEW articles were processed (only 30% involved textual changes). For ComplexEW, we processed \u223c16M revisions for 19407 articles.\nExtracting lexical edit instances. For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes). From the aligned sentence pairs, we obtained the aforementioned lexical edit instances A \u2192 a. Since the focus of our study was not word alignment, we used a simple method that identified the longest differing segments (based on word boundaries) between each sentence, except that to prevent the extraction of entire (highly nonmatching) sentences, we filtered out A \u2192 a pairs if either A or a contained more than five words."}, {"heading": "3.2 Comparison points", "text": "Baselines RANDOM returns lexical edit instances drawn uniformly at random from among those extracted from SimpleEW. FREQUENT returns the most frequent lexical edit instances extracted from SimpleEW.\nDictionary of simplifications The SimpleEW editor \u201cSpencerk\u201d (Spencer Kelly) has assembled a list of simple words and simplifications using a combination of dictionaries and manual effort9. He provides a list of 17,900 simple words \u2014 words that do not need further simplification \u2014 and a list of 2000 transformation pairs. We did not use Spencerk\u2019s set as the gold standard because many transformations we found to be reasonable were not on his list. Instead, we measured our agreement with the list of transformations he assembled (SPLIST).\n8Results at http://www.cs.cornell.edu/home/llee/data/simple 9http://www.spencerwaterbed.com/soft/simple/about.html"}, {"heading": "3.3 Preliminary results", "text": "The top 100 pairs from each system (edit model10 and SIMPL and the two baselines) plus 100 randomly selected pairs from SPLIST were mixed and all presented in random order to three native English speakers and three non-native English speakers (all non-authors). Each pair was presented in random orientation (i.e., either as A \u2192 a or as a \u2192 A), and the labels included \u201csimpler\u201d, \u201cmore complex\u201d, \u201cequal\u201d, \u201cunrelated\u201d, and \u201c?\u201d (\u201chard to judge\u201d). The first two labels correspond to simplifications for the orientations A \u2192 a and a \u2192 A, respectively. Collapsing the 5 labels into \u201csimplification\u201d, \u201cnot a simplification\u201d, and \u201c?\u201d yields reasonable agreement among the 3 native speakers (\u03ba = 0.69; 75.3% of the time all three agreed on the same label). While we postulated that non-native speakers11 might be more sensitive to what was simpler, we note that they disagreed more than the native speakers (\u03ba = 0.49) and reported having to consult a dictionary. The nativespeaker majority label was used in our evaluations.\nHere are the results; \u201c-x-y\u201d means that x and y are the number of instances discarded from the precision calculation for having no majority label or majority label \u201c?\u201d, respectively:\nMethod Prec@100 # of pairs SPLIST 86% (-0-0) 2000\nEdit model 77% (-0-1) 1079 SIMPL 66% (-0-0) 2970\nFREQUENT 17% (-1-7) - RANDOM 17% (-1-4) -\nBoth baselines yielded very low precisions \u2014 clearly not all (frequent) edits in SimpleEW were simplifications. Furthermore, the edit model yielded higher precision than SIMPL for the top 100 pairs. (Note that we only examined one simplification per A for those A where P\u0302 (o2 | A) was well-defined; thus \u201c# of pairs\u201d does not directly reflect the full potential recall that either method can achieve.) Both, however, produced many high-quality pairs (62% and 71% of the correct pairs) not included in SPLIST. We also found the pairs produced by these two systems to be complementary to each other. We\n10We only considered those A such that freq(A \u2192 \u2217) > 1 \u2227 freq(A) > 100 on both SimpleEW and ComplexEW. The final top 100 A \u2192 a pairs were those with As with the highest P (o2 | A). We set \u03b1 = 1.\n11Native languages: Russian; Russian; Russian and Kazakh.\nbelieve that these two approaches provide a good starting point for further explorations.\nFinally, some examples of simplifications found by our methods: \u201cstands for\u201d \u2192 \u201cis the same as\u201d, \u201cindigenous\u201d \u2192 \u201cnative\u201d, \u201cpermitted\u201d \u2192 \u201callowed\u201d, \u201cconcealed\u201d \u2192 \u201chidden\u201d, \u201ccollapsed\u201d \u2192 \u201cfell down\u201d, \u201cannually\u201d \u2192 \u201cevery year\u201d."}, {"heading": "3.4 Future work", "text": "Further evaluation could include comparison with machine-translation and paraphrasing algorithms. It would be interesting to use our proposed estimates as initialization for EM-style iterative re-estimation. Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length. Acknowledgments We first wish to thank Ainur Yessenalina for initial investigations and helpful comments. We are also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J. Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the reviewers for helpful comments; W. Arms and L. Walle for access to the Cornell Hadoop cluster; J. Cantwell for access to computational resources; R. Hwa & A. Owens for annotation software; M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J. Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annotations. Supported by NSF grant IIS-0910664."}], "references": [{"title": "Automatic induction of rules for text simplification", "author": ["R. Chandrasekar", "B. Srinivas"], "venue": "Knowledge-Based Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora", "author": ["L. Del\u00e9ger", "P. Zweigenbaum"], "venue": "Workshop on Building and Using Comparable Corpora,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The use of a psycholinguistic database in the simplification of text for aphasic readers", "author": ["S. Devlin", "J. Tait"], "venue": "In Linguistic Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Mining a lexicon of technical terms and lay equivalents", "author": ["N. Elhadad", "K. Sutaria"], "venue": "Workshop on BioNLP,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Text simplification for information-seeking applications", "author": ["B. Beigman Klebanov", "K. Knight", "D. Marcu"], "venue": "OTM Conferences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Towards robust context-sensitive sentence alignment for monolingual corpora", "author": ["R. Nelken", "S.M. Shieber"], "venue": "EACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Mining Wikipedia\u2019s article revision history for training computational linguistics algorithms", "author": ["R. Nelken", "E. Yamangil"], "venue": "WikiAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Extracting lexical reference rules from Wikipedia", "author": ["E. Shnarch", "L. Barak", "I. Dagan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Syntactic simplification for improving content selection in multidocument summarization", "author": ["A. Siddharthan", "A. Nenkova", "K. McKeown"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Sentence simplification for semantic role labeling", "author": ["D. Vickrey", "D. Koller"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 8, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 9, "context": "org formation rules [1, 9, 10].", "startOffset": 20, "endOffset": 30}, {"referenceID": 6, "context": "g, \u201ceggcorns\u201d3 [7] and entailments [8].", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "g, \u201ceggcorns\u201d3 [7] and entailments [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "One exception [5] changes verb tense and replaces pronouns.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 49, "endOffset": 55}, {"referenceID": 1, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 49, "endOffset": 55}, {"referenceID": 2, "context": "Other lexical-level work focuses on medical text [4, 2], or uses frequency-filtered WordNet synonyms [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes).", "startOffset": 132, "endOffset": 135}], "year": 2010, "abstractText": "We report on work in progress on extracting lexical simplifications (e.g., \u201ccollaborate\u201d \u2192 \u201cwork together\u201d), focusing on utilizing edit histories in Simple English Wikipedia for this task. We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations. We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list. Published at: NAACL 2010 (short paper)", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}