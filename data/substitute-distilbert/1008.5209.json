{"id": "1008.5209", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2010", "title": "Network Flow Algorithms for Structured Sparsity", "abstract": "we consider a matrix of learning problems that involve a structured sparsity - inducing norm with unique smooth sum of $ \\ ell _ \\ infty $ - norms over groups dependent variables. whereas a lot special attention has been put in developing fast optimization methods requiring the groups are disjoint or converge in a specific hierarchical structure, we address here the case of general overlapping groups. to this end, we show that the corresponding optimization problem becomes related to network flow optimization. more precisely, the proximal problem associated with the norm we consider is dual to a quadratic n - cost diffusion problem. we propose an efficient procedure which computes its solution exactly in polynomial time. our algorithm scales up twelve millions cubic variables, and brought up a whole new range concerning applications for performing sparse models. we present several experiments on image and video data, demonstrating the applicability and significance of our approach for various problems.", "histories": [["v1", "Tue, 31 Aug 2010 03:39:49 GMT  (560kb)", "http://arxiv.org/abs/1008.5209v1", "accepted for publication in Adv. Neural Information Processing Systems, 2010"]], "COMMENTS": "accepted for publication in Adv. Neural Information Processing Systems, 2010", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["julien mairal", "rodolphe jenatton", "guillaume obozinski", "francis r bach"], "accepted": true, "id": "1008.5209"}, "pdf": {"name": "1008.5209.pdf", "metadata": {"source": "CRF", "title": "Network Flow Algorithms for Structured Sparsity", "authors": ["Julien Mairal", "Francis Bach", "Rodolphe Jenatton", "Guillaume Obozinski"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n00 8.\n52 09\nv1 [\ncs .L\nG ]\n3 1\nA ug\n2 01\n0\nappor t de r ech er ch e\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 73\n72 --\nF R\n+ E\nN G\nVision, Perception and Multimedia Understanding"}, {"heading": "INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE", "text": ""}, {"heading": "Network Flow Algorithms for Structured Sparsity", "text": ""}, {"heading": "Julien Mairal \u2014 Rodolphe Jenatton \u2014 Guillaume Obozinski \u2014 Francis Bach", "text": "N\u00b0 7372\nAugust 2010"}, {"heading": "Centre de recherche INRIA Paris \u2013 Rocquencourt Domaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay Cedex", "text": "T\u00e9l\u00e9phone : +33 1 39 63 55 11 \u2014 T\u00e9l\u00e9copie : +33 1 39 63 53 30"}, {"heading": "Network Flow Algorithms for Structured Sparsity", "text": ""}, {"heading": "Julien Mairal\u2217\u2020 , Rodolphe Jenatton\u2217\u2020 , Guillaume Obozinski\u2020 , Francis Bach\u2020", "text": "Theme : Vision, Perception and Multimedia Understanding Perception, Cognition, Interaction\n\u00c9quipe-Projet Willow\nRapport de recherche n\u00b0 7372 \u2014 August 2010 \u2014 23 pages\nAbstract: We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of \u2113\u221e-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.\nKey-words: network flow optimization, convex optimization, sparse methods, proximal algorithms\n\u2217 Equal contribution. \u2020 INRIA - WILLOW Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS\nUMR 8548). 23, avenue d\u2019Italie, 75214 Paris. France"}, {"heading": "Algorithmes de Flots pour Parcimonie Structur\u00e9e", "text": "R\u00e9sum\u00e9 : Nous consid\u00e9rons une classe de probl\u00e8mes d\u2019apprentissage r\u00e9gularis\u00e9s par une norme induisant de la parcimonie structur\u00e9e, d\u00e9finie comme une somme de normes \u2113\u221e sur des groupes de variables. Alors que de nombreux efforts ont \u00e9t\u00e9s mis pour d\u00e9velopper des algorithmes d\u2019optimisation rapides lorsque les groupes sont disjoints ou structur\u00e9s hi\u00e9rarchiquement, nous nous int\u00e9ressons au cas g\u00e9n\u00e9ral de groupes avec recouvrement. Nous montrons que le probl\u00e8me d\u2019optimisation correspondant est li\u00e9 \u00e0 l\u2019optimisation de flots sur un r\u00e9seau. Plus pr\u00e9cis\u00e9ment, l\u2019op\u00e9rateur proximal associ\u00e9 \u00e0 la norme que nous consid\u00e9rons est dual \u00e0 la minimisation d\u2019un co\u00fbt quadratique de flot sur un graphe particulier. Nous proposons une proc\u00e9dure efficace qui calcule cette solution en un temps polynomial. Notre algorithme peut traiter de larges probl\u00e8mes, comportant des millions de variables, et ouvre de nouveaux champs d\u2019applications pour les mod\u00e8les parcimonieux structur\u00e9s. Nous pr\u00e9sentons diverses exp\u00e9riences sur des donn\u00e9es d\u2019images et de vid\u00e9os, qui d\u00e9montrent l\u2019utilit\u00e9 et l\u2019efficacit\u00e9 de notre approche pour r\u00e9soudre de nombreux probl\u00e8mes.\nMots-cl\u00e9s : optimisation de flots, optimisation convexe, m\u00e9thodes parcimonieuses, algorithmes proximaux"}, {"heading": "Network Flow Algorithms for Structured Sparsity 3", "text": ""}, {"heading": "1 Introduction", "text": "Sparse linear models have become a popular framework for dealing with various unsupervised and supervised tasks in machine learning and signal processing. In such models, linear combinations of small sets of variables are selected to describe the data. Regularization by the \u21131-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].\nThe \u21131-norm primarily encourages sparse solutions, regardless of the potential structural relationships (e.g., spatial, temporal or hierarchical) existing between the variables. Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].\nBy considering sums of norms of appropriate subsets, or groups, of variables, these regularizations control the sparsity patterns of the solutions. The underlying optimization problem is usually difficult, in part because it involves nonsmooth components. Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their ability to deal with large problems [3, 4]. While the settings where the penalized groups of variables do not overlap [12] or are embedded in a tree-shaped hierarchy [11] have already been studied, sparsityinducing regularizations of general overlapping groups have, to the best of our knowledge, never been considered within the proximal method framework.\nThis paper makes the following contributions:\n\u2022 It shows that the proximal operator associated with the structured norm we consider can be computed by solving a quadratic min-cost flow problem, thereby establishing a connection with the network flow optimization literature.\n\u2022 It presents a fast and scalable procedure for solving a large class of structured sparse regularized problems, which, to the best of our knowledge, have not been addressed efficiently before.\n\u2022 It shows that the dual norm of the sparsity-inducing norm we consider can also be evaluated efficiently, which enables us to compute duality gaps for the corresponding optimization problems.\n\u2022 It demonstrates that our method is relevant for various applications, from video background subtraction to estimation of hierarchical structures for dictionary learning of natural image patches."}, {"heading": "2 Structured Sparse Models", "text": "We consider in this paper convex optimization problems of the form\nmin w\u2208Rp f(w) + \u03bb\u2126(w), (1)\nwhere f : Rp \u2192 R is a convex differentiable function and \u2126 : Rp \u2192 R is a convex, nonsmooth, sparsity-inducing regularization function. When one knows a priori that the solutions of this learning problem only have a few non-zero coefficients, \u2126 is often chosen to be the \u21131-norm, leading for instance to the Lasso [13]. When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16]. Such a penalty might for example take the form\n\u2126(w) , \u2211\ng\u2208G\n\u03b7g max j\u2208g\n|wj | = \u2211\ng\u2208G\n\u03b7g\u2016wg\u2016\u221e, (2)\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 4", "text": "where G is a set of groups of indices, wj denotes the j-th coordinate of w for j in [1; p] , {1, . . . , p}, the vector wg in R|g| represents the coefficients of w indexed by g in G, and the scalars \u03b7g are positive weights. A sum of \u21132-norms is also used in the literature [7], but the \u2113\u221e-norm is piecewise linear, a property that we take advantage of in this paper. Note that when G is the set of singletons of [1; p], we get back the \u21131-norm.\nIf G is a more general partition of [1; p], variables are selected in groups rather than individually. When the groups overlap, \u2126 is still a norm and sets groups of variables to zero together [5]. The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].1 Solving Eq. (1) in this context becomes challenging and is the topic of this paper. Following [11] who tackled the case of hierarchical groups, we propose to approach this problem with proximal methods, which we now introduce."}, {"heading": "2.1 Proximal Methods", "text": "In a nutshell, proximal methods can be seen as a natural extension of gradient-based techniques, and they are well suited to minimizing the sum f + \u03bb\u2126 of two convex terms, a smooth function f \u2014continuously differentiable with Lipschitz-continuous gradient\u2014 and a potentially non-smooth function \u03bb\u2126 (see [18] and references therein). At each iteration, the function f is linearized at the current estimate w0 and the so-called proximal problem has to be solved:\nmin w\u2208Rp\nf(w0) + (w \u2212w0) \u22a4\u2207f(w0) + \u03bb\u2126(w) +\nL 2 \u2016w\u2212w0\u2016 2 2.\nThe quadratic term keeps the solution in a neighborhood where the current linear approximation holds, and L > 0 is an upper bound on the Lipschitz constant of \u2207f . This problem can be rewritten as\nmin w\u2208Rp\n1 2 \u2016u\u2212w\u201622 + \u03bb \u2032\u2126(w), (3)\nwith \u03bb\u2032 , \u03bb/L, and u , w0 \u2212 1L\u2207f(w0). We call proximal operator associated with the regularization \u03bb\u2032\u2126 the function that maps a vector u in Rp onto the (unique, by strong convexity) solution w\u22c6 of Eq. (3). Simple proximal method use w\u22c6 as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates. Note that when \u2126 is the \u21131-norm, the solution of Eq. (3) is obtained by a soft-thresholding [18].\nThe approach we develop in the rest of this paper extends [11] to the case of general overlapping groups when \u2126 is a weighted sum of \u2113\u221e-norms, broadening the application of these regularizations to a wider spectrum of problems.2"}, {"heading": "3 A Quadratic Min-Cost Flow Formulation", "text": "In this section, we show that a convex dual of problem (3) for general overlapping groups G can be reformulated as a quadratic min-cost flow problem. We propose an efficient algorithm to solve it exactly, as well as a related algorithm to compute the dual norm of \u2126. We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u2126 is a sum of \u2113\u221e-norms:\n1Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].\n2For hierarchies, the approach of [11] applies also to the case of where \u2126 is a weighted sum of \u21132-norms.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 5", "text": "Lemma 1 (Dual of the proximal problem [11]) Given u in Rp, consider the problem\nmin \u03be\u2208Rp\u00d7|G|\n1 2 \u2016u\u2212 \u2211\ng\u2208G\n\u03beg\u201622 s.t. \u2200g \u2208 G, \u2016\u03be g\u20161 \u2264 \u03bb\u03b7g and \u03be g j = 0 if j /\u2208 g, (4)\nwhere \u03be = (\u03beg)g\u2208G is in R p\u00d7|G|, and \u03begj denotes the j-th coordinate of the vector \u03be g. Then, every solution \u03be\u22c6=(\u03be\u22c6g)g\u2208G of Eq. (4) satisfies w \u22c6=u\u2212 \u2211 g\u2208G \u03be \u22c6g, where w\u22c6 is the solution of Eq. (3).\nWithout loss of generality,3 we assume from now on that the scalars uj are all non-negative, and we constrain the entries of \u03be to be non-negative. We now introduce a graph modeling of problem (4)."}, {"heading": "3.1 Graph Model", "text": "Let G be a directed graph G = (V,E, s, t), where V is a set of vertices, E \u2286 V \u00d7 V a set of arcs, s a source, and t a sink. Let c and c\u2032 be two functions on the arcs, c : E \u2192 R and c\u2032 : E \u2192 R+, where c is a cost function and c\u2032 is a non-negative capacity function. A flow is a non-negative function on arcs that satisfies capacity constraints on all arcs (the value of the flow on an arc is less than or equal to the arc capacity) and conservation constraints on all vertices (the sum of incoming flows at a vertex is equal to the sum of outgoing flows) except for the source and the sink.\nWe introduce a canonical graph G associated with our optimization problem, and uniquely characterized by the following construction: (i) V is the union of two sets of vertices Vu and Vgr , where Vu contains exactly one vertex for each index j in [1; p], and Vgr contains exactly one vertex for each group g in G. We thus have |V | = |G|+ p. For simplicity, we identify groups and indices with the vertices of the graph. (ii) For every group g in G, E contains an arc (s, g). These arcs have capacity \u03bb\u03b7g and zero cost. (iii) For every group g in G, and every index j in g, E contains an arc (g, j) with zero cost and infinite capacity. We denote by \u03begj the flow on this arc. (iv) For every index j in [1; p], E contains an arc (j, t) with infinite capacity and a cost cj, 12 (uj \u2212 \u03be\u0304j) 2, where \u03be\u0304j is the flow on (j, t). Note that by flow conservation, we necessarily have \u03be\u0304j= \u2211 g\u2208G \u03be g j .\nExamples of canonical graphs are given in Figures 1(a)-(c). The flows \u03begj associated with G can now be identified with the variables of problem (4): indeed, the sum of the costs on the edges leading to the sink is equal to the objective function of (4), while the capacities of the arcs (s, g) match the constraints on each group. This shows that finding a flow minimizing the sum of the costs on such a graph is equivalent to solving problem (4).\nWhen some groups are included in others, the canonical graph can be simplified to yield a graph with a smaller number of edges. Specifically, if h and g are groups with h \u2282 g, the edges (g, j) for j \u2208 h carrying a flow \u03begj can be removed and replaced by a single edge (g, h) of infinite capacity and zero cost, carrying the flow \u2211\nj\u2208h \u03be g j . This simplification is illustrated in Figure 1(d), with a\ngraph equivalent to the one of Figure 1(c). This does not change the optimal value of \u03be\u0304 \u22c6 , which is the quantity of interest for computing the optimal primal variable w\u22c6. We present in Appendix A a formal definition of equivalent graphs. These simplifications are useful in practice, since they reduce the number of edges in the graph and improve the speed of the algorithms we are now going to present.\n3 Let \u03be\u22c6 denote a solution of Eq. (4). Optimality conditions of Eq. (4) derived in [11] show that for all j in [1; p], the signs of the non-zero coefficients \u03be\u22c6g\nj for g in G are the same as the signs of the entries uj . To solve Eq. (4), one\ncan therefore flip the signs of the negative variables uj , then solve the modified dual formulation (with non-negative variables), which gives the magnitude of the entries \u03be\u22c6g\nj (the signs of these being known).\nRR n\u00b0 7372\nNetwork Flow Algorithms for Structured Sparsity 6\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 7", "text": ""}, {"heading": "3.2 Computation of the Proximal Operator", "text": "Quadratic min-cost flow problems have been well studied in the operations research literature [19]. One of the simplest cases, where G contains a single group g as in Figure 1(a), can be solved by an orthogonal projection on the \u21131-ball of radius \u03bb\u03b7g. It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations. When the group structure is a tree as in Figure 1(d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O(pd) operations, where d is the depth of the tree.\nThe general case of overlapping groups is more difficult. Hochbaum and Hong have shown in [19] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [22].4 While this approach could be used to solve Eq. (4), it ignores the fact that our graphs have non-zero costs only on edges leading to the sink. To take advantage of this specificity, we propose the dedicated Algorithm 1. Our method clearly shares some similarities with a simplified version of [22] presented in [23], namely a divide and conquer strategy. Nonetheless, we performed an empirical comparison described in Appendix D, which shows that our dedicated algorithm has significantly better performance in practice. Informally,\nAlgorithm 1 Computation of the proximal operator for overlapping groups. 1: Inputs: u \u2208 Rp, a set of groups G, positive weights (\u03b7g)g\u2208G , and \u03bb (regularization parameter).\n2: Build the initial graph G0 = (V0, E0, s, t) as explained in Section 3.2. 3: Compute the optimal flow: \u03be\u0304 \u2190 computeFlow(V0, E0). 4: Return: w = u\u2212 \u03be\u0304 (optimal solution of the proximal problem).\nFunction computeFlow(V = Vu \u222a Vgr, E)\n1: Projection step: \u03b3 \u2190 argmin\u03b3 \u2211 j\u2208Vu 1 2 (uj \u2212 \u03b3j) 2 s.t. \u2211 j\u2208Vu \u03b3j \u2264 \u03bb \u2211 g\u2208Vgr \u03b7g. 2: For all nodes j in Vu, set \u03b3j to be the capacity of the arc (j, t). 3: Max-flow step: Update (\u03be\u0304j)j\u2208Vu by computing a max-flow on the graph (V,E, s, t). 4: if \u2203 j \u2208 Vu s.t. \u03be\u0304j 6= \u03b3j then 5: Denote by (s, V +) and (V \u2212, t) the two disjoint subsets of (V, s, t) separated by the minimum\n(s, t)-cut of the graph, and remove the arcs between V + and V \u2212. Call E+ and E\u2212 the two remaining disjoint subsets of E corresponding to V + and V \u2212.\n6: (\u03be\u0304j)j\u2208V +u \u2190 computeFlow(V +, E+). 7: (\u03be\u0304j)j\u2208V \u2212u \u2190 computeFlow(V \u2212, E\u2212). 8: end if 9: Return: (\u03be\u0304j)j\u2208Vu .\ncomputeFlow(V0, E0) returns the optimal flow vector \u03be\u0304, proceeding as follows: This function first solves a relaxed version of problem Eq. (4) obtained by replacing the sum of the vectors \u03beg by a single vector \u03b3 whose \u21131-norm should be less than, or equal to, the sum of the constraints on the vectors \u03beg. The optimal vector \u03b3 therefore gives a lower bound ||u \u2212 \u03b3||22/2 on the optimal cost. Then, the maximum-flow step [24] tries to find a feasible flow such that the vector \u03be\u0304 matches \u03b3. If \u03be\u0304 = \u03b3, then the cost of the flow reaches the lower bound, and the flow is optimal. If \u03be\u0304 6= \u03b3, the lower bound cannot be reached, and we construct a minimum (s, t)-cut of the graph [25] that defines two disjoints sets of nodes V + and V \u2212; V + is the part of the graph that can potentially receive more flow from the source, whereas all arcs linking s to V \u2212 are saturated. The properties of a min (s, t)-cut [26] imply that there are no arcs from V + to V \u2212 (arcs inside V have infinite\n4By definition, a parametric max-flow problem consists in solving, for every value of a parameter, a max-flow problem on a graph whose arc capacities depend on this parameter.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 8", "text": "capacity by construction), and that there is no flow on arcs from V \u2212 to V +. At this point, it is possible to show that the value of the optimal min-cost flow on these arcs is also zero. Thus, removing them yields an equivalent optimization problem, which can be decomposed into two independent problems of smaller size and solved recursively by the calls to computeFlow(V+, E+) and computeFlow(V\u2212, E\u2212). Note that when \u2126 is the \u21131-norm, our algorithm solves problem (4) during the first projection step in line 1 and stops. A formal proof of correctness of Algorithm 1 and further details are relegated to Appendix B.\nThe approach of [19, 22] is guaranteed to have the same worst-case complexity as a single maxflow algorithm. However, we have experimentally observed a significant discrepancy between the worst case and empirical complexities for these flow problems, essentially because the empirical cost of each max-flow is significantly smaller than its theoretical cost. Despite the fact that the worstcase guarantee of our algorithm is weaker than their (up to a factor |V |), it is more adapted to the structure of our graphs and has proven to be much faster in our experiments (see supplementary material).\nSome implementation details are crucial to the efficiency of the algorithm:\n\u2022 Exploiting maximal connected components: When there exists no arc between two subsets of V , it is possible to process them independently to solve the global min-cost flow problem. To that effect, before calling the function computeFlow(V,E), we look for maximal connected components (V1, E1), . . . , (VN , EN ) and call sequentially the procedure computeFlow(Vi, Ei) for i in [1;N ].\n\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]). Our implementation uses the so-called \u201chighest-active vertex selection rule, global and gap heuristics\u201d (see [24, 27]), and has a worst-case complexity of O(|V |2|E|1/2) for a graph (V,E, s, t). This algorithm leverages the concept of pre-flow that relaxes the definition of flow and allows vertices to have a positive excess.\n\u2022 Using flow warm-restarts: Our algorithm can be initialized with any valid pre-flow, enabling warm-restarts when the max-flow is called several times as in our algorithm.\n\u2022 Improved projection step: The first line of the procedure computeFlow can be replaced by \u03b3 \u2190 argmin\u03b3 \u2211 j\u2208Vu 1 2 (uj \u2212 \u03b3j) 2 s.t. \u2211 j\u2208Vu \u03b3j \u2264 \u03bb \u2211 g\u2208Vgr \u03b7g and |\u03b3j | \u2264 \u03bb \u2211 g\u220bj \u03b7g. The\nidea is that the structure of the graph will not allow \u03be\u0304j to be greater than \u03bb \u2211\ng\u220bj \u03b7g after the max-flow step. Adding these additional constraints leads to better performance when the graph is not well balanced. This modified projection step can still be computed in linear time [21]."}, {"heading": "3.3 Computation of the Dual Norm", "text": "The dual norm \u2126\u2217 of \u2126, defined for any vector \u03ba in Rp by \u2126\u2217(\u03ba) , max\u2126(z)\u22641 z\u22a4\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28]. We use it here to monitor the convergence of the proximal method through a duality gap, and define a proper optimality criterion for problem (1). We denote by f\u2217 the Fenchel conjugate of f [29], defined by f\u2217(\u03ba) , supz[z\n\u22a4\u03ba\u2212f(z)]. The duality gap for problem (1) can be derived from standard Fenchel duality arguments [29] and it is equal to f(w) + \u03bb\u2126(w) + f\u2217(\u2212\u03ba) for w,\u03ba in Rp with \u2126\u2217(\u03ba) \u2264 \u03bb. Therefore, evaluating the duality gap requires to compute efficiently \u2126\u2217 in order to find a feasible dual variable \u03ba. This is equivalent to solving another network flow problem, based on the following variational formulation:\n\u2126\u2217(\u03ba) = min \u03be\u2208Rp\u00d7|G|\n\u03c4 s.t. \u2211\ng\u2208G\n\u03beg = \u03ba, and \u2200g \u2208 G, \u2016\u03beg\u20161 \u2264 \u03c4\u03b7g with \u03be g j = 0 if j /\u2208 g. (5)\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 9", "text": "In the network problem associated with (12), the capacities on the arcs (s, g), g \u2208 G, are set to \u03c4\u03b7g , and the capacities on the arcs (j, t), j in [1; p], are fixed to \u03baj . Solving problem (12) amounts to finding the smallest value of \u03c4 , such that there exists a flow saturating the capacities \u03baj on the arcs leading to the sink t (i.e., \u03be\u0304 = \u03ba). Equration (12) and the algorithm below are proven to be correct in Appendix B.\nAlgorithm 2 Computation of the dual norm. 1: Inputs: \u03ba \u2208 Rp, a set of groups G, positive weights (\u03b7g)g\u2208G . 2: Build the initial graph G0 = (V0, E0, s, t) as explained in Section 3.3. 3: \u03c4 \u2190 dualNorm(V0, E0). 4: Return: \u03c4 (value of the dual norm).\nFunction dualNorm(V = Vu \u222a Vgr , E)\n1: \u03c4\u2190( \u2211\nj\u2208Vu \u03baj)/(\n\u2211\ng\u2208Vgr \u03b7g) and set the capacities of arcs (s, g) to \u03c4\u03b7g for all g in Vgr .\n2: Max-flow step: Update (\u03be\u0304j)j\u2208Vu by computing a max-flow on the graph (V,E, s, t). 3: if \u2203 j \u2208 Vu s.t. \u03be\u0304j 6= \u03baj then 4: Define (V +, E+) and (V \u2212, E\u2212) as in Algorithm 1, and set \u03c4 \u2190 dualNorm(V \u2212, E\u2212). 5: end if 6: Return: \u03c4 ."}, {"heading": "4 Applications and Experiments", "text": "Our experiments use the algorithm of [4] based on our proximal operator, with weights \u03b7g set to 1. We present this algorithm in more details in Appendix C."}, {"heading": "4.1 Speed Comparison", "text": "We compare our method (ProxFlow) and two generic optimization techniques, namely a subgradient descent (SG) and an interior point method,5 on a regularized linear regression problem. Both SG and ProxFlow are implemented in C++. Experiments are run on a single-core 2.8 GHz CPU. We consider a design matrix X in Rn\u00d7p built from overcomplete dictionaries of discrete cosine transforms (DCT), which are naturally organized on one- or two-dimensional grids and display local correlations. The following families of groups G using this spatial information are thus considered: (1) every contiguous sequence of length 3 for the one-dimensional case, and (2) every 3\u00d73-square in the two-dimensional setting. We generate vectors y in Rn according to the linear model y = Xw0+\u03b5, where \u03b5 \u223c N (0, 0.01\u2016Xw0\u201622). The vectorw0 has about 20% percent nonzero components, randomly selected, while respecting the structure of G, and uniformly generated between [\u22121, 1].\nIn our experiments, the regularization parameter \u03bb is chosen to achieve this level of sparsity. For SG, we take the step size to be equal to a/(k + b), where k is the iteration number, and (a, b) are the best parameters selected in {10\u22123, . . . , 10}\u00d7{102, 103, 104}. For the interior point methods, since problem (1) can be cast either as a quadratic (QP) or as a conic program (CP), we show in Figure 2 the results for both formulations. Our approach compares favorably with the other methods, on three problems of different sizes, (n, p) \u2208 {(100, 103), (1024, 104), (1024, 105)}, see Figure 2. In addition, note that QP, CP and SG do not obtain sparse solutions, whereas ProxFlow does. We have also run ProxFlow and SG on a larger dataset with (n, p) = (100, 106): after 12 hours, ProxFlow and SG have reached a relative duality gap of 0.0006 and 0.02 respectively.6\n5In our simulations, we use the commercial software Mosek, http://www.mosek.com/ 6Due to the computational burden, QP and CP could not be run on every problem.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 10", "text": ""}, {"heading": "4.2 Background Subtraction", "text": "Following [8], we consider a background subtraction task. Given a sequence of frames from a fixed camera, we try to segment out foreground objects in a new image. If we denote by y \u2208 Rn this image composed of n pixels, we model y as a sparse linear combination of p other images X \u2208 Rn\u00d7p, plus an error term e in Rn, i.e., y \u2248 Xw + e for some sparse vector w in Rp. This approach is reminiscent of [30] in the context of face recognition, where e is further made sparse to deal with small occlusions. The term Xw accounts for background parts present in both y and X, while e contains specific, or foreground, objects in y. The resulting optimization problem is minw,e 1 2\u2016y\u2212Xw\u2212e\u2016 2 2+\u03bb1\u2016w\u20161+\u03bb2\u2016e\u20161, with \u03bb1, \u03bb2 \u2265 0. In this formulation, the \u21131-norm penalty on e does not take into account the fact that neighboring pixels in y are likely to share the same label (background or foreground), which may lead to scattered pieces of foreground and background regions (Figure 3). We therefore put an additional structured regularization term \u2126 on e, where the groups in G are all the overlapping 3\u00d73-squares on the image. A dataset with hand-segmented evaluation images is used to illustrate the effect of \u2126.7 For simplicity, we use a single regularization parameter, i.e., \u03bb1 = \u03bb2, chosen to maximize the number of pixels matching the ground truth. We consider p = 200 images with n = 57600 pixels (i.e., a resolution of 120\u00d7160, times 3 for the RGB channels). As shown in Figure 3, adding \u2126 improves the background subtraction results for the two tested images, by removing the scattered artifacts due to the lack of structural constraints of the \u21131-norm, which encodes neither spatial nor color consistency."}, {"heading": "4.3 Multi-Task Learning of Hierarchical Structures", "text": "In [11], Jenatton et al. have recently proposed to use a hierarchical structured norm to learn dictionaries of natural image patches. Following their work, we seek to represent n signals {y1, . . . ,yn} of dimension m as sparse linear combinations of elements from a dictionary X = [x1, . . . ,xp] in R\nm\u00d7p. This can be expressed for all i in [1;n] as yi \u2248 Xwi, for some sparse vector wi in Rp. In [11], the dictionary elements are embedded in a predefined tree T , via a particular instance of the structured norm \u2126, which we refer to it as \u2126tree, and call G the underlying set of groups. In this case, each signal yi admits a sparse decomposition in the form of a subtree of dictionary elements.\nInspired by ideas from multi-task learning [16], we propose to learn the tree structure T by pruning irrelevant parts of a larger initial tree T0. We achieve this by using an additional regularization term \u2126joint across the different decompositions, so that subtrees of T0 will simultaneously be removed for all signals yi. In other words, the approach of [11] is extended by the following\n7http://research.microsoft.com/en-us/um/people/jckrumm/wallflower/testimages.htm\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 11", "text": "formulation:\nmin X,W\n1\nn\nn \u2211\ni=1\n[1\n2 \u2016yi \u2212Xwi\u201622 + \u03bb1\u2126tree(w\ni) ] +\u03bb2\u2126joint(W), s.t. \u2016xj\u20162 \u2264 1, for all j in [1; p], (6)\nwhere W , [w1, . . . ,wn] is the matrix of decomposition coefficients in Rp\u00d7n. The new regularization term operates on the rows of W and is defined as \u2126joint(W) , \u2211 g\u2208G maxi\u2208[1;n] |w i g|.\n8 The overall penalty on W, which results from the combination of \u2126tree and \u2126joint, is itself an instance of \u2126 with general overlapping groups, as defined in Eq (2).\nTo address problem (6), we use the same optimization scheme as [11], i.e., alternating between X and W, fixing one variable while optimizing with respect to the other. The task we consider is the denoising of natural image patches, with the same dataset and protocol as [11]. We study whether learning the hierarchy of the dictionary elements improves the denoising performance, compared to standard sparse coding (i.e., when \u2126tree is the \u21131-norm and \u03bb2 = 0) and the hierarchical dictionary learning of [11] based on predefined trees (i.e., \u03bb2 = 0). The dimensions of the training set \u2014 50 000 patches of size 8\u00d78 for dictionaries with up to p = 400 elements \u2014 impose to handle extremely large graphs, with |E| \u2248 |V | \u2248 4.107. Since problem (6) is too large to be solved exactly sufficiently many times to select the regularization parameters (\u03bb1, \u03bb2) rigorously, we use the following heuristics: we optimize mostly with the currently pruned tree held fixed (i.e., \u03bb2 = 0), and only prune the tree (i.e., \u03bb2 > 0) every few steps on a random subset of 10 000 patches. We consider the same hierarchies as in [11], involving between 30 and 400 dictionary elements. The regularization parameter \u03bb1 is selected on the validation set of 25 000 patches, for both sparse coding (Flat) and hierarchical dictionary learning (Tree). Starting from the tree giving the best performance (in this case the largest one, see Figure 4), we solve problem (6) following our heuristics, for increasing values of \u03bb2. As shown in Figure 4, there is a regime where our approach performs significantly better than the two other compared methods. The standard deviation of the noise is 0.2 (the pixels have values in [0, 1]); no significant improvements were observed for lower levels of noise.\n8The simplified case where \u2126tree and \u2126joint are the \u21131- and mixed \u21131/\u21132-norms [14] corresponds to [31].\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 12", "text": ""}, {"heading": "5 Conclusion", "text": "We have presented a new optimization framework for solving sparse structured problems involving sums of \u2113\u221e-norms of any (overlapping) groups of variables. Interestingly, this sheds new light on connections between sparse methods and the literature of network flow optimization. In particular, the proximal operator for the formulation we consider can be cast as a quadratic min-cost flow problem, for which we propose an efficient and simple algorithm. This allows the use of accelerated gradient methods. Several experiments demonstrate that our algorithm can be applied to a wide class of learning problems, which have not been addressed before within sparse methods.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 13", "text": ""}, {"heading": "A Equivalence to Canonical Graphs", "text": "Formally, the notion of equivalence between graphs can be summarized by the following lemma:\nLemma 2 (Equivalence to canonical graphs.) Let G = (V,E, s, t) be the canonical graph corresponding to a group structure G with weights (\u03b7g)g\u2208G . Let G\u2032 = (V,E\u2032, s, t) be a graph sharing the same set of vertices, source and sink as G, but with a different arc set E\u2032. We say that G\u2032 is equivalent to G if and only if the following conditions hold:\n\u2022 Arcs of E\u2032 outgoing from the source are the same as in E, with the same costs and capacities.\n\u2022 Arcs of E\u2032 going to the sink are the same as in E, with the same costs and capacities.\n\u2022 For every arc (g, j) in E, with (g, j) in Vgr \u00d7 Vu, there exists a unique path in E\u2032 from g to j with zero costs and infinite capacities on every arc of the path.\n\u2022 Conversely, if there exists a path in E\u2032 between a vertex g in Vgr and a vertex j in Vu, then there exists an arc (g, j) in E.\nThen, the cost of the optimal min-cost flow on G and G\u2032 are the same. Moreover, the values of the optimal flow on the arcs (j, t), j in Vu, are the same on G and G \u2032.\nProof. We first notice that on both G and G\u2032, the cost of a flow on the graph only depends on the flow on the arcs (j, t), j in Vu, which we have denoted by \u03be\u0304 in E.\nWe will prove that finding a feasible flow \u03c0 on G with a cost c(\u03c0) is equivalent to finding a feasible flow \u03c0\u2032 on G\u2032 with the same cost c(\u03c0) = c(\u03c0\u2032). We now use the concept of path flow, which is a flow vector in G carrying the same positive value on every arc of a directed path between two nodes of G. It intuitively corresponds to sending a positive amount of flow along a path of the graph.\nAccording to the definition of graph equivalence introduced in the Lemma, it is easy to show that there is a bijection between the arcs in E, and the paths in E\u2032 with positive capacities on every arc. Given now a feasible flow \u03c0 in G, we build a feasible flow \u03c0\u2032 on G\u2032 which is a sum of path flows. More precisely, for every arc a in E, we consider its equivalent path in E\u2032, with a path flow carrying the same amount of flow as a. Therefore, each arc a\u2032 in E\u2032 has a total amount of flow that is equal to the sum of the flows carried by the path flows going over a\u2032. It is also easy to show that this construction builds a flow on G\u2032 (capacity and conservation constraints are satisfied) and that this flow \u03c0\u2032 has the same cost as \u03c0, that is, c(\u03c0) = c(\u03c0\u2032).\nConversely, given a flow \u03c0\u2032 on G\u2032, we use a classical path flow decomposition (see Proposition 1.1 in [26]), saying that there exists a decomposition of \u03c0\u2032 as a sum of path flows in E\u2032. Using the bijection described above, we know that each path in the previous sums corresponds to a unique arc in E. We now build a flow \u03c0 in G, by associating to each path flow in the decomposition of \u03c0\u2032, an arc in E carrying the same amount of flow. The flow of every other arc in E is set to zero. It is also easy to show that this builds a valid flow in G that has the same cost as \u03c0\u2032."}, {"heading": "B Convergence Analysis", "text": "We show in this section the correctness of Algorithm 1 for computing the proximal operator, and of Algorithm 2 for computing the dual norm \u2126\u22c6.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 14", "text": ""}, {"heading": "B.1 Computation of the Proximal Operator", "text": "We now prove that our algorithm converges and that it finds the optimal solution of the proximal problem. This requires that we introduce the optimality conditions for problem (4) derived in [11], since our convergence proof essentially checks that these conditions are satisfied upon termination of the algorithm.\nLemma 3 (Optimality conditions of the problem (4), [11]) The primal-dual variables (w, \u03be) are respectively solutions of the primal (3) and dual problems (4) if and only if the dual variable \u03be is feasible for the problem (4) and\nw = u\u2212 \u2211 g\u2208G \u03be g,\n\u2200g \u2208 G,\n{\nw\u22a4g \u03be g g = \u2016wg\u2016\u221e\u2016\u03be g\u20161 and \u2016\u03be g\u20161 = \u03bb\u03b7g, or wg = 0.\nNote that these optimality conditions provide an intuitive view of our min-cost flow problem. Solving the min-cost flow problem is equivalent to sending the maximum amount of flow in the graph under the capacity constraints, while respecting the rule that the flow outgoing from a group g should always be directed to the variables uj with maximum residual uj \u2212 \u2211 g\u2208G \u03be g j .\nBefore proving the convergence and correctness of our algorithm, we also recall classical properties of the min capacity cuts, which we intensively use in the proofs of this paper. The procedure computeFlow of our algorithm finds a minimum (s, t)-cut of a graph G = (V,E, s, t), dividing the set V into two disjoint parts V + and V \u2212. V + is by construction the sets of nodes in V such that there exists a non-saturating path from s to V , while all the paths from s to V \u2212 are saturated. Conversely, arcs from V + to t are all saturated, whereas there can be non-saturated arcs from V \u2212 to t. Moreover, the following properties hold\n\u2022 There is no arc going from V + to V \u2212. Otherwise the value of the cut would be infinite. (Arcs inside V have infinite capacity by construction of our graph).\n\u2022 There is no flow going from V \u2212 to V + (see properties of the minimum (s, t)-cut [26]).\n\u2022 The cut goes through all arcs going from V + to t, and all arcs going from s to V \u2212.\nAll these properties are illustrated on Figure 5. Recall that we assume (cf. Section 3.1) that the scalars uj are all non negative, and that we add non-negativity constraints on \u03be. With the optimality conditions of Lemma 3 in hand, we can show our first convergence result.\nProposition 1 (Convergence of Algorithm 1) Algorithm 1 converges in a finite and polynomial number of operations.\nProof. Our algorithm splits recursively the graph into disjoints parts and processes each part recursively. The processing of one part requires an orthogonal projection onto an \u21131-ball and a max-flow algorithm, which can both be computed in polynomial time. To prove that the procedure converges, it is sufficient to show that when the procedure computeFlow is called for a graph (V,E, s, t) and computes a cut (V +, V \u2212), then the components V + and V \u2212 are both non-empty.\nSuppose for instance that V \u2212= \u2205. In this case, the capacity of the min-cut is equal to \u2211\nj\u2208Vu \u03b3j ,\nand the value of the max-flow is \u2211\nj\u2208Vu \u03be\u0304j . Using the classical max-flow/min-cut theorem [25], we\nhave equality between these two terms. Since, by definition of both \u03b3 and \u03be\u0304, we have for all j in Vu, \u03be\u0304j \u2264 \u03b3j , we obtain a contradiction with the existence of j in Vu such that \u03be\u0304j 6= \u03b3j .\nConversely, suppose now that V += \u2205. Then, the value of the max-flow is still \u2211\nj\u2208Vu \u03be\u0304j , and\nthe value of the min-cut is \u03bb \u2211\ng\u2208Vgr \u03b7g. Using again the max-flow/min-cut theorem, we have that\nRR n\u00b0 7372\nNetwork Flow Algorithms for Structured Sparsity 15\n\u2211\nj\u2208Vu \u03be\u0304j = \u03bb\n\u2211\ng\u2208Vgr \u03b7g. Moreover, by definition of \u03b3, we also have\n\u2211\nj\u2208Vu \u03be\u0304j \u2264\n\u2211\nj\u2208Vu \u03b3j \u2264\n\u03bb \u2211\ng\u2208Vgr \u03b7g, leading to a contradiction with the existence of j in Vu such that \u03be\u0304j 6= \u03b3j . This proof\nholds for any graph that is equivalent to the canonical one.\nAfter proving the convergence, we prove that the algorithm is correct with the next proposition.\nProposition 2 (Correctness of Algorithm 1) Algorithm 1 solves the proximal problem of Eq. (3).\nProof. For a group structure G, we first prove the correctness of our algorithm if the graph used is its associated canonical graph that we denote G0 = (V0, E0, s, t). We proceed by induction on the number of nodes of the graph. The induction hypothesis H(k) is the following:\nFor all canonical graphs G = (V = Vu \u222a Vgr , E, s, t) associated with a group structure GV with weights (\u03b7g)g\u2208GV such that |V | \u2264 k, computeFlow(V,E) solves the following optimization problem:\nmin (\u03beg\nj )j\u2208Vu,g\u2208Vgr\n\u2211\nj\u2208Vu\n1 2 (uj \u2212 \u2211\ng\u2208Vgr\n\u03be g j ) 2 s.t. \u2200g \u2208 Vgr , \u2211\nj\u2208Vu\n\u03be g j \u2264 \u03bb\u03b7g and \u03be g j = 0, \u2200j /\u2208 g. (7)\nSince GV0 = G, it is sufficient to show that H(|V0|) to prove the proposition. We initialize the induction by H(2), corresponding to the simplest canonical graph, for which |Vgr| = |Vu| = 1). Simple algebra shows that H(2) is indeed correct. We now suppose that H(k\u2032) is true for all k\u2032 < k and consider a graph G = (V,E, s, t), |V | = k. The first step of the algorithm computes the variable (\u03b3j)j\u2208Vu by a projection on the \u21131-ball. This is itself an instance of the dual formulation of Eq. (4) in a simple case, with one group containing all variables. We can therefore use Lemma 3 to characterize the optimality of (\u03b3j)j\u2208Vu , which yields\n{ \u2211\nj\u2208Vu (uj \u2212 \u03b3j)\u03b3j =\n( maxj\u2208Vu |uj \u2212 \u03b3j | ) \u2211 j\u2208Vu \u03b3j and \u2211 j\u2208Vu \u03b3j = \u03bb \u2211 g\u2208Vgr \u03b7g,\nor uj \u2212 \u03b3j = 0, \u2200j \u2208 Vu. (8)\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 16", "text": "The algorithm then computes a max-flow, using the scalars \u03b3j as capacities, and we now have two possible situations:\n1. If \u03be\u0304j = \u03b3j for all j in Vu, the algorithm stops; we write wj = uj \u2212 \u03be\u0304j for j in Vu, and using Eq. (8), we obtain\n{ \u2211\nj\u2208Vu wj \u03be\u0304j = (maxj\u2208Vu |wj |)\n\u2211\nj\u2208Vu \u03be\u0304j and\n\u2211\nj\u2208Vu \u03be\u0304j = \u03bb\n\u2211\ng\u2208Vgr \u03b7g,\nor wj = 0, \u2200j \u2208 Vu. (9)\nWe can rewrite the condition above as \u2211\ng\u2208Vgr\n\u2211\nj\u2208g\nwj\u03be g j =\n\u2211\ng\u2208Vgr\n(max j\u2208Vu\n|wj |) \u2211\nj\u2208Vu\n\u03be g j .\nSince all the quantities in the previous sum are positive, this can only hold if for all g \u2208 Vgr ,\n\u2211\nj\u2208Vu\nwj\u03be g j = (max\nj\u2208Vu |wj |)\n\u2211\nj\u2208Vu\n\u03be g j .\nMoreover, by definition of the max flow and the optimality conditions, we have\n\u2200g \u2208 Vgr , \u2211\nj\u2208Vu\n\u03be g j \u2264 \u03bb\u03b7g , and\n\u2211\nj\u2208Vu\n\u03be\u0304j = \u03bb \u2211\ng\u2208Vgr\n\u03b7g,\nwhich leads to \u2200g \u2208 Vgr , \u2211\nj\u2208Vu\n\u03be g j = \u03bb\u03b7g.\nBy Lemma 3, we have shown that the problem (7) is solved.\n2. Let us now consider the case where there exists j in Vu such that \u03be\u0304j 6= \u03b3j . The algorithm splits the vertex set V into two parts V + and V \u2212, which we have proven to be non-empty in the proof of Proposition 1. The next step of the algorithm removes all edges between V + and V \u2212 (see Figure 5). Processing (V +, E+) and (V \u2212, E\u2212) independently, it updates the value of the flow matrix \u03begj , j \u2208 Vu, g \u2208 Vgr, and the corresponding flow vector \u03be\u0304j , j \u2208 Vu. As for\nV , we denote by V +u , V + \u2229 Vu, V \u2212u , V \u2212 \u2229 Vu and V +gr , V + \u2229 Vgr , V \u2212gr , V \u2212 \u2229 Vgr . Then, we notice that (V +, E+, s, t) and (V \u2212, E\u2212, s, t) are respective canonical graphs for the group structures GV + , {g \u2229 V +u | g \u2208 Vgr}, and GV \u2212 , {g \u2229 V \u2212 u | g \u2208 Vgr}. Writing wj = uj \u2212 \u03be\u0304j for j in Vu, and using the induction hypotheses H(|V +|) and H(|V \u2212|), we now have the following optimality conditions deriving from Lemma 3 applied on Eq. (7) respectively for the graphs (V +, E+) and (V \u2212, E\u2212):\n\u2200g \u2208 V +gr , g \u2032 , g \u2229 V +u ,\n{ w\u22a4g\u2032\u03be g g\u2032 = \u2016wg\u2032\u2016\u221e \u2211 j\u2208g\u2032\u03be g j and \u2211 j\u2208g\u2032\u03be g j = \u03bb\u03b7g,\nor wg\u2032 = 0, (10)\nand\n\u2200g \u2208 V \u2212gr , g \u2032 , g \u2229 V \u2212u ,\n{ w\u22a4g\u2032\u03be g g\u2032 = \u2016wg\u2032\u2016\u221e \u2211 j\u2208g\u2032\u03be g j and \u2211 j\u2208g\u2032\u03be g j = \u03bb\u03b7g,\nor wg\u2032 = 0. (11)\nWe will now combine Eq. (10) and Eq. (11) into optimality conditions for Eq. (7). We first notice that g \u2229 V +u = g since there are no arcs between V\n+ and V \u2212 in E (see the properties of the cuts discussed before this proposition). It is therefore possible to replace g\u2032 by g in\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 17", "text": "Eq. (10). We will show that it is possible to do the same in Eq. (11), so that combining these two equations yield the optimality conditions of Eq. (7).\nMore precisely, we will show that for all g \u2208 V \u2212gr and j \u2208 g \u2229 V + u , |wj | \u2264 maxl\u2208g\u2229V \u2212u |wl|, in which case g\u2032 can be replaced by g in Eq. (11). This result is relatively intuitive: (s, V +) and (V \u2212, t) being an (s, t)-cut, all arcs between s and V \u2212 are saturated, while there are unsaturated arcs between s and V +; one therefore expects the residuals uj \u2212 \u03be\u0304j to decrease on the V + side, while increasing on the V \u2212 side. The proof is nonetheless a bit technical.\nLet us show first that for all g in V +gr , \u2016wg\u2016\u221e \u2264 maxj\u2208Vu |uj \u2212 \u03b3j |. We split the set V + into disjoint parts:\nV ++gr , {g \u2208 V + gr s.t. \u2016wg\u2016\u221e \u2264 maxj\u2208Vu |uj \u2212 \u03b3j |}, V ++u , {j \u2208 V + u s.t. \u2203g \u2208 V ++ gr , j \u2208 g}, V +\u2212gr , V + gr \\ V ++ gr = {g \u2208 V\n+ gr s.t. \u2016wg\u2016\u221e > maxj\u2208Vu |uj \u2212 \u03b3j |},\nV +\u2212u , V + u \\ V ++ u .\nAs previously, we denote V +\u2212, V +\u2212u \u222a V +\u2212 gr and V ++ ,V ++u \u222a V ++ gr . We want to show that V +\u2212gr is necessarily empty. We reason by contradiction and assume that V +\u2212 gr 6= \u2205.\nAccording to the definition of the different sets above, we observe that no arcs are going from V ++ to V +\u2212, that is, for all g in V ++gr , g \u2229 V +\u2212 u = \u2205. We observe as well that the flow from V +\u2212gr to V ++ u is the null flow, because optimality conditions (10) imply that for a group g only nodes j \u2208 g such that wj = \u2016wg\u2016\u221e receive some flow, which excludes nodes in V ++u provided V +\u2212gr 6= \u2205; Combining this fact and the inequality \u2211 g\u2208V +gr \u03bb\u03b7g \u2265 \u2211 j\u2208V +u \u03b3j (which is a direct consequence of the minimum (s, t)-cut), we have as well \u2211\ng\u2208V +\u2212gr\n\u03bb\u03b7g \u2265 \u2211\nj\u2208V +\u2212u\n\u03b3j .\nLet j \u2208 V +\u2212u , if \u03be\u0304j 6= 0 then for some g \u2208 V +\u2212 gr such that j receives some flow from g, which from the optimality conditions (10) implies wj = \u2016wg\u2016\u221e; by definition of V +\u2212gr , \u2016wg\u2016\u221e > uj \u2212\u03b3j . But since at the optimum, wj = uj \u2212 \u03be\u0304j , this implies that \u03be\u0304j < \u03b3j , and in turn that \u2211\nj\u2208V +\u2212u \u03be\u0304j = \u03bb\n\u2211\ng\u2208V +\u2212gr \u03b7g. Finally,\n\u03bb \u2211\ng\u2208V +\u2212gr\n\u03b7g = \u2211\nj\u2208V +\u2212u , \u03be\u0304j 6=0\n\u03be\u0304j < \u2211\nj\u2208V +\u2212u\n\u03b3j\nand this is a contradiction.\nWe now have that for all g in V +gr , \u2016wg\u2016\u221e \u2264 maxj\u2208Vu |uj \u2212 \u03b3j |. The proof showing that for all g in V \u2212gr , \u2016wg\u2016\u221e \u2265 maxj\u2208Vu |uj \u2212 \u03b3j |, uses the same kind of decomposition for V\n\u2212, and follows along similar arguments. We will therefore not detail it.\nTo recap, we have shown that for all g \u2208 V \u2212gr and j \u2208 g \u2229 V + u , |wj | \u2264 maxl\u2208g\u2229V \u2212u |wl|. Since there is no flow from V \u2212 to V +, i.e., \u03begj = 0 for g in V \u2212 gr and j in V + u , we can now replace the definition of g\u2032 in Eq. (11) by g\u2032 , g \u2229 Vu, the combination of Eq. (10) and Eq. (11) gives us optimality conditions for Eq. (7).\nThe proposition being proved for the canonical graph, we extend it now for an equivalent graph in the sense of Lemma 2. First, we observe that the algorithm gives the same values of \u03b3 for two\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 18", "text": "equivalent graphs. Then, it is easy to see that the value \u03be\u0304 given by the max-flow, and the chosen (s, t)-cut is the same, which is enough to conclude that the algorithm performs the same steps for two equivalent graphs."}, {"heading": "B.2 Computation of the Dual Norm \u2126\u22c6", "text": "Similarly to the proximal operator, the computation of dual norm \u2126\u2217 can itself shown to solve another network flow problem, based on the following variational formulation, which extends a previous result from [5]:\nLemma 4 (Dual formulation of the dual-norm \u2126\u22c6.) Let \u03ba \u2208 Rp. We have\n\u2126\u2217(\u03ba) = min \u03be\u2208Rp\u00d7|G|,\u03c4\n\u03c4 s.t. \u2211\ng\u2208G\n\u03beg = \u03ba, and \u2200g \u2208 G, \u2016\u03beg\u20161 \u2264 \u03c4\u03b7g with \u03be g j = 0 if j /\u2208 g. (12)\nProof. By definition of \u2126\u2217(\u03ba), we have\n\u2126\u2217(\u03ba) , max \u2126(z)\u22641 z\u22a4\u03ba.\nBy introducing the primal variables (\u03b1g)g\u2208G \u2208 R|G|, we can rewrite the previous maximization problem as\n\u2126\u2217(\u03ba) = max\u2211 g\u2208G\u03b7g\u03b1g\u22641 \u03ba\u22a4z, s.t. \u2200 g \u2208 G, \u2016zg\u2016\u221e \u2264 \u03b1g,\nwith the additional |G| conic constraints \u2016zg\u2016\u221e \u2264 \u03b1g. This primal problem is convex and satisfies Slater\u2019s conditions for generalized conic inequalities, which implies that strong duality holds [32]. We now consider the Lagrangian L defined as\nL(z, \u03b1g , \u03c4, \u03b3g, \u03be) = \u03ba \u22a4z+ \u03c4(1\u2212\n\u2211\ng\u2208G\n\u03b7g\u03b1g) + \u2211\ng\u2208G\n(\n\u03b1g zg )\u22a4( \u03b3g \u03begg ) ,\nwith the dual variables {\u03c4, (\u03b3g)g\u2208G , \u03be} \u2208 R+\u00d7R |G|\u00d7Rp\u00d7|G| such that for all g \u2208 G, \u03begj = 0 if j /\u2208 g and \u2016\u03beg\u20161 \u2264 \u03b3g. The dual function is obtained by taking the derivatives of L with respect to the primal variables z and (\u03b1g)g\u2208G and equating them to zero, which leads to\n\u2200j \u2208 {1, . . . , p}, \u03baj + \u2211 g\u2208G \u03be g j = 0\n\u2200g \u2208 G, \u03c4\u03b7g \u2212 \u03b3g = 0.\nAfter simplifying the Lagrangian and flipping the sign of \u03be, the dual problem then reduces to\nmin \u03be\u2208Rp\u00d7|G|,\u03c4 \u03c4 s.t.\n{\n\u2200j \u2208 {1, . . . , p},\u03baj = \u2211 g\u2208G \u03be g j and \u03be g j = 0 if j /\u2208 g, \u2200g \u2208 G, \u2016\u03beg\u20161 \u2264 \u03c4\u03b7g ,\nwhich is the desired result.\nWe now prove that Algorithm 2 is correct.\nProposition 3 (Convergence and correctness of Algorithm 2) Algorithm 2 computes the value of the dual norm of Eq. (12) in a finite and polynomial number of operations.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 19", "text": "Proof. The convergence of the algorithm only requires to show that the cardinality of V in the different calls of the function computeFlow strictly decreases. Similar arguments to those used in the proof of Proposition 1 can show that each part of the cuts (V +, V \u2212) are both non-empty. The algorithm thus requires a finite number of calls to a max-flow algorithm and converges in a finite and polynomial number of operations.\nLet us now prove that the algorithm is correct for a canonical graph. We proceed again by induction on the number of nodes of the graph. More precisely, we consider the induction hypothesis H\u2032(k) defined as:\nfor all canonical graphs G = (V,E, s, t) associated with a group structure GV and such that |V | \u2264 k, dualNormAux(V = Vu \u222a Vgr , E) solves the following optimization problem:\nmin \u03be,\u03c4\n\u03c4 s.t. \u2200j \u2208 Vu,\u03baj = \u2211\ng\u2208Vgr\n\u03be g j , and \u2200g \u2208 Vgr,\n\u2211\nj\u2208Vu\n\u03be g j \u2264 \u03c4\u03b7g with \u03be g j = 0 if j /\u2208 g. (13)\nWe first initialize the induction by H(2) (i.e., with the simplest canonical graph, such that |Vgr | = |Vu| = 1). Simple algebra shows that H(2) is indeed correct.\nWe next consider a canonical graphG = (V,E, s, t) such that |V | = k, and suppose thatH\u2032(k\u22121) is true. After the max-flow step, we have two possible cases to discuss:\n1. If \u03be\u0304j = \u03b3j for all j in Vu, the algorithm stops. We know that any scalar \u03c4 such that the constraints of Eq. (13) are all satisfied necessarily verifies \u2211\ng\u2208Vgr \u03c4\u03b7g \u2265\n\u2211\nj\u2208Vu \u03baj . We have\nindeed that \u2211\ng\u2208Vgr \u03c4\u03b7g is the value of an (s, t)-cut in the graph, and\n\u2211\nj\u2208Vu \u03baj is the value of\nthe max-flow, and the inequality follows from the max-flow/min-cut theorem [25]. This gives a lower-bound on \u03c4 . Since this bound is reached, \u03c4 is necessarily optimal.\n2. We now consider the case where there exists j in Vu such that \u03be\u0304j 6= \u03baj , meaning that for the given value of \u03c4 , the constraint set of Eq. (13) is not feasible for \u03be, and that the value of \u03c4 should necessarily increase. The algorithm splits the vertex set V into two non-empty parts V + and V \u2212 and we remark that there are no arcs going from V + to V \u2212, and no flow going from V \u2212 to V +. Since the arcs going from s to V \u2212 are saturated, we have that \u2211\ng\u2208V \u2212gr \u03c4\u03b7g \u2264\n\u2211\nj\u2208V \u2212u \u03baj .\nLet us now consider \u03c4\u22c6 the solution of Eq. (13). Using the induction hypothesis H\u2032(|V \u2212|), the algorithm computes a new value \u03c4 \u2032 that solves Eq. (13) when replacing V by V \u2212 and this new value satisfies the following inequality \u2211\ng\u2208V \u2212gr \u03c4 \u2032\u03b7g \u2265\n\u2211\nj\u2208V \u2212u \u03baj . The value of \u03c4 \u2032\nhas therefore increased and the updated flow \u03be now satisfies the constraints of Eq. (13) and therefore \u03c4 \u2032 \u2265 \u03c4\u22c6. Since there are no arcs going from V + to V \u2212, \u03c4\u22c6 is feasible for Eq. (13) when replacing V by V \u2212 and we have that \u03c4\u22c6 \u2265 \u03c4 \u2032 and then \u03c4 \u2032 = \u03c4\u22c6.\nTo prove that the result holds for any equivalent graph, similar arguments to those used in the proof of Proposition 1 can be exploited, showing that the algorithm computes the same values of \u03c4 and same (s, t)-cuts at each step."}, {"heading": "C Algorithm FISTA with duality gap", "text": "In this section, we describe in details the algorithm FISTA [4] when applied to solve problem (1), with a duality gap as stopping criterion.\nWithout loss of generality, let us assume we are looking for models of the form Xw, for some matrix X \u2208 Rn\u00d7p (typically, linear models where X is the data matrix of n observations). Thus,\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 20", "text": "we can consider the following primal problem\nmin w\u2208Rp f(Xw) + \u03bb\u2126(w), (14)\nin place of (1). Based on Fenchel duality arguments [29],\nf(Xw) + \u03bb\u2126(w) + f\u2217(\u2212\u03ba), for w \u2208 Rp,\u03ba \u2208 Rn and \u2126\u2217(X\u22a4\u03ba) \u2264 \u03bb,\nis a duality gap for (14). where f\u2217(\u03ba) , supz[z \u22a4\u03ba \u2212 f(z)] is the Fenchel conjugate of f . Given a primal variable w, a good dual candidate \u03ba can be obtained by looking at the conditions that have to be satisfied by the pair (w,\u03ba) at optimality [29]. In particular, the dual variable \u03ba is chosen to be\n\u03ba = \u2212\u03c1\u22121\u2207f(Xw), with \u03c1 , max { \u03bb\u22121\u2126\u2217(X\u22a4\u2207f(Xw)), 1 } .\nConsequently, computing the duality gap requires evaluating the dual norm \u2126\u2217. We sum up the computation of the duality gap in Algorithm 3.\nMoreover, we refer to the proximal operator associated with \u03bb\u2126 as prox[\u03bb\u2126]. As a brief reminder, it is defined as the function that maps the vector u in Rp to the (unique, by strong convexity) solution of Eq. (3).\nAlgorithm 3 FISTA procedure to solve problem (14).\n1: Inputs: initial w(0) \u2208 Rp, \u2126, \u03bb > 0, \u03b5gap > 0 (precision for the duality gap). 2: Parameters: \u03bd > 1, L0 > 0. 3: Outputs: solution w. 4: Initialization: y(1) = w(0), t1 = 1, k = 1. 5: while { computeDualityGap ( w(k\u22121) ) > \u03b5gap }\ndo 6: Find the smallest integer sk\u22650 such that 7: f(prox[\u03bb\u2126](y(k))) \u2264 f(y(k)) + \u2206 \u22a4 (k)\u2207f(y(k)) + L\u0303 2 \u2016\u2206(k)\u2016 2 2, 8: with L\u0303 , Lk\u03bdsk and \u2206(k) , y(k)\u2212prox[\u03bb\u2126](y(k)). 9: Lk \u2190 Lk\u22121\u03bdsk .\n10: w(k) \u2190 prox[\u03bb\u2126](y(k)). 11: tk+1 \u2190 (1 + \u221a\n1 + t2k)/2. 12: y(k+1) \u2190 w(k) +\ntk\u22121 tk+1 (w(k) \u2212w(k\u22121)). 13: k \u2190 k + 1. 14: end while 15: Return: w \u2190 w(k\u22121).\nProcedure computeDualityGap(w)\n1: \u03ba \u2190 \u2212\u03c1\u22121\u2207f(Xw), with \u03c1 , max { \u03bb\u22121\u2126\u2217(X\u22a4\u2207f(Xw)), 1 }\n. 2: Return: f(Xw) + \u03bb\u2126(w) + f\u2217(\u2212\u03ba)."}, {"heading": "D Additional Experimental Results", "text": "D.1 Speed comparison of Algorithm 1 with parametric max-flow algo-\nrithms\nAs shown in [19], min-cost flow problems, and in particular, the dual problem of (3), can be reduced to a specific parametric max-flow problem. We thus compare our approach (ProxFlow) with the\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 21", "text": "efficient parametric max-flow algorithm proposed by Gallo, Grigoriadis, and Tar- jan [22] and a simplified version of the latter proposed by Babenko and Goldberg in [23]. We refer to these two algorithms as GGT and SIMP respectively. The benchmark is established on the same datasets as those already used in the experimental section of the paper, namely: (1) three datasets built from overcomplete bases of discrete cosine transforms (DCT), with respectively 104, 105 and 106 variables, and (2) images used for the background subtraction task, composed of 57600 pixels. For GGT and SIMP, we use the paraF software which is a C++ parametric max-flow implementation available at http://www.avglab.com/andrew/soft.html. Experiments were conducted on a single-core 2.33 Ghz.\nWe report in the following table the execution time in seconds of each algorithm, as well as the statistics of the corresponding problems:\nNumber of variables p 10 000 100 000 1 000 000 57 600\n|V | 20 000 200 000 2 000 000 75 600 |E| 110 000 500 000 11 000 000 579 632\nProxFlow (in sec.) 0.4 3.1 113.0 1.7 GGT (in sec.) 2.4 26.0 525.0 16.7 SIMP (in sec.) 1.2 13.1 284.0 8.31\nAlthough we provide the speed comparison for a single value of \u03bb (the one used in the corresponding experiments of the paper), we observed that our approach consistently outperforms GGT and SIMP for values of \u03bb corresponding to different regularization regimes."}, {"heading": "Network Flow Algorithms for Structured Sparsity 22", "text": "[10] S. Kim and E. P. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In Proc. ICML, 2010.\n[11] R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for sparse hierarchical dictionary learning. In Proc. ICML, 2010.\n[12] V. Roth and B. Fischer. The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms. In Proc. ICML, 2008.\n[13] R. Tibshirani. Regression shrinkage and selection via the Lasso. J. Roy. Stat. Soc. B, 58(1):267\u2013 288, 1996.\n[14] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Roy. Stat. Soc. B, 68:49\u201367, 2006.\n[15] J. Huang and T. Zhang. The benefit of group sparsity. Technical report, 2009. Preprint arXiv:0901.2962.\n[16] G. Obozinski, B. Taskar, and M. I. Jordan. Joint covariate selection and joint subspace selection for multiple classification problems. Stat. Comput., 20(2):231\u2013252, 2010.\n[17] F. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In Adv. NIPS, 2008.\n[18] P. L. Combettes and J.-C. Pesquet. Proximal splitting methods in signal processing. In FixedPoint Algorithms for Inverse Problems in Science and Engineering. Springer, 2010.\n[19] D. S. Hochbaum and S. P. Hong. About strongly polynomial time algorithms for quadratic optimization over submodular constraints. Math. Program., 69(1):269\u2013309, 1995.\n[20] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the \u21131-ball for learning in high dimensions. In Proc. ICML, 2008.\n[21] P. Brucker. An O(n) algorithm for quadratic knapsack problems. Oper. Res. Lett., 3:163\u2013166, 1984.\n[22] G. Gallo, M. E. Grigoriadis, and R. E. Tarjan. A fast parametric maximum flow algorithm and applications. SIAM J. Comput., 18:30\u201355, 1989.\n[23] M. Babenko and A.V. Goldberg. Experimental evaluation of a parametric flow algorithm. Technical report, Microsoft Research, 2006. MSR-TR-2006-77.\n[24] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum flow problem. In Proc. of ACM Symposium on Theory of Computing, pages 136\u2013146, 1986.\n[25] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian J. Math., 8(3):399\u2013404, 1956.\n[26] D. P. Bertsekas. Linear Network Optimization. MIT Press, 1991.\n[27] B. V. Cherkassky and A. V. Goldberg. On implementing the push-relabel method for the maximum flow problem. Algorithmica, 19(4):390\u2013410, 1997.\n[28] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for highdimensional analysis of M-estimators with decomposable regularizers. In Adv. NIPS, 2009.\nRR n\u00b0 7372"}, {"heading": "Network Flow Algorithms for Structured Sparsity 23", "text": "[29] J. M. Borwein and A. S. Lewis. Convex analysis and nonlinear optimization: Theory and examples. Springer, 2006.\n[30] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE T. Pattern. Anal., 31(2):210\u2013227, 2009.\n[31] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar. Collaborative hierarchical sparse modeling. Technical report, 2010. Preprint arXiv:1003.0400v1.\n[32] S. P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\nRR n\u00b0 7372\nCentre de recherche INRIA Paris \u2013 Rocquencourt Domaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex (France)\nCentre de recherche INRIA Bordeaux \u2013 Sud Ouest : Domaine Universitaire - 351, cours de la Lib\u00e9ration - 33405 Talence Cedex Centre de recherche INRIA Grenoble \u2013 Rh\u00f4ne-Alpes : 655, avenue de l\u2019Europe - 38334 Montbonnot Saint-Ismier\nCentre de recherche INRIA Lille \u2013 Nord Europe : Parc Scientifique de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d\u2019Ascq Centre de recherche INRIA Nancy \u2013 Grand Est : LORIA, Technop\u00f4le de Nancy-Brabois - Campus scientifique\n615, rue du Jardin Botanique - BP 101 - 54602 Villers-l\u00e8s-Nancy Cedex Centre de recherche INRIA Rennes \u2013 Bretagne Atlantique : IRISA, Campus universitaire de Beaulieu - 35042 Rennes Cedex\nCentre de recherche INRIA Saclay \u2013 \u00cele-de-France : Parc Orsay Universit\u00e9 - ZAC des Vignes : 4, rue Jacques Monod - 91893 Orsay Cedex Centre de recherche INRIA Sophia Antipolis \u2013 M\u00e9diterran\u00e9e : 2004, route des Lucioles - BP 93 - 06902 Sophia Antipolis Cedex\n\u00c9diteur INRIA - Domaine de Voluceau - Rocquencourt, BP 105 - 78153 Le Chesnay Cedex (France)\nhttp://www.inria.fr\nISSN 0249-6399"}], "references": [{"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P. Bickel", "Y. Ritov", "A. Tsybakov"], "venue": "Ann. Stat.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Ann. Stat.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "Technical report, Center for Operations Research and Econometrics (CORE), Catholic University of Louvain,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Imag. Sci.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["R. Jenatton", "J-Y. Audibert", "F. Bach"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Group Lasso with overlap and graph Lasso", "author": ["L. Jacob", "G. Obozinski", "J.-P. Vert"], "venue": "In Proc. ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Ann. Stat.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "Z. Zhang", "D. Metaxas"], "venue": "In Proc. ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Model-based compressive sensing", "author": ["R.G. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde"], "venue": "IEEE T. Inform. Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E.P. Xing"], "venue": "In Proc. ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "In Proc. ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "The Group-Lasso for generalized linear models: uniqueness of solutions and efficient algorithms", "author": ["V. Roth", "B. Fischer"], "venue": "In Proc. ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Stat. Soc. B,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Technical report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Stat. Comput.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Exploring large feature spaces with hierarchical multiple kernel learning", "author": ["F. Bach"], "venue": "In Adv. NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Proximal splitting methods in signal processing. In Fixed- Point Algorithms for Inverse Problems in Science and Engineering", "author": ["P.L. Combettes", "J.-C. Pesquet"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "About strongly polynomial time algorithms for quadratic optimization over submodular constraints", "author": ["D.S. Hochbaum", "S.P. Hong"], "venue": "Math. Program.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Efficient projections onto the l1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proc. ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "An O(n) algorithm for quadratic knapsack problems", "author": ["P. Brucker"], "venue": "Oper. Res. Lett.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1984}, {"title": "A fast parametric maximum flow algorithm and applications", "author": ["G. Gallo", "M.E. Grigoriadis", "R.E. Tarjan"], "venue": "SIAM J. Comput.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "Experimental evaluation of a parametric flow algorithm", "author": ["M. Babenko", "A.V. Goldberg"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A new approach to the maximum flow problem", "author": ["A.V. Goldberg", "R.E. Tarjan"], "venue": "In Proc. of ACM Symposium on Theory of Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1986}, {"title": "Maximal flow through a network", "author": ["L.R. Ford", "D.R. Fulkerson"], "venue": "Canadian J. Math.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1956}, {"title": "Linear Network Optimization", "author": ["D.P. Bertsekas"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "On implementing the push-relabel method for the maximum flow problem", "author": ["B.V. Cherkassky", "A.V. Goldberg"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "A unified framework for highdimensional analysis of M-estimators with decomposable regularizers", "author": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "In Adv. NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Convex analysis and nonlinear optimization: Theory and examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE T. Pattern. Anal.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Collaborative hierarchical sparse modeling", "author": ["P. Sprechmann", "I. Ramirez", "G. Sapiro", "Y.C. Eldar"], "venue": "Technical report,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Centre de recherche INRIA Paris \u2013 Rocquencourt Domaine de Voluceau", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 2, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 3, "context": "Regularization by the l1-norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4].", "startOffset": 221, "endOffset": 230}, {"referenceID": 4, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 5, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 6, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 7, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 8, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 177, "endOffset": 192}, {"referenceID": 5, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 241, "endOffset": 248}, {"referenceID": 9, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 241, "endOffset": 248}, {"referenceID": 10, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 265, "endOffset": 269}, {"referenceID": 7, "context": "Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higherorder information about allowed patterns of non-zero coefficients [5, 6, 7, 8, 9], with successful applications in bioinformatics [6, 10], topic modeling [11] and computer vision [8].", "startOffset": 290, "endOffset": 293}, {"referenceID": 2, "context": "Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their ability to deal with large problems [3, 4].", "startOffset": 160, "endOffset": 166}, {"referenceID": 3, "context": "Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their ability to deal with large problems [3, 4].", "startOffset": 160, "endOffset": 166}, {"referenceID": 11, "context": "While the settings where the penalized groups of variables do not overlap [12] or are embedded in a tree-shaped hierarchy [11] have already been studied, sparsityinducing regularizations of general overlapping groups have, to the best of our knowledge, never been considered within the proximal method framework.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "While the settings where the penalized groups of variables do not overlap [12] or are embedded in a tree-shaped hierarchy [11] have already been studied, sparsityinducing regularizations of general overlapping groups have, to the best of our knowledge, never been considered within the proximal method framework.", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "When one knows a priori that the solutions of this learning problem only have a few non-zero coefficients, \u03a9 is often chosen to be the l1-norm, leading for instance to the Lasso [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 13, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 14, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 15, "context": "When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models [12, 14, 15, 16].", "startOffset": 185, "endOffset": 201}, {"referenceID": 6, "context": "A sum of l2-norms is also used in the literature [7], but the l\u221e-norm is piecewise linear, a property that we take advantage of in this paper.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "When the groups overlap, \u03a9 is still a norm and sets groups of variables to zero together [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 9, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 16, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 61, "endOffset": 72}, {"referenceID": 4, "context": "The latter setting has first been considered for hierarchies [7, 10, 17], and then extended to general group structures [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 10, "context": "Following [11] who tackled the case of hierarchical groups, we propose to approach this problem with proximal methods, which we now introduce.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "1 Proximal Methods In a nutshell, proximal methods can be seen as a natural extension of gradient-based techniques, and they are well suited to minimizing the sum f + \u03bb\u03a9 of two convex terms, a smooth function f \u2014continuously differentiable with Lipschitz-continuous gradient\u2014 and a potentially non-smooth function \u03bb\u03a9 (see [18] and references therein).", "startOffset": 322, "endOffset": 326}, {"referenceID": 2, "context": "Simple proximal method use w as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates.", "startOffset": 75, "endOffset": 81}, {"referenceID": 3, "context": "Simple proximal method use w as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates.", "startOffset": 75, "endOffset": 81}, {"referenceID": 17, "context": "(3) is obtained by a soft-thresholding [18].", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "The approach we develop in the rest of this paper extends [11] to the case of general overlapping groups when \u03a9 is a weighted sum of l\u221e-norms, broadening the application of these regularizations to a wider spectrum of problems.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 232, "endOffset": 235}, {"referenceID": 7, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 268, "endOffset": 274}, {"referenceID": 8, "context": "We start by considering the dual formulation to problem (3) introduced in [11], for the case where \u03a9 is a sum of l\u221e-norms: Note that other types of structured sparse models have also been introduced, either through a different norm [6], or through non-convex criteria [8, 9].", "startOffset": 268, "endOffset": 274}, {"referenceID": 10, "context": "For hierarchies, the approach of [11] applies also to the case of where \u03a9 is a weighted sum of l2-norms.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Lemma 1 (Dual of the proximal problem [11]) Given u in R, consider the problem", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "(4) derived in [11] show that for all j in [1; p], the signs of the non-zero coefficients \u03be j for g in G are the same as the signs of the entries uj .", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "The graphs (c) and (d) correspond to a special case of tree-structured hierarchy in the sense of [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "2 Computation of the Proximal Operator Quadratic min-cost flow problems have been well studied in the operations research literature [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "It has been shown, both in machine learning [20] and operations research [19, 21], that such a projection can be done in O(p) operations.", "startOffset": 73, "endOffset": 81}, {"referenceID": 10, "context": "When the group structure is a tree as in Figure 1(d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O(pd) operations, where d is the depth of the tree.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "When the group structure is a tree as in Figure 1(d), strategies developed in the two communities are also similar [11, 19], and solve the problem in O(pd) operations, where d is the depth of the tree.", "startOffset": 115, "endOffset": 123}, {"referenceID": 18, "context": "Hochbaum and Hong have shown in [19] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "Hochbaum and Hong have shown in [19] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "Our method clearly shares some similarities with a simplified version of [22] presented in [23], namely a divide and conquer strategy.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "Our method clearly shares some similarities with a simplified version of [22] presented in [23], namely a divide and conquer strategy.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Then, the maximum-flow step [24] tries to find a feasible flow such that the vector \u03be\u0304 matches \u03b3.", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "If \u03be\u0304 6= \u03b3, the lower bound cannot be reached, and we construct a minimum (s, t)-cut of the graph [25] that defines two disjoints sets of nodes V + and V ; V + is the part of the graph that can potentially receive more flow from the source, whereas all arcs linking s to V \u2212 are saturated.", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "The properties of a min (s, t)-cut [26] imply that there are no arcs from V + to V \u2212 (arcs inside V have infinite By definition, a parametric max-flow problem consists in solving, for every value of a parameter, a max-flow problem on a graph whose arc capacities depend on this parameter.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "The approach of [19, 22] is guaranteed to have the same worst-case complexity as a single maxflow algorithm.", "startOffset": 16, "endOffset": 24}, {"referenceID": 21, "context": "The approach of [19, 22] is guaranteed to have the same worst-case complexity as a single maxflow algorithm.", "startOffset": 16, "endOffset": 24}, {"referenceID": 23, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 196, "endOffset": 204}, {"referenceID": 26, "context": "\u2022 Efficient max-flow algorithm: We have implemented the \u201cpush-relabel\u201d algorithm of [24] to solve our max-flow problems, using classical heuristics that significantly speed it up in practice (see [24, 27]).", "startOffset": 196, "endOffset": 204}, {"referenceID": 23, "context": "Our implementation uses the so-called \u201chighest-active vertex selection rule, global and gap heuristics\u201d (see [24, 27]), and has a worst-case complexity of O(|V ||E|) for a graph (V,E, s, t).", "startOffset": 109, "endOffset": 117}, {"referenceID": 26, "context": "Our implementation uses the so-called \u201chighest-active vertex selection rule, global and gap heuristics\u201d (see [24, 27]), and has a worst-case complexity of O(|V ||E|) for a graph (V,E, s, t).", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "This modified projection step can still be computed in linear time [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 16, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 27, "context": "3 Computation of the Dual Norm The dual norm \u03a9 of \u03a9, defined for any vector \u03ba in R by \u03a9(\u03ba) , max\u03a9(z)\u22641 z\u03ba, is a key quantity to study sparsity-inducing regularizations [5, 17, 28].", "startOffset": 168, "endOffset": 179}, {"referenceID": 28, "context": "We denote by f the Fenchel conjugate of f [29], defined by f(\u03ba) , supz[z \u03ba\u2212f(z)].", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The duality gap for problem (1) can be derived from standard Fenchel duality arguments [29] and it is equal to f(w) + \u03bb\u03a9(w) + f(\u2212\u03ba) for w,\u03ba in R with \u03a9(\u03ba) \u2264 \u03bb.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "4 Applications and Experiments Our experiments use the algorithm of [4] based on our proximal operator, with weights \u03b7g set to 1.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "2 Background Subtraction Following [8], we consider a background subtraction task.", "startOffset": 35, "endOffset": 38}, {"referenceID": 29, "context": "This approach is reminiscent of [30] in the context of face recognition, where e is further made sparse to deal with small occlusions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "3 Multi-Task Learning of Hierarchical Structures In [11], Jenatton et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In [11], the dictionary elements are embedded in a predefined tree T , via a particular instance of the structured norm \u03a9, which we refer to it as \u03a9tree, and call G the underlying set of groups.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Inspired by ideas from multi-task learning [16], we propose to learn the tree structure T by pruning irrelevant parts of a larger initial tree T0.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "In other words, the approach of [11] is extended by the following http://research.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "To address problem (6), we use the same optimization scheme as [11], i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "The task we consider is the denoising of natural image patches, with the same dataset and protocol as [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": ", when \u03a9tree is the l1-norm and \u03bb2 = 0) and the hierarchical dictionary learning of [11] based on predefined trees (i.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "We consider the same hierarchies as in [11], involving between 30 and 400 dictionary elements.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "2 (the pixels have values in [0, 1]); no significant improvements were observed for lower levels of noise.", "startOffset": 29, "endOffset": 35}, {"referenceID": 13, "context": "The simplified case where \u03a9tree and \u03a9joint are the l1- and mixed l1/l2-norms [14] corresponds to [31].", "startOffset": 77, "endOffset": 81}, {"referenceID": 30, "context": "The simplified case where \u03a9tree and \u03a9joint are the l1- and mixed l1/l2-norms [14] corresponds to [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "1 in [26]), saying that there exists a decomposition of \u03c0 as a sum of path flows in E.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "This requires that we introduce the optimality conditions for problem (4) derived in [11], since our convergence proof essentially checks that these conditions are satisfied upon termination of the algorithm.", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "Lemma 3 (Optimality conditions of the problem (4), [11]) The primal-dual variables (w, \u03be) are respectively solutions of the primal (3) and dual problems (4) if and only if the dual variable \u03be is feasible for the problem (4) and w = u\u2212 \u2211 g\u2208G \u03be , \u2200g \u2208 G, { w g \u03be g g = \u2016wg\u2016\u221e\u2016\u03be \u20161 and \u2016\u03be \u20161 = \u03bb\u03b7g, or wg = 0.", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "\u2022 There is no flow going from V \u2212 to V + (see properties of the minimum (s, t)-cut [26]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Using the classical max-flow/min-cut theorem [25], we have equality between these two terms.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "2 Computation of the Dual Norm \u03a9 Similarly to the proximal operator, the computation of dual norm \u03a9 can itself shown to solve another network flow problem, based on the following variational formulation, which extends a previous result from [5]: Lemma 4 (Dual formulation of the dual-norm \u03a9.", "startOffset": 241, "endOffset": 244}, {"referenceID": 31, "context": "This primal problem is convex and satisfies Slater\u2019s conditions for generalized conic inequalities, which implies that strong duality holds [32].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "j\u2208Vu \u03baj is the value of the max-flow, and the inequality follows from the max-flow/min-cut theorem [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "C Algorithm FISTA with duality gap In this section, we describe in details the algorithm FISTA [4] when applied to solve problem (1), with a duality gap as stopping criterion.", "startOffset": 95, "endOffset": 98}, {"referenceID": 28, "context": "Based on Fenchel duality arguments [29], f(Xw) + \u03bb\u03a9(w) + f(\u2212\u03ba), for w \u2208 R,\u03ba \u2208 R and \u03a9(X\u03ba) \u2264 \u03bb, is a duality gap for (14).", "startOffset": 35, "endOffset": 39}, {"referenceID": 28, "context": "Given a primal variable w, a good dual candidate \u03ba can be obtained by looking at the conditions that have to be satisfied by the pair (w,\u03ba) at optimality [29].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "1 Speed comparison of Algorithm 1 with parametric max-flow algorithms As shown in [19], min-cost flow problems, and in particular, the dual problem of (3), can be reduced to a specific parametric max-flow problem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "efficient parametric max-flow algorithm proposed by Gallo, Grigoriadis, and Tar- jan [22] and a simplified version of the latter proposed by Babenko and Goldberg in [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "efficient parametric max-flow algorithm proposed by Gallo, Grigoriadis, and Tar- jan [22] and a simplified version of the latter proposed by Babenko and Goldberg in [23].", "startOffset": 165, "endOffset": 169}], "year": 2010, "abstractText": "We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of l\u221e-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems. Key-words: network flow optimization, convex optimization, sparse methods, proximal algorithms \u2217 Equal contribution. \u2020 INRIA WILLOW Project, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00e9rieure (INRIA/ENS/CNRS UMR 8548). 23, avenue d\u2019Italie, 75214 Paris. France Algorithmes de Flots pour Parcimonie Structur\u00e9e R\u00e9sum\u00e9 : Nous consid\u00e9rons une classe de probl\u00e8mes d\u2019apprentissage r\u00e9gularis\u00e9s par une norme induisant de la parcimonie structur\u00e9e, d\u00e9finie comme une somme de normes l\u221e sur des groupes de variables. Alors que de nombreux efforts ont \u00e9t\u00e9s mis pour d\u00e9velopper des algorithmes d\u2019optimisation rapides lorsque les groupes sont disjoints ou structur\u00e9s hi\u00e9rarchiquement, nous nous int\u00e9ressons au cas g\u00e9n\u00e9ral de groupes avec recouvrement. Nous montrons que le probl\u00e8me d\u2019optimisation correspondant est li\u00e9 \u00e0 l\u2019optimisation de flots sur un r\u00e9seau. Plus pr\u00e9cis\u00e9ment, l\u2019op\u00e9rateur proximal associ\u00e9 \u00e0 la norme que nous consid\u00e9rons est dual \u00e0 la minimisation d\u2019un co\u00fbt quadratique de flot sur un graphe particulier. Nous proposons une proc\u00e9dure efficace qui calcule cette solution en un temps polynomial. Notre algorithme peut traiter de larges probl\u00e8mes, comportant des millions de variables, et ouvre de nouveaux champs d\u2019applications pour les mod\u00e8les parcimonieux structur\u00e9s. Nous pr\u00e9sentons diverses exp\u00e9riences sur des donn\u00e9es d\u2019images et de vid\u00e9os, qui d\u00e9montrent l\u2019utilit\u00e9 et l\u2019efficacit\u00e9 de notre approche pour r\u00e9soudre de nombreux probl\u00e8mes. Mots-cl\u00e9s : optimisation de flots, optimisation convexe, m\u00e9thodes parcimonieuses, algorithmes proximaux Network Flow Algorithms for Structured Sparsity 3", "creator": "LaTeX with hyperref package"}}}