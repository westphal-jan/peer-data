{"id": "1012.0322", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2010", "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in Safety-Critical Systems", "abstract": "uncertainty of decisions in safety - critical engineering applications can be estimated on the basis of the bayesian markov chain monte carlo ( mcmc ) technique of averaging over decision models. computers use generalized decision tree ( dt ) models assists experts to interpret feedback relations and find applications of the uncertainty. bayesian averaging also allows experts remotely estimate the uncertainty accurately when a priori information on the favored structure of dts is published. then an expert independently select a single dt values, typically the maximum a posteriori model, for interpretation checks. unfortunately, a priori estimation on favored structure of dts is nonetheless always available. for this reason, we suggest a new mapping on dts between the bayesian mcmc tools. we also suggest a new procedure of selecting a meaningful dt and describe an application setting. in our experiments on projecting large - term conflict alert data our technique outperforms the existing bayesian techniques in predictive accuracy of the selected robust dts.", "histories": [["v1", "Wed, 1 Dec 2010 21:08:04 GMT  (203kb)", "http://arxiv.org/abs/1012.0322v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vitaly schetinin", "jonathan fieldsend", "derek partridge", "wojtek krzanowski", "richard everson", "trevor bailey", "adolfo hernandez"], "accepted": false, "id": "1012.0322"}, "pdf": {"name": "1012.0322.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Methodology for Estimating Uncertainty of Decisions in Safety-Critical Systems", "authors": ["Vitaly SCHETININ", "Jonathan E. FIELDSEND", "Derek PARTRIDGE", "Wojtek J. KRZANOWSKI", "Richard M. EVERSON", "Trevor C. BAILEY"], "emails": ["vitaly.schetinin@luton.ac.uk."], "sections": [{"heading": null, "text": "_________________________________ 1Corresponding Author: Vitaly Schetinin, Department of Computing and Information Systems, University of Luton, Luton, LU1 3JU, The UK; E-mail: vitaly.schetinin@luton.ac.uk.\nKeywords. Uncertainty, decision tree, Bayesian averaging, MCMC.\nIntroduction\nThe assessment of uncertainty of decisions is of crucial importance for many safetycritical engineering applications [1], e.g., in air-traffic control [2]. For such applications Bayesian model averaging provides reliable estimates of the uncertainty [3, 4, 5]. In theory, uncertainty of decisions can be accurately estimated using Markov Chain Monte Carlo (MCMC) techniques to average over the ensemble of diverse decision models. The use of decision trees (DT) for Bayesian model averaging is attractive for experts who want to interpret causal relations and find factors to account for the uncertainty [3, 4, 5].\nBayesian averaging over DT models allows the uncertainty of decisions to be estimated accurately when a priori information on favored structure of DTs is available as described in [6]. Then for interpretation purposes, an expert can select a single DT\nmodel which provides the Maximum a Posteriori (MAP) performance [7]. Unfortunately, in most practical cases, a priori information on the favored structure of DTs is not available. For this reason, we suggest a new prior on DT models within a sweeping strategy that we described in [8].\nWe also suggest a new procedure for selecting a single DT, described in Section 3. This procedure is based on the estimates obtained within the Uncertainty Envelope technique that we described in [9]. An application scenario, which can be implemented within the proposed Bayesian technique, is described in Section 5.\nIn this Chapter we aim to compare the predictive accuracy of decisions obtained with the suggested Bayesian DT technique and the standard Bayesian DT techniques. The comparison is run on air-traffic control data made available by the National Air Traffic Services (NATS) in the UK. In our experiments, the suggested technique outperforms the existing Bayesian techniques in terms of predictive accuracy."}, {"heading": "1. Bayesian Averaging over Decision Tree Models", "text": "In general, a DT is a hierarchical system consisting of splitting and terminal nodes. DTs are binary if the splitting nodes ask a specific question and then divide the data points into two disjoint subsets [3]. The terminal node assigns all data points falling in that node to the class whose points are prevalent. Within a Bayesian framework, the class posterior distribution is calculated for each terminal node, which makes the Bayesian integration computationally expensive [4].\nTo make the Bayesian averaging DTs a feasible approach, Denison et al. [5] have suggested the use of the MCMC technique, taking a stochastic sample from the posterior distribution. During sampling, the parameters \u03b8 of candidate-models are drawn from the given proposal distributions. The candidate is accepted or rejected accordingly to Bayes rule calculated on the given data D. Thus, for the m-dimensional input vector x, data D and parameters \u03b8 , the class posterior distribution ),|( Dxyp is\n \n N\ni\niyp N dpypyp 1\n)( ),,|(1)|(),,|(),|( D\u03b8x\u03b8D\u03b8D\u03b8xDx ,\nwhere )|( D\u03b8p is the posterior distribution of parameters \u03b8 conditioned on data D, and N is the number of samples taken from the posterior distribution.\nSampling across DT models of variable dimensionality, the above technique exploits a Reversible Jump (RJ) extension suggested by Green [10]. When priori information is not distorted and the number of samples is reasonably large, the RJ MCMC technique, making birth, death, change-question, and change-rule moves, explores the posterior distribution and as a result provides accurate estimates of the posterior.\nTo grow large DTs from real-world data, Denison et al. [5] and Chipman et al. [6] suggested exploring the posterior probability by using the following types of moves:\nBirth. Randomly split the data points falling in one of the terminal nodes by a new splitting node with the variable and rule drawn from the corresponding priors.\nDeath. Randomly pick a splitting node with two terminal nodes and assign it to be a single terminal with the united data points.\nChange-split. Randomly pick a splitting node and assign it a new splitting variable and rule drawn from the corresponding priors.\nChange-rule. Randomly pick a splitting node and assign it a new rule drawn from a given prior.\nThe first two moves, birth and death, are reversible and change the dimensionality of  as described in [10]. The remaining moves provide jumps within the current dimensionality of . Note that the change-split move is included to make \u201clarge\u201d jumps which potentially increase the chance of sampling from a maximal posterior whilst the change-rule move does \u201clocal\u201d jumps.\nFor the birth moves, the proposal ratio R is written\n, )()|'( )()|( \u03b8\u03b8\u03b8 \u03b8'\u03b8'\u03b8 pq pqR \nwhere )|( \u03b8'\u03b8q and )|'( \u03b8\u03b8q are the proposed distributions, \u03b8' and \u03b8 are (k + 1) and k-dimensional vectors of DT parameters, respectively, and p(\u03b8 ) and )(\u03b8'p are the probabilities of the DT with parameters \u03b8 and \u03b8' :\nKS k msN p\nk\nk\ni i\n11 )( 1)( 1 1 var           \u03b8 ,\nwhere )( varisN is the number of possible values of sivar which can be assigned as a new splitting rule, Sk is the number of ways of constructing a DT with k terminal nodes, and K is the maximal number of terminal nodes, K = n \u2013 1.\nThe proposal distributions are as follows\n,)'|( '\n1\nQ\nk D dq \u03b8\u03b8\nwhere DQ1 = DQ + 1 is the number of splitting nodes whose branches are both terminal nodes.\nThen the proposal ratio for a birth is given by\n. 11\n1\n  k k Qk k S S D k b dR\nThe number DQ1 is dependent on the DT structure and it is clear that DQ1 < k  k = 1, \u2026, K. Analyzing the above equation, we can also assume dk+1 = bk. Then letting the DTs grow, i.e., k  K, and considering Sk+1 > Sk, we can see that the value of R  c, where c is a constant lying between 0 and 1.\nAlternatively, for the death moves the proposal ratio is written as\n. )1( 11    k\nkQ\nk\nk\nS S k D d bR\nHowever, in practice the lack of a priori information brings bias to the posterior estimates, and as a result the evaluation of classification uncertainty may be incorrect [11].\nWithin the RJ MCMC technique, the prior on the number of splitting nodes should be given properly. Otherwise, most samples may be taken from the posterior calculated for DTs that are located far away from a region containing the desired DT models. Likewise, when the prior on the number of splits is assigned as uniform, the minimal number of data points, pmin, allowed to be at nodes may be set inappropriately small. In this case, the DTs will grow excessively and most of the samples will be taken from the posterior distribution calculated for over-fitted DTs. As a result, the use of inappropriately assigned priors leads to poor results [5, 6].\nFor the special cases when there is knowledge of the favored DT structure, Chipman et al. [6] suggested the prior probability, with which a terminal node should be split further. This probability is dependent on how many splits have been made above it. For the given constants  > 0 and   0, the probability Ps of splitting the ith node is\n  )1()( is diP ,\nwhere di is the number of splits made above node i. Here the additional parameters  and  serving as hyperpriors should be given properly."}, {"heading": "2. A Sweeping Strategy", "text": "Clearly, the lack of a priori knowledge on the favored DT structure, which often happens in practice, increases the uncertainty in results of the Bayesian averaged DTs. To decrease the uncertainty of decisions, a new Bayesian strategy of sampling DT models has been suggested [8]. The main idea behind this strategy is to assign the a priori probability of further splitting DT nodes dependent on the range of values within which the number of data points will be not less than pmin. This prior is explicit because at the current partition the range of such values is unknown.\nWithin the above prior, the new splitting value 'jq for variable j is drawn from a uniform distribution:\n),(~' ,1max ,1 min jj j xxUq ,\nand from a Gaussian with a given variance j :\n),(~' jjj qNq  ,\nfor the birth and change moves, respectively. Because of the hierarchical structure, new moves applied to the first partition levels can heavily change the shape of the DT and, as a result, at its bottom partitions the terminal nodes can contain fewer data points than pmin. However, if the DT contains one such node, we can sweep it and then make the death move. Less likely, after birth or change moves the DT will contain more than one node containing fewer than pmin data points. In such cases we have to resample the DT."}, {"heading": "3. Selection of a Single DT", "text": "In this Section we describe our method of interpreting Bayesian DT ensembles. This method is based on the estimates of confidence in the outcomes of the DT ensemble which can be quantitatively estimated on the training data within the Uncertainty Envelope technique."}, {"heading": "3.1. Selection Techniques", "text": "There are two approaches to interpreting DT ensembles. The first approach is based on searching a DT of MAP [11]. The second approach is based on the idea of clustering DTs in the two-dimensional space of DT size and DT fitness [12].\nOur approach is based on the quantitative estimates of classification confidence, which can be made within the Uncertainty Envelope technique described in [9]. The idea behind our method of interpreting the Bayesian DT ensemble is to find a single DT which covers most of the training examples classified as confident and correct. For multiple classification systems the confidence of classification outputs can be easily estimated by counting the consistency of the classification outcomes.\nIndeed, within a given classification scheme the outputs of the multiple classifier system depend on how well the classifiers were trained and how representative were the training data. For a given data sample, the consistency of classification outcomes depends on how close this sample is to the class boundaries. So for the ith class, the confidence in the set of classification models can be estimated as a ratio i between the number of classifier outcomes of the ith class, Ni, and the total number of classifiers N:  i = Ni /N, i = 1, \u2026, C, where C is the number of classes.\nClearly the classification confidence is maximal, equal to 1.0, if all the classifiers assign a given input to the same class, otherwise the confidence is less than 1.0. The minimal value of confidence is equal to 1/C if the classifiers assign the input datum to the C classes in equal proportions. So for a given input the classification confidence in the set of classifiers can be properly estimated by the ratio .\nWithin the above framework in real-world applications, we can define a given level of the classification confidence, 0: 1/C \u2264 0 \u2264 1, for which cost of misclassification is small enough to be accepted. Then for the given input, the outcome of the set of classifiers is said to be confident if the ratio  \u2265 0. Clearly, on the labeled data we can distinguish between confident and correct outcomes and confident but incorrect outcomes. The latter outcomes may appear in a multiple classifier system due to noise or overlapping classes in the data."}, {"heading": "3.2. A Selection Procedure", "text": "In practice, the number of DTs in the ensemble as well as the number of the training examples can be large. Nevertheless, counting the number of confident and correct outcomes as described above, we can find a desired DT which can be used for interpreting the confident classification. The performance of such a DT can be slightly worse than that of the Bayesian DT ensemble. Within the Chapter we provide the experimental comparison of their performances. The main steps of the selection procedure are next.\nAll that we need is to find a set of DTs which cover the maximal number of the training samples classified as confident and correct while the number of misclassifications on the remaining examples is kept minimal. To find such a DT set, we can remove the conflicting examples from the training data and then select the DTs with a maximal cover of the training samples classified by the DT ensemble as confident and correct.\nThus the main steps of the selection procedure are as follows: 1. Amongst a given Bayesian DT ensemble find a set of DTs, S1, which cover a\nmaximal number of the training samples classified as confident and correct with a given confidence level 0. 2. Find the training samples which were misclassified by the Bayesian DT ensemble and then remove them from the training data. Denote the remaining training samples as D1. 3. Amongst the set S1 of DTs find those which provide a minimal misclassification rate on the data D1. Denote the found set of such DTs as S2. 4. Amongst the set S2 of DTs select those whose size is minimal. Denote a set of such DTs as S3. The set S3 contains the desired DTs.\nThe above procedure finds one or more DTs and puts them in the set S3. These DTs cover a maximal number of the training samples classified as confident and correct with a given confident level 0. The size of these DTs is minimal and any of them can be finally selected for interpreting the confident classification."}, {"heading": "4. Experimental Results", "text": "In this Section first we describe the data used in our experiments. Then we show how the suggested Bayesian technique runs on these data. The resultant Bayesian averaging over DT models gives us a feature importance diagram. The suggested selection procedure gives us the single DTs for each run and finally we compare the predictive accuracies obtained with the existing procedures."}, {"heading": "4.1. The Experimental Data", "text": "The data used in our experiments are related to the Short-Term Conflict Alert (STCA) problem which emerges when the distance between two aircraft, landing or taking off, might be critically short. Table 1 lists 12 features selected for predicting STCA.\nIn this table x , y , and z are the distances between pairs of aircraft on the X,\nY, and height Z axes, respectively. Feature 2224 zyxx  is the distance between pairs of aircraft in 3-dimensional space. 1,xV is the velocity of craft 1 on axis X, \u2026, 2,zV is the velocity of craft 2 on height Z. 1T and 2T are the times since the last correlated plot in the lateral plane for aircraft 1 and aircraft 2, respectively.\nIn our experiments we used 2500 examples of radar cycles taken each 6 seconds. From these examples 984 cycles were labeled as alerts. The number of cycles relating to one pair is dependent on the velocity and, on average, is around 40. All the examples of radar cycles were split into halves for training and test data sets within 5 fold crossvalidation.\nFig. 1 shows two pairs of aircraft flying with different velocities: the distance between the aircraft is shown here by x4 versus the radar cycles. The alert cycles are shown as stars, and the cycles, recognized by experts as normal, are shown as circles.\nFrom Fig. 1, we can see that the left hand trace seems more complex for predicting the STCA than the right hand trace. First the series of alert cycles on the left hand trace is disrupted by 2 normal cycles, and second the aircraft having passed a critical 30th cycle remain in the alert zone. In contrast, the right hand trace seems straight and predictable."}, {"heading": "4.2. Performance of the Bayesian DT averaging technique", "text": "We ran the Bayesian DT technique without a priori information on the preferable DT shape and size. The minimal number of data points allowed in the splits, pmin, was set equal to 15 or 1.2% of the 1250 training examples. The proposal probabilities for the death, birth, change-split and change-rules were set to 0.1, 0.1, 0.2, and 0.6, respectively. The numbers of burn-in and post burn-in samples were set equal to 100k and 10k, respectively. The sampling rate was set equal to 7, and the proposal variance was set at 0.3 in order to achieve the rational rate of acceptance rate around 0.25, which was recommended in [5].\n5 fold cross-validation was used to estimate the variability of the resultant DTs. The performances of all the 5 runs were nearly the same, and for the first run Fig. 2 depicts samples of log likelihood and numbers of DT nodes as well as the densities of DT nodes for burn-in and post burn-in phases.\nFrom the top left plot we can see that the Markov chain converges to the stationary value of log likelihood near to \u2013500 after starting around \u20131200. During the post burnin phase the values of log likelihood slightly oscillate between \u2013550 and \u2013500.\nThe acceptance rates were 0.24 for the burn-in and 0.22 for the post burn-in phases. The average number of DT nodes and its variance were equal to 54.4 and 2.2, respectively.\nOn the first run, the Bayesian DT averaging technique misclassified 14.3% of the test examples. The rate of the confident and correct outcomes was 62.77%."}, {"heading": "4.3. Feature Importance", "text": "Table 2 lists the average posterior weights of all the 12 features sorted by value. The bigger the posterior weight of a feature, the greater is its contribution to the outcome. On this basis, Table 2 provides ranks for all the 12 features.\nFig. 3 shows us the error bars calculated for the contributions of the 12 features to the outcome averaged over the 5 fold cross-validation. From this figure we can see that such features as x8, x1, and x9 are used in the Bayesian DTs, on average, more frequently than the others. In contrast, feature x12 is used with a less frequency. Additionally, the widths of the error bars in Fig. 3 give us the estimates of variance of the contributions.\nTable 2. Posterior weights of the features sorted on their contribution to the outcome.\nFeature Posterior weight\nRank\nx8 0.168 1 x1 0.137 2 x9 0.120 3 x6 0.110 4 x4 0.095 5 x5 0.090 6 x3 0.078 7 x2 0.061 8 x10 0.050 9 x7 0.042 10 x11 0.001 11 x12 0.008 12\nFigure 3. Feature importance averaged over 5 fold cross-validation."}, {"heading": "4.4. A Resultant DT", "text": "The resultant DT selected by the SC procedure is presented as a machine diagram in Fig. 4. Each splitting node of the DT provides a specific question that has a yes/no answer, and two branches. The terminal nodes provide the predictive probabilities of alert, whose values range between 0.0 and 1.0.\nnode01 X04 < 1847.05, then node03, otherwise node45 node03 X04 < 1459.91, then node06, otherwise node28 node06 X05 < -281.95, then node15, otherwise node07 node07 X03 < 1713.61, then node08, otherwise node12"}, {"heading": "4.5. Comparison of Performances", "text": "In this section we compare our technique of extracting a sure correct (SC) DT with MAP, and the maximum a posterior weight (MAPW). The comparison is made in terms of misclassification within 5 fold cross-validation. The misclassification rates of the above three techniques: SC, MAP, and MAPW are shown in Fig. 5. The left side plot shows the misclassification rates of the single DTs on the test data, and the right side plot shows its sizes.\nIn theory, the Bayesian averaging technique should provide lower misclassification rates than any other single DTs selected by the SC, MAP, and MAPW techniques. On the first run, we can observe that all the single DTs perform worse than the Bayesian ensemble of DTs which has misclassified 14.3% of the test data.\nComparing the misclassification rates of the SC, MAP, and MAPW shown in Fig. 5, we can see that the suggested SC technique more often out-performs the other two techniques, that is, the SC technique out-performs the MAP and MAPW techniques on the 4 runs.\nComparing the DT sizes on the right side plot, we can see that the SC technique has extracted shorter DTs than the MAP technique in 4 runs. At the same time, comparing the sizes of the SC and MAPW DTs, we can see that the SC technique has extracted shorter DTs in 2 runs only."}, {"heading": "5. An Application Scenario", "text": "In a general form an application scenario within our approach can be described as a sequence of the following steps.\n1. Define a set of features considered by domain experts as the most important\nones for the classification. For example, a domain expert can assume the velocities of aircraft in coordinates X and Y as the most important features for predicting the STCA. 2. Within a defined set of features, collect a representative set of data samples confidently classified by domain experts. For instance, a domain expert can arrange a set of radar data received from different pairs of aircraft in which some of the radar cycles were labeled as the STCA. 3. Analyze a priori knowledge and formulate priors within the Bayesian methodology. For example, domain experts can represent their knowledge in a form of decision tree asking specific questions. Such a priori information can be used within our approach in order to improve the performance. 4. Define parameters of DTs such as the minimal number of data samples, pmin, allowed to be in splitting nodes. By changing this parameter, a modeler can find in an ad hoc manner the number of splitting nodes providing the best performance of DTs. 5. Define parameters of MCMC such as the number of burn-in and post burn-in samples, proposal probabilities for the death, birth, change-split and changerule, as well as the sampling rate. At this step a modeler can also specify suitable proposal distributions for the moves. However, if a modeler has no idea about the proposal distribution, a uniform distribution, known also as an \u201cuninformative\u201d prior, is used. 6. Specify a criterion for convergence of Markov Chain. Within the Bayesian MCMC technique, the convergence of a Markov Chain is usually achieved if the number of burn-in samples set enough large. Practically, the convergence is achieved if after approximately 1/3 of burn-in samples the likelihood values do not change significantly. This can be easily visualized by observing the log likelihood values plotted versus the number of post burn-in samples. 7. A modeler has to control the diversity of DTs collected during the post burn-in phase. The diversity is estimated implicitly by estimating an acceptance level which is the ratio of the number of the proposed and accepted DTs to the total number of proposed DTs. Practically, when the acceptance level is near 0.25, a set of collected DTs is optimally diversified. 8. Practically, the desired acceptance level is achieved by changing the following parameters: pmin, the variance of proposal distribution, and the expected number of splitting nodes. 9. To select a single DT providing the most confident classification, a modeler has to predefine a level of confident classification, 0. The value of 0 is dependent on the cost of misclassifications allowed in an application. Clearly, if the cost of classification is high, the value of 0 is defined close to 1.0, say 0.9990. This means that a decision is confident if no more than 10 classifiers from 10000 are contradictory. Otherwise, a decision is assigned uncertain. 10. Having obtained a confident DT, a modeler can observe a decision model and analyze the features used in this model. Additionally, a modeler can run the\nBayesian MCMC and DT selection techniques within n-fold cross validation in order to estimate the contribution of each feature to the classification. As a result, the contributions of all features can be ranked, and a modeler can find the most critical features in an application.\nRelated to the STCA problem, the application scenario used in our work is described as follows. The domain experts have defined 12 features listed in Table 1. From the collected 2500 data samples, 1250 were used for training and the remaining 1250 for testing. A priori information was not available in our case and, therefore, priors were given as \u201cuninformative\u201d except for a Gaussian for the proposal distribution with the variance set to 0.3. The number pmin = 15 was experimentally found to provide the best performance. The numbers of burn-in and post burn-in samples were set equal to 100 k and 10 k, respectively. The proposal probabilities for the death, birth, change-split and change-rules were set to be 0.1, 0.1, 0.2, and 0.6, respectively. Every 7th DT was collected during the post burn-in phase, i.e., the sampling rate was 7. The convergence of the Markov Chain can be visually observed from the top right plot in Fig. 2 \u2013 from this plot we can see that after approximately 1/3 of burn-in samples the values of log likelihood do not change significantly. The acceptance level during the post burn-in phase was obtained equal to 0.22 that is close to 0.25 when the diversity of DTs is optimal. The level of confident classification, 0, was predefined to be 0.99. Fig 4 represents the machine diagram of the resultant DT selected under the given 0. Finally, the performance of the resultant DT is compared with the performances of the Bayesian DT technique as well as the MAP DT within the 5 fold cross-validation as shown in Fig. 5."}, {"heading": "6. Conclusion", "text": "For estimating uncertainty of decisions in safety-critical engineering applications, we have suggested the Bayesian averaging over decision models using a new strategy of the RJ MCMC sampling for the cases when a priori information on the favored structure of models is unavailable. The use of DT models assists experts to interpret causal relations and find factors to account for the uncertainty. However, the Bayesian averaging over DTs allows experts to estimate the uncertainty accurately when a priori information on favored structure of DTs is available.\nTo interpret an ensemble of diverse DTs sampled by the RJ MCMC technique, experts select the single DT model that has maximum a posteriori probability. However in practice this selection technique tends to choose over-fitted DTs which are incapable of providing a high predictive accuracy.\nIn this Chapter we have proposed a new procedure of selecting a single DT. This procedure is based on the estimates of uncertainty in the ensemble of the Bayesian DTs. For estimating the uncertainty, the use of an Uncertainty Envelope technique has been advocated. As a result, in our experiments with the STCA data, the suggested technique outperforms the existing Bayesian techniques in terms of predictive accuracy.\nThus, we conclude that the technique proposed for interpreting the ensemble of DTs allows experts to select a single DT providing the most confident estimates of outcomes. These are very desirable properties for classifiers used in safety-critical systems, in which assessment of uncertainty of decisions is of crucial importance."}, {"heading": "7. Acknowledgements", "text": "The work reported was largely supported by a grant from the EPSRC under the Critical Systems Program, grant GR/R24357/01."}], "references": [{"title": "Probability and Statistical Methods in Engineering Design", "author": ["A. Haldar", "S. Mahadevan"], "venue": "Wiley", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Multi-Objective optimization of safety related Systems: An application to short term conflict alert", "author": ["R. Everson", "J.E. Fieldsend"], "venue": "IEEE Transactions on Evolutionary Computation (forthcoming)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L. Kuncheva"], "venue": "Wiley", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Classification and Regression Trees", "author": ["L. Brieman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": "Belmont, CA, Wadsworth", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1984}, {"title": "Bayesian Methods for Nonlinear Classification and Regression", "author": ["D. Denison", "C. Holmes", "B. Malick", "A. Smith"], "venue": "Wiley", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Bayesian CART model search", "author": ["H. Chipman", "E. George", "R. McCullock"], "venue": "J. American Statistics 93 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Bayesian averaging of classifiers and the overfitting problem, International Conference on Machine Learning", "author": ["P. Domingos"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "The Bayesian decision tree technique with a sweeping strategy", "author": ["V. Schetinin", "J.E. Fieldsend", "D. Partridge", "W.J. Krzanowski", "R.M. Everson", "T.C. Bailey", "A. Hernandez"], "venue": "Int. Conference on Advances in Intelligent Systems - Theory and Applications, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Bayesian inductively learned modules for safety critical systems", "author": ["J.E. Fieldsend", "T.C. Bailey", "R.M. Everson", "W.J. Krzanowski", "D. Partridge", "V. Schetinin"], "venue": "Symposium on the Interface: Computing Science and Statistics, Salt Lake City", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Reversible jump Markov Chain Monte Carlo computation and Bayesian model determination", "author": ["P. Green"], "venue": "Biometrika 82 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Knowledge discovery via multiple models", "author": ["P. Domingos"], "venue": "Intelligent Data Analysis 2 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Making sense of a forest of trees", "author": ["H. Chipman", "E. George", "R. McCulloch"], "venue": "Symposium on the Interface, S. Weisberg, Ed., Interface Foundation of North America", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "The assessment of uncertainty of decisions is of crucial importance for many safetycritical engineering applications [1], e.", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": ", in air-traffic control [2].", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "For such applications Bayesian model averaging provides reliable estimates of the uncertainty [3, 4, 5].", "startOffset": 94, "endOffset": 103}, {"referenceID": 3, "context": "For such applications Bayesian model averaging provides reliable estimates of the uncertainty [3, 4, 5].", "startOffset": 94, "endOffset": 103}, {"referenceID": 4, "context": "For such applications Bayesian model averaging provides reliable estimates of the uncertainty [3, 4, 5].", "startOffset": 94, "endOffset": 103}, {"referenceID": 2, "context": "The use of decision trees (DT) for Bayesian model averaging is attractive for experts who want to interpret causal relations and find factors to account for the uncertainty [3, 4, 5].", "startOffset": 173, "endOffset": 182}, {"referenceID": 3, "context": "The use of decision trees (DT) for Bayesian model averaging is attractive for experts who want to interpret causal relations and find factors to account for the uncertainty [3, 4, 5].", "startOffset": 173, "endOffset": 182}, {"referenceID": 4, "context": "The use of decision trees (DT) for Bayesian model averaging is attractive for experts who want to interpret causal relations and find factors to account for the uncertainty [3, 4, 5].", "startOffset": 173, "endOffset": 182}, {"referenceID": 5, "context": "Bayesian averaging over DT models allows the uncertainty of decisions to be estimated accurately when a priori information on favored structure of DTs is available as described in [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "model which provides the Maximum a Posteriori (MAP) performance [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "For this reason, we suggest a new prior on DT models within a sweeping strategy that we described in [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "This procedure is based on the estimates obtained within the Uncertainty Envelope technique that we described in [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "DTs are binary if the splitting nodes ask a specific question and then divide the data points into two disjoint subsets [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Within a Bayesian framework, the class posterior distribution is calculated for each terminal node, which makes the Bayesian integration computationally expensive [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "[5] have suggested the use of the MCMC technique, taking a stochastic sample from the posterior distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Sampling across DT models of variable dimensionality, the above technique exploits a Reversible Jump (RJ) extension suggested by Green [10].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "[5] and Chipman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] suggested exploring the posterior probability by using the following types of moves: Birth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The first two moves, birth and death, are reversible and change the dimensionality of \uf071 as described in [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "However, in practice the lack of a priori information brings bias to the posterior estimates, and as a result the evaluation of classification uncertainty may be incorrect [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "As a result, the use of inappropriately assigned priors leads to poor results [5, 6].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "As a result, the use of inappropriately assigned priors leads to poor results [5, 6].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "[6] suggested the prior probability, with which a terminal node should be split further.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "To decrease the uncertainty of decisions, a new Bayesian strategy of sampling DT models has been suggested [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "The first approach is based on searching a DT of MAP [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "The second approach is based on the idea of clustering DTs in the two-dimensional space of DT size and DT fitness [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "Our approach is based on the quantitative estimates of classification confidence, which can be made within the Uncertainty Envelope technique described in [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "25, which was recommended in [5].", "startOffset": 29, "endOffset": 32}], "year": 2010, "abstractText": "Uncertainty of decisions in safety-critical engineering applications can be estimated on the basis of the Bayesian Markov Chain Monte Carlo (MCMC) technique of averaging over decision models. The use of decision tree (DT) models assists experts to interpret causal relations and find factors of the uncertainty. Bayesian averaging also allows experts to estimate the uncertainty accurately when a priori information on the favored structure of DTs is available. Then an expert can select a single DT model, typically the Maximum a Posteriori model, for interpretation purposes. Unfortunately, a priori information on favored structure of DTs is not always available. For this reason, we suggest a new prior on DTs for the Bayesian MCMC technique. We also suggest a new procedure of selecting a single DT and describe an application scenario. In our experiments on real data our technique outperforms the existing Bayesian techniques in predictive accuracy of the selected single DTs.", "creator": "Microsoft Word - book_chapter_final - Bayesian Methodology for Estimating Uncertainty of Decisions in Safety-Critical Systems.doc"}}}