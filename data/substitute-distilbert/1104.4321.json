{"id": "1104.4321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2011", "title": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and Compounds", "abstract": "chinese authors can be compared to a fundamental structure : a character is analogous to a molecule, radicals are like atoms, calligraphic strokes correspond to elementary particles, and when characters form compounds, they are like molecular structures. in chemistry the conjunction of all of these structural levels produces equations we read as matter. though language, the conjunction of strokes, radicals, characters, and compounds will difference. but when does meaning arise? we all know that radicals are, in conventional sense, the basic semantic components of chinese script, but what about strokes? considering the fact that many characters are made besides adding individual strokes to ( combinations of ) radicals, we sometimes legitimately ask the scholars whether writers carry meaning, or symbols. with this talk people will present my project of extending traditional nlp techniques to radicals and strokes, aiming towards increase a deeper understanding of the way ideographic languages model the world.", "histories": [["v1", "Thu, 21 Apr 2011 17:56:50 GMT  (443kb)", "http://arxiv.org/abs/1104.4321v1", "13 pages, 5 figures"]], "COMMENTS": "13 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yannis haralambous"], "accepted": false, "id": "1104.4321"}, "pdf": {"name": "1104.4321.pdf", "metadata": {"source": "CRF", "title": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and Compounds", "authors": ["Yannis Haralambous"], "emails": [], "sections": [{"heading": "1 Introduction: the Chinese Writing System", "text": "The Chinese writing system uses characters (called h\u00e0nz\u00ec in Chinese, kanji in Japanese, and hanja in Korean) which are logographic (i.e., a grapheme represents a word or a morpheme). KangXi, one of the most important Chinese dictionaries, includes more than 47,000 characters, and Unicode v. 6 [15] encodes almost 75,000 of them. Such quantities of symbols would require superhuman abilities to memorize if there were not an internal structure allowing the reader to infer at least an approximation of the character\u2019s meaning. This structure is based on radicals and on strokes."}, {"heading": "1.1 Radicals", "text": "Quoting [14], \u201cthere are actually two different Chinese terms that can be translated into the word radical, making this word potentially confusing. First there are approximatively 214 unit called b\u00f9sh\u01d2u, that are used to look up a character in a dictionary. For horizontally structured characters, these are often found on the left-hand side. [...] Second, though there is the larger set of components, called b\u00f9ji\u00e0n that includes all components no matter where in the character they appear.\u201d They later add that in a Chinese dictionary they found 541 such radicals.\nUnicode has encoded the former 214 in a dedicated character table (see Fig. 1). In the Unihan database, which is provided by the Unicode Consortium, each of the 75,000 characters encoded in Unicode is marked as being based on one of these radicals. The CHISE project [8] provides a decomposition of characters into radicals plus some calligraphic strokes.\nBesides characters that are exact copies of radicals, characters can be graphical (horizontal, vertical, enclosing) combinations of radicals (including multiple copies of the same radical as\n*The University of Aizu and T\u00e9l\u00e9com Bretagne.\n2FDFKangxi Radicals2F00\nin \u6797 and \u68ee, which are the double and triple copy of \u6728), or combinations of radicals and individual strokes, like in \u72ac which is radical \u5927 with an additional stroke (cf. \u00a72).\nAs explained in [13], about 80% of the most frequent characters in Chinese are semanticphonetic compounds. These characters contain at least two radicals, of which the one (usually the one on the left) bears the meaning of the character and the other (on the right) provides partial information regarding the pronunciation of the character. For example, \u6c90 means \u201ctake a bath\u201d and it contains, on the left, the radical\u6c34 for \u2018water\u2019 (in its special graphical form\u6c35, used whenever it appears on the left half of a character) and on the right a radical pronounced m\u00f9, so that the character itself is also pronounced m\u00f9. Characters which have the pronunciation of their phonetic radical are called regular. Other possible cases are those that have the same pronunciation but with a different tone (semi-regular) and those that have an entirely different pronunciation (irregular).\nAccording to TomoMorioka [9], Japanese on reading of kanjis often inherits from this (Chinese) feature of having a phonetic right component, but generally modern Japanese speakers are not conscious of this underlying structure."}, {"heading": "1.2 Strokes", "text": "Chinese characters are drawn using a specific repertoire of strokes. While there is a consensus on the very basic strokes, their combinations are considered by some authors as equally fundamental strokes and not by others. In Fig. 2 one can see the basic calligraphic strokes as encoded by Unicode and those used by the Character Description Language. The two tables agree on most of the strokes with just a few exceptions which are always combinations of the basic strokes.\nCharacter Description Language [1] is a project of theWenlin Institute aiming to graphically describe all Chinese characters through their strokes. A CDL description of a character is an XML element containing a recursive structure, the leaves of which are fundamental calligraphic strokes. To accurately place a stroke in the ideographic square, the coordinates of the bounding box of the stroke are used, as in the following example:\n<cdl char=\u2019\u4eac\u2019 uni=\u20194eac\u2019> <comp points=\u20190,0 128,68\u2019 >\n<comp char=\u2019\u4ea0\u2019 uni=\u20194ea0\u2019 points=\u20190,0 128,38\u2019 > <stroke type=\u2019d\u2019 points=\u201954,0 68,92\u2019 /> <stroke type=\u2019h\u2019 points=\u20190,128 128,128\u2019 /> </comp> <comp char=\u2019\u53e3\u2019 uni=\u201953e3\u2019 points=\u201930,74 98,128\u2019 >\n<comp points=\u20190,0 128,128\u2019 > <stroke type=\u2019s\u2019 points=\u20190,0 0,128\u2019 tail=\u2019long\u2019 /> <stroke type=\u2019hz\u2019 points=\u20190,0 128,0 128,128\u2019 head=\u2019cut\u2019 tail=\u2019long\u2019 /> </comp> <stroke type=\u2019h\u2019 points=\u20190,128 128,128\u2019 head=\u2019cut\u2019 tail=\u2019cut\u2019 />\n</comp> </comp> <comp points=\u20190,68 124,128\u2019 >\n<stroke type=\u2019sg\u2019 points=\u201968,0 68,128 38,99\u2019 head=\u2019cut\u2019 /> <stroke type=\u2019p\u2019 points=\u201935,40 0,115\u2019 tail=\u2019long\u2019 /> <stroke type=\u2019d\u2019 points=\u201987,34 128,115\u2019 />\n</comp> </cdl>\nwhere d (and d\u2032), h (and h\u2032), s, hz, p, and sg are the fundamental calligraphic strokes di\u01cen, h\u00e9ng, sh\u00f9, h\u00e9ng-zh\u00e9, pi\u011b and sh\u00f9-g\u014du from Table 2.\nThe example above is not in standard CDL syntax; in fact, whey have recursively replaced closed <comp> elements by open elements (with or without Unicode ID and glyph) containg other <comp> elements as well as <stroke> elements, which are the leaves of our CDL tree."}, {"heading": "1.3 Going from strokes to radicals to characters", "text": "With strokes we can form b\u00f9sh\u01d2u radicals, which bear meaning. But there are also \u201cphonetic\u201d radicals, which, supposedly bear no meaning but indicate pronunciation, and there are also other components in characters, always obtained by using graphical elements from the same set of calligraphic strokes.\nThis leads us to raise the question: \u201cwhen we go from strokes to radicals, components and characters, when does meaning arise?\u201d In other words: do specific combinations of strokes, other than b\u00f9sh\u01d2u radicals, carry meaning, or contribute to supply meaning?"}, {"heading": "1.4 Compound words", "text": "Regarding meaning, there is another semantic stratum in the Chinese writing system, namely that of compounds. A compound is a group of mostly two (but sometimes more) Chinese characters where emerges a newmeaning, different from the sequence of individual meanings. A typical example is \u767e\u59d3 which a compound of \u767e (a hundred) and \u59d3 (surname) and means \u201cfarmer.\u201d\nJapanese WordNet [6] contains more than 40,000 compound word entries (written as two or more kanji letters).\nSo actually there are four structural levels of the Chinese writing system:\n1. stroke;\n2. radical, be it b\u00f9sh\u01d2u, phonetic, or just a graphical component;\n3. character;\n4. compound word.\nWe can compare this stratification with that of matter: strokes can be compared to elementary particles, which form atoms (radicals). Atoms connect in various ways to form molecules (characters), and molecules form macromolecular structures (compound words)."}, {"heading": "2 Our model", "text": "To study the Chinese writing system we use the following model: Let K be the set of all Chinese characters as encoded in Unicode, and G be a graph with set of vertices K. Each k \u2208 K carries the following information:\n1. the main b\u00f9sh\u01d2u radical (information obtained from Unihan database);\n2. strokes of the character (information obtained from CDL);\n3. one or more meanings in Chinese or Japanese (information obtained from Japanese and Chinese WordNets).\nItems 1 and 2 are mandatory; item 3 is optional (and depends on the use or not of a given character in one of the two languages, as well on the completeness of the two WordNet databases).\nIn the remainder of this section we describe various edge schemes which can be added to G, as well as induced weights on edges and vertices."}, {"heading": "2.1 Modeling strokes", "text": "Let us first formalize the notion of stroke. In CDL every stroke has a type (it belongs to one of the 39 fundamental calligraphic strokes of Fig. 2) and a bounding box. On Fig. 3, the reader can see the decomposition of character \u4eac (= capital) into strokes, and the corresponding bounding boxes. It should be noted that we have numbered the boxes according to the standard order of strokes, but this information is not contained in CDL, so our model of the character must be independent of stroke order.\nWe would like to model strokes so that:\n1. frequent pattern search may be possible;\n2. order of strokes is not taken into consideration;\n3. patterns depend upon stroke type and geometric disposal, but not on size;\n4. the model should be robust with respect to small bounding box variations;\n5. the modeling algorithm should be entirely automatic, without human intervention.\nIt should be noted that in the literature one can find many Chinese character description schemes, based on two different goals:\n1. OCR (for example, [12, 7, 2], where the input data is a bitmap image and structure must be extracted from it;\n2. font generation [3, 11], where the input data is some logical and well organized database (containing a description of the character skeleton) and the output is a typographically acceptable Chinese character font.\nOur model lies between those approaches, since our input data (CDL) is much more precise than a bitmap image, but does not contain a logical description of a character skeleton.\nAs can be seen on Fig. 3, character \u4eac contains two strokes of type h, two d and one s, hz, sg and p. We define S(\u4eac) = {h, hz, . . .} the set of strokes of \u4eac. To describe the geometric\ndisposal of S(\u4eac) we take horizontal and vertical projections of the stroke bounding boxes (see Fig. 4).\nLet h\u2113 be the projection of the left side of the bounding box of stroke h, and hr , ht, hb those of the right, top and bottom sides, resp. We have total orders for each dimension:\np\u2113 = h\u2113 < s\u2113 = hz\u2113 < h\u2032\u2113 < sr < sg\u2113 < pr < d\u2113 < sgr < dr < d \u2032 \u2113 < h \u2032 r < hzr < d \u2032 r < hr,\ndt > ht > db = hb > hzt > st > h\u2032t = sgt > sb = h \u2032 b = hzb > pt = d \u2032 t > pb = d \u2032 b > sgb.\nBy using concatenation to represent strict inequality and brackets for enclosing equal values, we obtain the following notation:\n[p\u2113h\u2113][s\u2113hz\u2113]h\u2032\u2113srsg\u2113prd\u2113sgrdrd \u2032 \u2113h \u2032 rhzrd \u2032 rhr,\ndtht[dbhb]hztst[h\u2032tsgt][sbh \u2032 bhzb][ptd \u2032 t][pbd \u2032 b]sgb.\nwhich we consider the description of character \u4eac. It is clear that this description is independent of the order and of the (absolute) size of strokes. To make it more robust, we can round up the numeric values before comparison1.\nInterpreting brackets as parts of regular expressions, we can consider all the strings in which every [x1x2 \u00b7 \u00b7 \u00b7xn] is replaced by some xi. These are words of a formal language, whose alphabet is the set of x\u2113, xr, xt, xb for each bounding box x. To find frequent patterns we can use common subword detection techniques.\nTo illustrate this method, let us compare characters \u4eac and \u4f59, whose CDL description is:\n<cdl char=\u2019\u4f59\u2019 uni=\u20194f59\u2019> <comp char=\u2019\ud840\udda2\u2019 uni=\u2019201a2\u2019 points=\u20190,0 128,48\u2019 >\n<stroke type=\u2019p\u2019 points=\u201964,0 0,128\u2019 tail=\u2019long\u2019 /> <stroke type=\u2019n\u2019 points=\u201964,0 128,128\u2019 head=\u2019cut\u2019 />\n</comp> <comp points=\u20190,46 124,128\u2019 >\n<stroke type=\u2019h\u2019 points=\u201939,0 91,0\u2019 /> <stroke type=\u2019h\u2019 points=\u201917,41 116,41\u2019 /> <stroke type=\u2019sg\u2019 points=\u201967,0 67,128 39,116\u2019 head=\u2019cut\u2019 />\n1Nevertheless, this is a delicate issue, since although most values can be rounded without changing the global aspect of the character, in some cases a small change may bear a new reading. This is the case of stroke 1 vs. stroke 2: if stroke 1 would continue underneath stroke 2, the reading of the character could be different. One needs only compare characters \u529b (= strength) and \u5200 (= knife): disappearance of the small vertical extension on top of \u529b because of rounding calculations leads to wrong identification of the character.\n<stroke type=\u2019p\u2019 points=\u201943,59 0,125\u2019 tail=\u2019long\u2019 /> <stroke type=\u2019d\u2019 points=\u201988,59 128,116\u2019 />\n</comp> </cdl>\nAs we can see already in the CDL code, these two characters share the same lower part (strokes sg, p, d). The formula of \u4f59 is:\np\u2113p \u2032 \u2113h \u2032 \u2113h\u2113sg\u2113p \u2032 rh\u2113sgrprd\u2113hr[h \u2032 rdr]nr,\nptntht[hbsgt]nbpbh\u2032th \u2032 b[p \u2032 tdt][p \u2032 bdb]sgb.\nLet us compare the two:\n\u4eac \u4f59 hor. [p\u2113h\u2113][s\u2113hz\u2113]h\u2032\u2113srsg\u2113prd\u2113sgrdrd\u2032\u2113h\u2032rhzrd\u2032rhr p\u2113p\u2032\u2113h\u2032\u2113h\u2113sg\u2113p\u2032rh\u2113sgrprd\u2113hr[h\u2032rdr]nr vert. dtht[dbhb]hztst[h\u2032tsgt][sbh\u2032bhzb][ptd\u2032t][pbd\u2032b]sgb ptntht[hbsgt]nbpbh\u2032th\u2032b[p\u2032tdt][p\u2032bdb]sgb\nBy renaming strokes p\u2032 \u2192 p and d \u2192 d\u2032 in \u4f59, we see that the boundaries of p, d\u2032 and sg keep the same relative orders both in horizontal and vertical direction:\n\u4eac \u4f59 hor. [p\u2113h\u2113][s\u2113hz\u2113]h\u2032\u2113srsg\u2113prd\u2113sgrdrd\u2032\u2113h\u2032rhzrd\u2032rhr p\u2032\u2113p\u2113h\u2032\u2113h\u2113sg\u2113prh\u2113sgrp\u2032rd\u2032\u2113hr[h\u2032rd\u2032r]nr vert. dtht[dbhb]hztst[h\u2032tsgt][sbh\u2032bhzb][ptd\u2032t][pbd\u2032b]sgb p\u2032tntht[hbsgt]nbp\u2032bh\u2032th\u2032b[ptd\u2032t][pbd\u2032b]sgb\nnamely p\u2113 < sg\u2113 < pr < sgr < d\u2032\u2113 < d\u2032r and sgt > pt = d\u2032t > pb = d\u2032b > sgb. We say that characters \u4eac and \u4f59 share the pattern of three strokes p, d\u2032 and sg.\nLet us formalize this approach:\n\u2022 let K be the set of all Chinese characters, T = {h, t, s, sg, p, . . .} the set of types of calligraphic strokes;\n\u2022 let k \u2208 K be a Chinese character of N(k) strokes, S(k) = {s1, . . . , sN(k)} its set of strokes, \u03c4(sj) \u2208 S the type of stroke sj , (\u2113(sj), b(sj), r(sj), t(sj)) \u2208 R4 the bounding box of sj (where \u2113 is the horizontal projection of left side, r the hor. proj. of right side, b the vertical projection of the lower side, and t the vert. proj. of upper side);\n\u2022 then there is a total order of sets {\u2113(s1), r(s1), \u2113(s2), r(s2), . . . , \u2113(sN(k)), r(sN(k))} and {t(s1), b(s1), . . . , t(sN(k)), b(sN(k))} such that we can write\n\u03d5(si1) \u2022 \u03d5(si2) \u2022 \u00b7 \u00b7 \u00b7 \u2022 \u03d5(siN(k)) \u03c8(sj1) \u2022 \u03c8(sj2) \u2022 \u00b7 \u00b7 \u00b7 \u2022 \u03c8(sjN(k))\nwhere \u03d5 is either \u2113 or r, \u03c8 is either t or b, and \u2022 is either = or <;\n\u2022 in the above expression the order of terms is not relevant whenever \u2022 denotes equality=. This means that we have as many equivalent expressions as there are permutations of the terms separated by = signs;\n\u2022 we call the equivalence class \u03c3(k) of these expressions, the signature of k."}, {"heading": "2.2 Common strokes and frequent patterns", "text": "Using the notation of previous section, we say that k, k\u2032 \u2208 K have common strokes \u03b31, \u03b32, . . . , \u03b3m \u2208 S(k) and \u03b3\u20321, \u03b3\u20322, . . . , \u03b3\u2032m \u2208 S(k\u2032) whenever \u03c4(\u03b3i) = \u03c4(\u03b3\u2032i) for all i, and the gi and g\u2032i all appear in the signatures of k and k\u2032, in the same order.\nOur first edge-structure GS on G will be the following: two Chinese characters k and k\u2032 are connected by an edge e(k, k\u2032) of weightwS(k, k\u2032) if and only if they contain exactlywS(k, k\u2032) > 0 common strokes, as defined above. To each edge e corresponds a set of common strokes \u0393(e) = {\u03b31, . . . , \u03b3wS(k,k\u2032)}.\nExperiment 1. Calculate GS and find the most frequent subsets of all \u0393(e).\nAmong the most frequent subsets we expect to find b\u00f9sh\u01d2u radicals, and probably also other components. In the remainder of this paper, we will investigate whether the weight wS can be correlated with semantic similarity."}, {"heading": "2.3 Radical segmentation", "text": "A different approach to Chinese character description is to decompose them into b\u00f9sh\u01d2u radicals and a few strokes, using not precise coordinates or local behavior as in the method provided above, but Ideographic Description Sequences (IDS). These use special characters\u2ff0\u2ff1\u2ff2\u2ff3 \u2ff4\u2ff5\u2ff6\u2ff7\u2ff8\u2ff9\u2ffa\u2ffb as operators to denote specific geometric assemblings of character pairs or triples. For example, \u2ff0\u529b\u56d7 means that character \u52a0 can be assembled by a horizontal combination of \u529b and \u56d7. Operators can be combined, so for example \u884b can be written as \u2ff3\u807f\u2ff0\u2ff1\u4e00\u767d\u2ff1\u4e00\u767d\u2ff1\u4e3f\u76bf (that is: \u2ff3(\u807f\u2ff0(\u2ff1(\u4e00\u767d)\u2ff1(\u4e00\u767d))\u2ff1(\u4e3f\u76bf))).\nThe CHISE project [8] has provided IDS descriptions of all Unicode-encoded Chinese characters, segmenting them into b\u00f9sh\u01d2u radicals and 1,683 components (the glyphs of which are taken from various resources, such as GT [20], CDP [16, 17], CNS 11643 [18], Dai Kanwa dictionary [19], and others. For instance we find that our example from last section \u4eac has the (radicals-only) IDS\u2ff1\u2ff1\u4ea0\u53e3\u5c0f, which means: first assemble \u4ea0 and\u53e3 and then add a squeezed version of \u5c0f underneath.\nWe can formalize that process as follows:\n\u2022 let K be the set of all Unicode Chinese characters, B the set of b\u00f9sh\u01d2u radicals and A a set of auxiliary strokes used in CHISE;\n\u2022 let IDS = {\u2ff0,\u2ff1,\u2ff2,\u2ff3,\u2ff4,\u2ff5,\u2ff6,\u2ff7,\u2ff8,\u2ff9,\u2ffa,\u2ffb} be the twelve IDS operators, defined as follows:\nX : (K \u222a A)2 \u2192 K if X \u2208 {\u2ff0,\u2ff1,\u2ff4,\u2ff5,\u2ff6,\u2ff7,\u2ff8,\u2ff9,\u2ffa,\u2ffb}, X : (K \u222a A)3 \u2192 K if X \u2208 {\u2ff2,\u2ff3.}\nand such that if #(k) is the number of strokes of k \u2208 K and X \u2208 IDS, then #(X(k, k\u2032)) = #(k) + #(k\u2032) (and #(X(k, k\u2032, k\u2032\u2032)) = #(k) + #(k\u2032) + #(k\u2032\u2032))2;\n\u2022 let G be a formal grammar with nonterminals K \\ B, terminals B \u222a A, and production rules of the form\nk \u2192 X(\u03ba, \u03ba\u2032) where X \u2208 {\u2ff0,\u2ff1,\u2ff4,\u2ff5,\u2ff6,\u2ff7,\u2ff8,\u2ff9,\u2ffa,\u2ffb}, or k \u2192 X(\u03ba, \u03ba\u2032, \u03ba\u2032\u2032) where X \u2208 {\u2ff2,\u2ff3.}\nwhere \u03ba, \u03ba\u2032 and \u03ba\u2032\u2032 \u2208 K \u222a A; 2There is an exception to this rule: in some cases a b\u00f9sh\u01d2u radical may change form when combined with other\nradicals or strokes, and its new form may have a different number of strokes than the original.\n\u2022 then every k \u2208 K can be derived into a (possibly nonunique) word in (IDS\u222aB\u222aA)\u2217 (that is: a word consisting only of IDS operators, b\u00f9sh\u01d2u radicals and elements from A. We denote that word by R(k)."}, {"heading": "2.4 Common components and heaviest characters", "text": "If we call the elements c\u2217 of B \u222aA components, we can use an approach similar that described in \u00a72.2 and say that k, k\u2032 \u2208 K have common components c1, c2, . . . , cm \u2208 B \u222a A, whenever c1, c2, . . . , cm \u2208 R(k) \u2229R(k\u2032).\nOur second edge-structure GR on G is the following: two Chinese characters k and k\u2032 are connected by an edge r(k, k\u2032) of weightwR(k, k\u2032) if and only if they contain exactlywR(k, k\u2032) > 0 common components, as defined above. To each edge r corresponds a set of common strokes R(r) = {c1, . . . , cwR(k,k\u2032)}.\nThe weight wR allocates one unit to each common component of k and k\u2032:\nwR(k, k\u2032) = \u2211\nci\u2208R(k)\u2229R(k\u2032)\n1.\nWe generalize this weight in the following fashion:\nwgR(k, k\u2032) = \u2211\nci\u2208R(k)\u2229R(k\u2032)\n\u03bb(ci)\u03bb\u2032(ci) 2\nd(ci) + d\u2032(ci)\nwhere:\n\u2022 \u03bb(c0) > 0 when c0 is the main semantic b\u00f9sh\u01d2u radical of k (as given in the Unihan database), and \u03bb\u2032(c0) > 0) when it is the main semantic radical of k\u2032. For all other components \u03bb(c) = \u03bb\u2032(c) = 1. In this way we can give more importance to the main semantic radical of each character;\n\u2022 d(c) is the depth of c in k (and d\u2032(c) the depth of c in k\u2032), defined as follows: it is the minimum number of productions needed to obtain c from k (resp. from k\u2032). For example, in \u62ad \u2192 \u2ff0\u624c\u2ff1\u5b80\u513f, \u513f is of depth 2, while in \u5725 \u2192 \u2ff1\u571f\u513f it is of depth 1. As the size of radicals is halved (and sometimes even divided by three) whenever an IDS operator is applied, depth corresponds not only to length of the minimal path in the derivation tree, but also to the inverse of size. This refinement of the weight allows us to prioritize large components3.\nIf we take \u03bb \u2261 \u03bb\u2032 \u2261 d \u2261 d\u2032 \u2261 1 then wgR \u2261 wR.\nExperiment 2. Calculate GR and find the heaviest cliques. If the weight of a vertex is the sum of the weights of the edges adjacent to it, find the heaviest vertices.\n3A possible variant of this weight would be to consider not the average of the weights of components in the two characters, but to prioritize cases where the components are of the same size (even if this size is small). In that case, the formula would be:\nwgR(k, k \u2032) = \u2211 ci\u2208R(k)\u2229R(k\u2032) \u03bb(ci)\u03bb \u2032(ci)\n1\n|d(ci) \u2212 d\u2032(ci)| + 1 ."}, {"heading": "2.5 Components vs. Strokes", "text": "Experiment 3. If (GS , wS) is the graph G with edges and weight derived from strokes and (GR, wgR) that derived from components with generalized weight, measure the similarity of the two graphs.\nQuestions 1. Which of the two provides better disambiguation of Chinese characters? If we cluster them, do we obtain the same clusters? Does the additional complexity of GS provide useful information, not available in GR?"}, {"heading": "2.6 Characters, Compounds and Meaning", "text": "While English (and other Western) WordNet provides sets of synonyms (called synsets) for words and collocations, the situation is a bit more complicated for sinographic languages. In [5], Hsieh & Huang introduce HanziNet, an ontological character net, in which they align Chinese characters which \u201cshare a given putatively primitive meaning extracted from traditional philological resources.\u201d They propose a new notion: a conset is a \u201cgroup of Chinese characters similar in concept and each of which shares similar conceptual information with the other characters in the same conset.\u201d\nThe difference between HanziNet and Chinese WordNet is that the former provides only single Chinese characters as conset of a Chinese character, while the latter provides both single characters and compound ones as synset of a given monocharacter or multicharacter word. For example, for the same example character \u4eac, Chinese WordNet supplies the following five senses:\n1. \u4eac1:\u300c\u9996\u90fd\u300d (capital)\n2. \u4eac2:\u300c\u5317\u4eac\u300d,\u300c\u5317\u5e73\u300d,\u300c\u71d5\u4eac\u300d,\u300c\u5e73\u300d (Beijing)\n3. \u4eac3:\u300c\u4eac\u90fd\u300d (Kyoto)\n4. \u4eac4: \u5146\u7684\u5341\u500d (ten trillion)\n5. \u4eac5: (proper noun, name),\nwhile in HanziNet the same character gives: [to be completed once we obtain HanziNet data from Academia Sinica] Our next edge-structure GM on G will be the following: two Chinese characters k and k\u2032 are connected by an edge m(k, k\u2032) if and only if they share a common meaning in Chinese or Japanese WordNet or in the Unihan database, and by an edge H(k, k\u2032) if and only if k is an hyperonym of k\u2032 in one of these resources.\nExperiment 4. Calculate GM and evaluate the similarity between GM , and GS and GR. For how many edges of these two graphs do we have corresponding edges in GM?\nComparing the stroke, radical, and meaning graphs allows us to answer the fundamental question of this article: \u201cIs there a correlation between sharing strokes/radicals and sharing meaning?\u201d\nThe two edge types m, H are to be considered separately: in the first case we have pure synonyms, while in the second case we have a hyperonymy/hyponymy relation. If a stroke or radical edge is attested for the same pair of characters, verify if it goes in the opposite sense (k hyperonymous of k\u2032 \u21d2 #S(k) < #S(k\u2032) and/or #R(k) < #R(k\u2032). These studies are to be conducted separately for Japanese and Chinese.\nOnce the data are loaded in the various graphs, we will apply (large) graph mining methods to obtain relations between strokes, radicals, characters and meaning."}, {"heading": "Acknowledgments", "text": "The author would like to thank: (1) the University of Aizu and in particular Prof. Michael Cohen for inviting him for a three-month stay in his laboratory, and (2) Richard Cook and Tom Bishop from the Wenlin Institute for the tremendous work they have done in describing Chinese characters and for allowing him to use the XML data of CDL in this paper. Without their help this paper would not be possible."}], "references": [{"title": "Wenlin CDL: Character Description Language, Multi- Lingual Computing", "author": ["Bishop", "Tom", "Cook", "Richard"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Chinese character recognition: history, status and prospects", "author": ["Dai", "Ru-Wei", "Liu", "Cheng-Lin", "Xiao", "Bai-Hua"], "venue": "Front. Comput. Sci. China", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Coordinate-independent font description using Kanji as an example", "author": ["Duerst", "Martin"], "venue": "Electronic Publishing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "When Conset meets Synset: A Preliminary Survey of an Ontological Lexical Resource based on Chinese Characters", "author": ["Hsieh", "Shu-Kai", "Huang", "Chu-Ren"], "venue": "Proc. of the COLING/ACL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Development of the Japanese WordNet", "author": ["Isahara", "Hitoshi", "Bond", "Francis", "Uchimoto", "Kiyotaka", "Utiyama", "Masao", "Kanzaki", "Kyoko"], "venue": "Proc. of the Sixth International Language Resources and Evaluation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Statistical Character Structure Modeling and its Application to Handwritten Chinese Character Recognition", "author": ["Kim", "In-Jung", "Jin-Hyung"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "CHISE: Character Processing Based on Character Ontology", "author": ["Morioka", "Tomohiko"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Surface of Essence: Beyond the Coded Character Set Model, in \u66f8\u4f53\u30fb\u7d44\u7248\u30ef\u30fc\u30af\u30b7\u30e7\u30c3\u30d7 \u8cc7\u6599\u96c6", "author": ["Moro", "Shigeki"], "venue": "(Proc. of the Glyph and Typesetting Workshop in Kyo\u0304to\u0304),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "SCML: A Structural Representation for Chinese Characters, Dartmouth College Technical Report TR2007-592", "author": ["Peebles", "Daniel G"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Substructure Shape Analysis for Kanji Character Recognition", "author": ["Rocha", "Jairo", "Fujisawa", "Hiromichi"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Learning to Read Chinese: The Development of Metalinguistic Awareness, in Reading Chinese Script", "author": ["Shu", "Hua", "Anderson", "Richard C"], "venue": "A Cognitive Analysis, Lawrence Erlbaum Associates Publishers,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Submorphemic Processing in Reading Chinese, Journal of Experimental Psychology: Learning, Memory and Cognition", "author": ["Taft", "Marcus", "Zhu", "Xiao-Ping"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}], "referenceMentions": [{"referenceID": 11, "context": "1 Radicals Quoting [14], \u201cthere are actually two different Chinese terms that can be translated into the word radical, making this word potentially confusing.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "The CHISE project [8] provides a decomposition of characters into radicals plus some calligraphic strokes.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "As explained in [13], about 80% of the most frequent characters in Chinese are semanticphonetic compounds.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Character Description Language [1] is a project of theWenlin Institute aiming to graphically describe all Chinese characters through their strokes.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "\u201d Japanese WordNet [6] contains more than 40,000 compound word entries (written as two or more kanji letters).", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "OCR (for example, [12, 7, 2], where the input data is a bitmap image and structure must be extracted from it; 2.", "startOffset": 18, "endOffset": 28}, {"referenceID": 5, "context": "OCR (for example, [12, 7, 2], where the input data is a bitmap image and structure must be extracted from it; 2.", "startOffset": 18, "endOffset": 28}, {"referenceID": 1, "context": "OCR (for example, [12, 7, 2], where the input data is a bitmap image and structure must be extracted from it; 2.", "startOffset": 18, "endOffset": 28}, {"referenceID": 2, "context": "font generation [3, 11], where the input data is some logical and well organized database (containing a description of the character skeleton) and the output is a typographically acceptable Chinese character font.", "startOffset": 16, "endOffset": 23}, {"referenceID": 8, "context": "font generation [3, 11], where the input data is some logical and well organized database (containing a description of the character skeleton) and the output is a typographically acceptable Chinese character font.", "startOffset": 16, "endOffset": 23}, {"referenceID": 6, "context": "The CHISE project [8] has provided IDS descriptions of all Unicode-encoded Chinese characters, segmenting them into b\u00f9sh\u01d2u radicals and 1,683 components (the glyphs of which are taken from various resources, such as GT [20], CDP [16, 17], CNS 11643 [18], Dai Kanwa dictionary [19], and others.", "startOffset": 18, "endOffset": 21}], "year": 2011, "abstractText": "Chinese characters can be compared to a molecular structure: a character is analogous to a molecule, radicals are like atoms, calligraphic strokes correspond to elementary particles, and when characters form compounds, they are like molecular structures. In chemistry the conjunction of all of these structural levels produces what we perceive as matter. In language, the conjunction of strokes, radicals, characters, and compounds produces meaning. But when does meaning arise? We all know that radicals are, in some sense, the basic semantic components of Chinese script, but what about strokes? Considering the fact that many characters are made by adding individual strokes to (combinations of) radicals, we can legitimately ask the question whether strokes carry meaning, or not. In this talk I will present my project of extending traditional NLP techniques to radicals and strokes, aiming to obtain a deeper understanding of the way ideographic languages model the world.", "creator": " XeTeX output 2011.03.01:1359"}}}