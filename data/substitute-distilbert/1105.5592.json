{"id": "1105.5592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Kernel Belief Propagation", "abstract": "we propose a nonparametric generalization of belief propagation, kernel belief propagation ( kbp ), for pairwise markov random fields. messages correctly represented as functions in random reproducing kernel hilbert space ( rkhs ), called message updates are simple linear operations in elementary rkhs. reliability makes none of the assumptions commonly required in generalized bp algorithms : the variables need not arise since perfectly finite median or a gaussian distribution, neither must their relations in exactly particular parametric form. rather, the cycles between variables are represented implicitly, and are created nonparametrically from training data. programming has the advantage that it may be used on any domain where kernels are defined ( rd, strings, groups ), even where explicit parametric models are strictly known, or program form expressions for the bp updates don't exist. the computational cost of message assurance over kbp is estimated in the training data size. we also propose a constant time approximate message update procedure by representing messages using a small number bit basis functions. in experiments, we apply kbp to image denoising, depth evaluation from still images, and protein parameter prediction : consistency is faster than competing classical and nonparametric approaches ( by orders of magnitude, in some formulas ), while seeking significantly more accurate results.", "histories": [["v1", "Fri, 27 May 2011 15:56:11 GMT  (303kb,D)", "http://arxiv.org/abs/1105.5592v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["le song", "arthur gretton", "danny bickson", "yucheng low", "carlos guestrin"], "accepted": false, "id": "1105.5592"}, "pdf": {"name": "1105.5592.pdf", "metadata": {"source": "CRF", "title": "Kernel Belief Propagation", "authors": ["Le Song", "Arthur Gretton", "Danny Bickson", "Yucheng Low", "Carlos Guestrin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Belief propagation is an inference algorithm for graphical models that has been widely and successfully applied in a great variety of domains, including vision (Sudderth et al., 2003), protein folding (Yanover & Weiss, 2002), and turbo decoding (McEliece et al., 1998). In these applications, the variables are usually assumed either to be finite dimensional, or in continuous cases, to have a Gaussian distribution (Weiss & Freeman, 2001). In many applications of graphical models, however, the variables of interest are nat-\nAppearing in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS) 2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR: W&CP 15. Copyright 2011 by the authors.\nurally specified by continuous, non-Gaussian distributions. For example, in constructing depth maps from 2D images, the depth is both continuous valued and has a multimodal distribution. Likewise, in protein folding, angles are modeled as continuous valued random variables, and are predicted from amino acid sequences. In general, multimodalities, skewness, and other non-Gaussian statistical features are present in a great many real-world problems. The corresponding inference procedures for parametric models typically involve integrals for which no closed form solutions exist, and are without computationally tractable exact message updates. Worse still, parametric models for the relations between the variables may not even be known, or may be prohibitively complex.\nOur first contribution in this paper is a novel generalization of belief propagation for pairwise Markov random fields, Kernel BP, based on a reproducing kernel Hilbert space (RKHS) representation of the relations between random variables. This extends earlier work of Song et al. (2010) on inference for trees to the case of graphs with loops. The algorithm consists of two parts, both nonparametric: first, we learn RKHS representations of the relations between variables directly from training data, which removes the need for an explicit parametric model. Second, we propose a belief propagation algorithm for inference based on these learned relations, where each update is a linear operation in the RKHS (although the relations themselves may be highly nonlinear in the original space of the variables). Our approach applies not only to continuous-valued non-Gaussian variables, but also generalizes to strings and graphs (Scho\u0308lkopf et al., 2004), groups (Fukumizu et al., 2009), compact manifolds (Wendland, 2005, Chapter 17), and other domains on which kernels may be defined.\nA number of alternative approaches have been developed to perform inference in the continuous-valued non-Gaussian setting. Sudderth et al. (2003) proposed an approximate belief propagation algorithm for pairwise Markov random fields, where the parametric forms of the node and edge potentials are supplied in advance, and the messages are approximated as mixtures of Gaussians: we refer to this approach as Gaussian Mixture BP (this method was introduced as \u201cnonparametric BP\u201d, but it is in fact a Gaussian mixture approach). Instead of mixtures of Gaussians, Ihler & McAllester (2009) used particles to approximate the messages, resulting in the Particle BP algorithm. Both Gaussian mixture BP and particle BP assume the potentials\nar X\niv :1\n10 5.\n55 92\nv1 [\ncs .L\nG ]\n2 7\nM ay\n2 01\n1\nto be pre-specified by the user: the methods described are purely approximate message update procedures, and do not learn the model from training data. By contrast, kernel BP learns the model, is computationally tractable even before approximations are made, and leads to an entirely different message update formula than the Gaussian Mixture and Particle representations.\nA direct implementation of kernel BP has a reasonable computational cost: each message update costs O(m2dmax) when computed exactly, where m is the number of training examples and dmax is the maximum degree of a node in the graphical model. For massive data sets and numbers of nodes, as occur in image processing, this cost might still be expensive. Our second contribution is a novel constant time approximate message update procedure, where we express the messages in terms of a small number ` m of representative RKHS basis functions learned from training data. Following an initialization cost linear in m, the cost per message update is decreased to O(`2dmax), independent of the number of training points m. Even without these approximate constant time updates, kernel BP is substantially faster than Gaussian mixture BP and particle BP. Indeed, an exact implementation of Gaussian mixture BP would have an exponentially increasing computational and storage cost with number of iterations. In practice, both Gaussian mixture and particle BP require a Monte Carlo resampling procedure at every node of the graphical model.\nOur third contribution is a thorough evaluation of kernel BP against other nonparametric BP approaches. We apply both kernel BP and competing approaches to an image denoising problem, depth prediction from still images, protein configuration prediction, and paper topic inference from citation networks: these are all large-scale problems, with continuous-valued or structured random variables having complex underlying probability distributions. In all cases, kernel BP performs outstandingly, being orders of magnitude faster than both Gaussian mixture BP and particle BP, and returning more accurate results."}, {"heading": "2 Markov Random Fields And Belief Propagation", "text": "We begin with a short introduction to pairwise Markov random fields (MRFs) and the belief propagation algorithm. A pairwise Markov random field (MRF) is defined on an undirected graph G := (V, E) with nodes V := {1, . . . , n} connected by edges in E . Each node s \u2208 V is associated with a random variable Xs on the domainX (we assume a common domain for ease of notation, but in practice the domains can be different), and \u0393s := {t|(s, t) \u2208 E} is the set of neighbors of node s with size ds := |\u0393s|. In a pairwise MRF, the joint distribution of the variables X := {X1, . . . , X|V|} is assumed to factorize according to a model P(X) = 1Z \u220f (s,t)\u2208E \u03a8st(Xs, Xt) \u220f s\u2208V \u03a8s(Xs),\nwhere \u03a8s(Xs) and \u03a8st(Xs, Xt) are node and edge potentials respectively, and Z is the partition function that normalizes the distribution.\nThe inference problem in an MRF is defined as calculating the marginals P(Xs) for nodes s \u2208 V and P(Xs, Xt) for edges (s, t) \u2208 E . The marginal P(Xs) not only provides a measure of uncertainty of Xs, but also leads to a point estimate x?s := argmaxP(Xs). Belief Propagation (BP) is an iterative algorithm for performing inference in MRFs (Pearl, 1988). BP represents intermediate results of marginalization steps as messages passed between adjacent nodes: a message mts from t to s is calculated based on messages mut from all neighboring nodes u of t besides s, i.e.,\nmts(Xs) = \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f u\\s mut(Xt)dXt. (1)\nNote that we use \u220f u\\s to denote \u220f\nu\u2208\u0393t\\s, where it is understood that the indices range over all neighbors u of t except s. This notation also applies to operations other than the product. The update in (1) is iterated across all nodes until a fixed point,m?ts, for all messages is reached. The resulting node beliefs (estimates of node marginals) are given by B(Xs) \u221d \u03a8s(Xs) \u220f t\u2208\u0393s m ? ts(Xs).\nFor acyclic or tree-structured graphs, BP results in node beliefs B(Xs) that converge to the node marginals P(Xs). This is generally not true for graphs with cycles. In many applications, however, the resulting loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999). Several theoretical studies have also provided insight into the approximations made by loopy BP, partially justifying its application to graphs with cycles (Wainwright & Jordan, 2008; Yedidia et al., 2001).\nThe learning problem in MRFs is to estimate the node and edge potentials, which is often done by maximizing the expected log-likelihood EX\u223cP?(X)[logP(X)] of the model P(X) with respect to the true distribution P?(X). The resulting optimization problem usually requires solving a sequence of inference problems as an inner loop (Koller & Friedman, 2009); BP is often deployed for this purpose."}, {"heading": "3 Properties of Belief Propagation", "text": "Our goal is to develop a nonparametric belief propagation algorithm, where the potentials are nonparametric functions learned from data, such that multimodal and other non-Gaussian statistical features can be captured. Most crucially, these potentials must be represented in such a way that the message update in (1) is computationally tractable. Before we go into the details of our kernel BP algorithm, we will first explain a key property of BP, which relates message updates to conditional expectations. When the messages are RKHS functions, these expectations can be evaluated efficiently.\nYedidia et al. (2001) showed BP to be an iterative algorithm for minimizing the Bethe free energy, which is a variational approximation to the log-partition function, logZ, in the MRF model P(X). The beliefs are fixed points of BP algorithm if and only if they are zero gradient points of the Bethe free energy. In Section 5 of the Appendix, we show maximum likelihood learning of MRFs using BP results in the following equality, which relates the conditional of the true distribution, the learned potentials, and the fixed point messages,\nP?(Xt|Xs) = \u03a8st(Xs, Xt)\u03a8t(Xt)\n\u220f u\\sm ? ut(Xt)\nm?ts(Xs) , (2)\nwhere P?(Xs) and m?ts(Xs) are assumed strictly positive. Wainwright et al. (2003, Section 4) derived a similar relation, but for discrete variables under the exponential family setting. By contrast, we do not assume an exponential family model, and our reasoning applies to continuous variables. A further distinction is that Wainwright et al. specify the node potential \u03a8s(Xs) = P?(Xs) and edge potential \u03a8(Xs, Xt) = P?(Xs, Xt)P?(Xs)\u22121P?(Xt)\u22121, which represent just one possible choice among many that satisfies (2). Indeed, we next show that in order to run BP for subsequent inference, we do not need to commit to a particular choice for \u03a8s(Xs) and \u03a8(Xs, Xt), nor do we need to optimize to learn \u03a8s(Xs) and \u03a8(Xs, Xt).\nWe start by dividing both sides of (1) by m?ts(Xs), and introducing 1 = \u220f u\\s m?ut(Xt) m?ut(Xt) ,\nmts(Xs) m?ts(Xs) = \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f u\\sm ? ut(Xt) m?ts(Xs)\n\u00d7 \u220f\nu\\s\nmut(Xt) m?ut(Xt) dXt. (3)\nWe next substitute the BP fixed point relation (2) into (3), and reparametrize the messages mts(Xs) \u2190 mts(Xs)m?ts(Xs) , to obtain the following property for BP updates (see Section 6 in the Appendix for details):\nProperty 1 If we learn an MRF using BP and subsequently use the learned potentials for inference, BP updates can be viewed as conditional expectations,\nmts(Xs) = \u222b X P?(Xt|Xs) \u220f u\\s mut(Xt) dXt\n= EXt|Xs [\u220f u\\s mut(Xt) ] . (4)\nUsing similar reasoning, the node beliefs on convergence of BP take the form B(Xs) \u221d P?(Xs) \u220f t\u2208\u0393s m ? ts(Xs). In the absence of external evidence, a fixed point occurs at the true node marginals, i.e., B(Xs) \u221d P?(Xs) for all s \u2208 V . Typically there can be many evidence variables, and the belief is then an estimate of the true conditional distribution given the evidence.\nThe above property of BP immediately suggests that if belief propagation is the inference algorithm of choice, then MRFs can be learned very simply: given training data drawn from P?(X), the empirical conditionals P\u0302(Xt|Xs)\nare estimated (either in parametric form, or nonparametrically), and the conditional expectations are evaluated using these estimates. Evidence can also be incorporated straightforwardly: if an observation xt is made at node t, the message from t to its neighbor s is simply the empirical likelihood function mts(Xs) \u221d P\u0302(xt|Xs), where we use lowercase to denote observed variables with fixed values, and capitalize unobserved random variables.\nWith respect to kernel belief propagation, our key insight from Property 1, however, is that we need not explicitly recover the empirical conditionals P\u0302(Xt|Xs) as an intermediate step, as long as we can compute the conditional expectation directly. We will pursue this approach next."}, {"heading": "4 Kernel Belief Propagation", "text": "We develop a novel kernelization of belief propagation, based on Hilbert space embeddings of conditional distributions (Song et al., 2009), which generalizes an earlier kernel algorithm for exact inference on trees (Song et al., 2010). As might be expected, the kernel implementation of the BP updates in (4) is nearly identical to the earlier tree algorithm, the main difference being that we now consider graphs with loops, and iterate until convergence (rather than obtaining an exact solution in a single pass). This difference turns out to have major implications for the implementation: the earlier solution of Song et al. is polynomial in the sample size, which was not an issue for the the smaller trees considered by Song et al., but becomes expensive for the large, loopy graphical models we address in our experiments. We defer the issue of efficient implementation to Section 5, where we present a novel approximation strategy for kernel BP which achieves constant time message updates.\nIn the present section, we will provide a detailed derivation of kernel BP in accordance with Song et al. (2010). While the immediate purpose is to make the paper selfcontained, there are two further important reasons: to provide the background necessary in understanding our efficient kernel BP updates in Section 5, and to demonstrate how kernel BP differs from the competing Gaussian mixture and particle based BP approaches in Section 6 (which was not addressed in earlier work on kernel tree graphical models)."}, {"heading": "4.1 Message Representations", "text": "We begin with a description of the properties of a message mut(xt), given it is in the reproducing kernel Hilbert space (RKHS) F of functions on the separable metric space X (Aronszajn, 1950; Scho\u0308lkopf & Smola, 2002). As we will see, the advantage of this assumption is that the update procedure can be expressed as a linear operation in the RKHS, and results in new messages that are likewise RKHS functions. The RKHS F is defined in terms of a unique positive definite kernel k(xs, x\u2032s) with the reproducing property \u3008mts(\u00b7), k(xs, \u00b7)\u3009F = mts(xs), where k(xs, \u00b7) indi-\ncates that one argument of the kernel is fixed at xs. Thus, we can view the evaluation of message mts at any point xs \u2208 X as a linear operation in F : we call k(xs, \u00b7) the representer of evaluation at xs, and use the shorthand k(xs, \u00b7) = \u03c6(xs). Note that k(xs, x\u2032s) = \u3008\u03c6(xs), \u03c6(x\u2032s)\u3009F ; the kernel encodes the degree of similarity between xs and x\u2032s. The restriction of messages to RKHS functions need not be onerous: on compact domains, universal kernels (in the sense of Steinwart, 2001) are dense in the space of bounded continuous functions (e.g., the Gaussian RBF kernel k(xs, x\u2032s) = exp(\u2212\u03c3 \u2016xs \u2212 x\u2032s\u2016 2 ) is universal). Kernels may be defined when dealing with random variables on additional domains, such as strings, graphs, and groups."}, {"heading": "4.2 Kernel BP Message Updates", "text": "We next define a representation for message updates, under the assumption that messages are RKHS functions. For simplicity, we first establish a result for a three node chain, where the middle node t incorporates an incoming message from u, and then generates an outgoing message to s (we will deal with multiple incoming messages later). In this case, the outgoing message mts(xs) evaluated at xs simplifies to mts(xs) = EXt|xs [mut(Xt)]. Under some regularity conditions for the integral, we can rewrite message updates as inner products, mts(xs) = EXt|xs [\u3008mut, \u03c6(Xt)\u3009F ] = \u2329 mut,EXt|xs [\u03c6(Xt)] \u232a F using the reproducing property of the RKHS. We refer to \u00b5Xt|xs := EXt|xs [\u03c6(Xt)] \u2208 F as the feature space embedding of the conditional distribution P(Xt|xs). If we can estimate this quantity directly from data, we can perform message updates via a simple inner product, avoiding a two-step procedure where the conditional distribution is first estimated and the expectation then taken.\nAn expression for the conditional distribution embedding was proposed by Song et al. (2009). We describe this expression by analogy with the conditioning operation for a Gaussian random vector z \u223c N (0, C), where we partition z = (z>1 , z > 2 ) > such that z1 \u2208 Rd and z2 \u2208 Rd \u2032 . Given the covariances C11 := E[z1z>1 ] and C12 := E[z1z>2 ], we can write the conditional expectation E[Z1|z2] = C12C\u2212122 z2. We now generalize this notion to RKHSs. Following Fukumizu et al. (2004), we define the covariance operator CXsXt which allows us to compute the expectation of the product of function f(Xs) and g(Xt), i.e. EXsXt [f(Xs)g(Xt)], using linear operation in the RKHS. More formally, let CXsXt : F 7\u2192 F such that for all f, g, h \u2208 F , EXsXt [f(Xs)g(Xt)] = \u3008f, EXsXt [\u03c6(Xs)\u2297 \u03c6(Xt)] g\u3009F = \u3008f, CXsXtg\u3009F (5) where we use tensor notation (f \u2297 g)h = f \u3008g, h\u3009F . This can be understood by analogy with the finite dimensional case: if x, y, z \u2208 Rd, then (x y>)z = x(y>z); furthermore, (x>x\u2032)(y>y\u2032)(z>z\u2032) = \u3008x\u2297 y \u2297 z, x\u2032 \u2297 y\u2032 \u2297 z\u2032\u3009Rd3 given x, y, z, x\u2032, y\u2032, z\u2032 \u2208 Rd. Based on covariance operators, Song et al. define a conditional embedding operator which allow us to compute conditional expecta-\ntions EXt|xs [f(Xt)] as linear operations in the RKHS. Let UXt|Xs := CXtXsC \u22121 XsXs\nsuch that for all f \u2208 F , EXt|xs [f(Xt)] = \u2329 f, EXt|xs [\u03c6(Xt) \u232a F = \u2329 f, \u00b5Xt|xs \u232a F\n= \u2329 f, UXt|Xs\u03c6(xs) \u232a F . (6)\nAlthough we used the intuition from the Gaussian case in understanding this formula, it is important to note that the conditional embedding operator allows us to compute the conditional expectation of any f \u2208 F , regardless of the distribution of the random variable in feature space (aside from the condition that h(xs) := EXt|xs [f(Xt)] is in the RKHS on xs, as noted by Song et al.). In particular, we do not need to assume the random variables have a Gaussian distribution in feature space (the definition of feature space Gaussian BP remains a challenging open problem: see Appendix, Section 7).\nWe can thus express the message update as a linear operation in the feature space,\nmts(xs) = \u2329 mut, UXt|Xs\u03c6(xs) \u232a F .\nFor multiple incoming messages, the message updates follow the same reasoning as in the single message case, albeit with some additional notational complexity (see also Song et al., 2010). We begin by defining a tensor product reproducing kernel Hilbert space H := \u2297dt\u22121F , under which the product of incoming messages can be written as a single inner product. For a node t with degree dt = |\u0393t|, the product of incoming messages mut from all neighbors except s becomes an inner product inH,\u220f\nu\\s mut(Xt) = \u220f u\\s \u3008mut, \u03c6(Xt)\u3009F\n= \u2329\u2297 u\\s mut, \u03be(Xt) \u232a H , (7)\nwhere \u03be(Xt) := \u2297\nu\\s \u03c6(Xt). The message update (4) becomes\nmts(xs) = \u2329\u2297 u\\s mut, EXt|xs [\u03be(Xt)] \u232a H . (8) By analogy with (6), we can define the conditional embedding operator for the tensor product of features, such that UX\u2297t |Xs : F \u2192 F \u2297 satisfies\n\u00b5X\u2297t |xs := EXt|xs [\u03be(Xt)] = UX\u2297t |xs\u03c6(xs). (9)\nAs in the single variable case, UX\u2297t |xs is defined in terms of a covariance operator CX\u2297t Xs := EXtXs [\u03be(Xt) \u2297 \u03c6(Xs)] in the tensor space, and the operator CXsXs . The operator UX\u2297t |Xs takes the feature map \u03c6(xs) of the point on which we condition, and outputs the conditional expectation of the tensor product feature \u03be(Xt). Consequently, we can express the message update as a linear operation, but in a tensor product feature space,\nmts(xs) = \u2329\u2297 u\\s mut, UX\u2297t |Xs\u03c6(xs) \u232a H . (10)\nThe belief at a specific node s can be computed as B(Xs) = P?(Xs) \u220f u\u2208\u0393s mus(Xs) where the true marginal P\n?(Xr) can be estimated using Parzen windows. If this is undesirable (for instance, on domains where density estimation\ncannot be performed), the belief can instead be expressed as a conditional embedding operator (Song et al., 2010)."}, {"heading": "4.3 Learning Kernel Graphical Models", "text": "Given a training sample of m pairs { (xit, x i s) }m i=1 drawn i.i.d. from P?(Xt, Xs), we can represents messages and their updates based purely on these training examples. We first define feature matrices \u03a6 = (\u03c6(x1t ), . . . , \u03c6(x m t )), \u03a5 = (\u03c6(x1s), . . . , \u03c6(x m s )) and \u03a6 \u2297 = ( \u03be(x1t ), . . . , \u03be(x m t ) ) , and corresponding kernel matrices K = \u03a6>\u03a6 and L = \u03a5>\u03a5. The assumption that messages are RKHS functions means that messages can be represented as linear combinations of the training features \u03a6, i.e., m\u0302ut = \u03a6\u03b2ut, where \u03b2ut \u2208 Rm. On this basis, Song et al. (2009) propose a direct regularized estimate of the conditional embedding operators from the data. This approach avoids explicit conditional density estimation, and directly provides the tools needed for computing the RKHS message updates in (10). Following this approach, we first estimate the covariance operators C\u0302XtXs = 1m\u03a6\u03a5 >, C\u0302X\u2297t Xs = 1 m\u03a6 \u2297\u03a5> and C\u0302XsXs = 1m\u03a5\u03a5 >, and obtain an empirical estimate of the conditional embedding operator, U\u0302X\u2297t |Xs = \u03a6\n\u2297(L> + \u03bbmI)\u22121\u03a5>, (11) where \u03bb is a regularization parameter. Note that we need not compute the feature space covariance operators explicitly: as we will see, all steps in kernel BP are carried out via operations on kernel matrices.\nWe now apply the empirical conditional embedding operator to obtain a finite sample message update for (10). Since the incoming messages m\u0302ut can be expressed as m\u0302ut = \u03a6\u03b2ut, the outgoing message m\u0302ts at xs is\u2329\u2297\nu\\s \u03a6\u03b2ut, \u03a6\n\u2297(L+ \u03bbmI)\u22121\u03a5>\u03c6(xs) \u232a H\n= (\u2299 u\\s K\u03b2ut )> (L+ \u03bbmI)\u22121\u03a5>\u03c6(xs) (12)\nwhere \u2299\nis the elementwise vector product. If we define \u03b2ts = (L+\u03bbmI) \u22121( \u2299\nu\\sK\u03b2ut), then the outgoing message can be expressed as m\u0302ts = \u03a5\u03b2ts. In other words, given incoming messages expressed as linear combinations of feature mapped training samples from Xt, the outgoing message will likewise be a weighted linear combination of feature mapped training samples from Xs. Importantly, only m mapped points will be used to express the outgoing message, regardless of the number of incoming messages or the number of points used to express each incoming message. Thus the complexity of message representation does not increase with BP iterations or degree of a node.\nAlthough we have identified the model parameters with specific edges (s, t), our approach extends straightforwardly to a templatized model, where parameters are shared across multiple edges (this setting is often natural in image processing, for instance). Empirical estimates of the parameters are computed on the pooled observations.\nThe computational complexity of the finite sample BP update in (12) is polynomial in term of the number of training samples. Assuming a preprocessing step of cost O(m3) to compute the matrix inverses, the update for a single message costsO(m2dmax) where dmax is the maximum degree of a node in the MRF. While this is reasonable in comparison with competing nonparametric approaches (see Section 6 and the experiments), and works well for smaller graphs and trees, a polynomial time update can be costly for very large m, and for graphical models with loops (where many iterations of the message updates are needed). In Section 5, we develop a message approximation strategy which reduces this cost substantially."}, {"heading": "5 Constant Time Message Updates", "text": "In this section, we formulate a more computationally efficient alternative to the full rank update in (12). Our goal is to limit the computational cost of each update to O(`2dmax) where ` m. We will require a one-off preprocessing step which is linear inm. This efficient message passing procedure makes kernel BP practical even for very large graphical models and/or training set sizes."}, {"heading": "5.1 Approximating Feature Matrices", "text": "The key idea of the preprocessing step is to approximate messages in the RKHS with a few informative basis functions, and to estimate these basis functions in a data dependent way. This is achieved by approximating the feature matrix \u03a6 as a weighted combination of a subset of its columns. That is, \u03a6 \u2248 \u03a6IWt, where I := {i1, . . . , i`} \u2286 {1, . . . ,m}, Wt has dimension ` \u00d7m, and \u03a6I = (\u03c6(x i1 t ), . . . , \u03c6(x i` t )) is a submatrix formed by taking the columns of \u03a6 corresponding to the indices in I. Likewise, we approximate \u03a5 \u2248 \u03a5JWs, assuming |J | = ` for simplicity. We thus can approximate the kernel matrices as low rank factorizations, i.e., K \u2248 W>t KIIWt and L = W>s LJJWs, where KII := \u03a6 > I \u03a6I and LJJ = \u03a5 > J\u03a5J .\nA common way to obtain the approximation \u03a6 \u2248 \u03a6IWt is via a Gram-Schmidt orthogonalization procedure in feature space, where an incomplete set of ` orthonormal basis vectors Q := (q1t , . . . , q ` t ) is constructed from a greedily selected subset of the data, chosen to minimize the reconstruction error (Shawe-Taylor & Cristianini, 2004, p.126). The original feature matrix can be approximately expressed using this basis subset as \u03a6 \u2248 QR where R \u2208 R`\u00d7m are the coefficients under the new basis. There is a simple relation between Q and the chosen data points \u03a6I , i.e., Q = \u03a6IR\u22121I , where RI is the submatrix formed by taking the columns of R corresponding to I. It follows that Wt = R\u22121I R. All operations involved in GramSchmidt orthogonalization are linear in feature space, and the entries of R can be computed based solely on kernel values k(xt, x\u2032t). The cost of performing this orthogonalization is O(m`2). The number ` of chosen basis vectors is inversely related to the approximation error or residual\n= maxi \u2016\u03c6(xit) \u2212 \u03a6IW it \u2016F (W it denotes column i of Wt). In many cases of interest (for instance, when a Gaussian RBF kernel is used), a small ` m will be sufficient to obtain a small residual for the feature matrix, due to the fast decay of the eigenspectrum in feature space (Bach & Jordan, 2002, Appendix C)."}, {"heading": "5.2 Approximating Tensor Features", "text": "The approximations \u03a6 \u2248 \u03a6IWt and \u03a5 \u2248 \u03a5JWs, and associated low rank kernel approximations are insufficient for a constant time approximate algorithm, however. In fact, directly applying these results will only lead to a linear time approximate algorithm: this can be seen by replacing the kernel matrices in (12) by their low rank approximations.\nTo achieve a constant approximate update, our strategy is to go a step further: in addition to approximating the kernel matrices, we further approximate the tensor product feature matrix in equation (11), \u03a6\u2297 \u2248 \u03a6\u2297I\u2032W \u2297 t (W \u2297 t \u2208 R`\n\u2032\u00d7m). Crucially, the individual kernel matrix approximations neglect to account for the subsequent tensor product of these messages. By contrast, our proposed approach also approximates the tensor product directly. The computational advantage of a direct tensor approximation approach is substantial in practice (a comparison between exact kernel BP and its constant and linear time approximations can be found in Section 3 of the Appendix) .\nThe decomposition procedure for tensor \u03a6\u2297 \u2248 \u03a6\u2297I\u2032W \u2297 t follows exactly the same steps as in the original feature space, but using the kernel kdt\u22121(xt, x\u2032t), and yielding an incomplete orthonormal basis in the tensor product space. In general the index sets I \u2032 6= I, meaning they select different training points to construct the basis functions. Furthermore, the size `\u2032 of I \u2032 is not equal to the size ` of I for a given approximation error . Typically `\u2032 > `, since the tensor product space has a slower decaying spectrum, however we will write ` in place of `\u2032 to simplify notation."}, {"heading": "5.3 Constant Time Approximate Updates", "text": "We now compute the message updates based on the various low rank approximations. The incoming messages and the conditional embedding operators become\u2297\nu\\s mut \u2248 \u2297 u\\s \u03a6IWt\u03b2ut, (13)\nU\u0303X\u2297t |Xs\u03c6(xs) \u2248 \u03a6 \u2297 I\u2032Wts\u03a5 > J \u03c6(xs), (14)\nwhere Wts := W\u2297t (W > s LJJWs + \u03bbmI) \u22121W>s . If we reparametrize the messages mut as mut = \u03a6I\u03b1ut where \u03b1ut := Wt\u03b2ut, we can express the message updates for mts(xs) as\nmts(xs) \u2248 (\u2299\nu\\s KI\u2032I\u03b1ut\n)> Wst\u03a5 > J \u03c6(xs), (15)\nwhere KI\u2032I denotes the submatrix of K with rows indexed I \u2032 and columns indexed I. The outgoing message mts can also be reparametrized as a vector \u03b1ts = W>st (\u2299 u\\sKI\u2032I\u03b1ut ) . In short, the message from t to\ns is a weighted linear combination of the ` vectors in \u03a5J .\nWe note that Wts can be computed efficiently prior to the message update step, since W\u2297t (W > s LJJWs + \u03bbmI)\u22121W>s = W \u2297 t W > s (WsW > s + \u03bbmL \u22121 JJ )\n\u22121L\u22121JJ via the Woodbury expansion of the matrix inverse. In the latter form, matrix products WsW>s and W \u2297 t W > s cost O(`\n2m); the remaining operations (size ` matrix products and inversions) are significantly less costly at O(`3). This initialization cost of O(`3 + `2m) need only be borne once.\nThe cost of updating a single message mts in (15) becomes O(`2dmax) where dmax is the maximum degree of a node. This also means that our approximate message update scheme will be independent of the number of training examples. With these approximate messages, the evaluation of the belief B\u0302(xr) of a node r at xr can be carried out in time O(`dmax).\nFinally, approximating the tensor features introduces additional error into each message update. This is caused by the difference between the full rank conditional embedding operator U\u0302X\u2297t |Xs in (11) and its low rank counterpart U\u0303X\u2297t |Xs in (14). Under suitable conditions, this difference is bounded by the feature approximation error , i.e., \u2016U\u0302X\u2297t |Xs \u2212 U\u0303X\u2297t |Xs\u2016HS \u2264 2 (\u03bb\n\u22121 + \u03bb\u22123/2) (see Section 8 of the Appendix for details)."}, {"heading": "6 Gaussian Mixture And Particle BP", "text": "We briefly review two state-of-the-art approaches to nonparametric belief propagation: Gaussian Mixture BP (Sudderth et al., 2003) and Particle BP (Ihler & McAllester, 2009). By contrast with our approach, we must provide these algorithms in advance with an estimate of the conditional density P?(Xt|Xs), to compute the conditional expectation in (4). For Gaussian Mixture BP, this conditional density must take the form of a mixture of Gaussians. We describe how we learn the conditional density from data, and then show how the two algorithms use it for inference.\nA direct approach to estimating the conditional density P?(Xt|Xs) would be to take the ratio of the joint empirical density to the marginal empirical density. The ratio of mixtures of Gaussians is not itself a mixture of Gaussians, however, so this approach is not suitable for Gaussian Mixture BP (indeed, message updates using this ratio of mixtures would be non-trivial, and we are not aware of any such inference approach). We propose instead to learn P?(Xt|Xs) directly from training data following Sugiyama et al. (2010), who provide an estimate in the form of a mixture of Gaussians (see Section 1 of the Appendix for details). We emphasize that the updates bear no resemblance to our kernel updates in (12), which do not attempt density ratio estimation.\nGiven the estimated P\u0302(Xt|Xs) as input, each nonparametric inference method takes a different approach. Gaussian mixture BP assumes incoming messages to be a mixture of\nb Gaussians. The product of dt incoming messages to node t then contains bdt Gaussians. This exponential blow-up is avoided by replacing the exact update with an approximation. An overview of approximation approaches can be found in Bickson et al. (2011); we used an efficient KD-tree method of Ihler et al. (2003) for performing the approximation step. Particle BP represents the incoming messages using a common set of particles.These particles must be re-drawn via Metropolis-Hastings at each node and BP iteration, which is costly (although in practice, it is sufficient to resample periodically, rather than strictly at every iteration). By contrast, our updates are simply matrix-vector products. See Appendix for further discussion."}, {"heading": "7 Experiments", "text": "We performed four sets of experiments. The first two were image denoising and depth prediction problems, where we show that kernel BP is superior to discrete, Gaussian mixture and particle BP in both speed and accuracy, using a GraphLab implementation of each (Low et al., 2010). The remaining two experiments were protein structure and paper category prediction problems, where domain-specific kernels were crucial (for the latter see Appendix, Sec. 4).\nImage denoising: In our first experiment, the data consisted of grayscale images of size 100\u00d7 100, resembling a sunset (Figure 1(a)). The number of colors (gray levels) in the images ranged across 10, 25, 50, 75, 100, 125, 150, 175, 200, 225 and 250, with gray levels varying evenly from 0 to 255 from the innermost ring of the sunset to the outermost. As we increased the number of colors, the grayscale transition became increasingly smooth. Our goal was to recover the original images from noisy versions, to which we had added zero mean Gaussian noise with \u03c3 = 30. We compared the denoising performance and runtimes of discrete, Gaussian mixture, particle, and kernel BP.\nThe topology of our graphical model was a grid of hidden noise-free pixels with noisy observations made at each. The maximum degree of a node was 5 (four neighbours and an observation), and we used a template model where\nboth the edge potentials and the likelihood functions were shared across all variables. We generated a pair of noisefree and noisy images as training data, at each color number. For kernel BP, we learned both the likelihood function and the embedding operators nonparametrically from the data. We used a Gaussian RBF kernel k(x, x\u2032), with kernel bandwidth set at the median distance between training points, and residual = 10\u22123 as the stopping criterion for the feature approximation (see definition of in Section 5.1). For discrete, Gaussian mixture, and particle BP, we learned the edge potentials from data, but supplied the true likelihood of the observation given the hidden pixel (i.e., a Gaussian with standard deviation 30). This gave competing methods an important a priori advantage over kernel BP: in spite of this, kernel BP still outperformed competing approaches in speed and accuracy.\nIn Figure 1(c) and (d), we report the average denoising performance (RMSE: root mean square error) and runtime over 30 BP iterations, using 10 independently generated noisy test images. The RMSE of kernel BP is significantly lower than Gaussian mixture and particle BP for all numbers of colors. Although the RMSE of discrete BP is about the same as kernel BP when the number of colors is small, its performance becomes worse than kernel BP as the number of colors increases beyond 100 (despite discrete BP receiving the true observation likelihood in advance). In terms of speed, kernel BP has a considerable advantage over the alternatives: the runtime of KBP is barely affected by the number of colors. For discrete BP, the scaling is approximately square in the number of colors. For Gaussian mixture and particle BP, the runtimes are orders of magnitude longer than kernel BP, and are affected by the variability of the resampling algorithm.\nPredicting depth from 2D images: The prediction of 3D depth information from 2D image features is a difficult inference problem, as the depth may be ambiguous: similar features can occur at different depths. This creates a multimodal depth distribution given the image feature. Furthermore, the marginal distribution of the depth can itself be multimodal, which makes the Gaussian approximation a poor choice (see Figure 2(b)). To make a spatially consistent prediction of the depth map, we formulated the problem as an undirected graphical model, where a depth variable yi \u2208 R was associated with each patch of an image, and these variables were connected according to a 2D grid topology. Each hidden depth variable was linked to an image feature variable xi \u2208 R273 for the corresponding patch. This formulation resulted in a graphical model with 9, 202 = 107 \u00d7 86 continuous depth variables, and a maximum node degree of 5. Due to the way the images were taken (upright), we used a templatized model where horizontal edges in a row shared the same potential, vertical edges at the same height shared the same potential, and patches at the same row shared the same likelihood\nfunction. Both the edge potentials between adjacent depth variables and the likelihood function between image feature and depth were unknown, and were learned from data.\nWe used a set of 274 images taken on the Stanford campus, including both indoor and outdoor scenes (Saxena et al., 2009). Images were divided into patches of size 107 by 86, with the corresponding depth map for each patch obtained using 3D laser scanners (e.g., Figure 2(a)). Each patch was represented by a 273 dimensional feature vector, which contained both local features (such as color and texture) and relative features (features from adjacent patches). We took the logarithm of the depth map and performed learning and prediction in this space. The entire dataset contained more than 2 million data points (107 \u00d7 86 \u00d7 274). We applied a Gaussian RBF kernel on the depth information, with the bandwidth parameter set to the median distance between training depths, and an approximation residual of = 10\u22123. We used a linear kernel for the image features.\nOur results were obtained by leave-one-out cross validation. For each test image, we ran discrete, Gaussian mixture, particle, and kernel BP for 10 BP iterations. The average prediction error (MAE: mean absolute error) and runtime are shown in Figures 2(c) and (d). Kernel BP produces the lowest error (MAE=0.145) by a significant margin, while having a similar runtime to discrete BP. Gaussian mixture and particle BP achieve better MAE than discrete BP, but their runtimes are two order of magnitude slower. We note that the error of kernel BP is slightly better than the\nresults of pointwise MRF reported in Saxena et al. (2009).\nProtein structure prediction: Our final experiment investigates the protein folding problem. The folded configuration of a protein of length n is roughly determined by a sequence of angle pairs {(\u03b8i, \u03c9i)}ni=1, each specific to an amino acid position. The goal is to predict the sequence of angle pairs given only the amino acid sequence. The two angles (\u03b8i, \u03c9i) have ranges [0, 180] and (\u2212180, 180] respectively, such that they correspond to points on the unit sphere S2. Kernels yield an immediate solution to inference on these data: Wendland (2005, Theorem 17.10) provides a sufficient condition for a function on S2 to be positive definite, satisfied by k(x, x\u2032) := exp(\u03c3 \u3008x, x\u2032\u3009), where \u3008x, x\u2032\u3009 is the standard inner product between Euclidean coordinates. Given the data are continuous, multimodal, and on a non-Euclidean domain (Figure 3(a)), it is not obvious how Gaussian mixture or discrete BP might be applied. We therefore focus on comparing kernel and particle BP.\nWe obtained a collection of 1, 400 proteins with length larger than 100 from PDB. We first ran PSI-BLAST to generate the sequence profile (a 20 dimensional feature for each amino acid position), and then used this profile as features for predicting the folding structure (Jones, 1999). The graphical model was a chain of connected angle pairs, where each angle pair was associated with a 20 dimensional feature. We used a linear kernel on the sequence features. For the kernel between angles, the bandwidth parameter was set at the median inner product between training points, and we used the approximation residual = 10\u22123. For particle BP, we learned the nonparametric potentials using exp(\u03c3 \u3008x, x\u2032\u3009) as the basis functions.\nIn Figure 3(b), we report the average prediction accuracy (Mean Cosine Similarity between the true coordinate x and the predicted x\u2032, i.e., \u3008x, x\u2032\u3009) over a 10-fold crossvalidation process. In this case, kernel BP achieves a significantly better result than particle BP while running much faster (runtimes not shown due to space constraints)."}, {"heading": "8 Conclusions and Further Work", "text": "We have introduced kernel belief propagation, where the messages are functions in an RKHS. Kernel BP performs learning and inference on challenging graphical models with structured and continuous random variables, and is more accurate and much faster than earlier nonparametric BP algorithms. A possible extension to this work would be to kernelize tree-reweighted belief propagation (Wainwright et al., 2003). The convergence of kernel BP is a further challenging topic for future work (Ihler et al., 2005).\nAcknowledgements: We thank Alex Ihler for the Gaussian mixture BP codes and helpful discussions. LS is supported by a Stephenie and Ray Lane Fellowship. This research was also supported by ARO MURI W911NF0710287, ARO MURI W911NF0810242, NSF Mundo IIS-0803333, NSF Nets-NBD CNS-0721591 and ONR MURI N000140710747.\nSupplementary to Kernel Belief Propagation\nSection 1 contains a review of Gaussian mixture BP and particle BP, as well as a detailed explanation of our strategy for learning edge potentials for these approaches from training data. Section 2 provides parameter settings and experiment details for particle BP and discrete BP, in the synthetic image denoising and depth reconstruction experiments. Section 3 contains a comparison of two different approximate feature sets: low rank approximation of the tensor features and low rank approximation of the individual features alone. Section 4 is an experiment on learning paper categories using citation networks. Sections 5 and 6 demonstrate the optimization objective of locally consistent BP updates, and provide a derivation of these updates in terms of the conditional expectation. Section 7 discusses the kernelization of Gaussian BP. Section 8 gives the error introduced by low rank approximation of the messages."}, {"heading": "1 Gaussian Mixture and Particle BP", "text": "We describe two competing approaches for nonparametric belief propagation: Gaussian mixture BP, originally known as non-parametric BP (Sudderth et al., 2003), and particle BP (Ihler & McAllester, 2009). For these algorithms, the edge potentials \u03a8(xs, xt), self-potentials \u03c8(xt), and evidence potentials \u03a8(xt, yt) must be provided in advance by the user. Thus, we begin by describing how the edge potentials in Section 2 of the main document may be learned from training data, but in a form applicable to these inference algorithms: we express P(xt|xs) as a mixture of Gaussians. We then describe the inference algorithms themselves.\nIn learning the edge potentials, we turn to Sugiyama et al. (2010), who provide a least-squares estimate of a conditional density in the form of a mixture of Gaussians,\nP(v|u) = b\u2211\ni=1\n\u03b1i\u03bai(u, v) = \u03b1 >\u03bau,v,\nwhere \u03bai(u, v) is a Gaussian with diagonal covariance centred at1 (qi, ri). Given a training set {(uj , vj)}mj=1, we obtain the coefficients\n\u03b1 := [( H\u0302 + \u03bbI )\u22121 h\u0302 ] + ,\nwhere H\u0302 := \u2211m\nj=1 \u222b V \u03bauj ,v\u03ba > uj ,vdv, h\u0302 := \u2211m j=1\u03bauj ,vj , \u03bb is a regularization coefficient, and [\u03b1]+ sets all negative entries\nof its argument to zero (the integral in H\u0302 can easily be computed in closed form). We emphasize that the Gaussian mixture representation takes a quite different form to the RKHS representation of the edge potentials. Finally, to introduce evidence, we propose to use kernel ridge regression to provide a mean value of the hidden variable xt given the observation yt, and to center a Gaussian at this value: again, the regression function is learned nonparametrically from training data.\nWe now describe how these edge potentials are incorporated into Gaussian mixture BP. Assuming the incoming messages are each a mixture of bGaussians, the product of dt such messages will contain bdt Gaussians, which causes an exponential blow-up in representation size and computational cost. In their original work, Sudderth et al. address this issue using an approximation scheme. First, they subsample from the incoming mixture of bdt Gaussians to draw b Gaussians, at a computational cost of O(dt\u03c4b2) for each node, where \u03c4 is the number of iterations of the associated Gibbs sampler (see their Algorithm 1). The evidence introduced via kernel ridge regression is then incorporated, using a reweighting described by their Algorithm 2. Finally, in Algorithm 3, b samples { xit }b i=1 are drawn from the reweighted mixture of b Gaussians,\nand for each of these, { xis }b i=1\nare drawn from the conditional distribution xs|xit arising from the edge potential \u03c8(xs, xt) (which is itself a Gaussian mixture, learned via the approach of Sugiyama et al.). Gaussians are placed on each of the centres { xis }b i=1 , and the process is iterated.\nIn our implementation, we used the more efficient multiscale KD-tree sampling method of Ihler et al. (2003). We converted the Matlab Mex implementation of Ihler (2003) to C++, and used GraphLab to execute sampling in parallel with up to 16 cores. An input parameter to the sampling procedure is , the level of accuracy. We performed a line search to set for high accuracy, but limited the execution time to be at most 1000 times slower than KBP.\nFinally, we describe the inference procedure performed by Particle BP. In this case, each node t is associated with a set of particles { xit }b i=1\n, drawn i.i.d. from a distribution Wt(xt). Incoming messages mut are expressed as weights of the particles xit. Unlike Gaussian mixture BP, the incoming messages all share the same set of particles, which removes\n1These centres may be selected at random from the training observations. We denote the mixture kernel by \u03ba(u, v) to distinguish it from the RKHS kernels used earlier.\nthe need for Parzen window smoothing. The outgoing message mts is computed by summing over the product of these weights and the edge and evidence potentials at the particles, yielding a set of weights over samples { xis }b i=1\nat node s; the procedure is then iterated (see Ihler & McAllester, 2009, eq. 8). We again implement this algorithm using edge potentials computed according to Sugiyama et al. Since an appropriate sample distribution Wt is hard to specify in advance, a resampling procedure must be carried out at each BP iteration, to refresh the set of samples at each node and ensure the samples cover an appropriate support (this is a common requirement in particle filtering). Thus, each iteration of Particle BP requires a Metropolis-Hastings chain to be run for every node, which incurs a substantial computational cost. That said, we found that in practice, the resampling could be conducted less often without an adverse impact on performance, but resulting in major improvements in runtime, as described in Section 2 below. See (Ihler & McAllester, 2009, Section 6) for more detail."}, {"heading": "2 Settings for Discrete and Particle BP", "text": ""}, {"heading": "2.1 Depth Reconstruction from 2-D Images", "text": ""}, {"heading": "2.1.1 Discrete BP", "text": "The log-depth was discretized into 30 bins, and edge parameters were selected to achieve locally consistent Loopy BP marginals using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve resultant accuracy, but increased runtime significantly. We used the Splash scheduling of Gonzalez et al. (2009) since it provided the lowest runtime among all tested schedulings."}, {"heading": "2.1.2 Particle BP", "text": "The particle belief propagation implementation was particularly difficult to tune due to its excessively high runtime. Theoretically, results comparable to the Kernel BP method were attainable. However in practice, the extremely high cost of the resampling phase on large models meant that only a small number of particles could be maintained if a reasonable runtime was to be achieved on our evaluation set of 274 images.\nUltimately, we decided to find a configuration which allowed us to complete the evaluation in about 2 machine-days on an 8-core Intel Nehalem machine; allowing inference on each evaluation image to take 10 minutes of parallel computation. For each image, we ran 100 iterations of a simple linear-sweep scheduling, using 20 particles per message, and resampling every 10 iterations. Each resampling phase ran MCMC for a maximum of 10 steps per particle. We also implemented acceleration tricks where low weight particles (< 1E \u2212 7 after normalization) were ignored during the message passing process. Empirically this decreased runtime without affecting the quality of results."}, {"heading": "2.2 Synthetic Image Denoising", "text": ""}, {"heading": "2.2.1 Discrete BP", "text": "To simplify evaluation, we permitted a certain degree of \u201coracle\u201d information, by matching the discretization levels during inference with the color levels in the ground-truth image.\nWe evaluated combined gradient/IPF + BP methods here to learn the edge parameters. We found that gradient/IPF performed well when there were few colors in the image, but failed to converge when the number of colors increased into the hundreds. This is partly due to the instability of BP, as well as the large number of free parameters in the edge potential.\nTherefore once again, edge potentials were selected using the technique described in Wainwright et al. (2003). This performed quite well empirically, as seen in Figure 1(c) (main document)."}, {"heading": "2.2.2 Particle BP", "text": "The high runtime of the Particle Belief Propagation again made accuracy evaluation difficult. As before, we tuned the particle BP parameters to complete inference on the evaluation set of 110 images in 2 machine days, allowing about 25 minutes per evaluation image. We ran 100 iterations of 30 particles per message, resampling every 15 iterations. Each resampling phase ran MCMC for a maximum of 10 steps per particle."}, {"heading": "3 Effects of Approximate Message Updates", "text": "In this section, we study how different levels of feature approximation error affect the speed of kernel BP and the resulting performance. Our experimental setup was the image denoising experiment described in Section 5.1 of the main document. We note that the computational cost of our constant message update is O(`2dmax) where ` is inversely related to the approximation error . This is a substantial runtime improvement over naively applying a low rank kernel matrix approximation, which only results in a linear time update with computational cost O(`mdmax). In this experiment, we varied the feature approximation error over three levels, i.e. 10\u22121, 10\u22122, 10\u22123, and compared both speed and denoising performance of the constant time update to the linear time update.\nFrom Figures 4 (a) and (c), we can see that for each approximation level, the constant time update achieves about the same denoising performance as the linear time update, while at the same time being orders of magnitude faster (by comparing Figures 4 (b) and (d)). Despite the fact that the constant time update algorithm makes an additional approximation to the tensor product features, its denoising performance is not affected. We hypothesize that the degradation in performance is largely caused by representing the messages in terms of a small number of basis functions, while the approximation to the tensor features introduces little additional degradation.\nAnother interesting observation from Figure 4 (d) is that the runtime of constant time kernel BP update increases as the number of colors in the image increases. This is mainly due to the increased number of test points as the color number increases; and also partially due to the increased rank needed for approximating the tensor features. In Figure 5, we plot the rank needed for kernel feature approximation and tensor feature approximation for different numbers of colors and different approximation errors . It can be seen that in general, as we use a smaller approximation error, the rank increases, leading to a slight increase in runtime.\nFinally, we compare with kernel belief propagation in the absence of any low rank approximation (KBP Full). Since KBP Full is computationally expensive, we reduced the denoising problem to images of size 50 \u00d7 50 to allow KBP to finish in reasonable time. We only compared on 100 color images, again for reasons of cost. We varied the feature approximation error for the constant time and linear time approximation over three levels, 10\u22121, 10\u22122, 10\u22123, and compared both speed and denoising performance of KBP Full versus the constant time and linear time updates.\nThe comparisons are shown in Figure 6. We can see from Figure 6(a) that the denoising errors for constant time and linear time approximations decrease as we decrease the approximation error . Although the denoising error of KBP Full is slightly lower than constant time approximations, it is a slight increase over the linear time approximation at = 10\u22123. One reason might be that the kernel approximation also serves as a means of regularization when learning the conditional embedding operator. This additional regularization may have slightly improved the generalization ability of the linear time approximation scheme. In terms of runtime (Figure 6(b)), constant time approximation is substantially faster than linear time approximation and KBP Full. In particular, it is nearly 100 times faster than the linear time algorithm, and 10000 times faster than KBP Full."}, {"heading": "4 Predicting Paper Categories", "text": "In this experiment, we predict paper categories from a combination of the paper features and their citation network. Our data were obtained by crawling 143,086 paper abstracts from the ACM digital library, and extracting the citation networks linking these papers. Each paper was labeled with a variable number of categories, ranging from 1 to 10; there were a total of 367 distinct categories in our dataset. For simplicity, we ignored directions in the citation network, and treated it as an undirected graph (i.e, we did not distinguish \u201cciting\u201d and \u201cbeing cited\u201d). The citation network was sparse, with more than 85% of the papers having fewer than 10 links. The maximum number of links was 450.\nPaper category prediction is a multi-label problem with a large output space. The output is very sparse: the label vectors have only a small number of nonzero entries. In this case, the simple one-against-all approach of learning a single predictor for each category can become prohibitively expensive, both in training and in testing. Recently, Hsu et al. (2009) proposed to solve this problem using compressed sensing techniques: high dimensional sparse category labels are first compressed to lower dimensional real vectors using a random projection, and regressors are learned for these real vectors. In the testing stage, high dimensional category labels are decoded from the predicted real vectors of the test data using orthogonal marching pursuit (OMP).\nFor the purposed of the present task, however, the compressed sensing approach ignores information from the citation network: papers that share categories tend to cite each other more often. Intuitively, taking into account category information\nfrom neighboring papers in the citation network should improve the performance of category prediction. This intuition can be formalized using undirected graphical models: each paper i contains a category variable yi \u2208 {0, 1}367, and these variables are connected according to the citation network; each category variable is also connected to a variable xi corresponding to the abstract of the paper. In our experiment, we used 9700 stem words for the abstracts, and xi \u2208 R9700 was the tf-idf vector for paper i. The graphical model thus contains two types of edge potential, \u03a8(yi, yi) and \u03a8(yi, yk), where k \u2208 N (j) is the neighbor of j according to the citation network.\nIt is difficult to learn this graphical model and perform inference on it, since the category variables yi have high cardinality, making the marginalization step in BP prohibitively expensive. Inspired by the compressed sensing approach for multilabel prediction, we first employed random projection kernels, and then used our kernel BP algorithm. Let A \u2208 Rd\u00d7367 be a random matrix containing i.i.d. Gaussian random variables of zero mean and variance 1/d. We defined the random projection kernel for the category labels to be k(y, y\u2032) = \u3008Ay,Ay\u2032\u3009 = \u3008\u03c6(y), \u03c6(y\u2032)\u3009, and used a linear kernel for the abstract variables. We ran kernel BP for 5 iterations, since further iterations did not improve the performance. MAP assignment based on the belief was performed by finding a unit vector \u03c6(y\u0302) = Ay\u0302 that maximized the belief. The sparse category labels y\u0302 were decoded from \u03c6(y\u0302) using OMP.\nTo measure experimental performance, we performed 10 random splits of the papers, where in each split we used 1/3 of the papers for training and the remaining 2/3 for testing. The random splits were controlled in such a way that high degree nodes (with degree > 10) in the citation networks always appeared in the training set. Such splitting reflects the data properties expected in real-world scenarios: important papers (high degree nodes which indicate either influential papers\nor survey papers) are usually labeled, whereas the majority of papers may not have a label; the automatic labeling is mainly needed for these less prominent papers. We used recall@k in evaluating the performance of our method on the test data. We compared against the regression technique of Hsu et al. for multilabel prediction, and a baseline prediction using the top k most frequent categories. For both our method and the method of Hsu et al., we used a random projection matrix with d = 100.\nResults are shown in Figure 7. Kernel BP performs better than multilabel prediction via compressed sensing (i.e., the independent regression approach, which ignores graphical model structure) over a range of k values. In particular, for the top 10 and 20 predicted categories, kernel BP achieves recall scores of 0.419 and 0.476, respectively, as opposed to 0.362 and 0.417 for independent regression."}, {"heading": "5 Local Marginal Consistency Condition When Learning With BP", "text": "In this section, we show that fixed points of BP satisfy particular marginal consistency conditions (29) and (30) below. As we will see, these arise from the fact that we are using a Bethe free energy approximation in fitting our model, and the form of the fixed point equations that define the minimum of the Bethe free energy. The material in this section draws from a number of references (for instance Yedidia et al., 2001, 2005; Wainwright & Jordan, 2008; Koller & Friedman, 2009), but\nis presented in a form specific to our case, since we are neither in a discrete domain nor using exponential families.\nThe parameters of a pairwise Markov random field (MRF) can be learned by maximizing the log-likelihood of the model P with respect to true underlying distribution P?. Denote the model by\nP(X) := 1\nZ \u220f (s,t)\u2208E \u03a8st(Xs, Xt) \u220f s\u2208V \u03a8s(Xs)\nwhere Z := \u222b X \u220f (s,t)\u2208E \u03a8st(Xs, Xt) \u220f s\u2208V \u03a8s(Xs) is the partition function that normalizes the distribution. The model parameters {\u03a8st(Xs, Xt),\u03a8s(Xs)} can be estimated by maximizing L = EX\u223cP?(X) [logP(X)]\n= EX\u223cP?(X)  \u2211 (s,t)\u2208E log \u03a8st(Xs, Xt) + \u2211 s\u2208V log \u03a8s(Xs)\u2212 logZ  . (16) Define \u03a8\u0303st(Xs, Xt) := log \u03a8st(Xs, Xt) and \u03a8\u0303s(Xs) := log \u03a8s(Xs). Setting the derivatives of L with respect to{\n\u03a8\u0303st(Xs, Xt), \u03a8\u0303s(Xs) } to zero, we have\n\u2202L \u2202\u03a8\u0303s(Xs) = P?(Xs, Xt)\u2212 \u2202 logZ \u2202\u03a8\u0303st(Xs, Xt) = 0, (17)\n\u2202L \u2202\u03a8\u0303s(Xs) = P?(Xs)\u2212 \u2202 logZ \u2202\u03a8\u0303s(Xs) = 0. (18)\nFor a general pairwise MRF on a loopy graph, computing the log-partition function, logZ, is intractable. Following e.g. Yedidia et al. (2001, 2005) and Koller & Friedman (2009, Ch. 11), logZ may be approximated as a minimum of the Bethe free energy with respect to a new set of parameters {bst, bs},\nF ({bst, bs}) = \u2211\n(s,t)\u2208E\n\u222b X \u222b X bst(Xs, Xt) [log bst(Xs, Xt)\u2212 log \u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt)] dXsdXt\n\u2212 \u2211 s\u2208V (ds \u2212 1) \u222b X bs(Xs) [log bs(Xs)\u2212 log \u03a8s(Xs)] dXs, (19)\nsubject to normalization and marginalization constraints, \u222b X bs(Xs)dXs = 1, \u222b X bst(Xs, Xt)dXs = bt(Xt). Let F\n? := min{bst,bs} F (we note that Bethe free energy is not convex, and hence there can be multiple local minima. Our reasoning does not require constructing a specific local minimum, and therefore we simply write F ?). The zero gradient conditions\non the partial derivatives of L are then approximated as \u2202L\n\u2202\u03a8\u0303s(Xs) \u2248 P?(Xs, Xt)\u2212\n\u2202F ?\n\u2202\u03a8\u0303st(Xs, Xt) = 0, (20)\n\u2202L \u2202\u03a8\u0303s(Xs) \u2248 P?(Xs)\u2212 \u2202F ? \u2202\u03a8\u0303s(Xs) = 0. (21)\nSince F is a linear function of { \u03a8\u0303st(Xs, Xt), \u03a8\u0303s(Xs) }\nfor every fixed {bst, bs}, Danskin\u2019s theorem (Bertsekas, 1999, p. 717) gives us a way to compute the partial derivatives of F ?. These are\n\u2202F ? \u2202\u03a8\u0303st(Xs, Xt) = \u2202F ({b?st, b?s}) \u2202\u03a8\u0303st(Xs, Xt) = b?st(Xs, Xt), (22)\n\u2202F ? \u2202\u03a8\u0303s(Xs) = \u2202F ({b?st, b?s}) \u2202\u03a8\u0303s(Xs) = b?s(Xs), (23)\nwhere {b?st, b?s} := argmin{bst,bs} F . Therefore, according to (20) and (21), learning a pairwise MRF using the Bethe energy variational approximation to the log partition function results in the following matching conditions,\nP?(Xs, Xt) = b?st(Xs, Xt), (24) P?(Xs) = b?s(Xs). (25)\nWe now introduce the notion of belief propagation as a means of finding the minima of the Bethe free energy. This will in turn lead to local marginal consistency conditions for learning with BP. Yedidia et al. (2001) showed that the fixed point of F (and therefore the global minimum {b?st, b?s}) must satisfy the relations\nb?st(Xs, Xt) = \u03b1\u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt) \u220f\nu\u2208\u0393s\\t\nm?us(Xs) \u220f\nv\u2208\u0393t\\s\nm?vt(Xt), (26)\nb?s(Xs) = \u03b1\u03a8s(Xs) \u220f u\u2208\u0393s m?us(Xs), (27)\nwhere \u03b1 denotes a normalization constant and {m?ts} are the fixed point messages,\nm?ts(Xs) = \u03b1 \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f\nu\u2208\u0393t\\s\nm?ut(Xt) dXt. (28)\nThus, P?(Xs, Xt) = \u03b1\u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt) \u220f u\u2208\u0393s\\t m?us(Xs) \u220f v\u2208\u0393t\\s m?vt(Xt), (29)\nP?(Xs) = \u03b1\u03a8s(Xs) \u220f u\u2208\u0393s m?us(Xs). (30)\nCombining these relations and assuming that P?(Xs) and m?ts(Xs) are strictly positive, we can also obtain the consistent relation for the local conditionals,\nP?(Xt|Xs) = P?(Xs, Xt) P?(Xs) = \u03a8st(Xs, Xt)\u03a8s(Xt)\n\u220f u\u2208\u0393t\\sm ? ut(Xt)\nm?ts(Xs) . (31)"}, {"heading": "6 BP Inference Using Learned Potentials", "text": "The inference problem in pairwise MRFs is to compute the marginals or the log partition function for the model with learned potentials. Belief propagation is an iterative algorithm for performing approximate inference in MRFs. BP can also be viewed as an iterative algorithm for minimizing the Bethe free energy approximation to the log partition function. The results of this algorithm are a set of beliefs which can be used for obtaining the MAP assignment of the corresponding variables.\nThe BP message update (with the learned potentials) is\nmts(Xs) = \u03b1 \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f\nu\u2208\u0393t\\s\nmut(Xt) dXt, (32)\nand at any iteration, the beliefs can be computed using the current messages, Bst(Xs, Xt) = \u03b1\u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt) \u220f u\u2208\u0393s\\t mus(Xs) \u220f v\u2208\u0393t\\s mvt(Xt), (33)\nBs(Xs) = \u03b1\u03a8s(Xs) \u220f u\u2208\u0393s mus(Xs). (34)\nTo see how the message update equation can be expressed using the true local conditional P?(Xt|Xs), we divide both size\nof (32) by the fixed point message m?ts(Xs) during BP learning stage, and introduce 1 = \u220f u\u2208\u0393t\\s m?ut(Xt)\u220f\nu\u2208\u0393t\\s m?ut(Xt) . The message update equation in (32) can then be re-written as\nmts(Xs) m?ts(Xs) = \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f u\u2208\u0393t\\smut(Xt) m?ts(Xs) \u220f u\u2208\u0393t\\s mut(Xt) m?ut(Xt) dXt. (35)\nThe belief at any iteration becomes Bs(Xs) = \u03b1\u03a8s(Xs) \u220f u\u2208\u0393s mus(Xs) = (\u220f u\u2208\u0393s mst(Xs) m?st(Xs) )( \u03b1\u03a8s(Xs) \u220f u\u2208\u0393s m?st(Xs) ) . (36) We reparameterize the message as\nmst(Xt)\u2190\u2212 mst(Xt)\nm?st(Xt) . (37)\nSince the potentials are learned via BP, we can use the relation in (31) to obtain\nmts(Xs) = \u222b X P?(Xt|Xs) \u220f u\u2208\u0393t\\s mut(Xt) dXt. (38)\nSimilarly, we obtain from (30) that\nBs(Xs) = (\u220f u\u2208\u0393s mst(Xs) ) P?(Xs). (39)\nMessages from Evidence Node Given evidence xt at node Xt, the outgoing message from Xt to Xs is mts(Xs) = \u03b1\u03a8st(Xs, xt)\u03a8t(xt). Using similar reasoning to the case of an internal node, we have\nmts(Xs) m?ts(Xs) = \u03a8st(Xs, xt)\u03a8t(xt) m?ts(Xs) (40)\n= \u03a8st(Xs, xt)\u03a8t(xt)\n\u220f u\u2208\u0393t\\sm ? ut(xt)\nm?ts(Xs)\n1\u220f u\u2208\u0393t\\sm ? ut(xt)\n(41)\n= P?(xt|Xs) 1\u220f\nu\u2208\u0393t\\sm ? ut(xt)\n(42)\n\u221d P?(xt|Xs) (43) where \u220f u\u2208\u0393t\\sm ? ut(xt) is constant given a fixed value Xt = xt. Reparametrizing the message mts(Xs)\u2190 mts(Xs) m?ts(Xs)\n, the outgoing message from the evidence node is simply the true likelihood function evaluated at xt."}, {"heading": "7 A Note on Kernelization of Gaussian BP", "text": "In this section, we consider the problem of defining a joint Gaussian graphical model in the feature space induced by a kernel. We follow Bickson (2008) in our presentation of the original Gaussian BP setting. We will show that assuming a Gaussian in an infinite feature space leads to challenges in interpretation and estimation of the model.\nConsider a pairwise MRF, P(X) = \u220f s\u2208V \u03a8s(Xs) \u220f (s,t)\u2208E \u03a8st(Xs, Xt). (44) In the case of the Gaussian, the probability density function takes the form P(X) \u221d exp ( \u2212 12 (X\u2212 \u00b5) >A(X\u2212 \u00b5) )\n\u221d exp ( \u2212 12X >AX\u2212 b>X ) ,\nwhere A = C\u22121 is the precision matrix, and A\u00b5 = b. Putting this in the form (44), the node and edge potentials are written \u03a8s(Xs) , exp ( \u2212 12X > s AssXs + bsXs ) (45) and \u03a8st(Xs, Xt) , exp ( \u2212X>s AstXt ) . (46) We now consider how these operations would appear in Hilbert space. In this case, we would have \u03a8s(Xs) := exp ( \u2212 12 \u3008\u03c6(Xs), Ass\u03c6(Xs)\u3009F + \u3008bs, \u03c6(Xs)\u3009F ) , and \u03a8st(Xs, Xt) := exp (\u2212\u3008\u03c6(Xs), Ast\u03c6(Xt)\u3009F ) .\nWe call the Ass and Ast precision operators, by analogy with the finite dimensional case. At this point, we already encounter a potential difficulty in kernelizing Gaussian BP: how do we learn the operators Ass, bs and Ast from data? We could in principle define a covariance operator C with (s, t)th block the pairwise covariance operator Cst, but Ast would then be the (s, t)th block of C\u22121, which is difficult to compute. As we shall see below, however, these operators appear in the BP message updates.\nNext, we describe a message passing procedure for the Gaussian potentials in (45) and (46). The message from t to s is written\nmts(Xs) = \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f u\u2208\u0393t\\s\nmut(Xt)\ufe38 \ufe37\ufe37 \ufe38 (a) dXt.\nWe first consider term (a) in the above. We will assume, with justification to follow, that mut(Xt) takes the form mut(Xt) \u221d exp ( \u2212 12X > t PutXt + \u00b5 > utXt ) , where the terms Put and \u00b5ut are defined by recursions specified below (we retain linear algebraic notation for simplicity). It follows that \u03a8t(Xt) \u220f u\u2208\u0393t\\smut(Xt) is proportional to a Gaussian,\n\u03a8t(Xt) \u220f\nu\u2208\u0393t\\s\nmut(Xt) \u221d exp ( \u2212 12X > t Pt\\sXt + \u00b5 > t\\sXt ) ,\nwhere we define the intermediate operators \u00b5t\\s := \u00b5t + \u2211 u\u2208\u0393t\\s \u00b5ut and Pt\\s := Ass +\n\u2211 u\u2208\u0393t\\s Put .\nTo compute the message mts(Xs), we integrate\nmts(Xs) = \u222b X \u03a8st(Xs, Xt)\u03a8t(Xt) \u220f u\u2208\u0393t\\s mut(Xt) dXt\n= \u222b X exp (\u2212XtAtsXs) exp ( \u2212 12X > t Pt\\sXt + \u00b5 > t\\sXt ) dXt (47)\nCompleting the square, we get the parameters of the message mts in the standard form, Pts = \u2212A>tsP\u22121t\\sAts (48) \u00b5ts = \u2212\u00b5>t\\sP \u22121 t\\sAts. (49)\nThere are two main difficulties in implementing the above procedure in feature space. First, it is not clear how to learn the precision operators from the data. Second, we need to invert these precision operators. Thus, it remains a challenging open question to define Gaussian BP in feature space. The feature space Gaussian BP updates may be contrasted with the kernel BP updates we propose in the main text. The latter have regularized closed form empirical estimates, and they are very different from the Gaussian BP form in (47) and parameter updates in (48) and (49)."}, {"heading": "8 Message Error Incurred by the Additional Feature Approximation", "text": "We bound the difference between the estimated conditional embedding operator U\u0302X\u2297t |Xs and its counterpart U\u0303X\u2297t |Xs after further feature approximation. Assume \u2016\u03c6(x)\u2016F \u2264 1, and define the tensor feature \u03be(x) := \u2297 u\\s \u03c6(x). Denote by \u03be\u0303(x) and \u03c6\u0303(x) the respective approximations of \u03be(x) and \u03c6. Furthermore, let the approximation error after the incomplete QR decomposition be = max { maxX \u2225\u2225\u2225\u03c6(x)\u2212 \u03c6\u0303(x)\u2225\u2225\u2225 F , maxX \u2225\u2225\u2225\u03be(X)\u2212 \u03be\u0303(x)\u2225\u2225\u2225 H\n} . It follows that\u2225\u2225\u2225U\u0302X\u2297t |Xs \u2212 U\u0303X\u2297t |Xs\u2225\u2225\u2225HS (50)\n\u2264 \u2225\u2225\u2225C\u0302X\u2297t Xs(C\u0302XsXs + \u03bbmI)\u22121 \u2212 C\u0303X\u2297t Xs(C\u0303XsXs + \u03bbmI)\u22121\u2225\u2225\u2225HS (51)\n\u2264 \u2225\u2225\u2225(C\u0302X\u2297t Xs \u2212 C\u0303X\u2297t Xs)(C\u0302XsXs + \u03bbmI)\u22121\u2225\u2225\u2225HS + \u2225\u2225\u2225C\u0303X\u2297t Xs [(C\u0303XsXs + \u03bbmI)\u22121 \u2212 (C\u0302XsXs + \u03bbmI)\u22121]\u2225\u2225\u2225HS (52) \u2264 1 \u03bbm \u2225\u2225\u2225C\u0302X\u2297t Xs \u2212 C\u0303X\u2297t Xs\u2225\u2225\u2225HS + 1\u03bb3/2m \u2225\u2225\u2225C\u0302XsXs \u2212 C\u0303XsXs\u2225\u2225\u2225 HS . (53)\nFor the first term, 1\n\u03bbm \u2225\u2225\u2225C\u0302X\u2297t Xs \u2212 C\u0303X\u2297t Xs\u2225\u2225\u2225HS (54) = 1\n\u03bbm \u2225\u2225\u2225\u2225\u2225 1m\u2211 i \u03be(xis)\u03c6(x i s) > \u2212 1 m \u2211 i \u03be\u0303(xis)\u03c6\u0303(x i s) > \u2225\u2225\u2225\u2225\u2225 HS\n(55)\n\u2264 1 \u03bbm 1 m \u2211 i \u2225\u2225\u2225\u03be(xis)\u03c6(xis)> \u2212 \u03be\u0303(xis)\u03c6\u0303(xis)>\u2225\u2225\u2225 HS\n(56)\n\u2264 1 \u03bbm max i \u2225\u2225\u2225\u03be(xis)\u03c6(xis)> \u2212 \u03be\u0303(xis)\u03c6\u0303(xis)>\u2225\u2225\u2225 HS\n(57)\n\u2264 1 \u03bbm max i {\u2225\u2225\u2225\u03be(xis)\u03c6(xis)> \u2212 \u03be\u0303(xis)\u03c6(xis)>\u2225\u2225\u2225 HS + \u2225\u2225\u2225\u03be\u0303(xis)\u03c6(xis)> \u2212 \u03be\u0303(xis)\u03c6\u0303(xis)>\u2225\u2225\u2225 HS } (58) \u2264 2 \u03bbm . (59)\nSimilarly, for the second term, 1\n\u03bb 3/2 m \u2225\u2225\u2225C\u0302XsXs \u2212 C\u0303XsXs\u2225\u2225\u2225 HS \u2264 2 \u03bb 3/2 m . (60)\nCombining the results, we obtain \u2225\u2225\u2225U\u0302X\u2297t |Xs \u2212 U\u0303X\u2297t |Xs\u2225\u2225\u2225HS \u2264 2 (\u03bb\u22121m + \u03bb\u22123/2m ). (61)"}], "references": [{"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Math. Soc., 68, 337\u2013404.", "citeRegEx": "Aronszajn,? 1950", "shortCiteRegEx": "Aronszajn", "year": 1950}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Fault identification via non-parametric belief propagation", "author": ["D. Bickson", "D. Baron", "A. Ihler", "H. Avissar", "D. Dolev"], "venue": "IEEE Transactions on Signal Processing. ISSN 1053-587X", "citeRegEx": "Bickson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bickson et al\\.", "year": 2011}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Characteristic kernels on groups and semigroups", "author": ["K. Fukumizu", "B. Sriperumbudur", "A. Gretton", "B. Schoelkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Fukumizu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2009}, {"title": "Particle belief propagation", "author": ["A. Ihler", "D. McAllester"], "venue": "In AISTATS", "citeRegEx": "Ihler and McAllester,? \\Q2009\\E", "shortCiteRegEx": "Ihler and McAllester", "year": 2009}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["A.T. Ihler", "J.W. Fisher III", "A.S. Willsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ihler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2005}, {"title": "Efficient multiscale sampling from products of gaussian mixtures", "author": ["E.T. Ihler", "E.B. Sudderth", "W.T. Freeman", "A.S. Willsky"], "venue": null, "citeRegEx": "Ihler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2003}, {"title": "Protein secondary structure prediction based on position-specific scoring matrices", "author": ["D.T. Jones"], "venue": "J. Mol. Biol., 292, 195\u2013202.", "citeRegEx": "Jones,? 1999", "shortCiteRegEx": "Jones", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "GraphLab: A new parallel framework for machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein"], "venue": "In Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Low et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Low et al\\.", "year": 2010}, {"title": "Turbo decoding as an instance of Pearl\u2019s belief propagation algorithm. J-SAC", "author": ["R. McEliece", "D. MacKay", "J. Cheng"], "venue": null, "citeRegEx": "McEliece et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McEliece et al\\.", "year": 1998}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": null, "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufman.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A.Y. Ng"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Saxena et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2009}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": "In 13th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "In Proc. Intl. Conf. Machine Learning", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "J. Mach. Learn. Res., 2, 67\u201393.", "citeRegEx": "Steinwart,? 2001", "shortCiteRegEx": "Steinwart", "year": 2001}, {"title": "Nonparametric belief propagation", "author": ["E. Sudderth", "A. Ihler", "W. Freeman", "A. Willsky"], "venue": "In CVPR", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "Conditional density estimation via least-squares density ratio estimation", "author": ["M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori", "H. Hachiya", "D. Okanohara"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Correctness of belief propagation in Gaussian graphical models of arbitrary topology", "author": ["Y. Weiss", "W.T. Freeman"], "venue": "Neural Computation,", "citeRegEx": "Weiss and Freeman,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Freeman", "year": 2001}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": "Cambridge, UK: Cambridge University Press.", "citeRegEx": "Wendland,? 2005", "shortCiteRegEx": "Wendland", "year": 2005}, {"title": "Approximate inference and protein-folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Yanover and Weiss,? \\Q2002\\E", "shortCiteRegEx": "Yanover and Weiss", "year": 2002}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Belmont, MA: Athena Scientific, second edn.", "citeRegEx": "Bertsekas,? 1999", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "Gaussian Belief Propagation: Theory and Application", "author": ["D. Bickson"], "venue": "Ph.D. thesis, The Hebrew University of Jerusalem.", "citeRegEx": "Bickson,? 2008", "shortCiteRegEx": "Bickson", "year": 2008}, {"title": "Residual splash for optimally parallelizing belief propagation", "author": ["J. Gonzalez", "Y. Low", "C. Guestrin"], "venue": null, "citeRegEx": "Gonzalez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2009}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "KDE Matlab ToolBox", "author": ["A. Ihler"], "venue": "http://www.ics.uci.edu/ ihler/code/.", "citeRegEx": "Ihler,? 2003", "shortCiteRegEx": "Ihler", "year": 2003}, {"title": "Particle belief propagation", "author": ["A. Ihler", "D. McAllester"], "venue": "In AISTATS,", "citeRegEx": "Ihler and McAllester,? \\Q2009\\E", "shortCiteRegEx": "Ihler and McAllester", "year": 2009}, {"title": "Efficient multiscale sampling from products of gaussian mixtures", "author": ["E.T. Ihler", "E.B. Sudderth", "W.T. Freeman", "A.S. Willsky"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ihler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2003}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Nonparametric belief propagation", "author": ["E. Sudderth", "A. Ihler", "W. Freeman", "A. Willsky"], "venue": "In CVPR", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "Conditional density estimation via least-squares density ratio estimation", "author": ["M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori", "H. Hachiya", "D. Okanohara"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Belief propagation is an inference algorithm for graphical models that has been widely and successfully applied in a great variety of domains, including vision (Sudderth et al., 2003), protein folding (Yanover & Weiss, 2002), and turbo decoding (McEliece et al.", "startOffset": 175, "endOffset": 198}, {"referenceID": 11, "context": ", 2003), protein folding (Yanover & Weiss, 2002), and turbo decoding (McEliece et al., 1998).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "Our approach applies not only to continuous-valued non-Gaussian variables, but also generalizes to strings and graphs (Sch\u00f6lkopf et al., 2004), groups (Fukumizu et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 4, "context": ", 2004), groups (Fukumizu et al., 2009), compact manifolds (Wendland, 2005, Chapter 17), and other domains on which kernels may be defined.", "startOffset": 16, "endOffset": 39}, {"referenceID": 15, "context": "This extends earlier work of Song et al. (2010) on inference for trees to the case of graphs with loops.", "startOffset": 29, "endOffset": 48}, {"referenceID": 21, "context": "Sudderth et al. (2003) proposed an approximate belief propagation algorithm for pairwise Markov random fields, where the parametric forms of the node and edge potentials are supplied in advance, and the messages are approximated as mixtures of Gaussians: we refer to this approach as Gaussian Mixture BP (this method was introduced as \u201cnonparametric BP\u201d, but it is in fact a Gaussian mixture approach).", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "Sudderth et al. (2003) proposed an approximate belief propagation algorithm for pairwise Markov random fields, where the parametric forms of the node and edge potentials are supplied in advance, and the messages are approximated as mixtures of Gaussians: we refer to this approach as Gaussian Mixture BP (this method was introduced as \u201cnonparametric BP\u201d, but it is in fact a Gaussian mixture approach). Instead of mixtures of Gaussians, Ihler & McAllester (2009) used particles to approximate the messages, resulting in the Particle BP algorithm.", "startOffset": 0, "endOffset": 463}, {"referenceID": 13, "context": "Belief Propagation (BP) is an iterative algorithm for performing inference in MRFs (Pearl, 1988).", "startOffset": 83, "endOffset": 96}, {"referenceID": 12, "context": "In many applications, however, the resulting loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999).", "startOffset": 105, "endOffset": 126}, {"referenceID": 28, "context": "Several theoretical studies have also provided insight into the approximations made by loopy BP, partially justifying its application to graphs with cycles (Wainwright & Jordan, 2008; Yedidia et al., 2001).", "startOffset": 156, "endOffset": 205}, {"referenceID": 19, "context": "4 Kernel Belief Propagation We develop a novel kernelization of belief propagation, based on Hilbert space embeddings of conditional distributions (Song et al., 2009), which generalizes an earlier kernel algorithm for exact inference on trees (Song et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 18, "context": ", 2009), which generalizes an earlier kernel algorithm for exact inference on trees (Song et al., 2010).", "startOffset": 84, "endOffset": 103}, {"referenceID": 18, "context": "In the present section, we will provide a detailed derivation of kernel BP in accordance with Song et al. (2010). While the immediate purpose is to make the paper selfcontained, there are two further important reasons: to provide the background necessary in understanding our efficient kernel BP updates in Section 5, and to demonstrate how kernel BP differs from the competing Gaussian mixture and particle based BP approaches in Section 6 (which was not addressed in earlier work on kernel tree graphical models).", "startOffset": 94, "endOffset": 113}, {"referenceID": 0, "context": "1 Message Representations We begin with a description of the properties of a message mut(xt), given it is in the reproducing kernel Hilbert space (RKHS) F of functions on the separable metric space X (Aronszajn, 1950; Sch\u00f6lkopf & Smola, 2002).", "startOffset": 200, "endOffset": 242}, {"referenceID": 16, "context": "An expression for the conditional distribution embedding was proposed by Song et al. (2009). We describe this expression by analogy with the conditioning operation for a Gaussian random vector z \u223c N (0, C), where we partition z = (z> 1 , z > 2 ) > such that z1 \u2208 R and z2 \u2208 R \u2032 .", "startOffset": 73, "endOffset": 92}, {"referenceID": 3, "context": "Following Fukumizu et al. (2004), we define the covariance operator CXsXt which allows us to compute the expectation of the product of function f(Xs) and g(Xt), i.", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "cannot be performed), the belief can instead be expressed as a conditional embedding operator (Song et al., 2010).", "startOffset": 94, "endOffset": 113}, {"referenceID": 18, "context": "On this basis, Song et al. (2009) propose a direct regularized estimate of the conditional embedding operators from the data.", "startOffset": 15, "endOffset": 34}, {"referenceID": 21, "context": "6 Gaussian Mixture And Particle BP We briefly review two state-of-the-art approaches to nonparametric belief propagation: Gaussian Mixture BP (Sudderth et al., 2003) and Particle BP (Ihler & McAllester, 2009).", "startOffset": 142, "endOffset": 165}, {"referenceID": 22, "context": "We propose instead to learn P(Xt|Xs) directly from training data following Sugiyama et al. (2010), who provide an estimate in the form of a mixture of Gaussians (see Section 1 of the Appendix for details).", "startOffset": 75, "endOffset": 98}, {"referenceID": 2, "context": "An overview of approximation approaches can be found in Bickson et al. (2011); we used an efficient KD-tree method of Ihler et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "An overview of approximation approaches can be found in Bickson et al. (2011); we used an efficient KD-tree method of Ihler et al. (2003) for performing the approximation step.", "startOffset": 56, "endOffset": 138}, {"referenceID": 10, "context": "The first two were image denoising and depth prediction problems, where we show that kernel BP is superior to discrete, Gaussian mixture and particle BP in both speed and accuracy, using a GraphLab implementation of each (Low et al., 2010).", "startOffset": 221, "endOffset": 239}, {"referenceID": 14, "context": "We used a set of 274 images taken on the Stanford campus, including both indoor and outdoor scenes (Saxena et al., 2009).", "startOffset": 99, "endOffset": 120}, {"referenceID": 14, "context": "We note that the error of kernel BP is slightly better than the results of pointwise MRF reported in Saxena et al. (2009).", "startOffset": 101, "endOffset": 122}, {"referenceID": 8, "context": "We first ran PSI-BLAST to generate the sequence profile (a 20 dimensional feature for each amino acid position), and then used this profile as features for predicting the folding structure (Jones, 1999).", "startOffset": 189, "endOffset": 202}, {"referenceID": 23, "context": "A possible extension to this work would be to kernelize tree-reweighted belief propagation (Wainwright et al., 2003).", "startOffset": 91, "endOffset": 116}, {"referenceID": 6, "context": "The convergence of kernel BP is a further challenging topic for future work (Ihler et al., 2005).", "startOffset": 76, "endOffset": 96}, {"referenceID": 21, "context": "We describe two competing approaches for nonparametric belief propagation: Gaussian mixture BP, originally known as non-parametric BP (Sudderth et al., 2003), and particle BP (Ihler & McAllester, 2009).", "startOffset": 134, "endOffset": 157}, {"referenceID": 22, "context": "In learning the edge potentials, we turn to Sugiyama et al. (2010), who provide a least-squares estimate of a conditional density in the form of a mixture of Gaussians,", "startOffset": 44, "endOffset": 67}, {"referenceID": 6, "context": "In our implementation, we used the more efficient multiscale KD-tree sampling method of Ihler et al. (2003). We converted the Matlab Mex implementation of Ihler (2003) to C++, and used GraphLab to execute sampling in parallel with up to 16 cores.", "startOffset": 88, "endOffset": 108}, {"referenceID": 6, "context": "In our implementation, we used the more efficient multiscale KD-tree sampling method of Ihler et al. (2003). We converted the Matlab Mex implementation of Ihler (2003) to C++, and used GraphLab to execute sampling in parallel with up to 16 cores.", "startOffset": 88, "endOffset": 168}, {"referenceID": 23, "context": "The log-depth was discretized into 30 bins, and edge parameters were selected to achieve locally consistent Loopy BP marginals using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve resultant accuracy, but increased runtime significantly.", "startOffset": 160, "endOffset": 185}, {"referenceID": 23, "context": "The log-depth was discretized into 30 bins, and edge parameters were selected to achieve locally consistent Loopy BP marginals using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve resultant accuracy, but increased runtime significantly. We used the Splash scheduling of Gonzalez et al. (2009) since it provided the lowest runtime among all tested schedulings.", "startOffset": 160, "endOffset": 350}, {"referenceID": 23, "context": "Therefore once again, edge potentials were selected using the technique described in Wainwright et al. (2003). This performed quite well empirically, as seen in Figure 1(c) (main document).", "startOffset": 85, "endOffset": 110}, {"referenceID": 32, "context": "Recently, Hsu et al. (2009) proposed to solve this problem using compressed sensing techniques: high dimensional sparse category labels are first compressed to lower dimensional real vectors using a random projection, and regressors are learned for these real vectors.", "startOffset": 10, "endOffset": 28}, {"referenceID": 28, "context": "Yedidia et al. (2001) showed that the fixed point of F (and therefore the global minimum {bst, bs}) must satisfy the relations bst(Xs, Xt) = \u03b1\u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt) \u220f", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "We follow Bickson (2008) in our presentation of the original Gaussian BP setting.", "startOffset": 10, "endOffset": 25}], "year": 2011, "abstractText": "We propose a nonparametric generalization of belief propagation, Kernel Belief Propagation (KBP), for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS), and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution, nor must their relations take any particular parametric form. Rather, the relations between variables are represented implicitly, and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (R, strings, groups), even where explicit parametric models are not known, or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments, we apply KBP to image denoising, depth prediction from still images, and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude, in some cases), while providing significantly more accurate results.", "creator": "LaTeX with hyperref package"}}}