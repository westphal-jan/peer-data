{"id": "1106.0800", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2011", "title": "Optimal Reinforcement Learning for Gaussian Systems", "abstract": "the exploration - exploitation tradeoff is among the central challenges of reinforcement learning. a normally classical bayesian learner would provide the numerical solution, but is intractable in general. i show that, however, when the specific case of gaussian process inference, it easily possible to make analytic statements about optimal learning of utility rewards and constraints dynamics, for theoretical, time - sensitive systems interacting continuous time and space, subject to a relatively heavy restriction on the inequality. the solution is described by an infinite - dimensional differential equation. for a first impression of how this result may be evaluated, i also provide an approximate reduction to a finite - dimensional problem, with a numeric solution.", "histories": [["v1", "Sat, 4 Jun 2011 08:14:59 GMT  (2456kb,AD)", "http://arxiv.org/abs/1106.0800v1", null], ["v2", "Wed, 7 Sep 2011 16:11:15 GMT  (37kb,D)", "http://arxiv.org/abs/1106.0800v2", "updated to camera-ready version for publication in NIPS 2011. Note some nontrivial changes to the Equations on page 4"], ["v3", "Fri, 14 Oct 2011 15:01:11 GMT  (39kb,D)", "http://arxiv.org/abs/1106.0800v3", "final pre-conference version of this NIPS 2011 paper. Once again, please note some nontrivial changes to exposition and interpretation of the results, in particular in Equation (9) and Eqs. 11-14. The algorithm and results have remained the same, but their theoretical interpretation has changed"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["philipp hennig"], "accepted": true, "id": "1106.0800"}, "pdf": {"name": "1106.0800.pdf", "metadata": {"source": "CRF", "title": "Optimal Reinforcement Learning for Gaussian Systems", "authors": ["Philipp Hennig"], "emails": ["phennig@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction \u2014 optimal reinforcement learning", "text": "Reinforcement learning is about doing two things at once: Optimising a function while learning about it. These two objectives must be balanced: Ignorance precludes efficient optimization; time spent hunting after irrelevant knowledge incurs unnecessary loss. This dilemma is famously known as the exploration exploitation tradeoff. Classic reinforcement learning often considers time cheap; the tradeoff then plays a subordinate role to the desire for learning a \u201ccorrect\u201d model or policy. Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as \u201c -greedy\u201d [1], or \u201cThompson sampling\u201d [2]. However, at least since a thesis by Duff [3] it has been known that Bayesian inference allows optimal balance between exploration and exploitation. It requires integration over every possible future trajectory under the current belief about the system\u2019s dynamics, all possible new data acquired along those trajectories, and their effect on decisions taken along the way. The trouble is that this amounts to optimization and integration over a tree of exponential cost in the size of the state space [4]. The situation is particularly dire for continuous space-times, where both depth and branching factor of the \u201ctree\u201d are uncountably infinite. Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9]. When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14]. But bound-based algorithms can not be extended to continuous spaces without making further assumptions [15], and doing so invalidates the strongest argument in their favour \u2014 that they are free of assumptions.\nIn a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems. In particular, a recent paper by Simpkins et al. [18] showed that it is actually possible to solve for the exploration exploitation tradeoff locally, by constructing a linear approximation for the system using a Kalman filter. Simpkins and colleagues further assumed known dynamics, and a reward distribution known up to Brownian drift. Here, I will use their work as inspiration for a novel reinforcement\nar X\niv :1\n10 6.\n08 00\nv1 [\nst at\n.M L\n] 4\nJ un\nlearning paradigm that simultaneously seeks to learn and optimally control the dynamics and reward distributions of an unknown, nonlinear, time-varying system (note that most reinforcement learning algorithms are restricted to time-invariant systems). This framework uses nonparametric Gaussian process (GP) priors to track beliefs over (infinite-dimensional) nonlinear functions. It describes the global Bayes-optimal solution, in the sense that it assigns a value to every point in the problem space, and every possible state of knowledge. The main result is a novel, explicit statement about exploration and exploitation, in the form of an infinite-dimensional differential equation. This kind of description opens up new approaches to reinforcement learning. As an only initial example of such treatments, I present an approximate Ansatz that affords an explicit reinforcement learning algorithm (Section 4); tested in some simple but instructive experiments (Section 5).\nAn intuitive description of the paper\u2019s results is this: Prior and corresponding choice of learning machinery (Section 2) allow explicit statements about the dynamics of the learning process (Section 3). The learning machine itself also provides an estimate of the dynamics of the unknown physical system. We combine both dynamics into a joint system, which can be optimally controlled. Doing so amounts to simultaneously controlling exploration (controlling the learning system) and exploitation (controlling the physical system).\nBecause large parts of the analysis relie on concepts from optimal control theory, it will use notation from that field. Readers more familiar with the reinforcement learning literature may wish to mentally replace coordinates x with states s, controls u with actions a, dynamics with transitions p(s\u2032 | s, a) and utilities q with losses (negative rewards) \u2212r. The latter is potentially confusing, so note that optimal control in this paper will attempt to minimize values, rather than to maximize them, as usual in reinforcement learning (these two options are, of course, equivalent)."}, {"heading": "2 A class of learning problems", "text": "Consider the task of optimally controlling an uncertain system whose states s \u2261 (x, t) \u2208 K = RD\u00d7R lie in a D + 1 dimensional Euclidean (thus Hilbert) phase space: A cost Q (negative cumulated reward) is acquired at (x, t) with rate dQ/dt = q(x, t), and the first learning problem is to learn this analytic function q. A second, independent learning problem concerns the dynamics of the system. Assume the dynamics separate into a free and a controlled term linear in the control:\ndx(t) = [f(x, t) + g(x, t)u(x, t)] dt (1)\nwhere u(x, t) is the control function we seek to optimize, and f, g are analytic functions. For the following analysis, we have to assume that either f or g are known, while the other may be uncertain (or, alternatively, that it is possible to obtain independent samples from both functions). A relaxation of this requirement is future work (see also Section 3). W.l.o.g., let f be uncertain and g known. Information about both q(x, t) and f(x, t) = [f1, . . . , fD] is acquired stochastically: A Poisson process of constant rate \u03bb produces mutually independent samples\nyq(x, t) = q(x, t)+ q and yfd(x, t) = fd(x, t)+ fd where q \u223c N (0, \u03c32q ); fd \u223c N (0, \u03c32fd). (2)\nThe noise levels \u03c3q and \u03c3f are presumed known. We assume i.i.d. noise for notational simplicity; more complex Gaussian noise models are possible. Let our initial beliefs about q and f be given by Gaussian processes GPkq (q;\u00b5q,\u03a3q); and independent Gaussian processes \u220fD d GPkfd(fd;\u00b5fd,\u03a3fd), respectively, with kernels kr, kf1, . . . , kfD over K, and mean / covariance functions \u00b5 / \u03a3. To ensure continuous trajectories, we also need to regularize the control. Following a customary approach in control, we introduce a quadratic control cost \u03c1(u(t)) = 12u(t)\n\u1d40R\u22121u(t) with control cost scaling matrix R. This matrix emerges as a result of using measurable quantities: Its units [R] = [x/t]/[v/x] relate the cost of changing location to the utility gained by doing so.\nThe overall task is to find the optimal discounted horizon value\nv(x, t) = min u \u222b \u221e t e\u2212(\u03c4\u2212t)/\u03b3 [ q[\u03c7[\u03c4, u(\u03c7, \u03c4)], \u03c4 ] + 1 2 u(\u03c7, \u03c4)\u1d40R\u22121u(\u03c7, \u03c4) ] d\u03c4 (3)\nwhere \u03c7(\u03c4) is the trajectory generated by the dynamics defined in Equation (1), using the control law (policy) u(x, t). Note the control-style definition of the discount factor \u03b3 > 0, which has the advantage of giving a meaningful unit, time, to \u03b3.\nBefore beginning the analysis, consider the relative generality of this definition: We allow for a continuous phase space. Both rewards and dynamics may be uncertain, of rather general nonlinear form, and may change over time. Standard reinforcement learning assumptions \u2014 discrete space, time-invariance, known reward function \u2014 are a special case. The Poisson process governing the generation of samples is a somewhat ad-hoc choice. Some probability measure is required to make the flow of information measurable through time, and the Poisson process is in some sense the simplest such measure, assigning uniform probability density. The discrete time setting usually used in reinforcement learning is recovered by making the time steps small (see also the construction in the following Section). On the downside, we had to restrict the form of the dynamics. However, Eq. (1) still covers numerous physical systems studied in control, for example many mechanical systems, from classics like cart-and-pole to realistic models for helicopters [19]."}, {"heading": "3 Optimal control for the learning process", "text": "From a control-theoretic standpoint, the optimal solution to the exploration exploitation tradeoff is formed by the dual control [20] of a joint representation of both the physical system and the learning machine used to model it. In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem. This section constructs the Hamilton-Jacobi-Bellman (HJB) equation of the joint control problem for the system described in Sec. 2, and analytically solves the equation for the optimal control. This requires a description of the learning algorithm\u2019s dynamics:\nAt time t = \u03c4 , let the system be at phase space-time s\u03c4 = (x(\u03c4), \u03c4) and have the Gaussian process belief GP(q;\u00b5\u03c4 (s),\u03a3\u03c4 (s, s\u2032)) over the function q (all derivations in this section will focus on q, and we will drop the sub-script q from many quantities for readability. The forms for f , or g, are entirely analogous, with independent Gaussian processes for each dimension d = 1, . . . , D). This belief stems from a finite number N of samples y0 = [y1, . . . , yN ]\n\u1d40 \u2208 RN collected at space-times S0 = [(x1, t1), . . . , (xN , tN )]\n\u1d40 \u2261 [s1, . . . , sN ]\u1d40 \u2208 KN (note that t1 to tN need not be equally spaced, ordered, or < \u03c4 ). For arbitrary points s\u2217 = (x\u2217, t\u2217) \u2208 K, the belief over q(s\u2217) is a Gaussian with mean function \u00b5\u03c4 , and co-variance function \u03a3\u03c4 [21]\n\u00b5\u03c4 (s \u2217) = k\u1d40(s\u2217,S0)[K(S0,S0) + \u03c3 2 rI] \u22121y0\n\u03a3\u03c4 (s \u2217 i , s \u2217 j ) = k(s \u2217 i , s \u2217 j )\u2212 k \u1d40(s\u2217i ,S0)[K(S0,S0) + \u03c3yI] \u22121k(S0, s \u2217 j )\n(4)\nwhere K(S0,S0) is the Gram matrix with elements Kab = k(sa, sb). We will abbreviate K0 \u2261 [K(S0,S0) + \u03c3 2 yI] from here on. The co-vector k \u1d40(s\u2217,S0) has elements k \u1d40 i = k(s\n\u2217, si) and will be shortened to k0. The core concern here is, how does this belief change as time moves from \u03c4 to \u03c4 + dt? If dt _ 0, the chance of acquiring a datapoint y\u03c4 in this time is \u03bb dt. Marginalising over this Poisson stochasticity, we expect the mean after dt to be\n\u00b5\u03c4+ dt = \u03bbdt (k0, k\u03c4 ) ( K0 \u03be\u03c4 \u03be\u1d40\u03c4 \u03ba\u03c4 )\u22121( y0 y\u03c4 ) +(1\u2212\u03bb\u2212O(\u03bb2 dt))dt \u00b7k0K\u221210 y0 +O[(\u03bbdt)2] (5)\nwhere we have defined the map k\u03c4 = k(s\u2217, s\u03c4 ), the vector \u03be\u03c4 with elements \u03be\u03c4,i = k(si, s\u03c4 ), and the scalar \u03ba\u03c4 = k(s\u03c4 , s\u03c4 ) + \u03c32y . Algebraic re-formulation yields\n\u00b5\u03c4+ dt = k0K \u22121 0 y0 + \u03bb(kt \u2212 k0 \u1d40K\u221210 \u03bet)(\u03bat \u2212 \u03be \u1d40 tK \u22121 0 \u03bet) \u22121(yt \u2212 \u03be\u1d40tK\u221210 y0) dt. (6)\nNote that \u03be\u1d40\u03c4K \u22121 0 y0 is the mean prediction at s\u03c4 and (\u03ba\u03c4 \u2212 \u03be \u1d40 \u03c4K \u22121 0 \u03be\u03c4 ) is the marginal variance there. Hence, (\u03ba\u03c4 \u2212 \u03be\u1d40\u03c4K\u221210 \u03be\u03c4 )\u22121/2(y\u03c4 \u2212 \u03be \u1d40 \u03c4K \u22121 0 y0) \u223c N (0, 1) and\n(\u03ba\u03c4 \u2212 \u03be\u1d40\u03c4K\u221210 \u03be\u03c4 )\u22121/2(y\u03c4 \u2212 \u03be \u1d40 \u03c4K \u22121 0 y0) dt = d\u03c9 (7)\nwhere d\u03c9 is the Wiener [22] measure. So the change to the mean is the stochastic rate\nd\u00b5 = (m\u03c4+ dt \u2212m\u03c4 ) = \u03bb(k\u03c4 \u2212 k0\u1d40K\u221210 \u03be\u03c4 )(\u03ba\u03c4 \u2212 \u03be \u1d40 \u03c4K \u22121 0 \u03be\u03c4 ) \u22121/2 d\u03c9 \u2261 \u03bbL d\u03c9 (8)\n(where we have implicitly defined the innovation function L). A similar argument finds the change of the covariance function to be the deterministic rate\nd\u03a3t = \u2212\u03bbLL\u1d40 dt. (9)\nSo the dynamics of learning consist of a deterministic change to the covariance, and a stochastic change to the mean, itself a sample from a Wiener (Gaussian) process with covariance function LL\u1d40. This separation is a fundamental characteristic of GPs (it is the nonparametric version of a more straightforward corresponding notion for finite-dimensional Gaussian beliefs, for data with known noise magnitude), L d\u03c9 is known as the innovation process [e.g. 23]. Its significance here is that it allows a joint dynamic description of physical and learning system.\nWe introduce the belief-augmented spaceH containing states z(\u03c4) \u2261 [x(\u03c4), \u03c4, \u00b5\u03c4q (s), \u00b5\u03c4f1, . . . , \u00b5\u03c4fD, \u03a3\u03c4q (s, s \u2032),\u03a3\u03c4f1, . . . ,\u03a3 \u03c4 fD]. Note that the means and covariances are functions, hence H is infinitedimensional. Evidently, z(\u03c4) obeys the stochastic differential equation\ndz = (A(z) +B(z)u) dt+ C(z) d\u03c9 (10)\nwith the free dynamics A(z), the controlled dynamics Bu, and the noise matrix C(z) given by A(z) = [ f(zx, zt) , 1 , 0 , 0 , . . . , 0 , \u2212\u03bbLqL\u1d40q , \u2212\u03bbLf1L \u1d40 f1 , . . . , \u2212\u03bbLfDL \u1d40 fD ] (11)\nand B = [g(s\u2217), 0, 0, 0, . . . ] and C(z) = diag(0, 0, \u03bbLr, \u03bbLf1, . . . , \u03bbLfD, 0, . . . )\n(some readers may prefer an alternative rendering of Eq. (10) in Fokker-Planck form with drift A + Bu and diffusion 12CC\n\u1d40). We do not actually know f , of course, but the belief provides an expected value (the mean of the corresponding GPs). Replacing q and f or g in the following with their expected values is an approximation, but does not make the method myopic, because we will optimize the dynamics of this expected value, not the value itself). The value (discounted cost to go) of any state s\u2217 = (x\u2217, t\u2217) under the control u satisfies the Hamilton-Jacobi-Bellman equation, which, for the discounted setting of Eq. (28), reads [18]\n\u03b3\u22121v(z) = min u\n{ q(z) + 1\n2 u\u1d40R\u22121u+ (A(z) +B(z)u)\u1d40\u2207v + 1 2 tr [ C(z)\u1d40[\u22072v]C(z) ]} (12)\nwhere \u2207 is the (function space) gradient with respect to z, \u22072 the (function space) Hessian, tr the trace. Analytic minimisation over u bears\nu(z) = \u2212RB(z)\u1d40\u2207v(z) (13) and results in the optimal Hamilton-Jacobi-Bellman equation\n\u03b3\u22121v(z) = q +A\u1d40\u2207v \u2212 1 2 [\u2207v]\u1d40BRB\u1d40\u2207v + 1 2\ntr [ C\u1d40[\u22072v]C ] (14)\nA more explicit, novel form emerges upon re-inserting the definitions of Eq. (11) into Eq. (14):\n\u03b3\u22121v(z)\ufe38 \ufe37\ufe37 \ufe38 dv/dt = q(zx, zt)\ufe38 \ufe37\ufe37 \ufe38 \u2202v/\u2202t\n+ [ f(zx, zt)\u2207x +\u2207t ] v(z)\ufe38 \ufe37\ufe37 \ufe38\nfree drift\n\u2212 1 2 [\u2207xv(z)]\u1d40g\u1d40(zx, zt)Rg(zx, zt)\u2207xv(z)\ufe38 \ufe37\ufe37 \ufe38 control benefit\n+ \u2211\nc=q,f1,...,fD\n1 2 \u03bb2 tr\n[ diag(Lfd) \u1d40\u22072\u00b5fdv(z) diag(Lfd) ]\n\ufe38 \ufe37\ufe37 \ufe38 diffusion cost \u2212\u03bbLcL\u1d40c\u2207\u03a3cv(z)\ufe38 \ufe37\ufe37 \ufe38 exploration bonus (15)\nEquation (15) is the central result of this paper: For Gaussian process inference on nonlinear dynamic systems as defined in Section 2, optimal reinforcement learning, up to expectation, reduces to an infinite-dimensional quadratic differential equation, which can be interpreted as follows (labels in the equation, note the negative signs of \u201cbeneficial\u201d terms): The total time derivative of the value comprises the immediate utility rate q; the effect of free drift through space-time and the benefit of optimal control; as well as a diffusion cost engendered by the curvature of the belief mean, and an exploration bonus caused by the increase in certainty. Note that the first line of the right hand side of this equation only depends on the phase space-time subspace of the augmented space, while the second line only depends on the belief part of the augmented space. I will call the first line exploitation terms, the second line exploration terms, for the following reason: If the first line dominates the right hand side of Equation (15) in absolute size, then the controller is governed by the physical sub-space \u2014 it is exploiting its knowledge to control the physical system. On the other hand, if the second line dominates the value function, learning is more important than exploitation \u2014 the algorithm explores the physical space to optimize knowledge. As far as I understand, this relationship is the first exact quantitative statement about reinforcement learning\u2019s two objectives.\nSolving Equation (15) for v is nontrivial for two reasons: First, although the vector product notation may make the objects seem finite, the mean and covariance functions are of course infinitedimensional, and what looks like straightforward inner vector products are in fact integrals. For example, the exploration bonus for the reward, writ large, reads\n\u03bbLrL \u1d40 r\u2207\u03a3rv(z) = \u222b\u222b K \u03bbLr(s \u2217 i )Lr(s \u2217 j ) \u2202v(z) \u2202\u03a3(s\u2217i , s \u2217 j ) ds\u2217i ds \u2217 j . (16)\nFor general kernels k, these integrals may only be solved numerically. However, for at least one specific choice of kernel \u2014 square-exponentials \u2014 and parametric Ansatz, the required integrals can be solved in closed form. This analytic structure is so interesting, and the square-exponential kernel so widely used that, for the \u201cnumerical\u201d part of the paper (Section 4), I restrict the choice of kernel to square exponentials. The necessary derivations are a marginal contribution of this paper. Technical and lengthy, they can be found in Appendix C.\nThe other problem, of course, is that Equation (15) is a nontrivial differential Equation. Section 4 presents one, initial attempt at a numerical solution that should not be mistaken for a definitive answer. Despite all this, Eq. (15) constitutes a useful gain for Bayesian reinforcement learning: It replaces the intractable definition of the value in terms of future trajectories with a deterministic differential equation. This opens up a new family of approaches to reinforcement learning, based on numerical analysis rather than sampling.\nDigression: relaxing some assumptions\nThis paper only applies to the specific problem class of Section 2. Any generalisations and extensions are future work, and I do not claim to solve them. But it is instructive to consider some easier extensions, and some harder ones: If time is discrete, Equation (14) turns into a stochastic difference equation, and a finite horizon formulation becomes feasible, albeit at linearly higher cost. On the other hand, it is far from straightforward to simultaneously learn both g and f , if only the actual transitions are observed, because the beliefs over the two functions become strongly dependent when conditioned on data, and factorizing approximations have to be used. A nonlinear effect of the control (i.e. replacing g(x, t)u with g(x, t, u)) certainly requires further regularising assumptions. Otherwise, that case poses a harder form of the exploration exploitation trade-off, because the control itself governs the fidelity of information about g (in the absence of additional knowledge, it would be necessary to try arbitrarily large controls, just to see what they do). On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not \u201ccovered\u201d by the notion of optimality of the learning as addressed here. Another, unrelated, interesting direction concerns the use of other probabilistic models with known dynamics for non-Gaussian systems."}, {"heading": "4 Numerically solving the Hamilton-Jacobi-Bellman equation", "text": "Solving Equation (14) is principally a problem of numerical analysis, and a battery of numerical methods may be considered. In this section, I report on one specific Ansatz, for which I break with the generality of the previous Sections and assume that the kernels k are given by square exponentials k(a, b) = kSE(a, b; \u03b8, S) = \u03b82 exp(\u2212 12 (a\u2212 b)\n\u1d40S\u22121(a\u2212 b)) with parameters \u03b8, S. We find an approximate solution through a factorizing parametric Ansatz: Let the value of any point z \u2208 H in the belief space be given through a set of parametersw and some nonlinear functionals \u03c6, such that their contributions separate over phase space, mean, and covariance functions:\nv(z) = \u2211\ne=x,\u03a3r,\u00b5r,\u03a3f ,\u00b5f\n\u03c6e(ze) \u1d40we with \u03c6e,we \u2208 RNe (17)\nThis description is obviously restrictive, but it should be compared to the use of radial basis functions for function approximation, a similarly restrictive assumption widely used in reinforcement learning. The functionals \u03c6 have to be chosen such that they are conducive to the form of Eq. (15). If the\nkernels k of the beliefs are square exponentials, one convenient option is to choose\n\u03c6as(zs) = k(sz, sa; \u03b8a, Sa) (18)\n\u03c6b\u03a3(z\u03a3) = \u222b\u222b K [\u03a3z(s \u2217 i , s \u2217 j )\u2212 k(s\u2217i , s\u2217j )]k(s\u2217i , sb; \u03b8b, Sb)k(s\u2217j , sb; \u03b8b, Sb) ds\u2217i ds\u2217j and (19)\n\u03c6c\u00b5(z\u00b5) = \u222b K 1 2 \u00b52z(s \u2217)k(s\u2217, sc, \u03b8c, Sc) ds \u2217 (20)\n(the subtracted term in the first integral serves numerical purposes only). With this choice, the integrals of Equation (15) can be solved analytically (Appendix C). The approximate Ansatz turns Equation (15) into an algebraic equation quadratic in wx and linear in all other we:\n1 2 w\u1d40x\u03a8(zx)wx \u2212 q(zx) + \u2211 e=x,\u00b5q,\u03a3q,\u00b5f ,\u03a3f \u039ee(ze)we = 0 (21)\nusing co-vectors \u039e and a matrix \u03a8 with elements (Dirac\u2019s \u03b4 replaces the trace operation)\n\u039exa(zs) = \u03b3 \u22121\u03c6as(zs)\u2212 f(zx)\u1d40\u2207x\u03c6as(zs)\u2212\u2207t\u03c6as(zs)\n\u039e\u03a3a (z\u03a3) = \u03b3 \u22121\u03c6a\u03a3(z\u03a3) + \u03bb \u222b\u222b K L(s\u2217i )L(s \u2217 j ) \u2202\u03c6\u03a3(z\u03a3) \u2202\u03a3z(s\u2217i s \u2217 j ) ds\u2217i ds \u2217 j\n\u039e\u00b5a(z\u00b5) = \u03b3 \u22121\u03c6a\u00b5(z\u00b5)\u2212\n\u03bb2\n2 \u222b\u222b K \u03b4(s\u2217i \u2212 s\u2217j )L(s\u2217i )L(s\u2217j ) \u22022\u03c6a\u00b5(z\u00b5) \u2202\u00b5z(s\u2217i )\u2202\u00b5z(s \u2217 j ) ds\u2217i ds \u2217 j\n\u03a8(z)k` = [\u2207x\u03c6ks(z)]\u1d40g(zx)Rg(zx)\u1d40[\u2207x\u03c6`s(z)]\n(22)\nTo solve for w, we simply choose a sufficiently large number of evaluation points zeval to constrain the resulting system of quadratic equations, and then find the least-squares solution wopt by function minimisation, using standard methods, such as Levenberg-Marquardt [25]. A disadvantage of this approach is that is has a number of degrees of freedom \u0398, such as the kernel parameters, and the number and locations xa of the feature functionals. The experiments (Section 5) suggest that it is nevertheless possible to get interesting results simply by choosing these parameters heuristically.\nAlgorithm 1: Approximate Bayes-Optimal Learning Controller for Gaussian Systems Data: Observations Sq ,Yq ,Sf ,Yf for utilities and dynamics. Scales \u03b3,R, kq,kf ,hv , Noises \u03beq, \u03bef Result: Optimal Control u = \u2212R[g(s\u03c4 )]\u1d40\u2207x\u03c6x(s\u03c4 )wx\n1 begin 2 [\u0398, zeval] ^ GenerateNumericalBasis() ; // Heuristic. May be cached and reused 3 [Lq, Lf , q\u0302, f\u0302 ] ^ GPRegression(Sq, Yq, Sf , Yf) ; // Eq. (11) 4 [\u039e,\u03a8] ^ ConstructLinearMaps(Lq, Lf , q\u0302, f\u0302 ,\u0398, zeval) ; // Eq. (22), Appendix C 5 wopt ^ Minimize(\u2016\u039ew \u2212 r(zeval)\u2212 12w\u1d40\u03a8w\u20162) ; // standard problem"}, {"heading": "5 Experiments", "text": "I first apply the new method to a simple, one-dimensional environment, to demonstrate some aspects that are perhaps not obvious. This is then followed by a sample application, comparing to other algorithms."}, {"heading": "5.1 Illustrative experiment using an artificial environment", "text": "I constructed a simple example system in a one-dimensional state space by sampling f, q from the model described in Section 2, and setting g to the unit function, for simplicity. The state space was tiled regularly, in a bounded region, with 231 square exponential (\u201cradial\u201d) basis functions (Equation 39), initially all with weight wix = 0. For the information terms, only a single basis function was used for each term (i.e. one single \u03c6\u03a3q , one single \u03c6\u00b5q , and equally for f , all with very large length scales S, covering the entire region of interest). We will see below that this does not imply a trivial structure for these terms. Five times the number of parameters, i.e. Neval = 1175 evaluation points zeval were\nsampled, at each time step, uniformly over the same region. It is not intuitively clear whether each ze should have its own belief (i.e. whether the points should cover the belief space as well as the phase space), but anecdotal evidence of the experiments suggests that it suffices to use the current beliefs for all evaluation points. A more comprehensive evaluation of such aspects will be the subject of a future paper. The discount factor was set to \u03b3 = 50s, the sampling rate at \u03bb = 2/s, the control cost at 10m2/($s). Value and optimal control were evaluated at time steps of \u03b4t = 1/\u03bb = 0.5s.\nFigure 1 shows the situation 50s after initialisation (The supplement contains a more revealing video of a second initialisation, see detailed comments in Appendix A). The most noteworthy aspect is the nontrivial structure of exploration and exploitation terms. Despite the simple parameterisation of the corresponding functionals, the innovation function L(x, t) differs for every point in the phase space, and induces a complex shape that depends on the value function virtually everywhere else. The system constantly balances exploration and exploitation. This is an important insight that casts doubt on the usefulness of simple, local exploration boni, used in many reinforcement learning algorithms.\nSecondly, note that the system\u2019s trajectory does not necessarily follow what would be the optimal path under full information. The value estimate reflects this, by assigning low (good) value to regions behind the system\u2019s trajectory. This amounts to a sense of \u201cremorse\u201d: If the learner would have known about these regions earlier, it would have strived to reach them. But this is not a sign of sub-optimality: Remember that the value is defined on the augmented space. The plots in Figure 1 are merely a slice through that space at some level set in the belief space. The video in the supplementary material can be construed as a flight through this space along one learning trajectory.\n5.2 Comparative experiment \u2014 the Furuta pendulum\nThe cart-and-pole system is an under-actuated problem widely studied in reinforcement learning. For variation, I test the algorithm on its cylindrical version, the pendulum on the rotating arm [26]. In this numerical simulation, the task is to swing up the pendulum from the lower resting point. To emphasise efficient exploration, I set the discount scale to only \u03b3 = 5s. I compare the average loss of a controller with access to the true f, g, q, but otherwise using Algorithm 1, to that of an -greedy TD(\u03bb) learner with linear function approximation, Simpkins\u2019 et al.\u2019s [18] Kalman method and the Gaussian process learning controller (Fig. 2). None of these methods is free of assumptions; details on the setups can be found in the Appendix B. The GP method clearly outperforms the other two learners, which barely explore. Interestingly, none of the tested methods, not even the informed controller, achieve a stable controlled balance, although the GP learner does swing up the pendulum. This demonstrates a need for future research in more elaborate solution methods for the central result, Equation (15)."}, {"heading": "6 Conclusion", "text": "I have presented a nontrivial class of reinforcement learning problems for which the optimal balance of exploration and exploitation can be expressed compactly as a differential equation. To my knowledge, this is the first analytical statement about this tradeoff. It decouples the problem of optimal reinforcement learning from the difficult prediction of future trajectories, replacing it with the problem of solving a differential equation, for which a considerable body of prior work is available. For some intuition into how such solutions might work, I have presented one specific approximation, using functionals to reduce the problem to finite least-squares parameter estimation.\nThe class of systems for which this method applies is not arbitrarily general, but provides reasonable descriptions for a wide range of physical systems. It even extends on some assumptions of classic reinforcement learning, e.g. by allowing dynamics and reward expectations to change over time. There are two arguments for \u201cstructured\u201d, probabilistic, approaches like the one presented here. The first is the immense complexity of the general reinforcement learning problem, which makes it unlikely that efficient \u2014 in the probabilistic sense \u2014 yet universal reinforcement learning algorithms for realistic systems will be found in the foreseeable future. The other is the impressive collection of successes that control theory has historically had using structured models.\nThis work raises new questions, theoretical and applied, for future research. From the theoretical perspective, an intriguing query is how computational complexity interacts with prior assumptions: What is the class of reinforcement learning problems that can be solved in polynomial time? Another concern is the utility of approximate inference methods for expanding this class. Regarding the applied viewpoint, I have (superficially) studied only one approach to solving the Hamilton-JacobiBellman equation, and an empirical evaluation has identified some of its weaknesses. More options are available. Open research questions include the trade-off between performance and computational cost of such methods, and the viability of policy search methods in this context."}, {"heading": "Acknowledgments", "text": "I would like to thank Carl E. Rasmussen and Jan Peters for helpful discussions. This project was funded through a fellowship of the Max Planck Society.\nAppendix"}, {"heading": "A Details on the supplementary animation", "text": "As described in the paper, the environment for this experiment was sampled from the model: A square exponential kernel with time length-scale St = 10s and space length-scale Sx = 5m was used for both q and f (the latter is not shown in the plots). The discount factor was \u03b3 = 50s, the sampling rate \u03bb = 2/s, the control cost 10m2/($s). The trajectory shown in Figure 1 of the paper and the one shown in the animation are both on the same environment, but generated from separate initialisations (because the data is stochastic, trajectories differ in each initialisation).\nThe six panels of the animation all show the same system, from different points of view. In each panel, time is on the abscissa, space on the ordinate. The third, the \u201cmovie-dimension\u201d, shows the development throughout the learning and control process. In each panel, the current state of the system is shown as a big black diamond with green border. In the top left panel, the trajectory of the system is also denoted with black dots.\nThe movie shows two \u201cepisodes\u201d. That is, the system is run from t = 0, x = 0 to t = 100 once, then placed back at t = 0, x = 0. The point of this somewhat artificial setup is to demonstrate the behaviour of the controller in regions of non-uniform prior knowledge. The following list describes the individual panels, and points out some interesting observations:\nTop Left: Loss Belief Mean belief over q as color, uncertainty (marginal variance) as transparency. This panel shows that the controller is doing a meaningful job of controlling the system, by avoiding (red) regions of high loss, and spending more time in (blue) regions of low loss.\nTop Middle: Exploitation Terms the exploitation terms, from Equation (15) in the main paper, are q(zx, zt) + [ f(zx, zt)\u2207x +\u2207t ] v(z)\u2212 1\n2 [\u2207xv(z)]\u1d40g\u1d40(zx, zt)Qg(zx, zt)\u2207xv(z) (23)\nevaluated at every point in the plot. The structure of Equation (23) is reflected in the plot, which roughly traces the structure of the top left (q) plot, but with slightly broader structure, effected by the free and control dynamics\nTop Right: Exploration Terms the exploration terms, also from Equation (15), are + \u2211\nc=q,f1,...,fD\n1 2 \u03bb2 tr\n[ diag(Lfd) \u1d40\u22072\u00b5fdv(z) diag(Lfd) ] \u2212 \u03bbLcL\u1d40c\u2207\u03a3cv(z) (24)\nThe relationship between exploration and exploitation terms may be the most interesting aspect of the animation. First, note the nontrivial structure of both these panels. This structure is inherent from the HJB equation, and not created by the finite-dimensional approximation (the exploration terms are fitted with only four free parameters w\u00b5q, w\u03a3q, w\u00b5f , w\u03a3f , for the entire plot). In particular, it is striking that the exploration terms are not just a \u201ctube\u201d around the explored regions, but that they also depend on the curvature of the mean beliefs on q and f , leading to a comparably complex picture. This is an important insight, because the reinforcement learning literature has often presented the exploration-exploitation tradeoff as a kind of binary heuristic problem of pure exploration vs. pure exploitation (witness the widely used -greedy policy, which randomly switches between the two behaviours). The top middle and right panels show that exploration and exploitation depend in complex and not entirely local ways on the current beliefs about the system.\nBottom Left: Value Estimate This panel shows the current value estimate v(z) = \u2211\nc=x,\u03a3r,\u00b5r,\u03a3f ,\u00b5f\n\u03c6c(zc) \u1d40wc with \u03c6c,wc \u2208 RNc (25)\nNote that the value under the prior (first frame) is entirely flat. This amounts to a boundary condition on the HJB equation. The panel also reveals some weaknesses of the parametric approach used here: Its structure is not sufficiently fine-grained everywhere to capture the full structure of the true value function. This also leads to non-optimal behaviour, e.g. towards the end of the second episode. See also the next panel.\nBottom Middle: Value Target The \u201cvalue target\u201d is given by the right hand side of the HJB equation, i.e. by the sum of exploration and exploitation terms. Note that these terms also depend on the parametric approximation, so they are not a bona fide target function \u2014 however, in an exact solution of the HJB equation, this panel would be identical to the \u201cvalue estimate\u201d panel. An interesting aspect of this panel is that it generally has finer structure than the value estimate. And interesting question for future research is whether using this function actually provides a better controller than the value estimate.\nBottom Right: Value Error This panel shows the square error between the left hand side of the HJB equation, and the right hand side of the equation (note that this is not exactly the same as the square difference between the bottom left and bottom right panels, but only up to a factor of 1/\u03b32, explaining the different scales of these plots. If this panel were uniform 0 everywhere in the augmented space (i.e. throughout the movie), an exact solution to the HJB equation would have been found. The experiment suggests that the approximation process works relatively well, although there are some regions of imprecise modelling. Interestingly, these regions tend to lie close to the current state of the controller (because the curvature of the mean functions is usually particularly high close to data points). It is tempting to try to correct for this in some ad-hoc way, e.g. by putting more weight on the modelling error close to the current state. But it is unclear whether doing so really improves performance. Good value estimates, and thus good control, often have more to do with specific regions of the phase space, which need not lie close to the current location. The second experiment, the Furuta pendulum, is an example of such a case, where good control depends on whether a small manifold in the phase space (the one leading to an upright balanced state) is captured well in the value function.\nOverall, the animation shows the system expressing relatively complex behaviour, both exploring and exploiting. E.g. around half way through the second episode, the system \u201cdecides\u201d that the region it is currently exploring is, as it were, \u201cnot worth the effort\u201d and jumps back, through a region of high loss, towards a region of certain low cost."}, {"heading": "B Comparative experiments: Furuta pendulum", "text": "In this simulation, the pendulum is modelled as two massive rods of masses m1 = 0.5kg (arm) and m2 = 2kg (pendulum) and lengths `1 = 0.5m, `2 = 0.25m. The exact forms of the dynamics can be found in Fantoni and Rogelio [19]. I chose the loss function to be given by\nq(x) = 3 \u00b7 ( 1\n2 \u2212 exp\n( \u22121\n2 d\u1d40D\u22121d\n)) (26)\nwhere d = [dtip, \u03b8\u03071, \u03b8\u03072]\u1d40 contains dtip, the absolute distance of the tip of the pendulum from the upright position, and the angular velocities of arm and pendulum. The scaling matrix is D = [1m, 25/s, 25/s]2. The advantage of this form of loss function is that it has roughly the right scale (-3,3) to be learned from a Gaussian process prior with zero mean and unit covariance function. Since this reward function does not contain \u03b81, the problem is spherically symmetric, and the phase space is 3-dimensional. To capture the rotational symmetry of the problem for the learning algorithms, I embed the 3D phase manifold in a 4-dimensional space by choosing the coordinate system x = [sin(\u03b82), cos(\u03b82), \u03b8\u03071, \u03b8\u03072]. The discount scale was set to \u03b3 = 5s, the sampling rate to \u03bb = 50/s.\nEach of the methods in the comparison relies on a number of approximations with free parameters. In an effort to keep the results comparable, these parameters were chosen similarly for each method, where applicable. All methods parameterised the value function using the radial basis function features of Equation (22) in the main paper, their centres distributed over a regular grid with 232 loci, with widths [0.5, 0.5, 5, 10]. The Kalman and Gaussian process methods also require a tiling for the\nbelief space. To keep analogy with Equation (22) in the paper, the Kalman method sums over the current covariance measures. The Gaussian process learner uses the functionals of Equation (22) in the paper, with very large width. The corresponding concept for the TD learner is the parameter of the -greedy policy, which was set to 10%. The TD learner also needs a learning rate, which was set to decay like 1/n where n is the number of data points. The corresponding, albeit more powerful, concept for the Bayesian learners is the kernel similarity measure, which were square exponential kernels for both f and q, with width parameters chosen to roughly represent the actual length scales of these functions.\nIn each experiment, the pendulum starts from the lower resting position. None of the methods succeeded in balancing the pendulum well. Nevertheless, the Gaussian Process controller did learn to swing up the pendulum and enter a limit cycle with a slowly rotating pendulum in two out of five experiments, and to speed up the pendulum to fly roughly horizontally in the other three. These variations also lead to the relatively large variance noted in the Table in the paper\u2019s Figure (2). The informed controller showed a more reproducible yet similar behaviour, always speeding up the pendulum to fly at half height. The other two learning methods did not explore enough to reach any interesting states. TD\u2019s main problem is that it is inherently a discrete time method, so there is no concept of \u201cguided\u201d exploration over a finite time. The Kalman method seemed to suffer from the inadequacy of its linear model as a good description of this nonlinear system. (Note that, in the original paper by Simpkins et al. [18], this method was used only as a tracking controller).\nApart from the not overly surprising fact that the more elaborate learning method performed better than its competitors, the main takeaway of these experiments is that the approach of globally covering the phase space with basis functions of uniform precision is flawed. Intuitively, a good approximate value function for this problem requires high resolution around the upper balanced position, and along a manifold of trajectories that lead to it. It is also clear that the information required for such a focusing of the descriptive alphabet is contained in the mean and covariance beliefs used in this paper.\nThere are of course many other reinforcement learning methods to potentially compare to. However, many of them do not apply to continuous time systems. Others do not try to balance exploration and exploitation. This makes them uninteresting for this comparison, but interesting candidates for combination with this framework. One such method is that of Deisenroth and Rasmussen [27], which is a purely greedy method, but uses a Gaussian Process forward model for optimization."}, {"heading": "C Mathematical appendix", "text": "The main paper, solving a problem with several different conceptual dimensions, required rather dense notation. In this Appendix, more straightforwardly, the focus lies on solving a series of integrals, and notation should be as clean as possible. To this end, we will drop irrelevant indices, and also introduce some new, more compact notation. This may sometimes come at the cost of some variation between the notations in the two texts, but makes it much easier to parse the following derivations. In particular, we will not treat time as a special dimension of the phase space any more, and instead denote phase space-time coordinates by x \u2208 K = RD, with a new definition for D as Dappendix = Dpaper + 1.\nC.1 Preliminaries\nThe derivations in this text, at their core, all rely on the Gaussian integral.\u222b exp ( \u22121\n2 (x\u2212 a)\u1d40C\u22121(x\u2212 a)\n) dx = \u221a (2\u03c0)D|C| (27)\nWe will use, throughout, kernels k : K \u00d7K_ R in the square exponential (SE) class [21]:\nk(xi, xj ; \u03b8, S) = \u03b8 2 k exp\n( \u22121\n2 (xi \u2212 xj)S\u22121(xi \u2212 xj)\n) (28)\nwith a strength parameter \u03b8k and a scale S. We will adopt the notation from Rasmussen and Williams [21], where k(a, b) denotes a row vector \u2208 R1\u00d7dim b and similarly k(a, b) \u2208 Rdim a\u00d7dim b. The\nfollowing integrals will feature heavily in the derivations. They can all be derived straightforwardly from Equation (27), and hold for all a, b, c, d \u2208 K\u222b\nk(x, a; \u03b8, S) dx = \u03b82 \u221a (2\u03c0)D|S| = k ( a, a; \u03b8 4 \u221a (2\u03c0)D|S|, I ) (29)\u222b\nk(x, a; \u03b81, S1)k(x, b; \u03b82, S2) dx = k (a, b;u2, S1 + S2) (30)\u222b k(x, a; \u03b81, S1)k(x, b; \u03b81, S1)k(x, c; \u03b82, S2) d = k(a, b, 1, 2S1)k ( c, 1\n2 (a+ b);u3,\n1 2 S1 + S2 ) (31)\nNote that the third line is a special case (containing \u03b81, S1 twice). The general case is just as easy, but not needed here. The constants are\nu2 = \u03b81\u03b82 ( (2\u03c0)D|S1S2| |S1 + S2| )1/4 (32)\nu3 = \u03b8 2 1\u03b82 ( (2\u03c0)D|S1S1S2| | 12S1 + S2| )1/4 (33)\nFinally, the notation in this Appendix will make frequent use of the summation convention: Indices showing up in at least two terms of a product should be understood as being summed over, unless they also feature only once on the other side of the equation.\nC.2 Square-exponential kernels on beliefs\nRecall that the experiments in the paper use the following parameterisation of the value function: v(z) = \u2211\ni=x,\u03a3r,\u00b5r,\u03a3f ,\u00b5f\n\u03c6i(zi) \u1d40wi with \u03c6i,wi \u2208 RNi (34)\nwith the functions / functionals\n\u03c6ax(zx) = k(xz, xa; \u03b8a, Sa) (35)\n\u03c6b\u03a3(z\u03a3) = \u222b\u222b K [\u03a3z(x \u2217 i , x \u2217 j )\u2212 k(x\u2217i , x\u2217j )]k(x\u2217i , xb; \u03b8b, Sb)k(x\u2217j , xb; \u03b8b, Sb) dx\u2217i dx\u2217j and (36)\n\u03c6c\u00b5(z\u00b5) = \u222b K 1 2 \u00b52z(x \u2217)k(x\u2217, xc, \u03b8c, Sc) dx \u2217 (37)\nfor simplicity, we choose Sa = diag(s2) for all phase space basis functions (the dense generalisation is not difficult, but tedious). As mentioned in the paper, these choices allow analytic representation of the differential terms in the Hamilton-Jacobi-Bellman Equation, as detailed in the following paragraphs.\nC.2.1 Phase space gradients\nThe gradient\u2019s elements with respect to the phase space dimensions is\n\u2202\u03c6ax \u2202xi = \u2212xi \u2212 x a i s2i \u03c6ax(x). (38)\nHence the phase space terms of the Hamilton Jacobi Bellman equation of the augmented system are (note the summations over i and a)\n[f(x), 1]\u2207xv(z) = \u2212fi(x) xi \u2212 xai s2i k(x, xa; \u03b8a, Sa)wa (39)\nC.2.2 Terms in \u03a3(x\u2217i , x\u2217j )\nThe parametrized HJB equation contains two types of terms involving the covariance function \u03a3. The term in the direct time derivative of the value is exactly Equation (36). For its evaluation, we introduce a compact notation. Recall that the augmented state z consists of the current phase space-time x and a belief with mean and covariance functions. Both these functions are induced by the kernel and a dataset (Xz,Y z) \u2208 (RD \u00d7 R)Nz and may be evaluated at any point x\u2217 \u2208 K. The following derivations will repeatedly contain certain objects: Kernel projections into the evaluation space, of the current location: kx \u2261 k(x\u2217, x); and of the dataset kX \u2261 k(x\u2217,X). Kernel values: between location and itself kxx \u2261 k(x, x), between location and dataset kxX \u2261 k(x,X) (and its transpose kXx), and the inverse of the Gram matrix K\u22121 \u2261 [k(X,X) + \u03c32I]\u22121. With these notational shortcuts, we can evaluate Equation (36) to\n\u03c6b\u03a3(z)wb = \u222b\u222b K kX(x \u2217 i ) \u1d40K\u22121kX(x \u2217 j )k(x \u2217 i , xb; \u03b8b, Sb)k(x \u2217 j , xb; \u03b8b, Sb) dx \u2217 i dx \u2217 jwb\n= k(xb,X;u2, Sz + Sb)K \u22121k(X, xb;u2, Sz + Sb)wb\n(40)\nwhere \u03b8z, Sz are the kernel parameters of the applicable GP belief over the phase space (i.e. the one for q, or any dimension of f or g).\nWe use the definition of the innovation term L from the paper, to find that the exploration bonus terms, evaluates to\n\u2212\u03bbLL\u1d40\u2207\u03a3\u03c6b\u03a3(z)wb = \u2212 \u222b\u222b\nL(x\u2217i )L(x \u2217 j )k(x \u2217 i , xb; \u03b8b, Sb)k(x \u2217 j , xb; \u03b8b, Sb) dx \u2217 i dx \u2217 j \u00b7 wb\n= \u2212 \u03bb kxx \u2212 kxXK\u22121kXx\n[\u222b [kx(x \u2217)\u2212 kX(x\u2217)K\u22121kXx]k(x\u2217; \u03b8b, Sb) dx\u2217 ]2 \u00b7 wb\n= \u2212\u03bb [k(x, xb;u2, Sz + Sb)\u2212 k(xb,X;u2, Sz + Sb)K \u22121kXx] 2\nkxx \u2212 kxXK\u22121kXx \u00b7 wb\n(41)\nC.2.3 Terms in \u00b5(x\u2217)\nThe mean-belief term in the direct time derivative is given by Equation (37). This time, the notation becomes clearer when the elements ofX are explicitly mentioned in the summation, as Xi:\n\u03c6\u00b5(z) cwc =\n1\n2\n\u222b k(x\u2217, Xi)k(x \u2217, Xj)(K \u22121Y )i(K \u22121Y )jk(x \u2217, xc; \u03b8c, Sc) dx \u2217 \u00b7 wc\n= 1\n2 (K\u22121Y )ik(Xi, Xj ; 1, 2Sz)k(x\nc, 1\n2 (Xi +Xj), u3; Sz 2 + Sc)(K \u22121Y )jwc\n(42)\nThe computational cost of evaluating this term, assuming that K\u22121 is already available, is O(N2zNc), where Nc is the number of functional-parameter pairs used to model the effect of the mean. The diffusion cost term, taken from the paper, is (note that k(a, a, 1, C) = 1 \u2200a,C)\n\u2212 \u03bb 2\n2\n\u222b\u222b \u03b4(x\u2217i \u2212 x\u2217j )L(x\u2217i )L(x\u2217j )\n\u22022\u03c6c\u00b5(z\u00b5)\n\u2202\u00b5z(x\u2217i )\u2202\u00b5z(x \u2217 j )\ndx\u2217i dx \u2217 j \u00b7 wc\n= \u2212\u03bb2wc\n2[kxx \u2212 kxXK\u22121kXx]\n\u222b [kxkx\u22122kxkXK\u22121kXx+kxXK\u22121kXk\u1d40XK \u22121kXx]k(x \u2217, xc; \u03b8c, Sc)dx \u2217\n= \u2212 \u03bb 2wc\n2[kxx \u2212 kxXK\u22121kXx]\n[ k(x, xc;u3,\nSz 2 + Sc)\n\u2212 2k(x,X; 1, 2Sz)k(xc \u2212 1\n2 x,\n1 2 X;u3, Sz 2 + Sc)K \u22121kXx\n+ kxXK \u22121k(X,X; 1, 2Sz)k(xc \u2212\n1 2 X, 1 2 X;u3, Sz 2 + Sc)K \u22121kXx\n] (43)\nthe summation of scalar and vector in the last line should be understood such that xc is subtracted from every element ofX ."}], "references": [{"title": "Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "On the likelihood that one unknown probability exceeds another in view of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, 25:275\u2013294", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1933}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M.O.G. Duff"], "venue": "PhD thesis, U of Massachusetts, Amherst", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pages 697\u2013704", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model based Bayesian exploration", "author": ["Richard Dearden", "Nir Friedman", "David Andre"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "International Conference on Machine Learning, pages 956\u2013963", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M.L. Littman", "A. Nouri", "D. Wingate"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th International Conference on Machine Learning. Morgan Kaufmann", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6(1):4\u201322", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47(2):235\u2013256", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, 49 (2):209\u2013232", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "R-max \u2014 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research, 3:213\u2013231", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences, 74(8):1309\u20131331", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-armed bandits in metric spaces", "author": ["R. Kleinberg", "A. Slivkins", "E. Upfal"], "venue": "Proc. ACM Symposium on Theory of Computing, pages 681\u2013690", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Linearly-solvable Markov decision problems", "author": ["E. Todorov"], "venue": "Advances in Neural Information Processing Systems, 19", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to stochastic control theory", "author": ["H.J. Kappen"], "venue": "path integrals and reinforcement learning. In 9th Granada seminar on Computational Physics: Computational and Mathematical Modeling of Cooperative Behavior in Neural Systems., pages 149\u2013181", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimal trade-off between exploration and exploitation", "author": ["A. Simpkins", "R. De Callafon", "E. Todorov"], "venue": "American Control Conference, 2008, pages 33\u201338", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-linear Control for Underactuated Mechanical Systems", "author": ["I. Fantoni", "L. Rogelio"], "venue": "Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1973}, {"title": "Dual control theory", "author": ["A.A. Feldbaum"], "venue": "Automation and Remote Control,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1961}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential space", "author": ["N. Wiener"], "venue": "Journal of Mathematical Physics, 2:131\u2013174", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1923}, {"title": "An innovations approach to least-squares estimation \u2014 part I: Linear filtering in additive white noise", "author": ["T. Kailath"], "venue": "IEEE Transactions on Automatic Control, 13(6):646\u2013655", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1968}, {"title": "Slice sampling covariance hyperparameters of latent Gaussian models", "author": ["I. Murray", "R.P. Adams"], "venue": "arXiv:1006.0868", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "An algorithm for least-squares estimation of nonlinear parameters", "author": ["D.W. Marquardt"], "venue": "Journal of the Society for Industrial and Applied Mathematics, 11(2):431\u2013441", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1963}, {"title": "Swing-up control of inverted pendulum using pseudo-state feedback", "author": ["K. Furuta", "M. Yamakita", "S. Kobayashi"], "venue": "Journal of Systems and Control Engineering, 206(6):263\u2013269", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M.P. Deisenroth", "C.E. Rasmussen"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as \u201c -greedy\u201d [1], or \u201cThompson sampling\u201d [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "Many classic reinforcement learning algorithms thus rely on ad-hoc methods to control exploration, such as \u201c -greedy\u201d [1], or \u201cThompson sampling\u201d [2].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "However, at least since a thesis by Duff [3] it has been known that Bayesian inference allows optimal balance between exploration and exploitation.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "The trouble is that this amounts to optimization and integration over a tree of exponential cost in the size of the state space [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 5, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 6, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 7, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 75, "endOffset": 87}, {"referenceID": 8, "context": "Several authors have proposed approximating this lookahead through samples [5, 6, 7, 8], or ad-hoc estimators that can be shown to be in some sense close to the Bayes-optimal policy [9].", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 10, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 11, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 12, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 13, "context": "When the state space is finite and discrete, bound-based reasoning is possible, which can guarantee that, at least, the resulting algorithms always over-explore, never under-explore [10, 11, 12, 13, 14].", "startOffset": 182, "endOffset": 202}, {"referenceID": 14, "context": "But bound-based algorithms can not be extended to continuous spaces without making further assumptions [15], and doing so invalidates the strongest argument in their favour \u2014 that they are free of assumptions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "In a parallel development, recent work by Todorov [16], Kappen [17] and others has introduced an idea into reinforcement learning that has long been commonplace in other areas of machine learning: That structural assumptions, while restrictive, can greatly simplify inference problems.", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "[18] showed that it is actually possible to solve for the exploration exploitation tradeoff locally, by constructing a linear approximation for the system using a Kalman filter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "(1) still covers numerous physical systems studied in control, for example many mechanical systems, from classics like cart-and-pole to realistic models for helicopters [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "From a control-theoretic standpoint, the optimal solution to the exploration exploitation tradeoff is formed by the dual control [20] of a joint representation of both the physical system and the learning machine used to model it.", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.", "startOffset": 96, "endOffset": 102}, {"referenceID": 3, "context": "In reinforcement learning, this representation has come to be known as a belief-augmented POMDP [3, 4], but is not usually construed as a control problem.", "startOffset": 96, "endOffset": 102}, {"referenceID": 20, "context": "For arbitrary points s\u2217 = (x\u2217, t\u2217) \u2208 K, the belief over q(s\u2217) is a Gaussian with mean function \u03bc\u03c4 , and co-variance function \u03a3\u03c4 [21]", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "where d\u03c9 is the Wiener [22] measure.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "(28), reads [18]", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not \u201ccovered\u201d by the notion of optimality of the learning as addressed here.", "startOffset": 141, "endOffset": 149}, {"referenceID": 23, "context": "On the question of learning the kernels for Gaussian process regression on q and f or g, it is clear that standard ways of inferring kernels [21, 24] can be used without complication, but that they are not \u201ccovered\u201d by the notion of optimality of the learning as addressed here.", "startOffset": 141, "endOffset": 149}, {"referenceID": 24, "context": "To solve for w, we simply choose a sufficiently large number of evaluation points zeval to constrain the resulting system of quadratic equations, and then find the least-squares solution wopt by function minimisation, using standard methods, such as Levenberg-Marquardt [25].", "startOffset": 270, "endOffset": 274}, {"referenceID": 25, "context": "For variation, I test the algorithm on its cylindrical version, the pendulum on the rotating arm [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "\u2019s [18] Kalman method and the Gaussian process learning controller (Fig.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "The exact forms of the dynamics can be found in Fantoni and Rogelio [19].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "[18], this method was used only as a tracking controller).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "One such method is that of Deisenroth and Rasmussen [27], which is a purely greedy method, but uses a Gaussian Process forward model for optimization.", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "We will use, throughout, kernels k : K \u00d7K_ R in the square exponential (SE) class [21]:", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "We will adopt the notation from Rasmussen and Williams [21], where k(a, b) denotes a row vector \u2208 R1\u00d7dim b and similarly k(a, b) \u2208 R a\u00d7dim .", "startOffset": 55, "endOffset": 59}], "year": 2017, "abstractText": "The exploration-exploitation tradeoff is among the central challenges of reinforcement learning. A hypothetical exact Bayesian learner would provide the optimal solution, but is intractable in general. I show that, however, in the specific case of Gaussian process inference, it is possible to make analytic statements about optimal learning of both rewards and transition dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics. The solution is described by an infinite-dimensional differential equation. For a first impression of how this result may be useful, I also provide an approximate reduction to a finite-dimensional problem, with a numeric solution.", "creator": "TeX"}}}