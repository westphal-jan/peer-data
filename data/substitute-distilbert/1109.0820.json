{"id": "1109.0820", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2011", "title": "ShareBoost: Efficient multiclass learning with feature sharing", "abstract": "multiclass prediction is the problem of classifying some object encompassing a relevant target class. the consider the problem of performing a multiclass predictor that uses only few features, having in particular, one number of complementary features should increase sub - linearly with the number of possible classes. this implies that features should be shared multiple several classes. we describe and analyze the shareboost algorithm for learning a multiclass predictor that uses few shared features. we prove that shareboost efficiently finds a predictor that uses these shared features ( if such a predictor exists ) and that object has a small generalization error. we also describe how to use shareboost they design a non - linear predictor that has a fast evaluation time. in a series of attempts with natural data sets algorithms demonstrate commercial benefits of shareboost and reduce its success relatively to purely state - of - the - line approaches.", "histories": [["v1", "Mon, 5 Sep 2011 07:52:17 GMT  (156kb,D)", "http://arxiv.org/abs/1109.0820v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV stat.ML", "authors": ["shai shalev-shwartz", "yonatan wexler", "amnon shashua"], "accepted": true, "id": "1109.0820"}, "pdf": {"name": "1109.0820.pdf", "metadata": {"source": "META", "title": "ShareBoost: Efficient Multiclass Learning with Feature Sharing", "authors": ["Shai Shalev-Shwartz", "Yonatan Wexler"], "emails": ["shais@cs.huji.ac.il", "yonatan.wexler@orcam.com", "shashua@cs.huji.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Learning to classify an object into a relevant target class surfaces in many domains such as document categorization, object recognition in computer vision, and web advertisement. In multiclass learning problems we use training examples to learn a classifier which will later be used for accurately classifying new objects. Typically, the classifier first calculates several features from the input object and then classifies the\nA short version of this manuscript will be presented at NIPS, Dec. 2011. Part of this work was funded by ISF 519/09. A.S. is on sabbatical from the Hebrew University.\nobject based on those features. In many cases, it is important that the runtime of the learned classifier will be small. In particular, this requires that the learned classifier will only rely on the value of few features.\nWe start with predictors that are based on linear combinations of features. Later, in Section 4, we show how our framework enables learning highly non-linear predictors by embedding non-linearity in the construction of the features. Requiring the classifier to depend on few features is therefore equivalent to sparseness of the linear weights of features. In recent years, the problem of learning sparse vectors for linear classification or regression has been given significant attention. While, in general, finding the most accurate sparse predictor is known to be NP hard (Natarajan, 1995; Davis et al., 1997), two main approaches have been proposed for overcoming the hardness result. The first approach uses `1 norm as a surrogate for sparsity (e.g. the Lasso algorithm (Tibshirani, 1996) and the compressed sensing literature (Candes & Tao, 2005; Donoho, 2006)). The second approach relies on forward greedy selection of features (e.g. Boosting (Freund & Schapire, 1999) in the machine learning literature and orthogonal matching pursuit in the signal processing community (Tropp & Gilbert, 2007)).\nA popular model for multiclass predictors maintains a weight vector for each one of the classes. In such case, even if the weight vector associated with each class is sparse, the overall number of used features might grow with the number of classes. Since the number of classes can be rather large, and our goal is to learn a model with an overall small number of features, we would like that the weight vectors will share the features with non-zero weights as much as possible. Organizing the weight vectors of all classes as rows of a single matrix, this is equivalent to requiring sparsity of the columns of the matrix.\nar X\niv :1\n10 9.\n08 20\nv1 [\ncs .L\nG ]\n5 S\nep 2\nIn this paper we describe and analyze an efficient algorithm for learning a multiclass predictor whose corresponding matrix of weights has a small number of non-zero columns. We formally prove that if there exists an accurate matrix with a number of non-zero columns that grows sub-linearly with the number of classes, then our algorithm will also learn such a matrix. We apply our algorithm to natural multiclass learning problems and demonstrate its advantages over previously proposed state-of-the-art methods.\nOur algorithm is a generalization of the forward greedy selection approach to sparsity in columns. An alternative approach, which has recently been studied in (Quattoni et al., 2009; Duchi & Singer, 2009), generalizes the `1 norm based approach, and relies on mixednorms. We discuss the advantages of the greedy approach over mixed-norms in Section 1.2."}, {"heading": "1.1. Formal problem statement", "text": "Let V be the set of objects we would like to classify. For example, V can be the set of gray scale images of a certain size. For each object v \u2208 V, we have a pool of predefined d features, each of which is a real number in [\u22121, 1]. That is, we can represent each v \u2208 V as a vector of features x \u2208 [\u22121, 1]d. We note that the mapping from v to x can be non-linear and that d can be very large. For example, we can define x so that each element xi corresponds to some patch, p \u2208 {\u00b11}q\u00d7q, and a threshold \u03b8, where xi equals 1 if there is a patch of v whose inner product with p is higher than \u03b8. We discuss some generic methods for constructing features in Section 4. From this point onward we assume that x is given.\nThe set of possible classes is denoted by Y = {1, . . . , k}. Our goal is to learn a multiclass predictor, which is a mapping from the features of an object into Y. We focus on the set of predictors parametrized by matrices W \u2208 Rk,d that takes the following form:\nhW (x) = argmax y\u2208Y (Wx)y . (1)\nThat is, the matrix W maps each d-dimensional feature vector into a k-dimensional score vector, and the actual prediction is the index of the maximal element of the score vector. If the maximizer is not unique, we break ties arbitrarily.\nRecall that our goal is to find a matrix W with few non-zero columns. We denote by W\u00b7,i the i\u2019th column of W and use the notation\n\u2016W\u2016\u221e,0 = |{i : \u2016W\u00b7,i\u2016\u221e > 0}|\nto denote the number of columns of W which are not\nidentically the zero vector. More generally, given a matrix W and a pair of norms \u2016 \u00b7 \u2016p, \u2016 \u00b7 \u2016r we denote \u2016W\u2016p,r = \u2016(\u2016W\u00b7,1\u2016p, . . . , \u2016W\u00b7,d\u2016p)\u2016r, that is, we apply the p-norm on the columns of W and the r-norm on the resulting d-dimensional vector.\nThe 0\u22121 loss of a multiclass predictor hW on an example (x, y) is defined as 1[hW (x) 6= y]. That is, the 0\u22121 loss equals 1 if hW (x) 6= y and 0 otherwise. Since this loss function is not convex with respect to W , we use a surrogate convex loss function based on the following easy to verify inequalities:\n1[hW (x) 6= y] \u2264 1[hW (x) 6= y]\u2212 (Wx)y + (Wx)hW (x) \u2264 max\ny\u2032\u2208Y 1[y\u2032 6= y]\u2212 (Wx)y + (Wx)y\u2032\n(2) \u2264 ln \u2211 y\u2032\u2208Y e1[y \u2032 6=y]\u2212(Wx)y+(Wx)y\u2032 . (3)\nWe use the notation `(W, (x, y)) to denote the righthand side (eqn. (3)) of the above. The loss given in eqn. (2) is the multi-class hinge loss (Crammer & Singer, 2003) used in Support-Vector-Machines, whereas `(W, (x, y)) is the result of performing a \u201csoftmax\u201d operation: maxx f(x) \u2264 (1/p) ln \u2211 x e\npf(x), where equality holds for p\u2192\u221e.\nThis logistic multiclass loss function `(W, (x, y)) has several nice properties \u2014 see for example (Zhang, 2004). Besides being a convex upper-bound on the 0\u22121 loss, it is smooth. The reason we need the loss function to be both convex and smooth is as follows. If a function is convex, then its first order approximation at any point gives us a lower bound on the function at any other point. When the function is also smooth, the first order approximation gives us both lower and upper bounds on the value of the function at any other point1. ShareBoost uses the gradient of the loss function at the current solution (i.e. the first order approximation of the loss) to make a greedy choice of which column to update. To ensure that this greedy choice indeed yields a significant improvement we must know that the first order approximation is indeed close to the actual loss function, and for that we need both lower and upper bounds on the quality of the first order approximation.\nGiven a training set S = (x1, y1), . . . , (xm, ym), the average training loss of a matrix W is: L(W ) = 1 m \u2211 (x,y)\u2208S `(W, (x, y)). We aim at approximately\n1Smoothness guarantees that |f(x)\u2212f(x\u2032)\u2212\u2207f(x\u2032)(x\u2212 x\u2032)| \u2264 \u03b2\u2016x \u2212 x\u2032\u20162 for some \u03b2 and all x, x\u2032. Therefore one can approximate f(x) by f(x\u2032) + \u2207f(x\u2032)(x \u2212 x\u2032) and the approximation error is upper bounded by the difference between x, x\u2032.\nsolving the problem\nmin W\u2208Rk,d\nL(W ) s.t. \u2016W\u2016\u221e,0 \u2264 s . (4)\nThat is, find the matrix W with minimal training loss among all matrices with column sparsity of at most s, where s is a user-defined parameter. Since `(W, (x, y)) is an upper bound on 1[hW (x) 6= y], by minimizing L(W ) we also decrease the average 0\u22121 error of W over the training set. In Section 5 we show that for sparse models, a small training error is likely to yield a small error on unseen examples as well.\nRegrettably, the constraint \u2016W\u2016\u221e,0 \u2264 s in eqn. (4) is non-convex, and solving the optimization problem in eqn. (4) is NP-hard (Natarajan, 1995; Davis et al., 1997). To overcome the hardness result, the ShareBoost algorithm will follow the forward greedy selection approach. The algorithm comes with formal generalization and sparsity guarantees (described in Section 5) that makes ShareBoost an attractive multiclass learning engine due to efficiency (both during training and at test time) and accuracy."}, {"heading": "1.2. Related Work", "text": "The centrality of the multiclass learning problem has spurred the development of various approaches for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass to binary, e.g. the one-vs-rest or all pairs constructions. The more direct approach we choose, in particular, the multiclass predictors of the form given in eqn. (1), has been extensively studied and showed a great success in practice \u2014 see for example (Duda & Hart, 1973; Vapnik, 1998; Crammer & Singer, 2003).\nAn alternative construction, abbreviated as the singlevector model, shares a single weight vector, for all the classes, paired with class-specific feature mappings. This construction is common in generalized additive models (Hastie & Tibshirani, 1995), multiclass versions of boosting (Freund & Schapire, 1997; Schapire & Singer, 1999), and has been popularized lately due to its role in prediction with structured output where the number of classes is exponentially large (see e.g. (Taskar et al., 2003)). While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y. In contrast, in this paper we tackle general multiclass prediction problems, like object recognition or document classification, where it is not straightforward or even plausible how one would go about to construct a-priori good class specific feature mappings,\nand therefore the single-vector model is not adequate.\nThe class of predictors of the form given in eqn. (1) can be trained using Frobenius norm regularization (as done by multiclass SVM \u2013 see e.g. (Crammer & Singer, 2003)) or using `1 regularization over all the entries of W . However, as pointed out in (Quattoni et al., 2009), these regularizers might yield a matrix with many nonzeros columns, and hence, will lead to a predictor that uses many features.\nThe alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010). For example, (Duchi & Singer, 2009) solves the following problem:\nmin W\u2208Rk,d\nL(W ) + \u03bb\u2016W\u2016\u221e,1 . (5)\nwhich can be viewed as a convex approximation of our objective (eqn. (4)). This is advantageous from an optimization point of view, as one can find the global optimum of a convex problem, but it remains unclear how well the convex program approximates the original goal. For example, in Section 6 we show cases where mix-norm regularization does not yield sparse solutions while ShareBoost does yield a sparse solution. Despite the fact that ShareBoost tackles a nonconvex program, and thus limited to local optimum solutions, we prove in Theorem 2 that under mild conditions ShareBoost is guaranteed to find an accurate sparse solution whenever such a solution exists and that the generalization error is bounded as shown in Theorem 1.\nWe note that several recent papers (e.g. (Huang & Zhang, 2010)) established exact recovery guarantees for mixed norms, which may seem to be stronger than our guarantee given in Theorem 2. However, the assumptions in (Huang & Zhang, 2010) are much stronger than the assumptions of Theorem 2. In particular, they have strong noise assumptions and a group RIP like assumption (Assumption 4.1-4.3 in their paper). In contrast, we impose no such restrictions. We would like to stress that in many generic practical cases, the assumptions of (Huang & Zhang, 2010) will not hold. For example, when using decision stumps, features will be highly correlated which will violate Assumption 4.3 of (Huang & Zhang, 2010).\nAnother advantage of ShareBoost is that its only parameter is the desired number of non-zero columns of W . Furthermore, obtaining the whole-regularizationpath of ShareBoost, that is, the curve of accuracy as a function of sparsity, can be performed by a single run\nof ShareBoost, which is much easier than obtaining the whole regularization path of the convex relaxation in eqn. (5). Last but not least, ShareBoost can work even when the initial number of features, d, is very large, as long as there is an efficient way to choose the next feature. For example, when the features are constructed using decision stumps, d will be extremely large, but ShareBoost can still be implemented efficiently. In contrast, when d is extremely large mixnorm regularization techniques yield challenging optimization problems.\nAs mentioned before, ShareBoost follows the forward greedy selection approach for tackling the hardness of solving eqn. (4). The greedy approach has been widely studied in the context of learning sparse predictors for linear regression. However, in multiclass problems, one needs sparsity of groups of variables (columns of W ). ShareBoost generalizes the fully corrective greedy selection procedure given in (Shalev-Shwartz et al., 2010) to the case of selection of groups of variables, and our analysis follows similar techniques.\nObtaining group sparsity by greedy methods has been also recently studied in (Huang et al., 2009; Majumdar & Ward, 2009), and indeed, ShareBoost shares similarities with these works. We differ from (Huang et al., 2009) in that our analysis does not impose strong assumptions (e.g. group-RIP) and so ShareBoost applies to a much wider array of applications. In addition, the specific criterion for choosing the next feature is different. In (Huang et al., 2009), a ratio between difference in objective and different in costs is used. In ShareBoost, the L1 norm of the gradient matrix is used. For the multiclass problem with log loss, the criterion of ShareBoost is much easier to compute, especially in large scale problems. (Majumdar & Ward, 2009) suggested many other selection rules that are geared toward the squared loss, which is far from being an optimal loss function for multiclass problems.\nAnother related method is the JointBoost algorithm (Torralba et al., 2007). While the original presentation in (Torralba et al., 2007) seems rather different than the type of predictors we describe in eqn. (1), it is possible to show that JointBoost in fact learns a matrix W with additional constraints. In particular, the features x are assumed to be decision stumps and each column W\u00b7,i is constrained to be \u03b1i(1[1 \u2208 Ci] , . . . ,1[k \u2208 Ci]), where \u03b1i \u2208 R and Ci \u2282 Y. That is, the stump is shared by all classes in the subset Ci. JointBoost chooses such shared decision stumps in a greedy manner by applying the GentleBoost algorithm on top of this presentation. A major disadvantage of JointBoost is that in its pure form, it should exhaustively search C among all\n2k possible subsets of Y. In practice, (Torralba et al., 2007) relies on heuristics for finding C on each boosting step. In contrast, ShareBoost allows the columns of W to be any real numbers, thus allowing \u201dsoft\u201d sharing between classes. Therefore, ShareBoost has the same (or even richer) expressive power comparing to JointBoost. Moreover, ShareBoost automatically identifies the relatedness between classes (corresponding to choosing the set C) without having to rely on exhaustive search. ShareBoost is also fully corrective, in the sense that it extracts all the information from the selected features before adding new ones. This leads to higher accuracy while using less features as was shown in our experiments on image classification. Lastly, ShareBoost comes with theoretical guarantees.\nFinally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al., 2010; Bengio et al., 2011), shared hidden structure (LeCun et al., 1998; Amit et al., 2007), shared prototypes (Quattoni et al., 2008), or sharing underlying metric (Xing et al., 2003)."}, {"heading": "2. The ShareBoost Algorithm", "text": "ShareBoost is a forward greedy selection approach for solving eqn. (4). Usually, in a greedy approach, we update the weight of one feature at a time. Now, we will update one column of W at a time (since the desired sparsity is over columns). We will choose the column that maximizes the `1 norm of the corresponding column of the gradient of the loss at W . Since W is a matrix we have that \u2207L(W ) is a matrix of the partial derivatives of L. Denote by \u2207rL(W ) the r\u2019th column of \u2207L(W ), that is, the vector ( \u2202L(W ) \u2202W1,r , . . . , \u2202L(W )\u2202Wk,r ) . A standard calculation shows that\n\u2202L(W )\n\u2202Wq,r =\n1\nm \u2211 (x,y)\u2208S \u2211 c\u2208Y \u03c1c(x, y)xr(1[q = c]\u2212 1[q = y])\nwhere\n\u03c1c(x, y) = e1[c6=y]\u2212(Wx)y+(Wx)c\u2211\ny\u2032\u2208Y e 1[y\u2032 6=y]\u2212(Wx)y+(Wx)y\u2032\n. (6)\nNote that \u2211 c \u03c1c(x, y) = 1 for all (x, y). Therefore, we can rewrite,\n\u2202L(W )\n\u2202Wq,r =\n1\nm \u2211 (x,y) xr(\u03c1q(x, y)\u2212 1[q = y]) .\nBased on the above we have\n\u2016\u2207rL(W )\u20161 = 1\nm \u2211 q\u2208Y \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 (x,y) xr(\u03c1q(x, y)\u2212 1[q = y]) \u2223\u2223\u2223\u2223\u2223\u2223 . (7)\nFinally, after choosing the column for which \u2016\u2207rL(W )\u20161 is maximized, we re-optimize all the columns of W which were selected so far. The resulting algorithm is given in Algorithm 1.\nAlgorithm 1 ShareBoost\n1: Initialize: W = 0 ; I = \u2205 2: for t=1,2,. . . ,T do 3: For each class c and example (x, y) define \u03c1c(x, y) as in eqn. (6) 4: Choose feature r that maximizes the right-hand side of eqn. (7) 5: I \u2190 I \u222a {r} 6: Set W \u2190 argminW L(W ) s.t. W\u00b7,i = 0 for all i /\u2208 I 7: end for\nThe runtime of ShareBoost is as follows. Steps 3-5 requires O(mdk). Step 6 is a convex optimization problem in tk variables and can be performed using various methods. In our experiments, we used Nesterov\u2019s accelerated gradient method (Nesterov & Nesterov, 2004) whose runtime is O(mtk/ \u221a ) for a smooth objective, where is the desired accuracy. Therefore, the overall runtime is O(Tmdk + T 2mk/ \u221a ). It is interesting to compare this runtime to the complexity of minimizing the mixed-norm regularization objective given in eqn. (5). Since the objective is no longer smooth, the runtime of using Nesterov\u2019s accelerated method would be O(mdk/ ) which can be much larger than the runtime of ShareBoost when d T ."}, {"heading": "3. Variants of ShareBoost", "text": "We now describe several variants of ShareBoost. The analysis we present in Section 5 can be easily adapted for these variants as well."}, {"heading": "3.1. Modifying the Greedy Choice Rule", "text": "ShareBoost chooses the feature r which maximizes the `1 norm of the r-th column of the gradient matrix. Our analysis shows that this choice leads to a sufficient decrease of the objective function. However, one can easily develop other ways for choosing a feature which may potentially lead to an even larger decrease of the objective. For example, we can choose a feature r that minimizes L(W ) over matrices W with support of\nI\u222a{r}. This will lead to the maximal possible decrease of the objective function at the current iteration. Of course, the runtime of choosing r will now be much larger. Some intermediate options are to choose r that minimizes\nmin \u03b1\u2208R\nW + \u03b1\u2207rR(W )\nor to choose r that minimizes\nmin w\u2208Rk\nW + we\u2020r,\nwhere e\u2020r is the all-zero row vector except 1 in the r\u2019th position."}, {"heading": "3.2. Selecting a Group of Features at a Time", "text": "In some situations, features can be divided into groups where the runtime of calculating a single feature in each group is almost the same as the runtime of calculating all features in the group. In such cases, it makes sense to choose groups of features at each iteration of ShareBoost. This can be easily done by simply choosing the group of features J that maximizes\u2211 j\u2208J \u2016\u2207jL(W )\u20161."}, {"heading": "3.3. Adding Regularization", "text": "Our analysis implies that when |S| is significantly larger than O\u0303(Tk) then ShareBoost will not overfit. When this is not the case, we can incorporate regularization in the objective of ShareBoost in order to prevent overfitting. One simple way is to add to the objective function L(W ) a Frobenius norm regularization term of the form \u03bb \u2211 i,jW 2 i,j , where \u03bb is a regularization parameter. It is easy to verify that this is a smooth and convex function and therefore we can easily adapt ShareBoost to deal with this regularized objective. It is also possible to rely on other norms such as the `1 norm or the `\u221e/`1 mixed-norm. However, there is one technicality due to the fact that these norms are not smooth. We can overcome this problem by defining smooth approximations to these norms. The main idea is to first note that for a scalar a we have |a| = max{a,\u2212a} and therefore we can rewrite the aforementioned norms using max and sum operations. Then, we can replace each max expression with its soft-max counterpart and obtain a smooth version of the overall norm function. For example, a smooth version of the `\u221e/`1 norm will\nbe \u2016W\u2016\u221e,1 \u2248 1\u03b2 \u2211d j=1 log (\u2211k i=1(e \u03b2Wi,j + e\u2212\u03b2Wi,j ) ) , where \u03b2 \u2265 1 controls the tradeoff between quality of approximation and smoothness."}, {"heading": "4. Non-Linear Prediction Rules", "text": "We now demonstrate how ShareBoost can be used for learning non-linear predictors. The main idea is similar to the approach taken by Boosting and SVM. That is, we construct a non-linear predictor by first mapping the original features into a higher dimensional space and then learning a linear predictor in that space, which corresponds to a non-linear predictor over the original feature space. To illustrate this idea we present two concrete mappings. The first is the decision stumps method which is widely used by Boosting algorithms. The second approach shows how to use ShareBoost for learning piece-wise linear predictors and is inspired by the super-vectors construction recently described in (Zhou et al., 2010)."}, {"heading": "4.1. ShareBoost with Decision Stumps", "text": "Let v \u2208 Rp be the original feature vector representing an object. A decision stump is a binary feature of the form 1[vi \u2264 \u03b8], for some feature i \u2208 {1, . . . , p} and threshold \u03b8 \u2208 R. To construct a non-linear predictor we can map each object v into a feature-vector x that contains all possible decision stumps. Naturally, the dimensionality of x is very large (in fact, can even be infinite), and calculating Step 4 of ShareBoost may take forever. Luckily, a simple trick yields an efficient solution. First note that for each i, all stump features corresponding to i can get at most m + 1 values on a training set of size m. Therefore, if we sort the values of vi over the m examples in the training set, we can calculate the value of the right-hand side of eqn. (7) for all possible values of \u03b8 in total time of O(m). Thus, ShareBoost can be implemented efficiently with decision stumps."}, {"heading": "4.2. Learning Piece-wise Linear Predictors with ShareBoost", "text": "To motivate our next construction let us consider first a simple one dimensional function estimation problem. Given sample (x1, yi), . . . , (xm, ym) we would like to find a function f : R \u2192 R such that f(xi) \u2248 yi for\nall i. The class of piece-wise linear functions can be a good candidate for the approximation function f . See for example an illustration in Fig. 1. In fact, it is easy to verify that all smooth functions can be approximated by piece-wise linear functions (see for example the discussion in (Zhou et al., 2010)). In general, we can express piece-wise linear vector-valued functions as\nf(v) = q\u2211 j=1 1[\u2016v \u2212 vj\u2016 < rj ] (\u3008uj ,v\u3009+ bj) ,\nwhere q is the number of pieces, (uj , bj) represents the linear function corresponding to piece j, and (vj , rj) represents the center and radius of piece j. This expression can be also written as a linear function over a different domain, f(v) = \u3008w, \u03c8(v)\u3009 where\n\u03c8(v) = [1[\u2016v \u2212 v1\u2016 < r1] [v , 1] , . . . , 1[\u2016v \u2212 vq\u2016 < rq] [v , 1] ] .\nIn the case of learning a multiclass predictor, we shall learn a predictor v 7\u2192 W\u03c8(v), where W will be a k by dim(\u03c8(v)) matrix. ShareBoost can be used for learning W . Furthermore, we can apply the variant of ShareBoost described in Section 3.2 to learn a piecewise linear model which few pieces (that is, each group of features will correspond to one piece of the model). In practice, we first define a large set of candidate centers by applying some clustering method to the training examples, and second we define a set of possible radiuses by taking values of quantiles from the training examples. Then, we train ShareBoost so as to choose a multiclass predictor that only use few pairs (vj , rj).\nThe advantage of using ShareBoost here is that while it learns a non-linear model it will try to find a model with few linear \u201cpieces\u201d, which is advantageous both in terms of test runtime as well as in terms of generalization performance."}, {"heading": "5. Analysis", "text": "In this section we provide formal guarantees for the ShareBoost algorithm. The proofs are deferred to the appendix. We first show that if the algorithm has managed to find a matrix W with a small number of non-zero columns and a small training error, then the generalization error of W is also small. The bound below is in terms of the 0\u22121 loss. A related bound, which is given in terms of the convex loss function, is described in (Zhang, 2004).\nTheorem 1 Suppose that the ShareBoost algorithm runs for T iterations and let W be its output matrix. Then, with probability of at least 1\u2212 \u03b4 over the choice\nof the training set S we have that\nP (x,y)\u223cD [hW (x) 6= y] \u2264 P (x,y)\u223cS [hW (x) 6= y]\n+O\n(\u221a Tk log(Tk) log(k) + T log(d) + log(1/\u03b4)\n|S|\n)\nNext, we analyze the sparsity guarantees of ShareBoost. As mentioned previously, exactly solving eqn. (4) is known to be NP hard. The following main theorem gives an interesting approximation guarantee. It tells us that if there exists an accurate solution with small `\u221e,1 norm, then the ShareBoost algorithm will find a good sparse solution.\nTheorem 2 Let > 0 and let W ? be an arbitrary matrix. Assume that we run the ShareBoost algorithm for T = \u2308 4 1 \u2016W ?\u20162\u221e,1 \u2309\niterations and let W be the output matrix. Then, \u2016W\u2016\u221e,0 \u2264 T and L(W ) \u2264 L(W ?) + ."}, {"heading": "6. Feature Sharing \u2014 Illustrative Examples", "text": "In this section we present illustrative examples, showing that whenever strong feature sharing is possible then ShareBoost will find it, while competitive methods might fail to produce solutions with a small number of features.\nIn the analysis of the examples below we use the following simple corollary of Theorem 2.\nCorollary 1 Assume that there exists a matrix W ? such that L(W ?) \u2264 , all entries of W ? are in [\u2212c, c], and \u2016W ?\u2016\u221e,0 = r. Then, ShareBoost will find a matrix W with L(W ) \u2264 2 and \u2016W\u2016\u221e,0 \u2264 4r2c2/ .\nThe first example we present shows an exponential gap between the number of features required by ShareBoost (as well as mixed norms) and the number of features required by `2 or `1 regularization methods. Consider a set of examples such that each example, (x, y), is of the form x = [bin(y), 2 log(k) ey] \u2208 Rlog(k)+k, where bin(y) \u2208 {\u00b11}log(k) is the binary representation of the number y in the alphabet {\u00b11} and ey is the vector which is zero everywhere except 1 in the y\u2019th coordinate. For example, if k = 4 then bin(1) = [\u22121, 1], bin(2) = [1,\u22121], bin(3) = [1, 1], and bin(4) = [\u22121,\u22121].\nConsider two matrices. The first matrix, denoted W (s), is the matrix whose row y equals to [bin(y), (0, . . . , 0)]. The second matrix, denoted W (f), is the matrix whose row y equals to [(0, . . . , 0), ey]. Clearly, the number of features used by hW (s) is log(k) while the number of features used by hW (f) is k.\nObserve that both hW (f)(x) and hW (s)(x) (see definition in eqn. (1)), will make perfect predictions on the training set. Furthermore, since for each example (x, y), for each r 6= y we have that (W (s)x)r \u2208 [\u2212 log(k), log(k) \u2212 2], for the logistic multiclass loss, for any c > 0 we have that\nL(cW (f)) = log(1 + (k \u2212 1)e1\u22122c log(k)) < L(cW (s))\n< log(1 + (k \u2212 1)e1\u2212c(log(k)\u22122)) .\nIt follows that for\nc \u2265 1 + log(k \u2212 1)\u2212 log(e \u2212 1)\nlog(k)\u2212 2\nwe have that L(cW (s)) \u2264 .\nConsider an algorithm that solves the regularized problem\nmin W\nL(W ) + \u03bb \u2016W\u2016p,p ,\nwhere p is either 1 or 2. In both cases, we have that2 \u2016W (f)\u2016p,p < \u2016W (s)\u2016p,p. It follows that for any value of \u03bb, and for any c > 0, the value of the objective at cW (f) is smaller than the value at cW (s). In fact, it is not hard to show that the optimal solution takes the form cW (f) for some c > 0. Therefore, no matter what the regularization parameter \u03bb is, the solution of the above regularized problem will use k features, even though there exists a rather good solution that relies on log(k) shared features.\nIn contrast, using Corollary 1 we know that if we stop ShareBoost after poly(log(k)) iterations it will produce a matrix that uses only poly(log(k)) features and has a small loss. Similarly, it is possible to show that for an appropriate regularization parameter, the mixnorm regularization \u2016W\u2016\u221e,1 will also yield the matrix W (s) rather than the matrix W (f).\nIn our second example we show that in some situations using the mix-norm regularization,\nmin W\nL(W ) + \u03bb\u2016W\u2016\u221e,1 ,\nwill also fail to produce a sparse solution, while ShareBoost is still guaranteed to learn a sparse solution. Let s be an integer and consider examples (x, y) where each x is composed of s blocks, each of which is in {\u00b11}log(k). We consider two types of examples. In the first type, each block of x equals to bin(y). In the second type, we generate example as in the first type, but then we zero one of the blocks (where we\n2\u2016W (f)\u2016pp,p = k whereas \u2016W (s)\u2016pp,p = k log(k).\nchoose uniformly at random which block to zero). As before, (1 \u2212 )m examples are of the first type while m examples are of the second type.\nConsider again two matrices. The first matrix, denoted W (s), is the matrix whose row y equals to [bin(y), (0, . . . , 0)]. The second matrix, denoted W (f), is the matrix whose row y equals to [bin(y), . . . ,bin(y)]/s. Note that \u2016W (f)\u2016\u221e,1 = \u2016W (s)\u2016\u221e,1. In addition, for any (x, y) of the second type we have that E[W (s)x] = W (f)x, where expectation is with respect to the choice of which block to zero. Since the loss function is strictly convex, it follows from Jensen\u2019s inequality that L(W (f)) < L(W (s)). We have thus shown that using the (\u221e, 1) mix-norm as a regularization will prefer the matrix W (f) over W (s). In fact, it is possible to show that the minimizer of L(W ) + \u03bb\u2016W\u2016\u221e,1 will be of the form cW (f) for some c. Since the number of blocks, s, was arbitrarily large, and since ShareBoost is guaranteed to learn a matrix with at most poly(log(k)) non-zero columns, we conclude that there can be a substantial gap between mixnorm regularization and ShareBoost. The advantage of ShareBoost in this example follows from its ability to break ties (even in an arbitrary way).\nNaturally, the aforementioned examples are synthetic and capture extreme situations. However, in our experiments below we show that ShareBoost performs better than mixed-norm regularization on natural data sets as well."}, {"heading": "7. Experiments", "text": "In this section we demonstrate the merits (and pitfalls) of ShareBoost by comparing it to alternative algorithms in different scenarios. The first experiment exemplifies the feature sharing property of ShareBoost. We perform experiments with an OCR data set and demonstrate a mild growth of the number of features as the number of classes grows from 2 to 36. The second experiment compares ShareBoost to mixed-norm regularization and to the JointBoost algorithm of (Torralba et al., 2007). We follow the same experimental setup as in (Duchi & Singer, 2009). The main finding is that ShareBoost outperforms the mixed-norm regularization method when the output predictor needs to be very sparse, while mixed-norm regularization can be better in the regime of rather dense predictors. We also show that ShareBoost is both faster and more accurate than JointBoost. The third and final set of experiments is on the MNIST handwritten digit dataset where we demonstrate state-of-the-art accuracy at extremely efficient runtime performance."}, {"heading": "7.1. Feature Sharing", "text": "The main motivation for deriving the ShareBoost algorithm is the need for a multiclass predictor that uses only few features, and in particular, the number of features should increase slowly with the number of classes. To demonstrate this property of ShareBoost we experimented with the Char74k data set which consists of images of digits and letters. We trained ShareBoost with the number of classes varying from 2 classes to the 36 classes corresponding to the 10 digits and 26 capital letters. We calculated how many features were required to achieve a certain fixed accuracy as a function of the number of classes. The description of the feature space is described in Section 7.4.\nWe compared ShareBoost to the 1-vs-rest approach, where in the latter, we trained each binary classifier using the same mechanism as used by ShareBoost. Namely, we minimize the binary logistic loss using a greedy algorithm. Both methods aim at constructing sparse predictors using the same greedy approach. The difference between the methods is that ShareBoost selects features in a shared manner while the 1-vs-rest approach selects features for each binary problem separately. In Fig. 2 we plot the overall number of features required by both methods to achieve a fixed accuracy on the test set as a function of the number of classes. As can be easily seen, the increase in the number of required features is mild for ShareBoost but significant for the 1-vs-rest approach."}, {"heading": "7.2. Comparing ShareBoost to Mixed-Norms Regularization", "text": "Our next experiment compares ShareBoost to the use of mixed-norm regularization (see eqn. (5)) as a surro-\ngate for the non-convex sparsity constraint. See Section 1.2 for description of the approach. To make the comparison fair, we followed the same experimental setup as in (Duchi & Singer, 2009) (using code provided by ).\nWe calculated the whole regularization path for the mixed-norm regularization by running the algorithm of (Duchi & Singer, 2009) with many values of the regularization parameter \u03bb. In Fig. 3 we plot the results on three UCI datasets: StatLog, Pendigits and Isolet. The number of classes for the datasets are 7,10,26, respectively. The original dimensionality of these datasets is not very high and therefore, following (Duchi & Singer, 2009), we expanded the features by taking all products over ordered pairs of features. After this transformation, the number of features were 630, 120, 190036, respectively.\nFig. 3 displays the results. As can be seen, ShareBoost decreases the error much faster than the mixednorm regularization, and therefore is preferable when the goal is to have a rather sparse solution. When more features are allowed, ShareBoost starts to overfit. This is not surprising since here sparsity is our only mean for controlling the complexity of the learned classifier. To prevent this overfitting effect, one can use the variant of ShareBoost that incorporates regularization\u2014 see Section 3."}, {"heading": "7.3. Comparing ShareBoost to JointBoost", "text": "Here we compare ShareBoost to the JointBoost algorithm of (Torralba et al., 2007). See Section 1.2 for description of JointBoost. As in the previous experiment, we followed the experimental setup as in (Duchi & Singer, 2009) and ran JointBoost of (Torralba et al., 2007) using their published code with additional implementation of the BFS heuristic for pruning the 2k space of all class-subsets as described in their paper.\nFig. 3 (bottom) displays the results. Here we used stump features for both algorithms since these are needed for JointBoost. As can be seen, ShareBoost decreases the error much faster and therefore is preferable when the goal is to have a rather sparse solution. As in the previous experiment we observe that when more features are allowed, ShareBoost starts to overfit. Again, this is not surprising and can be prevented by adding additional regularization. The training runtime of ShareBoost is also much shorter than that of JointBoost (see discussion in Section 1.2)."}, {"heading": "7.4. MNIST Handwritten Digits Dataset", "text": "The goal of this experiment is to show that ShareBoost achieves state-of-the-art performance while constructing very fast predictors. We experimented with the MNIST digit dataset, which consists of a training set of 60, 000 digits represented by centered size-normalized 28 \u00d7 28 images, and a test set of 10, 000 digits (see Fig. 6 for some examples). The MNIST dataset has been extensively studied and is considered the standard test for multiclass classification of handwritten digits. The error rate achieved by the most advanced algorithms are below 1% of the test set (i.e., below 100 classification mistakes on the test set). To get a sense of the challenge involved with the MNIST dataset, consider a straightforward 3-Nearest-Neighbor (3NN) approach where each test example x, represented as a vector with 282 entries, is matched against the entire training set xj using the distance d(x,xj) = \u2016x\u2212xj\u20162. The classification decision is then the majority class label of the three most nearest training examples. This naive 3NN approach achieves an error rate of 2.67% (i.e., 267 mis-classification errors) with a run-time of unwieldy proportions. Going from 3NN to qNN with q = 4, ..., 12 does not produce a better error rate.\nMore advanced shape-similarity measures could improve the performance of the naive qNN approach but at a heavier run-time cost. For example, the Shape Context similarity measure introduced by (Belongie et al., 2002) uses a Bipartite matching algorithm between descriptors computed along 100 points in each image. A 3NN using Shape-Context similarity achieves an error rate of 0.63% but at a very high (practically unwieldy) run-time cost. The challenge with the MNIST dataset is, therefore, to design a multiclass algorithm with a small error rate (say below 1%) and have an efficient run-time performance.\nThe top MNIST performer (Ciresan et al., 2010) uses a feed-forward Neural-Net with 7.6 million connections which roughly translates to 7.6 million multiplyaccumulate (MAC) operations at run-time as well. During training, geometrically distorted versions of the original examples were generated in order to expand the training set following (Simard et al., 2003) who introduced a warping scheme for that purpose. The top performance error rate stands at 0.35% at a run-time cost of 7.6 million MAC per test example.\nTable 1 summarizes the discussion so far including the performance of ShareBoost. The error-rate of ShareBoost with 266 rounds stands on 0.71% using the original training set and 0.47% with the expanded training set of 360, 000 examples generated by adding five deformed instances per original example and with\nT = 305 rounds. The run-time on test examples is around 40% of the leading MNIST performer. The error rate of 0.47% is better than that reported by (Decoste & Bernhard, 2002) who used a 1-vs-all SVM with a 9-degree polynomial kernel and with an expanded training set of 780, 000 examples. The number of support vectors (accumulated over the ten separate binary classifiers) was 163, 410 giving rise to a run-time of 21-fold compared to ShareBoost. We describe below the details of the ShareBoost implementation on the MNIST dataset.\nThe feature space we designed consists of 7\u00d77 image patches with corresponding spatial masks, constructed as follows. All 7\u00d77 patches were collected from all images and clustered using K-means to produce 1000 centers wf . For each such center (patch) we also associated a set of 16 possible masks gf in order to limit the spatial locations of the maximal response of the 7\u00d77 patch. The pairs F = {(vf , gf )} form the pool of d = 16, 000 templates (shape plus location). The vector of feature measurements x \u2208 Rm = (. . . , xfc, . . .) has each of its entries associated with one of the templates where an entry xfc = max { (I \u2297 wf )\u00d7 gcf } . That is, a feature is the maximal response of the convolution of the template wf over the image, weighted by the Gaussian gcf .\nShareBoost selects a subset of the templates j1, . . . , jT where each ji represents some template pair (wfi , g ci fi ), and the matrix W \u2208 Rk\u00d7T . A test image I is then converted to x\u0303 \u2208 RT using x\u0303i = max{(I\u2297wfi)\u00d7g ci fi } with the maximum going over the image locations. The prediction y\u0302 is then argmaxy\u2208[k](W x\u0303)y. Fig. 5(a) shows the first 30 templates that were chosen by ShareBoost and their corresponding spatial masks. For example, the first templates matches a digit part along the top of the image, the eleventh template matches a horizontal stroke near the top of the image and so forth. Fig. 5(b) shows the weights (columns of W ) of the first 30 templates of the model that produced the best results. For example, the eleventh template which encodes a horizontal line close to the top is expected in the digit \u201c9\u201d but not in the digit \u201c4\u201d. Fig. 6 shows the 47 misclassified samples after T = 305 rounds of ShareBoost, and Fig. 4 displays the convergence curve of error-rate as a function of the number of rounds.\nIn terms of run-time on a test image, the system requires 305 convolutions of 7 \u00d7 7 templates and 540 dot-product operations which totals to roughly 3.3\u00b7106 MAC operations \u2014 compared to around 7.5 \u00b7106 MAC operations of the top MNIST performer. Moreover, due to the fast convergence of ShareBoost, 75 rounds are enough for achieving less than 1% error. Further\nimprovements of ShareBoost on the MNIST dataset are possible such as by extending further the training set using more deformations and by increasing the pool of features with other type of descriptors \u2013 but those were not pursued here. The point we desired to make is that ShareBoost can achieve competitive performance with the top MNIST performers, both in accuracy and in run-time, with little effort in feature space design while exhibiting great efficiency during training time as well."}, {"heading": "7.5. Comparing ShareBoost to kernel-based SVM", "text": "In the experiments on the MNIST data set reported above, each feature is the maximal response of the convolution of a 7\u00d7 7 patch over the image, weighted by a spatial mask.\nOne might wonder if the stellar performance of ShareBoost is maybe due to the patch-based features we designed. In this section we remove doubt by using ShareBoost for training a piece-wise linear predictor, as described in Section 4.2, on MNIST using generic features. We show that ShareBoost comes close to the error rate of SVM with Gaussian kernels, while only requiring 230 anchor points, which is well below the number of support-vectors needed by kernel-SVM. This underscores the point that ShareBoost can find an extremely fast predictor without sacrificing stateof-the-art performance level.\nRecall that the piece-wise linear predictor is of the following form:\nh(x) = argmax y\u2208Y (\u2211 j\u2208I 1 [ \u2016x\u2212 v(j)\u2016 < r(j) ] (W (j)y,\u00b7 x + b (j) y ) ) ,\nwhere v(j) \u2208 Rd are anchor points with radius of influence r(j), and W (j), b(j) define together a linear classifier for the j\u2019th anchor. ShareBoost selects the\nset of anchor points and their radiuses together with the corresponding linear classifiers. In this context it is worthwhile to compare classification performance to SVM with Gaussian kernels applied in a 1-vs-all framework. Kernel-SVM also selects a subset of the training set S with corresponding weight coefficients, thus from a mechanistic point of view our piece-wise linear predictor shares the same principles as kernelSVM.\nWe performed a standard dimensionality reduction using PCA from the original raw pixel dimension of 282 to 50, i.e., every digit was mapped to x \u2208 R50 using PCA. The pool of anchor points was taken from a reduced training set by means of clustering S into 1500 clusters and the range of radius values per anchor point was taken from a discrete set of 35 values. Taken together, each round of ShareBoost selected an anchor point v(j) and radius r(j) from a search space of size 52500. Fig. 7 shows the error-rate per ShareBoost rounds. As can be seen, ShareBoost comes close to the error rate of SVM while only requiring 230 anchor points, which is well below the number of supportvectors needed by kernel-SVM. This underscores the point that ShareBoost can find an extremely fast predictor without sacrificing state-of-the-art performance level."}, {"heading": "8. Acknowledgements", "text": "We would like to thank Itay Erlich and Zohar BarYehuda for their dedicated contribution to the implementation of ShareBoost."}, {"heading": "A. Proofs", "text": "A.1. Proof of Theorem 1\nThe proof is based on an analysis of the Natarajan dimension of the class of matrices with small number of non-zero columns. The Natarajan dimension is a generalization of the VC dimension for classes of multiclass hypotheses. In particular, we rely on the analysis given in Theorem 25 and Equation 6 of (Daniely et al., 2011). This implies that if the set of T columns of W are chosen in advance then\nP (x,y)\u223cD [hW (x) 6= y] \u2264 P (x,y)\u223cS\n[hW (x) 6= y] +O (\u221a Tk log(Tk) log(k) + log(1/\u03b4)/ \u221a |S| ) .\nApplying the union bound over all ( T d ) options to choose the relevant features we conclude our proof.\nA.2. Proof of Theorem 2\nTo prove the theorem, we start by establishing a certain smoothness property of L. First, we need the following.\nLemma 1 . Let ` : Rk \u2192 R be defined as\n`(v) = log 1 + \u2211 i\u2208[k]\\{j} e1\u2212vj+vi  . Then, for any u,v we have\n`(u + v) \u2264 `(u) + \u3008\u2207`(u),v\u3009+ \u2016v\u20162\u221e .\nProof Using Taylor\u2019s theorem, it suffices to show that the Hessian of ` at any point satisfies\nv\u2020Hv \u2264 2\u2016v\u20162\u221e .\nConsider some vector w and without loss of generality assume that j = 1. We have,\n\u2202`(w)\n\u2202w1 = \u2212\n\u2211k i=2 e 1\u2212w1+wi\n1 + \u2211k p=2 e 1\u2212w1+wp def = \u03b11\nand for i \u2265 2 \u2202`(w)\n\u2202wi =\ne1\u2212w1+wi 1 + \u2211k p=2 e 1\u2212w1+wp def = \u03b1i .\nNote that \u2212\u03b11 = \u2211k i=2 \u03b11 \u2264 1, and that for i \u2265 2, \u03b1i \u2265 0. Let H be the Hessian of ` at w. It follows that for i \u2265 2, Hi,i = e1\u2212w1+wi\n1 + \u2211k\np=2 e 1\u2212w1+wi\n\u2212 (e 1\u2212w1+wi)2 1 + ( \u2211k\np=2 e 1\u2212w1+wi)2\n= \u03b1i \u2212 \u03b12i .\nIn addition, for j 6= i where both j and i are not 1 we have\nHi,j = 0\u2212 e1\u2212w1+wie1\u2212w1+wj ( \u2211k p=2 e 1\u2212w1+wi)2 = \u2212\u03b1i\u03b1j .\nFor i = 1 we have H1,1 = \u2212\u03b11 \u2212 \u03b121\nand for i > 1 Hi,1 = \u2212\u03b1i \u2212 \u03b11\u03b1i\nWe can therefore rewrite H as\nH = \u2212\u03b1\u03b1\u2020 + diag([\u2212\u03b11, \u03b12, . . . , \u03b1k])\u2212 e1[0, \u03b11, . . . , \u03b1k] \u2212[0, \u03b11, . . . , \u03b1k]\u2020(e1)\u2020 .\nIt thus follows that:\nv\u2020Hv = \u2212(\u3008\u03b1,v\u3009)2 \u2212 \u03b11v21 + \u2211 i>1 \u03b1iv 2 i \u2212 2v1 \u2211 i>1 \u03b1ivi\n\u2264 0 + \u2211 i>1 \u03b1i(v 2 i \u2212 v21 \u2212 2v1vi)\n= \u2211 i>1 \u03b1i((vi \u2212 v1)2 \u2212 2v21) \u2264 2 max i v2i = 2\u2016v\u20162\u221e ,\nwhere the last step is because for any vi \u2208 [\u2212c, c], the function f(v1) = (vi \u2212 v1)2 \u2212 2v21 receives its maximum when v1 = \u2212vi and then its value is 2v2i . This concludes our proof.\nThe above lemma implies that L is smooth in the following sense:\nLemma 2 For any W,U s.t. U = ue\u2020r (that is, only the r\u2019th column of U is not zero) we have that\nL(W \u2212 U) \u2264 L(W )\u2212 \u3008\u2207L(W ), U\u3009+ \u2016u\u20162\u221e .\nProof Recall that L(W ) is the average over (x, y) of a function of the form `(Wx), where ` is as defined in Lemma 1. Therefore,\n`((W + U)x) \u2264 `(Wx) + \u3008\u2207`(Wx), Ux\u3009+ \u2016Ux\u20162\u221e = `(Wx) + \u3008\u2207`(Wx), Ux\u3009+ |xr|2\u2016u\u20162\u221e \u2264 `(Wx) + \u3008\u2207`(Wx), Ux\u3009+ \u2016u\u20162\u221e ,\nwhere the last inequality is because we assume that \u2016x\u2016\u221e \u2264 1 for all x. The above implies that\nL(W \u2212 U) \u2264 L(W )\u2212 \u3008\u2207L(W ), U\u3009+ \u2016u\u20162\u221e . (8)\nEquipped with the smoothness property of L, we now turn to show that if the greedy algorithm has not yet identified all the features of W ? then a single greedy iteration yields a substantial progress. We use the notation supp(W ) to denote the indices of columns of W which are not all-zeros.\nLemma 3 Let F, F\u0304 be two subsets of [d] such that F\u0304 \u2212 F 6= \u2205 and let\nW = argmin V :supp(V )=F L(V ) , W ? = argmin V :supp(V )=F\u0304 L(V ) .\nThen, if L(W ) > L(W ?) we have\nL(W )\u2212min u L(W + ue\u2020j) \u2265\n(L(W )\u2212 L(W ?))2 4 (\u2211 i\u2208F\u0304\u2212F \u2016W ?\u00b7,i\u2016\u221e )2 ,\nwhere j = argmaxi \u2016\u2207iL(W )\u20161.\nProof To simplify notation, denote F c = F\u0304 \u2212 F . Using Lemma 2 we know that for any u:\nL(W \u2212 ue\u2020j) \u2264 L(W )\u2212 \u3008\u2207L(W ),ue \u2020 j\u3009+ \u2016u\u2016 2 \u221e ,\nIn particular, the above holds for the vector of u = 12\u2016\u2207jL(W )\u20161 sgn(\u2207jL(W )) and by rearranging we obtain that\nL(W )\u2212 L(W \u2212 ue\u2020j) \u2265 \u3008\u2207L(W ),ue \u2020 j\u3009 \u2212 \u2016u\u2016 2 \u221e\n= 14\u2016\u2207jL(W )\u2016 2 1 .\nIt is therefore suffices to show that\n1 4 \u2016\u2207jL(W )\u201621 \u2265\n(L(W )\u2212 L(W ?))2 4 (\u2211 i\u2208F\u0304\u2212F \u2016W ?\u00b7,i\u2016\u221e )2 .\nDenote s = \u2211 j\u2208F c \u2016W ?\u00b7,j\u2016\u221e, then an equivalent inequality3 is\ns \u2016\u2207jL(W )\u20161 \u2265 L(W )\u2212 L(W ?) .\nFrom the convexity of L, the right-hand side of the above is upper bounded by \u3008\u2207L(W ),W \u2212W ?\u3009. Hence, it is left to show that\ns \u2016\u2207jL(W )\u20161 \u2265 \u3008\u2207L(W ),W \u2212W ?\u3009 .\nSince we assume that W is optimal over F we get that \u2207iL(W ) = 0 for all i \u2208 F , hence \u3008\u2207L(W ),W \u3009 = 0. Additionally, W ?\u00b7,i = 0 for i 6\u2208 F\u0304 . Therefore,\n\u3008\u2207L(W ),W \u2212W ?\u3009 = \u2212 \u2211 i\u2208F c \u3008\u2207iL(W ),W ?\u00b7,i\u3009\n\u2264 \u2211 i\u2208F c \u2016\u2207iL(W )\u20161 \u2016W ?\u00b7,i\u2016\u221e \u2264 s max i \u2016\u2207iL(W )\u20161 = s \u2016\u2207jL(W )\u20161 ,\nand this concludes our proof.\nUsing the above lemma, the proof of our main theorem easily follows.\nProof [of Theorem 2] Denote t = L(W (t))\u2212L(W ?), where W (t) is the value of W at iteration t. The definition of the update implies that L(W (t+1)) \u2264 mini,u L(W (t) + ue\u2020i ). The conditions of Lemma 3 hold and therefore we obtain that (with F = F (t))\nt \u2212 t+1 = L(W (t))\u2212 L(W (t+1)) \u2265 2t 4 (\u2211 i\u2208F\u0304\u2212F \u2016W ?\u00b7,i\u2016\u221e )2\n\u2265 2 t\n4 \u2016W ?\u20162\u221e,1 .\n(9)\nUsing Lemma B.2 from (Shalev-Shwartz et al., 2010), the above implies that for t \u2265 4 \u2016W ?\u20162\u221e,1/ we have that t \u2264 , which concludes our proof.\n3This is indeed equivalent because the lemma assumes that L(W ) > L(W ?)"}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In NIPS, pp", "citeRegEx": "Argyriou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2006}, {"title": "Consistency of the group lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE PAMI,", "citeRegEx": "Belongie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belongie et al\\.", "year": 2002}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "Candes and Tao,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao", "year": 2005}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["D.C. Ciresan", "U. Meier", "L. Maria", "J. Schmidhuber"], "venue": null, "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2003\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2003}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. ShalevShwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "Greedy adaptive approximation", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Journal of Constructive Approximation,", "citeRegEx": "Davis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1997}, {"title": "Training invariant support vector machines", "author": ["D. Decoste", "S. Bernhard"], "venue": "Mach. Learn.,", "citeRegEx": "Decoste and Bernhard,? \\Q2002\\E", "shortCiteRegEx": "Decoste and Bernhard", "year": 2002}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "Technical Report, Stanford University,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Boosting with structural sparsity", "author": ["J. Duchi", "Y. Singer"], "venue": "In Proc. ICML, pp", "citeRegEx": "Duchi and Singer,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer", "year": 2009}, {"title": "Online multiclass learning by interclass hypothesis sharing", "author": ["M. Fink", "S. Shalev-Shwartz", "Y. Singer", "S. Ullman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Fink et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fink et al\\.", "year": 2006}, {"title": "A short introduction to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. of Japanese Society for AI,", "citeRegEx": "Freund and Schapire,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1995\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1995}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Hsu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2010}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Huang and Zhang", "year": 2010}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D.N. Metaxas"], "venue": "In ICML,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P.L. Bartlett", "Ghaoui", "L. El", "M.I. Jordan"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y.L. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fast group sparse classification", "author": ["A. Majumdar", "R.K. Ward"], "venue": "Electrical and Computer Engineering, Canadian Journal of,", "citeRegEx": "Majumdar and Ward,? \\Q2009\\E", "shortCiteRegEx": "Majumdar and Ward", "year": 2009}, {"title": "Sparse approximate solutions to linear systems", "author": ["B. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan,? \\Q1995\\E", "shortCiteRegEx": "Natarajan", "year": 1995}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov", "I.U.E. Nesterov"], "venue": null, "citeRegEx": "Nesterov and Nesterov,? \\Q2004\\E", "shortCiteRegEx": "Nesterov and Nesterov", "year": 2004}, {"title": "Transfer learning for image classification with sparse prototype representations", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Quattoni et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2008}, {"title": "An efficient projection for l 1,infinity regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In ICML, pp", "citeRegEx": "Quattoni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2009}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "Trading accuracy for sparsity in optimization problems with sparsity constraints", "author": ["S. Shalev-Shwartz", "T. Zhang", "N. Srebro"], "venue": "Siam Journal on Optimization,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "Dave", "Platt", "John C"], "venue": "Document Analysis and Recognition,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Learning to learn: Introduction", "author": ["S. Thrun"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Thrun,? \\Q1996\\E", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B.,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Sharing visual features for multiclass and multiview object detection", "author": ["A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "Torralba et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2007}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Tropp and Gilbert,? \\Q2007\\E", "shortCiteRegEx": "Tropp and Gilbert", "year": 2007}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E. Xing", "A.Y. Ng", "M. Jordan", "S. Russell"], "venue": "In NIPS,", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Class-size independent generalization analysis of some discriminative multi-category classification", "author": ["T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Image classification using super-vector coding of local image descriptors", "author": ["X. Zhou", "K. Yu", "T. Zhang", "T. Huang"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "While, in general, finding the most accurate sparse predictor is known to be NP hard (Natarajan, 1995; Davis et al., 1997), two main approaches have been proposed for overcoming the hardness result.", "startOffset": 85, "endOffset": 122}, {"referenceID": 9, "context": "While, in general, finding the most accurate sparse predictor is known to be NP hard (Natarajan, 1995; Davis et al., 1997), two main approaches have been proposed for overcoming the hardness result.", "startOffset": 85, "endOffset": 122}, {"referenceID": 32, "context": "the Lasso algorithm (Tibshirani, 1996) and the compressed sensing literature (Candes & Tao, 2005; Donoho, 2006)).", "startOffset": 20, "endOffset": 38}, {"referenceID": 11, "context": "the Lasso algorithm (Tibshirani, 1996) and the compressed sensing literature (Candes & Tao, 2005; Donoho, 2006)).", "startOffset": 77, "endOffset": 111}, {"referenceID": 26, "context": "An alternative approach, which has recently been studied in (Quattoni et al., 2009; Duchi & Singer, 2009), generalizes the `1 norm based approach, and relies on mixednorms.", "startOffset": 60, "endOffset": 105}, {"referenceID": 36, "context": "This logistic multiclass loss function `(W, (x, y)) has several nice properties \u2014 see for example (Zhang, 2004).", "startOffset": 98, "endOffset": 111}, {"referenceID": 23, "context": "(4) is NP-hard (Natarajan, 1995; Davis et al., 1997).", "startOffset": 15, "endOffset": 52}, {"referenceID": 9, "context": "(4) is NP-hard (Natarajan, 1995; Davis et al., 1997).", "startOffset": 15, "endOffset": 52}, {"referenceID": 30, "context": "(Taskar et al., 2003)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 36, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 30, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 13, "context": "While this approach can yield predictors with a rather mild dependency of the required features on k (see for example the analysis in (Zhang, 2004; Taskar et al., 2003; Fink et al., 2006)), it relies on a-priori assumptions on the structure of X and Y.", "startOffset": 134, "endOffset": 187}, {"referenceID": 26, "context": "However, as pointed out in (Quattoni et al., 2009), these regularizers might yield a matrix with many nonzeros columns, and hence, will lead to a predictor that uses many features.", "startOffset": 27, "endOffset": 50}, {"referenceID": 20, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 1, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 2, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 26, "context": "The alternative approach, and the most relevant to our work, is the use of mix-norm regularizations like \u2016W\u2016\u221e,1 or \u2016W\u20162,1 (Lanckriet et al., 2004; Turlach et al., 2000; Argyriou et al., 2006; Bach, 2008; Quattoni et al., 2009; Duchi & Singer, 2009; Huang & Zhang, 2010).", "startOffset": 122, "endOffset": 269}, {"referenceID": 28, "context": "ShareBoost generalizes the fully corrective greedy selection procedure given in (Shalev-Shwartz et al., 2010) to the case of selection of groups of variables, and our analysis follows similar techniques.", "startOffset": 80, "endOffset": 109}, {"referenceID": 19, "context": "Obtaining group sparsity by greedy methods has been also recently studied in (Huang et al., 2009; Majumdar & Ward, 2009), and indeed, ShareBoost shares similarities with these works.", "startOffset": 77, "endOffset": 120}, {"referenceID": 19, "context": "We differ from (Huang et al., 2009) in that our analysis does not impose strong assumptions (e.", "startOffset": 15, "endOffset": 35}, {"referenceID": 19, "context": "In (Huang et al., 2009), a ratio between difference in objective and different in costs is used.", "startOffset": 3, "endOffset": 23}, {"referenceID": 33, "context": "Another related method is the JointBoost algorithm (Torralba et al., 2007).", "startOffset": 51, "endOffset": 74}, {"referenceID": 33, "context": "While the original presentation in (Torralba et al., 2007) seems rather different than the type of predictors we describe in eqn.", "startOffset": 35, "endOffset": 58}, {"referenceID": 33, "context": "In practice, (Torralba et al., 2007) relies on heuristics for finding C on each boosting step.", "startOffset": 13, "endOffset": 36}, {"referenceID": 31, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al.", "startOffset": 103, "endOffset": 116}, {"referenceID": 17, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al., 2010; Bengio et al., 2011), shared hidden structure (LeCun et al.", "startOffset": 208, "endOffset": 247}, {"referenceID": 4, "context": "Finally, we mention that feature sharing is merely one way for transferring information across classes (Thrun, 1996) and several alternative ways have been proposed in the literature such as target embedding (Hsu et al., 2010; Bengio et al., 2011), shared hidden structure (LeCun et al.", "startOffset": 208, "endOffset": 247}, {"referenceID": 21, "context": ", 2011), shared hidden structure (LeCun et al., 1998; Amit et al., 2007), shared prototypes (Quattoni et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 0, "context": ", 2011), shared hidden structure (LeCun et al., 1998; Amit et al., 2007), shared prototypes (Quattoni et al.", "startOffset": 33, "endOffset": 72}, {"referenceID": 25, "context": ", 2007), shared prototypes (Quattoni et al., 2008), or sharing underlying metric (Xing et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 35, "context": ", 2008), or sharing underlying metric (Xing et al., 2003).", "startOffset": 38, "endOffset": 57}, {"referenceID": 37, "context": "The second approach shows how to use ShareBoost for learning piece-wise linear predictors and is inspired by the super-vectors construction recently described in (Zhou et al., 2010).", "startOffset": 162, "endOffset": 181}, {"referenceID": 37, "context": "In fact, it is easy to verify that all smooth functions can be approximated by piece-wise linear functions (see for example the discussion in (Zhou et al., 2010)).", "startOffset": 142, "endOffset": 161}, {"referenceID": 36, "context": "A related bound, which is given in terms of the convex loss function, is described in (Zhang, 2004).", "startOffset": 86, "endOffset": 99}, {"referenceID": 33, "context": "The second experiment compares ShareBoost to mixed-norm regularization and to the JointBoost algorithm of (Torralba et al., 2007).", "startOffset": 106, "endOffset": 129}, {"referenceID": 33, "context": "Here we compare ShareBoost to the JointBoost algorithm of (Torralba et al., 2007).", "startOffset": 58, "endOffset": 81}, {"referenceID": 33, "context": "As in the previous experiment, we followed the experimental setup as in (Duchi & Singer, 2009) and ran JointBoost of (Torralba et al., 2007) using their published code with additional implementation of the BFS heuristic for pruning the 2 space of all class-subsets as described in their paper.", "startOffset": 117, "endOffset": 140}, {"referenceID": 3, "context": "For example, the Shape Context similarity measure introduced by (Belongie et al., 2002) uses a Bipartite matching algorithm between descriptors computed along 100 points in each image.", "startOffset": 64, "endOffset": 87}, {"referenceID": 6, "context": "The top MNIST performer (Ciresan et al., 2010) uses a feed-forward Neural-Net with 7.", "startOffset": 24, "endOffset": 46}, {"referenceID": 29, "context": "During training, geometrically distorted versions of the original examples were generated in order to expand the training set following (Simard et al., 2003) who introduced a warping scheme for that purpose.", "startOffset": 136, "endOffset": 157}, {"referenceID": 29, "context": "The set was expanded with 5 deformed versions of each input, using the method in (Simard et al., 2003).", "startOffset": 81, "endOffset": 102}], "year": 2011, "abstractText": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}