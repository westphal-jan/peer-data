{"id": "1201.3117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2012", "title": "Design of Emergent and Adaptive Virtual Players in a War RTS Game", "abstract": "basically, in ( one - player ) war real time strategy ( mls ) games a human player controls, in real time, actual army consisting ; a number of soldiers and her aim is to destroy the opponent's assets where the opponent is heavily virtual ( w. e., non - human player role ) player that potentially consists of a pre - programmed decision - making script. these mistakes have usually associated some well - known effects ( e. g., predictability, non - rationality, repetitive thinking, and sensation of artificial stupidity among spectators ). this paper describes a method for the overall generation of virtual players that adapt to the player skills ; this is done by building initially a model of the player behavior in real time during the game, simultaneously further defining the target player via this model in - between two teams. the technique also shows preliminary results obtained on a one player wrts game constructed again for experimentation.", "histories": [["v1", "Sun, 15 Jan 2012 20:06:07 GMT  (642kb,D)", "http://arxiv.org/abs/1201.3117v1", "IWINAC International Work-conference on the Interplay between Natural and Artificial Computation 2011"]], "COMMENTS": "IWINAC International Work-conference on the Interplay between Natural and Artificial Computation 2011", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["jos\\'e a garc\\'ia guti\\'errez", "carlos cotta", "antonio j fern\\'andez-leiva"], "accepted": false, "id": "1201.3117"}, "pdf": {"name": "1201.3117.pdf", "metadata": {"source": "CRF", "title": "Design of Emergent and Adaptive Virtual Players in a War RTS Game", "authors": ["Jos\u00e9 A. Gar\u0107\u0131a Guti\u00e9rrez", "Carlos Cotta", "Antonio J. Fern\u00e1ndez Leiva"], "emails": ["ccottap@lcc.uma.es", "afdez@lcc.uma.es"], "sections": [{"heading": "1 Introduction and related work", "text": "In an era in which computing power has boosted the graphical quality of videogames, players have turned their attention to other aspects of the game. In particular, they mostly request opponents exhibiting intelligent behavior. However, intelligence does not generally equate to playing proficiency but rather to interesting behaviors [1]. This issue is specifically relevant in real-time strategy (RTS) games that often employ two kinds of artificial intelligence (AI) [2]: one represented by a virtual player (VP, or non-player character \u2013 NPC) making decisions on a set of units (i.e., warriors, machines, etc.), and another one corresponding to the small units (usually with little or no intelligence). The design of these AIs is a complex task, and the reality in the industry is that in most of the RTS games, the NPC is basically controlled by a fixed script that has been previously programmed based on the experience of the designer/programmer. This script often comprises hundreds of rules, in the form if the game is in state S then Unit U should execute action A, to control the behavior of the components (i.e., units) under its control. This arises known problems: for instance, for players, the opponent behavior guided by fixed scripts can become predictable for the experienced player. Also, for AI programmers, the design of ar X iv :1\n20 1.\n31 17\nv1 [\ncs .N\nE ]\n1 5\nJa n\nvirtual players (VPs) can be frustrating because the games are becoming more and more complex with hundreds of different situations/states and therefore it is not easy to predict all the possible situations that could potentially happen and even more difficult to decide which is the most appropriate actions to take in such situations. As consequence, many RTS games contain \u2019holes\u2019 in the sense that the game stagnates or behaves incorrectly under very specific conditions (these problems rely on the category of \u2019artificial stupidity\u2019 [1]). Thus the reality of the simulation is drastically reduced and so too the interest of the player;\nIn addition, there are very interesting research problems in developing AI for Real-Time Strategy (RTS) games including planning in an uncertain world with incomplete information, learning, opponent modeling, and spatial and temporal reasoning [3]. This AI design is usually very hard due to the complexity of the search space (states describe large playing scenarios with hundreds of units simultaneously acting). Particular problems are caused by the large search spaces (environments consisting of many thousands of possible positions for each of hundreds, possibly thousands, of units) and the parallel nature of the problem - unlike traditional games, any number of moves may be made simultaneously [4]. Qualitative spatial reasoning (QSR) techniques can be used to reduce complex spatial states (e.g., using abstract representations of the space [5]). Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].\nEven in the case of designing a very good script for VPs, the designer has to confront another well known problem: the VP behavior is usually fixed and rarely adapts to the level (i.e., skills) of the player. In fact the player can lose interest in the game because either she is able to beat all the opponents in each level, or the virtual player always beat her. In this sense, the design of interesting (e.g., non-predictable) NPCs is not the only challenge though. It also has to adapt to the human player, since she might lose interest in the game otherwise (i.e., if the NPCs are too easy or too hard to beat).\nThere are many benefits attempting to build adaptive learning AI systems which may exist at multiple levels of the game hierarchy, and which evolve over time. This paper precisely deals with the issue of generating behaviors for the virtual player that evolve in accordance with the player\u2019s increasing abilities. This behavior emerges according to the player skill, and this emergent feature can make a RTS game more entertaining and less predictable in the sense that emergent behavior is not explicitly programmed but simply happens [14]. The attainment of adjustable and emergent virtual players consists here of a process of two stages that are iteratively executed in sequence: (1) a behavior model of the human player is created in real time during the execution of a game, and further the virtual player is evolved off-line (i.e., in between two games) via evolutionary algorithms till a state that it can compete with the player (not necessarily to beat her but to keep her interest in the game). This approach has been applied on a RTS game constructed specifically for experimentation, and we report here our experience."}, {"heading": "2 The Game", "text": "Here we describe the basic elements that compose our wRTS game.\nScenario. It will also be called indistinctly region or world and consists of a two-dimensional non-toroidal heterogeneous hostile and dynamic grid-world. The world is heterogeneous because the terrain is not uniform, hostile because there exist a virtual army whose mission is to destroy the human player-controlled army, and dynamic because the game conditions change depending on the actions taken by units of both armies. Regarding the heterogeneity of the terrain, each grid in the region can have one of the three following values: passable (each uni can traverse this grid), impassable (no unit can traverse it), and semi-impassable (there is a penalization of 1 point of energy - see below -to traverse it).\nArmy. There are two armies (also called indistinctly teams, represented by spiders and ladybirds) with a number of units (also called indistinctly soldiers or agents) and a flag to be defended. One army is controlled by the human player whereas the other one is guided by some kind of artificial intelligence (AI).\nGiven a unit u placed in a position (xu, yu) in the grid, its visual range (VRu) embraces any position (x, y) that is placed to a maximum distance \u03c6, that is to say, VRu = {(x, y) | \u221a (xu \u2212 x)2 + (yu \u2212 y)2 \u2264 \u03c6}. Initially, each agent only knows the grids that belongs to its visual range. In fact, during the game the scenario is not completely known for an army and only those regions that were already visualized by its agents are known. The global information that each army (independently if this is controlled by the AI or the player) has is the sum of all the information of their constituent soldiers. Note that (human or virtual) players only know the rival flag position if this has been detected in a grid by some of its agents previously. Also, each unit interacts with the environment by executing a number of actions (see below) and has certain level of health and energy that decreases with these interactions. The initial values for the health and energy of each soldier are 100 and 1000 respectively.\nFights. Full body combat can be executed at the soldier level by assigning a random value (in the interval [0,1]) to each unit involved in a fight with a rival soldier; the energy level of the unit with lowest value decreases 1 unit. A unit dies when its energy is zero. All the combats between soldiers are executed in sequence in one turn of the game.\nDecision-making. Along the game, the role of players is to make decisions with respect to the actions that the units of their respective associated armies should take. More specifically, both the virtual and human player can select a set of agents and further assign it a specific order that basically consists of a common action that each unit in this set has to execute.\nActions. The game is executed in turns; every turn, each soldier executes the last order received by its player. Six actions are possible in this game:\n\u2013 Move forward enemy: if applied to unit u, then this has to move towards the closer rival soldier according to VRu, otherwise (i.e., if no rival soldier exists in VRu) agent u has to move in direction to the region where there are more enemies according to the global information of the army.\n\u2013 Group/Run away: the soldier has to group with other team mates placed inside its visual range; if there are not mates in this range, then the agent has to move towards the position where its flag is placed. \u2013 Move forward objective: the agent advances towards the opponent\u2019s flag position if this is known, otherwise it moves randomly. \u2013 No operation: execute no operation. \u2013 Explore: the soldier moves towards a non-explored-yet region of the map. \u2013 Protect flag: The soldier has to move towards the position of its own flag; if\nit is already near to it, unit just should scout the zone.\nPerceptions. Each unit knows its specific situation in the game from the perceptions that receives in its visual range. In our game, the agent state is determined by its health (measured in three range values: low, medium and high), and the following boolean perceptions:\n1. Advantage state? (Sa): \u201cthe unit is into a handicap situation\u201d. This perception is true if, according to the information in its visual range, the number of mate soldiers is higher than the number of rival units, and false otherwise. 2. Under attack? (Ua): this perception is true of the soldier is involved in a full body combat with another opponent unit, and false otherwise. 3. Objective visible? (Ov): the opponent\u2019s flag is visible in its visual range.\nGame objective. The army that first captures the rival flag is the winner; if this situation is never reached after a number of turns, then the winner is the player whose army inflicts a higher damage to its rival army and where the damage is measured as number of dead units."}, {"heading": "2.1 Notes on specific issues", "text": "Observe that \u2018fighting\u2019 is not a specific action to be executed by the army units. This should not be surprising because \u2018fighting\u2019 is not usually controlled by players in standard RTS games as this action should often is executed at the unit level and not at the army level; in our game this action is automatically executed when a unit and a rival soldier meet in any grid of the scenario.\nAlso, note that five of the six possible actions that units can execute require to make movements in the game world. This basically means that some kind of pathfinding should be processed to do the movement realistic. Classical algorithms such as A\u2217 were not good candidates for this task since, as already indicated, most of the scenario is not known for the army units. A practical solution was to let units make a good-enough movement depending on its scenario knowledge; several cases were covered: Concave obstacles: the unit moves by considering different angles with respect to its position in the scenario; Convex obstacles: the unit moves by following the obstacle contour in a specific direction; and Bridges and narrow passages: a simple ant-colony algorithm was executed.\nDue to space limitations we will not explain them in details as this is beyond the objective of this paper. In any case, Figures 1(a),(b) and (c) show illustrative examples of the cases, Figure 1(d) illustrates the classical behavior of our ant\ncolony algorithms, and Figure 1(e) displays a screenshot of our wRTS game1 titled aracnia. Note also that a description of the technical issues of the game (e.g., implementation of basic actions, graphical considerations, input/output interface, scene rendering and memory management, among others) is beyond the scope of this paper. In the following we will focus on the process of designing virtual players."}, {"heading": "3 Virtual player design", "text": "Here we describe our proposal to design adaptive virtual players in our wRTS."}, {"heading": "3.1 General issues", "text": "Our aim is to generate controllers governing automatically (i.e., without human intervention) the behavior of an entire team (i.e., a set of autonomous agents/soldiers). An army controller can be viewed as a set of rules that will control the reactions of its agents according to the current state of play. Depending on its situation, the agent executes a particular action that may modify\n1 A prototype version of this game can be unloaded from http://www.lcc.uma.es/\u223cafdez/aracnia.\nAlgorithm 1: PMEA (VP)\n1 VP \u2190 RBP; // Rule-based expert system 2 for i \u2208 N\u2118 do 3 PlayerModel \u2190 PlayGameOn(VP); // Player modeling (PM) 4 VP \u2190 EA(PlayerModel,VP); // Evolutionary optimization (EA) 5 end for 6 return VP;\nthe state of the agent itself. The global team strategy emerges as the result of the sum of the specific behavior of all the agents.\nHowever, the definition of specific strategies for each agent is complex, requires a profound knowledge of the required behavior of each agent, and results in predictable behavior. Moreover, devising independent controllers for each unit in a RTS game is costly; in fact this is not realistic as requires a high computation effort that surely decreases the quality of the real-time rendering of the game. One solution consists of designing global team strategies; however, again this is very complex as the programmer has to cope with too many possibilities that arise from the different interactions among all the agents of both armies. We have opted by a more realistic process: all the soldiers will be managed by the same controller, and in any instant of the game, the team behavior will be the result of summing all the action taken by each of its constituent units. This means that we have to produce just one controller to devise a global team strategy. Note however that this does not mean all the agents execute the same action because the action to be executed by a soldier will depend on its particular situation in the match."}, {"heading": "3.2 A two phases process", "text": "The procedure described in the following has the aim of creating a virtual player whose behavior auto-evolves according to the skills that human player exhibits along the game. Algorithm 1 displays the general schema of the process: initially (in the first game), the virtual player (VP) is constructed as an expert system (i.e., a rule-based prototype (RBP)) that comprises tens of rules trying to cover all the possible situations in which an agent might be involved. This system was the result of a number of trials designed from the experiences of the authors playing wRTS games. Then, let Nh be {1, . . . , h} henceforth, and assume the human player will play a number of \u2118 games, the procedure consists of two phases that are sequentially executed. Firstly, a player modeling phase is conducted; this step is described in Section 3.3 and basically consists of building a model of the behavior that human player exhibits during the game (i.e., on-line). The second phase, described in Section 3.4, is devoted to construct the virtual player by evolving a strategy controlling the functioning of a virtual army unit via an evolutionary algorithm (EA). In the following we describe in details both phases. Firstly, we discuss how the virtual player is internally encoded as this is important to understand the functioning of the whole schema.\nRepresentation: the virtual player contains a NPC army strategy, and its internal encoding corresponds with both the rule-based expert system and any\nindividual in the EA population. In the following we will refer to \u2018individual\u2019 in a general sense. Every individual represents a team strategy, that is to say, a common set of actions that each army unit has to execute under its specific situation in the environment. An individual is represented as a vector v of k cells where k is the number of different situations in which the agent can be in the game, and v[i] (for 0 \u2264 i < k) contains the action to be taken in situation i. In other words, assumingm perceptions, where the perception pj (for 0 \u2264 j \u2264 m\u22121) can have kj possible values, then k = k0 \u2217 k1 \u2217 . . . \u2217 km\u22121 and the cell:\nv[em\u22121+em\u22122\u2217km\u22121+em\u22123\u2217(km\u22121\u2217km\u22122)+em\u22124\u2217(km\u22121\u2217km\u22122\u2217km\u22123)+. . . . . . ]\ncontains the action to be executed by a specific unit when its perceptions p0, p1, . . . , pm\u22121 have the values e0, e1, . . . , em\u22121 respectively. As indicated in Section 2, the state of a unit in our wRTS game is determined by three boolean perceptions and its energy level (with 3 different values) so that the encoding of an individual consists of a vector with 24 genes; this vector will be called answer matrix, and each cell in the vector will contain an action. Remember that 6 possible actions can be executed in our wRTS and thus the search space (i.e., the number of different strategies that can be generated from this representation) is 624. Figure 2(left) displays an example of a possible encoding. The optimal solution (if it exists) would be that strategy which always select the best action to be executed for the agents under all possible environmental conditions. In fact, this vast search space makes this problem impracticable for exact methods and justifies the use of evolutionary algorithms."}, {"heading": "3.3 On-line phase: User modeling", "text": "The player behavior will be modeled as a ruled-based strategy encoded as explained above. This process requires collecting, during the execution of the game (i.e., in real time) the actions that human player executes, recording additionally the specific conditions under which these are performed. At the end of this process we generate an extended answer matrix (v+) that is an answer matrix (i.e.,\nAlgorithm 2: EA(PModel, VP)) // PModel = Player Model, VP = Virtual Player 1 for i \u2208 Npopsize\u22121 do 2 popi \u2190Random-Solution(); 3 end for 4 poppopsize \u2190VP; 5 i\u2190 0; 6 while i < MaxGenerations do 7 Rank-Population (pop); // sort population according to fitness 8 parent1 \u2190Select (pop); // roulette wheel 9 parent2 \u2190 Select (pop);\n10 if Rand[0, 1] < pX then // recombination is done 11 (child1, child2)\u2190 Recombine (parent1, parent2); 12 else 13 (child1, child2)\u2190 (parent1, parent2); 14 end if 15 child1 \u2190 Mutate (child1, pM ); // pM = mutation probability 16 child2 \u2190 Mutate (child2, pM ); 17 fitness1 \u2190 PlayGameOff(PModel, child1); 18 fitness2 \u2190 PlayGameOff(PModel, child2); 19 pop\u2190 replace(pop, child1, child2); // (popsize + 2) replacement 20 i\u2190 i + 1; 21 end while 22 return best solution in pop;\nan individual encoding v) where each cell v+[i] (for 0 \u2264 i < k) represents now a vector of 6 positions (one per action) and v+[i][a] (for some action a \u2208 [1, 6]) contains the probability that human player executes action a under the environment conditions (i.e., the state) associated to cell v[i]. Figure 2(right) displays an example that shows the probability of executing each of the 6 possible actions in a specific situation of the unit (i.e., the soldier has medium energy, is in an advantage state, is not suffering an attack, and knows where the opponent flag is placed). This extended answer matrix is finally used to design the virtual player as follows: VP[i] = argmaxa\u2208[1,6]{v+[i][a]}, for all possible situations i."}, {"heading": "3.4 Off-line phase: evolutionary optimization", "text": "Algorithm 2 shows the basic schema of our EA. The initial population is randomly generated except one individual that is charged with the virtual player (lines 1-4). Then a classical process of evolution (lines 6-21) is performed and the best solution in the population is finally returned (draws are broken randomly).\nEvaluating the fitness of an individual x requires the simulation (off-line) of the game between the player model and the virtual player strategy encoded in x. The fitness function depends on the statistical data collected at the end of the simulation. The higher the number of statistical data to be collected, the higher the computational cost will be. A priori, a good policy is to consider a limited number of data. Four data were used in our experiments during offline evaluation: A: Number of deaths in the human player army; B: number of deaths in virtual player army; C: number of movements; and D: victory degree (i.e., 1 if virtual player wins and 2 otherwise). Fitness function was then defined as fitness(x) = 10000\u2217(A\u2212B)C\u2217D ; higher the fitness value, better the strategy. This fitness was coded to evolve towards aggressive solutions."}, {"heading": "4 Experimental analysis", "text": "The experiments were performed using two algorithms: our initial expert system (RBP), and the algorithm PMEA (i.e., player modeling + EA) presented in Algorithm 1. As to the PMEA, the EA uses popsize = 50, pX = .7, pM = .01, and MaxGenerations = 125; mutation is executed as usual at the gene level by changing an action to any other action randomly chosen. Three different scenarios where created for experimentation: (1) A map with size 50\u00d7 50 grids, 48 agents in VP army, and 32 soldiers in the human player (HP) team; (2) a map 54 \u00d7 46, with 43 VP soldiers, and 43 HP units; and (3) a map 50 \u00d7 28, with 48 VP soldiers, and 53 HP units. Algorithm 1 was executed for a value of \u2118 = 20 (i.e., 20 different games were sequentially played), and the RBP was also executed 20 times; Table 1 shows the results obtained.\nEven though in two of the three scenarios PMEA behaves better than RBP, note that no significant differences are shown; this is however an expected result as we have considered just one player what means that the player models obtained in-between two games are likely similar and thus their corresponding virtual players also are. In any case, this demonstrates that our approach is feasible as it produces virtual players comparable - and sometimes better - to specific and specialized pre-programmed scripts."}, {"heading": "5 Conclusions", "text": "We have described an algorithm to design automatically strategies exhibiting emergent behaviors that adapt to the user skills in a one-player war real time strategy game (wRTS); to do so, a model to mimic how the human player acts in the game is first constructed during the game, and further a strategy for a virtual player is evolved (in between games) via an evolutionary algorithm.\nOur proposal was compared with an expert system designed specifically for the game. Whereas no significance differences have been highlighted in the experiments, we make note that our approach has evident advantages compared to classical manufactured scripts (i.e., expert systems) used in videogame industry:\nfor instance, it avoids the predictability of actions to be executed by the virtual player and thus guarantees to maintain the interest of the player This is specially interesting when the game involves more than one player as our approach would allow to construct virtual players adapted particularly to each of the human players (and this cannot be obtained with a pre-programmed script).\nFurther research will cope with multi-player games and thus multi-objective evolutionary programming techniques should be considered.\nAcknowledgements This work is supported by project NEMESIS (TIN-200805941) of the Spanish Ministerio de Ciencia e Innovacio\u0301n, and project TIC-6083 of Junta de Andaluc\u0301\u0131a."}], "references": [{"title": "Artificial stupidity: The art of intentional mistakes", "author": ["L. Lid\u00e9n"], "venue": "AI Game Programming Wisdom 2, Charles River Media, INC.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Game Artificial Intelligence", "author": ["J.B. Ahlquist", "J. Novak"], "venue": "Game Development essentials. Thomson Delmar Learning, Canada", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Call for AI research in RTS games", "author": ["M. Buro"], "venue": "In Fu, D., Orkin, J., eds.: AAAI workshop on Challenges in Game AI, San Jose", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Steps toward building of a good ai for complex wargame-type simulation games", "author": ["V. Corruble", "C.A.G. Madeira", "G. Ramalho"], "venue": "In Mehdi, Q.H., Gough, N.E., eds.: 3rd International Conference on Intelligent Games and Simulation (GAME-ON 2002), London, UK", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "How qualitative spatial reasoning can improve strategy game ais", "author": ["K.D. Forbus", "J.V. Mahoney", "K. Dill"], "venue": "IEEE Intelligent Systems 17(4)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Playing to learn: case-injected genetic algorithms for learning to play computer games", "author": ["S.J. Louis", "C. Miles"], "venue": "IEEE Trans. Evol. Comput. 9(6)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time neuroevolution in the nero video game", "author": ["K.O. Stanley", "B.D. Bryant", "R. Miikkulainen"], "venue": "IEEE Trans. Evol. Comput. 9(6)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Coevolution in hierarchical ai for strategy games", "author": ["D. Livingstone"], "venue": "IEEE Symposium on Computational Intelligence and Games (CIG05), Essex, UK, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Co-evolving real-time strategy game playing influence map trees with genetic algorithms", "author": ["C. Miles", "S.J. Louis"], "venue": "In Press, I., ed.: International Congress on Evolutionary Computation, Portland, Oregon", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolving teams of cooperating agents for real-time strategy game", "author": ["P. Lichocki", "K. Krawiec", "W. Jaskowski"], "venue": "In et al., M.G., ed.: 1st European Workshop on Bio-inspired Algorithms in Games. Volume 5484 of LNCS., T\u00fcbingen, Germany, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Intelligent anti-grouping in real-time strategy games", "author": ["N Beume"], "venue": "In Press, I., ed.: International Symposium on Computational Intelligence in Games, Perth, Australia", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolving robust strategies for an abstract realtime strategy game", "author": ["D. Keaveney", "C. O\u2019Riordan"], "venue": "In Press, I., ed.: International Symposium on Computational Intelligence in Games, Milano. Italy", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A multi-agent potential field-based bot for a full RTS game scenario", "author": ["J. Hagelb\u00e4ck", "S.J. Johansson"], "venue": "In Darken, C., Youngblood, G.M., eds.: Proc.Fifth Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE 2009), Stanford, California, USA, The AAAI Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Emergence in Games", "author": ["P. Sweetser"], "venue": "Game development. Charles River Media, Boston, Massachussetts", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "However, intelligence does not generally equate to playing proficiency but rather to interesting behaviors [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "This issue is specifically relevant in real-time strategy (RTS) games that often employ two kinds of artificial intelligence (AI) [2]: one represented by a virtual player (VP, or non-player character \u2013 NPC) making decisions on a set of units (i.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "As consequence, many RTS games contain \u2019holes\u2019 in the sense that the game stagnates or behaves incorrectly under very specific conditions (these problems rely on the category of \u2019artificial stupidity\u2019 [1]).", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "In addition, there are very interesting research problems in developing AI for Real-Time Strategy (RTS) games including planning in an uncertain world with incomplete information, learning, opponent modeling, and spatial and temporal reasoning [3].", "startOffset": 244, "endOffset": 247}, {"referenceID": 3, "context": "Particular problems are caused by the large search spaces (environments consisting of many thousands of possible positions for each of hundreds, possibly thousands, of units) and the parallel nature of the problem unlike traditional games, any number of moves may be made simultaneously [4].", "startOffset": 287, "endOffset": 290}, {"referenceID": 4, "context": ", using abstract representations of the space [5]).", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 6, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 7, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 8, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 9, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 10, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 11, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 12, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 13, "context": "This behavior emerges according to the player skill, and this emergent feature can make a RTS game more entertaining and less predictable in the sense that emergent behavior is not explicitly programmed but simply happens [14].", "startOffset": 222, "endOffset": 226}, {"referenceID": 0, "context": "Full body combat can be executed at the soldier level by assigning a random value (in the interval [0,1]) to each unit involved in a fight with a rival soldier; the energy level of the unit with lowest value decreases 1 unit.", "startOffset": 99, "endOffset": 104}, {"referenceID": 0, "context": "9 parent2 \u2190 Select (pop); 10 if Rand[0, 1] < pX then // recombination is done", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "an individual encoding v) where each cell v[i] (for 0 \u2264 i < k) represents now a vector of 6 positions (one per action) and v[i][a] (for some action a \u2208 [1, 6]) contains the probability that human player executes action a under the environment conditions (i.", "startOffset": 152, "endOffset": 158}, {"referenceID": 5, "context": "an individual encoding v) where each cell v[i] (for 0 \u2264 i < k) represents now a vector of 6 positions (one per action) and v[i][a] (for some action a \u2208 [1, 6]) contains the probability that human player executes action a under the environment conditions (i.", "startOffset": 152, "endOffset": 158}, {"referenceID": 0, "context": "This extended answer matrix is finally used to design the virtual player as follows: VP[i] = argmaxa\u2208[1,6]{v[i][a]}, for all possible situations i.", "startOffset": 101, "endOffset": 106}, {"referenceID": 5, "context": "This extended answer matrix is finally used to design the virtual player as follows: VP[i] = argmaxa\u2208[1,6]{v[i][a]}, for all possible situations i.", "startOffset": 101, "endOffset": 106}], "year": 2012, "abstractText": "Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent\u2019s assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a oneplayer wRTS game constructed specifically for experimentation.", "creator": "TeX"}}}