{"id": "1202.3723", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Approximation by Quantization", "abstract": "inference in graphical models consists of repeatedly multiplying and summing some potentials. it is generally intractable because the derived potentials obtained measure this way, be exponentially large. approximate inference techniques such as belief propagation data reflection methods encourage this by simplifying the derived potentials, explicitly by dropping variables incorporating them. we propose an alternate method for fuzzy potentials : quantizing their values. quantization causes different states of computed potential to express the same value, and therefore introduces context - specific independencies algorithms can be exploited to alter the values more compactly. we mix algebraic decision diagrams ( adds ) to do this efficiently. we apply quantization and add reduction to variable elimination and junction tree propagation, yielding a family of bounded approximate inference schemes. our experimental considerations show that our approximate schemes significantly outperform state - making - the - art approaches on many benchmark instances.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (149kb)", "http://arxiv.org/abs/1202.3723v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vibhav gogate", "pedro domingos"], "accepted": false, "id": "1202.3723"}, "pdf": {"name": "1202.3723.pdf", "metadata": {"source": "CRF", "title": "Approximation by Quantization", "authors": ["Vibhav Gogate"], "emails": ["vgogate@cs.washington.edu", "pedrod@cs.washington.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Many widely used approximate inference algorithms such as mini-bucket elimination (Dechter and Rish, 2003) and the generalized mean-field algorithm (Xing et al., 2003) are essentially scope-based approximations. The approximation is invoked when either the factors of the posterior distribution or intermediate functions generated during the execution of a variable elimination algorithm are too large to fit in memory or too time-consuming to compute. Since the time and memory cost of processing a function is exponential in its scope size (in this paper, we consider only discrete graphical models), these schemes reduce complexity by approximating a large-scope function by several small-scope functions. For instance, the generalized mean field algorithm approximates each component Pi of the posterior distribution by a tractable component Qi defined over a subset\nof the scope of Pi such that the KL divergence between Qi and Pi is minimized. The reasons for the popularity of the scope-based approach are obvious; it is a very natural and simple idea, it is easy to implement and its complexity can be easily controlled.\nIn this paper, we propose a fundamentally different but complementary class of range-based approximations: the main idea is to quantize a function by mapping a number of distinct values in its range to a single value. When the number of distinct values in the range is reduced, the function becomes more compressible and the time required to manipulate it may decrease substantially. Unfortunately, if we represent functions using tables, namely if we store a real number for every possible configuration of all variables appearing in the function\u2019s scope, quantization will be useless because we will not reduce the representation size. In other words, we need structured representations to take advantage of quantization.\nMany structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al., 2008) and formula-based representations (Gogate and Domingos, 2010). When a function has a large number of similar values (as a result of quantization or not), the size and compute time of these representations can be exponentially smaller than the tabular representation. Although one can use any of these structured representations or combinations to compactly represent a quantized function, in this paper we propose to use ADDs (Bahar et al., 1993). ADDs are canonical representations of functions, and have many efficient manipulation algorithms. In particular, all inference operations: multiplication, maximization, and elimination can be efficiently implemented using standard ADD operations. Another advantage of ADDs is that there is a large literature on them. This has led to the wide availability of many efficient open source software implementations (e.g., CUDD Somenzi (1998)), which can be leveraged to efficiently and quickly implement the ideas presented in this paper.\nQuantization is a general principle that can be applied to a variety of probabilistic inference algorithms. In this paper, we apply it to two standard algorithms: bucket (or variable) elimination (Dechter, 1999) and the junction tree algorithm (Lauritzen and Spiegelhalter, 1988), yielding approximate, anytime and coarse-to-fine versions of these schemes. Just like mini-bucket elimination (Dechter and Rish, 2003) and related iterative algorithms such as expectation propagation (Minka, 2001) and generalized belief propagation (Yedidia et al., 2004), one can view our new schemes as running exact inference on a simplified version of the graphical model. All approximate schemes proposed to date define a simplified model as a low treewidth model.1 However, treewidth is an overly strong condition for determining feasibility of exact inference (Chavira and Darwiche, 2008). For example, algorithms such as ADD-VE (Chavira and Darwiche, 2007) and formula decomposition and conditioning (Gogate and Domingos, 2010) can solve problems having large treewidth by taking advantage of context-specific independence (or identical potential values) (Boutilier et al., 1996) and determinism. Quantization artificially introduces context-specific independence and thus enables us to define a new class of approximations that take advantage of the efficiency and power of the aforementioned schemes by simplifying the graphical model in a much finer manner.\nWe present experimental results on four classes of benchmark problems: Ising models, logistics planning instances, networks for medical diagnosis and coding networks. Our experiments show that schemes that utilize quantization and ADD reduction significantly outperform state-of-theart bounding and approximate inference approaches when the graphical model has a large number of similar probability values or local structure such as determinism and context-specific independence. When the network does not have these properties, our algorithms are slightly inferior to the best-performing state-of-the-art scheme but superior to other state-of-the-art approaches.\nThe rest of the paper is organized as follows. Section 2 describes background. Section 3 presents quantization. Section 4 presents approximate inference schemes based on quantization. Experimental results are presented in Section 5 and we conclude in Section 6."}, {"heading": "2 PRELIMINARIES", "text": ""}, {"heading": "2.1 MARKOV NETWORKS", "text": "For simplicity, we focus on Markov networks defined over bi-valued variables. Our approach can be easily applied\n1The only exception we are aware of is the recent work of (Lowd and Domingos, 2010), who compile an arithmetic circuit (which are structured representations similar to ADDs) from dependent samples generated from the posterior distribution. Our approach is very different, and empirically seems to yield much greater speedups (although to date there is no head-to-head comparison in the same domains because an implementation of the Lowd and Domingos scheme is not available).\nto multi-valued variables, and other graphical models such as Bayesian networks and Markov logic (Domingos and Lowd, 2009). Let X = {X1, . . . , Xn} be a set of bivalued (Boolean) variables taking values from the domain {0, 1} (or {False,True}). A Markov network denoted by M, is a pair (X,F) where X is a set of variables and F = {F1, . . . , Fm} is a collection of potentials or realvalued Boolean functions of the form {0, 1}k \u2192 R+. Each potential Fi is defined over a subset of variables, denoted by V (Fi) \u2282 X, also called its scope. The set of values in the range of Fi is denoted by R(Fi). A Markov network represents the following probability distribution:\nPr(x) = 1\nZ\nm \u220f\ni=1\nFi(xV (Fi)) (1)\nwhere x is a 0/1 truth assignment to all variables X \u2208 X, xV (Fi) is the projection of x on the scope of Fi and Z = \u2211\nx\n\u220fm\ni=1 Fi(xV (Fi)) is the normalization constant, also called the partition function.\nIn this paper, we will focus on the approximating the partition function Z and the marginal distribution P (Xi = xi) at each variable Xi. Our approach can be easily extended to other problems such as computing the most probable explanation (MPE)."}, {"heading": "2.2 ALGEBRAIC DECISION DIAGRAMS", "text": "An algebraic decision diagram (ADD) is an efficient graph representation of a real-valued Boolean function. It is a directed acyclic graph (DAG) in which each leaf node is labeled by a real value and each non-leaf decision node is labeled by a variable. Each decision node has two outgoing arcs corresponding to the true and false assignments of the corresponding variable. ADDs enforce a strict variable ordering from the root to the leaf node and impose the following three constraints on the DAG: (i) no two arcs emanating from a decision node can point to the same node, (ii) if two decision nodes have the same variable label, then they cannot have (both) the same true child node and the same false child node and (iii) no two leaf nodes are labeled by the same real value. ADDs that do not satisfy these constraints are referred to as unreduced ADDs (while those that do are called reduced ADDs). An unreduced ADD can be reduced by merging isomorphic subgraphs and eliminating any nodes whose two children are isomorphic (for details, see Bahar et al. (1993)). ADDs are canonical representations of real-valued Boolean functions, namely, two functions will have the same ADD (under the same variable ordering) iff they are the same.\nFigure 1 shows a real-valued Boolean function and its corresponding ADD.\nAll inference operations (including sum, product, elimination, etc.) can be efficiently implemented using ADDs; their complexity is polynomial in the size of the corresponding ADDs. Unfortunately, the time and memory constants involved in using ADDs are much larger than those involved\nin using tables. Because of this, ADD-based elimination (and multiplication) may be more expensive, both timewise and memory-wise, even when they perform fewer numeric operations than table-based elimination (and multiplication). However, when a function has a substantial amount of context-specific independence, the ADD operations can be significantly faster."}, {"heading": "3 QUANTIZATION", "text": "Quantization is the process of replacing a range of real numbers by a single number. Formally, a quantization function denoted by Q, is a many-to-one mapping from a set T to a set Q of real numbers, where |T| \u2265 |Q|. Let F be a real valued Boolean function, Q be a set of real numbers and Q be a quantization function from R(F ) to Q. We say that a function FQ is a quantization of F w.r.t. Q if FQ is constructed from F by replacing each value w in the range of F by Q(w). Quantization may reduce the size of the ADD of a function, but it will never increase it. Formally: Proposition 1. Let FQ denote the quantization of F w.r.t. Q. Then, the ADD of FQ is smaller than or equal to the ADD of F .\nFigure 2 demonstrates the effect of quantization on the size of the ADD given in Figure 1(b).\nAs mentioned in the introduction, the main problem in approximate inference is to find a small bounded function that approximates a large intractable function such that the approximation error is minimized. Assuming that we represent the function using ADDs and approximate using quantizations, we can formalize this problem as follows.\nQuantization Problem: Given a function F , an integer constant k and an error measure D (e.g., KL divergence, mean-squared error, etc.), find a (optimal) quantization FQ of F such that:\n\u2022 Size Constraint: The size of the ADD of FQ is less than or equal to k.\n\u2022 Error Constraint: There does not exist a quantization FQ\u2032 of F such that the size of the ADD of FQ\u2032 is less than or equal to k and D(F, FQ\u2032) < D(F, FQ).\nUnfortunately, finding an optimal quantization is extremely hard because the quantization problem is a multi-objective constrained optimization problem. Therefore, we propose the following three heuristics.\nOur first heuristic optimizes for error and solves the following relaxation: given an integer l and an ADD \u03c6F (\u03c6F represents a function F ) having t leaves, find an ADD \u03c6FQ (that represents the quantization FQ of F ) having l leaves such that D(F, FQ) is minimized. This problem can be solved in O(lt) time using dynamic programming and matrix searching (see Wu (1991) for details). Given l, the relaxation optimizes FQ in terms of the error measure D while disregarding the size of the ADD of FQ (although since l < t, \u03c6FQ will be smaller than \u03c6F ). To use this heuristic for solving the quantization problem, we have to determine the value of l that will yield an ADD having less than k + 1 nodes. To find l, we use binary search. We call this heuristic the min-error heuristic.\nOur second heuristic solves the following relaxation: given an integer l and an ADD \u03c6F having t leaves, find an ADD \u03c6FQ having l leaves such that there does not exist an ADD \u03c6F \u2032\nQ that has l leaves but fewer nodes than \u03c6FQ (FQ\u2032 is a quantization of F ). Unfortunately, this relaxation is much harder to solve than the relaxation that optimizes the error. Therefore, we use the following (heuristic) technique to solve it. As before, we perform a binary search over l starting with l = t/2. At each search point, we select a leaf node and merge it with another leaf that shares the largest number of parents with it (ties broken by the relative difference between the leaf values). When two leaves having the same parent are merged, the parent will point to the same leaf node in the new (unreduced) ADD and will be deleted when the ADD is reduced. Notice that the heuristic ignores the error measure D (except when breaking ties) and reduces the ADD size by merging as fewer leaves as possible. Therefore, we call this heuristic the min-merge heuristic.\nIn practice, we can run both heuristics in parallel, compute the error between the original function and the quantized function obtained using each heuristic, and choose the quantized function having the smallest error. We call this heuristic the min-error-merge heuristic.\nWe will evaluate the performance of both the heuristics as well as the combination in the experimental section. Note that when approximations without bounding guarantees are\nAlgorithm 1: ABQ(k) Input: A Markov network M and a size bound k Output: An estimate of the partition function of M begin\nHeuristically select a variable ordering o = (X1, . . . , Xn). Express each potential of M as an ADD. // Create Buckets Let BXi be the bucket of Xi. Put each ADD in the bucket of its highest ordered variable. Z = 1 for i = n downto 1 do\nrepeat // Process the Bucket of Xi if BXi contains only one ADD \u03c61 then\n\u03c6 = \u2211\nXi \u03c61\nPut \u03c6 in the bucket of its highest ordered variable. If \u03c6 has no variables then Z = Z \u00d7 \u03c6 Delete \u03c61 from BXi\nelse Heuristically select \u03c61 and \u03c62 from BXi . \u03c6 = \u03c61 \u00d7 \u03c62 Delete \u03c61 and \u03c62 from BXi . if the size of \u03c6 is greater than k then\n// Quantization step \u03c6q= ADD formed by repeatedly quantizing and reducing \u03c6 until its size is less than k. Put \u03c6q in BXi .\nelse Put \u03c6 in BXi .\nuntil BXi is empty return Z\nend\ndesired, we assign the median value of the merged leaves to the new leaf. When upper (or lower) bounds are desired, we assign the maximum (or the minimum) value instead."}, {"heading": "4 APPROXIMATION BY QUANTIZATION", "text": "In this section, we apply quantization and ADD reduction to two standard inference algorithms: (i) bucket or variable elimination (Dechter, 1999), and (ii) junction tree propagation (Lauritzen and Spiegelhalter, 1988). Applying quantization and ADD reduction to the former yields a one-pass algorithm for computing the partition function similar to mini-bucket elimination (Dechter and Rish, 2003), and applying it to the latter yields an iterative algorithm that can compute posterior marginal distribution at each variable, similar to expectation propagation (Minka, 2001)."}, {"heading": "4.1 ONE-PASS APPROXIMATION BY QUANTIZATION (ABQ)", "text": "Before describing our algorithm, we give background on bucket elimination. Bucket elimination (BE) (Dechter, 1999) is an exact algorithm for computing the partition function. The algorithm maintains a database of valid functions that is partitioned into buckets, one for each variable.\nGiven an ordering o of variables, the algorithm partitions the potentials of a Markov network by putting each potential in the bucket of the highest ordered variable in its scope. The algorithm operates by eliminating variables one by one, along o. A variable X is eliminated by computing a product of all the functions in its bucket, and then summing out X from this product. This creates a new function, whose scope is the union of the scopes of all functions that mention X , minus {X}. The algorithm then deletes the functions involving X (namely the bucket of X) from the database of valid functions, adds the newly created function to it and continues. The function (a real number) created by eliminating the last bucket equals the partition function. It is known that the time and space complexity of BE is exponential in the treewidth of the Markov network.\nBE assumes tabular representation of functions. It can be easily extended to use ADDs yielding the ADD-BE algorithm, first presented in (Chavira and Darwiche, 2007). In ADD-BE, we represent all functions using ADDs and use ADD operators for elimination and multiplication. Unfortunately, just like BE, it is an exact algorithm and is therefore not scalable to interesting real-world applications.\nWe propose to make ADD-BE practical by quantizing large ADDs generated during its execution. Algorithm 1 describes the proposed scheme. The algorithm takes as input a Markov network M and a size bound k and outputs an estimate of the partition function. It is essentially a standard ADD-based bucket elimination algorithm except for the quantization step. Here, given an ADD whose size is greater than k, we repeatedly merge its leaf nodes using the heuristics described in the previous section, until its size is smaller than k. Note that when k = \u221e the algorithm runs full bucket elimination and is equivalent to the ADD-BE algorithm of (Chavira and Darwiche, 2007). Thus, ABQ represents an anytime, anyspace bounded approximation of ADD-BE, controlled by the size bound k.\nWe mention an important technical detail which can positively impact both the complexity and accuracy of ABQ. Notice that after quantizing an ADD, some variables may become irrelevant (for example, variable C is irrelevant to the ADD of Figure 2(b) because it does not appear in any of its internal nodes). Thus, instead of adding the quantized ADD to the current bucket, we can safely transfer it to the bucket of its highest ordered relevant variable. Note that variables may also become irrelevant when we multiply two ADDs or eliminate the bucket variable from the ADD. Obviously, we can use the same approach in these cases too and transfer the newly generated ADD to the bucket of its highest ordered relevant variable.\nThe time and space complexity of Algorithm 1 is summarized in the following theorem: Theorem 1. The time complexity of ABQ(k) is O(mk2) where m is the number of potentials and k is the size bound. Its space complexity is O(max(mk, k2)).\nAlgorithm 1 can be easily extended to yield an upper\nAlgorithm 2: IABQ(k) Input: A Markov network M and a ADD size-bound k Output: A set of junction tree cliques containing potentials and messages received from neighbors begin\nConstruct a junction tree for M Let (e1, . . . , el) be an ordering of edges of the junction-tree for message-passing from leaves to the root repeat\nfor i = 1 to l do Let ei = (ui, vi) send-message(ui, vi, k) for i = l downto 1 do Let ei = (ui, vi) send-message(vi, ui, k)\nuntil convergence or timeout end\n(lower) bound on the partition function. All we have to do is ensure that the quantization function Q(x) used by ABQ is an upper (lower) approximation, namely \u2200w FQ(w) \u2265 F (w) (\u2200w FQ(w) \u2264 F (w)). Trivially, a quantization function that replaces each value in the interval by the maximum (minimum) value is an upper (lower) approximation. Formally, Theorem 2. If all quantizations in Algorithm ABQ(k) use a quantization function Q satisfying \u2200w FQ(w) \u2265 F (w), then the output of ABQ(k) is an upper bound on the partition function. On the other hand, if Q satisfies \u2200w FQ(w) \u2264 F (w), then ABQ(k) yields a lower bound on the partition function."}, {"heading": "4.2 ITERATIVE APPROXIMATION BY QUANTIZATION (IABQ)", "text": "In this section, we will show how to approximate the junction tree algorithm (Lauritzen and Spiegelhalter, 1988) using quantization and ADD reduction. The junction tree algorithm is a message-passing algorithm over a modified graph called the junction tree, which is obtained by clustering together variables of a Markov network until the network becomes a tree. The clusters are also called cliques. Each clique is associated with a subset of potentials such that the scope of each potential is covered by the variables in the cliques. The message-passing works as follows. First, we designate an arbitrary cluster as the root and send messages in two passes: from the leaves to the root (inward pass) and then from the root to the leaves (outward pass). The message that a clique u sends to its neighbor v is constructed as follows. In clique u, we multiply all the potentials associated with u, with all the messages received from its neighbors except v, and then eliminate all variables that appear in u but not in v. The time and space complexity of the junction tree algorithm is exponential in the maximum cluster size of the junction tree used.\nWe can construct an approximate version of the junctiontree algorithm using quantization and ADD reduction in\nProcedure send-message(u, v, k) Input: Cliques u and v of a junction tree and a constant k Output: v with the old message (ADD) from u replaced by a new message begin\nLet (\u03c6u,1, . . . , \u03c6u,k) be a heuristic ordering of the ADDs currently in the clique u except the message received from v \u03c6u,v = 1 for i = 1 to k do\n\u03c6u,v = \u03c6u,v \u00d7 \u03c6u,i if the size of \u03c6u,v is greater than k then\n// Quantization step \u03c6u,v= ADD formed by repeatedly quantizing and reducing \u03c6u,v until its size is smaller than k\nLet sep(u, v) = clique(u) \u2229 clique(v) \u03c6u,v = \u2211 clique(u)\\sep(u,v) \u03c6u,v\nReplace the old message from u in v with \u03c6u,v end\na straight-forward manner. Algorithm 2 describes our approach. The algorithm first constructs a junction tree for the Markov network and then sends messages along its edges using the send-message procedure. In the send-message procedure, we send a message from a clique u to clique v by multiplying all ADDs corresponding to the messages (except the one received from v) and potentials. Just as in ABQ, if the size of the product ADD is larger than k, we recursively apply quantization and ADD reduction until its size is smaller than or equal to k. Since, the message propagation is performed on a tree, the algorithm will always converge in two passes (assuming that the quantization heuristics do not change between passes).\nIABQ belongs to the class of sum-product expectation propagation (EP) algorithms (see Minka (2001) and Koller and Friedman (2009), Chapter 11) which perform inference by sending approximate messages. In practice, we can further improve the accuracy of IABQ by performing belief-update propagation instead of sum-product propagation. Belief-update IABQ constructs the message from clique u to clique v by first multiplying, and quantizing if necessary, all the incoming messages (including the one received from v). Then, it projects the resulting factor on sep(u, v) and divides it by the message \u03c6v,u received from v (thus unlike sum-product IABQ, belief-update IABQ requires the division operation). Belief-update IABQ is not guaranteed to converge in two passes and may not converge at all. However, as we shall see in the experimental section, when it does converge, it often converges very quickly (in 10-30 iterations) and yields highly accurate estimates.\nIABQ yields a new class of bounded EP algorithms. Existing bounded EP algorithms use treewidth to determine feasibility of inference. In particular, in the junction tree algorithm, the message between u and v corresponds to a (local) fully-connected (clique) graphical model over the separator sep(u, v). Existing EP algorithms ensure tractability by sending bounded treewidth messages (achieved by introducing new conditional independencies between the sepa-\nrator variables). IABQ, on the other hand, can create messages having substantially larger treewidth than existing EP algorithms. This is because it uses quantization and ADDs to introduce context-specific independencies between the separator variables."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we compare the performance of ABQ and IABQ with other algorithms from the literature. We also evaluate the impact of various quantization heuristics on accuracy. We experimented with instances from four benchmark domains: (i) logistics planning (Sang et al., 2005), (ii) linear block coding, (iii) Promedas Bayesian networks for medical diagnosis (Wemmenhove et al., 2007) and (iv) Ising models. We implemented our algorithms in C++. We ran our experiments on a Linux machine with a 2.33 GHz Intel Xeon quad-core processor and 16 GB of RAM. We gave each algorithm a memory limit of 2GB and (unless otherwise specified) a time limit of 2 hours. We used the CUDD package (Somenzi, 1998) to implement ADDs. We used the minfill ordering heuristic for constructing the junction tree in IABQ and for eliminating variables in ABQ."}, {"heading": "5.1 EXPERIMENTS EVALUATING THE BOUNDING POWER OF ABQ", "text": "When exact results are not available, evaluating the capability of approximate schemes is problematic because the quality of the approximation (namely how close the approximation is to the exact) cannot be assessed. To allow some comparison on large, hard instances, we evaluate the upper bounding power of ABQ, and compare it with three algorithms from literature: mini-bucket elimination (MBE) (Dechter and Rish, 2003; Rollon and Dechter, 2010), Treereweighted Belief Propagation (TRW) (Wainwright et al., 2003) and Box propagation (BoxProp) (Mooij and Kappen, 2008). For a fair comparison, we also compare with our own ADD based implementation of mini-bucket elimination (ADD-MBE). ADD-MBE represents all messages and potentials in the MBE algorithm using ADDs instead of tables (both ADD-MBE and ABQ use the same variable ordering). BoxProp was derived for bounding posterior probabilities and therefore Z is obtained by applying the chain rule to individual bounds on posteriors. We experimented with anytime versions of MBE, ADD-MBE and ABQ. Namely, we start with a crude size-bound, k = 2, and increase it progressively by multiplying it by 2, until the algorithm runs out of memory or time. Recall that in ABQ, k bounds the size of the ADD. In MBE, it bounds the size of the new functions created by the algorithm. The results in this subsection were obtained using the min-error-merge heuristic described in Section 3 (we compare the impact of heuristics on accuracy in the next subsection).\nNote that almost all the instances that we consider in this subsection are quite hard and the exact value of their par-\ntition function is not known (except for the logistics instances). Table 1 shows the results. The first column shows the instance name. The second column shows various statistics for the instance such as the number of variables (n), the domain size (d), the number of potentials (p) and the upper bound on treewidth obtained using the minfill ordering heuristic (w). Columns 3, 4, 5, 6 and 7 show the upper bound on the partition function computed by ABQ, MBE, BoxProp, TRW and ADD-MBE respectively. For each scheme, we also report the relative difference \u2206 (defined below) between the log of the best known upper bound UBest and the log of the upper bound U output by the current scheme.\n\u2206 = log(U)\u2212 log(UBest)\nlog(UBest) (2)\nThe log-relative difference provides a quantitative measure for assessing the relative approximation quality of the bounding schemes (smaller is better).\nLogistics planning instances Our first domain is that of logistics planning (the networks are available from (Sang\net al., 2005)). Given prior probabilities on actions and facts, the task is to compute the probability of evidence. From Table 1, we can see that ABQ significantly outperforms MBE, ADD-MBE, TRW and BoxProp on all instances (the logrelative difference is quite large). ADD-MBE is much superior to MBE on most instances. This is because the domain has a large amount of determinism and identical probability values which ADD-MBE exploits effectively. ADD-MBE is worse than ABQ suggesting that quantization-based approximations are much better in terms of accuracy than MBE-based approximations.\nMedical Diganosis: Promedas networks Our second domain is that of noisy-OR medical diagnosis networks generated by the Promedas expert system for internal medicine (Wemmenhove et al., 2007). The global architecture of the diagnostic model in Promedas is similar to the QMR-DT medical diagnosis networks (Shwe et al., 1991). Each network can be specified using a two layer bipartite graph in which the top layer consists of diseases and the bottom layer consists of symptoms. If a disease causes a symptom, there is an edge from the disease to the symptom. The networks are available from UAI 2008 evaluation website (Darwiche et al., 2008). From Table 1, we can see that ABQ is superior to MBE, ADD-MBE, TRW and BoxProp on all instances (notice that for all instances the log-relative difference between ADD based schemes and others is quite large).\nCoding networks Our third domain is random coding networks from the class of linear block codes (Kask and Dechter, 1999) (the networks are available from the UAI 2008 evaluation website (Darwiche et al., 2008)). From Table 1, we can see that ABQ outperforms MBE, TRW and BoxProp on all instances, except BN 131. On this network, MBE is slightly better than ABQ (because of the overhead of ADDs). On all other networks, ABQ is slightly superior to MBE. ADD-MBE is worse than ABQ on all instances. Again, our results on the coding networks clearly demonstrate that quantization with ADD reduction is a better approximation strategy than MBE.\nIsing models Our last domain is that of Ising models which are n \u00d7 n pair-wise grid networks. They are specified using potentials defined over each edge and each node. Each node potential is given by (\u03b3, 1/\u03b3) where \u03b3 is drawn uniformly between 0 and 1. The edge potentials are either (\u03b8, 1/\u03b8, 1/\u03b8, \u03b8) or their mirror image (1/\u03b8, \u03b8, \u03b8, 1/\u03b8) where \u03b8 is drawn uniformly between 1 and \u03b2 (\u03b2 is called the coupling strength). We use \u03b2 = 100 to generate our networks. From Table 1, we can see that ABQ outperforms BoxProp, TRW and ADD-MBE on these models. However, it is slightly inferior to MBE (notice that the log-relative difference between ABQ and MBE is very small).\nIntuitively, ABQ should do well when the graphical model contains many similar or identical probability values in each potential. Ising models are interesting in this respect because they represent the worst possible case for ABQ, with no determinism or context-specific structure at\nall. Remarkably, ABQ still outperforms BoxProp, TRW and ADD-MBE on these models. In our initial experiments it also outperformed MBE, but it does slightly worse than the latest version, which is the one reported in Table 1. MBE employs sophisticated partitioning heuristics (Rollon and Dechter, 2010) that could also be incorporated into ABQ, and many other optimizations characteristic of a mature system; its good performance relative to ABQ is likely due to these improvements, rather than to the basic algorithm. However, there is in general a tradeoff in using ADDs versus tables, as shown by the ADD-MBE results: ADDs can be exponentially smaller and faster by taking advantage of context-specific independence and determinism, but ADDs incur higher overhead than tables, so the latter may be preferable when there is no structure to exploit.\nOverall, we see that that ABQ always outperforms TRW, BoxProp and ADD-MBE, and outperforms MBE on all domains except Ising models. ABQ\u2019s advantage increases with the the amount of (approximate or exact) contextspecific independence and determinism in the domain, but ABQ still does quite well even when these are absent."}, {"heading": "5.2 EXPERIMENTS EVALUATING THE QUANTIZATION HEURISTICS", "text": "In this subsection, we evaluate the performance of the three quantization heuristics described in Section 3. Table 2 shows the results. We can see that the min-error-merge heuristic performs the best overall. The min-merge heuristic is only slightly inferior to the min-error-merge heuristic. The min-error heuristic is inferior to the min-merge heuristic except on the promedas networks. The promedas networks have many similar probability values (approximate context-specific independence) which the min-error heuristic exploits quite effectively. On the other hand, the Ising models represent the worst possible case for the min-error heuristic because the intermediate potentials generated during ABQs execution have almost no similar probability values."}, {"heading": "5.3 EXPERIMENTS EVALUATING THE ACCURACY OF IABQ", "text": "In this subsection, we evaluate the accuracy of belief-update IABQ for computing posterior marginals. We compare IABQ with Iterative Join Graph propagation (IJGP) (Mateescu et al., 2010), a state-of-the-art generalized belief propagation scheme (IJGP won 2 out of the 3 marginal estimation categories at the 2010 UAI approximate evaluation challenge (Elidan and Globerson, 2010)). As a baseline, we\ncompare with Gibbs sampling (Geman and Geman, 1984). We ran both IJGP and IABQ as anytime algorithms. Both algorithms take as input a size parameter which determine their complexity. We vary this parameter starting with its lowest possible value, progressively increasing it until the algorithm runs out of memory or time. We ran each algorithm for 1 hour and gave each algorithm a memory limit of 2GB. Both IJGP and IABQ may or may not converge to a fixed point. Therefore, we ran each for 50 iterations or until convergence, whichever was earlier. Convergence is detected by comparing the absolute difference between messages at the current and previous iteration.\nWe measure performance using the KL divergence. Let P (Xi) and Q(Xi) denote the exact and approximate marginals of variable Xi. Then, the average KL divergence is defined as:\nKL(P,A) = 1\n|X|\n\u2211\nXi\u2208X\n\u2211\nxi\nP (xi) log\n(\nP (xi)\nQ(xi)\n)\nFor brevity, we only describe our results for two sample instances: (a) a logistics planning instance, and (b) a 18 x 18 Ising model. Average KL divergence vs. time plots for these instances are given in Figure 3. Our results are consistent with the empirical evidence in the previous subsection. Specifically, when the graphical model has many identical or similar probability values, IABQ dominates IJGP (e.g., on the log-3 instance). However, when the graphical model does not have these properties, IJGP is slightly better than IABQ because of the overhead of ADDs.\nFigure 4 shows the impact of increasing the number of iterations on the accuracy of IABQ for different values of the size bound parameter k. We can see that IABQ converges to its fixed point in about 10-20 iterations. Its accuracy typically increases with k and with the number of iterations. This shows that the belief-update IABQ performs better than sum-product IABQ (sum-product IABQ is equivalent to running just one iteration of belief-update IABQ)."}, {"heading": "6 CONCLUSION", "text": "The most challenging problem in approximate inference is how to approximate a large function that is computation-\nally infeasible by a collection of tractable functions. The paper proposes to solve this problem using quantization. Quantization replaces a number of values in the range of a function by a single value, and thus artificially introduces context-specific independence. Conventional tabular representations of functions are inadequate at exploiting this structure. We therefore proposed to use structured representations such as algebraic decision diagrams (ADDs).\nWe showed how quantization can be applied to two standard algorithms in probabilistic inference, variable elimination and junction tree propagation, yielding two new schemes: (i) A one-pass algorithm that can be used to approximate and bound the partition function and (ii) An iterative algorithm that can be used for approximating posterior marginals. Our new approximate schemes significantly enhance the class of approximations considered by existing algorithms, which constrain their approximations to have low treewidth. By imposing context-specific independencies between variables via quantization, our new algorithms construct structured approximations in the high treewidth space. Our empirical evaluation demonstrates that schemes that employ quantization often yield more accurate results than schemes that do not. Thus approximation by quantization is a promising approach for future investigations.\nAcknowledgements This research was partly funded by ARO grant W911NF-08-1-0242, AFRL contract FA8750-09-C0181, DARPA contracts FA8750-05-2-0283, FA8750-07-D-0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-D030010, NSF grants IIS-0534881 and IIS-0803481, and ONR grant N00014-08-1-0670. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, DARPA, NSF, ONR, or the U.S. Government."}], "references": [{"title": "Algebraic decision diagrams and their applications", "author": ["R. Bahar", "E. Frohm", "C. Gaona", "G. Hachtel", "E. Macii", "A. Pardo", "F. Somenzi"], "venue": "ICCAD, pages 188\u2013191.", "citeRegEx": "Bahar et al\\.,? 1993", "shortCiteRegEx": "Bahar et al\\.", "year": 1993}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "UAI, pages 115\u2013123.", "citeRegEx": "Boutilier et al\\.,? 1996", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Compiling bayesian networks using variable elimination", "author": ["M. Chavira", "A. Darwiche"], "venue": "IJCAI, pages 2443\u20132449.", "citeRegEx": "Chavira and Darwiche,? 2007", "shortCiteRegEx": "Chavira and Darwiche", "year": 2007}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Arti. Intell., 172(6-7):772\u2013799.", "citeRegEx": "Chavira and Darwiche,? 2008", "shortCiteRegEx": "Chavira and Darwiche", "year": 2008}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "JACM, 50(3):280\u2013305.", "citeRegEx": "Darwiche,? 2003", "shortCiteRegEx": "Darwiche", "year": 2003}, {"title": "Results from the Probablistic Inference Evaluation of UAI\u201908", "author": ["A. Darwiche", "R. Dechter", "A. Choi", "V. Gogate", "L. Otten"], "venue": "Available online at: http://graphmod.ics.uci.edu/uai08/Evaluation/Report.", "citeRegEx": "Darwiche et al\\.,? 2008", "shortCiteRegEx": "Darwiche et al\\.", "year": 2008}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Arti. Intell., 113(1-2):41\u201385.", "citeRegEx": "Dechter,? 1999", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. Dechter", "I. Rish"], "venue": "JACM, 50(2):107\u2013153.", "citeRegEx": "Dechter and Rish,? 2003", "shortCiteRegEx": "Dechter and Rish", "year": 2003}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["P. Domingos", "D. Lowd"], "venue": "Morgan and Claypool.", "citeRegEx": "Domingos and Lowd,? 2009", "shortCiteRegEx": "Domingos and Lowd", "year": 2009}, {"title": "The 2010 UAI approximate inference challenge", "author": ["G. Elidan", "A. Globerson"], "venue": "Available online at: http://www.cs.huji.ac.il/project/UAI10/index.php", "citeRegEx": "Elidan and Globerson,? 2010", "shortCiteRegEx": "Elidan and Globerson", "year": 2010}, {"title": "Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transaction on Pattern analysis and Machine Intelligence, 6(6):721\u2013742.", "citeRegEx": "Geman and Geman,? 1984", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Formula-based probabilistic inference", "author": ["V. Gogate", "P. Domingos"], "venue": "UAI, pages 210\u2013219.", "citeRegEx": "Gogate and Domingos,? 2010", "shortCiteRegEx": "Gogate and Domingos", "year": 2010}, {"title": "Mini-bucket heuristics for improved search", "author": ["K. Kask", "R. Dechter"], "venue": "UAI, pages 314\u2013323.", "citeRegEx": "Kask and Dechter,? 1999", "shortCiteRegEx": "Kask and Dechter", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman.,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman.", "year": 2009}, {"title": "Bayesian inference in presence of determinism", "author": ["D. Larkin", "R. Dechter"], "venue": "AISTATS.", "citeRegEx": "Larkin and Dechter,? 2003", "shortCiteRegEx": "Larkin and Dechter", "year": 2003}, {"title": "Local computation with probabilities on graphical structures and their application to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B, 50(2):157\u2013224.", "citeRegEx": "Lauritzen and Spiegelhalter,? 1988", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Approximate inference by compilation to arithmetic circuits", "author": ["D. Lowd", "P. Domingos"], "venue": "NIPS, pages 1477\u20131485.", "citeRegEx": "Lowd and Domingos,? 2010", "shortCiteRegEx": "Lowd and Domingos", "year": 2010}, {"title": "AND/OR multi-valued decision diagrams (AOMDDs) for graphical models", "author": ["R. Mateescu", "R. Dechter", "Marinescu"], "venue": null, "citeRegEx": "Mateescu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mateescu et al\\.", "year": 2008}, {"title": "Joingraph propagation algorithms", "author": ["R. Mateescu", "K. Kask", "V. Gogate", "R. Dechter"], "venue": "JAIR, 37:279\u2013328.", "citeRegEx": "Mateescu et al\\.,? 2010", "shortCiteRegEx": "Mateescu et al\\.", "year": 2010}, {"title": "Expectation Propagation for approximate Bayesian inference", "author": ["T. Minka"], "venue": "UAI, pages 362\u2013369.", "citeRegEx": "Minka,? 2001", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Bounds on marginal probability distributions", "author": ["J.M. Mooij", "H.J. Kappen"], "venue": "NIPS, pages 1105\u20131112.", "citeRegEx": "Mooij and Kappen,? 2008", "shortCiteRegEx": "Mooij and Kappen", "year": 2008}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "UAI, pages 467\u2013475.", "citeRegEx": "Murphy et al\\.,? 1999", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Exploiting contextual independence in probabilistic inference", "author": ["D. Poole", "N. Zhang"], "venue": "JAIR, 18:263\u2013313.", "citeRegEx": "Poole and Zhang,? 2003", "shortCiteRegEx": "Poole and Zhang", "year": 2003}, {"title": "New mini-bucket partitioning heuristics for bounding the probability of evidence", "author": ["E. Rollon", "R. Dechter"], "venue": "AAAI, pages 1199\u20131204.", "citeRegEx": "Rollon and Dechter,? 2010", "shortCiteRegEx": "Rollon and Dechter", "year": 2010}, {"title": "Heuristics for fast exact model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "SAT, pages 226\u2013240.", "citeRegEx": "Sang et al\\.,? 2005", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Probabilistic diagnosis using a reformulation of the internist- 1/qmr knowledge base i", "author": ["M. Shwe", "B. Middleton", "D. Heckerman", "M. Henrion", "E. Horvitz", "H. Lehmann", "G. Cooper."], "venue": "the probabilistic model and inference algorithms. Methods of Information in Medicine, 30:241\u2013255.", "citeRegEx": "Shwe et al\\.,? 1991", "shortCiteRegEx": "Shwe et al\\.", "year": 1991}, {"title": "Cudd: CU decision diagram package release", "author": ["F. Somenzi"], "venue": null, "citeRegEx": "Somenzi,? \\Q1998\\E", "shortCiteRegEx": "Somenzi", "year": 1998}, {"title": "Tree-reweighted belief propagation algorithms and approximate ml estimation by pseudo-moment matching", "author": ["M.J. Wainwright", "T.S. Jaakkola", "A.S. Willsky"], "venue": "AISTATS.", "citeRegEx": "Wainwright et al\\.,? 2003", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Inference in the promedas medical expert system", "author": ["B. Wemmenhove", "J. Mooij", "W. Wiegerinck", "M. Leisink", "H. Kappen", "J. Neijt"], "venue": "AI in Medicine, volume 4594, pages 456\u2013460.", "citeRegEx": "Wemmenhove et al\\.,? 2007", "shortCiteRegEx": "Wemmenhove et al\\.", "year": 2007}, {"title": "Optimal quantization by matrix searching", "author": ["X. Wu"], "venue": "Journal of Algorithms 12(4):663\u2013673.", "citeRegEx": "Wu,? 1991", "shortCiteRegEx": "Wu", "year": 1991}, {"title": "A generalized mean field algorithm for variational inference in exponential families", "author": ["E. Xing", "M. Jordan", "S. Russell"], "venue": "UAI, pages 583\u2013591.", "citeRegEx": "Xing et al\\.,? 2003", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory, 51:2282\u20132312.", "citeRegEx": "Yedidia et al\\.,? 2004", "shortCiteRegEx": "Yedidia et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 7, "context": "Many widely used approximate inference algorithms such as mini-bucket elimination (Dechter and Rish, 2003) and the generalized mean-field algorithm (Xing et al.", "startOffset": 82, "endOffset": 106}, {"referenceID": 30, "context": "Many widely used approximate inference algorithms such as mini-bucket elimination (Dechter and Rish, 2003) and the generalized mean-field algorithm (Xing et al., 2003) are essentially scope-based approximations.", "startOffset": 148, "endOffset": 167}, {"referenceID": 22, "context": "Many structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 14, "context": "Many structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al.", "startOffset": 132, "endOffset": 158}, {"referenceID": 2, "context": "Many structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al.", "startOffset": 195, "endOffset": 223}, {"referenceID": 4, "context": "Many structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al.", "startOffset": 245, "endOffset": 261}, {"referenceID": 17, "context": "Many structured representations have been proposed in literature such as confactors (Poole and Zhang, 2003), sparse representations (Larkin and Dechter, 2003), algebraic decision diagrams (ADDs) (Chavira and Darwiche, 2007), arithmetic circuits (Darwiche, 2003), AND/OR multi-valued decision diagrams (Mateescu et al., 2008) and formula-based representations (Gogate and Domingos, 2010).", "startOffset": 301, "endOffset": 324}, {"referenceID": 11, "context": ", 2008) and formula-based representations (Gogate and Domingos, 2010).", "startOffset": 42, "endOffset": 69}, {"referenceID": 0, "context": "Although one can use any of these structured representations or combinations to compactly represent a quantized function, in this paper we propose to use ADDs (Bahar et al., 1993).", "startOffset": 159, "endOffset": 179}, {"referenceID": 0, "context": "Although one can use any of these structured representations or combinations to compactly represent a quantized function, in this paper we propose to use ADDs (Bahar et al., 1993). ADDs are canonical representations of functions, and have many efficient manipulation algorithms. In particular, all inference operations: multiplication, maximization, and elimination can be efficiently implemented using standard ADD operations. Another advantage of ADDs is that there is a large literature on them. This has led to the wide availability of many efficient open source software implementations (e.g., CUDD Somenzi (1998)), which can be leveraged to efficiently and quickly implement the ideas presented in this paper.", "startOffset": 160, "endOffset": 619}, {"referenceID": 6, "context": "In this paper, we apply it to two standard algorithms: bucket (or variable) elimination (Dechter, 1999) and the junction tree algorithm (Lauritzen and Spiegelhalter, 1988), yielding approximate, anytime and coarse-to-fine versions of these schemes.", "startOffset": 88, "endOffset": 103}, {"referenceID": 15, "context": "In this paper, we apply it to two standard algorithms: bucket (or variable) elimination (Dechter, 1999) and the junction tree algorithm (Lauritzen and Spiegelhalter, 1988), yielding approximate, anytime and coarse-to-fine versions of these schemes.", "startOffset": 136, "endOffset": 171}, {"referenceID": 7, "context": "Just like mini-bucket elimination (Dechter and Rish, 2003) and related iterative algorithms such as expectation propagation (Minka, 2001) and generalized belief propagation (Yedidia et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 19, "context": "Just like mini-bucket elimination (Dechter and Rish, 2003) and related iterative algorithms such as expectation propagation (Minka, 2001) and generalized belief propagation (Yedidia et al.", "startOffset": 124, "endOffset": 137}, {"referenceID": 31, "context": "Just like mini-bucket elimination (Dechter and Rish, 2003) and related iterative algorithms such as expectation propagation (Minka, 2001) and generalized belief propagation (Yedidia et al., 2004), one can view our new schemes as running exact inference on a simplified version of the graphical model.", "startOffset": 173, "endOffset": 195}, {"referenceID": 3, "context": "1 However, treewidth is an overly strong condition for determining feasibility of exact inference (Chavira and Darwiche, 2008).", "startOffset": 98, "endOffset": 126}, {"referenceID": 2, "context": "For example, algorithms such as ADD-VE (Chavira and Darwiche, 2007) and formula decomposition and conditioning (Gogate and Domingos, 2010) can solve problems having large treewidth by taking advantage of context-specific independence (or identical potential values) (Boutilier et al.", "startOffset": 39, "endOffset": 67}, {"referenceID": 11, "context": "For example, algorithms such as ADD-VE (Chavira and Darwiche, 2007) and formula decomposition and conditioning (Gogate and Domingos, 2010) can solve problems having large treewidth by taking advantage of context-specific independence (or identical potential values) (Boutilier et al.", "startOffset": 111, "endOffset": 138}, {"referenceID": 1, "context": "For example, algorithms such as ADD-VE (Chavira and Darwiche, 2007) and formula decomposition and conditioning (Gogate and Domingos, 2010) can solve problems having large treewidth by taking advantage of context-specific independence (or identical potential values) (Boutilier et al., 1996) and determinism.", "startOffset": 266, "endOffset": 290}, {"referenceID": 16, "context": "The only exception we are aware of is the recent work of (Lowd and Domingos, 2010), who compile an arithmetic circuit (which are structured representations similar to ADDs) from dependent samples generated from the posterior distribution.", "startOffset": 57, "endOffset": 82}, {"referenceID": 8, "context": "to multi-valued variables, and other graphical models such as Bayesian networks and Markov logic (Domingos and Lowd, 2009).", "startOffset": 97, "endOffset": 122}, {"referenceID": 0, "context": "An unreduced ADD can be reduced by merging isomorphic subgraphs and eliminating any nodes whose two children are isomorphic (for details, see Bahar et al. (1993)).", "startOffset": 142, "endOffset": 162}, {"referenceID": 29, "context": "This problem can be solved in O(lt) time using dynamic programming and matrix searching (see Wu (1991) for details).", "startOffset": 93, "endOffset": 103}, {"referenceID": 6, "context": "In this section, we apply quantization and ADD reduction to two standard inference algorithms: (i) bucket or variable elimination (Dechter, 1999), and (ii) junction tree propagation (Lauritzen and Spiegelhalter, 1988).", "startOffset": 130, "endOffset": 145}, {"referenceID": 15, "context": "In this section, we apply quantization and ADD reduction to two standard inference algorithms: (i) bucket or variable elimination (Dechter, 1999), and (ii) junction tree propagation (Lauritzen and Spiegelhalter, 1988).", "startOffset": 182, "endOffset": 217}, {"referenceID": 7, "context": "Applying quantization and ADD reduction to the former yields a one-pass algorithm for computing the partition function similar to mini-bucket elimination (Dechter and Rish, 2003), and applying it to the latter yields an iterative algorithm that can compute posterior marginal distribution at each variable, similar to expectation propagation (Minka, 2001).", "startOffset": 154, "endOffset": 178}, {"referenceID": 19, "context": "Applying quantization and ADD reduction to the former yields a one-pass algorithm for computing the partition function similar to mini-bucket elimination (Dechter and Rish, 2003), and applying it to the latter yields an iterative algorithm that can compute posterior marginal distribution at each variable, similar to expectation propagation (Minka, 2001).", "startOffset": 342, "endOffset": 355}, {"referenceID": 6, "context": "Bucket elimination (BE) (Dechter, 1999) is an exact algorithm for computing the partition function.", "startOffset": 24, "endOffset": 39}, {"referenceID": 2, "context": "It can be easily extended to use ADDs yielding the ADD-BE algorithm, first presented in (Chavira and Darwiche, 2007).", "startOffset": 88, "endOffset": 116}, {"referenceID": 2, "context": "Note that when k = \u221e the algorithm runs full bucket elimination and is equivalent to the ADD-BE algorithm of (Chavira and Darwiche, 2007).", "startOffset": 109, "endOffset": 137}, {"referenceID": 15, "context": "In this section, we will show how to approximate the junction tree algorithm (Lauritzen and Spiegelhalter, 1988) using quantization and ADD reduction.", "startOffset": 77, "endOffset": 112}, {"referenceID": 18, "context": "IABQ belongs to the class of sum-product expectation propagation (EP) algorithms (see Minka (2001) and Koller and Friedman (2009), Chapter 11) which perform inference by sending approximate messages.", "startOffset": 86, "endOffset": 99}, {"referenceID": 13, "context": "IABQ belongs to the class of sum-product expectation propagation (EP) algorithms (see Minka (2001) and Koller and Friedman (2009), Chapter 11) which perform inference by sending approximate messages.", "startOffset": 103, "endOffset": 130}, {"referenceID": 24, "context": "We experimented with instances from four benchmark domains: (i) logistics planning (Sang et al., 2005), (ii) linear block coding, (iii) Promedas Bayesian networks for medical diagnosis (Wemmenhove et al.", "startOffset": 83, "endOffset": 102}, {"referenceID": 28, "context": ", 2005), (ii) linear block coding, (iii) Promedas Bayesian networks for medical diagnosis (Wemmenhove et al., 2007) and (iv) Ising models.", "startOffset": 90, "endOffset": 115}, {"referenceID": 26, "context": "We used the CUDD package (Somenzi, 1998) to implement ADDs.", "startOffset": 25, "endOffset": 40}, {"referenceID": 7, "context": "To allow some comparison on large, hard instances, we evaluate the upper bounding power of ABQ, and compare it with three algorithms from literature: mini-bucket elimination (MBE) (Dechter and Rish, 2003; Rollon and Dechter, 2010), Treereweighted Belief Propagation (TRW) (Wainwright et al.", "startOffset": 180, "endOffset": 230}, {"referenceID": 23, "context": "To allow some comparison on large, hard instances, we evaluate the upper bounding power of ABQ, and compare it with three algorithms from literature: mini-bucket elimination (MBE) (Dechter and Rish, 2003; Rollon and Dechter, 2010), Treereweighted Belief Propagation (TRW) (Wainwright et al.", "startOffset": 180, "endOffset": 230}, {"referenceID": 27, "context": "To allow some comparison on large, hard instances, we evaluate the upper bounding power of ABQ, and compare it with three algorithms from literature: mini-bucket elimination (MBE) (Dechter and Rish, 2003; Rollon and Dechter, 2010), Treereweighted Belief Propagation (TRW) (Wainwright et al., 2003) and Box propagation (BoxProp) (Mooij and Kappen, 2008).", "startOffset": 272, "endOffset": 297}, {"referenceID": 20, "context": ", 2003) and Box propagation (BoxProp) (Mooij and Kappen, 2008).", "startOffset": 38, "endOffset": 62}, {"referenceID": 28, "context": "Medical Diganosis: Promedas networks Our second domain is that of noisy-OR medical diagnosis networks generated by the Promedas expert system for internal medicine (Wemmenhove et al., 2007).", "startOffset": 164, "endOffset": 189}, {"referenceID": 25, "context": "The global architecture of the diagnostic model in Promedas is similar to the QMR-DT medical diagnosis networks (Shwe et al., 1991).", "startOffset": 112, "endOffset": 131}, {"referenceID": 5, "context": "The networks are available from UAI 2008 evaluation website (Darwiche et al., 2008).", "startOffset": 60, "endOffset": 83}, {"referenceID": 12, "context": "Coding networks Our third domain is random coding networks from the class of linear block codes (Kask and Dechter, 1999) (the networks are available from the UAI 2008 evaluation website (Darwiche et al.", "startOffset": 96, "endOffset": 120}, {"referenceID": 5, "context": "Coding networks Our third domain is random coding networks from the class of linear block codes (Kask and Dechter, 1999) (the networks are available from the UAI 2008 evaluation website (Darwiche et al., 2008)).", "startOffset": 186, "endOffset": 209}, {"referenceID": 23, "context": "MBE employs sophisticated partitioning heuristics (Rollon and Dechter, 2010) that could also be incorporated into ABQ, and many other optimizations characteristic of a mature system; its good performance relative to ABQ is likely due to these improvements, rather than to the basic algorithm.", "startOffset": 50, "endOffset": 76}, {"referenceID": 18, "context": "We compare IABQ with Iterative Join Graph propagation (IJGP) (Mateescu et al., 2010), a state-of-the-art generalized belief propagation scheme (IJGP won 2 out of the 3 marginal estimation categories at the 2010 UAI approximate evaluation challenge (Elidan and Globerson, 2010)).", "startOffset": 61, "endOffset": 84}, {"referenceID": 9, "context": ", 2010), a state-of-the-art generalized belief propagation scheme (IJGP won 2 out of the 3 marginal estimation categories at the 2010 UAI approximate evaluation challenge (Elidan and Globerson, 2010)).", "startOffset": 171, "endOffset": 199}, {"referenceID": 10, "context": "compare with Gibbs sampling (Geman and Geman, 1984).", "startOffset": 28, "endOffset": 51}], "year": 2011, "abstractText": "Inference in graphical models consists of repeatedly multiplying and summing out potentials. It is generally intractable because the derived potentials obtained in this way can be exponentially large. Approximate inference techniques such as belief propagation and variational methods combat this by simplifying the derived potentials, typically by dropping variables from them. We propose an alternate method for simplifying potentials: quantizing their values. Quantization causes different states of a potential to have the same value, and therefore introduces contextspecific independencies that can be exploited to represent the potential more compactly. We use algebraic decision diagrams (ADDs) to do this efficiently. We apply quantization and ADD reduction to variable elimination and junction tree propagation, yielding a family of bounded approximate inference schemes. Our experimental tests show that our new schemes significantly outperform state-of-the-art approaches on many benchmark instances.", "creator": "gnuplot 4.4 patchlevel 2"}}}