{"id": "1203.2002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2012", "title": "Graph partitioning advance clustering technique", "abstract": "scaling is a common technique for statistical data analysis, then is the process of grouping the data into classes or clusters so that objects within a directory have high populations having respect to one another, but are very dissimilar to objects in other clusters. dissimilarities are assessed based on the attribute values describing the objects. often, distance measures are available. clustering provide an unsupervised measurement solution, where observed values and structures can be computed directly containing very large data sets with little or none of the background background. this paper also considers the partitioning of m - dimensional lattice graphs using fiedler's approach, which requires the determination of the eigenvector belonging to the second smallest eigenvalue against the laplacian with k - means partitioning algorithm.", "histories": [["v1", "Fri, 9 Mar 2012 07:08:10 GMT  (245kb)", "http://arxiv.org/abs/1203.2002v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["t soni madhulatha"], "accepted": false, "id": "1203.2002"}, "pdf": {"name": "1203.2002.pdf", "metadata": {"source": "CRF", "title": "GRAPH PARTITIONING ADVANCE CLUSTERING TECHNIQUE", "authors": ["T. Soni Madhulatha"], "emails": ["latha.gannarapu@gmail.com"], "sections": [{"heading": null, "text": "DOI : 10.5121/ijcses.2012.3109 91\nClustering is a common technique for statistical data analysis, Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters. Dissimilarities are assessed based on the attribute values describing the objects. Often, distance measures are used. Clustering is an unsupervised learning technique, where interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. This paper also considers the partitioning of mdimensional lattice graphs using Fiedler\u2019s approach, which requires the determination of the eigenvector belonging to the second smallest Eigen value of the Laplacian with K-means partitioning algorithm.\nKEYWORDS\nClustering, K-means, Iterative relocation, Fiedler Approach, Symmetric Matrix, Laplacian matrix, Eigen values."}, {"heading": "1. INTRODUCTION", "text": "Unlike classification and regression, which analyze class-labeled data sets, clustering analyzes data objects without consulting class labels. In many cases, class labeled data may simply not exist at the beginning[1]. The objects are clustered or grouped based on the principle of maximizing the intraclass similarity and minimizing the interclass similarity. That is a cluster is a collection of data objects that are similar to one another within the same cluster and are dissimilar to the objects in other cluster. Each cluster so formed cab be viewed as a class of object from which rules can be derived. Besides the term data clustering as synonyms like cluster analysis, automatic classification, numerical taxonomy, botrology and typological analysis."}, {"heading": "1.1 Types of Data in cluster Analysis and variable types", "text": "Before looking into the clustering algorithms first of all we need to study about the type of data that often occur in the cluster analysis. Main memory based clustering algorithms operates on either on object-by-variable structure or object-by-object structure. Object-by-variable is represented as Data matrix where as the object-by-object structure is represented as Dissimilarity matrix. As per the clustering principle we need to calculate the dissimilarity between the objects. The objects cited in data mining text book by Han and Kamber are described as: Interval-scaled variables: The variables which are continuous measurements of a roughly linear scale. Example: Marks, Age, Height etc.\nBinary variables: This variable has only two states either 0 or 1.\nNominal variables: Nominal is the generalization of binary variable which can take more than two states. Example rainbow colors have VIBGRO colors so six states are considered.\nOrdinal variables: These variables are very useful for registering subjective assessment qualities that cannot be measured objectively. It is a set of continuous data of an unknown scale\nRatio-scaled variables: These variables make a positive measurement on a non-linear scale, such as an exponential scale."}, {"heading": "1.2 Categorization of clustering methods", "text": "There exist a large number of clustering algorithms in the literature. The choice of clustering algorithm depends both on the type of data available and on the particular purpose and application. If cluster analysis is used as a descriptive or exploratory tool, it is possible to try several algorithms on the same data to see what the data may disclose. In general, major clustering methods can be classified into the following categories.\n1. Hierarchical\n2. Density\nThe above methods are not contemporary methods\nPartitioning methods\n3. K-means\n4. K-Medoids\n5. Markov Clustering Algorithm(MCL)\n6. Non-negative matrix factorization (NMF)\n7. Singular Value Decomposition (SVD)\nThe above require preliminary knowledge of data in order to choose k\nSome clustering algorithms integrate the ideas of several clustering methods, so that it is sometimes difficult to classify a given algorithm as uniquely belonging to only one clustering method category."}, {"heading": "2. CLASSICAL PARTITIONING METHODS", "text": "The simplest and the most fundamental version of cluster analysis is partitioning, which organizes the object of a set into several exclusive groups or clusters. The most commonly used partitioning methods are:\nk-mean algorithm\nk-medoids algorithm and their variations"}, {"heading": "2.1 K-MEANS ALGORITHM", "text": "K means clustering algorithm was developed by J. McQueen and then by J. A. Hartigan and M. A. Wong around 1975. The k-means algorithm takes the input parameter, k, and partitions a set of n objects into k clusters so that the resulting intra-cluster similarity is high whereas the inter-cluster similarity is low. Cluster similarity is measured in regard to the mean value of the objects in a cluster, which can be viewed as the cluster's center of gravity.\nAlgorithm [1] : The k-means algorithm for partitioning based on the mean value of the objects in the cluster.\nInput: The number of clusters k, and a database containing n objects.\nOutput: A set of k clusters which minimizes the squared-error criterion.\nMethod:\n1) arbitrarily choose k objects as the initial cluster centers;\n2) repeat\n3) (re)assign each object to the cluster to which the object is the most similar,\nbased on the mean value of the objects in the cluster;\n4) update the cluster means, i.e., calculate the mean value of the objects for each cluster;\n5) until no change;\nProcedure\nConsider a set of objects with 2-Dimensions (PSCP and CO), let k=4 where k is number\nof clusters which a user would like the objects to be partitioned.\nAccording to the algorithm we arbitrarily choose four objects as four initial cluster\ncenters. Each object is assigned to the cluster based on the cluster center to which it is nearest. The distance between the object and cluster center is measured by Euclidean distance measure because the variables which we are using are of type of interval-based.\nIteration0\nHALL TICKET NO. PSCP CO 11087-i-0001 72 55 11087-i-0002 47 42 11087-i-0003 74 50 11087-i-0004 60 59 11087-i-0005 47 42 11087-i-0006 46 42 11087-i-0007 83 65 11087-i-0008 83 71 11087-i-0009 59 61 11087-i-0010 0 0 11087-i-0011 64 47 11087-i-0012 56 66 11087-i-0013 67 49 11087-i-0014 57 52 11087-i-0015 54 54 11087-i-0016 42 48 11087-i-0017 74 76 11087-i-0018 75 54 11087-i-0019 84 60 11087-i-0020 42 44 11087-i-0021 56 58 11087-i-0022 59 61 11087-i-0023 49 43 11087-i-0024 0 0 11087-i-0025 70 58\n11087-i-0026 55 50 11087-i-0027 53 70 11087-i-0028 77 71 11087-i-0029 68 51 11087-i-0030 56 52 11087-i-0031 47 62 11087-i-0032 72 64 11087-i-0033 67 43 11087-i-0034 0 0 11087-i-0035 80 66 11087-i-0036 0 0 11087-i-0037 42 42 11087-i-0038 67 54 11087-i-0039 70 49 11087-i-0040 73 60\nLet 1c , c2, c3 and c4 denote the coordinate of the cluster centers, as c1=(33,49) , c2=(68,51),\nc3=(75,65) and c4=(84,71). The Euclidean distance function measures the distance. The\nformula for this distance between a point X (X1, X2, etc.) and a point Y (Y1, Y2, etc.) is:\n\u2211 = \u2212=\nn\nj jj yxd 1\n2)(\n-----------------------------------(1)\nDeriving the Euclidean distance between two data points involves computing the square root of the sum of the squares of the differences between corresponding values. We can make the calculation faster by using excel function as =SQRT(SUMSQ(33-B3,49-C3)). The clusters labels used to denote the group are{1,2,3,4}. In the first Iteration we find the distances between data points and the cluster center. Now observing the column values which ever has minimum distance then under cluster group assign its label as:\nNext, the cluster centers are updated. That is, the mean value of each cluster is recalculated based on the current objects in the cluster. Using the new cluster centers, the objects are redistributed to the clusters based on which cluster center is the nearest. Hence after first iteration the new cluster centers are cluster center-1 (30,30), cluster center-2 (63,54),cluster center-3 (73,65) and cluster center-4 (80,71).\nThis process of iteratively reassigning objects to clusters to improve the partitioning is referred to as iterative relocation. Eventually, no reassignment of the objects in the cluster occurs and so the process terminates. The resulting clusters are returned by clustering process.\nIteration3\nHALL TICKET NO. PSCP CO\ncluster center-1 (24,24) cluster center-3 (75,59) cluster center-4 (79,70) cluster labels (1,2,3,4)\n11087-i-0001 72 55 57 5 17 3 11087-i-0002 47 42 29 33 43 2 11087-i-0003 74 50 56 9 21 3 11087-i-0004 60 59 50 15 22 2\nThe k-means algorithm is applied on the 40 records with two attributes and to obtain\nfinal result, the algorithm under goes seven iteration and stops. After Iteration7 we see that the new mean values obtained for four clusters are the same as that of previous step.\nInstead of considering four clusters, if we consider six clusters (k=6) the algorithm\niterates for two times and terminates as the mean values never change after it.\nK-Means method is not guaranteed to converge to global optimum and often terminates at a local optimum. To obtain good results in practice it is common to run the k-means algorithm multiple times with different initial clusters.\nAdvantages and Disadvantages of K-Means\n\u2022 K-means is relatively scalable and efficient in processing large datasets.\n\u2022 The method is not suitable for discovering clusters with non-convex shapes or of very\ndifferent sizes.\n\u2022 It is sensitive to noise and outlier data"}, {"heading": "3. FIEDLER\u2019S METHOD", "text": "Fiedler\u2019s approach to clustering, which theoretically determines the relation between the size of the obtained clusters and the number of links that are cut by this partitioning as a function of a threshold \u03b1 and of graph properties such as the number of nodes and links. When applying Fiedler\u2019s beautiful results to the Laplacian matrix Q of a graph, the eigenvector belonging to the second smallest eigenvalue, known as the algebraic connectivity, needs to be computed. Finding the laplacian matrix requires construction of A adjacency matrix, and D degree matrix, So the Laplacian matrix L is formed as:\nL = D \u2013 A ---------------------------------------(2)\nGiven a simple graph G with n vertices, its laplacian matrix is defined as:"}, {"heading": "3.1 Adjacency Matrix", "text": "The adjacency matrix of a finite graph G of n vertices is the n \u00d7 n matrix where the nondiagonal entry aij is the number of edges from vertex i to vertex j, and the diagonal entry aii, is either once or twice the number of edges from vertex i to itself."}, {"heading": "3.2 Degree matrix", "text": "In the mathematical field of graph theory the degree matrix is a diagonal matrix which contains information about the degree of each vertex. That is the count of edges connecting a vertex v. If i\u2260j then replace the cell value with 0 other wise degree of the vertex vi"}, {"heading": "3.3 Laplacian matrix", "text": "Given a simple graph G with n vertices, its Laplacian matrix is defined as: L(i,j)=degree of vertex vi if i=j, if i\u2260j and vi is not adjacent to vj and in all other case fill it with 0."}, {"heading": "3.4 Fiedler method", "text": "This method partitions the data set S into two sets S1 and S2 based on the eigen Vector V\ncorresponding to the 2nd smallest eigen value of laplacian matrix. Consider the Equations\n)3(n1........,j n\n1k \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212==\u2211 = kkji xay\nRepresent a linear transformation from the variables x1,x2,\u2026\u2026\u2026\u2026..xn to the variables\ny1,\u2026\u2026\u2026\u2026yn ; we can write this in matrix notation as Y=AX, where Y is a column vector and\nA=(aij) is matrix transformation. In many situations, we need to transform a vector into a scalar\nmultiple of itself.\ni.e. AX=\u03bbX--------------------------------------------------(4) where \u03bb is a scalar.\nSuch problems are known as eign value problems. Let A be an n x n symmetric matrix and x is\nknown as eigen vector corresponding to the eigen values. To obtaine eigen vector we need to\nsolve (A-\u03bbI)x=0. x=0 is a trivial solution of this linear system for any \u03bb. For the system to have\na non-trivial solution, the matrix A-\u03bbI must be singular. The scalar \u03bb and the non-zero vector x\nsatisfying(4) exist if |A-\u03bbI|=0.\npn(\u03bb)= \u03bb\n\u03bb\n\u03bb\n\u2212\n\u2212\nnnn\nn\naa\na\naa\n....................a\n.\n.\n.\n.....a..........-a\n....................a\nn21\n2n2221\n11211\n=0.\nBy expansion of this determinant we get an nth degree polynomial in \u03bb and pn(\u03bb) is known as the characteristic polynomial of A and pn(\u03bb)=0 as the characteristic equation of A. pn(\u03bb)=0 has n roots, which may be real or complex. The roots are the eigen values of the matrix. As per Rayleigh Quotient Theorem Solution:\n\u2013\u03bb1=0, the smallest right-hand eigenvalue of the symmetric matrix, L\n\u2013\u03bb1 corresponds to the trivial eigenvector\nv1= e = [1, 1, \u2026, 1].\nBased on a symmetric matrix, L, we search for the eigenvector, v2, which is furthest away from e. Now v2 gives relation information about the nodes. This relation is usually decided by separating the values across zero. A theoretical justification is given by Miroslav Fiedler. Hence, v2 is called the Fiedler vector. Hence v2 is used to recursively partition the graph by separating the components into negative and positive values. Entire Graph: sign(V)=[ +, +, +, -, -, -, +, +, +, -]\n1 2 3 4 5 6 7 8 9 10\nIteration 1: Positives = [+, +, +, +, +, +]\n1 2 3 7 8 9\nNegatives= [-, -, -, -]\n4 5 6 10\nSign(v2)=[+, -, -, -, +, +, ]\n1 2 3 7 8 9\nIteration 2: Positives =[+, +, + ]\n1 8 9\nNegatives =[-, -, - ]\n1 3 7\nFigure 13: Graph after iteration -2\nSign(v3)= [ +,+, - ]\n1 8 9"}, {"heading": "4. METHODOLOGY OF EXPERIMENTATION", "text": "We observed at several different eigenvectors, followed the Fiedler algorithm and then coded in Matlab using eigs(), eig() by taking small samples having known clusters.\nMATLAB CODE: Steps: 1. Enter the Laplacian matrix in matlab as:\na=[2 0 0 0 0 0 0 -1 -1 0;0 3 -1 0 0 0 -1 0 -1 0; 0 -1 4 0 0 0 -1 0 -1 -1;0 0 0 3 -1 -1 0 0 0 -1;0 0 0 - 1 3 -1 0 0 0 -1;0 0 0 -1 -1 3 0 0 0 -1;0 -1 -1 0 0 0 2 0 0 0; -1 0 0 0 0 0 0 2 -1 0;-1 -1 -1 0 0 0 0 -1 4 0;0 0 -1 -1 -1 -1 0 0 0 4]\na =\n2 0 0 0 0 0 0 -1 -1 0 0 3 -1 0 0 0 -1 0 -1 0 0 -1 4 0 0 0 -1 0 -1 -1 0 0 0 3 -1 -1 0 0 0 -1 0 0 0 -1 3 -1 0 0 0 -1 0 0 0 -1 -1 3 0 0 0 -1 0 -1 -1 0 0 0 2 0 0 0\n-1 0 0 0 0 0 0 2 -1 0 -1 -1 -1 0 0 0 0 -1 4 0\n0 0 -1 -1 -1 -1 0 0 0 4\n2. Find the eign values from eign vector\n>> eig(a) [V D]=eigs(a, 2, 'SA');\nans =\n0.0000 0.2602 0.8638 3.0000 3.0607 4.0000 4.0000 4.0000 5.0000 5.8154\n3. Display the second smallest of Laplacian matrix\nD(2,2) ans = 0.2602\n4. The sign obtained for the entire graph is\nsign(V)=[ +, +, +, -, -, -, +, +, +, -]\n1 2 3 4 5 6 7 8 9 10\nIteration-2\na=[2 0 0 0 -1 -1;0 3 -1 -1 0 -1;0 -1 4 -1 0 -1;0 -1 -1 2 0 0;-1 0 0 0 2 -1;-1 -1 -1 0 -1 4] [V D]=eigs(a,2,\u2019SA\u2019); D(2,2) ans = 0.8591\nsign(V)= = [+, +, +, +, +, +]\n1 2 3 7 8 9\nThe final graph obtained for the graph-1"}, {"heading": "4. CONCLUSION", "text": "The advantage of the K-Means algorithm is its favorable execution time. Its drawback is that the user has to know in advance how many clusters are searched for. It is observed that K-Means algorithm is efficient for smaller data sets only. Fielder\u2019s method doesn\u2019t require the preliminary knowledge of the number of clusters, but most clustering methods require matrices to be symmetric. Symmetrizing techniques either distort original information or greatly increase the size of the dataset moreover there are many applications where the data is not symmetric like Hyperlinks on the Web."}, {"heading": "5. REFERENCES", "text": "[1] Han, J. and Kamber, M. Data Mining: Concepts and Techniques, 2001 (Academic Press,\nSan Diego, California, USA).\n[2] Comparision between clustering algorithms- Osama Abu Abbas [3] Pham, D.T. and Afify, A.A. Clustering techniques and their applications in engineering.\nSubmitted to Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 2006.\n[4] Jain, A.K. and Dubes, R.C. Algorithms for Clustering Data, 1988 (Prentice Hall, Englewood\nCliffs, New Jersey, USA).\n[5] Bottou, L. and Bengio, Y. Convergence properties of the k-means algorithm. [6] B. Mohar, The Laplacian Spectra of Graphs. Graph Theory, Combinatorics, and\nApplications, Wiley, pp. 871, 1991.\n[7] D. Cvetkovi\u00b4c, P. Rowlinson, S. Simi\u00b4c. An introduction to the Theory of Graph\nSpectra.Addison-Wesley, Cambridge University Press, 2010.\n[8] P. Van Mieghem. Performance Analysis of Communications Networks and Systems.\nAddison-Wesley, Cambridge University Press, 2006.\nAuthor\nT. Soni Madhulatha obtained MCA from Kakatiya University in 2003 and M. Tech (CSE) from JNTUH in 2010.She has 8 years of teaching experience and she is presently working as Associate professor in Department of Informatics in Alluri Institute of Management Sciences, Hunter Road Warangal. She published papers in various National and International Journals and Conferences. She is a Life Member of ISTE , IAENG and APSMS."}], "references": [{"title": "Clustering techniques and their applications in engineering", "author": ["D.T. Pham", "A.A. Afify"], "venue": "Submitted to Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "The Laplacian Spectra of Graphs", "author": ["B. Mohar"], "venue": "Graph Theory, Combinatorics, and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "An introduction to the Theory of Graph Spectra.Addison-Wesley", "author": ["D. Cvetkovi \u0301c", "P. Rowlinson", "S. Simi \u0301c"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Performance Analysis of Communications Networks and Systems", "author": ["P. Van Mieghem"], "venue": "She published papers in various National and International Journals and Conferences. She is a Life Member of ISTE , IAENG and APSMS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}], "referenceMentions": [], "year": 2012, "abstractText": "Clustering is a common technique for statistical data analysis, Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters. Dissimilarities are assessed based on the attribute values describing the objects. Often, distance measures are used. Clustering is an unsupervised learning technique, where interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. This paper also considers the partitioning of mdimensional lattice graphs using Fiedler\u2019s approach, which requires the determination of the eigenvector belonging to the second smallest Eigen value of the Laplacian with K-means partitioning algorithm.", "creator": "PScript5.dll Version 5.2.2"}}}