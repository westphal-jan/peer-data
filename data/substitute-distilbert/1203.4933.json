{"id": "1203.4933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2012", "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS Tagger", "abstract": "this entry gives a detail overview about the modified features selection in crf ( conditional random field ) based manipuri pos ( part of speech ) tagging. convergence of features is so important in crf that the better are observed features then the better receive potential outputs. this work is an attempt for an experiment to make the previous theory more efficient. multiple new tags are generated to run up crf and again tried with the reduplicated multiword check ( rmwe ) as basic feature. the terms run with relevance because manipuri is rich of rmwe and regardless of rmwe becomes satisfied of the necessities then end up the result of pos tagging. the draft crf system shows a recall of 78. 22 %, revision of 73. 15 % and f - measure of 75. 60 %. with the identification of rmwe and considering it as a feature makes an improvement to a recall of 80. 20 %, precision of 74. 31 % and f - measure of 77. 14 %.", "histories": [["v1", "Thu, 22 Mar 2012 09:50:51 GMT  (384kb)", "http://arxiv.org/abs/1203.4933v1", "15 pages, 4 tables, 2 figures, the linkthis http URLarXiv admin note: text overlap witharXiv:1111.2399"]], "COMMENTS": "15 pages, 4 tables, 2 figures, the linkthis http URLarXiv admin note: text overlap witharXiv:1111.2399", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kishorjit nongmeikapam", "lairenlakpam nonglenjaoba", "yumnam nirmal", "sivaji bandyopadhyay"], "accepted": false, "id": "1203.4933"}, "pdf": {"name": "1203.4933.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["MANIPURI POS TAGGER", "Kishorjit Nongmeikapam", "Lairenlakpam Nonglenjaoba", "Yumnam Nirmal", "Sivaji Bandhyopadhyay"], "emails": ["kishorjit.nongmeikapa@gmail.com", "nonglen.ran@gmail.com", "yumnamnirmal@gmail.com", "sivaji_cse_ju@yahoo.com"], "sections": [{"heading": null, "text": "This paper gives a detail overview about the modified features selection in CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging. Selection of features is so important in CRF that the better are the features then the better are the outputs. This work is an attempt or an experiment to make the previous work more efficient. Multiple new features are tried to run the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as another feature. The CRF run with RMWE because Manipuri is rich of RMWE and identification of RMWE becomes one of the necessities to bring up the result of POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15% and F-measure of 75.60%. With the identification of RMWE and considering it as a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and F-measure of 77.14%.\nKEYWORDS\nCRF, RMWE, POS, Features, Stemming, Root"}, {"heading": "1. INTRODUCTION", "text": "The Manipuri language or simply Manipuri is among the 21 scheduled language of Indian Constitution. This language is mainly spoken in Manipur in India and in some parts of Myanmar and Bangladesh. Manipuri uses two scripts, one is the borrowed Bengali script and another is the original script that is Meitei Mayek (Script). Our present work adopts the Manipuri in Bengali Script. It is because most of the Manipuri manuscripts are written in Bengali script and thus the corpus collection for Manipuri in Bengali script becomes an easy task.\nThis language is a Tibeto-Burman language and the distinction from other language Indian is its highly agglutinative nature. As an example (observe Section 2) single word can have 10 suffixes or more. The affixes play the most important role in the structure of the language. The affixes play an important role in determining the POS of a word. Apart from the agglutinative nature, Manipuri is a mono-syllabic, influenced and enriched by the Indo-Aryan languages of Sanskrit origin and English. A clear-cut demarcation between morphology and syntax is not possible in this language. In Manipuri, words are formed in three processes called affixation, derivation and compounding as mentioned in [1]. In Manipuri roots are of two types, they are free and bound root [2]. The majority of the roots found in the language are bound and the affixes are the determining factor of the class of the words in the language.\nThe POS tagging is an important topic in the application of Information Retrieval (IR), Question Answering (QA), Summarization, Machine Translation (MT), Event Tracking etc. for all languages and so is for Manipuri. Lack of an efficient POS tagger for Manipuri hampers the other research area of NLP. So the real challenges to bring up the accuracy level motivate us for more experiment and more research in this area.\nEvery language linguistically has their own POS and computer experts tried their best for the automatic POS tagging using machine. Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8]. For Chinese, the works are found ranging from rule based, HMM to Genetic Algorithm [9]-[12]. For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16]. Works of POS tagging using CRF can also be seen in [17].\nWorks of Manipuri POS tagging are reported in [18]-[19]. For the identification of Reduplicated Multiword Expression (RMWE) is reported in [20]. Identification of RMWE using CRF is reported in [21] and improvement of MWE using RMWE is reported in [22]. Web Based Manipuri Corpus for Multiword NER and Reduplicated MWEs Identification using SVM is reported in [23].\nSection 2 describes about the agglutinative nature of Manipuri which leads us to the idea of stemming. Section 3 gives the idea of Manipuri Reduplicated MWEs, stemming of Manipuri words so that it can be used as a feature is describe in Section 4. Section 5 gives the concept of Conditional Random Field (CRF). CRF model and Feature selection is described in Section 6 and Section 7 reports the experiments and the evaluation, improvement using reduplicated MWEs is discussed in Section 8 and the conclusion is drawn in Section 9."}, {"heading": "2. ROOT AND AN EXAMPLE OF AN AGGLUTINATIVE MANIPURI WORD", "text": "In Manipuri roots are generally considered by replacing a large number of morphemes. They are the carrier of principal meaning in the composition of a word [2]. Roots are different in different languages. In English roots are almost free which means that there are separate sets of roots that denote separate grammatical categories such as noun (box, boy, bird etc), verb (run, cry, try etc), adjective (tall, thin, large etc) etc. This is not in the case of Manipuri since there are no separate roots for adjective and verb in Manipuri.\nIn Manipuri roots are of two types, they are free and bound root. Free roots can stand alone without suffixes in a sentence and bound root takes other affixes excepting the one with free roots.\nThis language is highly agglutinative and to prove with this point let us site an example word: \u201c \u201d (\u201cpusinh\u0259nj\u0259r\u0259mg\u0259d\u0259b\u0259nid\u0259ko\u201d), which means \u201c(I wish I) myself would have caused to bring in (the article)\u201d. Here there are 10 (ten) suffixes being used in a verbal root, they are \u201cpu\u201d is the verbal root which means \u201cto carry\u201d, \u201csin\u201d(in or inside), \u201ch\u0259n\u201d (causative), \u201cj\u0259\u201d (reflexive), \u201cr\u0259m\u201d (perfective), \u201cg\u0259\u201d (associative), \u201cd\u0259\u201d (particle), \u201cb\u0259\u201d (infinitive), \u201cni\u201d (copula), \u201cd\u0259\u201d (particle) and \u201cko\u201d (endearment or wish).\nAltogether 72 (seventy two) affixes are listed in Manipuri out of which 11 (eleven) are prefixes and 61 (sixty one) are suffixes. Table 1 shows the prefixes of 10 (ten number) because the prefix\n(m\u0259) is used as formative and pronomial so only one is included and like the same way table 2 shows the suffixes in Manipuri with only are 55 (fifty five) suffix in the table since some of\nthe suffixes are used with different form of usage such as 3 (gum) which is used as particle as well as proposal negative, (d\u0259) as particle as well as locative and (n\u0259) as nominative, adverbial, instrumental or reciprocal."}, {"heading": "3. REDUPLICATED MULTIWORD EXPRESSIONS (RMWE)", "text": "In Manipuri works for identification of reduplicated MWEs has been reported for the first time in [20]. In this paper the process of reduplication is defined as: \u2018reduplication is that repetition, the result of which constitutes a unit word\u2019. These single unit words are the RMWE.\nThe reduplicated MWEs in Manipuri are classified mainly into four different types. These are: 1) Complete Reduplicated MWEs, 2) Mimic Reduplicated MWEs, 3) Echo Reduplicated MWEs and 4) Partial Reduplicated MWEs. Apart from these fours there are also cases of a) Double reduplicated MWEs and b) Semantic Reduplicated MWEs."}, {"heading": "3.1 Complete Reduplication MWEs", "text": "In the complete reduplication MWEs the single word or clause is repeated once forming a single unit regardless of phonological or morphological variations. Interestingly in Manipuri these complete reduplication MWEs can occur as Noun, Adjective, Adverb, Wh- question type, Verbs, Command and Request.\nNoun\nHere the reduplication words are generally noun but sometimes the suffixes \u2013d\u01dd, \u2013gi and \u2013ki are generally added to the second word of reduplication to identify itself as noun. One such MWE is\n(\u2018m\u01ddrik m\u01ddrik\u2019) which means \u2018drop by drop\u2019.\nAlso some of these words are inflected MWEs too with one of the above suffixes, say; 45 45 (\u2018yum yum-d\u01dd\u2019) which means \u2018to every house\u2019. The following example sentence in Manipuri:\nlam lam-gi m\u01ddsa-gi lon lei place place own-gen language has\nMeans, \u2018Every place/country has its own language\u2019, when translated in English.\nAdjective\nIn this type of reduplication MWEs, the first word is repeated to form an adjective by adding \u2013 b\u01dd or \u2013p\u01dd to the second word. For example, 6 7 6 7 (\u2018\u01ddnou \u01ddou-ba\u2019) which means \u2018new\u2019 and 6 8 6 8 (\u2018\u01ddtek \u01ddtek-pa\u2019 ) which means \u2018fresh\u2019 etc. The following example sentence in Manipuri:\n\u01ddy \u01ddtek \u01ddtekpa l\u01ddy pam-mi I fresh fresh flower like-asp\nMeans, \u2018I like fresh flowers.\u2019 when translated in English.\nVerb\nThe verb reduplication is different from the reduplication of noun, adjective, adverb and whquestion since they are generally of subject-objectless structure. For example, ;J F ;J F (\u2018c\u01ddtle c\u01ddtle\u2019) means \u2018am going/leaving\u2019. Sometimes with this reduplication \u2018I\u2019 can be silent and can mean \u2018I am going/leaving\u2019. The verb word generally ends with any of these verbal inflections -le, -re, -gani, -lur\u01dd etc. depending on the tense.\nAdverb\nIn case of adverbs \u2013n\u01dd is usually added to both the words of the reduplication. It is different from the above in such a way that the inflection is added to both the words in reduplication. For example, (\u2018k\u01ddp-na k\u01ddp-na\u2019) which means \u2018cryingly\u2019 and 4 4 (\u2018yam-n\u01dd yam-n\u01dd\u2019) which means \u2018very\u2019. The following example sentence in Manipuri:\n\u01ddy k\u01ddpna k\u01ddpna skul-d\u01dd c\u01ddtli I cryingly cryingly school-loc go\nMeans, \u2018I go to school cryingly.\u2019 when translated to English.\nWh- question type\nLike English, wh- type of questions can be framed in Manipuri but it uses reduplication MWEs. For example:\n(\u2018k\u01ddri k\u01ddri\u2019) means \u2018what/which\u2019. (\u2018k\u01ddna k\u01ddna\u2019) means \u2018who\u2019. 4 4 (\u2018k\u01ddday k\u01ddday\u2019) means \u2018where\u2019. (\u2018k\u01ddr\u01ddm k\u01ddr\u01ddm\u2019) means \u2018how\u2019.\nThe following example sentence in Manipuri: k\u01ddna k\u01ddna lam yengb\u01dd c\u01ddtkani?\nwho who land to see go-asp\nMeans \u2018who will go to see the land?\u2019 when translated to English.\nCommand\nThe reduplication is used in such a manner that the subject is generally a second person but it is not explicitly mentioned in the sentence.\nFor example, ;J F ;J F (\u2018c\u01ddtlo c\u01ddtlo\u2019) means \u2018go-comd go-comd\u2019 and which can be translated to English as \u2018Go.\u2019\nRequest\nReduplication can be present in request sentences also. For example, >FD >FD (\u2018le\u019e-bi-ro le\u019e-bi-ro\u2019) means \u2018please go/please leave\u2019."}, {"heading": "3.2 Partial Reduplicated MWEs", "text": "In case of partial reduplication the second word carries some part of the first word as an affix to the second word, either as a suffix or a prefix. For example, ;J = ;J K ( \u2018c\u01ddt-thok c\u01ddt-sin\u2019) means \u2018to go to and fro\u2019, @ F @ ( \u2018sa-mi lan-mi\u2019) means \u2018army\u2019."}, {"heading": "3.3 Echo Reduplicated MWEs", "text": "The second word does not have a dictionary meaning and is basically an echo word of the first word. For example, = K : K (\u2018thk-si kha-si\u2019) means \u2018good manner\u2019. Here the first word has a dictionary meaning \u2018good manner\u00b4 but the second word does not have a dictionary meaning and is an echo of the first word."}, {"heading": "3.4 Mimic Reduplicated MWEs", "text": "In the mimic reduplication the words are complete reduplication but the morphemes are\nonomatopoetic, usually emotional or natural sounds. For example, (\u2018khr\u01ddk khr\u01ddk\u2019) means \u2018cracking sound of earth in drought\u2019."}, {"heading": "3.5 Double Reduplicated MWEs", "text": "In double Reduplicated MWE there consist of three words, where the prefix or suffix of the first two words is reduplicated but in the third word the prefix or suffix is absent. An example of double prefix reduplication is 9 5 9 5 5 (\u2018i-mun i-mun mun-ba\u2019) which means, \u2018completely ripe\u2019. It may be noted that the prefix is duplicated in the first two words while in the following example suffix reduplication take place, >B >B >B (\u2018\u019e\u01dd\u03c9-srok \u019e\u01dd\u03c9-srok \u019e\u01dd\u03c9-ba\u2019) whichmeans \u2018shining white\u2019."}, {"heading": "3.6 Semantic Reduplicated MWEs", "text": "Both the reduplication words have the same meaning as well as the MWE. Such type of MWEs is very special to the Manipuri language. For example, A (\u2018pamba k\u01ddy\u2019 ) means \u2018tiger\u2019 and each of the component words means \u2018tiger\u2019. Semantic reduplication exists in Manipuri in abundance as such words have been generated from similar words used by seven clans in Manipur during the evolution of the language."}, {"heading": "4. STEMMING OF MANIPURI WORDS", "text": "The Stem words which are considered as feature for running the CRF follows an algorithm mention in [25]. In this algorithm Manipuri words are stemmed by stripping the suffixes in an iterative manner. As mentioned in Section 2 a word is rich with suffixes and prefixes. In order to stem a word an iterative method of stripping is done by using the acceptable list of prefixes (11 numbers) and suffixes (61 numbers) as mentioned in the Table 1 and Table 2 above."}, {"heading": "4.1 The Algorithm", "text": "This stemmer mainly consist of four algorithms the first one is to read the prefixes, the second one is to read the suffixes, the third one is to identify the stem word removing the prefixes and the last algorithm is to identify the stem word removing the suffixes.\nTwo file, prefixes_list and suffixes_list are created for prefixes and suffixes of Manipuri. In order to test the system another testing file, test_file is used. The prefixes and suffixes are removed in an iterative approach as shown in the algorithm 3 and algorithm 4 until all the affixes are removed. The stem word is stored in stemwrd.\nAlgorithm1: read_prefixes() 1. Repeat 2 to 4 until all prefixes (pi) are read from\nprefixes_list 2. Read a prefix pi 3. p_array[i]=pi 4. p_wrd_count =i++; 5. exit\nAlgorithm2: read_suffixes() 1. Repeat 2 to 4 until all suffixes (si) are read from\nsuffixes_list 2. Read a suffix si 3. s_array[i]=si 4. s_wrd_count=i++; 5. exit\nAlgorithm3: Stem_removing_prefixes(p_array, p_wrd_count) 1. Repeat 2 to 16 for every word (wi) are read from the\ntest_file\n2. String stemwrd=\" \"; 3. for(int j=0;j<p_wrd_count;j++) 4. { 5. if(wi.startsWith(p_array[j])) 6. { 7. stemwrd=wi.substring(wi.length()-((wi.length()-\n((p_array[j].toString()).length()))),wi.length());\n8. wi=stemwrd; 9. j=-1; 10. } 11. else 12. { 13. stemwrd=wi; 14. } 15. } 16. write stemwrd; 17. exit;\nAlgorithm4: Stem_removing_suffixes(s_array,s_wrd_count) 1. Repeat 2 to 16 for every word (wi) are read from the\ntest_file\n2. String stemwrd=\" \"; 3. for(int j=0;j<s_wrd_count;j++) 4. { 5. if(wi.endsWith(s_array[j])) 6. { 7. stemwrd=wi.substring(0,wi.indexOf(s_array[j])); 8. wi=stemwrd; 9. j=-1; 10. } 11. else 12. { 13. stemwrd=wi; 14. } 15. } 16. write stemwrd; 17. exit"}, {"heading": "5. CONCEPTS OF CRF", "text": "The concept of Conditional Random Field in [24] is developed in order to calculate the conditional probabilities of values on other designated input nodes of undirected graphical models. CRF encodes a conditional probability distribution with a given set of features. It\u2019s an unsupervised approach where the system learns by giving some training and can be used for testing other text. The conditional probability of a state sequence Y=(y1, y2,..yT) given an observation sequence X=(x1, x2,..xT) is calculated as :\nP(Y|X) = t))X, ,y,y (fexp( 1\nt1-t\nT\n1t\nk\nk\nk XZ \u2211\u2211 =\n\u03bb ----(1)\nWhere, fk( yt-1,yt, X,t) is a feature function whose weight \u03bbk is a learnt weight associated with fk and to be learned via training. The values of the feature functions may range between -\u221e \u2026 +\u221e, but typically they are binary. ZX is normalization factor:\n\u2211\u2211\u2211 =\n= T\nt k\nkk\ny XZ 1 t1-t t))X, ,y,y (fexp \u03bb ----(2)\nWhich is calculated in order to makes the probability of all state sequences sum to 1. This is calculated as in HMM and can be obtain efficiently by dynamic programming. Since CRF defines the conditional probability P(Y|X), the appropriate objective for parameter learning is to maximize the conditional likelihood of the state sequence or training data.\n\u2211 =\nN\n1i\n)x|P(y log ii ---(3)\nWhere, {(xi, yi)} is the labeled training data. Gaussian prior on the \u03bb\u2019s is used to regularize the training (i.e smoothing). If \u03bb ~ N(0,\u03c12), the objective becomes,\n\u2211\u2211 \u2212 = k\ni\n2\n2N\n1i 2 )x|P(y log ii \u03c1\n\u03bb ---(4)\nThe objective is concave, so the \u03bb\u2019s have a unique set of optimal values."}, {"heading": "6. CRF MODEL AND FEATURE SELECTION", "text": ""}, {"heading": "6.1 The CRF Model", "text": "The work of [19] also shows the use of CRF in order to tag the POS in a running text. It was the first attempt for POS tagging using CRF and had a low efficiency. This work is an attempt or an experiment to make the previous work more efficient. The C++ based CRF++ 0.53 package1 is used in this work and it is readily available as open source for segmenting or labeling sequential data.\nThe CRF model for Manipuri POS tagging (Figure 1) consists of mainly data training and data testing. The important processes required in POS tagging using CRF are feature selection, preprocessing which includes arrangement of tokens or words into sentences with other notations, creation of model file after training and finally the testing with the test corpus.\nFollowing sub sections explain the overall process in detail:\n1 http://crfpp.sourceforge.net/"}, {"heading": "6.1 Feature Selection", "text": "The feature selection is important in CRF. The various features used in the system are,\nF= { Wi-m, \u2026 ,W i-1, W i, W i+1, \u2026, W i+n, SWi-m, \u2026, SWi-1, SWi, SWi+1,\u2026 , SWi-n , number of acceptable standard suffixes, number of acceptable standard prefixes, acceptable suffixes present in the word, acceptable prefixes present in the word, word length, word frequency, digit feature, symbol feature }\nThe details of the set of features that have been applied for POS tagging in Manipuri are as follows:\nSurrounding words as feature: Preceeding word(s) or the successive word(s) are important in POS tagging because these words play an important role in determining the POS of the present word.\nSurrounding Stem words as feature: The Stemming algorithm mentioned in Section 4 is used so that the preceding and following stem words of a particular word can be used as features. It is because the preceding and following words influence the present word POS tagging.\nNumber of acceptable standard suffixes as feature: As mention in Section 2, Manipuri being an aggllutinative language the suffixes plays an important in determining the POS of a word. For every word the number of suffixes are identified during stemming and the number of suffixes is used as a feature.\nNumber of acceptable standard prefixes as feature: Same is the case for the prefixes thus it also plays an important role too for Manipuri since it is a highly agglutinative language. For every word the number of prefixes are identified during stemming and the number of prefixes is used as a feature.\nAcceptable suffixes present as feature: The standard 61 suffixes of Manipuri which are identified is used as one feature. As mention with an example in Section 2, suffixes are appended one after another and so far the maximum number of appended suffixes is reported as ten. So taking into account of such case, for every word ten columns separated by a space are created for every suffix present in the word. A \u201c0\u201d notation is being used in those columns when the word consists of no acceptable suffixes.\nAcceptable prefixes present as feature: 11 prefixes have been manually identified in Manipuri and the list of prefixes is used as one feature. For every word if the prefix is presents then a column is created mentioning the prefix, otherwise the \u201c0\u201d notation is used. Upto three prefixes are considered for observation.\nLength of the word: Length of the word is set to 1 if it is greater than 3 otherwise, it is set to 0. Very short words are generally pronouns and rarely proper nouns.\nWord frequency: A range of frequency for words in the training corpus is set: those words with frequency <100 occurrences are set the value 0, those words which occurs >=100 are set to 1. The word frequency is considered as one feature since occurrence of determiners, conjunctions and pronouns are abundant.\nDigit features: Quantity measurement, date and monetary values are generally digits. Thus the digit feature is an important feature. A binary notation of \u20181\u2019 is used if the word consist of a digit else \u20180\u2019.\nSymbol feature: Symbols like $,%, - etc. are meaningful in textual uses, so the feature is set to 1 if it is found in the token otherwise, it is set to 0. This helps to recognize SYM (Symbols) and QFNUM (Quantifier number) tags."}, {"heading": "6.2 Pre-processing and feature selection", "text": "A Manipuri text document is used as an input file. The training and test files consist of multiple tokens. In addition, each token consists of multiple (but fixed number) columns where the columns are used by a template file. The template file gives the complete idea about the feature selection. Each token must be represented in one line, with the columns separated by a white spaces (spaces or tabular characters). A sequence of tokens becomes a sentence. Before undergoing training and testing in the CRF the input document is converted into a multiple token file with fixed columns and the template file allows the feature combination and selection which is specified in section 6.1.\nTwo standard files of multiple tokens with fixed columns are created: one for training and another one for testing. In the training file the last column is manually tagged with all those identified POS tag2 whereas in the test file we can either use the same tagging for comparisons or only \u2018O\u2019 for all the tokens regardless of POS."}, {"heading": "6.3 Training to get the Model File", "text": "Training of CRF system is done in order to train the system which gives an ouput as a model file. The system is feeded with the gold standard training file.\nIn the training of the CRF we used a template file whose function is to choose the features from the feature list. Model file is the output file after the training. Model files are the learnt file by the CRF system. The model file is obtain after training the CRF using the training file. This model file is a ready-made file by the CRF tool for use in the testing process. There is no need of using the template file and training file again since the model file consists of the detail information of the template file and training file."}, {"heading": "6.4 Testing", "text": "The testing proceeds using the model file that have been generated while training the CRF system. One among the two gold standard file which are being created before running the CRF system that is the testing file is used for testing. As mentioned earlier in section 6.2 this file has to be created in the same format as that of training file, i.e., of fixed number of columns with the same field as that of training file.\nThus after testing process the output file is a new file with an extra column which is tagged with the POS tags."}, {"heading": "7. EXPERIMENT AND THE RESULT", "text": "Manual tagging is time consuming so as mention earlier in Section 6.2 three linguist experts from Linguistic Department, Manipur University is hired for rectifying the spelling and syntax of a sentence. Also they have performed the most important part of POS tagging on the corpus which consists of 25,000 tokens. It is considered to be the Gold standard but later split into two file one for training file and another for testing.\nAs mention in section 6 a Java program is written in order to identify the required features except the POS tagging since it\u2019s manually tag and arrange the feature column wise separated by a space. So, each line becomes a sentence. The last column of both the training and testing\n2 http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf\nfiles are tagged with the POS. The last column of the test file is also tagged with POS so that the output can be compared easily.\nIn order to evaluation the experiment, the system used the parameters of Recall, Precision and F-score. It is defined as follows:\nRecall, R = texttheinanscorrectofNo\nsystemthebygivenanscorrectofNo\nPrecision, P = systemthebygivenansofNo\nsystemthebygivenanscorrectofNo\nF-score, F = RP2\u03b2 1)PR2(\u03b2 ++ Where \u03b2 is one, precision and recall are given equal weight.\nGenerally a number of problems have been identified because of the typical nature of the Manipuri language. The first thing is the highly agglutinative nature. Another problem is the word category which is not so distinct. The verbs consist of both free and bound category but classifying the bounded categories is a problem. Another problem is to classify basic root forms according to the word class. Although the distinction between the noun class and verb classes is relatively clear; the distinction between nouns and adjectives is often vague. Distinction between a noun and an adverb becomes unclear because structurally a word may be a noun but contextually it is adverb. Also a part of root may also be a prefix, which leads to wrong tagging. The verb morphology is more complex than that of noun. Sometimes two words get fused to form a complete word"}, {"heading": "7.1 Experiment for selection of best feature", "text": "As mention in Section 6.2 the preprocessing of the file is done and a total of 25,000 words are divided into 2 file, one consisting of 20000 words and the second file consist of 5000 words. The first file is used for training and the second file is used for testing. As mention earlier, a Java program is used to separate the sentence into equal numbers of columns separated by a blank space with different features.\nThe experiment is performed with different combinations of features. The features are manually selected in such a way that the result shows an improvement in the F-measure. Among the different experiments with different combinations table 4 list some of the best combinations. Note that the combinations in the list are not the only combinations tried but are some of the best combinations. Table 3 explains the notations used in table 4."}, {"heading": "7.2 Evaluation and best Feature", "text": "The drawback of manual selection is the feature combination. Also the rigidness in the CRF model is the feature combination and feature selection. The best feature so far reported in the previous CRF based model [19] is as follows:"}, {"heading": "F= {Wi-2, Wi-1, Wi, Wi+1, |prefix|<=n, |suffix|<=n, Dynamic POS tag of the previous word, Digit information, Length of the word, Frequent word, Symbol feature}", "text": "The list consists of surrounding words, prefixes, suffixes, surrounding POS, digit information, word length, word frequency and symbol features. The prefixes and suffixes used are just a combination of the n characters. It does not adopt the standard prefixes and suffixes. The model which has been adopted here has a different list and the best feature is chosen after all possible combinations. The best result is the one which shows the best efficiency among the results. This happens with the following feature:\nF= { Wi-2, W i-1, W i, W i+1, SWi-1, SWi, SWi+1, number of acceptable standard suffixes, number of acceptable standard prefixes, acceptable suffixes present in the word, acceptable prefixes present in the word, word length, word frequency, digit feature, symbol feature }\nThe best feature set in the model gives the Recall (R) of 78.22%, Precision (P) of 73.15% and F-measure (F) of 75.60%. The earlier model in [19] reports that the CRF based system shows 72.04% efficient."}, {"heading": "8. IMPROVEMENT USING RMWE", "text": "With the intention of improving the efficiency of the POS tagging in the CRF system the concept of identifying the RMWE and including it as a feature is tried."}, {"heading": "8.1. The RMWE identification Model", "text": "The Algorithms and models for finding reduplicated MWEs in Manipuri text as suggested in [20] is used for the identification of reduplicated MWEs. The first model is used to identify the complete, mimic, partial, double and echo reduplicated MWEs. Figure 2gives the idea about the model 1.\nThe functions performed by the different parts of the proposed model are: a) Tokenizer, separates the words based on blank space or special symbols to identify two\nconsecutive words Wi and Wi+1. b) Reduplication MWE Identifier, it verifies the valid inflections present in the words\nand also checks the semantics of Wi+1 in the dictionary for the Echo words. c) Valid Inflection List, list of commonly used valid inflection is listed. The inflection list\nis an important resource for MWE identification. d) Dictionary, it includes the lexicon and the associated semantics."}, {"heading": "8.1. The Algorithm for first model", "text": "Algorithm to identify the types of reduplicating MWEs: 1. Repeat 2 to 21 until all the tokens (Wi) are read in the text, where (i=1 to n). 2. Check whether Wi and Wi+1 are same word, if same go to 3 else go to 14 3. Check whether Wi Wi+1 is in the dictionary if found then identify as mimic reduplication 4. Repeat 5 to 8 until the entire prefixes (Pk) list is read, where (k=1 to m) 5. temp1= (Wi ) - Pk 6. Check whether temp1 is a starting substring of Wi+2 if so go to 9 7. temp2= (Wi ) - Sj 8. Check whether temp2 is a starting substring of Wi+2 if so go to 9 else go to 13 9. Identify it as a double reduplication 10. Repeat 11 to 12 until the entire suffixes (Sj) list is read, where (j=1 to l). 11. temp3=( Wi ) + Sj 12. Check temp3 and Wi+1 are same words if same go to 13 else go to 14 13. Identify as complete reduplication. 14. Repeat 15 until the entire suffixes (Sj) list is read, where (j=1 to l). 15. Check Sj is a substring present at the end of Wi and Wi+1 if so go to 16 16. Check whether Wi+1 is in the dictionary if not identify as echo reduplication else go to 21 17. Repeat 18 until the entire prefixes (Pk) list is read, where (k=1 to m) 18. Check Pk is a substring present at the beginning of Wi and Wi+1 if so go to 21 19. Repeat 20 until the entire prefixes (Pk) list are read, where (k=1 to m) 20. Check Pk is a substring present at the end of Wi and at the beginning of Wi+1 if so go to\n21\n21. Identify as partial reduplication. 22. End.\nThe second model (Figure 3) is used for identification of semantic reduplicated MWEs.\nSome of the functions of the second model are same with the first model like Tokenizer and Dictionary parts but the Semantic Comparator works for checking the similarity in semantics of W1 and W2."}, {"heading": "8.1.2 The Algorithm for second model", "text": "Algorithm to identify the semantic reduplicating MWEs: 1. Repeat 2 until all the tokens (Wi) are read in the text, where (i=1 to n). 2. Check in the dictionary whether Wi and Wi+1 are semantically same if same identify as\nsemantic reduplication.\n3. End."}, {"heading": "8.2 The improvement after using RMWE as a feature", "text": "After running the training and test files with above model (Section 8.1) the outputs are mark with B-RMWE for the beginning and I-RMWE for the rest of the RMWE and O for the non RMWEs. This output is placed as a new column in the multiple token file for both training and testing.\nThe training file is run again with the CRF toolkit which outputs a new model file. This model file is used to run the test file which adds up a new output column which is the POS tag by the machine after learning process. This new tagging is used to compare with the previous output. The output shows an improvement of the following:"}, {"heading": "9. CONCLUSION", "text": "The previous reported model of [19] it has an accuracy of 72.04% when tested in the test set with CRF system and has an accuracy of 74.38% when tested in SVM system but this purpose system is a better model. This model outperformed the previous model in terms of efficiency with the Recall (R) of 78.22%, Precision (P) of 73.15% and F-measure (F) of 75.60%. With the RMWE as a feature it even betters the efficiency by the Recall (R) of 80.20%, Precision (P) of 74.31% and F-measure (F) of 77.14%.\nOf course features selections were manual so a better approach is acceptable to avoid the hit and trial method. A better approach must be the future research area road map because until and unless we have an efficient POS tagger much of the NLP works of Manipuri will be hampered."}], "references": [{"title": "A Meitei Grammar of Roots and Affixes, A Thesis, Unpublish, Manipur", "author": ["N Nonigopal Singh"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "A Simple Rule-based Part of Speech Tagger", "author": ["Brill", "Eric"], "venue": "In the Proceedings of Third International Conference on Applied Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging", "author": ["Brill", "Eric"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Decision tree models applied to labeling of texts with parts of speech", "author": ["E. Black", "F. Jelinek", "J. Lafferty", "R. Mercer", "S. Roukos"], "venue": "In the DARPA Workshop on Speech and Natural Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "A Practical Part of Speech Tagger", "author": ["D. Cutting", "J. Kupiec", "J. Pederson", "P. Sibun"], "venue": "In the Proceedings of the 3 ANLP Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "A maximum entropy Parts- of- Speech Tagger", "author": ["A. Ratnaparakhi"], "venue": "In the Proceedings EMNLP 1, ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Part-of-speech tagging using a Hidden Markov Model", "author": ["R. Kupiec"], "venue": "In Computer Speech and Language,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Discrimination oriented probabilistic tagging", "author": ["Lin", "Y.C.T.H. Chiang", "K.Y. Su"], "venue": "In the Proceedings of ROCLING V,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "HMM-based Part-of-Speech Tagging for Chinese Corpora", "author": ["C.H. Chang", "C.D. Chen"], "venue": "In the Proceedings of the Workshop on Very Large Corpora,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": " K", "author": ["C.J. Chen", "M.H. Bai"], "venue": "J. Chen, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Part of Speech Tagging of Chinese Sentences Using Genetic Algorithm", "author": ["T. K"], "venue": "In the Proceedings of ICCC96, National University of Singapore,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "POS Tagging using HMM and Rule-based Chunking", "author": ["Ekbal", "Asif", "S Mondal", "B. Sivaji"], "venue": "In the Proceedings of SPSAL2007, IJCAI, India,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Bengali Part of Speech Tagging using Conditional Random Field, In the Proceedings 7th SNLP, Thailand", "author": ["Ekbal", "Asif", "R. Haque", "B. Sivaji"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Maximum Entropy based Bengali Part of Speech Tagging", "author": ["Ekbal", "Asif", "R. Haque", "B. Sivaji"], "venue": "Advances in Natural Language Processing and Applications, Research in Computing Science (RCS) Journal,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Morphological Richness offsets Resource Demand \u2013Experiences in constructing a POS tagger for Hindi", "author": ["Smriti Singh", "Kuhoo Gupta", "Manish Shrivastava", "Pushpak Bhattacharya"], "venue": "In the Proceedings of COLING- ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Shallow Parsing with Conditional Random fields", "author": ["F. Sha", "F. Pereira"], "venue": "In the Proceedings of NAACL-HLT, Canada,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Morphology Driven Manipuri POS Tagger", "author": ["T. Doren Singh", "B. Sivaji"], "venue": "In the Proceeding of IJCNLP NLPLPL", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Manipuri POS tagging using CRF and SVM: A language independent approach", "author": ["T. Doren Singh", "A. Ekbal", "B. Sivaji"], "venue": "In the proceeding of 6th International conference on Natural Language Processing (ICON -2008),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Identification of Reduplicated MWEs in Manipuri: A Rule based Approached", "author": ["N. Kishorjit", "B. Sivaji"], "venue": "In the Proceeding of 23 International Conference on the Computer Processing of Oriental Languages", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Identification of Reduplicated Multiword Expressions Using CRF, A", "author": ["N. Kishorjit", "L. Dhiraj", "N. Bikramjit Singh", "Mayekleima Chanu", "Ng", "B. Sivaji"], "venue": "Gelbukh (Ed.):CICLing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Identification of MWEs Using CRF in Manipuri and Improvement Using Reduplicated MWEs", "author": ["N. Kishorjit", "B. Sivaji"], "venue": "In the Proceedings of 8 International Conference on Natural Language (ICON-2010), IIT Kharagpur, India,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Web Based Manipuri Corpus for Multiword NER and Reduplicated MWEs Identification using SVM", "author": ["T. Doren Singh", "B. Sivaji"], "venue": "In the Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), the 23rd International Conference on Computational Linguistics (COLING), Beijing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In the Procceedings of the 18th International Conference on Machine Learning (ICML01),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "A Light Weight Manipuri Stemmer", "author": ["N. Kishorjit", "S. Bishworjit", "M. Romina", "Mayekleima Chanu", "Ng", "B. Sivaji"], "venue": "Proceedings of Natioanal Conference on Indian Language Computing (NCILC),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In Manipuri, words are formed in three processes called affixation, derivation and compounding as mentioned in [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 3, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 303, "endOffset": 306}, {"referenceID": 4, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 324, "endOffset": 327}, {"referenceID": 5, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 353, "endOffset": 356}, {"referenceID": 6, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 421, "endOffset": 424}, {"referenceID": 7, "context": "For Chinese, the works are found ranging from rule based, HMM to Genetic Algorithm [9]-[12].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "For Chinese, the works are found ranging from rule based, HMM to Genetic Algorithm [9]-[12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Works of POS tagging using CRF can also be seen in [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Works of Manipuri POS tagging are reported in [18]-[19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Works of Manipuri POS tagging are reported in [18]-[19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "For the identification of Reduplicated Multiword Expression (RMWE) is reported in [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "Identification of RMWE using CRF is reported in [21] and improvement of MWE using RMWE is reported in [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Identification of RMWE using CRF is reported in [21] and improvement of MWE using RMWE is reported in [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "Web Based Manipuri Corpus for Multiword NER and Reduplicated MWEs Identification using SVM is reported in [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "In Manipuri works for identification of reduplicated MWEs has been reported for the first time in [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "The Stem words which are considered as feature for running the CRF follows an algorithm mention in [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "The concept of Conditional Random Field in [24] is developed in order to calculate the conditional probabilities of values on other designated input nodes of undirected graphical models.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "The work of [19] also shows the use of CRF in order to tag the POS in a running text.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "The best feature so far reported in the previous CRF based model [19] is as follows:", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "The earlier model in [19] reports that the CRF based system shows 72.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The Algorithms and models for finding reduplicated MWEs in Manipuri text as suggested in [20] is used for the identification of reduplicated MWEs.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "Feature R (in %) P (in %) FS (in %) W[-2,+1], SW[-1,+1], P[1], S[4], L, F, NS, NP, D, SF 78.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Feature R (in %) P (in %) FS (in %) W[-2,+1], SW[-1,+1], P[1], S[4], L, F, NS, NP, D, SF 78.", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "60 W[-2,+2], SW[-2,+1], P[1], S[4], L, F, NS, NP, D, SF 77.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "60 W[-2,+2], SW[-2,+1], P[1], S[4], L, F, NS, NP, D, SF 77.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "64 W[-2,+3], SW[-2,+2], P[1], S[4], L, F, NS, NP, D, SF 75.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "64 W[-2,+3], SW[-2,+2], P[1], S[4], L, F, NS, NP, D, SF 75.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "04 W[-3,+1], SW[-3,+1], P[1], S[4], L, F, NS, NP, D, SF 72.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "04 W[-3,+1], SW[-3,+1], P[1], S[4], L, F, NS, NP, D, SF 72.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "57 W[-3,+3], SW[-3,+2], P[1], S[5], L, F, NS, NP, D 61.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "57 W[-3,+3], SW[-3,+2], P[1], S[5], L, F, NS, NP, D 61.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "02 W[-3,+4], SW[-2,+3], P[2], S[5], L, F, NS, SF 53.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "37 W[-4,+1], SW[-4,+1], P[2], S[6], L, NP, D, SF 47.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "78 W[-4,+3], SW[-3,+3], P[3], S[9], L, F, D, SF 38.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "78 W[-4,+3], SW[-3,+3], P[3], S[9], L, F, D, SF 38.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "70 W[-4,+4], SW[-4,+4], P[3], S[10], NS, NP 34.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "70 W[-4,+4], SW[-4,+4], P[3], S[10], NS, NP 34.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The previous reported model of [19] it has an accuracy of 72.", "startOffset": 31, "endOffset": 35}], "year": 2012, "abstractText": "This paper gives a detail overview about the modified features selection in CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging. Selection of features is so important in CRF that the better are the features then the better are the outputs. This work is an attempt or an experiment to make the previous work more efficient. Multiple new features are tried to run the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as another feature. The CRF run with RMWE because Manipuri is rich of RMWE and identification of RMWE becomes one of the necessities to bring up the result of POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15% and F-measure of 75.60%. With the identification of RMWE and considering it as a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and F-measure of 77.14%.", "creator": "PScript5.dll Version 5.2.2"}}}