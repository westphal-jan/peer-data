{"id": "1204.2248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2012", "title": "Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media", "abstract": "many real - world phenomena can be represented by having positive - temporal signal : where, when, and how much. social media indicates a tantalizing data source for those who wish to monitor such signals. unlike most scholarly work, we assume that the target phenomenon is known and they are given a parameter to count its occurrences vs social media. however, counting is plagued by sample bias, negative data, and, paradoxically, data scarcity - - issues radically addressed my prior work. we formulate value recovery as a poisson point process estimation problem. we explicitly incorporate human sensory bias, time delays and spatial distortions, facilitating spatio - temporal regularization requiring interdisciplinary field to address the noisy count issues. we form an efficient optimization algorithm and discuss its theoretical properties. we show exactly our model is more accurate than commonly - used baselines. finally, we present a case study on wildlife roadkill monitoring, where our model produces qualitatively convincing scenarios.", "histories": [["v1", "Tue, 10 Apr 2012 18:56:29 GMT  (908kb,D)", "http://arxiv.org/abs/1204.2248v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["jun-ming xu", "aniruddha bhargava", "robert nowak", "xiaojin zhu"], "accepted": false, "id": "1204.2248"}, "pdf": {"name": "1204.2248.pdf", "metadata": {"source": "CRF", "title": "Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media", "authors": ["Jun-Ming Xu", "Aniruddha Bhargava", "Robert Nowak", "Xiaojin Zhu"], "emails": ["xujm@cs.wisc.edu,", "aniruddha@wisc.edu,", "nowak@ece.wisc.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "Many real-world phenomena of interest to science are spatio-temporal in nature. They can be characterized by a real-valued intensity function f \u2208 R\u22650, where the value fs,t quantifies the prevalence of the phenomenon at location s and time t. Examples include wildlife mortality, algal blooms, hail damage, and seismic intensity. Direct instrumental sensing of f is often difficult and expensive. Social media offers a unique sensing opportunity for such spatio-temporal signals, where users serve the role of \u201csensors\u201d by posting their experiences of a target phenomenon. For instance, social media users readily post their encounters with dead animals: \u201cI saw a dead crow on its back in the middle of the road.\u201d\nThere are at least three challenges faced when using human social media users as sensors:\n1. Social media sources are not always reliable and consistent, due to factors including the vagaries of language and the psychology of users. This makes identifying topics of interest and labeling social media posts extremely challenging.\n2. Social media users are not under our control. In most cases, users cannot be directed or focused or maneuvered as we wish. The distribution of human users (our sensors) depends on many factors unrelated to the sensing task at hand.\nar X\niv :1\n20 4.\n22 48\nv1 [\ncs .A\nI] 1\n0 A\npr 2\n3. Location and time stamps associated with social media posts may be erroneous or missing. Most posts do not include GPS coordinates, and self-reported locations can be inaccurate or false. Furthermore, there can be random delays between an event of interest and the time of the social media post related to the event.\nMost prior work in social media event analysis has focused on the first challenge. Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33]. We discuss the related work in detail in Section 3.\nOur work in this paper focuses on the latter two challenges. We are interested in a specific topic or target phenomenon of interest that is given and fixed beforehand, and we assume that we are also given a (perhaps imperfect) method, such as a trained text classifier, to identify target posts. The first challenge is relevant here, but is not the focus of our work. The main concerns of this paper are to deal with the highly non-uniform distribution of human users (sensors), which profoundly affects our capabilities for sensing natural phenomena such as wildlife mortality, and to cope with the uncertainties in the location and time stamps associated with related social media posts. The main contribution of the paper is robust methodology for deriving accurate spatiotemporal maps of the target phenomenon in light of these two challenges."}, {"heading": "2 The Socioscope", "text": "We propose Socioscope, a probabilistic model that robustly recovers spatiotemporal signals from social media data. Formally, consider f defined on discrete spatiotemporal bins. For example, a bin (s, t) could be a U.S. state s on day t, or a county s in hour t. From the first stage we obtain xs,t, the count of target social media posts within that bin. The task is to estimate fs,t from xs,t. A commonly-used estimate is f\u0302s,t = xs,t itself. This estimate can be justified as the maximum likelihood estimate of a Poisson model x \u223c Poisson(f). This idea underlines several emerging systems such as earthquake damage monitoring from Twitter [12]. However, this estimate is unsatisfactory since the counts xs,t can be noisy : as mentioned before, the estimate ignores population bias \u2013 more target posts are generated when and where there are more social media users; the location of a target post is frequently inaccurate or missing, making it difficult to assign to the correct bin; and target posts can be quite sparse even though the total volume of social media is huge. Socioscope addresses these issues.\nFor notational simplicity, we often denote our signal of interest by a vector f = (f1, . . . , fn) > \u2208 Rn\u22650, where fj is a non-negative target phenomenon intensity in source bin j = 1 . . . n. We will use a wildlife example throughout the section. In this example, a source bin is a spatiotemporal unit such as \u201cCalifornia, day 1,\u201d and fj is the squirrel activity level in that unit. The mapping between index j and the aforementioned (s, t) is one-one and will be clear from context."}, {"heading": "2.1 Correcting Human Population Bias", "text": "For now, assume each target post comes with precise location and time meta data. This allows us to count xj , the number of target posts in bin j. Given xj , it is tempting to use the maximum likelihood estimate f\u0302j = xj which assumes a simple Poisson model xj \u223c Poisson(fj). However, this model is too naive: Even if fj = fk, e.g., the level of squirrel activities is the same in two bins, we\nwould expect xj > xk if there are more people in bin j than in bin k, simply because more people see the same group of squirrels.\nTo account for this population bias, we define an \u201cactive social media user population intensity\u201d (loosely called \u201chuman population\u201d below) g = (g1, . . . , gn)\n> \u2208 Rn\u22650. Let zj be the count of all social media posts in bin j, the vast majority of which are not about the target phenomenon. We assume zj \u223c Poisson(gj). Since typically zj 0, the maximum likelihood estimate g\u0302j = zj is reasonable.\nImportantly, we then posit the Poisson model\nxj \u223c Poisson(\u03b7(fj , gj)). (1)\nThe intensity is defined by a link function \u03b7(fj , gj). In this paper, we simply define \u03b7(fj , gj) = fj \u00b7gj but note that other more sophisticated link functions can be learned from data. Given xj and zj , one can then easily estimate fj with the plug-in estimator f\u0302j = xj/zj ."}, {"heading": "2.2 Handling Noisy and Incomplete Data", "text": "This would have been the end of the story if we could reliably assign each post to a source bin. Unfortunately, this is often not the case for social media. In this paper, we focus on the problem of spatial uncertainty due to noisy or incomplete social media data. A prime example of spatial uncertainty is the lack of location meta data in posts from Twitter (called tweets).1 In recent data we collected, only 3% of tweets contain the latitude and longitude at which they were created. Another 47% contain a valid user self-declared location in his or her profile (e.g., \u201cNew York, NY\u201d). However, such location does not automatically change while the user travels and thus may not be the true location at which a tweet is posted. The remaining 50% do not contain location at all. Clearly, we cannot reliably assign the latter two kinds of tweets to a spatiotemporal source bin. 2\nTo address this issue, we borrow an idea from Positron Emission Tomography [28]. In particular, we define m detector bins which are conceptually distinct from the n source bins. The idea is that an event originating in some source bin goes through a transition process and ends up in one of the detector bins, where it is detected. This transition is modeled by an m\u00d7 n matrix P where\nPij = Pr(detector i | source j). (2)\nP is column stochastic: \u2211m\ni=1 Pij = 1,\u2200j. We defer the discussion of our specific P to a case study, but we mention that it is possible to reliably estimate P directly from social media data (more on this later). Recall the target post intensity at source bin j is \u03b7(fj , gj). We use the transition matrix to define the target post intensity hi (note that hi can itself be viewed as a link function \u03b7\u0303(f ,g)) at detector bin i:\nhi = n\u2211 j=1 Pij\u03b7(fj , gj). (3)\n1It may be possible to recover occasional location information from the tweet text itself instead of the meta data, but the problem still exists.\n2Another kind of spatiotemporal uncertainty exists in social media even when the local and time meta data of every post is known: social media users may not immediately post right at the spot where a target phenomenon happens. Instead, there usually is an unknown time delay and spatial shift between the phenomenon and the post generation. For example, one may not post a squirrel encounter on the road until she arrives at home later; the local and time meta data only reflects tweet-generation at home. This type of spatiotemporal uncertainty can be addressed by the same source-detector transition model.\nFor the spatial uncertainty that we consider, we create three kinds of detector bins. For a source bin j such as \u201cCalifornia, day 1,\u201d the first kind collects target posts on day 1 whose latitude and longitude meta data is in California. The second kind collects target posts on day 1 without latitude and longitude meta data, but whose user self-declared profile location is in California. The third kind collects target posts on day 1 without any location information. Note the third kind of detector bin is shared by all other source bins for day 1, such as \u201cNevada, day 1,\u201d too. Consequently, if we had n = 50T source bins corresponding to the 50 US states over T days, there would be m = (2\u00d7 50 + 1)T detector bins.\nCritically, our observed target counts x are now with respect to the m detector bins instead of the n source bins: x = (x1, . . . , xm)\n>. We will also denote the count sub-vector for the first kind of detector bins by x(1), the second kind x(2), and the third kind x(3). The same is true for the overall counts z. A trivial approach is to only utilize x(1) and z(1) to arrive at the plug-in estimator\nf\u0302j = x (1) j /z (1) j . (4)\nAs we will show, we can obtain a better estimator by incorporating noisy data x(2) and incomplete data x(3). z(1) is sufficiently large and we will simply ignore z(2) and z(3)."}, {"heading": "2.3 Socioscope: Penalized Poisson Likelihood Model", "text": "We observe target post counts x = (x1, . . . , xm) in the detector bins. These are modeled as independently Poisson distributed random variables:\nxi \u223c Poisson(hi), for i = 1 . . .m. (5)\nThe log likelihood factors as\n`(f) = log m\u220f i=1 hxii e \u2212hi xi! = m\u2211 i=1 (xi log hi \u2212 hi) + c, (6)\nwhere c is a constant. In (6) we treat g as given. Target posts may be scarce in some detector bins. Indeed, we often have zero target posts for the wildlife case study to be discussed later. This problem can be mitigated by the fact that many real-world phenomena are spatiotemporally smooth, i.e., \u201cneighboring\u201d source bins in space or time tend to have similar intensity. We therefore adopt a penalized likelihood approach by constructing a graph-based regularizer. The undirected graph is constructed so that the nodes are the source bins. Let W be the n\u00d7 n symmetric non-negative weight matrix. The edge weights are such that wjk is large if j and k correspond to neighboring bins in space and time. Since W is domain specific, we defer its construction to the case study.\nBefore discussing the regularizer, we need to perform a change of variables. Poisson intensity f is non-negative, necessitating a constrained optimization problem. It is more convenient to work with an unconstrained problem. To this end, we work with the exponential family natural parameters of Poisson. Specifically, let\n\u03b8j = log fj , \u03c8j = log gj . (7)\nOur specific link function becomes \u03b7(\u03b8j , \u03c8j) = e \u03b8j+\u03c8j . The detector bin intensities become hi =\u2211n\nj=1 Pij\u03b7(\u03b8j , \u03c8j).\nOur graph-based regularizer applies to \u03b8 directly:\n\u2126(\u03b8) = 1\n2 \u03b8>L\u03b8, (8)\nwhere L is the combinatorial graph Laplacian [7]: L = D\u2212W, and D is the diagonal degree matrix with Djj = \u2211n k=1wjk.\nFinally, Socioscope is the following penalized likelihood optimization problem:\nmin \u03b8\u2208Rn \u2212 m\u2211 i=1 (xi log hi \u2212 hi) + \u03bb\u2126(\u03b8), (9)\nwhere \u03bb is a positive regularization weight."}, {"heading": "2.4 Optimization", "text": "We solve the Socioscope optimization problem (9) with BFGS, a quasi-Newton method [20]. The gradient can be easily computed as\n\u2207 = \u03bbL\u03b8 \u2212HP>(r\u2212 1), (10)\nwhere r = (r1 . . . rm) is a ratio vector with ri = xi/hi, and H is a diagonal matrix with Hjj = \u03b7(\u03b8j , \u03c8j).\nWe initialize \u03b8 with the following heuristic. Given counts x and the transition matrix P , we compute the least-squared projection \u03b70 to \u2016x\u2212P\u03b70\u20162. This projection is easy to compute. However, \u03b70 may contain negative components not suitable for Poisson intensity. We force positivity by setting \u03b70 \u2190 max(10\u22124, \u03b70) element-wise, where the floor 10\u22124 ensures that log \u03b70 > \u2212\u221e. From the definition \u03b7(\u03b8, \u03c8) = exp(\u03b8 + \u03c8), we then obtain the initial parameter\n\u03b80 = log \u03b70 \u2212 \u03c8. (11)\nOur optimization is efficient: problems with more than one thousand variables (n) are solved in about 15 seconds with fminunc() in Matlab."}, {"heading": "2.5 Parameter Tuning", "text": "The choice of the regularization parameter \u03bb has a profound effect on the smoothness of the estimates. It may be possible to select these parameters based on prior knowledge in certain problems, but for our experiments we select these parameters using a cross-validation (CV) procedure, which gives us a fully data-based and objective approach to regularization.\nCV is quite simple to implement in the Poisson setting. A hold-out set of data can be constructed by simply sub-sampling events from the total observation uniformly at random. This produces a partial data set of a subset of the counts that follows precisely the same distribution as the whole set, modulo a decrease in the total intensity per the level of subsampling. The complement of the hold-out set is what remains of the full dataset, and we will call this the training set. The hold-out set is taken to be a specific fraction of the total. For theoretical reasons beyond the scope of this paper, we do not recommend leave-one-out CV [27, 8].\nCV is implemented by generating a number of random splits of this type (we can generate as many as we wish), and for each split we run the optimization algorithm above on the training set for\na range of values of \u03bb. Then compute the (unregularized) value of the log-likelihood on the hold-out set. This provides us with an estimate of the log-likelihood for each setting of \u03bb. We simply select the setting that maximizes the estimated log-likelihood."}, {"heading": "2.6 Theoretical Considerations", "text": "The natural measure of signal-to-noise in this problem is the number of counts in each bin. The higher the counts, the more stable and \u201cless noisy\u201d our estimators will be. Indeed, if we directly observe xi \u223c Poisson(hi), then the normalized error E[(xi\u2212hihi ) 2] = h\u22121i \u2248 x \u22121 i . So larger counts, due to larger underlying intensities, lead to small errors on a relative scale. However, the accuracy of our recovery also depends on the regularity of the underlying function f . If it is very smooth, for example a constant function, then the error would be inversely proportional to the total number of counts, not the number in each individual bin. This is because in the extreme smooth case, f is determined by a single constant.\nTo give some insight into dependence of the estimate on the total number of counts, suppose that f is the underlying continuous intensity function of interest. Furthermore, let f be a Ho\u0308lder \u03b1smooth function. The parameter \u03b1 is related to the number of continuous derivatives f has. Larger values of \u03b1 correspond to smoother functions. Such a model is reasonable for the application at hand, as discussed in our motivation for regularization above. We recall the following minimax lower bound, which follows from the results in [11, 31].\nTheorem 1. Let f be a Ho\u0308lder \u03b1-smooth d-dimensional intensity function and suppose we observe N events from the distribution Poisson(f). Then there exists a constant C\u03b1 > 0 such that\ninf f\u0302 sup f E[\u2016f\u0302 \u2212 f\u201621] \u2016f\u201621 \u2265 C\u03b1N \u22122\u03b1 2\u03b1+d ,\nwhere the infimum is over all possible estimators. The error is measured with the 1-norm, rather than two norm, which is a more appropriate and natural norm in density and intensity estimation. The theorem tells us that no estimator can achieve a faster rate of error decay than the bound above. There exist many types of estimators that nearly achieve this bound (e.g., to within a log factor), and with more work it is possible to show that our regularized estimators, with adaptively chosen bin sizes and appropriate regularization parameter settings, could also nearly achieve this rate. For the purposes of this discussion, the lower bound, which certainly applies to our situation, will suffice.\nFor example, consider just two spatial dimensions (d = 2) and \u03b1 = 1 which corresponds to Lipschitz smooth functions, a very mild regularity assumption. Then the bound says that the error is proportional to N\u22121/2. This gives useful insight into the minimal data requirements of our methods. It tells us, for example, that if we want to reduce the error of the estimator by a factor of say 2, then the total number of counts must be increased by a factor of 4. If the smoothness \u03b1 is very large, then doubling the counts can halve the error. The message is simple. More events and higher counts will provide more accurate estimates."}, {"heading": "3 Related Work", "text": "To our knowledge, there is no comparable prior work that focuses on robust single recovery from social media (i.e., the \u201csecond stage\u201d as we mentioned in the introduction). However, there has\nbeen considerable related work on the first stage, which we summarize below. Topic detection and tracking (TDT) aims at identifying emerging topics from text stream and grouping documents based on their topics. The early work in this direction began with news text streamed from newswire and transcribed from other media [1]. Recent research focused on usergenerated content on the web and on the spatio-temporal variation of topics. Latent Dirichlet Allocation (LDA) [4, 14] is a popular unsupervised method to detect topics. Mei et al. [18] extended LDA by taking spatio-temporal context into account to identify subtopics from weblogs. They analyzed the spatio-temporal pattern of topic \u03b8 by p(time|\u03b8, location) and p(location|\u03b8, time), and showed that documents created from the same spatio-temporal context tend to share topics. In the same spirit, Yin et al. [33] studied GPS-associated documents, whose coordinates are generated by Gaussian Mixture Model in their generative framework. Cataldi et al. [5] proposed a feature-pivot method. They first identified keywords whose occurrences dramatically increase in a specified time interval and then connected the keywords to detect emerging topics. Besides text, social network structure also provides important information for detecting community-based topics [24] and user interests [17].\nEvent detection is highly related to TDT. Yang et al. [32] uses clustering algorithm to identify events from news stream. Others tried to distinguish posts related to real world event from nonevents ones, such as describing daily life or emotions [3]. Such kind of events were also detected in Flickr photos with meta information [6] and Twitter [30]. Yet others were interested in events with special characteristics. Popescu et al. [22, 23] focused on the detection of controversial events which provoke a public debate in which audience members express opposing opinions. Watanabe et al. [29] studied smaller-scale local-events, such as sales at a supermarket. Sakaki et al. [25] monitored Twitter to detect real-time events such as earthquakes and hurricanes.\nAnother line of related work uses social media as a data source to answer scientific questions [16]. Most previous work studied questions in linguistic, sociology and human interactions. For example, Eisenstein et al. [13] studied the geographic linguistic variation with geotagged social media. Danescu-Niculescu-Mizil et al. [10] studied the psycholinguistic theory of communication accommodation with twitter conversations. Gupte et al. [15] studied social hierarchy and stratification in online social network. Crandall et al. [9] and Anagnostopoulos et al. [2] tried to understand the social influence through the interaction on social network.\nAs stated earlier, Socioscope differs from these related work in its focus on robust signal recovery on predefined target phenomena. The target posts may be generated at a very low, though sustained, rate, and are subject to noise corruption. The above approaches are unlikely to estimate the underlying intensity accurately."}, {"heading": "4 A Synthetic Experiment", "text": "We start with a synthetic experiment whose known ground-truth intensity f allows us to quantitatively evaluate the effectiveness of Socioscope. The synthetic experiment matches the case study in the next section. There are 48 US continental states plus Washington DC, and T = 24 hours. This leads to a total of n = 1176 source bins, and m = (2 \u00d7 49 + 1)T = 2376 detector bins. The transition matrix P is the same as in the case study, to be discussed later. The overall counts z are obtained from actual Twitter data and g\u0302 = z(1).\nWe design the ground-truth target signal f to be temporally constant but spatially varying. Figure 1(a) shows the ground-truth f spatially. It is a mixture of two Gaussian distributions dis-\ncretized at the state level. The modes are in Washington and New York, respectively. From P, f and g, we generate the observed target post counts for each detector bin by a Poisson random number generator: xi \u223c Poisson( \u2211n j=1 Pi,jfjgj), i = 1 . . .m. The sum of counts in x (1) is 56, in x(2) 1106, and in x(3) 1030. Considering the number of bins we have, the data is very sparse. Given x,P,g, We compare the relative error \u2016f \u2212 f\u0302\u20162/\u2016f\u20162 of several estimators in Table 1: (i) f\u0302 = x(1)/( 1 \u2211 z(1)), where 1 is the fraction of tweets with precise location stamp (discussed later in case study). Scaling matches it to the other estimators. Figure 1(b) shows this simple estimator, aggregated spatially. It is a poor estimator: besides being non-smooth, it contains 32 \u201choles\u201d (states with zero intensity, colored in blue) due to data scarcity. (ii) f\u0302 = x (1) j /( 1z (1) j ) which naively corrects the population bias as discussed in (4). It is even worse than the simple estimator, because naive bin-wise correction magnifies the variance in sparse x(1).\n(iii) Socioscope with x(1) only. This simulates the practice of discarding noisy or incomplete data, but regularizing for smoothness. The relative error was reduced dramatically.\n(iv) Same as (iii) but replace the values of x(1) with x(1) + x(2). This simulates the practice of ignoring the noise in x(2) and pretending it is precise. The result is worse than (iii), indicating that simply including noisy data may hurt the estimation.\n(v) Socioscope with x(1) and x(2) separately, where x(2) is treated as noisy by P . It reduces the relative error further, and demonstrates the benefits of treating noisy data specially.\n(vi) Socioscope with the full x. It achieves the lowest relative error among all methods, and is the closest to the ground truth (Figure 1(c)). Compared to (v), this demonstrates that even counts x(3) without location can also help us to recover f better."}, {"heading": "5 Case Study: Roadkill", "text": "We now turn to a real-world task of estimating the spatio-temporal intensity of roadkill for several common wildlife species from Twitter posts. The study of roadkill has values in ecology, conserva-\ntion, and transportation safety. The target phenomenon consists of roadkill events for a specific species within the continental United States during September 22\u2013November 30, 2011. Our spatio-temporal source bins are state\u00d7hour-of-day. Let s index the 48 continental US states plus District of Columbia. We aggregate the 10-week study period into 24 hours of a day. The target counts x are still sparse even with aggregation: for example, most state-hour combination have zero counts for armadillo and the largest count in x(1) and x(2) is 3. Therefore, recovering the underlying signal f remains a challenge. Let t index the hours from 1 to 24. This results in |s| = 49, |t| = 24, n = |s||t| = 1176,m = (2|s|+ 1)|t| = 2376. We will often index source or detector bins by the subscript (s, t), in addition to i or j, below. The translation should be obvious."}, {"heading": "5.1 Data Preparation", "text": "We chose Twitter as our data source because public tweets can be easily collected through its APIs. All tweets include time meta data. However, most tweets do not contain location meta data, as discussed earlier.\n5.1.1 Overall Counts z(1) and Human Population Intensity g.\nTo obtain the overall counts z, we collected tweets through the Twitter stream API using bounding boxes covering continental US. The API supplied a subsample of all tweets (not just target posts) with geo-tag. Therefore, all these tweets include precise latitude and longitude on where they were created. Through a reverse geocoding database (http://www.datasciencetoolkit.org), we mapped the coordinates to a US state. There are a large number of such tweets. Counting the number of tweets in each state-hour bin gave us z(1), from which g is estimated.\nFigure 2 shows the estimated g\u0302. The x-axis is hour of day and y-axis is the states, ordered by longitude from east (top) to west (bottom). Although g\u0302 in this matrix form contains full information, it can be hard to interpret. Therefore, we visualize aggregated results as well: First, we aggregate out time in g\u0302: for each state s, we compute \u221124 t=1 g\u0302s,t and show the resulting intensity maps in\nFigure 2(b). Second, we aggregate out state in g\u0302: for each hour of day t, we compute \u221149\ns=1 g\u0302s,t and show the daily curve in Figure 2(c). From these two plots, we clearly see that human population intensity varies greatly both spatially and temporally."}, {"heading": "5.1.2 Identifying Target Posts to Obtain Counts x.", "text": "To produce the target counts x, we need to first identify target posts describing roadkill events. Although not part of Socioscope, we detail this preprocessing step here for reproducibility.\nIn step 1, we collected tweets using a keyword API. Each tweet must contain the wildlife name (e.g., \u201csquirrel(s)\u201d) and the phrase \u201cran over\u201d. We obtained 5857 squirrel tweets, 325 chipmunk tweets, 180 opossum tweets and 159 armadillo tweets during the study period. However, many such tweets did not actually describe roadkill events. For example, \u201cI almost ran over an armadillo on my longboard, luckily my cat-like reflexes saved me.\u201d Clearly, the author did not kill the armadillo.\nIn step 2, we built a binary text classifier to identify target posts among them. Following [26], the tweets were case-folded without any stemming or stopword removal. Any user mentions preceded by a \u201c@\u201d were replaced by the anonymized user name \u201c@USERNAME\u201d. Any URLs staring with \u201chttp\u201d were replaced by the token \u201cHTTPLINK\u201d. Hashtags (compound words following \u201c#\u201d) were not split and were treated as a single token. Emoticons, such as \u201c:)\u201d or \u201c:D\u201d, were also included as tokens. Each tweet is then represented by a feature vector consisting of unigram and bigram counts. If any unigram or bigram included animal names, we added an additional feature by replacing the animal name with the generic token \u201cANIMAL\u201d. For example, we would created an extra feature \u201cover ANIMAL\u201d for the bigram \u201cover raccoon\u201d. The training data consists of 1,450 manually labeled tweets in August 2011 (i.e., outside our study period). These training tweets contain hundreds of animal species, not just the target species. The binary label is whether the tweet is a true firsthand roadkill experience. We trained a linear Support Vector Machine (SVM). The CV accuracy is nearly 90%. We then applied this SVM to classify tweets surviving step 1. Those tweets receiving a positive label were treated as target posts.\nIn step 3, we produce x(1),x(2),x(3) counts. Because these target tweets were collected by the keyword API, the nature of the Twitter API means that most do not contain precise location information. As mentioned earlier, only 3% of them contain coordinates. We processed this 3% by the same reverse geocoding database to map them to a US state s, and place them in the x (1) s,t detection bins. 47% of the target posts do not contain coordinates but can be mapped to a US state from user self-declared profile location. These are placed in the x (2) s,t detection bins. The remaining 50% contained no location meta data, and were placed in the x (3) t detection bins. 3"}, {"heading": "5.1.3 Constructing the Transition Matrix P.", "text": "In this study, P characterizes the fraction of tweets which were actually generated in source bin (s, t) end up in the three detector bins: precise location st(1), potentially noisy location st(2), and missing location t(3). We define P as follows:\nP (s,t)(1),(s,t) = 0.03, and P (r,t)(1),(s,t) = 0 for \u2200r 6= s to reflect the fact that we know precisely 3% of the target posts\u2019 location.\nP (r,t)(2),(s,t) = 0.47Mr,s for all r, s.M is a 49\u00d749 \u201cmis-self-declare\u201d matrix.Mr,s is the probability that a user self-declares in her profile that she is in state r, but her post is in fact generated in state s. We estimated M from a separate large set of tweets with both coordinates and self-declared profile locations. The M matrix is asymmetric and interesting in its own right: many posts self-declared in California or New York were actually produced all over the country; many self-declared in\n3There were actually only a fraction of all tweets without location which came from all over the world. We estimated this US/World fraction using z.\nWashington DC were actually produced in Maryland or Virgina; more posts self-declare Wisconsin but were actually in Illinois than the other way around.\nPt(3),(s,t) = 0.50. This aggregates tweets with missing information into the third kind of detector bins."}, {"heading": "5.1.4 Specifying the Graph Regularizer.", "text": "Our graph has two kinds of edges. Temporal edges connect source bins with the same state and adjacent hours by weight wt. Spatial edges connect source bins with the same hour and adjacent states by weight ws. The regularization weight \u03bb was absorbed into wt and ws. We tuned the weights wt and ws with CV on the 2D grid {10\u22123, 10\u22122.5, . . . , 103}2."}, {"heading": "5.2 Results", "text": "We present results on four animals: armadillos, chipmunks, squirrels, opossums. Perhaps surprisingly, precise roadkill intensities for these animals are apparently unknown to science (This serves as a good example of the value Socioscope may provide to wildlife scientists). Instead, domain experts were only able to provide a range map of each animal, see the left column in Figure 3. These maps indicate presence/absence only, and were extracted from NatureServe [21]. In addition, the experts defined armadillo and opossum as nocturnal, chipmunk as diurnal, and squirrels as both crepuscular (active primarily during twilight) and diurnal. Due to the lack of quantitative ground-truth, our comparison will necessarily be qualitative in nature.\nSocioscope provides sensible estimates on these animals. For example, Figure 4(a) shows counts x(1) + x(2) for chipmunks which is very sparse (the largest count in any bin is 3), and Figure 4(b) the Socioscope estimate f\u0302 . The axes are the same as in Figure 2(a). In addition, we present the state-by-state intensity maps in the middle column of Figure 3 by aggregating f\u0302 spatially. The Socioscope results match the range maps well for all animals. The right column in Figure 3 shows the daily animal activities by aggregating f\u0302 temporally. These curves match the animals\u2019 diurnal patterns well, too.\nThe Socioscope estimates are superior to the baseline methods in Table 1. Due to space limit we only present two examples on chipmunks, but note that similar observations exist for all animals. The baseline estimator of simply scaling x(1) + x(2) produced the temporal and spatial aggregates in Figure 5(a,b). Compared to Figure 3(b, right), the temporal curve has a spurious peak around 4-5pm. The spatial map contains spurious intensity in California and Texas, states outside the chipmunk range as shown in Figure 3(b, left). Both are produced by population bias when and where there were strong background social media activities (see Figure 2(b,c)). In addition, the spatial map contains 27 \u201choles\u201d (states with zero intensity, colored in blue) due to data scarcity. In contrast, Socioscope\u2019s estimates in Figure 3 avoid this problem by regularization. Another baseline estimator (x(1) + x(2))/z(1) is shown in Figure 5(c). Although corrected for population bias, this estimator lacks the transition model and regularization. It does not address data scarcity either."}, {"heading": "6 Future Work", "text": "Using social media as a data source for spatio-temporal signal recovery is an emerging area. Socioscope represents a first step toward this goal. There are many open questions:\n1. We treated target posts as certain. In reality, a natural language processing system can often supply a confidence. For example, a tweet might be deemed to be a target post only with probability 0.8. It will be interesting to study ways to incorporate such confidence into our framework.\n2. The temporal delay and spatial displacement between the target event and the generation of a post is commonplace, as discussed in footnote 2. Estimating an appropriate transition matrix P from social media data so that Socioscope can handle such \u201cpoint spread functions\u201d remains future\nwork. 3. It might be necessary to include psychology factors to better model the human \u201csensors.\u201d For instance, a person may not bother to tweet about a chipmunk roadkill, but may be eager to do so upon seeing a moose roadkill.\n4. Instead of discretizing space and time into bins, one may adopt a spatial point process model to learn a continuous intensity function instead [19].\nAddressing these considerations will further improve Socioscope."}, {"heading": "7 Acknowledgments", "text": "We thank Megan K. Hines from Wildlife Data Integration Network for providing range maps and guidance on wildlife."}], "references": [{"title": "Topic Detection and Tracking: Event-Based Information Organization", "author": ["James Allan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Influence and correlation in social networks", "author": ["Aris Anagnostopoulos", "Ravi Kumar", "Mohammad Mahdian"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Beyond trending topics: Real-world event identification on twitter", "author": ["Hila Becker", "Naaman Mor", "Luis Gravano"], "venue": "In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Emerging topic detection on twitter based on temporal and social terms evaluation", "author": ["Mario Cataldi", "Luigi Di Caro", "Claudio Schifanella"], "venue": "In Proceedings of the Tenth International Workshop on Multimedia Data Mining, MDMKDD", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Event detection from flickr data through wavelet-based spatial analysis", "author": ["Ling Chen", "Abhishek Roy"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Spectral graph theory, Regional Conference Series in Mathematics, No. 92", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Concentration inequalities of the cross-validation estimate for stable predictors", "author": ["M. Cornec"], "venue": "Arxiv preprint arXiv:1011.5133,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Feedback effects between similarity and social influence in online communities", "author": ["David Crandall", "Dan Cosley", "Daniel Huttenlocher", "Jon Kleinberg", "Siddharth Suri"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Mark my words!: linguistic style accommodation in social media", "author": ["Cristian Danescu-Niculescu-Mizil", "Michael Gamon", "Susan Dumais"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Density estimation by wavelet thresholding", "author": ["D. Donoho", "I. Johnstone", "G. Kerkyacharian", "D. Picard"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "OMG earthquake! Can Twitter improve earthquake response", "author": ["P.S. Earle", "M. Guy", "C. Ostrum", "S. Horvath", "R.A. Buckmaster"], "venue": "Eos Transactions, American Geophysical Union, Fall Meeting Supplement,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A latent variable model for geographic lexical variation", "author": ["Jacob Eisenstein", "Brendan O\u2019Connor", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Finding scientific topics", "author": ["Thomas L. Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Finding hierarchy in directed online social networks", "author": ["Mangesh Gupte", "Pravin Shankar", "Jing Li", "S. Muthukrishnan", "Liviu Iftode"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Life in the network: the coming age of computational social science", "author": ["David Lazer", "Alex (Sandy) Pentland", "Lada Adamic", "Sinan Aral", "Albert Laszlo Barabasi", "Devon Brewer", "Nicholas Christakis", "Noshir Contractor", "James Fowler", "Myron Gutmann", "Tony Jebara", "Gary King", "Michael Macy", "Deb Roy", "Marshall Van Alstyne"], "venue": "Science (New York, NY),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Pet: a statistical model for popular events tracking in social communities", "author": ["Cindy Xide Lin", "Bo Zhao", "Qiaozhu Mei", "Jiawei Han"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs", "author": ["Qiaozhu Mei", "Chao Liu", "Hang Su", "ChengXiang Zhai"], "venue": "In Proceedings of the 15th international conference on World Wide Web, WWW", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical inference and simulation for spatial point processes. Monographs on statistics and applied probability", "author": ["J. M\u00f8ller", "R.P. Waagepetersen"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Numerical optimization. Springer series in operations research", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Digital distribution maps of the mammals of the western hemisphere, version 3.0", "author": ["B.D. Patterson", "G. Ceballos", "W. Sechrest", "M.F. Tognelli", "T. Brooks", "L. Luna", "P. Ortega", "I. Salazar", "B.E. Young"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Detecting controversial events from twitter", "author": ["Ana-Maria Popescu", "Marco Pennacchiotti"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Extracting events and event descriptions from twitter", "author": ["Ana-Maria Popescu", "Marco Pennacchiotti", "Deepa Paranjpe"], "venue": "In Proceedings of the 20th international conference companion on World wide web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Mining blog stories using communitybased and temporal clustering", "author": ["Arun Qamra", "Belle Tseng", "Edward Y. Chang"], "venue": "In Proceedings of the 15th ACM international conference on Information and knowledge management,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Earthquake shakes twitter users: realtime event detection by social sensors", "author": ["Takeshi Sakaki", "Makoto Okazaki", "Yutaka Matsuo"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances", "author": ["B. Settles"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Unified cross-validation methodology for selection among estimators and a general cross-validated adaptive epsilon-net estimator: Finite sample oracle inequalities and examples", "author": ["M.J. Van Der Laan", "S. Dudoit"], "venue": "U.C. Berkeley Division of Biostatistics Working Paper Series,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "A statistical model for positron emission tomography", "author": ["Y. Vardi", "L.A. Shepp", "L. Kaufman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1985}, {"title": "Jasmine: a real-time local-event detection system based on geolocation information propagated to microblogs", "author": ["Kazufumi Watanabe", "Masanao Ochi", "Makoto Okabe", "Rikio Onai"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Event detection in twitter", "author": ["J. Weng", "B.S. Lee"], "venue": "In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media. AAAI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Multiscale poisson intensity and density estimation", "author": ["R. Willett", "R. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A study of retrospective and on-line event detection", "author": ["Yiming Yang", "Tom Pierce", "Jaime Carbonell"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Geographical topic discovery and comparison", "author": ["Zhijun Yin", "Liangliang Cao", "Jiawei Han", "Chengxiang Zhai", "Thomas Huang"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}], "referenceMentions": [{"referenceID": 31, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 2, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 24, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 0, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 17, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 32, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 11, "context": "This idea underlines several emerging systems such as earthquake damage monitoring from Twitter [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "2 To address this issue, we borrow an idea from Positron Emission Tomography [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "where L is the combinatorial graph Laplacian [7]: L = D\u2212W, and D is the diagonal degree matrix with Djj = \u2211n k=1wjk.", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "We solve the Socioscope optimization problem (9) with BFGS, a quasi-Newton method [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "For theoretical reasons beyond the scope of this paper, we do not recommend leave-one-out CV [27, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "For theoretical reasons beyond the scope of this paper, we do not recommend leave-one-out CV [27, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 10, "context": "We recall the following minimax lower bound, which follows from the results in [11, 31].", "startOffset": 79, "endOffset": 87}, {"referenceID": 30, "context": "We recall the following minimax lower bound, which follows from the results in [11, 31].", "startOffset": 79, "endOffset": 87}, {"referenceID": 0, "context": "The early work in this direction began with news text streamed from newswire and transcribed from other media [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Latent Dirichlet Allocation (LDA) [4, 14] is a popular unsupervised method to detect topics.", "startOffset": 34, "endOffset": 41}, {"referenceID": 13, "context": "Latent Dirichlet Allocation (LDA) [4, 14] is a popular unsupervised method to detect topics.", "startOffset": 34, "endOffset": 41}, {"referenceID": 17, "context": "[18] extended LDA by taking spatio-temporal context into account to identify subtopics from weblogs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] studied GPS-associated documents, whose coordinates are generated by Gaussian Mixture Model in their generative framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] proposed a feature-pivot method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Besides text, social network structure also provides important information for detecting community-based topics [24] and user interests [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Besides text, social network structure also provides important information for detecting community-based topics [24] and user interests [17].", "startOffset": 136, "endOffset": 140}, {"referenceID": 31, "context": "[32] uses clustering algorithm to identify events from news stream.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Others tried to distinguish posts related to real world event from nonevents ones, such as describing daily life or emotions [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Such kind of events were also detected in Flickr photos with meta information [6] and Twitter [30].", "startOffset": 78, "endOffset": 81}, {"referenceID": 29, "context": "Such kind of events were also detected in Flickr photos with meta information [6] and Twitter [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "[22, 23] focused on the detection of controversial events which provoke a public debate in which audience members express opposing opinions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[22, 23] focused on the detection of controversial events which provoke a public debate in which audience members express opposing opinions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[29] studied smaller-scale local-events, such as sales at a supermarket.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] monitored Twitter to detect real-time events such as earthquakes and hurricanes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Another line of related work uses social media as a data source to answer scientific questions [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "[13] studied the geographic linguistic variation with geotagged social media.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] studied the psycholinguistic theory of communication accommodation with twitter conversations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] studied social hierarchy and stratification in online social network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] and Anagnostopoulos et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] tried to understand the social influence through the interaction on social network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Following [26], the tweets were case-folded without any stemming or stopword removal.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "These maps indicate presence/absence only, and were extracted from NatureServe [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Instead of discretizing space and time into bins, one may adopt a spatial point process model to learn a continuous intensity function instead [19].", "startOffset": 143, "endOffset": 147}], "year": 2012, "abstractText": "Many real-world phenomena can be represented by a spatio-temporal signal: where, when, and how much. Social media is a tantalizing data source for those who wish to monitor such signals. Unlike most prior work, we assume that the target phenomenon is known and we are given a method to count its occurrences in social media. However, counting is plagued by sample bias, incomplete data, and, paradoxically, data scarcity \u2013 issues inadequately addressed by prior work. We formulate signal recovery as a Poisson point process estimation problem. We explicitly incorporate human population bias, time delays and spatial distortions, and spatiotemporal regularization into the model to address the noisy count issues. We present an efficient optimization algorithm and discuss its theoretical properties. We show that our model is more accurate than commonly-used baselines. Finally, we present a case study on wildlife roadkill monitoring, where our model produces qualitatively convincing results.", "creator": "LaTeX with hyperref package"}}}