{"id": "1205.2265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Efficient Constrained Regret Minimization", "abstract": "online learning constitutes a mathematical framework to analyze sequential decision making problems in adversarial environments. whilst learner repeatedly delays an action, the environment responds delivers an outcome, and then therefore learner receives a reward for the played action. the goal of the robot is to maximize his total reward. basically, there are situations specifically which, in addition to modeling the cumulative reward, there are some additional constraints / goals on the sequence of decisions that must represent satisfied by the learner. for example, in \\ textit { online marketing }, simultaneously maximizing the cumulative reward and the number enabling buyers to clear advantage of word - of - mouth advertising _ future marketing seems to be a more ambitious goal than only maximizing cumulative yield. as further example, learning from costly expert input captures more realistic functions than the original setting implies applications such as routing in networks concerning power constraint. in this paper by study an extension to the motivated learning where the learner desired to maximize the total reward given that some additional constraints need to be satisfied. we propose lagrangian exponentially weighted variation ( \\ textbf { lewa } ) algorithm, an efficient function to solve constrained online learning, why is a primal dual variant of existing well known exponentially weighted average algorithm and inspired by the theory of lagrangian method in constrained optimization. we establish the regret and the penalty without the constraint bounds in full information and bandit feedback models.", "histories": [["v1", "Tue, 8 May 2012 23:06:06 GMT  (31kb)", "https://arxiv.org/abs/1205.2265v1", null], ["v2", "Thu, 4 Oct 2012 06:49:29 GMT  (32kb)", "http://arxiv.org/abs/1205.2265v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehrdad mahdavi", "tianbao yang", "rong jin"], "accepted": false, "id": "1205.2265"}, "pdf": {"name": "1205.2265.pdf", "metadata": {"source": "CRF", "title": "Efficient Constrained Regret Minimization", "authors": ["Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin"], "emails": ["mahdavim@msu.edu", "yangtia1@msu.edu", "rongjin@msu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n22 65\nv2 [\ncs .L\nG ]\n4 O\nct 2\n01 2\nKeywords: online learning, bandit, regret-minimization, repeated game playing, constrained decision making"}, {"heading": "1 Introduction", "text": "Many practical problems such as online portfolio management [1], prediction from expert advice [2,3], and online shortest path problem [4], involve making repeated decisions in an unknown and unpredictable environment (see, e.g. [5] for a comprehensive review). These situations can be formulated as a repeated game between the decision maker (i.e., the learner) and the adversary (i.e., the environment). At each round of the game, the learner selects an action from a fixed set of actions and then receives feedback (i.e., reward) for the selected action. In the adversarial or non-stochastic feedback model, we make no statistical assumption on the sequence of rewards except that the rewards are bounded. The player would like to learn from the past and hopefully make better decisions as time goes by, so that the total accumulated reward is large.\nThe analysis of online learning algorithms focuses on establishing bounds on the regret that is the difference between the reward of the best fixed action with\nthe hindsight knowledge of the observed sequence and the cumulative reward of the online learner. If the online algorithm attains sublinear bound on the regret, is said to be Hannan consistent [5], which indicates that in the long run, the learner\u2019s average reward per round approaches the average reward per round of the best action. A point worthy of notice is that the performance bound must hold for any sequence of rewards, and in particular if the sequence is chosen adversarially. We also note that this setting differs from the framework of competitive analysis where the decision maker is allowed to first observe the reward vector, and then make the decision and get the reward accordingly [6].\nIn many current literature, the application of online learning is mostly limited to problems without constraints on the decisions. However, in most scenarios, beyond maximizing the cumulative reward, there are some restrictions on the sequence of decisions made by the learner that need to be satisfied on the average. Moreover, in some applications it seems beneficial to sacrifice some reward to get along with other goals simultaneously. Therefore, one might desire algorithms for a much more ambitious framework, where we need to maximize total reward under the constraints defined on the sequence of decisions. Attempts for such extension were made in [7], where the online learning with path constraints has been addressed and algorithms with asymptotically vanishing bound have been proposed.\nAs an illustrative example, let us consider a wireless communication system where the agent chooses an appropriate transmission power in order to transmit a message successfully. If one considers the amount of power required to transmit a packet through a path as its cost, the goal of the agent may be to maximize average throughput, while keeping the average power consumption under some required threshold. As another motivating example, consider the online ads placement with budgeted advertisers. This problem can be cast as a multi armed bandit (MAB) problem, with the set of arms being the set of ads. Since each advertiser has a limited budget to represent his adds, the online learner must consider the budget restriction of each advertiser in making decisions.\nTo model abovementioned situations, we consider modifying the online learning problem to achieve both goals simultaneously where the additional goal is called constraint throughout the paper to distinguish it from the regret. Roughly speaking, we try to devise online algorithms in order to maximize the revenue and to some degree guarantee vanishing bound on the additional constraint. The constraint defined over the actions necessitates a compromise: if the algorithm be too aggressive to satisfy the constraint, then there would be less hope to attain satisfactory cumulative reward at the end of the game and on the other hand, just trying to maximize the cumulative reward will end up in a situation in which the constraint vanishes linearly in terms of the number of rounds.\nAn algorithm addressing this problem has to balance between maximizing the adversary rewards and satisfying the constraint. To affirmatively address the problem, we provide a general framework for repeated games with constraint, and propose a simple randomized algorithm called Lagrangian exponentially weighted average (LEWA) algorithm for a particular class of these games.\nThe proposed formulation is inspired by the theory of Lagrangian method in constrained optimization and is based on primal-dual formulation of the exponentially weighted average (EWA) algorithm [3] [8]. To the best of our knowledge, this is the first time a Lagrangian style relaxation has been proposed for this type of problem.\nThe contribution of the present work is to 1) introduce a general primal-dual framework for solving online learning with constraints problem; 2) propose a Lagrangian based exponentially weighted average algorithm for solving repeated games with constraints; 3) establish expected and high probability bounds on the regret and the violation of the constraints on average; 4) extend the results to the bandit setting where only partial feedback about the rewards and constraints are available.\nNotations. Before proceeding, we define the notations used in this paper. Vectors are indicated in lower case bold letters such as x where x\u22a4 denotes it transpose. By default, all vectors are column vectors. For a vector x, xi denotes its ith coordinate. We use superscripts to index rounds of the game. Componentwise multiplication between vectors is denoted by \u25e6. We use [K] as a shorthand for the set of integers {1, 2, . . . ,K}. Throughout the paper we denote by [\u00b7]+ the projection onto the positive orthant. We shall use 1 to denote the vector of all ones. Finally, for a K-dimensional vector x, (x)2 represents (x21, . . . , x 2 K)."}, {"heading": "2 Statement of the Problem", "text": "We consider the general decision-theoretic framework for online learning and extend it to capture the constraint. In original online decision making, the learner is given access to a pool of K actions. In each round t \u2208 [T ], the learner chooses a probability distribution pt = (p t 1, ..., p t K) over the actions [K] and chooses an action i randomly based on pt. In the scenario of full information, at each iteration, the adversary reveals a reward vector rt = (r t 1, \u00b7 \u00b7 \u00b7 , rtK). Choosing an action i results in receiving a reward rti , which we shall assume without loss of generality to be bounded in [0, 1]. In the partial information or bandit setting, only the cost of selected action is revealed by the adversary. The learner competes with the best fixed action in hindsight and his/her goal is to minimize the regret defined as\nRegretT = max p\nT\u2211\nt=1\np\u22a4rt \u2212 T\u2211\nt=1\np\u22a4t rt.\nThis problem is a well studied problem and there are algorithms which attain an optimal regret bound of O( \u221a T lnK) after T rounds of the game. In this paper we focus on exponentially weighted average (EWA), which will be used later as the baseline of the proposed algorithm. The EWA algorithm maintains a weight vector wt = (w t 1, \u00b7 \u00b7 \u00b7 , wtK) which is used to define the probabilities over actions. After receiving the reward vector rt at round t, the EWA algorithm updates the weight vector according to wt+1i = w t i exp(\u03b7r t i) where \u03b7 is learning rate.\nIn the new setting addressed in this paper, which we refer to as constrained regret minimization, in addition to the rewards, there exist some constraints on the decisions that need to be satisfied. In particular, for the decision p made by the learner, there is an additional constraint p\u22a4c \u2265 c0 where c is a constraint vector for specifying the constraint (e.g. the cost vector for the arms in MAB problem). We note that, in general, the reward vector rt and the constraint vector c are different and can not be combined as a single objective. The learner\u2019s goal is to maximize the total reward with respect to the optimal decision in hindsight under the constraint p\u22a4c \u2265 c0, i.e.,\nmin p1,...,pT max p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 T\u2211\nt=1\np\u22a4t rt,\nand simultaneously satisfy the constraint. Note that the comparator class includes fixed decision p that attains maximal cumulative reward had he known the rewards beforehand, while satisfying the additional constraint.\nWithin our setting, we consider repeated games with adversarial rewards and stochastic constraint. More precisely, let c = (c1, \u00b7 \u00b7 \u00b7 , cK) be the constraint vector defined over actions. In stochastic setting the vector c is unknown to the learner and in each round t \u2208 [T ], beyond the reward feedback, the learner receives a random realization ct = (c t 1, \u00b7 \u00b7 \u00b7 , ctK) of c where E[cti] = ci. The learner\u2019s goal is to choose a sequence of decisions pt, t \u2208 [T ] to minimize the regret with respect to the optimal decision in hindsight under the constraint p\u22a4c \u2265 c0. Without loss of generality we assume ct \u2208 [0, 1]K and c0 \u2208 [0, 1]. Formally, the goal of the learner is to attain a gradually vanishing constrained regret as\nRegretT = max p\u22a4c\u2265c0\n\u2211\nt\np\u22a4rt \u2212 \u2211\nt\np\u22a4t rt \u2264 O(T 1\u2212\u03b21). (1)\nFurthermore, the decisions pt, t = 1, \u00b7 \u00b7 \u00b7 , T made by the learner are required to attain sub-linear bound on the violation of the constraint in long run, i.e.,\nViolationT =\n[ T\u2211\nt=1\n( c0 \u2212 p\u22a4t c\n) ]\n+\n\u2264 O(T 1\u2212\u03b22). (2)\nWe refer to the above bound as the violation of the constraint. We distinguish two different types of constraint satisfaction algorithms: one shot and long term satisfaction. In one shot constraint satisfaction, the learner is required to satisfy the constraint at each round, i.e., p\u22a4t c \u2265 c0. In contrast, in the long term version, the learner is allowed to violate the constraint for some rounds in a controlled way; but the constraint must hold on average for all rounds, i.e., ( \u2211T\nt=1 p \u22a4 t c)/T \u2265 c0.\nThe main questions addressed in this paper are how to modify EWA algorithm to take the constraints under consideration and what would be the bounds on the regret as well as the violation of the constraints attainable by the modified algorithm."}, {"heading": "3 Related Works", "text": "As is well known, a wide range of literature deals with the online decision making problem without constraints and there exist a number of regret-minimizing algorithms that have the optimal regret bound. The most well-known and successful work is probably the Hedge algorithm [8], which was a direct generalization of Littlestone and Warmuth\u2019s Weighted Majority (WM) algorithm [3]. Other recent studies include the improved theoretical bounds and the parameter-free hedging algorithm [9] and adaptive Hedge [10] for decision-theoretic online learning. We refer readers to the [5] for an in-depth discussion of this subject.\nAs the first seminal paper in adversarial setting, Mannor et al. [7] introduced the online learning with simple path constraints. They considered the infinitely repeated two player games with stochastic rewards where for every joint action of the players, there is an additional stochastic constraint vector that is accumulated by the decision maker. The learner is asked to keep the cumulative constraint vector in a predefined set in the space of constraint vectors. They showed that if the convex set is affected by both decisions and rewards, the optimal reward is generally unattainable online. The positive result is that a relaxed goal, which is defined in terms of the convex hull of the constrained reward in hindsight is attainable. For the relaxed setting, they suggested two inefficient algorithms: one relies on Blackwell\u2019s approachability theory and the other is based on calibrated forecast of the adversary\u2019s actions. Given the implementation difficulties associated with these two methods, they suggested two efficient heuristic methods to attain the reward with meeting the constraint in the long run. We note that the analysis in [7] is asymptotic while the bounds to be established in this work are applicable to finite repeated games.\nIn [11] the budget limited MAB was introduced where polling an arm is costly where the cost of each arm is fixed in advance. In this setting both the exploration and exploitation phases are limited by a global budget. This setting matches the stochastic rewards with deterministic constraints without violation game discussed before. It has been shown that existing MAB algorithms are not suitable to efficiently deal with costly arms. They proposed the \u01eb \u2212 first algorithm that dedicates the first \u01eb fraction of the total budget exclusively for exploration and the remaining (1 \u2212 \u01eb) fraction for exploitation. [12] improves the bound obtained in [11] by proposing a knapsack based UCB [12] algorithm which extends the UCB algorithm by solving a knapsack problem at each round to cope with the constraints. We note that knapsack based UCB does not make explicit distinction between exploration and exploitation steps as done in \u01eb\u2212first algorithm. In both [12] and [11] the algorithm proceeds as long as sufficient budget existing to play the arms.\nFinally, we remark that our setting differs from the setting considered in [13] which puts restrictions on the actions taken by the adversary and not the learner as in our case."}, {"heading": "4 Full Information Constrained Regret Minimization", "text": "In this section, we present the basic algorithm for the online learning with constraint problem and analyze its performance via the primal-dual method in adversarial setting.\nA straightforward approach to tackle the problem is to modify the reward functions of the learner to include constraint term with a penalty coefficient that adjust the probability of the actions when the constraint is violated. This approach circumvents the problem of a constrained online learning by turning it into an unconstrained problem. But a simple analysis shows that, in the adversarial setting, this simple penalty based approach fails to attain gradually vanishing bounds for regret and the violation of constraint. The main difficulty arises from the fact that an adaptive adversary can play with the penalty coefficient associated with the constraint in order to weaken the influence of the penalty parameter which results in linear bound on at least one of the measures, i.e. either regret bound or violation of the constraint.\nAlternatively, since the constraint vector in our setting is stochastic, one possible solution is to take an exploration and exploitation scheme, i.e., to burn a small portion \u01eb of the rounds to estimate the constraint vector c by c\u0303 and then in the remaining (1 \u2212 \u01eb)T rounds follow the existing algorithms with restricted decisions, i.e., p \u2208 \u2206K \u2229 p\u22a4c\u0303 \u2265 c0, where \u2206K is the simplex over [K]. The parameter \u01eb balances the accuracy of estimating c and the number of rounds for exploitation to increase the total reward. One may hope that by careful adjustment of \u01eb, it would be possible to get satisfactory bounds on regret and the violation of the constraint. But unfortunately this naive approach suffers from two main drawbacks. First, the number of rounds T is not known in advance. Second, the decisions are made by projecting into an estimated domain p\u22a4c\u0303 \u2265 c0 instead of the true domain p\u22a4c \u2265 c0 which is problematic as follows. In order to show the regret bound, we need to relate the best cumulative reward in the estimated domain to that in the true domain, which however requires imposing a regularity condition on reward and constrain vectors to be solvable [14]. Basically, we can make the algorithm adaptive to T by using a similar idea to epoch greedy [15] algorithm that runs exploration/exploitation in epochs, but it still suffers from the second drawback. Additionally, projection to the inaccurate estimated constraint c\u0303 does not exclude the possibility that the solution will be infeasible.\nHere we take a different path to solve the problem. The proposed algorithm is inspired by the theory of Lagrangian method in constrained optimization. The intuition behind the proposed algorithm is to optimize one criterion (i.e., minimizing regret or maximizing the reward) subject to explicit constraint on the restrictions that the learner needs to satisfy in average for the sequence of the decisions. A challenging ingredient in this formulation is that of establishing bounds on the regret and the violation of the constraint. In particular, our\nalgorithms will exhibit a bound in the following structure,\nRegretT + Violation2T O(T 1\u2212\u03b1) \u2264 O(T 1\u2212\u03b2), (3)\nwhere ViolationT is a term related to the violation of the constraint in long term. From (3) we can derive a bound on regret and the violation of the constraint as\nRegretT \u2264 O(T 1\u2212\u03b2) (4) ViolationT \u2264 \u221a O ([T + T 1\u2212\u03b2]T 1\u2212\u03b1), (5)\nwhere the last bound follows the fact \u2212RegretT \u2264 O(T ). The detailed steps of the proposed algorithm are shown in LEWA. The algorithm keeps two set of variables: the weight vector wt and the Lagrangian multiplier \u03bbt. The high level interpretation of the algorithm is as follows: if the constraint is being violated a lot, the decision maker places more weight on the constraint controlled by \u03bbt; but it tunes down the weight on the constraint when the constraint is satisfied reasonably. We note the LEWA is equivalent to the original EWA when the constraint is satisfied at each iteration, i.e., p\u22a4t ct \u2265 c0, which gives \u03bb1 = \u00b7 \u00b7 \u00b7 = \u03bbt = . . . = 0. It should be emphasized that in some previous works such as [11], the learner is not allowed to exceed the pre-specified threshold for the violation of the constraint and the game stops as soon as the learner violates the constraint. In contrast, within our setting, the learner\u2019s goal is to obtain sub-linear bound on the long term violation of the constraint.\nWe now state the main theorem about the performance of LEWA algorithm.\nTheorem 1. Let p1,p2, \u00b7 \u00b7 \u00b7 ,pT be the sequence of randomized decisions over the set of actions [K] := {1, 2, \u00b7 \u00b7 \u00b7 ,K} produced by LEWA algorithm under the sequence of adversarial rewards r1, r2, \u00b7 \u00b7 \u00b7 , rT \u2208 [0, 1]K observed for these decisions. Let \u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbT be the corresponding dual sequence. By setting \u03b7 =\n\u221a 4 lnK/(9T ) and \u03b4 = \u03b7/2 we have:\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 E [ T\u2211\nt=1\np\u22a4t rt\n] \u2264 3 \u221a T lnK and\nE\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 O(T 3/4),\nwhere expectation is taken over randomness in c1, \u00b7 \u00b7 \u00b7 , cT . From Theorem 1 we see that the LEWA algorithm attains the optimal bound for the regret and an O(T 3/4) bound on the violation of the constraint. Before proving the Theorem 1, we state two lemmas that pave the way to the proof of theorem.\nLemma 1. [Primal Inequality] Let Rt = R 1 t + \u03bbtR 2 t , where R 1 t ,R 2 t \u2208 RK+ , wt+1 = wt\u25e6exp(\u03b7Rt), and pt = wt/w\u22a4t 1. Assuming max(\u2016R1t\u2016\u221e, \u2016R2t\u2016\u221e) \u2264 s, we have the following primal equality\nT\u2211\nt=1\n(p\u2212 pt)\u22a4Rt \u2264 lnK\n\u03b7 + s2\n( \u03b7T\n4 +\n\u03b7\n4\nT\u2211\nt=1\n\u03bb2t\n) . (6)\nProof. Let Wt = \u2211K i=1 w t i . We first show an upper bound and a lower bound on lnWT+1/W1, followed by combining the bounds together. We have\nT\u2211\nt=1\nln Wt+1 Wt = ln WT+1 W1\n= ln\nK\u2211\ni=1\nwT+1i \u2212 lnK \u2265 ln K\u2211\ni=1\npiw T+1 i \u2212 lnK \u2265 \u03b7p\u22a4\nT\u2211\nt=1\nRt \u2212 lnK,\nwhere the last inequality follows from the concavity of the log function. By following Lemma 2.2 in [5], we obtain\nT\u2211\nt=1\nln Wt+1 Wt =\nT\u2211\nt=1\nK\u2211\ni=1\nwti exp(\u03b7R t i)\u2211K\nj=1 w t j\n\u2264 \u03b7 T\u2211\nt=1\nK\u2211\ni=1 wti\u2211K j=1 w t j Rti + \u03b72 8 s2(1 + \u03bbt) 2 \u2264 \u03b7 T\u2211 t=1 p\u22a4t Rt + \u03b72 8 T\u2211 t=1 s2(1 + \u03bbt) 2.\nCombining the lower and upper bounds and using the inequality (a + b)2 \u2264 2(a2 + b2), we obtain the desired inequality in (6).\nLemma 2. [Dual Inequality] Let gt(\u03bb) = \u03b4 2 \u03bb2 + \u03bb(\u03b2t \u2212 c0), \u03bbt+1 = [(\u03bbt \u2212 \u03b7\u2207gt(\u03bbt)]+, and \u03bb1 = 0. Assuming \u03b7 > 0, 0 \u2264 \u03b2t \u2264 \u03b20, we have T\u2211\nt=1\n(\u03bbt \u2212 \u03bb)(\u03b2t \u2212 c0) + \u03b4\n2\nT\u2211\nt=1\n(\u03bb2t \u2212 \u03bb2) \u2264 \u03bb2\n2\u03b7 + (c20 + \u03b2 2 0)\u03b7T. (7)\nProof. First we note that\n\u03bbt+1 = [\u03bbt \u2212 \u03b7\u2207gt(\u03bbt)]+ = [(1\u2212 \u03b4\u03b7)\u03bbt \u2212 \u03b7(\u03b2t \u2212 c0)]+ \u2264 [(1\u2212 \u03b4\u03b7)\u03bbt + \u03b7c0]+.\nBy induction on \u03bbt, we can obtain \u03bbt \u2264 c0 \u03b4 . Applying the standard analysis of online gradient descent [16] yields\n|\u03bbt+1 \u2212 \u03bb|2 = |\u03a0+[\u03bbt \u2212 \u03b7(\u03b4\u03bbt + \u03b2t \u2212 c0)]\u2212 \u03bb|2\n\u2264 |\u03bbt \u2212 \u03bb|2 + |\u03b7(\u03b4\u03bbt \u2212 c0) + \u03b7\u03b2t|2 \u2212 2(\u03bbt \u2212 \u03bb)(\u03b7\u2207gt(\u03bbt)) \u2264 |\u03bbt \u2212 \u03bb|2 + 2\u03b72c20 + 2\u03b72\u03b220 + 2\u03b7(gt(\u03bb) \u2212 gt(\u03bbt)).\nThen, by rearranging the terms we get\ngt(\u03bbt)\u2212 gt(\u03bb) \u2264 1\n2\u03b7\n( |\u03bbt+1 \u2212 \u03bb|2 \u2212 |\u03bbt \u2212 \u03bb|2 ) + \u03b7(c20 + \u03b2 2 0).\nExpanding the terms on l.h.s and taking the sum over t, we obtain the inequality as desired.\nProof. [of Theorem 1] Applying Rt = rt + \u03bbtct to the primal inequality in Lemma 1, where max(\u2016rt\u2016\u221e, \u2016ct\u2016\u221e) \u2264 1, we have\nT\u2211\nt=1\n(p\u2212 pt)\u22a4(rt + \u03bbtct) \u2264 lnK\n\u03b7 +\n\u03b7T\n4 +\n\u03b7\n4\nT\u2211\nt=1\n\u03bb2t .\nApplying \u03b2t = p \u22a4 t ct to the dual inequality in Lemma 2, where \u03b2t \u2264 1, c0 \u2264 1, we have\nT\u2211\nt=1\n(\u03bbt \u2212 \u03bb)(p\u22a4t ct \u2212 c0) + \u03b4\n2\nT\u2211\nt=1\n(\u03bb2t \u2212 \u03bb2) \u2264 \u03bb2\n2\u03b7 + 2\u03b7T.\nCombining the above two inequalities gives\nT\u2211\nt=1\n(p\u22a4rt \u2212 p\u22a4t rt) + T\u2211\nt=1\n\u03bb(c0 \u2212 p\u22a4t ct)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n\u2264 lnK \u03b7 + 9\u03b7T 4 +\n( \u03b7\n4 \u2212 \u03b4 2\n) T\u2211\nt=1\n\u03bb2t + \u2211\nt=1\n\u03bbt(c0 \u2212 p\u22a4ct).\nTaking expectation over ct, t = 1, \u00b7 \u00b7 \u00b7 , T , by using E[ct] = c and noting that pt and \u03bbt are independent of ct, we have\nE\n[ T\u2211\nt=1\n( p\u22a4rt \u2212 p\u22a4t rt ) + T\u2211\nt=1\n\u03bb(c0 \u2212 p\u22a4t c)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n]\n\u2264 lnK \u03b7 + 9 4 \u03b7T + E\n[( \u03b7\n4 \u2212 \u03b4 2\n) T\u2211\nt=1\n\u03bb2t\n] + E [ T\u2211\nt=1\n\u03bbt(c0 \u2212 p\u22a4c) ] .\nLet p be the solution satisfying p\u22a4c \u2265 c0. Noting that \u03b74 \u2212 \u03b42 \u2264 0 and taking maximization over \u03bb > 0 in l.h.s, we get\nE [ max\np\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 p\u22a4t rt ] + E   [\u2211T t=1(c0 \u2212 p\u22a4t c) ]2 +\n2(\u03b4T + 1/\u03b7)\n  \u2264 lnK\n\u03b7 +\n9 4 \u03b7T.\nBy plugging the values of \u03b7 and \u03b4, and noting the similar structure of above inequality as in (3) and writing in (4) and (5) formats, we obtain the desired bound for regret and the violation of the constraints in long term.\nRemark 1. We note that when deriving the bound for ViolationT , we simply use a weak lower bound on regret as RegretT \u2265 \u2212T . It is possible to obtain an improved bound by considering tighter bound for the RegretT . One way to do this is to bound the regret by the variation of the reward vectors as VariationT = \u2211T t=1 \u2016rt \u2212 r\u0302T \u2016\u221e, where r\u0302T = (1/T ) \u2211T t=1 rt denotes the mean of rt, t \u2208 [T ]. The analysis in A bounds the violation of the constraint in terms of VariationT as\n[ T\u2211\nt=1\n(c0 \u2212 x\u22a4t c) ]\n+\n\u2264 O( \u221a T ) +O(T 1/4 \u221a VariationT ).\nThis bound is significantly better when the variation of the reward vectors is small and in worst case it attains an O(T 3/4) bound similar to Theorem 1."}, {"heading": "4.1 A High Probability Bound", "text": "The performance bounds proved in the previous section for the regret and the violation of the constraint only holds in expectation which may have enormous fluctuations around its mean. Here, with a simple trick, we present a modified\nversion of the LEWA algorithm which attains similar bounds with overwhelming probability. To this end, we slightly change the original LEWA algorithm. More specifically, instead of using ct in updating \u03bbt+1, we use the average estimate and add a confidence bound to achieve a more accurate estimation of the constraint vector c. The following theorem bounds the regret and the violation of the constrain in high probability for the modified algorithm.\nTheorem 2. Let \u03b1t = 1\u221a t\n\u221a (1/2) ln (2/\u01eb), \u03b7 = O(T\u22121/2), and \u03b4 = \u03b7/2. By\nrunning Algorithm 2 we have with probability 1\u2212 \u01eb\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 T\u2211\nt=1 p\u22a4t rt \u2264 O\u0303(T 1/2) and [\nT\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 O(T 3/4),\nwhere O\u0303(\u00b7) omits the log term in T . Proof. Applying Rt = rt + \u03bbtct to the primal inequality in Lemma 1, where max(\u2016rt\u2016\u221e, \u2016ct\u2016\u221e) \u2264 1, we have\nT\u2211\nt=1\n(p\u2212 pt)\u22a4(rt + \u03bbtct) \u2264 lnK\n\u03b7 +\n\u03b7T\n4 +\n\u03b7\n4\nT\u2211\nt=1\n\u03bb2t .\nApplying \u03b2t = p \u22a4 t ct +\u03b1t to the dual inequality in Lemma 2, where \u03b2t \u2264 1+\u03b11, and c0 \u2264 1, we have T\u2211\nt=1\n(\u03bbt \u2212 \u03bb)(p\u22a4t ct + \u03b1t \u2212 c0) + \u03b4\n2\nT\u2211\nt=1\n(\u03bb2t \u2212 \u03bb2) \u2264 \u03bb2\n2\u03b7 + [1 + (1 + \u03b11)\n2]\u03b7T.\nCombining the above two inequalities results in\nT\u2211\nt=1\n( p\u22a4rt \u2212 p\u22a4t rt ) + T\u2211\nt=1\n\u03bb(c0 \u2212 p\u22a4t ct \u2212 \u03b1t)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n\u2264 lnK \u03b7 +\n( 13\n4 + 2\u03b121\n) \u03b7T + [( \u03b7\n4 \u2212 \u03b4 2\n) T\u2211\nt=1\n\u03bb2t\n] + [ T\u2211\nt=1\n\u03bbt(c0 \u2212 p\u22a4ct \u2212 \u03b1t) ] .\nLet p be the solution satisfying p\u22a4c \u2265 c0. Noting that \u03b74 \u2212 \u03b42 \u2264 0, and with a probability 1\u2212 \u01eb, |p\u22a4c\u2212 p\u22a4ct| \u2264 \u03b1t, which is due to the Hoeffding\u2019s inequality [17], by taking maximization over \u03bb > 0 on the l.h.s, we have with a probability 1\u2212 \u01ebT ,\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 p\u22a4t rt +   [\u2211T t=1(c0 \u2212 p\u22a4t ct \u2212 \u03b1t) ]2 +\n2(\u03b4T + 1/\u03b7)\n  \u2264 lnK\n\u03b7 +\n( 13\n4 + 2\u03b121\n) \u03b7T\nPluging the stated values of \u03b7 and \u03b4, we have, with a probability 1\u2212 \u01ebT ,\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 p\u22a4t rt \u2264 O ( T 1/2 ln(1/\u01eb) )\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 \u221a (T + T 1/2 ln(1/\u01eb))T 1/2 + \u2211\nt\n(p\u22a4t ct + \u03b1t \u2212 p\u22a4t c)\n\u2264 O(T 3/4) + 2 T\u2211\nt=1\n\u03b1t \u2264 O(T 3/4) +O ( T 1/2 ln(1/\u01eb) ) .\nBy replacing \u01eb with \u01eb/T and noting that O(T 1/2 lnT ) \u2264 O(T 3/4), we obtain the results stated in the theorem."}, {"heading": "5 Bandit Constrained Regret Minimization", "text": "In this section, we generalize our results to the bandit setting for both rewards and constraints. In the bandit setting, at each iteration, we are required to choose an action it from the pool of the actions [K]. Then only the reward and the constraint feedback for action it are revealed to the learner, i.e. r t it , c t it . In this\ncase, we are interested in the regret bound as maxp\u22a4c\u2265c0 \u2211T t=1 p \u22a4rt \u2212 \u2211T t=1 r t it . In the classical setting, i.e., without constraint, this problem can be solved in stochastic and adversarial settings by UCB and Exp3 algorithms proposed in [18] and [19], respectively. The algorithm is shown in BanditLEWA algorithm which uses the similar idea to Exp3 for exploration and exploitation.\nBefore presenting the performance bounds of the algorithm, let us introduce two vectors: r\u0302t is all zero vector except in itth component which is set to be r\u0302tit = r t it /ptit and similarly c\u0302t is all zero vector except in itth component which is set to be c\u0302tit = c t it /ptit . It is easy to verify that Eit [r\u0302t] = rt and Eit [c\u0302t] = ct. The following theorem shows that BanditLEWA algorithm achieves O(T 3/4) regret bound and O(T 3/4) bound on the violation of the constraint in expectation.\nTheorem 3. Let \u03b3 = O(T\u22121/2), \u03b7 = \u03b3\nK\n\u03b4\n\u03b4 + 1 , by running BanditLEWA algo-\nrithm, we have\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 E [ T\u2211\nt=1\nrtit\n] \u2264 O(T 3/4) and\nE\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 O(T 3/4).\nProof. In order to have an improved analysis, we first derive an improved primal inequality and an improved dual inequality. Let Rt = r\u0302t+\u03bbtc\u0302t. By following the\nanalysis for Exp3 algorithm [19], we have\nT\u2211\nt=1\n\u03b7q\u22a4t Rt + \u03b7 2q\u22a4t R 2 t \u2265 ln WT+1 W1\n\u2265 \u03b7p\u22a4 T\u2211\nt=1\nRt \u2212 lnK. (8)\nDividing both sides by \u03b7, and taking expectation we get\nE [ p\u22a4 T\u2211\nt=1\nRt \u2212 T\u2211\nt=1\nq\u22a4t Rt\n] \u2264 lnK\n\u03b7 + \u03b7E\n[ T\u2211\nt=1\nq\u22a4t (Rt) 2\n]\n\u2264 lnK \u03b7 + \u03b7E\n[ T\u2211\nt=1\n2q\u22a4t (r\u0302t) 2 + 2\u03bb2tq \u22a4 t (c\u0302t) 2\n] \u2264 lnK\n\u03b7 +\n2\u03b7KT 1\u2212 \u03b3 + 2\u03b7K 1\u2212 \u03b3\nT\u2211\nt=1\n\u03bb2t ,\n(9)\nwhere the third inequality follows from the following inequality\nE[q\u22a4t (c\u0302t) 2] = E [ qtit ( btit ptit )2] \u2264 1 1\u2212 \u03b3E [ ptit ( ctit ptit )2]\n= 1 1\u2212 \u03b3E [ (ctit) 2\nptit\n] = 1 1\u2212 \u03b3E [ K\u2211\ni=1\n(cti) 2\n] \u2264 K\n1\u2212 \u03b3 , (10)\nand the same inequality holds for E[q\u22a4t (r\u0302t) 2]. Next, we let gt(\u03bb) = \u03b4 2 \u03bb2+\u03bb(q\u22a4t c\u0302t\u2212 c0). By following the similar analysis in the proof of Lemma 2, we have\ngt(\u03bbt)\u2212 gt(\u03bb) \u2264 1\n2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7\n2 |\u2207gt(\u03bbt)|2\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7(q\u22a4t c\u0302t) 2 + \u03b7.\nTaking summation and expectation, we have\nE\n[ T\u2211\nt=1\ngt(\u03bbt)\u2212 gt(\u03bb) ] \u2264 \u03bb 2\n2\u03b7 + \u03b7E\n[ \u2211\nt\nq\u22a4t (c\u0302t) 2\n] + \u03b7T. \u2264 \u03bb 2\n2\u03b7 +\n\u03b7KT 1\u2212 \u03b3 + \u03b7T.\n(11)\nCombining equations (11) and (9) gives\nE\n[ T\u2211\nt=1\np\u22a4rt \u2212 q\u22a4t rt ] + E [ T\u2211\nt=1\n\u03bb(c0 \u2212 q\u22a4t c)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n]\n\u2264 lnK \u03b7 + 4\u03b7KT 1\u2212 \u03b3 + ( 2\u03b7K 1\u2212 \u03b3 \u2212 \u03b3 2 ) T\u2211\nt=1\n\u03bb2t + E\n[ T\u2211\nt=1\n\u03bbt(c0 \u2212 p\u22a4c) ] .\nNoting that (1 \u2212 \u03b3)qt \u2264 pt, so we get\nE\n[ T\u2211\nt=1\n(1\u2212 \u03b3)p\u22a4rt \u2212 p\u22a4t rt ] + E [ T\u2211\nt=1\n\u03bb((1 \u2212 \u03b3)c0 \u2212 p\u22a4t c)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n]\n\u2264 lnK \u03b7 + 4\u03b7KT +\n( 2\u03b7K \u2212 (1\u2212 \u03b3)\u03b4\n2\n) T\u2211\nt=1\n\u03bb2t + E\n[ T\u2211\nt=1\n\u03bbt(c0 \u2212 p\u22a4c) ] .\nLet c0 \u2265 p\u22a4c, 2\u03b7K \u2264 (1\u2212 \u03b3) \u03b42 . By taking maximization over \u03bb, we have\nE [ max\np\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 p\u22a4t rt ] + E   [\u2211T t=1((1\u2212 \u03b3)c0 \u2212 p\u22a4t c) ]2 +\n2(\u03b4T + 1/\u03b7)\n \n\u2264 lnK \u03b7 + 4\u03b7KT + \u03b3T = K(\u03b4 + 1) lnK \u03b4\u03b3 + 4 \u03b3\u03b4 \u03b4 + 1 T + \u03b3T\n\u2264 K(\u03b4 + 1) lnK \u03b4\u03b3 + 5\u03b4 + 1 \u03b4 + 1 \u03b3T \u2264\n\u221a (5\u03b4 + 1)K lnK\n\u03b4 T .\nThen we obtain\nmax p\u22a4c\u2265c0\nT\u2211\nt=1\np\u22a4rt \u2212 E [ T\u2211\nt=1\np\u22a4t rt\n] \u2264 \u221a (5\u03b4 + 1)K lnK\n\u03b4 T\nE\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 \u221a\u221a\u221a\u221a ( T + \u221a (5\u03b4 + 1)K lnK\n\u03b4 T\n) 2(\u03b4T + 1/\u03b7) + \u03b3T .\nLet \u03b3 = O(T\u22121/2), \u03b4 = O(T\u22121/2), then we get O(T 3/4) regret and O(T 3/4) constraint bounds as claimed.\nAs our previous results, we present an algorithm with a high probability bound on the regret and the violation of the constraint. For ease of exposition, we introduce ct = 1\nt \u2211t s=1 cs and c\u0303t = 1 t \u2211t s=1 c\u0302s. We modify BanditLEWA algorithm\nHigh Probability BanditLEWA(\u03b7, \u03b3, \u03b4, and \u01eb) initialize: w1 = exp ( \u03b7\u03b1 \u221a KT ) 1, and \u03bb1 = 0 , where \u03b1 = 2 \u221a ln(4KT/\u01eb)\niterate t = 1, 2, . . . , T Set qt = wt/ \u2211 j wtj\nSet pt = (1\u2212 \u03b3)qt + \u03b3/K Draw action it randomly accordingly to the probabilities pt Receive reward rtit and a realization of constraint c t it\nfor action it Update wt+1 by\nwt+1i = exp\n( \u03b7 [( r\u0302ti +\n\u03b1\npti \u221a KT\n) + \u03bbt ( c\u0303ti + 2K\n\u03b3 \u03b11 \u221a t\n)])\nUpdate \u03bbt+1 = [(1\u2212 \u03b4\u03b7)\u03bbt \u2212 \u03b7(x \u22a4\nt c\u0302t + \u03b1t \u2212 c0)]+ end iterate\nso that it uses more accurate estimations rather than using correct expectation in updating the primal and dual variables. To this end, we use upper confidence bound for rewards as Exp3.P algorithm [18] and for constraint vector c. The following theorem states the regret bound and the violation of constraints in long term for the high probability BanditLEWA. Theorem 4. Let \u03b1t = \u221a (1/2) ln(6KT/\u01eb)/ \u221a t, \u03b3 = O(T\u22121/2), \u03b7 = \u03b3\n\u03b2K\n\u03b4\n\u03b4 + 1 ,\nand \u03b1 = 2 \u221a ln(4KT/\u01eb), where \u03b2 = max{3, 1+2\u03b11}, by running High Probability BanditLEWA, we have with probability 1\u2212 \u01eb\nmax p\u22a4c\u2265c0\np\u22a4 T\u2211\nt=1\nrt \u2212 T\u2211\nt=1\nrtit \u2264 O(T 3/4/\n\u221a \u03b4) and\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 O( \u221a \u03b4T ).\nThe proof is deferred to B. From this theorem, when \u03b4 = O(T\u22121/4), the regret and the violation bounds are O(T 7/8) and O(T 7/8), respectively."}, {"heading": "6 Conclusions and Future Works", "text": "In this paper we proposed an efficient algorithm for regret minimization under stochastic constraints. The proposed algorithm, namely LEWA, is a primal-dual variant of the exponentially weighted average algorithm and relies on the theory of Lagrangian theory in constrained optimization. We establish expected and high probability bounds on the regret and the long term violation of the constraint in full information and bandit settings using novel theoretical analysis. In particular, in full information setting, LEWA algorithms attains optimal O\u0303( \u221a T ) regret bound and O(T 3/4) bound on the violation of the constraints in expectation, and with a simple trick in high probability. The present work leaves open a number of interesting directions for future work. In particular, extending the framework to handle multi-criteria online\ndecision making is left to future work. Turning the proposed algorithm to the one which exactly satisfies the constraint in the long run is also an interesting problem. Finally, it would be interesting to see if it is possible to improve the bound obtained for the violation of the constraint."}, {"heading": "7 References", "text": ""}, {"heading": "B Proof of Theorem 4", "text": "Similar to the analysis for Exp3.P algorithm in [18], we have have the following two upper confidence bounds,\nT\u2211\nt=1\nr\u0302ti + \u03b1\u03c3 t i \u2265\nT\u2211\nt=1\nrti , \u2200i (15)\nT\u2211\nt=1\n( c\u0303ti + 2K\n\u03b3 \u03b11\u221a t\n) \u2265 T\u2211\nt=1\ncti, \u2200i (16)\nwhere \u03c3ti = \u221a KT + 1KT \u2211t s=1 1/p s i . Following the same line of proof as in [18], we have\nT\u2211\nt=1\nln Wt+1 Wt\n\u2264 \u03b7 \u2211\nt=1\nq\u22a4t (r\u0302t + \u03bbtc\u0303t) + \u03b1\u03b7 1\u2212 \u03b3 \u221a kT + 4\u03b11\u03b7K \u03b3\u03b4 \u221a T\n+ 4\u03b72\n1\u2212 \u03b3\nT\u2211\nt=1\n\u2211\ni\nr\u0302ti + 4\u03b72\n1\u2212 \u03b3\nT\u2211\nt=1\n\u03bb2t c\u0303 \u22a4 t 1+\n4\u03b12\u03b72 \u03b3(1\u2212 \u03b3) + 16\u03b121\u03b7 2K2 \u03b32\u03b42 (1 + ln(T ))\nand\nT\u2211\nt=1\nln Wt+1 Wt\n\u2265 \u03b7p\u22a4 T\u2211\nt=1\n(rt + \u03bbtct)\u2212 lnK\n+ \u03b7\n( T\u2211\nt=1\n(r\u0302ti + \u03b1\u03c3 t i + \u03bbtc\u0303 t i + \u03bbt\n2K\n\u03b3 \u03b1t)\u2212 p\u22a4\nT\u2211\nt=1\n(rt + \u03bbtct)\n) .\nThen we have\np\u22a4 T\u2211\nt=1\n(rt + \u03bbtct) +\n( T\u2211\nt=1\n(r\u0302ti + \u03b1\u03c3 t i + \u03bbtc\u0303 t i + \u03bbt\n2k\n\u03b3 \u03b1t)\u2212 p\u22a4\nT\u2211\nt=1\n(rt + \u03bbtct) ) \u2212 q\u22a4t (r\u0302t + \u03bbtc\u0303t)\n\u2264 \u03b1 1\u2212 \u03b3\n\u221a KT + 4\u03b11K\n\u03b3\u03b4\n\u221a T + 4\u03b7\n1\u2212 \u03b3\nT\u2211\nt=1\n\u2211\ni\nr\u0302ti + 4\u03b7\n1\u2212 \u03b3\nT\u2211\nt=1\n\u03bb2t c\u0303 \u22a4 t 1\n+ 4\u03b12\u03b7 \u03b3(1\u2212 \u03b3) + 16\u03b121\u03b7K \u03b32\u03b42 (1 + lnT ) + lnK \u03b7\nOn the other side, let gt(\u03bb) = \u03b4 2 \u03bb2 +\u03bb(q\u22a4t c\u0302t +\u03b1t \u2212 c0), with probability 1\u2212 \u01eb/4, we have\ngt(\u03bbt)\u2212 gt(\u03bb) \u2264 1\n2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7\n2 |\u2207\u03bbgt(\u03bbt)|2\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7/2(x\u22a4t c\u0302t \u2212 c0 + \u03b1t + \u03b4\u03bbt)2\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7(x\u22a4t c\u0302t) 2 + \u03b7C\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7x\u22a4t (c\u0302t) 2 + \u03b7C\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7\n1\u2212 \u03b3 1 \u22a4c\u0302t + \u03b7C\n\u2264 1 2\u03b7\n( |\u03bb\u2212 \u03bbt|2 \u2212 |\u03bb\u2212 \u03bbt+1|2 ) + \u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7C\nwhere C = (1+\u03b11) 2, \u03b1t = \u03b11/ \u221a t. Taking summation over t = 1, \u00b7 \u00b7 \u00b7 , T of above inequalities, we have\nT\u2211\nt=1\n\u03b4 2 \u03bb2t \u2212 \u03bbt(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0302t) + \u03bb(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0302t)\u2212 \u03b4 2 \u03bb2\n\u2264 \u03bb 2\n2\u03b7 +\nT\u2211\nt=1\n\u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7CT\nCombing the primal inequality and the dual inequality, we have\n( T\u2211\nt=1\n(r\u0302ti + \u03b1\u03c3 t i + \u03bbtc\u0303 t i + \u03bbt\n2K\n\u03b3 \u03b1t)\u2212 p\u22a4\nT\u2211\nt=1\n(rt + \u03bbtct)\n)\n+\nT\u2211\nt=1\np\u22a4(rt + \u03bbtct)\u2212 q\u22a4t r\u0302t \u2212 \u03bbt(c0 \u2212 \u03b1t) + \u03b4 2 \u03bb2t + \u03bb(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0303t)\u2212 \u03b4 2 \u03bb2\n\u2264 \u03bb 2\n2\u03b7 +\nT\u2211\nt=1\n\u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7CT + \u03b1 1\u2212 \u03b3 \u221a KT + 4\u03b11K \u03b3\u03b4 \u221a T + 4\u03b7 1\u2212 \u03b3 T\u2211\nt=1\n\u2211\ni\nr\u0302ti\n+ 4\u03b7\n1\u2212 \u03b3\nT\u2211\nt=1\n\u03bb2t c\u0303 \u22a4 t 1+\n4\u03b12\u03b7 \u03b3(1\u2212 \u03b3) + 16\u03b121\u03b7K \u03b32\u03b42 (1 + lnT ) + lnK \u03b7 .\nThen with probability 1\u2212 \u01eb, we have the following inequality:\n( T\u2211\nt=1\n(r\u0302ti + \u03b1\u03c3 t i + \u03bbtc\u0303 t i + \u03bbt\n2K\n\u03b3 \u03b1t)\u2212 p\u22a4\nT\u2211\nt=1\n(rt + \u03bbtct)\n)\n+ T\u2211\nt=1\np\u22a4(rt + \u03bbtct)\u2212 q\u22a4t r\u0302t \u2212 \u03bbt(c0 \u2212 \u03b1t) + \u03b4 2 \u03bb2t + \u03bb(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0303t)\u2212 \u03b4 2 \u03bb2\n\u2264 \u03bb 2\n2\u03b7 +\nT\u2211\nt=1\n\u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7CT + \u03b1 1\u2212 \u03b3 \u221a KT + 4\u03b11k \u03b3\u03b4 \u221a T + 4\u03b7 1\u2212 \u03b3 T\u2211\nt=1\n\u2211\ni\nr\u0302ti\n+ 4\u03b7\n1\u2212 \u03b3\nT\u2211\nt=1\n\u03bb2t c\u0303 \u22a4 t 1+\n4\u03b12\u03b7 \u03b3(1\u2212 \u03b3) + 16\u03b121\u03b7K \u03b32\u03b42 (1 + lnT ) + lnK \u03b7 .\nLet U\u0302T = maxi \u2211T t=1(r\u0302 t i + \u03b1\u03c3 t i + \u03bbt(c\u0303 t i + 2K \u03b3 \u03b1t)), \u03b7 = \u03b3 \u03b2K \u03b4 \u03b4+1 , \u03b3 \u2264 (\u03b2)/(4 + \u03b2), then we have\n( 1\u2212 4\u03b3\n\u03b2(1 \u2212 \u03b3)\n) U\u0302T \u2212 p\u22a4 T\u2211\nt=1\n(rt + \u03bbtct)\n+ T\u2211\nt=1\np\u22a4(rt + \u03bbtct)\u2212 q\u22a4t r\u0302t \u2212 \u03bbt(c0 \u2212 \u03b1t) + T\u2211\nt=1\n\u03bb(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0303t)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n\u2264 T\u2211\nt=1\n\u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7CT + \u03b1 1\u2212 \u03b3 \u221a KT\n+ 4\u03b11K\n\u03b3\u03b4\n\u221a T + 4\u03b12\u03b7\n\u03b3(1\u2212 \u03b3) + 16\u03b121\u03b7K \u03b32\u03b42 (1 + lnT ) + lnK \u03b7 .\nSince U\u0302T \u2265 maxi \u2211T t=1 r t i +\u03bbtc t i, and p \u22a4 \u2211T t=1(rt+\u03bbtct) \u2264 maxi \u2211T t=1 r t i +\u03bbtc t i, then we have with probability 1\u2212 \u01eb,\nT\u2211\nt=1\np\u22a4rt \u2212 q\u22a4t r\u0302t \u2212 \u03bbt(c0 \u2212 \u03b1t \u2212 p\u22a4ct) + T\u2211\nt=1\n\u03bb(c0 \u2212 \u03b1t \u2212 q\u22a4t c\u0303t)\u2212 ( \u03b4T\n2 +\n1\n2\u03b7\n) \u03bb2\n\u2264 T\u2211\nt=1\n\u03b7\n1\u2212 \u03b3 (1 \u22a4c+\nK \u03b3 \u03b1t) + \u03b7CT + \u03b1 1\u2212 \u03b3 \u221a KT + 4\u03b11K \u03b3\u03b4 \u221a T + 4\u03b12\u03b7 \u03b3(1\u2212 \u03b3)\n+ 16\u03b121\u03b7K\n\u03b32\u03b42 (1 + lnT ) +\nlnK\n\u03b7 +\n4\u03b3\n\u03b2(1\u2212 \u03b3) maxi\n( T\u2211\nt=1\nrti + \u03bbtc t i\n)\n\u2264 \u03b11 \u221a T\n1\u2212 \u03b3 + \u03b3T \u03b2(1\u2212 \u03b3) + C\u03b3T \u03b2 + \u03b1 1\u2212 \u03b3 \u221a KT + 4\u03b11K \u03b3\u03b4 \u221a T\n+ 4\u03b12 \u03b2(1 \u2212 \u03b3)K + 16\u03b121 \u03b2\u03b3\u03b42 (1 + lnT ) + \u03b2(K lnK) \u03b3 \u03b4 + 1 \u03b4 + 4\u03b3T \u03b2(1 \u2212 \u03b3) \u03b4 + 1 \u03b4 .\nThen\nT\u2211\nt=1\np\u22a4rt \u2212 p\u22a4t r\u0302t +\n[\u2211T t=1((1\u2212 \u03b3)(c0 \u2212 \u03b1t)\u2212 p\u22a4t c\u0303t) ]2 +\n2(\u03b4T + 1/\u03b7)\n\u2264 \u03b11 \u221a T + C1\u03b3T\n\u03b2 + \u03b1\n\u221a KT + 4\u03b11k\n\u03b3\u03b4\n\u221a T + 4\u03b12\n\u03b2K + 16\u03b121 \u03b2\u03b3\u03b42 (1 + lnT )\n+ \u03b2(K lnK)\n\u03b3\n\u03b4 + 1\n\u03b4 + 4\u03b3T\n\u03b4 + 1\n\u03b2\u03b4 .\nLet \u03b3 = O(T\u22121/4), \u03b7 = O(T\u22121/4), then we obtain\nmax p\u22a4c\u2265c0\np\u22a4 T\u2211\nt=1\nrt \u2212 T\u2211\nt=1\nrtit \u2264 O(T 3/4/\n\u221a \u03b4) and\n[ T\u2211\nt=1\n(c0 \u2212 p\u22a4t c) ]\n+\n\u2264 O( \u221a \u03b4T ),\nwhen \u03b4 = O(T\u22121/4), the regret bound is O(T 7/8), the worse case constraint bound is O(T 7/8)."}], "references": [{"title": "On stochastic and worst-case models for investing", "author": ["E. Hazan", "S. Kale"], "venue": "in: NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The weighted majority algorithm, Inf. Comput", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M.K. Warmuth"], "venue": "Journal Machine Learnning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Unified algorithms for online learning and competitive analysis, COLT", "author": ["J.S.N. Niv Buchbinder", "Shahar Chen", "O. Shamir"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Online learning with sample path constraints", "author": ["S. Mannor", "J.N. Tsitsiklis", "J.Y. Yu"], "venue": "Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "A parameter-free hedging algorithm", "author": ["K. Chaudhuri", "Y. Freund", "D. Hsu"], "venue": "in: NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Epsilonfirst policies for budget-limited multi-armed bandits", "author": ["L. Tran-Thanh", "A.C. Chapman", "E.M. de Cote", "A. Rogers", "N.R. Jennings"], "venue": "in: AAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Knapsack based optimal policies for budget-limited multi-armed bandits, CoRR abs/1204.1909", "author": ["L. Tran-Thanh", "A.C. Chapman", "A. Rogers", "N.R. Jennings"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1909}, {"title": "Repeated games against budgeted adversaries", "author": ["J. Abernethy", "M.K. Warmuth"], "venue": "in: NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A characterization of stability in linear programming, Operations Research", "author": ["S.M. Robinson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1977}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "in: NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "in: Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem, Machine Learning", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["E. Hazan", "S. Kale"], "venue": "in: COLT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [], "year": 2012, "abstractText": "Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.", "creator": "LaTeX with hyperref package"}}}