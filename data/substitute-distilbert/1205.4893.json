{"id": "1205.4893", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2012", "title": "On the practically interesting instances of MAXCUT", "abstract": "the complexity of small statistical problem is traditionally quantified based on the hardness of its worst case. this approach has many advantages and has led to a deep and beautiful inference. however, from the practical perspective, this leaves much to be desired. in application areas, practically interesting instances very often occupy just a tiny part of an analyst's space of instances, estimates for vast majority of instances are simply irrelevant. sensing these issues leads a major challenge for theoretical computer systems whether may make theory more relevant to the practice of computer science.", "histories": [["v1", "Tue, 22 May 2012 12:30:27 GMT  (19kb,D)", "http://arxiv.org/abs/1205.4893v1", null]], "reviews": [], "SUBJECTS": "cs.CC cs.LG", "authors": ["yonatan bilu", "amit daniely", "nati linial", "michael saks"], "accepted": false, "id": "1205.4893"}, "pdf": {"name": "1205.4893.pdf", "metadata": {"source": "CRF", "title": "On the practically interesting instances of MAXCUT", "authors": ["Yonatan Bilu", "Amit Daniely", "Michael Saks"], "emails": ["yonatan@gmail.com", "amit.daniely@math.huji.ac.il", "nati@cs.huji.ac.il", "saks@math.rutgers.edu."], "sections": [{"heading": null, "text": "Following [BL], we apply this perspective to MAXCUT, viewed as a clustering problem. Using a variety of techniques, we investigate practically interesting instances of this problem. Specifically, we show how to solve in polynomial time distinguished, metric, expanding and dense instances of MAXCUT under mild stability assumptions. In particular, (1 + )-stability (which is optimal) suffices for metric and dense MAXCUT. We also show how to solve in polynomial time \u2126( \u221a n)-stable instances of MAXCUT, substantially improving the best previously known result.\n\u2217Parasight inc, Agudat sport hapoel 1, Jerusalem, Israel. yonatan@gmail.com \u2020Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il \u2021School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il \u00a7Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu.\nar X\niv :1\n20 5.\n48 93\nv1 [\ncs .C\nC ]\n2 2\nM ay\n2 01"}, {"heading": "1 Introduction", "text": "The primary criterion used in computational complexity to evaluate algorithms is worst case behavior, so that a problem is infeasible if no efficient algorithm can solve all its instances. In practice, this approach is often overly pessimistic, and a more realistic (but fuzzy) criterion would be to say that a problem is feasible if there is an efficient algorithm that correctly solves all of its practically interesting instances. The difference can be very substantial, since for many computational problems, the vast majority of instances are completely irrelevant for practical purposes.\nAn important case in point is clustering, where one seeks a meaningful partition of a given set of data. Almost every formal manifestation of the clustering problem is NP -Hard, yet, a clustering instance is of practical interest only if the data can indeed be partitioned in a meaningful way. Random instances are not likely to have a meaningful partition, so data sets with a meaningful partition are very special. Thus, even if no efficient algorithm can find the optimal partition for every data set, this does not imply that clustering is hard in practice. As Tali Tishby put it in conversation many years ago, many practitioners hold the opinion that \u201dclustering is either easy or pointless\u201d. That is, for a data sets that admit a meaningful partition of the data, finding it is not hard.\nCan this intuition be put on a solid theoretical foundation? Bilu and Linial [BL] proposed a framework for studying this issue. Generally speaking, their approach pertains to optimization problems with a continuous input space and discrete solution space. They proposed two criteria for an optimal solution to be evidently optimal. A solution is stable if it remains optimal under moderate perturbations of the input. A solution is distinguished if a transition to another solution reduces the value of the objective function in proportion to the distance between the two solutions. Concretely, they considered the case where the input is a weighted graph and the candidate solutions are cuts (or more generally, partitions). Here, a cut is \u03b3-stable (for \u03b3 \u2265 1) if it remains optimal even if each input weight wij is perturbed to a value between wij and \u03b3wij . A cut is \u03b1-distinguished (for \u03b1 \u2265 0) if moving to any other cut reduces the objective function by at least \u03b1 times the sum of (weighted) degrees of the vertices that switched side.\nFollowing Bilu and Linial [BL], we apply these notions to the study of the (weighted) MAXCUT problem. We also investigate the more restricted problem of Metric-MAXCUT1 which arises often in the field of machine learning. Our main results are:\nTheorem 1.1 1. For every > 0 there is a polynomial time algorithm that correctly solves all (1 + )-locally stable instances of Metric-MAXCUT.\n2. For every > 0 and C > 1 there is a polynomial time algorithm that correctly solves all (1 + )-locally stable and C-dense instances of MAXCUT.\nThe condition of C-density rules out overly-weighted edges. The notion of \u03b3-local stability is a substantial weakening of \u03b3-stability. It is defined similarly, but we only require resilience to perturbations that modify edges which are all incident with the same vertex.\nTheorem 1.2 There is a polynomial time algorithm that solves all instances of MAXCUT that are\n1. \u03b1-distinguished and \u03b3-locally stable with \u03b3 > 2 1\u2212 \u221a 1\u2212\u03b12 , or\n2. \u03b3-locally stable with \u03b3 > 2 1\u2212 \u221a 1\u2212h2 .\n1That is, MAXCUT, restricted to instances where the weight function is a metric.\nHere h is the Cheeger constant of the maximal cut.\nThis substantially improves a result from [BL] that works only for regular graphs and requires that \u03b3 > 5+ \u221a\n1\u2212\u03b12 1\u2212 \u221a 1\u2212\u03b12 or \u03b3 > 5+ \u221a 1\u2212h2 1\u2212 \u221a 1\u2212h2 . It is also shown in [BL] that n-stable instances are feasible. Here we\nderive the same conclusion under the weaker (but still impractical) assumption of \u2126( \u221a n)-stability.\nTheorem 1.3 There is a polynomial time algorithm that finds the optimal solution for every \u2126( \u221a n)-stable instance of MAXCUT.\nSome notation and terminology\nHere the input to the MAXCUT problem is the complete graph on n vertices G = (V,E) along with a symmetric function with zero diagonal w : V \u00d7 V \u2192 R+. Expressions such as \u201dw is bipartite\u201d refer to the graph which is the support of w, which is always assumed to be connected. Our purpose is to find a cut (S, S\u0304), S \u2286 V for which \u2211 a\u2208S, b\u2208S\u0304 w(a, b) is maximized.\nFix a cut (S, S\u0304). We use the self-explanatory terms \u201cthe vertices x, y are on the same side\u201d or \u201cseparated\u201d by this cut. We call the edge xy a cut edge or a non-cut edge when x, y are separated resp. on the same side of the cut. For A,B \u2282 V , we denote E(A,B) = {ab|a \u2208 A, b \u2208 B} and w(A,B) := \u2211 uv\u2208E(A,B)w(u, v). Also \u03c4w(A) = \u03c4(A) = w(A, A\u0304) and \u00b5(A) = \u00b5w(A) = w(A, V ).\nLet A \u2286 V . We denote by \u03be(A), the weight of the cut edges emanating from S, i.e., \u03be(A) =\u2211 vu\u2208E(A,A\u0304)\u2229E(S,S\u0304)w(u, v) and by \u03b9(A) = \u03c4(A)\u2212 \u03be(A) the weight of the non-cut edges. We slightly abuse notation for singletons A = {v} and pairs A = {u, v} and write \u03c4(v) or \u03b9(e) etc., where e = uv. The minimal, maximal and average degree of w are denoted by \u03b4(w) = minv\u2208V \u00b5(v),\n\u03b4\u0304(w) = maxv\u2208V \u00b5(v) and \u03b4(w) = \u2211 v\u2208V \u00b5(v) n respectively. (The potentially confused reader may find the following Greek-mathematical dictionary useful: \u03c4 stands for \u201ctotal\u201d, \u03be for \u201cexternal\u201d and \u03b9 for \u201cinternal\u201d)."}, {"heading": "1.1 Stable instances", "text": "Definition 1.4 Let w : V \u00d7 V \u2192 [0,\u221e) be an instance of MAXCUT and let \u03b3 \u2265 1. An instance w\u2032 : V \u00d7 V \u2192 [0,\u221e) is a \u03b3-perturbation of w if\n\u2200u, v \u2208 V, w(u, v) \u2264 w\u2032(u, v) \u2264 \u03b3 \u00b7 w(u, v)\nAn instance w is said to be \u03b3-stable if there is a cut which forms a maximal cut for every \u03b3perturbation w\u2032 of w.\nDefinition 1.5 Let \u03b3 \u2265 1. An instance w : V \u00d7 V \u2192 [0,\u221e) for MAXCUT is \u03b3-locally stable if there is a maximal cut (S, S\u0304) for which it is impossible to obtain a larger cut by switching the side of some vertex x and multiplying the edges in E(x, V \\ {x}) by numbers between 1 and \u03b3.\nThe definitions of stability and local stability capture the intuition of an \u201cevidently optimal\u201d solution. The following more concrete equivalent definitions are usually more convenient to use.\nObservation 1 [BL] Let w : V \u00d7 V \u2192 R be an instance of MAXCUT and let \u03b3 \u2265 1.\n\u2022 The instance w is \u03b3-stable iff there is a maximal cut for which \u03be(A) \u2265 \u03b3 \u00b7 \u03b9(A) for every A \u2282 V .\n\u2022 The instance w is \u03b3-locally stable iff there is a maximal cut for which \u03be(x) \u2265 \u03b3 \u00b7 \u03b9(x) for every x \u2208 V .\nWe say that a (not necessarily maximal) cut (S, S\u0304) is \u03b3-stable (resp. \u03b3-locally stable) if the first (resp. second) condition in Observation 1 holds.\nAs Observation 1 shows, every instance is 1-stable, and being \u03b3-stable for some \u03b3 > 1 is equivalent to having a unique maximal cut2. Finally, an instance is bipartite iff it is \u03b3-stable for every \u03b3 \u2265 1. Thus, \u03b3-stability is seen to be a relaxation of being bipartite.\nStability and local stability are quite different. As mentioned, for \u03b3 > 1 every instance has at most one \u03b3-stable cut. On the other hand, there can be numerous \u03b3-locally stable cuts: Consider the instance where w = 1 on the edges of a perfect matching and > 0 elsewhere. As \u2192 0, the local stability tends to \u221e. Yet, this instance is not \u03b3-stable for any \u03b3 > 1. It is easy to check that this instance has exponentially many \u03b3-locally stable maximal cuts. From the computational perspective the two properties are very different as well. Thus MAXCUT remains NP -hard even under arbitrarily high local stability (see [BL]), whereas we show here how to efficiently solve \u2126( \u221a n)-stable instances. Also, it is easy to decide whether a given cut is \u03b3-locally stable, but we do not know how to decide whether a given cut is \u03b3-stable and we suspect that this problem is hard. In Section 4, following a simplified version of the algorithm in [BL] for \u2126(n)-stable instances, we present a deterministic algorithm that solves every \u2126( \u221a n)-stable instance, proving Theorem 1.3."}, {"heading": "1.2 Distinguished and Expanding instances", "text": "Let w : V \u00d7 V \u2192 R+ be an instance of MAXCUT whose (unique) maximal cut is (S, S\u0304). We note that if all vertices of A \u2282 V switch side, then the weight of the cut decreases by \u03be(A)\u2212 \u03b9(A). Thus, we define\nDefinition 1.6 An instance w of MAXCUT is \u03b1-distinguished for \u03b1 \u2265 0 if for every \u2205 6= A \u2282 V , \u03be(A)\u2212 \u03b9(A) \u2265 \u03b1 \u00b7min{\u00b5(A), \u00b5(A\u0304)}.\nNote that every instance is 0-distinguished and being \u03b1-distinguished with \u03b1 > 0 is equivalent to having a unique maximal cut. It is not hard to see that 1+\u03b11\u2212\u03b1 -local stability is equivalent to \u03b1-local distinction, namely \u03be(x)\u2212 \u03b9(x) \u2265 \u03b1 \u00b7 \u00b5(x) for every x \u2208 V .\nDistinction vs Stability. Let (S, S\u0304) be a maximal cut of w : V \u00d7V \u2192 [0,\u221e). On the one hand, every \u03b1-distinguished instance is 1+\u03b11\u2212\u03b1 -stable, because \u03be(A)\u2212 \u03b9(A) \u2265 \u03b1\u00b5(A) \u2265 \u03b1(\u03be(A) + \u03b9(A)). On the other hand, highly stable instances need not be distinguished as the following bipartite example with V = {a1, . . . , an}\u222a\u0307{b1, . . . , bn} shows. Here w(ai, bj) is 1 when i = j and 1 otherwise. Clearly w is \u221e-stable. Yet, switching the sides of all the vertices in {a1, . . . , an\n2 } \u222a {b1, . . . , bn 2 }\ndecreases the weight of the cut only slightly. Such examples motivate the stronger notion of distinction. Although the cut ({a1, . . . , an}, {b1, . . . , bn}) is infinitely stable, its optimality does not seem completely evident.\nDistinction and Expansion. Call w : V \u00d7 V \u2192 R+ \u03b2-expanding if \u03b2 \u2264 h(w) where h(w) = min\u22056=A\u2282V \u03c4(A) min{\u00b5(A),\u00b5(A\u0304)} is w\u2019s Cheeger constant. An \u03b1-distinguished instance is \u03b1expanding, though highly expanding instances can even have multiple maximal cuts. However, an instance that is both \u03b3-stable and \u03b2-expanding is easily seen to be (\u03b2 \u00b7 \u03b3\u22121\u03b3+1)-distinguished. As this discussion implies, distinction is a conjunction of stability and expansion.\nIn section 3 we prove Theorem 1.2, using a spectral result from [BL]. In the appendix we re-derive this result and point out its close relation to the Geomans-Williamson algorithm [GW] and other spectral techniques.\n2To see that, note that if (S, S\u0304) is a \u03b3-stable cut and (T, T\u0304 ) is another cut then w(T, T\u0304 ) = w(S, S\u0304)\u2212 \u03be((T \u2229 S) \u222a (T\u0304 \u2229 S\u0304)) + \u03b9((T \u2229 S) \u222a (T\u0304 \u2229 S\u0304)) \u2264 w(S, S\u0304)\u2212 \u03b3\u22121\n\u03b3+1 \u03c4((T \u2229 S) \u222a (T\u0304 \u2229 S\u0304)) < w(S, S\u0304)."}, {"heading": "1.3 Metric and Dense instances", "text": "In Section 2 we study metric instances. This is done through a reduction from metric to dense instances, so we consider such instances as well (Section 2.1).\nWe call w : V \u00d7V \u2192 R C-dense for C \u2265 1 if \u2200x, y \u2208 V, w(x, y) \u2264 C \u00b7 \u03c4(x)n . As shown in [AKK], for C > 1 fixed, C-dense MAXCUT is NP -Hard, but it has a PTAS. As we show, this PTAS can be adapted to correctly solve all instances of MAXCUT that are (1 + )-locally stable and C-dense for every > 0, C > 1. The algorithm samples O(log n) vertices and tests each of their bipartitions as a seed to a cut. As we show, w.h.p., one of the resulting cuts is the maximal cut, proving the second part of Theorem 1.1.\nIn Section 2.2 we deal with Metric-MAXCUT. As shown in [VK] (with credit to L. Trevisan) Metric-MAXCUT is NP -Hard. That paper also gives a reduction from metric to (4 + o(1))-dense instances of MAXCUT, thus yielding a PTAS for Metric-MAXCUT. We show that a slight variation of this reduction preserves local stability3, and therefore yields an efficient algorithms for (1 + )- locally stable instances of Metric-MAXCUT, proving Theorem 1.1 in full.\nThis algorithm for metric instances is far from being a practically applicable clustering method. Even though it is polynomial-time, the actual run times are prohibitively high. We view this more as an invitation to seek practical algorithms for \u03b3-stable instances of metric MAXCUT for some reasonable values of \u03b3. Specifically we provide such an algorithm for (3 + )-locally stable metric instances."}, {"heading": "1.4 Relation with other work", "text": "Smoothed analysis is the best known example of a method for analyzing instances of computational problems based on their practical significance. As this method shows [ST], a certain variant of the simplex algorithm solves in polynomial time almost every input. Even closer to our theme are several recent papers on clustering. In [ABS] polynomial time algorithms are given for 3-stable instances of k-means, k-medians and other \u201ccenter based\u201d clustering problems. The constant 3 was improved in [BL2] to (1 + \u221a 2) for k-median. The papers [DLS, AB, BBV] consider data sets that admit a good clustering and show how to cluster them efficiently. Also related to our work are the planted partition model [B] and semirandom model [FK] for MAXCUT. In these models instances are generated by splitting the vertices at random V = S\u222a\u0307S\u0304. Edges in S\u00d7 S\u0304 (resp. S\u00d7S \u222a S\u0304\u00d7 S\u0304) are picked with probability p, resp. q < p. In the semirandom model we also allow an adversary to add edges to S \u00d7 S\u0304 and drop edges from S \u00d7 S \u222a S\u0304 \u00d7 S\u0304. As shown in [B, FK], a.a.s., (S, S\u0304) is the maximal cut and it can be efficiently found using certain algorithms. It not hard to see that for fixed p and q, this is a consequence of Theorem 1.1. The planted partition model is a random model that usually generates instances with a good partition, and those can be efficiently found. The semirandom model goes further by allowing an adversary to modify the input in a way that improves the optimal partition. Here we take an additional step forward, since we solve efficiently every instance with a good partition.\n3A word of caution: Our definition of stability and local stability for Metric-MAXCUT is more restrictive than one might think. We require the perturbed instance to satisfy the stability condition whether or not it is metric."}, {"heading": "2 Algorithms for locally stable dense and metric instances", "text": ""}, {"heading": "2.1 Dense instances", "text": "Theorem 2.1 For every C \u2265 1 and > 0 there is a randomized polynomial time algorithm that correctly solves all (1 + )-locally stable, C-dense instances of MAXCUT.\nThe analysis of the algorithm is based on the following lemma.\nLemma 2.2 Suppose that w : V \u00d7 V \u2192 [0,\u221e) is a C-dense instance and let (S, S\u0304) be a \u03b3-locally stable cut. Let X1, . . . , Xm be i.i.d. r.v. that are uniformly distributed on V . For x \u2208 V , let Ax be the event that S+ > S\u2212, where S\u00b1 = \u2211 w(x,Xi) over all i s.t. x and Xi are separated resp. on the same side. Then\nPr (\u222axAx) \u2264 |V | \u00b7 exp ( \u22121\n2\n( 1\nC \u00b7 \u03b3 \u2212 1 \u03b3 + 1\n)2 \u00b7m ) Proof The lemma follows from Hoeffding\u2019s bound. For every x \u2208 V , S+ \u2212 S\u2212 is a sum of m i.i.d. r.v.\u2019s of expectation \u03be(x)\u2212\u03b9(x)|V | \u2265 \u03b3\u22121 \u03b3+1 \u03c4(x) |V | . These r.v.\u2019s are bounded in absolute value, by C \u00b7 \u03c4(x) |V | .\n2 Proof (Of Theorem 2.1) Let D = 2 ( C \u00b7 2+ )2 . Let m = D \u00b7 ln(2|V |). Take an i.i.d. sample of m uniformly chosen points X1, . . . , Xm \u2208 V . By the above lemma, with probability \u2265 0.5, there is a partition {X1, . . . , Xm} = L \u2210 R such that the cut defined by S = {x \u2208 V : w(x,R) > w(x, L)} is the optimal cut. Since the number of such partitions is (2 \u00b7 |V |)ln(2)D, there are only polynomially many partitions to consider, yielding an efficient randomized algorithm for the problem.\n2\nCorollary 2.3 For every C \u2265 1 and > 0, a C-dense instance of MAXCUT has only poly(|V |)- many (1 + )-locally stable cuts.\nProof Consider the random cut (S, S\u0304), sampled as in the proof of Theorem 2.1, where the partition {X1, . . . , Xm} = L \u2210 R is chosen uniformly at random. The proof Theorem 2.1 shows that for every (1+ )-locally stable cut (T, T\u0304 ), the probability that (S, S\u0304) = (T, T\u0304 ) is \u2265 0.5 \u00b7 (2 \u00b7 |X|)\u2212D ln(2). Thus, there are at most 2 \u00b7 (2 \u00b7 |X|)D ln(2) such cuts.\n2"}, {"heading": "2.2 Metric instances", "text": "Given an instance w : V \u00d7 V \u2192 [0,\u221e) of MAXCUT, we split its vertices as follows. Pick a set V\u0303 and a surjective map \u03c0 : V\u0303 \u2192 V . A MAXCUT instance w\u0303 on V\u0303 is defined as follows:\nw\u0303(x\u0303, y\u0303) = w(x, y)\n|\u03c0\u22121(x)| \u00b7 |\u03c0\u22121(y)|\nwhere \u03c0(x\u0303) = x, \u03c0(y\u0303) = y. It is not hard to prove that\nProposition 2.4 Consider the following map from cuts of w to cuts of w\u0303 defined by\n(S, S\u0304) 7\u2192 (\u03c0\u22121(S), \u03c0\u22121(S\u0304))\nThen\n1. This map preserves weights, stability and local stability of cuts.\n2. Restricted to the locally stable cuts (i.e., \u03b3-locally stable cuts with \u03b3 > 1), this is a bijection onto the locally stable cuts of w\u0303.\n3. It maps maximal cuts to maximal cuts.\nAs the following proposition shows, the above construction is a reduction from metric to (4+o(1))dense instances.\nProposition 2.5 Let w : V \u00d7 V \u2192 [0,\u221e) be an instance of Metric-MAXCUT with w(V, V ) = 2 \u00b7 |V |2. Consider the map \u03c0 : \u2210 x\u2208V [b\u03c4w(x)c] \u2192 V . The instance w\u0303 obtained by \u03c0 is (4 + o(1))dense.\nProof Let x\u0303, y\u0303 \u2208 V\u0303 such that \u03c0(x\u0303) = x, \u03c0(y\u0303) = y. It is easy to see that (see [VK]) 2 \u00b7 |V | \u00b7 \u03c4w(x) \u2265 w(V, V ), b\u03c4w(x)c \u2265 ( 1\u2212 1|V | ) \u03c4w(x), \u03c4w\u0303(x\u0303) = \u03c4w(x) b\u03c4w(x)c \u2265 1 and w(x, y) \u2264 1 |V |(\u03c4w(x) + \u03c4w(y)). Thus, we have\nw\u0303(x\u0303, y\u0303) = w(x, y)\nb\u03c4w(x)c \u00b7 b\u03c4w(y)c\n\u2264 1 (1\u2212 1/|V |)2 \u00b7 w(x, y) \u03c4w(x) \u00b7 \u03c4w(y)\n\u2264 1 (1\u2212 1/|V |)2\n\u00b7 1 |V | [\u03c4w(x) + \u03c4w(y)]\n\u03c4w(x) \u00b7 \u03c4w(y)\n= 1 (1\u2212 1/|V |)2 \u00b7 (\n1\n|V |\u03c4w(x) +\n1\n|V |\u03c4w(y) ) \u2264 1\n(1\u2212 1/|V |)2 \u00b7 4 w(V, V )\n\u2264 1 (1\u2212 1/|V |)2 \u00b7 4 |V\u0303 |\n= (4 + o(1)) \u03c4w\u0303(x\u0303)\n|V\u0303 |\n2\nCorollary 2.6 Let > 0.\n1. There is a randomized polynomial time algorithm for (1+ )-locally stable instances of MetricMAXCUT.\n2. The number of (1 + )-locally stable cuts in a metric instance is polynomial in |V |.\n2.2.1 A faster algorithm for (3 + )-stable metric instances\nProposition 2.7 Let (L,R) be a \u03b3-locally stable cut of an instance, w, of Metric-MAXCUT. Then, for every x \u2208 L, z \u2208 R, w(x, z) \u2265 ( \u03b32\u22121 \u03b3 ) \u00b7 w(x,R)\u03b3\u00b7|R|+|L| .\nProof Using \u03b3-local stability and the triangle inequality we obtain\n1 \u03b3 w(x,R) \u2265 w(x, L) = \u2211 y\u2208L w(x, y)\n\u2265 \u2211 y\u2208L (w(z, y)\u2212 w(x, z)) = w(z, L)\u2212 |L|w(x, z) \u2265 \u03b3w(z,R)\u2212 |L|w(x, z) = \u03b3\n\u2211 y\u2208R w(z, y)\u2212 |L|w(x, z)\n\u2265 \u03b3 \u2211 y\u2208R (w(y, x)\u2212 w(z, x))\u2212 |L|w(x, z) = \u03b3w(x,R)\u2212 \u03b3|R|w(x, z)\u2212 |L|w(x, z)\n2\nTheorem 2.8 Let (X,w) be an instance of Metric-MAXCUT and let (L,R) be a \u03b3 = (3+ )-locally stable cut with > 0. Then either L or R is a (metric) ball.\nProof W.l.o.g., |L| \u2265 n2 . We find some x \u2208 L such that \u2200z \u2208 R, w(z, x) > diam(L), thus proving our claim. Select some x, y \u2208 L with w(x, y) = diam(L). For every z \u2208 L, we write w(x, y) \u2264 w(x, z) +w(y, z). Summing over every z \u2208 L, this yields |L| \u00b7w(x, y) \u2264 w(x, L) +w(y, L). W.l.o.g., assume that w(x, L) \u2265 |L|2 \u00b7 w(x, y). By local stability,\nw(x, y) \u2264 2 |L| w(x, L) \u2264 2 \u00b7 w(x,R) \u03b3 \u00b7 |L|\n(1)\nBy proposition 2.7, every z \u2208 R satisfies w(x, z) \u2265 ( \u03b32\u22121 \u03b3 ) \u00b7 w(x,R)\u03b3\u00b7|R|+|L| . Combined with equation (1), and the assumptions that \u03b3 > 3 and |L| \u2265 |R|, we obtain that w(x, z) > w(x, y) as claimed.\n2\nBy Theorem 2.8, the maximal cut of (3 + )-locally stable instances of Metric-MAXCUT can be found by simply considering all O(n2) balls.\nNote 2.9 Theorem 2.8 is tight in the following sense. We show an example of (3\u2212 )-stable metric instance (not just locally-stable), where neither side of its maximal cut is a ball, nor can it even be expressed as the union of few balls.\nHere is the example: It is a metric space (X,w) = (L \u2210 R,w) where L = {l1, . . . , l2n}, R = {r1, . . . , r2n}. Generally speaking, the distance between two points which are both in L or in R is 1. The distance between a point in L and a point in R is 3, the following are exceptions to the general rule: \u22001 \u2264 i \u2264 n, w(l2i\u22121, l2i) = w(r2i\u22121, r2i) = 2 and \u22001 \u2264 i \u2264 2n, w(li, ri) = 2 It is not hard to see that w is a (3\u2212 o(1))-stable metric instance and each side of its maximal cut cannot be decomposed into fewer than 2n balls."}, {"heading": "3 Distinguished and Expanding Instances", "text": "Let w : V \u00d7 V \u2192 [0,\u221e) be an instance of MAXCUT with a maximal cut (S, S\u0304). We identify w with an n\u00d7n matrix W , where Wij = w(i, j). Define wcut : V \u00d7V \u2192 R by wcut(u, v) = w(u, v) for uv \u2208 E(S, S\u0304) and wcut(u, v) = 0 otherwise. Similarly, denote wuncut = w\u2212wcut. Denote by Wcut and Wuncut the matrices corresponding to wcut and wuncut respectively. Finally, let D\ncut, Duncut, D and D\u2032 be the diagonal matrices defined by Dcutii = \u2211 jW cut ij , D uncut ii = \u2211 jW uncut ij , D = D cut +Duncut and D\u2032 = Dcut \u2212Duncut.\nLemma 3.1 If w is \u03b3-locally stable where \u03b3 > 2 1\u2212 \u221a 1\u2212(h(wcut))2 , then W + D\u2032 is a PSD matrix of rank n\u2212 1.\nAs shown in [BL] there is an efficient algorithm that correctly solves all instances that satisfy the conclusion of the Lemma (As pointed out in the Appendix, the GW-algorithm solves all such instances). This proves the second part of Theorem 1.2. Proof First, we note that it is enough to prove that D\u2212 1 2 (W +D\u2032)D\u2212 1 2 is a PSD matrix of rank n \u2212 1. Let f : V \u2192 R be the vector defined by fi = \u221a Dii for i \u2208 S and fi = \u2212 \u221a Dii for i \u2208 S\u0304. Since fTD\u2212 1 2 (W + D\u2032)D\u2212 1 2 f = 0, it is enough to show that vTD\u2212 1 2 (W + D\u2032)D\u2212 1 2 v > 0 for every unit vector v that is orthogonal to f . Note that\nD\u2212 1 2 (W +D\u2032)D\u2212 1 2 = D\u2212 1 2 (Dcut +W cut \u2212Duncut +W uncut)D\u2212 1 2 (2)\nThe matrix D\u2212 1 2 (W cut +Dcut)D\u2212 1 2 is positive semi-definite and f is in its kernel (to see that, note that for u \u2208 Rn, uT (W cut +Dcut)u = \u2211\nijW cut ij (ui + uj) 2). Therefore we have\nvTD\u2212 1 2 (W cut +Dcut)D\u2212 1 2 v \u2265 \u03bb2 (3)\nwhere 0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbn are the eigenvalues of D\u2212 1 2 (W cut +Dcut)D\u2212 1 2 . Moreover, W uncut + Duncut 0 \u21d2 2Duncut Duncut \u2212W uncut, where A B means that the matrix A \u2212 B is PSD. Thus, we have,\nvTD\u2212 1 2 (Duncut \u2212W uncut)D\u2212 1 2 v \u2264 2 \u00b7 vTD\u2212 1 2DuncutD\u2212 1 2 v \u2264 2 \u00b7max\ni Duncutii Dii \u2264 2 \u03b3 + 1\n(4)\nCombining equations (2), (3) and (4), it is enough to show that \u03bb2 > 2\n\u03b3+1 . However, since wcut is\nbipartite, the matrices D\u2212 1 2 (Dcut+W cut)D\u2212 1 2 and D\u2212 1 2 (Dcut\u2212W cut)D\u2212 1 2 have the same spectrum4. Also, D\u2212 1 2 (Dcut\u2212W cut)D\u2212 1 2 and D\u22121(Dcut\u2212W cut) have the same spectrum5 so it suffices to show that \u00b52 > 2 \u03b3+1 , where \u00b52 is the second smallest eigenvalue of D \u22121(Dcut \u2212W cut). By the known relation between expansion and the second eigenvalue of the Laplacian (e.g., Theorem 2.2 in [FN]), it follows that \u00b52 \u2265 mini Dcutii Dii \u00b7 (1\u2212 \u221a 1\u2212 h(wcut)2) \u2265 \u03b3\u03b3+1(1\u2212 \u221a 1\u2212 h(wcut)2)\n2\nFinally, to prove the first part of Theorem 1.2, it is enough to show that if w is \u03b1-distinguished then h(wcut) \u2265 \u03b1. Indeed, for \u2205 6= A \u2282 V we have\n\u03c4wcut(A) = \u03bew(A) \u2265 \u03bew(A)\u2212 \u03b9w(A) \u2265 \u03b1 \u00b7min{\u00b5w(A), \u00b5w(A\u0304)} \u2265 \u03b1 \u00b7min{\u00b5wcut(A), \u00b5wcut(A\u0304)} 4To see that, let P : Rn \u2192 Rn be the operator that multiply by \u22121 the coordinates corresponding to one side of the cut and fixes the other. The operator P commute with diagonal matrices and satisfies WP = \u2212PW . Thus, v be an eigenvector of D\u2212 1 2 (Dcut +W cut)D\u2212 1 2 with an eigenvalue \u03bb iff Pv an eigenvector of D\u2212 1 2 (Dcut +W cut)D\u2212 1 2 with an eigenvalue \u03bb. 5Since v is an eigenvector of D\u2212 1 2 (Dcut\u2212W cut)D\u2212 1 2 with eigenvalue \u03bb iff D\u2212 1 2 v is an eigenvector of D\u22121(Dcut\u2212 W cut) with eigenvalue \u03bb."}, {"heading": "4 Algorithms for stable instances", "text": "We begin with a useful observation.\nObservation 2 Let w be a \u03b3-stable instance of MAXCUT, and let w\u2032 be obtained from w by merging two vertices6 on the same side of w\u2019s maximal cut. Then w\u2032 is \u03b3-stable and its maximal cut is induced from w\u2019s maximal cut.\nBy the above observation, we conclude that in order to design an efficient algorithm for \u03b3-stable instances, it is enough to show in every \u03b3-stable instance, we can efficiently find a pair of vertices that are on the same side of the cut. Once two such vertices are found, we merge them and proceed recursively. This applies as well when \u03b3 is not a constant, but a non-decreasing function of n.\nAs an easy warm-up, we show how this observation yields a simple efficient algorithm that solves every 2n-stable instance w : V \u00d7 V \u2192 R of MAXCUT. This is a simplification of an algorithm from [BL]. By observation 2, it suffices to find two vertices which are on the same side of the maximal cut. Pick an arbitrary vertex v \u2208 V . If vu is the heaviest edge incident with v, then clearly w(v, u) \u2265 1n\u22121\u03c4(v). On the other hand, by observation 1, \u03b9(v) \u2264 1 2n+1\u03c4(v), so w(v, u) > \u03b9(v) and we conclude that vu is a cut edge. Now, let e be the heaviest edge incident with {u, v}, say e = vz. Again, w(v, z) \u2265 12(n\u22122)\u03c4({u, v}) and by observation 1, \u03b9({v, u}) \u2264 1 2n+1\u03c4({v, u}), implying that w(v, z) > \u03b9({v, u}). Consequently vz is a cut edge. But since vz and vu are cut edges, the vertices z and u are on the same side of the cut.\n4.1 A deterministic algorithm for O( \u221a n)-stable instances\nFollowing observation 2, the algorithm we present will find two vertices which are on the same side of the cut. Let w : V \u00d7 V \u2192 R be a \u03b3-stable instance of MAXCUT with \u03b3 > \u221a 8n+ 4 + 1 and let (S, S\u0304) be a maximal cut. We first deal with very heavy edges. Define\nT 1 := {vu : w(v, u) > 1 \u03b3 + 1 \u00b5(v)}\nBy observation 1, all edges in T 1 are cut edges. Thus if there are two incident edges uv, vz \u2208 T 1, then u and z are on the same side of the cut and we are done. It remains to consider the case where T 1 is a matching. Define\nT 2 = {uv /\u2208 T 1 : w(u, v) > 1 \u03b3 + 1 \u03c4({u, z}) for some uz \u2208 T 1}\nAgain, by observation 1, all edges in T 2 are cut edges. If T 2 is nonempty, say uv \u2208 T 2, then there exists some uz \u2208 T 1 with w(u, v) > 1\u03b3+1\u03c4({u, z}), which implies that v and z are on the same side of the cut. We proceed to consider the case where T 2 is empty.\nFor every u, v \u2208 V define\nw\u0303(u, v) =\n{ 0 vu \u2208 T 1\nw(u, v) o/w , w\u0302(v) = { \u03c4({u, v}) vu \u2208 T 1 for some u \u2208 V \u03c4(v) o/w\nNote that w\u0302(v) is well defined, since T 1 is a matching by assumption. Since T 2 = \u2205 and T 1 is a matching, we have, for every u \u2208 V , w\u0303(v, u) \u2264 1\u03b3+1 w\u0302(v) and, again by observation 1, \u03b9(v) \u2264\n6Let w : V \u00d7V \u2192 R be an instance and let v, u \u2208 V . The instance w\u2032 : V \u2032\u00d7V \u2032 \u2192 R obtained upon merging v, u is defined as follows. V \u2032 = V \\{u, v}\u222a{v\u2032} and w\u2032(x, y) = w(x, y) for x, y \u2208 V \\{v, u}, also, w\u2032(v\u2032, x) = w(v, x)+w(u, x).\n1 \u03b3+1 w\u0302(v). Next, we observe as well that separated vertices cannot have too many common neighbors. For u, v \u2208 V we define n(u, v) := \u2211\nz\u2208V w\u0303(v, z)w\u0303(z, u). If v and u are separated, say v \u2208 S, u \u2208 S\u0304, then\nn(u, v) = \u2211 z\u2208S\u0304 w\u0303(v, z)w\u0303(z, u) + \u2211 z\u2208S w\u0303(v, z)w\u0303(z, u)\n\u2264 1 \u03b3 + 1 w\u0302(v) \u00b7 \u03b9(u) + 1 \u03b3 + 1 w\u0302(u) \u00b7 \u03b9(v) \u2264 2 (\u03b3 + 1)2 w\u0302(u) \u00b7 w\u0302(v)\nThus, it suffices to find two vertices v, u with n(u, v) > 2 (\u03b3+1)2 w\u0302(u) \u00b7 w\u0302(v), and place them on the same side of the cut. Indeed, if no such pair exists we have\n1\n4 \u2211 v\u2208V w\u03022(v) \u2264 \u2211 v\u2208V \u03c42w\u0303(v)\n= \u2211\nu,v,z\u2208V w\u0303(u, z)w\u0303(z, v)\n= \u2211\nu,v\u2208V, u 6=v n(u, v) + \u2211 u,z\u2208V w\u03032(u, z)\n\u2264 2 (\u03b3 + 1)2 \u2211 u,v\u2208V, u 6=v w\u0302(u)w\u0302(v) + \u2211 u\u2208V 1 \u03b3 + 1 w\u0302(u) \u2211 z\u2208V w\u0303(u, z) \u2264 2 (\u03b3 + 1)2 ( \u2211 u\u2208V w\u0302(u))2 + 1 \u03b3 + 1 \u2211 u\u2208V w\u0302(u)\u03c4w\u0303(u) \u2264 2n (\u03b3 + 1)2 \u2211 u\u2208V w\u03022(u) + 1 \u03b3 + 1 \u2211 u\u2208V w\u03022(u)\nAnd it follows that \u03b3 \u2264 \u221a 8n+ 4 + 1. A contradiction."}, {"heading": "5 Conclusion and open problems", "text": "Our results together with work from [AB, ABS, BL, DLS, BL2] show that in many cases practically interesting instances of hard problems are computationally feasible. Still much remains to be done toward a new paradigm of analyzing the complexity of computational problems of practical significance. Even if we restrict our attention to MAXCUT, many problems remain open. Here are some of the more significant challenges:\n\u2022 Following [BL], we recall the (admittedly bold) conjecture that there is a constant \u03b3\u2217 > 1, s.t. \u03b3\u2217-stable instances can be solved in polynomial time.\n\u2022 It is interesting seek the best possible dependency of \u03b3 on \u03b1 in Theorem 1.2. We are quite certain that further improvements are possible.\n\u2022 With reference to Corollary 2.6, can you find a practically efficient algorithm for, say, 2-locally stable metric instances?"}, {"heading": "A The Spectral approach and the GW algorithm", "text": "Convex programming relaxations play a key role in the study of hard computational problem. They mostly play a prominent role in the search for approximate solutions. The Goemans-Williamson (GW) approximate solution for MAXCUT is a prime example of this approach. Can such algorithms\nprovide as well exact solutions for practically interesting instances? Many papers (e.g. [B, DP, GW, M]) study the relationships between the maximal cut and spectrum of matrices associated with the instance. Such ideas have led to various heuristics and approximation algorithms for MAXCUT. In section A we ask under which conditions those methods solve MAXCUT exactly. As shown is Section 3, distinguished instances satisfy such conditions.\nWe need some terminology. We identify an instance w of MAXCUT with an n\u00d7 n matrix W , where Wij = w(i, j). A vector v \u2208 Rn is called a generalized least eigenvector (GLEV) of W if there is a diagonal matrix D such that v it is an eigenvector of W +D, corresponding to (W +D)\u2019s least eigenvalue, \u03bb. By letting \u2206 := D \u2212 \u03bbI we see that v is a GLEV iff v is in the kernel of W + \u2206 for \u2206 diagonal with W + D 0. (As usual A 0 means that A is positive semi-definite). A vector v \u2208 Rn induces the cut (S, S\u0304) where S = {i : vi > 0}. An algorithm for MAXCUT is called spectral if it always returns a cut that is induced by a GLEV.\nMany popular approximation algorithms and heuristics for MAXCUT are spectral. They usually work by returning the cut induced by w\u2019s lowest eigenvector (LEV) or by LEV\u2019s of related matrices. As we note below, the GW-algorithm is also spectral. Here is the underlying logic of this approach. The characteristic vector of the cut (S, S\u0304) is defined as \u03b4S = \u03c7S \u2212 \u03c7S\u0304 where \u03c7A : V \u2192 {0, 1} is the indicator function of A. If D is a diagonal matrix, then \u03b4TS (W +D)\u03b4S = 2w(V ) + \u2211n i=1(Dii \u2212 Wii)\u2212 4w(S, S\u0304). Thus, the MAXCUT problem can be formulated as follows\nminimize vT (W +D)v subject to v \u2208 {1,\u22121}n (5)\nA natural relaxations to this problem is.\nminimize vT (W +D)v subject to ||v|| = 1 (6)\nwhere || \u00b7 || denotes the Euclidean norm. Now the set of solutions v of (6) coincides with the set of least eigenvectors of W +D. In view of (5), it is natural to consider the cut induced by such v.\nThe GW-Algorithm\nThere is another relaxation to (5), that seems unrelated to (6). It was suggested by [GW] and will play a major role in the sequel. In problem (5) we seek n vectors v1, . . . , vn in the 0 dimensional sphere S0 = {\u22121, 1} to minimize \u2211 i,jWi,j\u3008vi, vj\u3009. Interesting relaxations are obtained by replacing S0 with Sm for some m. As observed by [GW] for m = n\u2212 1, the relaxation\nminimize \u2211 i,j Wi,j\u3008vi, vj\u3009\nsubject to vi \u2208 Sn\u22121 (7)\nis feasible. In the ideal case, the solution v1, . . . , vn of (7) is contained in a copy of S 0, embedded in Sn\u22121. which makes it a solution for (5) (in its new formulation). Thus, in the ideal case, separated vectors correspond to two antipodal points, and all vertices that are on the same side of the cut get mapped to the same point. Even if this ideal picture does not hold, one may expect that the angle between separated vertices be large. Therefore, to extract a cut from v1, . . . , vn we need a method that tends to (combinatorially) separate vertices whose images on the sphere are far apart. In [GW] this is done by returning the cut induced by the vector u \u2208 Rn defined by ui = \u3008v, vi\u3009 where v \u2208 Sn\u22121 is sampled uniformly. This yields the approximation ratio 0.879.\nTo solve (7) the GW algorithm finds first a solution P to the problem\nminimize P \u25e6W subject to P 0\nPii = 1, \u2200i \u2208 [n] (8)\nWhere P \u25e6W := \u2211\n1\u2264i,j\u2264n Pij \u00b7Wij . Since P 0 it is possible to find next vectors v1, . . . , vn such that Pij = \u3008vi, vj\u3009. The dual to (8) is (see [GW])\nmaximize n\u2211 i=1 Dii subject to W \u2212D 0. D is diagonal\n(9)\nAs observed in [GW], by SDP duality the optima of (8) and (9) coincide. Denote by P(W ) and D(W ) the set of optimal solutions to (8) and (9) respectively. Denote also P = {P \u2208Mn(R) : P 0 and \u2200i, Pii = 1}, D = {D \u2208Mn(R) : D is diagonal}. We say that W is GW-bipolar if there exists a solution to (9) that also solves the binary problem (6) (i.e., it is contained in a copy of S0 embedded in Sn\u22121). Equivalently, W is GW-bipolar if P(W) contains a matrix of the form v \u00b7 vT for some v \u2208 {\u22121, 1}n. Finally, we shall say that W is strongly GW-bipolar if every solution to (7) is also a solution of (6). Our interest in strongly GW-bipolar instances is clear. The maximal cut of such an instance can be immediately read of the output of the GW-algorithm.\nAn overview. We start by asking which instances of MAXCUT can be solved exactly by a spectral algorithm. As we show, the maximal cut is induced by a \u00b11 GLEV iff the instance is GW-bipolar. More generally, an instance can be correctly solved by some spectral algorithm iff it is has a certain perturbation that is GW-bipolar. This provides additional motivation to the study of GW-bipolar instances.\nWe give a primal-dual characterizing of the set of solutions to the GW-relaxation. Specifically, we show that the dual GW problem (9) always has a unique solution D and the solutions of the primal problem are P(W ) = {P \u2208 P : P \u00b7 (W \u2212D) = 0}. This allows us to conclude that the GWalgorithm is a spectral algorithm according to our definition. We also show that GW-bipolarity is equivalent to a condition from [BL], under which MAXCUT can be solved exactly in polynomial time.\nA.1 Cuts induced by GLEV\u2019s\nLet w : V \u00d7 V \u2192 R+ be an instance with an associated matrix W . We seek conditions under which a given cut S is induced by GLEV. Let v \u2208 RV be a vector that induces the cut S. As noted before, v is a GLEV if and only if v is in the kernel of W +D for some diagonal matrix D for which W +D 0. Thus, v is a GLEV of W if and only if the optimum of the following SDP is 0.\nminimize P\nvT (W +D)v\nsubject to W +D 0 D is diagonal\n(10)\nThe dual program of (10) is\nmaximize P\nvTWv \u2212 P \u25e6W\nsubject to Pii = v 2 i\nP 0\n(11)\nSince (10) has a positive definite solution, strong duality holds. Thus, v is a GLEV iff the optimum of (11) is 0.\nNow, the optimum of the dual is 0 iff the perturbation of W defined by W \u2032ij = |vi| \u00b7 |vj | \u00b7Wij is GW-Bipolar. To see that, note that the mapping P \u2032 7\u2192 P where Pij = |vi| \u00b7 |vj | \u00b7 P \u2032ij maps the feasible solutions to the primal GW-relaxation (8) for W \u2032 onto the feasible solution to (11). Moreover, P \u25e6W = P \u2032 \u25e6W \u2032. Thus, the optimum of (11) is zero iff the optimum of the primal GW relaxation of W \u2032 is vTWv = \u03b4TSW\n\u2032\u03b4S . Consequently, the optimum of (11) is 0 iff the optimum of (8) is attained by a \u00b11 vector, making W \u2032 GW-bipolar. Note that if v \u201dstrongly induces\u201d the cut S \u2013 that is, if all coordinates |vi| are roughly equal, then W \u2032 is just a small perturbation of W . Taking this to the extreme, we conclude that the cut is induced by a \u00b11 GLEV iff W is GW-bipolar.\nA.2 The GW algorithm and GW-bipolar instances\nWe start with a primal-dual characterization of D(W ) and P(W ).\nTheorem A.1 Let W be a non-negative symmetric matrix with 0-diagonal. Then,\n1. D(W ) is a singleton7.\n2. P(W ) = {P \u2208 P : P (W \u2212D(W )) = 0}\nLemma A.2 For every D0 \u2208 D(W ), P 0 \u2208 P(W ) we have\nP(W ) = {P \u2208 P : P (W \u2212D0) = 0}\nD(W ) = {D \u2208 D : (W \u2212D) 0, P 0(W \u2212D) = 0}\nProof Let D0 \u2208 D(W ), P \u2208 P. By strong duality,\nP \u2208 P(W )\u21d4W \u25e6 P = n\u2211 i=1 D0i \u21d4W \u25e6 P = D0 \u25e6 P\nSince W \u2212D0 and P are PSDs, P \u25e6 (W \u2212D0) = 0\u21d4 P (W \u2212D0) = 0. Thus,\nP(W ) = {P \u2208 P : P (W \u2212D0) = 0}\nSimilarly, let P 0 \u2208 P(W ), D \u2208 D such that W \u2212D 0 then\nD \u2208 D(W )\u21d4W \u25e6 P 0 = n\u2211 i=1 Di \u21d4W \u25e6 P 0 = D \u25e6 P 0\nThus D(W ) = {D \u2208 D : (W \u2212D) 0, P 0(W \u2212D) = 0}\n2\nProof (of Theorem A.1) Part 2 follows from part 1 and Lemma A.2, so it only remains to prove part 1. Fix some P 0 \u2208 P(W ) and let D \u2208 D(W ). By considering the (j, j) entry of P 0(W \u2212D) = 0, we have\nDjj = n\u2211 i=1 P 0jiWij\nwhich determines D uniquely.\n7Henceforth we usually do not distinguish between D(W ) and the single matrix that it contains.\n2 Corollary A.3 GW is a spectral algorithm.\nProof Suppose that the optimum of the GW-relaxation is attained at P and let v1, . . . , vn \u2208 Sn\u22121 be vectors such that Pij = \u3008vi, vj\u3009. Let v \u2208 Sn\u22121 be the vector sampled by the algorithm and let\u2211n\nj=1 \u03b1jvj be its orthogonal projection on span{v1, . . . , vn}. The cut returned by the algorithm is the one induced by the vector ui = \u3008v, vi\u3009 = \u2211 j \u03b1jPij . The vector u is a linear combination of P \u2019s columns. Thus, by Theorem A.1 it is in the kernel of the PSD matrix W \u2212D(W ).\n2\nCorollary A.4 The GW algorithm correctly solves \u2126(n3)-stable instances.\nProof In [BL] it is shown that if u is a GLEV of a \u03b3-stable instance W such that \u03b3 \u2265 max(i,j)\u2208E |uiuj |min(i,j)\u2208E |uiuj | then u induces the optimal cut. Let u be defined as in the proof of Corollary A.3. As shown, u is a GLEV. Moreover, by an easy probabilistic argument, w.h.p., \u2200j, n\u22121.5 \u2264 |uj | \u2264 1.\n2\nHere is a characterization of GW-bipolar matrices.\nTheorem A.5 Let W be an instance for MAXCUT with maximal cut S. Denote v = \u03b4S and let D be the diagonal matrix defined by Dii = \u2212vi \u2211 jWijvj. The following conditions are equivalent.\n1. W is GW-bipolar.\n2. \u03b4S is a GLEV of W .\n3. W +D 0\n4. The optimum of the dual of the GW-relaxation is attained at \u2212D.\nProof As shown in section A.1 condition 1 is equivalent to condition 2. Suppose now that 3 holds. It is not hard to see that \u03b4S is in the kernel of W + D, so 2 holds. Condition 4 clearly entails condition 3. Finally, suppose that 1 holds. Let D\u2032 be the solution of problem (9). Since W is GWbipolar, \u03b4S \u00b7 \u03b4TS is an optimal primal solution. By Lemma A.2 we deduce that \u03b4S \u2208 ker(W \u2212D\u2032). It follows that D\u2032 = \u2212D and 4 holds.\n2\nAs noted before, strongly GW-bipolar instances can be efficiently solved using the GW algorithm. In fact, for those instances there is no need to choose a random vector to produce a cut. Moreover, those instances can be solved simply by taking the sign pattern of the least eigenvector of W +D where D is the solution to problem (9). As we explain next, strong GW-bipolarity is just slightly stronger than GW-bipolarity. Let W be a GW-bipolar instance with maximal cut (S, S\u0304). Let W \u2032 be the (1 + )-perturbation of W that is obtained by multiplying cut edges by 1 + with > 0 arbitrarily small. We claim that it is strongly GW-bipolar. Let D be the diagonal matrix defined in Theorem A.5. We have W +D 0 if and only if for every u \u2208 Sn\u22121\nuT (W +D)u = \u2211\nij\u2208E(S,S\u0304)\nWij(ui + uj) 2 \u2212 \u2211 ij /\u2208E(S,S\u0304) Wij(ui \u2212 uj)2 \u2265 0 (12)\nInequality (12) clearly holds for W \u2032 as well making it GW-bipolar. Moreover, since the maximal cut is connected, if u 6= \u00b1 1\u221a\nn \u00b7 \u03b4S then \u2211 ij\u2208E(S,S\u0304)W \u2032 ij(ui + uj) 2 > \u2211 ij\u2208E(S,S\u0304)Wij(ui + uj) 2. Thus,\nuT (W \u2032+D\u2032)u > uT (W +D)u \u2265 0 where D\u2032 is the matrix corresponding to W \u2032 from Theorem A.5. Thus, the matrix W \u2032 + D\u2032 has rank n \u2212 1. By Theorem A.1 we conclude that \u03b4S \u00b7 \u03b4TS is the only solution to the primal GW-problem for W \u2032, making W \u2032 strongly GW-bipolar.\nB A randomized algorithm for \u00b7 nlog(n)-stable instances\nWe now describe a simple randomized algorithm that correctly solves \u00b7 nlog(n) -stable instances of MAXCUT. So let w : V \u00d7 V \u2192 [0,\u221e) be a \u03b3-stable instance with \u03b3 = \u00b7 nlog(n) . Our algorithm proceeds as follows.\n1. Set V0 = {v0} for some v0 \u2208 V and set E0 = \u2205.\n2. For t = 1 to |V | \u2212 1\n\u2022 Sample a random edge vtut \u2208 E(Vt\u22121, V\u0304t\u22121), where the probability of every edge is proportional to its weight.\n\u2022 Set Vt = Vt\u22121 \u222a {vt, ut}, Et = Et\u22121 \u222a {vtut}\n3. Note that (Vt, Et) is a tree for every t and for t = |V | \u2212 1 this is a spanning tree. Return the bipartition corresponding to the two-coloring of this tree.\nAnalysis: In order to return the maximal cut, it is sufficient (in fact, also necessary) that for every t, the edge vtut be in the maximal cut. But, by observation 1, the edges in E(Vt, V\u0304t) that are in the maximal cut constitute \u2265 \u03b3\u03b3+1 fraction of all the edges in E(Vt, V\u0304t). Thus a lower bound on the success probability of the algorithm can be derived as follows:(\n\u03b3\n\u03b3 + 1\n)n\u22121 \u2265 ( 1\u2212 1\n\u03b3 + 1 )n = ( 1\u2212 1\n\u00b7 nlog(n) + 1\n)n\n\u2265 ( 1\u2212 1\n\u00b7 nlog(n) )n = ( e\u2212 + o(1)\n)ln(n) = nln(e \u2212 +o(1)) = n\u2212 +o(1)\nIn particular, for fixed the process succeeds with probability that is at least inverse polynomial in n."}], "references": [{"title": "Which data sets are clusterable? a theoretical study of clusterability", "author": ["M. Ackerman", "S. Ben David"], "venue": null, "citeRegEx": "Ackerman and David.,? \\Q2009\\E", "shortCiteRegEx": "Ackerman and David.", "year": 2009}, {"title": "Approximation schemes for dense instances of NP-hard problems", "author": ["S. Arora", "D. Karger", "M. Karpinski"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Arora et al\\.", "year": 1995}, {"title": "Center-based clustering under perturbation stability", "author": ["ABS] P. Awasthi", "A. Blum", "O. Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "Awasthi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2011}, {"title": "A discriminative framework for clustering via similarity functions", "author": ["BBV] M.F. Balcan", "A. Blum", "S. Vempala"], "venue": null, "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Clustering under Perturbation Resilience", "author": ["M.F. Balcan", "Y. Liang"], "venue": "To appear (see http://arxiv.org/pdf/1112.0826v3.pdf),", "citeRegEx": "Balcan and Liang.,? \\Q2012\\E", "shortCiteRegEx": "Balcan and Liang.", "year": 2012}, {"title": "Are Stable instances Easy", "author": ["Y. Bilu", "N. Linial"], "venue": "Innovations in Computer Science (Beijing, China,", "citeRegEx": "Bilu and Linial,? \\Q2010\\E", "shortCiteRegEx": "Bilu and Linial", "year": 2010}, {"title": "Eigenvalues and graph bisection: An average case analysis", "author": ["R. Boppana"], "venue": "FOCS", "citeRegEx": "Boppana.,? \\Q1987\\E", "shortCiteRegEx": "Boppana.", "year": 1987}, {"title": "Laplacian eigenvalues and the maximum cut problem", "author": ["DP] C. Delorme", "S. Poljak"], "venue": "Math. Programming,", "citeRegEx": "Delorme and Poljak.,? \\Q1993\\E", "shortCiteRegEx": "Delorme and Poljak.", "year": 1993}, {"title": "Clustering is difficult only when it does not matter", "author": ["DLS] A. Daniely", "N. Linial", "M. Saks"], "venue": "http://www.cs.huji.ac.il/~nati/PAPERS/cluster_ez.pdf),", "citeRegEx": "Daniely et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2012}, {"title": "Heuristics for semirandom graph problems", "author": ["U. Feige", "J. Kilian"], "venue": "J. Comput. System Sci.,", "citeRegEx": "Feige and Kilian.,? \\Q2001\\E", "shortCiteRegEx": "Feige and Kilian.", "year": 2001}, {"title": "On Cheeger-type inequalities for weighted graphs", "author": ["S. Friedland", "R. Nabban"], "venue": "Journal of Graph Theory, Volume 41, Issue", "citeRegEx": "Friedland and Nabban.,? \\Q2002\\E", "shortCiteRegEx": "Friedland and Nabban.", "year": 2002}, {"title": "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming", "author": ["M.X. Geomans", "D.P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "Geomans and Williamson.,? \\Q1995\\E", "shortCiteRegEx": "Geomans and Williamson.", "year": 1995}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": null, "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time", "author": ["D. Spielman", "S.H. Teng"], "venue": null, "citeRegEx": "Spielman and Teng.,? \\Q2001\\E", "shortCiteRegEx": "Spielman and Teng.", "year": 2001}, {"title": "A Randomized Approximation Scheme for Metric MAX-CUT", "author": ["VK] W. Fernandez de la Vega", "Claire Kenyon"], "venue": null, "citeRegEx": "Vega and Kenyon.,? \\Q1998\\E", "shortCiteRegEx": "Vega and Kenyon.", "year": 1998}], "referenceMentions": [], "year": 2012, "abstractText": "The complexity of a computational problem is traditionally quantified based on the hardness of its worst case. This approach has many advantages and has led to a deep and beautiful theory. However, from the practical perspective, this leaves much to be desired. In application areas, practically interesting instances very often occupy just a tiny part of an algorithm\u2019s space of instances, and the vast majority of instances are simply irrelevant. Addressing these issues is a major challenge for theoretical computer science which may make theory more relevant to the practice of computer science. Following [BL], we apply this perspective to MAXCUT, viewed as a clustering problem. Using a variety of techniques, we investigate practically interesting instances of this problem. Specifically, we show how to solve in polynomial time distinguished, metric, expanding and dense instances of MAXCUT under mild stability assumptions. In particular, (1 + )-stability (which is optimal) suffices for metric and dense MAXCUT. We also show how to solve in polynomial time \u03a9( \u221a n)-stable instances of MAXCUT, substantially improving the best previously known result. \u2217Parasight inc, Agudat sport hapoel 1, Jerusalem, Israel. yonatan@gmail.com \u2020Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il \u2021School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il \u00a7Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu. ar X iv :1 20 5. 48 93 v1 [ cs .C C ] 2 2 M ay 2 01 2", "creator": "LaTeX with hyperref package"}}}