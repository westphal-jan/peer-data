{"id": "1206.3265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "The Computational Complexity of Sensitivity Analysis and Parameter Tuning", "abstract": "while known algorithms for sensitivity analysis like parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity past these problems has not existed established as yet. in this paper we study several variants of the tuning problem and conclude that these problems are nr - complete into general. rigorous further show that reliability problems remain np - complete or pp - complete, for arbitrary number of restricted variants. these complexity results provide insight in whether or not recent achievements in error analysis and tuning can progress extended to more general, practicable methods.", "histories": [["v1", "Wed, 13 Jun 2012 15:35:54 GMT  (179kb)", "http://arxiv.org/abs/1206.3265v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["johan kwisthout", "linda c van der gaag"], "accepted": false, "id": "1206.3265"}, "pdf": {"name": "1206.3265.pdf", "metadata": {"source": "CRF", "title": "The Computational Complexity of Sensitivity Analysis and Parameter Tuning", "authors": ["Johan Kwisthout"], "emails": ["johank@cs.uu.nl", "linda@cs.uu.nl"], "sections": [{"heading": null, "text": "While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity of these problems has not been established as yet. In this paper we study several variants of the tuning problem and show that these problems are NPPP-complete in general. We further show that the problems remain NP-complete or PP-complete, for a number of restricted variants. These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general, practicable methods."}, {"heading": "1 Introduction", "text": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4]. Whether the parameter probabilities of a network are assessed by domain experts or estimated from data, they inevitably include some inaccuracies. In a sensitivity analysis of the network, the parameter probabilities are varied within a plausible range and the effect of the variation is studied on the output computed from the network, be it a posterior probability or the most likely value of an output variable.\nThe results of a sensitivity analysis are used, for example, to establish the robustness of the network\u2019s output. The results are used also upon engineering a probabilistic network, for example to distinguish between parameters which allow some imprecision and parameters which should be determined as accurately as possible [6]. Another use is for carefully tuning the parameter probabilities of a network to arrive at some desired model behavior [3].\nResearch efforts in sensitivity analysis and parameter tuning for probabilistic networks have resulted in a variety of fundamental insights and computational methods. While the majority of these insights and methods pertain to a one-way sensitivity analysis in which the effect of varying a single parameter probability on a single output probability or output value is studied, recently there also has been some pioneering work on extending these insights to higher-order analyses [2, 6].\nThe currently available algorithms for sensitivity analysis and parameter tuning of probabilistic networks have a running time that is exponential in the size of a network. This observation suggests that these problems are intractable in general. The actual computational complexity of the problems has not been studied yet, however. In this paper we define several variants of the tuning problem for probabilistic networks and show that these variants are NPPP-complete in general. We further show that the tuning problem remains NP-comlete, even if the topological structure of the network under study is restricted to a polytree, and PP-complete, even if the number of conditional probability tables involved is bounded.\nGiven the unfavorable complexity results obtained, even for restricted cases of the tuning problem, we have that we cannot expect to arrive at efficient, more general computational methods for sensitivity analysis and parameter tuning for probabilistic networks. Our complexity results in fact suggest that further research should concentrate on tuning a limited number of parameters, in networks where inference is tractable.\nThe paper is organized as follows. After briefly reviewing the basic concepts involved in sensitivity analysis and parameter tuning in Section 2, we present some preliminaries from complexity theory in Section 3 and formally define several variants of the tuning problem in Section 4. We give a general completeness proof for these problems in Section 5. We further address some special, restricted cases of these problems in Section 6. The paper ends with our concluding observations\nin Section 7."}, {"heading": "2 Sensitivity analysis and tuning", "text": "A probabilistic network B = (G,\u0393) includes a directed acyclic graph G = (V,A), where V = {V1, . . . , Vn} models a set of stochastic variables and A models the (in)dependences between them, and a set of parameter probabilities \u0393, capturing the strengths of the relationships between the variables. The network models a joint probability distribution Pr(V) = \u220fn i=1 Pr(vi | \u03c0(Vi)) over its variables, where \u03c0(V ) denotes the parents of V in G. We will use Pr(C = c |E = e) to denote the probability of the value c of the output variable C, given an instantiation e to the set of evidence variables E, which will be abbreviated as Pr(c | e). We will denote a particular set of parameter probabilities as X \u2286 \u0393, and we will use X to denote a single parameter. We will use x and x to denote the combination of values of a set of parameters, respectively the value of a single parameter. In sensitivity analysis and parameter tuning, we are interested in the effect of changes in the parameter probabilities X on an output probability for a designated variable C. The sensitivity function fPr(c|e)(X) expresses the probability of the output in terms of the parameter set X. We will omit the subscript if no ambiguity can occur.\nIn a one-way sensitivity analysis, we measure the sensitivity of an output probability of interest with respect to a single parameter. The parameter under consideration is systematically varied from 0 to 1 and the other parameters from the same CPT are co-varied such that their mutual proportional relationship is kept constant [20]. Thus, if the parameter X = Pr(bi | \u03c1) (denoting the conditional probability of the value bi of the variable B given a particular configuration \u03c1 of B\u2019s parents) is varied from 0 to 1, the other parameters Pr(bj |\u03c1) for the variable B are varied such that\nPr(bj |\u03c1)(X) = Pr(bj |\u03c1) \u00b7 1\u2212X\n1\u2212 Pr(bi |\u03c1) for any value bj other than bi. Under the condition of covariation, the sensitivity function f(X) is a quotient of two linear functions [7] and takes the form\nf(X) = c1 \u00b7X + c2 c3 \u00b7X + c4\nwhere the constants can be calculated from the other parameter probabilities in the network.\nA one-way sensitivity analysis can be extended to measure the effect of the simultaneous variation of two parameters on the output [5]. The sensitivity function then generalizes to\nf(X1, X2) = c1 \u00b7X1 \u00b7X2 + c2 \u00b7X1 + c3 \u00b7X2 + c4 c5 \u00b7X1 \u00b7X2 + c6 \u00b7X1 + c7 \u00b7X2 + c8\nIn this function, the terms c1 \u00b7X1 \u00b7X2 and c5 \u00b7X1 \u00b7X2 capture the interaction effect of the parameters on the output variable. This can further be generalized to nway sensitivity analyses [6, 2] where multiple parameters are varied simultaneously. While higher-order analyses can reveal synergistic effects of variation, the results are often difficult to interpret [20].\nFor performing a one-way sensitivity analysis, efficient algorithms are available that build upon the observation that for establishing the sensitivity of an output probability it suffices to determine the constants in the associated sensitivity function. The simplest method for this purpose is to compute, from the network, the probability of interest for up to three values for the parameter under study; using the functional form of the function to be established, a system of linear equations is obtained, which is subsequently solved [7]. For the network computations involved, any standard propagation algorithm can be used. A more efficient method determines the required constants by propagating information through a junction tree, similar to the standard junction-tree propagation algorithm [12]. This method requires a very small number of inward and outward propagations in the tree to determine either the constants of all sensitivity functions that relate the probability of interest to any one of the network parameters, or to determine the sensitivity functions for any output probability in terms of a single parameter. Both algorithms are exponential in the size of the network, yet have a polynomial running time for networks of bounded treewidth.\nClosely related to analyzing the effect of variation of parameters on the output\u2014and often the next step after performing such an analysis\u2014is tuning the parameters, such that the output has the desired properties. The output may need to satisfy particular constraints, e.g. Pr(c | e) \u2265 q, Pr(c1 | e)/Pr(c2 | e) \u2265 q or Pr(c1 | e) \u2212 Pr(c2 | e) \u2265 q, for a particular value q. There are a number of algorithms to determine the solution space for a set of parameters given such constraints [2]. The computational complexity of these algorithms is always exponential in the treewidth w of the graph (i.e., the size of the largest clique in the jointree), yet varies from O(cw) for single parameter tuning, to O(n \u00b7\u220fki=1 F (Xi) \u00b7 cw) for tuning n parameters, where c is a constant, k is the number of CPTs that include at least one of the parameters being varied, and F (Xi) denotes the size of the i-th CPT. Note that the tuning problem is related to the inference problem in so-called credal networks [8], where each variable is associated with sets of probability measures, rather than single values as in Bayesian networks. This problem has been proven NPPP-complete [9].\nOften, we want to select a combination of values for the\nparameters that satisfies the constraints on the output probability of interest, but has minimal impact on the other probabilities computed from the network. In other cases, we want the modification to be as small as possible. In other words, we want to find a tuning that not merely satisfies the constraints, but is also optimal, either with respect to the minimal amount of parameter change needed, or the minimal change in the joint probability distribution induced by the parameter change. Here we discuss two typical distance measures between joint probability distributions, namely those proposed by Kullback and Leibler [13], and Chan and Darwiche [3].\nThe distance measure introduced by Chan and Darwiche [3], denoted by DCD, between two joint probability distributions Prx and Prx\u2032 is defined as:\nDCD(Prx,Prx\u2032) def = ln max\n\u03c9 Prx(\u03c9) Prx\u2032(\u03c9) \u2212 ln min \u03c9 Prx(\u03c9) Prx\u2032(\u03c9)\nwhere \u03c9 is taken to range over the joint probabilities of the variables in the network. The Kullback-Leibler measure [13], denoted by DKL, is defined as:\nDKL(Prx,Prx\u2032) def = \u2211 \u03c9 Prx(\u03c9) ln Prx(\u03c9) Prx\u2032(\u03c9)\nCalculating either distance between two distributions is intractable in general. It can be proven that calculating DCD is NP-complete and that calculating DKL is PP-complete1. The Euclidean distance is a convenient way to measure the amount of change needed in x to go from Prx to Prx\u2032 . This distance, denoted by DE , is defined as:\nDE(x,x\u2032) def = \u221a \u2211 xi\u2208x,x\u2032i\u2208x\u2032 (xi \u2212 x\u2032i)2\nThe Euclidean distance depends only on the parameters that are changed and can be calculated in O(|X |)."}, {"heading": "3 Complexity theory", "text": "In the remainder, we assume that the reader is familiar with basic concepts of computational complexity theory, such as the classes P and NP, and completeness proofs. For a thorough introduction to these subjects we refer to textbooks like [10] and [16]. In addition to these basic concepts, we use the complexity class PP (Probabilistic Polynomial time). This class contains languages L accepted in polynomial time by a Probabilistic Turing Machine. Such a machine augments the more traditional non-deterministic Turing Machine with a probability distribution associated\n1These results are not yet published but will be substantiated in a forthcoming paper.\nwith each state transition, e.g. by providing the machine with a tape, randomly filled with symbols [11]. If all choice points are binary and the probability of each transition is 12 , then the majority of the computation paths accept a string s if and only if s \u2208 L. A typical problem in PP (in fact PP-complete) is the Inference problem [15, 18]: given a network B, a variable V1 in V, and a rational number 0 \u2264 q \u2264 1, determine whether Pr(V1 = v1) \u2265 q. Recall that Pr(V1, . . . , Vn) = \u220fn i=1 Pr(Vi | \u03c0(Vi)). To determine whether Pr(v1) \u2265 q, we sum over all marginal probabilities Pr(V1, . . . , Vn) that are consistent with v1. This can be done using a Probabilistic Turing Machine in polynomial time. The machine calculates the multiplication of conditional probabilities Pr(Vi | \u03c0(Vi)), i = 1, . . . , n, choosing a computation path in which each variable Vi is assigned a value according to the conditional probability Pr(Vi |\u03c0(Vi)). Each computation path corresponds to a specific joint value assignment, and the probability of arriving in a particular state corresponds with the probability of that assignment. At the end of this computation path, we accept with probability 12 + ( 1 q \u2212 1) , if the joint value assignment to V1, . . . , Vn is consistent with v1, and we accept with probability ( 12 \u2212 ) if the joint value assignment is not consistent with v1. The majority of the computation paths (i.e., 12 + ) then arrives in an accepting state if and only if Pr(v1) \u2265 q. Another concept from complexity theory that we will use in this paper is oracle access. A Turing Machine M has oracle access to languages in the class A, denoted as MA, if it can query the oracle in one state transition, i.e., in O(1). We can regard the oracle as a \u2018black box\u2019 that can answer membership queries in constant time. For example, NPPP is defined as the class of languages which are decidable in polynomial time on a non-deterministic Turing Machine with access to an oracle deciding problems in PP. Informally, computational problems related to probabilistic networks that are in NPPP typically combine some sort of selecting with probabilistic inference.\nNot all real numbers are exactly computable in finite time. Since using real numbers may obscure the true complexity of the problems under consideration, we assume that all parameter probabilities in our network are rational numbers, thus ensuring that all calculated probabilities are rational numbers as well. This is a realistic assumption, since the probabilities are normally either assessed by domain experts or estimated by a learning algorithm from data instances. For similar reasons, we assume that ln(x) is approximated within a finite precision, polynomial in the binary representation of x."}, {"heading": "4 Problem definitions", "text": "In the previous sections, we have encountered a number of computational problems related to sensitivity analysis and parameter tuning. To prove hardness results, we will first define decision problems related to these questions. Because of the formulation in terms of decision problems, all problems are in fact tuning problems."}, {"heading": "Parameter Tuning", "text": "Instance: Let B = (G,\u0393) be a Bayesian network where \u0393 is composed of rational probabilities, and let Pr be its joint probability distribution. Let X \u2286 \u0393 be a set of parameters in the network, let C denote the output variable, and c a particular value of C. Furthermore, let E denote a set of evidence variables with joint value assignment e, and let 0 \u2264 q \u2264 1. Question: Is there a combination of values x for the parameters in X such that Prx(c | e) \u2265 q?"}, {"heading": "Parameter Tuning Range", "text": "Instance: As in Parameter Tuning. Question: Are there combinations of values x and x\u2032 for the parameters in X such that Prx(c | e)\u2212 Prx\u2032(c | e) \u2265 q?"}, {"heading": "Evidence Parameter Tuning Range", "text": "Instance: As in Parameter Tuning; furthermore let e1 and e2 denote two particular joint value assignments to the set of evidence variables E. Question: Is there a combination of values x for the parameters in X such that Prx(c | e1)\u2212 Prx(c | e2) \u2265 q?"}, {"heading": "Minimal Parameter Tuning Range", "text": "Instance: As in Parameter Tuning; furthermore let r \u2208 Q+. Question: Are there combinations of values x and x\u2032 for the parameters in X such that DE(x,x\u2032) \u2264 r and such that Prx(c | e)\u2212 Prx\u2032(c | e) \u2265 q?"}, {"heading": "Minimal Change Parameter Tuning Range", "text": "Instance: As in Parameter Tuning; furthermore let s \u2208 Q+, and let D denote a distance measure for two joint probability distributions as reviewed in Section 2 . Question: Are there combinations of values x and x\u2032 for the parameters in X such that D(x,x\u2032) \u2264 s and Prx(c | e)\u2212 Prx\u2032(c | e) \u2265 q?"}, {"heading": "Mode Tuning", "text": "Instance: As in Parameter Tuning; furthermore let >(Pr(C)) denote the mode of Pr(C). Question: Are there combinations of values x and x\u2032 for the parameters in X such that >(Prx(C | e)) 6= >(Prx\u2032(C | e))?\nFurthermore, we define Evidence Mode Tuning, Minimal Parameter Mode Tuning, and Minimal Change Mode Tuning corresponding to the Parameter Tuning variants of these problems."}, {"heading": "5 Completeness results", "text": "We will construct a hardness proof for the Parameter Tuning Range problem. Hardness of the other problems can be derived with minimal changes to the proof construction. More specifically, we prove NPPPhardness of the Parameter Tuning Range-problem by a reduction from E-Majsat; this latter problem has been proven complete by Wagner [21] for the class NPPP. We will use a reduction technique, similar to the technique used by Park and Darwiche [17] to prove NPPP-hardness of the Partial Map-problem.\nWe first observe that all tuning problems from Section 4 are in NPPP: given x, x\u2032, q, r and s, we can verify all claims in polynomial time using a PP oracle, since inference is PP-complete [18]. For example, with the use of the oracle, we can verify in polynomial time whether Prx(c | e)\u2212 Prx\u2032(c | e) \u2265 q, for a given x, x\u2032, and q. Likewise, we can calculate the Euclidean distance of x and x\u2032 in polynomial time and verify that it is less than r. Determining whether a distance between two joint probability distributions is smaller than s is NP-complete (for the distance DCD defined by Chan and Darwiche [3]) or PP-complete (for the distance DKL defined by Kullback and Leibler [13]). Thus, we can non-deterministically compute an assignment to X and check (using a PP oracle) that the distance is smaller than s. Therefore, all problems are in NPPP.\nTo prove hardness, we will reduce Parameter Tuning Range from E-Majsat, defined as follows:"}, {"heading": "E-Majsat", "text": "Instance: Let \u03c6 be a Boolean formula with n variables Vi (1 \u2264 1 \u2264 n), grouped into two disjoint sets VE = V1, . . . , Vk and VM = Vk+1, . . . , Vn. Question: Is there an instantiation to VE such that for at least half of the instantiations to VM, \u03c6 is satisfied?\nWe construct a probabilistic network B\u03c6 from a given Boolean formula \u03c6 with n variables Vi and instantiation templates VE and VM. For all variables Vi, in the formula \u03c6, we create a matching stochastic variable Vi in V for the network B\u03c6, with possible values true and false with uniform distribution. These variables are roots in the network B\u03c6. We denote Xi = Pr(Vi = true) as the parameter of Vi.\nFor each logical operator in \u03c6, we create an additional stochastic variable in the network, whose parents are\nthe corresponding sub-formulas (or single variable in case of a negation operator) and whose conditional probability table is equal to the truth table of that operator. For example, the \u2227-operator would have a conditional probability Pr(\u2227 = true) = 1 if and only if both its parents have the value true, and 0 otherwise. We denote the stochastical variable that is associated with the top-level operator in \u03c6 with V\u03c6. Furthermore, we add a variable S with values true and false, with uniform probability distribution where XS = Pr(S = true) is the parameter of S. Lastly, we have an output variable C, with parents S and V\u03c6 and values true and false, whose CPT is equal to the truth table of the \u2227-operator. The set of parameters X in the Parameter Tuning Range problem now is defined to be {X1, . . . , Xk} \u222aXS , i.e., the parameters of the variables in VE and the parameter of S. We set q = 12 .\nFigure 1 shows the graphical structure of the probabilistic network constructed for the E-Majsat instance (\u03c6,VE,VM), where \u03c6 = \u00ac(V1 \u2228 V2) \u2227 \u00acV3, VE = {V1, V2}, and VM = {V3}. Note that this EMajsat instance is satisfiable with V1 = V2 = F ; for that instantiation to VE, at least half of the possible instantiations to VM will satisfy the formula.\nTheorem 1. Parameter Tuning Range is NPPPcomplete.\nProof. Membership of NPPP can be proved as follows. Given x\u2032 and x, we can verify whether Prx(c | e) \u2212 Prx\u2032(c | e) \u2265 q in polynomial time, given an oracle that decides Inference. Since Inference is PP-complete, this proves membership of NPPP.\nTo prove hardness, we construct a transformation from the E-Majsat problem. Let (\u03c6,VE,VM) be an instance of E-Majsat, and let B\u03c6 be a probabilistic network, with parameters x = {X1, . . . , Xk} \u222a XS ,\nconstructed as described above. Trivially, there exists a combination of parameter values x\u2032 such that Prx\u2032(C = true) = 0, namely all assignments in which XS = 0. In that case, at least one of the parents of C has the value false with probability 1 and thus Prx\u2032(C = true) = 0.\nOn the other hand, if x includesXS = 1, then Prx(C = true) depends on the values of X1, . . . , Xk. More in particular, there exist parameter values such that Prx(C = true) \u2265 12 , if and only if (\u03c6,VE,VM) has a solution. We can construct a solution x by assigning 1 to XS , 1 to all variables in {X1, . . . , Xk} where the corresponding variable in VE is set to true, and 0 where it is set to false. On the other hand, if (\u03c6,VE,VM) is not satisfiable, then Prx(C = true) will be less than 1 2 for any parameter setting. Due to the nature of the CPTs of the \u2018operator\u2019 variables which mimic the truth tables of the operators, Prx(C = true) = 1 for a value assignment to the parameters that is consistent with a satisfying truth assignment to \u03c6. If there does not exist a truth assignment to the variables in VE such that the majority of the truth assignments to the variables in VM satisfies \u03c6, then there cannot be a value assignment to X such that Prx(C = true) \u2265 12 . Thus, if we can decide whether there exist two sets of parameter settings x and x\u2032 such that in this network B\u03c6, Prx(C = true)\u2212Prx\u2032(C = true) \u2265 q, then we can answer (\u03c6,VE,VM) as well. This reduces E-Majsat to Tuning Parameter Range.\nNote that the constructed proof shows, that the Parameter Tuning Range problem remains NPPPcomplete, even if we restrict the set of parameters to constitute only prior probabilities, if all variables are binary, if all nodes have indegree at most 2, if the output is a singleton variable, and if there is no evidence. We will now show completeness proofs of the other problems.\nCorollary 2. All tuning problems defined in Section 4 are NPPP-complete.\nProof. We will show how the above construction can be adjusted to prove hardness for these problems.\n\u2022 Parameter Tuning: From the above construct, leave out the nodes S and C, such that x = {X1, . . . , Xk}. There is an instantiation x such that Prx(V\u03c6 = true) \u2265 12 , if and only if (\u03c6,VE,VM) has a solution.\n\u2022 Evidence Parameter Tuning Range: From the above construct, replace S with a singleton evidence variable E with values true and false and uniform distribution; denote E = true as e1 and E = false as e2 and let x = {X1, . . . , Xk}.\nPrx(C = true | E = e2) = 0 for all possible parameter settings of x. On the other hand, Prx(V\u03c6 = true) \u2265 12 and thus Prx(C = true | E = e1) \u2265 12 if and only if (\u03c6,VE,VM) has a solution.\n\u2022 Minimal Parameter Tuning Range and Minimal Change Parameter Tuning Range: These problems have Tuning Parameter Range as a special case (set r, s = \u221e) and thus hardness follows by restriction.\n\u2022 Mode Tuning: Since C has two values, Pr(C = false) = 1\u2212 Pr(C = true). In particular, >(C) = true if Pr(C = true) \u2265 12 , and >(C) = false if Pr(C = false) \u2265 12 . If XS = 0 then >(C) = false. Pr(C = true) \u2265 12 , if and only if >(C) = true.\nEvidence Mode Tuning, Minimal Parameter Mode Tuning, and Minimal Change Mode Tuning: Apply similar construct modifications as with the corresponding Parameter Tuning problems."}, {"heading": "6 Restricted problem variants", "text": "In the previous section, we have shown that in the general case, Parameter Tuning Range is NPPPcomplete. In this section, the complexity of the problem is studied for restricted classes of instances. More in particular, we will discuss tuning problems in networks with bounded topologies and tuning problems with a bounded number of CPTs containing parameters to be tuned."}, {"heading": "6.1 Bounded topologies", "text": "In this section we will show that restrictions on the topology of the network alone will not suffice to make the problem tractable. In fact, Parameter Tuning Range remains hard, even if B is a polytree. Similar results can be derived for the other problems. To prove NP-completeness of Parameter Tuning Range on polytrees, we reduce Maxsat to Parameter Tuning Range on polytrees, using a slightly modified proof from [17]. The (unweighted) Maxsat problem is defined as follows:"}, {"heading": "Maxsat", "text": "Instance: Let \u03c6 be a Boolean formula in CNF format, let C\u03c6 = C1 . . . Cm denote its clauses and V\u03c6 = V1 . . . Vn its variables, and let 1 \u2264 k \u2264 m. Question: Is there an assignment to the variables in \u03c6, such that at least k clauses are satisfied?\nWe will construct a polytree network B as follows. For each variable in the formula, we create a variable in the network with values true and false, with uniform\nprobability distribution. We denote the parameter of Vi as Xi as in the previous construct. We define a clause selector variable S0 with values c1, . . . , cm and uniform probability, i.e. Pr(S0 = ci) = 1m . Furthermore, we define clause satisfaction variables Si, with values c0, . . . , cm, associated with each variable. Every variable Si has Vi and Si\u22121 as parents. Lastly, we define a variable VS , with values true and false, with uniform probability distribution, and parameter XS , and a variable C with values true and false, parents XS and Sn. See Figure 2 for the topology of this network. The CPT for Si(i \u2265 1) and C is given in Table 1. In this table, T (Vi, j) and F (Vi, j) are Boolean predicates that evaluate to 1 if the truth assignment to Vi satisfies, respectively does not satisfy, the j-th clause.\nTheorem 3. Parameter Tuning Range remains NP-complete if B is restricted to polytrees.\nProof. Membership of NP is immediate, since we can decide Inference in polynomial time on polytrees. Given x\u2032 and x, we can thus verify whether Prx\u2032(C = c)\u2212 Prx(C = c) \u2265 q in polynomial time. To prove NP-hardness, we reduce Maxsat to Parameter Tuning Range. Let (\u03c6, k) be an instance of Maxsat. From the clauses C\u03c6 and variables V\u03c6, we construct B\u03c6 as discussed above. Similarly as in the previous proof, if XS = 0 then Pr(C = true) = 0 for any instantiation to the parameters X1 to Xn. If XS = 1 then we observe the following. For every instantiation cj to S0, the probability distribution of Si is as follows. Pr(Si = c0 | Vi) = 1 if the instantiation to V1 . . . Vi satisfies clause cj , and 0 otherwise. Pr(Si = cj | Vi) = 1 if this instantiation does not satisfy clause cj .\nPr(Si | xi) =  Si = c0, V1...i satisfies cj 1 Si = cj , V1...i satisfies cj 0 Si = c0, V1...i does not satisfy cj 0 Si = cj , V1...i does not satisfy cj 1 otherwise 0\nOf course, Pr(Si) is conditioned on Vi and thus depends on Xi. For Xi = 0 or Xi = 1, either Pr(Si = c0) = 1 or Pr(Si = cj) = 1, for intermediate values of Xi the probability mass is shared between Pr(Si = c0) and Pr(Si = cj). But then Pr(Sn = c0) is 1 for a particular clause selection cj in S0, if and only if the parameter setting to X1 to Xn satisfies that clause. Due to the conditional probability table of C and XS = 1, Prx(C = true) = 1 if and only if the parameter setting x satisfies that clause. Summing over S0 yields Prx(C = true) = kn , where k is the number of clauses that is satisfied by x. Thus, a Parameter Tuning Range query with values 0 and kn would solve the Maxsat problem. This proves NP-hardness of Parameter Tuning Range on polytrees."}, {"heading": "6.2 Bounded number of CPTs", "text": "In the previous section we have shown that a restriction on the topology of the network in itself does not suffice to make parameter tuning tractable. In this section we will show that bounding the number of CPTs containing parameters in X in itself is not sufficient either. Note that trivial solutions to the Parameter Tuning may exist for particular subsets X of the set of parameter probabilities \u0393. For example, if X constitutes all conditional probabilities Pr(C = c |\u03c0(C)), for all configurations of parents of C, then a trivial solution would set all these parameters to q. If the number of parameters in X is logarithmic in the total number of parameter probabilities, i.e., | X |\u2264 p(log | \u0393 |) for any polynomial p, then the problem is in PPP, since we can try all combinations of parameter settings to 0 or 1 in polynomial time, using a PP-oracle.\nIf both the number of CPTs containing one or more parameters in the set X is bounded by a factor k (independent of the number of total number of parameter probabilities), and the indegree of the corresponding nodes is bounded, then Parameter Tuning is PP-complete. Hardness follows immediately since Parameter Tuning has Inference as a trivial special case (for zero parameters). We will prove membership of PP for this problem for a single parameter in a root node and show that the result also holds for a k-bounded number of CPTs with m parents. Similar observations can be made for the other tuning problems defined in Section 4.\nTheorem 4. Parameter Tuning is PP-complete if the number of CPTs containing parameters and the indegree of the corresponding nodes are bounded.\nProof. First let us assume k = 1, i.e., all n parameters are taken from the CPT of a single node V . Furthermore, let us assume for now that V is a root node. To solve Parameter Tuning, we need to decide whether Pr(C = c) \u2265 q for a particular combination of values of the parameters in X. Conditioning on V gives us \u2211 i Pr(C = c | V = vi) \u00b7 Pr(V = vi).\nSince \u2211 i Pr(V = vi) = 1, Pr(C = c) is maximal for Pr(V = vi) = 1 for a particular vi. Thus, if we want to decide whether Pr(C = c) \u2265 q for a particular combination of values of the parameters, then it suffices to determine whether this is the case when we set Pr(V = vi) = 1 for a particular parameter vi.2\nUsing this observation, we will construct a Probabilistic Turing MachineM by combining several machines accepting Inference instances. At its first branching step, M either accepts with probability 12 , or runs, with probability 12n , one of n Probabilistic Turing Machines Mi(i = 1, . . . , n), which on input B\u03c6,i (with Pr(V = vi) = 1) and q accept if and only if Pr(C = c) = q. If any Mi accepts, then M accepts. The majority of computation paths of M accepts if and only if the Parameter Tuning instance is satisfiable. If V is not a root node, then we must branch over each parent configuration. For k CPTs with at most n parameters in each CPT and m incoming arcs, we need to construct a combined Probabilistic Turing Machine consisting of O(nm k\n) Probabilistic Turing Machines accepting instances of Inference. For bounded m and k, this is a polynomial number of machines and thus computation takes polynomial time. Thus, Parameter Tuning is in PP for a bounded number of CPTs containing parameters and a bounded indegree of the corresponding nodes m and k."}, {"heading": "7 Conclusion", "text": "In this paper, we have addressed the computational complexity of several variants of parameter tuning. Existing algorithms for sensitivity analysis and parameter tuning (see e.g. [2]) have a running time, exponential in both the treewidth of the graph and in the number of parameters varied. We have shown that parameter tuning is indeed hard, even if the network has a restricted polytree and if the number of parameters is bounded. We conclude, that Parameter Tuning is tractable only if both constraints are met, i.e., if probabilistic inference is easy and the number of parameters involved is bounded.\n2If the number of parameters subject to tuning does not constitute all parameter probabilities in the CPT, then we need to test whether Pr(C = c) \u2265 q when all parameters have the value 0 as well."}, {"heading": "Acknowledgments", "text": "This research has been (partly) supported by the Netherlands Organisation for Scientific Research (NWO). The authors wish to thank Hans Bodlaender and Gerard Tel for their insightful comments on earlier drafts of this paper. We wish to thank the anonymous reviewers for their thoughtful remarks."}], "references": [{"title": "Sensitivity analysis in discrete Bayesian networks", "author": ["E. Castillo", "J.M. Guti\u00e9rrez", "A.S. Hadi"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, 27:412\u2013423", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Sensitivity analysis in Bayesian networks: From single to multiple parameters", "author": ["H. Chan", "A. Darwiche"], "venue": "Twentieth Conference on Uncertainty in Artificial Intelligence, pages 67\u201375. AUAI Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "A distance measure for bounding probabilistic belief change", "author": ["H. Chan", "A. Darwiche"], "venue": "International Journal of Approximate Reasoning, 38(2):149\u2013174", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Sensitivity analysis for threshold decision making with DBNs", "author": ["Th. Charitos", "L.C. van der Gaag"], "venue": "In 22nd Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "L", "author": ["V. Coup\u00e9"], "venue": "C. van der Gaag, and J. D. F. Habbema. Sensitivity analysis: an aid for beliefnetwork quantification. Knowledge Engineering Review, 15:1\u201318", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "and L", "author": ["V.M.H. Coup\u00e9", "F.V. Jensen", "U.B. Kj\u00e6rulff"], "venue": "C. van der Gaag. A computational architecture for n-way sensitivity analysis of Bayesian networks. Technical Report 102156, Aalborg University", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Properties of sensitivity analysis of Bayesian belief networks", "author": ["V.M.H. Coup\u00e9", "L.C. van der Gaag"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "The inferential complexity of bayesian and credal networks", "author": ["C.P. de Campos", "F.G. Cozman"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Computers and Intractability", "author": ["M.R. Garey", "D.S. Johnson"], "venue": "A Guide to the Theory of NP- Completeness. W. H. Freeman and Co., San Francisco", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1979}, {"title": "Computational complexity of Probabilistic Turing Machines", "author": ["J.T. Gill"], "venue": "SIAM Journal of Computing, 6(4)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1977}, {"title": "Making sensitivity analysis computationally efficient. In Sixteenth Conference in Uncertainty in Artificial Intelligence, pages 317\u2013325", "author": ["U. Kj\u00e6rulff", "L.C. van der Gaag"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics, 22:79\u201386", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1951}, {"title": "Sensitivity analysis for probability assessments in Bayesian networks", "author": ["K.B. Laskey"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, 25:901\u2013 909", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Stochastic boolean satisfiability", "author": ["M.L. Littman", "S.M. Majercik", "T. Pitassi"], "venue": "Journal of Automated Reasoning, 27(3):251\u2013296", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison-Wesley", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Complexity results and approximation settings for MAP explanations", "author": ["J.D. Park", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research, 21:101\u2013133", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence, 82(1-2):273\u2013302", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Analysing sensitivity data", "author": ["L.C. van der Gaag", "S. Renooij"], "venue": "In Seventeenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Sensitivity analysis of probabilistic networks", "author": ["L.C. van der Gaag", "S. Renooij", "V.M.H. Coup\u00e9"], "venue": "Advances in Probabilistic Graphical Models, Studies in Fuzziness and Soft Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "The complexity of combinatorial problems with succinct input representation", "author": ["K.W. Wagner"], "venue": "Acta Informatica, 23:325\u2013356", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 12, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 6, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 1, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 17, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 3, "context": "The sensitivity of the output of a probabilistic network to small changes in the network\u2019s parameters, has been studied by various researchers [1, 14, 7, 2, 19, 4].", "startOffset": 143, "endOffset": 163}, {"referenceID": 5, "context": "The results are used also upon engineering a probabilistic network, for example to distinguish between parameters which allow some imprecision and parameters which should be determined as accurately as possible [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "Another use is for carefully tuning the parameter probabilities of a network to arrive at some desired model behavior [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "While the majority of these insights and methods pertain to a one-way sensitivity analysis in which the effect of varying a single parameter probability on a single output probability or output value is studied, recently there also has been some pioneering work on extending these insights to higher-order analyses [2, 6].", "startOffset": 315, "endOffset": 321}, {"referenceID": 5, "context": "While the majority of these insights and methods pertain to a one-way sensitivity analysis in which the effect of varying a single parameter probability on a single output probability or output value is studied, recently there also has been some pioneering work on extending these insights to higher-order analyses [2, 6].", "startOffset": 315, "endOffset": 321}, {"referenceID": 18, "context": "The parameter under consideration is systematically varied from 0 to 1 and the other parameters from the same CPT are co-varied such that their mutual proportional relationship is kept constant [20].", "startOffset": 194, "endOffset": 198}, {"referenceID": 6, "context": "Under the condition of covariation, the sensitivity function f(X) is a quotient of two linear functions [7] and takes the form", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "A one-way sensitivity analysis can be extended to measure the effect of the simultaneous variation of two parameters on the output [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "This can further be generalized to nway sensitivity analyses [6, 2] where multiple parameters are varied simultaneously.", "startOffset": 61, "endOffset": 67}, {"referenceID": 1, "context": "This can further be generalized to nway sensitivity analyses [6, 2] where multiple parameters are varied simultaneously.", "startOffset": 61, "endOffset": 67}, {"referenceID": 18, "context": "While higher-order analyses can reveal synergistic effects of variation, the results are often difficult to interpret [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "The simplest method for this purpose is to compute, from the network, the probability of interest for up to three values for the parameter under study; using the functional form of the function to be established, a system of linear equations is obtained, which is subsequently solved [7].", "startOffset": 284, "endOffset": 287}, {"referenceID": 10, "context": "A more efficient method determines the required constants by propagating information through a junction tree, similar to the standard junction-tree propagation algorithm [12].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "There are a number of algorithms to determine the solution space for a set of parameters given such constraints [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "This problem has been proven NP-complete [9].", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "Here we discuss two typical distance measures between joint probability distributions, namely those proposed by Kullback and Leibler [13], and Chan and Darwiche [3].", "startOffset": 133, "endOffset": 137}, {"referenceID": 2, "context": "Here we discuss two typical distance measures between joint probability distributions, namely those proposed by Kullback and Leibler [13], and Chan and Darwiche [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "The distance measure introduced by Chan and Darwiche [3], denoted by DCD, between two joint probability distributions Prx and Prx\u2032 is defined as:", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "The Kullback-Leibler measure [13], denoted by DKL, is defined as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "For a thorough introduction to these subjects we refer to textbooks like [10] and [16].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "For a thorough introduction to these subjects we refer to textbooks like [10] and [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "by providing the machine with a tape, randomly filled with symbols [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "A typical problem in PP (in fact PP-complete) is the Inference problem [15, 18]: given a network B, a variable V1 in V, and a rational number 0 \u2264 q \u2264 1, determine whether Pr(V1 = v1) \u2265 q.", "startOffset": 71, "endOffset": 79}, {"referenceID": 16, "context": "A typical problem in PP (in fact PP-complete) is the Inference problem [15, 18]: given a network B, a variable V1 in V, and a rational number 0 \u2264 q \u2264 1, determine whether Pr(V1 = v1) \u2265 q.", "startOffset": 71, "endOffset": 79}, {"referenceID": 19, "context": "More specifically, we prove NPhardness of the Parameter Tuning Range-problem by a reduction from E-Majsat; this latter problem has been proven complete by Wagner [21] for the class NP.", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "We will use a reduction technique, similar to the technique used by Park and Darwiche [17] to prove NP-hardness of the Partial Map-problem.", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "We first observe that all tuning problems from Section 4 are in NP: given x, x\u2032, q, r and s, we can verify all claims in polynomial time using a PP oracle, since inference is PP-complete [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 2, "context": "Determining whether a distance between two joint probability distributions is smaller than s is NP-complete (for the distance DCD defined by Chan and Darwiche [3]) or PP-complete (for the distance DKL defined by Kullback and Leibler [13]).", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "Determining whether a distance between two joint probability distributions is smaller than s is NP-complete (for the distance DCD defined by Chan and Darwiche [3]) or PP-complete (for the distance DKL defined by Kullback and Leibler [13]).", "startOffset": 233, "endOffset": 237}, {"referenceID": 15, "context": "To prove NP-completeness of Parameter Tuning Range on polytrees, we reduce Maxsat to Parameter Tuning Range on polytrees, using a slightly modified proof from [17].", "startOffset": 159, "endOffset": 163}, {"referenceID": 1, "context": "[2]) have a running time, exponential in both the treewidth of the graph and in the number of parameters varied.", "startOffset": 0, "endOffset": 3}], "year": 2008, "abstractText": "While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity of these problems has not been established as yet. In this paper we study several variants of the tuning problem and show that these problems are NP-complete in general. We further show that the problems remain NP-complete or PP-complete, for a number of restricted variants. These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general, practicable methods.", "creator": "TeX"}}}