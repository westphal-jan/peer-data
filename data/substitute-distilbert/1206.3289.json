{"id": "1206.3289", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Efficient inference in persistent Dynamic Bayesian Networks", "abstract": "numerous temporal inference tasks such as fault monitoring and trouble detection exhibit a persistence property : for example, and something breaks, it stays stationary until an intervention. when modeled as a dynamic bayesian network, persistence gains dependencies between adjacent time slices, initially making exact inference called time intractable using geometric inference algorithms. however, we show that persistence implies a regular structure that april be exploited for efficient inference. we present three successively more general classes of models : persistent causal chains ( pccs ), persistent trace trees ( pcts ) and persistent polytrees ( ppts ), and the corresponding exact inference algorithms that exploit persistence. we show that analytic asymptotic bounds for our implementation map favorably to junction causal inference ; and us demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate boyen - stewart method on simultaneously generated instances of boundary tree models. we continuously show how to handle weak - persistent variables and how persistence they be exploited effectively for memory filtering.", "histories": [["v1", "Wed, 13 Jun 2012 15:46:24 GMT  (237kb)", "http://arxiv.org/abs/1206.3289v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomas singliar", "denver dash"], "accepted": false, "id": "1206.3289"}, "pdf": {"name": "1206.3289.pdf", "metadata": {"source": "CRF", "title": "Efficient inference in persistent Dynamic Bayesian Networks", "authors": ["Tom\u00e1\u0161 \u0160ingliar"], "emails": [], "sections": [{"heading": null, "text": "Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property: for example, if something breaks, it stays broken until an intervention. When modeled as a Dynamic Bayesian Network, persistence adds dependencies between adjacent time slices, often making exact inference over time intractable using standard inference algorithms. However, we show that persistence implies a regular structure that can be exploited for efficient inference. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs) and persistent polytrees (PPTs), and the corresponding exact inference algorithms that exploit persistence. We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference; and we demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate Boyen-Koller method on randomly generated instances of persistent tree models. We also show how to handle non-persistent variables and how persistence can be exploited effectively for approximate filtering."}, {"heading": "1 Introduction", "text": "Persistence is a common trait of many real-world systems. It is used to model permanent changes in state, such as when components of a system that have broken until someone intervenes to fix them. Especially interesting and useful are diagnostic models where misalignments and other process drifts may cause a cascade of other failures, all of which may also persist until the root cause is fixed. Even when such changes are not truly permanent, they are often reversed slowly\nrelative to the time scale of the model, and persistence can be a good approximation in such systems. For instance, vehicular accidents cause obstructions on the road that last much longer than the required detection time and are thus persistent for the purpose of detection [20]. Another example is outbreak detection [4], where an infected population stays infected much longer than the desired detection time. There are many other examples of persistence and approximate persistence.\nDynamic Bayesian Networks (DBNs) [5] are a general formalism for modeling temporal systems under uncertainty. Many standard time-series methods are special cases of DBNs, including Hidden Markov Models [18] and Kalman filters [7]. Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14]. Unfortunately, modeling persistence with DBNs requires the introduction of many inter-temporal arcs, often making exact inference intractable with standard inference algorithms.\nIn this paper, we define Persistent Causal DBNs (PCDBNs), a particular class of DBN models capable of modeling many real-world systems that involve long chains of causal influence coupled with persistence of causal effects. We show that a linear time algorithm exists for inference (smoothing) in linear chain and tree-based PC-DBNs. We then generalize our results to polytree causal networks, where the algorithm remains exact, and to general networks, where it inherits properties of loopy belief propagation [21]. Our method relies on a transformation of the original prototype network, allowing smoothing to be done efficiently; however, this method does not readily deal with the incremental filtering problem. Nonetheless, we show empirically that, if evidence is observed at every time slice, approximate filtering can be accomplished with fixed window smoothing, producing lower error than approximate Boyen-Koller (BK) filtering [1]\nusing a fraction of the computation time.\nThe algorithm that we present exploits a particular type of determinism that is given by the persistence relation. There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19]. These more general methods have not been tailored to the important special cases of DBNs and persistency. To our knowledge, this is the first work to investigate persistency in DBNs.\nThe paper is organized as follows: In Section 2 we introduce the changepoint transformation. Section 3 introduces persistent causal chain DBNs and the corresponding inference algorithm, which retains all the essential properties of later models. Then, Section 4 will discuss the steps leading to a fully general algorithm. Experimental results are presented in Section 5, followed by conclusions."}, {"heading": "2 Notation and changepoints", "text": "Consider a Bayesian network (BN) with N binary variables Xi; we will refer to this network as the prototype. The corresponding Dynamic BN with M slices is created by replicating the prototype M times and connecting some of the variables to their copies in the next slice. In our notation, upper indices range over time slices of the DBN; lower indices range over variables in each time slice. Colon notation is used to denote sets and sequences. Thus, for instance, X1:M4 denotes the entire temporal sequence of values of X4 from time 1 to time M . Variables without an upper index will refer to their respective counterparts in the prototype. We say that a variable Xk is persistent if P (Xtk = 1|Xt\u22121k , U t) = { P (Xk|U) if Xt\u22121k = 0\n1 if Xt\u22121k = 1 ,\n(1) where U = Pa(Xk) refers to the parents of Xk in the prototype. In other words, 1 is an absorbing state. Sometimes [12] a variable is called persistent if it has an arc to the next-slice copy of itself. Our definition of persistence is strictly stronger, but no confusion should arise in this paper.\nThere are 2M temporal sequences of values of a binary variableXk. If the variable is persistent, the number of configurations is reduced to M+1. Information about X1:Mk can be summarized by looking at the time when X changed from 0 to 1 (we sometimes refer to the 0 state as the off state and 1 as the on state). Thus, inference in the persistent DBN with binary variables is equivalent to inference in a network whose topology closely resembles that of the prototype and whose\nvariables are M+1-ary discrete changepoint variables, with correspondingly defined conditional probability distributions (CPDs), as shown in Figure 1b. The models in Figure 1a and 1b are identical; one can go back and forth between them by recognizing that\n(X\u0303 = j) \u21d4 (Xj = 0) \u2227 (Xj+1 = 1) and (Xj = 0) \u21d4 (X\u0303 > j).\nIf the prototype is a tree, belief propagation in the transformed network yields an algorithm whose complexity is O(M2N). The quadratic part of the computation comes from summing over the M + 1 values of the single parent for each of the M + 1 values of the child. Similarly, if the prototype is a polytree, complexity will be proportional to MUmax+1, where Umax is the largest in-degree in the network. This transformation by itself, when all hidden state variables are persistent, allows us to perform smoothing much more efficiently than by operating on the original DBN. There is, however, additional structure in the CPDs that allows us to do better by a factor of M , and we can also adapt our algorithm to deal with the case when some hidden variables are not persistent."}, {"heading": "3 PCC-DBN inference", "text": "To simplify the exposition, let us now focus on a specific prototype, a persistent causal chain DBN (PCCDBN). This is a chain with Pa(Xi) = {Xi\u22121}, i = 1, ..., N and Pa(O) = XN (thus it has N+1 nodes). Let us further assume that the leaves are nonpersistent and observed, while the causes (X nodes) are all persistent and hidden. The network is shown in Figure 1a and its transformed version in Figure 1b.\nConsider the problem of computing P (O). This is in general one of the most difficult inference problems, requiring one to integrate out all hidden state variables, and is implicit in most inference queries:\nP(O1:M ) = \u2211\nX1:M1:N\nP(O1:M | X1:M1:N ) \u00b7 P(X1:M1:N ) (2)\nLet {jk : 0 \u2264 jk \u2264 M} index the sequence of X1:Mk in which variable Xjkk is the last (highest-time) variable to be in the off state, unless jk = 0 in which case it indexes the sequence in which all Xk are in the on state. As an example, if M = 3, then jk = {0, 1, 2, 3} indexes the statesX1:Mk = {111, 011, 001, 000}, respectively, for all k. All configurations not indexed by ji have zero probability due to the persistence assumption. To simplify notation, we use jk to denote the event that X1:Mk is the sequence indexed by jk. We also say that Xk fired at jk. We can decompose Equa-\ntion 2 according to the network structure as follows:\nP (O1:M ) = M\u2211\nj1=0\nP(j1) M\u2211\nj2=0\nP(j2 | j1) . . .\n. . .\nM\u2211\njN=0\nP(jN | jN\u22121) \u00b7 P(O1:M | jN ) (3)\nDenote by Pk the probability that variable Xk will fire for the first time given that its causal parent has fired, and by P\u0302k the probability that Xk will fire for the first time given that its causal parent has not fired:\nPk \u2261 P(Xjk = 1 | Xj\u22121k = 0, Xjk\u22121 = 1) P\u0302k \u2261 P(Xjk = 1 | Xj\u22121k = 0, Xjk\u22121 = 0).\nLet Pk and P\u0302k denote the complements 1 \u2212 Pk and 1 \u2212 P\u0302k, respectively. We can define \u03a3Lk recursively to denote the partial sum over jk from Equation 3, conditioned on jk\u22121 = L:\n\u03a3Lk \u2261 \u2211\njk\nP(jk | jk\u22121 = L) \u00b7 \u03a3jkk+1 (4)\nwith boundary condition \u03a3LN+1 \u2261 P(O1:M | jN = L). Using this notation, Equation 3 can be rewritten as:\nP (O1:M ) = M\u2211\nj1=0\nP(j1) \u00b7 \u03a3j12 (5)\nNow we now need to show that one can calculate the entire set \u03a30:M2:N in time O(MN). Each \u03a3 L k can be written as follows:\n\u03a3Lk = \u03c3\u0304 L k + \u03c3 L k + \u03c3\u0302 L k , (6)\nwhere \u03c3\u0304Lk contains all the terms in the sum such that Xk first fires when Xk\u22121 has not fired:\n\u03c3\u0304Lk = \u2211\njk<L\nP\u0302 jk k P\u0302k \u00b7 \u03a3jkk+1. (7)\n\u03c3Lk contains all the terms in which Xk first fires when Xk\u22121 has also fired:\n\u03c3Lk = \u2211\nL\u2264jk<M P\u0302L\nk P jk\u2212L k Pk \u00b7 \u03a3jkk+1, (8)\nand \u03c3\u0302Lk contains the final term in which Xk never fires:\n\u03c3\u0302Lk = P\u0302 L k PM\u2212L k \u00b7 \u03a3Mk+1. (9)\nIn order to calculate Equation 5 in time O(MN), we need to pre-compute \u03c3\u0304Lk , \u03c3 L k and \u03c3\u0302 L k for all values of L in O(M) for each variable Xk."}, {"heading": "3.1 Upward Recursion Relations", "text": "As a boundary condition for the recursion, assume we have calculated \u03a3kN+1 for all 0 \u2264 k \u2264 M . We show how to do this in time O(M) in Section 3.2. Also, this algorithm requires the pre-calculation and caching of P\u0302 i\nk for 0 \u2264 k \u2264 N and 0 \u2264 i \u2264 M , which can be done\nrecursively in O(MN) time and space.\nInspecting Equation 7 more closely, it should be easy to see that one can calculate \u03c3\u0304iN for 0 \u2264 i \u2264 M in O(M) time using the following recursion:\n\u03c3\u0304i+1l = \u03c3\u0304 i l + P\u0302 i l \u00b7 P\u0302l \u00b7 \u03a3ik+1, (10)\nwith boundary condition \u03c3\u03040l = 0 for all l. One can also calculate \u03c3ik for 0 \u2264 i \u2264M with the recursion:\n\u03c3i\u22121k = \u03c3ik\nP\u0302k Pk + P\u0302\ni\u22121 k \u00b7 Pk \u00b7 \u03a3i\u22121k+1, (11)\nwith boundary condition \u03c3Mk = 0 for all l. Finally, one can calculate \u03c3\u0302iN for 0 \u2264 i \u2264M with the recursion:\n\u03c3\u0302i\u22121k = \u03c3\u0302ik\nP\u0302k Pk (12)\nwith boundary condition \u03c3\u0302Mk = P\u0302 M k \u00b7 \u03a3Mk+1 for all l.\nOnce \u03c3\u0304iN , \u03c3 i N and \u03c3\u0302 i N are calculated, one can calculate all \u03a3iN for 0 \u2264 i \u2264 M in O(M) time using Equations 10, 11, 12 and 6. After \u03a30:MN is calculated, we can use Equation 4 to obtain \u03a30:MN\u22121 in time O(M), and repeat N times to get all values of \u03a30:M1:N . Thus the entire calculation takes O(MN) time."}, {"heading": "3.2 Computing \u03a3iN+1", "text": "To finalize the proof, we have to show how to calculate \u03a3iN+1 (the probability of the observations for a given configuration i of X1:MN ) for all 0 \u2264 i \u2264 M in time O(M). Recall that \u03a3iN+1 \u2261 P(O1:M | jN = i). Since the parent of each Oj is given, for each i, this calculation is simply the product of the observations:\nP(O1:M | jN = i) = M\u220f\nk=1\nP(Ok | XkN , jN = i) (13)\nUsing our existing notation, we define\n\u03c6`N+1 = P(O k | XkN = 1), (14) \u03c6\u0304`N+1 = P(O k | XkN = 0), (15)\n\u03a3iN+1 can be calculated for all 0 \u2264 i \u2264 M in time O(M) via the recursion relation:\n\u03a30N+1 = M\u220f\n`=1\n\u03c6`N+1 and \u03a3 `+1 N+1 = \u03a3 ` N+1 \u00b7 \u03c6\u0304`N+1 \u03c6`N+1 .\n(16) Note that this formulation puts no distributional assumption on P (O|XN ). The leaves can be distributed as multinomials, Gaussians etc, as is often done with Hidden Markov models [18] when they are put to their many uses."}, {"heading": "3.3 Downward Recurrences", "text": "The above discussion completes the description of the \u201c\u03bb-pass\u201d of PCC-DBN algorithm. Similar reasoning can be applied to obtain the \u201c\u03c0-pass\u201d recurrences that we now give without full derivation. Analogously to \u03a3, the semantics of \u03a8jk is p(Xk = j|O+k ), where O+k is the subset of evidence reachable from Xk through its parent1. \u03a8jk is again a sum of three components:\n\u03a8jk = \u03c8 j k + \u03c8\u0304 j k + \u03c8\u0302 j k (17)\n\u03c8\u0304 accounts for the terms where the parent has not yet changed:\n\u03c8\u0304`\u22121k = \u03c8\u0304 ` k \u00b7 1 P k + P\u0302 `\u22121 k \u00b7 P\u0302k \u00b7\u03a8`k\u22121 (18)\nwith initialization \u03c8\u0304Mk = 0 for all k.\n\u03c8 accounts for the terms where the parent has already changed:\n\u03c8`+1k = \u03c8 ` k \u00b7 Pk + P\u0302 `+1k \u00b7 Pk \u00b7\u03a8 `+1 k\u22121 (19)\n1We only have evidence in the bottom layer in PCCDBNs, but this will come handy in the next section.\nwith boundary condition \u03c80k = Pk\u03a8 0 k\u22121 for all k. Also, since Xk eventually changes in this scenario, \u03c8Mk = 0.\n\u03c8\u0302 accounts for the terms where the node never changes:\n\u03c8\u0302Mk = \u2211\n0\u2264i\u2264M P\u0302 i k \u00b7 PM\u2212i k \u00b7\u03a8ik\u22121. (20)\nBecause the upper index refers to the changepoint of Xk, only \u03c8\u0302Mk is non-zero. We can just compute this in O(M) without the need for recurrences.\nInitialization of the \u03a8-recurrences happens at the root(s) of the network. For any root r, \u03a80r = P\u0302k and recurrently \u03a8i+1r = \u03a8ir \u00b7 P\u0302r. Finally, \u03a8Mr = \u03a8M\u22121r P\u0302r/P\u0302k."}, {"heading": "3.4 PCC-DBN and belief propagation", "text": "We have just defined PCC-DBN, a version of belief propagation that first collects the evidence by passing the \u03bb-messages towards the root of the chain and the proceeds to distribute information towards the leaves via the \u03c0-messages. After propagation is complete, we can obtain any posterior as\np(X\u0303k|O) = \u03a3k+1 \u00b7\u03a8k. (21)\nIt is now useful to recall the types of potentials involved in Pearl\u2019s algorithm [8] and how they relate to the quantities above. For each node X, there are local potentials \u03c0X(x) \u2261def p(X = x|e+X) and \u03bbX(x) \u2261def p(e\u2212X |X = x), where e+X and e\u2212X denote respectively the evidence reachable through parents and the evidence reachable from X \u201cdownwards\u201d, X included. There are two types of messages in Pearl\u2019s algorithm: \u03c0X\u2192Yi sent by X to its children and \u03bbX\u2192Ui sent to its parents. A closer look at PCC-DBN reveals that each \u03a3k is identical to \u03bbXk\u2192Xk\u22121 \u2014 the message from Xk to its single parent Xk\u22121. The local potential \u03bbXk(jk) is identical to \u03a3k+1, because there are no children other than Xk+1 and evidence is only observed at the bottom of the chain. \u03a8k corresponds directly to \u03c0Xk(jk). This is why Equation 21 works."}, {"heading": "3.5 Simple Generalizations and Causal Trees", "text": "While PCC-DBNs are useful for demonstrating the general ideas of handling the probability distributions arising from the changepoint transformation, they form a rather restricted class of networks, and the inference query that we performed was also restricted. Here we state succinctly a set of simple alterations which allow this algorithm to be relaxed in various ways:\nGeneral evidence patterns We can have observations anywhere in the network, in any time slice.\nCasting the inference as belief propagation gives the answer to any probabilistic query as Equation 21 with one caveat: An observation such as X33 = 1 does not tell us with certainty the position of the changepoint, but it just provides evidence that j3 < 3, thus we cannot simply set the changepoint variable to state 3. Rather, the potentials corresponding to such evidence must be multiplied onto the messages as prescribed by the belief propagation algorithm (see Equation 22).\nNon-stationarity Stationarity of conditional probability distributions was used to simplify the formulae in the previous exposition, but is not required. All that is needed is to keep running products of respective probabilities instead of the powers in the exponents of Pk, P\u0302k. They need to be computed incrementally and tabulated to avoid hidden linear terms in the computation.\nExtension to trees The extension of PCC-DBN to causal trees (PCT-DBNs) is now fairly straightforward. Because each node Xk can now have multiple children Ch(Xk), we must replace \u03a3k in all recurrences with the true \u03bb-potential for Xk:\n\u03bbXk = \u03bbXk\u2192Xk \u00b7 \u220f\ni\u2208Ch(Xk) \u03a3i, (22)\nwhere \u03bbXk\u2192Xk accounts for evidence observed in Xk\u2019s temporal chain. The vector \u03bbXk\u2192Xk is zero where the evidence rules out a changepoint \u2014 before the time t of the last observed Xtk = 0 and after the time s of first observed Xsk = 1. Everywhere else, \u03bbXk\u2192Xk(jk) = 1. Note that \u03bbX(x) potential can be obtained in O(M) time per node.\nIn computation of \u03c8 potentials, \u03a8k on the right-hand side of the recurrences is replaced by the \u03c0Pa(Xk)\u2192Xk , which in turn include the influence of evidence under Xk\u2019s siblings:\n\u03c0Pa(Xk)\u2192Xk = \u03c0Xk \u00b7 \u220f\ni\u2208Ch(Pa(Xk))\\Xk \u03a3i. (23)\nAgain, this preserves the O(M) per-node complexity. Thus, PCT-DBN is linear in both N and M ."}, {"heading": "4 Further Generalizations", "text": "In this section we describe three more important generalizations of PC-DBNs: polytrees, non-persistent nodes, and finally an approximate algorithm for general DAGs. These relaxations are more involved than those of Section 3.5 and thus require more elaboration."}, {"heading": "4.1 Polytrees", "text": "Belief propagation [16, 17] is a powerful framework for exact inference in polytree networks. Polytrees, unlike\ntrees, allow multiple parents of a node, but remain acyclic in the undirected sense. In polytree changepoint networks, structure in the conditional probability table P (X = x|U) can be exploited to save a multiplicative factor of M + 1 just as we showed for tree networks. The \u03c8-recurrences run over the first parent variable, while the remaining parents are summed over by brute force. Similarly, the \u03c3-recurrences run over the parent that the message is addressed to. For instance, the definition of \u03c3 will be replaced by\n\u03c3ik \u221d \u2211\nL\u2264jk<M\n[ L\u220f\nz=1\nP\u0302 (z)\nk\n] \u00b7 [ jk\u220f\nz=L+1\nP (z)\nk\n] P\n(jk+1) k \u00b7 \u03a3jkk+1\n(24) and Equation 11 by\n\u03c3i\u22121k = \u03c3 i k \u00b7\nP (i)\nk\nP\u0302 (i)\nk\n+ [ i\u22121\u220f z=1 P\u0302 (z) k ] \u00b7 P (i)k \u00b7 \u03a3i\u22121k+1\n\u03c3Mk = 0, (25)\nwhere, assuming we are sending to the first parent,\nP (z) k = P (Xk = 1|U1 = 1, I{U2: \u2264 z}) P\u0302\n(z) k = P (Xk = 1|U1 = 0, I{U2: \u2264 z}) (26)\nare now functions of the joint configuration of the remaining parents U2:. The proportionality constant in Equation 24 equals the product of the remaining parents\u2019 \u03c0-messages. We call this PPT-DBN, the persistent polytree algorithm.\nThe worst-case time complexity of PPT-DBN is dominated by the cost associated with the largest familyclique: O((M + 1)Umax). The 2TBN algorithm [12] suffers a worst-case time complexity O(22NM), as all nodes in two slices may be entangled [9] in the clique to connect the two subsequent time-slices, even though the prototype network is a polytree [12, section 3.6.2]. Therefore, we expect PPT-DBN will be comparatively better for shorter temporal chains of larger networks. However, PPT-DBN really shines on space complexity. At most O(MN) memory is consumed, compared to 2TBN, where the potentials in the joint tree can grow as large as O(22N ). Later we show experimentally how dramatic the difference can be."}, {"heading": "4.2 Non-persistent nodes", "text": "While it is convenient to assume that all non-leaf variables are persistent, it does limit the modeling power at our disposal. We now show how an occasional nonpersistent variable in the network can also be handled in polynomial time. We assume the non-persistent variable is isolated, that is, all of its neighbors are persistent. We make this assumption in order to avoid having to invoke an embedded general DBN inference\nalgorithm such as 2TBN to handle connected nonpersistent variables. It can be done, but quickly becomes complex and inelegant. A simple way to handle connected non-persistent nodes is to combine them into a single joint node. Obviously, this solution causes exponential growth in the state space of the joined nodes, making it somewhat unappealing. A two-slice approach made aware of the determinism, e.g. by use of ADD compilation [3, 2], could very well work better for networks with only a few persistent variables."}, {"heading": "4.2.1 A simple example", "text": "To illustrate how an isolated non-persistent node would be handled, assume first a simple structure such as in Figure 2. Then we can efficiently compute P (Y\u0303 = j) by moving the sums inward:\nP (Y\u0303 = j) = \u2211 X\nP (X)P (Y\u0303 = j|X) = \u2211\nX1,...XM\n[ M\u220f\nk=1\nP (Xk|Xk\u22121) ] [\nM\u220f k=1 \u03c6kj\n] =\n\u2211 X1 P (X1)\u03c61j\n\u2211\nX2\nP (X2|X1)\u03c62j . . . \u2211\nXM P (XM |XM\u22121)\u03c6Mj \ufe38 \ufe37\ufe37 \ufe38\n\u03baM\ufe38 \ufe37\ufe37 \ufe38 \u03ba2\nThis gives rise to the recurrence\n\u03baMi (j) = \u2211\nv\nP (XM = v|XM\u22121 = i) \u00b7 \u03c6Mj (27)\n\u03baki (j) = \u2211\nv\nP (Xk = v|Xk\u22121 = i) \u00b7 \u03c6kj \u00b7 \u03bak+1v ,\nwith \u03c6kj defined appropriately:\n\u03c6kj (X k) =    P (Y 1 = 0|X1) if k = 1 P (Y k = 0|Y k\u22121 = 0, Xk) if 1 < k \u2264 j P (Y k = 1|Y k\u22121 = 0, Xk) if k = j + 1\n1 if k > j + 1\nNow, P (Y\u0303 = j) = \u03ba01. Moreover, a short analysis will reveal that\n\u03baki (j) = \u03ba k+1 i (j + 1) \u22001 \u2264 k < j < M \u2212 1.\nTherefore, we do not need to compute \u03ba for every j, but compute \u03baki (M) for all k as a special case and then \u03baki (M \u2212 1) for all k to start the recursion. All other values can be read off \u03baki (M \u2212 1) with the appropriate indexing shift. Thus, we can obtain the entire distribution P (Y\u0303 ) in O(M) time! Allowing non-persistent variables to take on multiple values is also straightforward: we only need to allow the bottom index in \u03baki to range over the domain of Xk."}, {"heading": "4.2.2 The general case", "text": "Pearl\u2019s belief propagation has been generalized to the clique tree propagation algorithm [21]. With belief propagation (BP), the cliques correspond to edges of the original polytree and the separators consist of single nodes. In the process of message passing, the variables not in the separator are summed out of the clique potentials.\nThe PCT-DBN algorithm used the \u201cnatural\u201d cliques induced by the transformed network. Assume we have a situation such as in Figure 3. Because the variable C is not persistent, the size of the induced separator C is 2M . However, we can work conceptually with a larger clique BCD. Message propagation then calls for summing out all C1:M , which we can do without actually instantiating the clique potential using a recurrence derived much like that of Equation 27. In the interest of space, we only show here the simple \u03ba recurrence. The full recurrence calls for summing over all persistent variables in the clique and the resulting complexity is O((M + 1)B), where B is the number of persistent neighbors of the non-persistent variable."}, {"heading": "5 Experimental evaluation", "text": "We implemented our algorithms in Matlab and compare them to the exact and approximate algorithms as implemented in the Bayesian Network Toolbox (BNT) [11]. Namely, we will compare to the Boyen-Koller (BK) algorithm [1] in its 1) exact and 2) fully factored setting. Although BK reduces in its exact form to the incremental junction-tree algorithm, we found it was faster in practice than the 2TBN implementation.\nTherefore the 2TBN algorithm is not included in the evaluation.\nMatlab run-time is not the ideal measure of algorithm complexity as it is arguably more sensititve to the quality of implementation compared to other languages. However, we should note that we did not make any special effort to optimize our code for Matlab, and the BNT library is a widely used and mature code base, so we expect any advantages due to code quality to fall to the competing approaches. Our Matlab code and further evaluation results can be downloaded at http://www.cs.pitt.edu/~tomas/papers/UAI08."}, {"heading": "5.1 Speed of the tree algorithm", "text": "To compare inference speed, a network with the structure of a full binary tree with N nodes was generated. Among theMN possible observations, 10% of the variables were set to a random value (subject to persistence constraints so that P (E) 6= 0). We measured the time to execute the query p(X\u03031|E)\u2014the posterior probability over the root node\u2014for each algorithm2. This process was repeated 100 times for each M , N combination and the respective times added up. The results are graphed out in Figures 4 and 5.\nPCT-DBN outperforms both the exact incremental joint tree algorithm and the approximate BK algorithm (assuming independence) by several orders of magnitude as N , the size of a slice grows (Figure 4). In fact, the exact algorithm soon runs out of memory (around N = 20) and only the approximate version keeps up. Exact PCT-DBN inference also performs consistently about 100 times faster than exact junction-tree and approximate BK inference when we look at scale-up with the number of slices, as shown in\n2The actual query is in fact irrelevant as all algorithms compute all posterior marginals simultaneously.\nFigure 5."}, {"heading": "5.2 Speed of the polytree algorithm", "text": "The asymptotic time complexity of PPT-DBN, as M increases, may be less favorable than that of the incremental approaches. However, its lower memory complexity is very favorable, as documented by the following experiment. We generated a network where most non-root nodes have exactly 2 parents and measured the time for the three inference algorithms. Quadratic scale-up with M is expected for PPT-DBN in such a network.\nFigure 6 shows the exact PPT-DBN algorithm to be several times faster than, but scaling very similarly to, the approximate fully factorized Boyen-Koller algorithm with an M = 20 time slice inference window. Peeking ahead into Figure 7 suggests the time performance would be about identical at M = 70 time slices. The junction-tree algorithm does not scale beyond 20 nodes due to memory usage.\nFigure 7 shows clearly that asymptotically, PPT-DBN\nscales with a steeper slope than both BK inference and junction-tree inference. Indeed, for about N = 70, BK eventually surpasses PPT-DBN in terms of speed. However, it remains faster than junction-tree incremental inference throughout the range. On a computer with 1 GB RAM, the exact version begins to hit memory limits around M = 200 and N = 17.\nWe conclude that if exact inference is desired for persistent polytree causal networks, using the PPT-DBN algorithm is a better choice for a wide range of inference window lengths. Furthermore, if approximate inference is acceptable, we show in Section 5.3 that for large enough N , fixed window smoothing using PPTDBN can outperform BK inference in terms of RMS error, while still performing many times faster. For the special case of persistent causal trees, the new algorithm dominates by orders of magnitude in all ranges that we tested versus both junction tree and BK assuming intra-slice independence."}, {"heading": "5.3 Fixed-window approximation", "text": "A minor disadvantage of the PPT-DBN algorithm is that it cannot do online inference yet. Therefore, when monitoring a process, M grows and so does the computation time. In practice, only a fixed number of most recent observations are usually considered with older observations falling out of the \u201cwindow\u201d. Thus we evaluate if reasonable precision can be attained with small window sizes, where PPT-DBN dominates.\nFigure 8 shows, for several time slices t, the root mean square error of computed posterior marginals\nErrtBK =\n\u221a\u221a\u221a\u221a N\u2211\ni=1\n[PBK(Xti |O1:t)\u2212 Pex(Xti |O1:t)]2\nincurred by the fully factored Boyen-Koller method and the same error for PPT-DBN which ignores all\nevidence older than W , for different values of W . We use a binary tree prototype with all leaf variables nonpersistent and observed. All non-leaf variables are persistent and hidden. The CPT probabilities are sampled uniformly at random. The observed evidence O is obtained by forward-sampling the DBN and restricting it to the observables. We find that the error of our algorithm falls with growing W as expected. The results become even more favorable for PC-DBN as N, the number of nodes per slice, grows (see also further results online). The error made by fixing the inference window tends to be lower than that of the Boyen-Koller approximation for reasonable values of W and we can eliminate the unfavorable dependence on M at a small price of accuracy. One clear drawback to a naive implementation of the fixed-window approach is that if evidence is not observed at each time slice, in the presence of persistence a piece of crucial evidence might drop off the window preventing the model from \u201cremembering\u201d that a persistent state was already acheived. This glitch could in principle be fixed by caching when persistent variables have turned on."}, {"heading": "6 Conclusions and future work", "text": "We presented an algorithm for PC-DBNs, a way to exploit the special structure of the DBN probability distribution when many variables are persistent. Unlike forward-backward approaches to DBN inference that work slice-to-slice, we collapse the entire temporal progression and perform inference in the original prototype network structure. For trees, the algorithm is many times faster than state-of-the-art general-purpose exact and approximate DBN inference algorithms, while having a space complexity of only\nO(MN). This continues to hold even in the polytree generalization with inference window lengths into the hundreds. While this method does not directly yield an incremental filtering algorithm, we show that a fixed-window smoothing version of PC-DBN inference can perform approximate filtering faster and with comparable or less error than BK-filtering.\nAlthough we have not presented a filtering algorithm that can exploit persistence, we do believe that one is possible. The number of possible joint configurations of variables in two subsequent slices is 3N with the persistence assumption as opposed to 4N in the general network. This hints at the possibility of a 2TBN-like algorithm leveraging persistence and still remaining linear in the number of time slices.\nAnother possible direction for this work is to allow multi resolution temporal modeling by modeling systems on very short time scales, but utilizing a persistence approximation for the slow processes. In such cases, a model with a single time-scale could efficiently and accurately deal with systems that have both fast and slow processes.\nAlso interesting is the vision of approximate inference algorithms not requiring persistence, but simply assuming that the hidden state changes at most once in the period of interest. If the change in the hidden state is relatively slow, this could be a fairly accurate approximation. Such problems are often found in bioinformatics areas such as phylogeny discovery, where time of a mutation is of interest [6]."}], "references": [{"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In Proceedings of UAI-98,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Compiling Bayesian networks with local stucture", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of IJCAI 2005,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Compiling Bayesian networks using variable elimination", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of IJCAI 2007,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Bayesian biosurveillance of disease outbreaks", "author": ["Gregory Cooper", "Denver Dash", "John Levander", "Weng- Keen Wong", "William Hogan", "Michael Wagner"], "venue": "In Proceedings of UAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A model for reasoning about persistence and causation", "author": ["Thomas Dean", "Keiji Kanazawa"], "venue": "Computational Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Continuous time Markov networks", "author": ["Tal El-Hay", "Nir Friedman", "Daphne Koller", "Raz Kupferman"], "venue": "In Proceedings of UAI-06. AUAI Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "A new approach to linear filtering and prediction problems. Transactions of the ASME", "author": ["R.E. Kalman"], "venue": "Journal of Basic Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "A computational model for combined causal and diagnostic reasoning in inference systems", "author": ["Jin H. Kim", "Judea Pearl"], "venue": "In Proceedings IJCAI-83 (Karlsruhe,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Bayesian inference in the presence of determinism", "author": ["David Larkin", "Rina Dechter"], "venue": "In Proceedings of Workshop on AI and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "The Bayes Net Toolbox for Matlab", "author": ["Kevin Murphy"], "venue": "Computing Science and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Dynamic Bayesian Networks: Representation, Inference and Learning", "author": ["Kevin Murphy"], "venue": "PhD thesis, EECS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "The factored frontier algorithm for approximate inference in DBNs", "author": ["Kevin Murphy", "Yair Weiss"], "venue": "In Proceedings of 12 NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Survey of Bayesian models for modelling of stochastic temporal processes", "author": ["Brenda M. Ng"], "venue": "Technical Report UCRL-TR-225272, Lawrence Livermore National Laboratory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Exploiting Locality in Probabilistic Inference", "author": ["Mark Andrew Paskin"], "venue": "PhD thesis, EECS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Fusion, propagation, and structuring in belief networks", "author": ["Judea Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Fusion and propagation with multiple observations in belief networks", "author": ["Mark A. Peot", "Ross D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference", "author": ["Scott Sanner", "David McAllester"], "venue": "In Proceedings of IJCAI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Learning to detect adverse traffic events from noisily labeled data", "author": ["Tom\u00e1\u0161 \u0160ingliar", "Milo\u0161 Hauskrecht"], "venue": "In Proceedings of PKDD", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Understanding belief propagation and its generalizations", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "In Exploring Artificial Intelligence in the New Millennium,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "For instance, vehicular accidents cause obstructions on the road that last much longer than the required detection time and are thus persistent for the purpose of detection [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 3, "context": "Another example is outbreak detection [4], where an infected population stays infected much longer than the desired detection time.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Dynamic Bayesian Networks (DBNs) [5] are a general formalism for modeling temporal systems under uncertainty.", "startOffset": 33, "endOffset": 36}, {"referenceID": 16, "context": "Many standard time-series methods are special cases of DBNs, including Hidden Markov Models [18] and Kalman filters [7].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "Many standard time-series methods are special cases of DBNs, including Hidden Markov Models [18] and Kalman filters [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 11, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 13, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 12, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 19, "context": "We then generalize our results to polytree causal networks, where the algorithm remains exact, and to general networks, where it inherits properties of loopy belief propagation [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "Nonetheless, we show empirically that, if evidence is observed at every time slice, approximate filtering can be accomplished with fixed window smoothing, producing lower error than approximate Boyen-Koller (BK) filtering [1]", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 199, "endOffset": 203}, {"referenceID": 2, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 263, "endOffset": 270}, {"referenceID": 17, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 263, "endOffset": 270}, {"referenceID": 10, "context": "Sometimes [12] a variable is called persistent if it has an arc to the next-slice copy of itself.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "The leaves can be distributed as multinomials, Gaussians etc, as is often done with Hidden Markov models [18] when they are put to their many uses.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "It is now useful to recall the types of potentials involved in Pearl\u2019s algorithm [8] and how they relate to the quantities above.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "Belief propagation [16, 17] is a powerful framework for exact inference in polytree networks.", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "Belief propagation [16, 17] is a powerful framework for exact inference in polytree networks.", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "The 2TBN algorithm [12] suffers a worst-case time complexity O(2M), as all nodes in two slices may be entangled [9] in the clique to connect the two subsequent time-slices, even though the prototype network is a polytree [12, section 3.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "by use of ADD compilation [3, 2], could very well work better for networks with only a few persistent variables.", "startOffset": 26, "endOffset": 32}, {"referenceID": 1, "context": "by use of ADD compilation [3, 2], could very well work better for networks with only a few persistent variables.", "startOffset": 26, "endOffset": 32}, {"referenceID": 19, "context": "Pearl\u2019s belief propagation has been generalized to the clique tree propagation algorithm [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "We implemented our algorithms in Matlab and compare them to the exact and approximate algorithms as implemented in the Bayesian Network Toolbox (BNT) [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "Namely, we will compare to the Boyen-Koller (BK) algorithm [1] in its 1) exact and 2) fully factored setting.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "Such problems are often found in bioinformatics areas such as phylogeny discovery, where time of a mutation is of interest [6].", "startOffset": 123, "endOffset": 126}], "year": 2008, "abstractText": "Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property: for example, if something breaks, it stays broken until an intervention. When modeled as a Dynamic Bayesian Network, persistence adds dependencies between adjacent time slices, often making exact inference over time intractable using standard inference algorithms. However, we show that persistence implies a regular structure that can be exploited for efficient inference. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs) and persistent polytrees (PPTs), and the corresponding exact inference algorithms that exploit persistence. We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference; and we demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate Boyen-Koller method on randomly generated instances of persistent tree models. We also show how to handle non-persistent variables and how persistence can be exploited effectively for approximate filtering.", "creator": " TeX output 2008.05.14:2033"}}}