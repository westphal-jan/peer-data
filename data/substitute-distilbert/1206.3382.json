{"id": "1206.3382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2012", "title": "Simple Regret Optimization in Online Planning for Markov Decision Processes", "abstract": "we consider online planning in markov decision processes. an algorithm for this problem should explore the set of possible policies from the known state, \u03be, by interrupted, recommend an action to follow based on the outcome of the exploration. the performance of such an algorithm is classified in terms past its simple regret, \u03b4 is the loss in performance resulting from choosing the recommended action instead of an optimal one, and / or in terms of recognition that the recommended action is not an optimal one. the best guarantees provided by the state - of - the - art algorithms for computing of these measures over resources are only polynomial. we introduce a new example, brue, that achieves over time exponential by relating these two measures. the algorithm is based on a simple yet non - standard state - space sampling scheme in which different samples are dedicated near different problems. unfortunately preliminary empirical guidance shows that brue not only provides maximal performance guarantees, but is also very effective in filtering and indeed compares to state - amongst - the - art.", "histories": [["v1", "Fri, 15 Jun 2012 07:23:28 GMT  (141kb,D)", "http://arxiv.org/abs/1206.3382v1", null], ["v2", "Wed, 19 Dec 2012 08:48:44 GMT  (361kb,D)", "http://arxiv.org/abs/1206.3382v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["zohar feldman", "carmel domshlak"], "accepted": false, "id": "1206.3382"}, "pdf": {"name": "1206.3382.pdf", "metadata": {"source": "CRF", "title": "Online Planning in MDPs: Rationality and Optimization", "authors": ["Zohar Feldman"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In the past couple of decades, Markov decision processes (MDPs) have become a standard framework for planning under uncertainty. An MDP \u3008S,A, Tr,R\u3009 is defined by a set of states S, a set of actions A, a stochastic transition function Tr : S \u00d7 A \u00d7 S \u2192 [0, 1], and a reward function\nar X\niv :1\n20 6.\n33 82\nv1 [\ncs .A\nR : S \u00d7 A\u00d7 S \u2192 R. The current state of the agent is fully observable, and the objective of the agent is to act so to maximize its accumulated reward. In the finite horizon setting that will be used for most of the paper, the reward is accumulated along H steps.\nThe desire to attack problems of increasing complexity has led researches to consider online planning in MDPs. In online planning, the decision process of the agent is focused on the next action to perform, rather than on computing a quality policy for the entire MDP. The agent is given a generative model which allows for simulated execution of all possible sequences of actions, from any state of the MDP. The decision process consists of a simulation-based planning, terminated either according to a predefined schedule or due to an external interrupt, and followed by a recommendation of action to perform at the current state. Once that action is applied in the real environment, it modifies the environment and the decision process is repeated from the new state to select the next action and so on.\nA prominent approach to online planning in MDPs, successfully used also in non-deterministic and imperfect information games, is Monte-Carlo (MC) planning. Numerous MC planning algorithms have been proposed in the literature. The sparse sampling algorithm by Kearns, Mansour, and Ng [10] offered near-optimal action selection in time exponential in H but independent of the state space size. Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19]. The quality of the action a recommended for state s with H steps-to-go is assessed in terms of the probability that a is sub-optimal, and in terms of the (closely related) simple regret \u2206H [s, a]. The latter is the performance loss resulting from taking a and then following an optimal policy \u03c0\u2217 for the remaining H \u2212 1 steps, instead of following \u03c0\u2217 from the beginning. That is,\n\u2206H [s, a] = QH(s, \u03c0 \u2217(s,H))\u2212QH(s, a),\nwhere QH(s, a) = Es\u2032 [ R(s, a, s\u2032) +QH\u22121(s \u2032, \u03c0\u2217(s\u2032, H \u2212 1)) ] .\nWhile empirical attractiveness of alternative MC planning algorithms depends on specifics of the problem in hand, only some of these algorithms provide formal guarantees on their expected performance improvement over time. In particular, none of the MC planning algorithms in use these days breaks the barrier of the worst-case polynomial reduction of error probability, and thus they at best guarantee polynomial simple regret over time.\nThis is precisely the contribution of this paper: We introduce an MC planning algorithm for MDPs, BRUE, that achieves over time exponential reduction of both the error probability and simple regret. The key in this simple to implement and computationally efficient algorithm is decoupling between certain contradicting objectives that should be addressed by the sampling process for online MDP planning. Our preliminary empirical evaluation on a standard benchmark for comparison between MC planning algorithms shows that BRUE not only provides superior performance guarantees, but is also very effective in practice."}, {"heading": "2 MONTE-CARLO PLANNING", "text": "A general scheme for Monte-Carlo planning, MCT, that gives rise to various specific algorithms for online MDP planning, is depicted in Figure 1. Starting with the current state s0, MCT performs an iterative construction of a tree rooted at s0. At each iteration, MCT issues a probe from s0, expands the tree based on the outcome of the probe, and updates information stored at the nodes of the tree. Once the simulation phase is over, MCT issues a recommendation of action to perform in s0, and this is based on the information collected at the nodes of the tree. For compatibility of the notation with prior literature, in what follows we refer to the tree nodes via the states associated with these nodes. Note that, due to the Markovian nature of MDPs, it is unreasonable to distinguish between nodes associated with the same state at the same depth. Hence, the actual graph constructed by most instances of MCT forms a DAG over nodes (s, h) \u2208 S \u00d7 {0, 1, . . . ,H}. By A(s) \u2286 A in what follows we refer to the subset of actions applicable in state s.\nNumerous concrete instances of MCT have been proposed, with UCT [12]\nprobably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11]. To give a concrete sense of MCT\u2019s components, as well as to ground some intuitions discussed later on, below we describe the specific setting of MCT corresponding to the core UCT algorithm:\n\u2022 probe-S: The probes \u03c1 = \u3008s0, a1, s1, . . . , ak, sk\u3009 are all issued from the root node s0. The probe ends either when reached a sink state, that is, A(sk) = \u2205, or when k = H. Each node/action pair (s, a) is associated with a counter n(s, a) and a value accumulator Q\u0302(s, a); both n(s, a) and Q\u0302(s, a) are initialized to 0, and then updated by the update-statistics procedure. Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a \u2208 A(si), then\nai+1 = argmax a\n[ Q\u0302(si, a) + c \u221a log n(si)\nn(si, a)\n] , (1)\nwhere n(s) = \u2211\na n(s, a). Otherwise, ai+1 is selected uniformly at random from actions a \u2208 A(si) with n(si, a) = 0. In both cases, si+1 is then sampled according to the conditional probability P(S|si, ai+1), induced by the transition function Tr.\n\u2022 expand-tree: Each probe \u03c1 = \u3008s0, a1, s1, . . . , ak, sk\u3009 induces a state trace \u3008s0, s1, . . . , si\u3009 inside T , as well as a state trace \u3008si+1, . . . , sk\u3009 outside of T . In principle, T can be expanded with any prefix of \u3008si+1, . . . , sk\u3009; a practically popular choice appears to be expanding T with only the upper-most node si+1. (If T is constructed as a DAG, it is expanded with the first node along \u03c1 that leaves T .)\n\u2022 update-statistics: For each node si along \u03c1 that is now part of the expanded tree T , the counter n(si, ai+1) is incremented and the estimated Q-value is updated as\nQ\u0302(si, ai+1)\u2190 Q\u0302(si, ai+1) + Ri \u2212 Q\u0302(si, ai+1) n(si, ai+1) , (2)\nwhere Ri = \u2211k\u22121 j=i R(sj , aj+1, sj+1).\n\u2022 recommend-action: Interestingly, the action recommendation protocol of UCT was never properly specified, and different applications of UCT\nadopt different decision rules, including maximization of the estimated Q-value, of the augmented estimated Q-value as in Eq. 1, of the number of times the action was selected during the simulation, as well as randomized protocols based on the information collected at the root.\nThe key property of MCT-based algorithms is that their exploration of the search space is obtained by considering a hierarchy of forecasters, each minimizing its own cumulative regret. Each such pseudo-agent forecaster corresponds to a state/steps-to-go pair (s, h). In that respect, according to Theorem 6 of Kocsis and Szepesva\u0301ri [12], UCT achieves the optimal logarithmic cumulative regret. However, the cumulative regret does not seem to be the right way to base MC planning on, and this is because the rewards \u201ccollected\u201d at the simulation phase are fictitious. In contrast, the same Theorem 6 of Kocsis and Szepesva\u0301ri [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time. Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9]. As we show later on, some of these attempts were actually very successful empirically. However, to the best of our knowledge, none of them breaks the barrier for the formal performance guarantees provided by UCT.\n3 THE BRUE ALGORITHM\nThe work of Bubeck et al. [4] was probably the first systematic attempt to analyze pure exploration in MABs, showing that the minimal simple regret in MAB can increase as the bound on the cumulative regret is getting smaller. The analysis of Bubeck et al. led us to suspect that the major deficiency of the current MC planning algorithms is in their very reliance on the MCT scheme, and to look for alternative search schemes that would better suit simple regret optimization in MDP planning. As a result, here we introduce a novel scheme for MC planning in MDPs, MCTer (see Figure 2), and then describe a concrete instance of this scheme that (1) guarantees that the probability of recommending non-optimal arm convergences to zero at exponential rate, and (2) achieves over time exponential rate of the simple regret reduction.1\n1Note that we are not claiming that no instance of MCT can achieve such guarantees, but only that so far no such instance has been discovered. This leaves a very interesting\nSimilarly to MCT, MCTer iteratively constructs a tree rooted at the current state s0, starting each iteration with issuing a probe from s0 and expanding the tree based on the outcome \u03c1 of the probe. The probes \u03c1 are issued to varying depth in a round-robin fashion, from the full depth of H to just a basic lookahead of 1. Importantly, instead of updating the statistics stored at the nodes based on \u03c1 and proceeding with the next iteration as in MCT, a new probe \u03c1 is issued from the end state of \u03c1, and the information at the tree nodes is updated only after this \u201ccomplementary probe\u201d \u03c1. Importantly, the two type of probes can be generated according to different strategies, dubbed in Figure 2 as probe-S and probe-R.\nThe intuition behind what we try to achieve with the non-standard flow of MCTer is as follows. As we already mentioned, the origin of the standard MCT scheme, as well as of its various instantiations, is in online optimization in the context of various MAB problems. Considering each state/steps-to-go pair (s, h) as a pseudo-agent, the sole task of the root pseudo-agent (s0, H) is to minimize its own simple regret in a stochastic MAB setting induced by the applicable actions A(s0). Therefore, if an oracle would provide (s0, H) with an optimal action \u03c0\u2217(s0, H), then no further planning will be needed until after the execution of \u03c0\u2217(s0, H). However, the task characteristics of (s0, H) is an exception rather than a rule. Suppose that an oracle provides us with optimal actions for all pseudo-agents (s, h) but (s0, H). Despite the richness of this information, (s0, H) in some sense remains as clueless as it was before. To choose between the alternatives A(s0), (s0, H) needs at least\nquestion for future work.\nrelative information about the expected value of these alternatives. Hence, each non-root pseudo-agent (s, h) is devoted to two tasks: (1) identifying an optimal action \u03c0\u2217(s, h), and (2) estimating the actual value of that action, because this information is needed by the predecessor(s) of (s, h) in T .\nThis is the key point that MCTer aims at targeting differently from MCT. While both schemes incrementally collect information about the search space by sampling it, they differ in the roles that different samples play in that process. In MCT, each probe \u03c1 issued at (s, h) is a priori devoted both to increasing the confidence in that some current candidate a\u2020 for \u03c0\u2217(s, h) is indeed \u03c0\u2217(s, h), as well as to improving the estimate of Qh(s, a\n\u2020), as if assuming that \u03c0\u2217(s, h) = a\u2020. Such an overloading of the probes is unavoidable in the \u201clearning while acting\u201d setup of reinforcement learning (RL) where agents should naturally care about their cumulative performance. However, while the second objective of (s, h) in online planning does prescribe (s, h) to act as to maximally exploit its current knowledge, this similarity to RL is somewhat misleading as RL-style exploitation per se is irrelevant in MC planning.\nIn principle, nothing requires our sampling mechanism to balance between the two contradicting objectives at the level of individual probes, the way instances of MCT are forced to do. In MCTer, the two roles are fulfilled by different sets of probes: the probes issued by probe-S aim at exploration while the probes issued by probe-R aim at improving the value estimates for the current candidates for \u03c0\u2217. In particular, this separation allows us to introduce a specific MCTer instance, BRUE,2 that is tailored to simple regret minimization. Inspired by the positive results of Bubeck et al. [4] for MABs, the BRUE setting of MCTer is as follows:\n\u2022 probe-S: The probes \u03c1 = \u3008s0, a1, . . . , sh\u22121, ah, sh\u3009 are all issued from the root node s0, with actions along the probe being selected at random according to uniform distribution. Each node/action pair (s, a) is associated with n(s, a) and Q\u0302(s, a) as in UCT; while counters n(s, a) are initialized to 0, value accumulators Q\u0302(s, a) are schematically initialized to \u2212\u221e.\n\u2022 expand-tree: T is expanded with the suffix of state sequence s1, . . . , sh\u22121 that is new to T .\n\u2022 probe-R: In the complementary probe \u03c1 issued at node sh, \u03c1 = \u3008sh = s1, a2, s2, . . . , ak, sk\u3009, each action ai is sampled uniformly at random, but only among the actions a \u2208 A(si\u22121) that maximize Q\u0302(si, a).\n2Short for Best Recommendation with Uniform Exploration.\n\u2022 update-statistics: The complementary probe \u03c1 as above is used to update statistics on the state/action pair (sh\u22121, ah) according to Eq. 2, but now with Ri = \u2211k\u22121 j=1 R(sj , aj+1, sj+1). Note that the information\nobtained by \u03c1 is not pushed further up the probe \u03c1. While that may appear wasteful and even counterintuitive, this locality of update is required to satisfy the formal guarantees of BRUE discussed later on.\n\u2022 recommend-action: The action recommended by BRUE is chosen uniformly at random among the actions a maximizing Q\u0302(s0, a).\nFor the sake of simplicity, in our formal analysis we assume uniqueness of the optimal policy \u03c0\u2217; that is, at each state s and each number h of stepsto-go, there is a single optimal action, and it is \u03c0\u2217(s, h). Likewise, let \u03c0Bn be the (possibly stochastic) policy induced by the value accumulators Q\u0302 after n iterations of BRUE: denoting by Tn the graph obtained by BRUE after n iterations, and by Q\u0302h(s, a) the accumulated value Q\u0302(s, a) for s at depth H \u2212 h, for all state/steps-to-go pairs (s, h) \u2208 Tn, \u03c0Bn (s, h) is a randomized strategy, uniformly choosing among actions a maximizing Q\u0302h(s, a). We also use some auxiliary notation:\nK = maxs\u2208S |A(s)|, i.e., the maximal number of actions per state.\np = mins,a,s\u2032:Tr(s,a,s\u2032)>0 Tr(s, a, s \u2032), i.e., the likelihood of the least likely\n(but still possible) outcome of an action in our problem.\nd = mins,a \u22061[s, a], i.e., the smallest difference between the value of the optimal and a second-best action at a state with just one step-to-go.\nTheorem 1 Let BRUE be called on a state s0 of an MDP \u3008S,A, Tr,R\u3009 with rewards in [0, 1] and finite horizon H. For any 1 \u2264 h \u2264 H, there exist parameters ch, c \u2032 h, c \u2032\u2032 h <\u221e, dependent of p, d and h, but independent of s0, |S|, and the number of BRUE iterations n, such that, for each state s reachable from s0 in H \u2212 h steps, for all n \u2265 c\u2032\u2032h, we have\n\u2022 Simple Regret E\u2206h[s, \u03c0Bn (s, h)] \u2264 che\u2212c \u2032 hn, (3)\n\u2022 Error Probability\nP { \u03c0Bn (s, h) 6= \u03c0\u2217(s, h) } \u2264 ch\nh e\u2212c \u2032 hn. (4)\nCorollary 1 The simple regret of the action \u03c0Bn (s0, H), recommended by BRUE after n > c1 iterations, is bounded by c2 \u00b7 exp (\u2212n \u00b7 c3) , where ci = fi(p, d,H) <\u221e.\nIn rest of this section we prove Theorem 1. To shorten the equations, when considering in what follows a state s with h steps-to-go, by a\u2217 we denote the optimal action \u03c0\u2217(s, h). When considering in that context an action a \u2208 A(s), by Q we denote Qh(s, a), and by Q\u0302 and n(a) we denote the respective value accumulator Q\u0302h(s, a) and counter n (s, a); Q \u2217 and Q\u0302\u2217 denote Qh(s, a \u2217) and Q\u0302h(s, a\n\u2217), respectively. Likewise, by ph we denote the minimal probability of reaching a (reachable this way) state with still h steps-to-go on a uniform sample from s0.\nThe proof is by induction on h. Recall that the recommended action \u03c0Bn (s, h) is an action maximizing Q\u0302h(s, a). Starting with proving the induction basis for h = 1, note that the probability of choosing an action a 6= a\u2217 is bounded by\nP { Q\u0302 \u2265 Q\u0302\u2217 } \u2264 P { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a]\n2\n} + P { Q\u0302 \u2265 Q+ \u22061[s, a]\n2\n} (5)\nThe analysis of the two summation terms in Eq. 5 is identical, and thus we detail it here only for the first term.\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a]\n2\n} \u2264\nP {n(a\u2217) \u2264 n0}+ P { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a]\n2 , n(a\u2217) > n0 } (6) To bound the first term of this summation, we note that, for each search node (s, h) and each action a \u2208 A(s), n(a) is a sum of n independent Bernouli trials with success probability ph \u2265 1K ( p K )\nH\u2212h, and E [n(a)] \u2265 nph. By choosing n0 = n p1 2 and employing Chernoff-Hoefdding bound, we obtain\nP {n(a\u2217) \u2264 n0} = P { n(a\u2217)\nn \u2264 E [n(a\n\u2217)] n \u2212 ( E [n(a\u2217)] n \u2212 n0 n )}\n\u2264 exp \u22122n(p(H\u22121) 2KH )2 = exp(\u2212np21 2 ) .\n(7)\nThe second term in Eq. 6 can be bounded as: P { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a]\n2 , n(a\u2217) > n0 } =\nn\u2211 t=n0+1 P { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a] 2 , n(a\u2217) = t }\n\u2264 n\u2211\nt=n0+1\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u22061[s, a]\n2 \u2223\u2223\u2223\u2223 n(a\u2217) = t}P {n(a\u2217) = t} =\nn\u2211 t=n0+1 P { Q\u0302\u2217 \u2264 E [ Q\u0302\u2217 ] \u2212 \u22061[s, a] 2 \u2223\u2223\u2223\u2223 n(a\u2217) = t}P {n(a\u2217) = t} since E [ Q\u0302\u2217 ] = Q\u2217\n\u2264 n\u2211\nt=n0+1\nexp ( \u22122t\u22061[s, a] 2\n4\n) P {n(a\u2217) = t}\nby Chernoff-Hoefdding bound\n\u2264 e\u22122n0 \u22061[s,a]\n2\n4\nn\u2211 t=n0+1 P {n(a\u2217) = t}\n\u2264 e\u2212n d2p1 4 . (8) Putting together Eqs. 5-8, we obtain P { Q\u0302 \u2265 Q\u0302\u2217 } \u2264 4 exp ( \u2212nd\n2p21 4\n) ,\nand thus, for all n \u2265 1,\nE\u22061[s, \u03c0Bn (s, 1)] \u2264 \u2211 a6=a\u2217 \u22061[s, a] \u00b7 P { Q\u0302 \u2265 Q\u0302\u2217 } \u2264 4Ke\u2212n d2p21 4 (9)\nand\nP { \u03c0Bn (s, 1) 6= \u03c0\u2217(s, 1) } \u2264 \u2211 a6=a\u2217 P { Q\u0302 \u2265 Q\u0302\u2217 } \u2264 4Ke\u2212n d2p21 4 (10)\nNow, assuming the claim holds for h \u2265 1, we prove it for h + 1. The probability of choosing a sub-optimal action a 6= a\u2217 when still h + 1 stepsto-go is bounded similarly to Eq. 5, with \u22061[s, a] replaced by \u2206h+1[s, a]. Similarly, we obtain\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2\n} \u2264\nP {n(a\u2217) \u2264 n0}+ P { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2 , n(a\u2217) > n0\n} (11)\nand\nP {n(a\u2217) \u2264 n0} \u2264 exp ( \u2212n\np2h+1 2\n) . (12)\nHowever, bounding the second term of summation in Eq. 11 differs from this for h = 1 in Eq. 8 for two reasons.\n(F1) For h = 1, Q\u0302 is an unbiased estimator of Q, that is, EQ\u0302 = Q. In contrast, the estimates inside the tree (at nodes with h > 1) are biased. This bias stems from Q\u0302 possibly being based on numerous sub-optimal choices in the sub-tree rooted in (s, h).\n(F2) For h = 1, the summands accumulated by Q\u0302 are independent. This is not so for h > 1 since in this case the accumulated reward depends on the selection of actions in subsequent nodes, which in turn depends on previous rewards.\nWe now show that these deficiencies of h > 1 can still be overcome. In what follows, we first tackle the issues F1-F2 by a different bounding of the quantity\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2 \u2223\u2223\u2223\u2223 n(a\u2217) = t} , and then use the outcome of this analysis to bound the second summand\nof Eq. 11.\nLemma 1 (Expected accumulated rewards) Considering the policy \u03c0Bn induced by n iterations3 of BRUE on an MDP \u3008S,A, Tr,R\u3009, for each state s \u2208 S, each action a \u2208 A(s), and each number of steps-to-go h \u2208 {1, . . . ,H}, let X = R (s, a, s1) + \u2211h\u22121 i=1 R ( si, \u03c0 B n (si, h\u2212 i), si+1 ) be a random variable corresponding to the reward accumulated by taking a at s, and then following \u03c0Bn for the rest h\u2212 1 steps. Then \u03b4n,h(s, a) , Q(s, a)\u2212 E [X] is\n\u03b4n,h(s, a) = h\u22121\u2211 i=1 E\u2206h\u2212i[si, \u03c0Bn (si, h\u2212 i)]. (13)\nProof: For any state/steps-to-go pair (s, h) \u2208 S \u00d7 {1, . . . ,H}, we have EB,s\u2032 [ R ( s, \u03c0Bn (s, h), s \u2032)] = EB [ Q(s, \u03c0Bn (s, h)) ] \u2212 EB,s\u2032 [ Q(s\u2032, \u03c0\u2217(s\u2032, h\u2212 1)) ] . (14)\n3Iteration = full run of the most-inner loop of BRUE.\nUsing that, we obtain a telescopic series that yields\nE [X] = EB,s1:sh [ R (s, a, s1) + h\u22121\u2211 i=1 R ( si, \u03c0 B n (si, h\u2212 i), si+1 )] = Q(s, a)\u2212 Es1 [Q(s1, \u03c0\u2217(s1, h\u2212 1))] +\nh\u22121\u2211 i=1 EB,s1:si [ Q(si, \u03c0 B n (si, h\u2212 i)) ] \u2212\nh\u22121\u2211 i=1 EB,s1:si+1 [Q(si+1, \u03c0 \u2217(si+1, h\u2212 i\u2212 1))]\n= Q(s, a)\u2212 h\u22121\u2211 i=1 EB,s1:si [ \u2206h\u2212i[si, \u03c0 B n (s, h\u2212 i)] ] .\n(15)\nIn what follows, we refer to the supremum of \u03b4n,h(s, a) from Lemma 1 as \u03b4n,h , maxs,a{\u03b4n,h(s, a)}.\nWith Lemma 1 in hand, let {Xt}n(a \u2217) t=1 be the summands of the value accumulator Q\u0302\u2217 = Q\u0302h+1(s, a \u2217). Approaching (F1) first, we exploit the fact that the entire bias in our estimate of Q\u2217 is due to the erroneous recommendations of \u03c0B at the successors of s, and the damage of these erroneous recommendations is fully captured by the simple regret of using \u03c0B at the immediate successors of s. When bounding Q\u2217 \u2212 EXt, we note that this difference is largest when each Xt is sampled at iteration t, that is, each Xt is acquired at the earliest possible (and thus least informed) stage of the algorithm. Hence, Q\u2217 \u2212 EXt \u2264 \u03b4t,h+1, and this allows us to bound the influence of the bias as follows.\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2 \u2223\u2223\u2223\u2223 n(a\u2217) = t} = P { Q\u0302\u2217 \u2264 EQ\u0302\u2217 \u2212 ( \u2206h+1[s, a] 2 \u2212 ( Q\u2217 \u2212 EQ\u0302\u2217\n)) \u2223\u2223\u2223\u2223 n(a\u2217) = t} \u2264 P { Q\u0302\u2217 \u2264 EQ\u0302\u2217 \u2212 ( \u2206h+1[s, a]\n2 \u2212 1 t t\u2211 \u03c4=1 \u03b4\u03c4,h+1 ) \u2223\u2223\u2223\u2223\u2223 n(a\u2217) = t }\nsince Q\u2217 \u2212 EQ\u0302\u2217 = 1 t t\u2211 \u03c4=1 (Q\u2217 \u2212 EX\u03c4 ) \u2264 1 t t\u2211 \u03c4=1 \u03b4\u03c4,h+1\n(16)\nWe now proceed with (F2), bounding the influence of the Q\u0302\u2217 summands dependency. At high level, we achieve that by modifying the ChernoffHoefdding bound for independent random variables to certain sequences of dependent random variables {Yt} such that Yt|Yt\u22121, . . . , Y1 concentrates around its expectation with a probability that approaches 1 at rate exponential in t.\nWe begin with establishing some auxiliary notions and properties. Let At be the event in which Xt is sampled along the optimal actions in each of the h subsequent nodes4. We note that\nP {\u00acAt} \u2264 h\u2211 i=1 P { \u03c0Bt (si, i) 6= \u03c0\u2217(si, i) } . (17)\nDenoting cp,h = \u2211h i=1 ci i and c\u03b4,h = \u2211h i=1 ci, from (stemming from induction hypothesis) Eq. 10, monotonic decrease of c\u2032i in i, and monotonic increase of c\u2032\u2032i in i, for all t \u2265 c\u2032\u2032h we have\nP {\u00acAt} \u2264 h\u2211 i=1 ci i e\u2212c \u2032 it \u2264 cp,he\u2212c \u2032 ht, (18)\nand\n\u03b4t,h+1 \u2264 h\u2211 i=1 cie \u2212c\u2032it \u2264 c\u03b4,he\u2212c \u2032 ht. (19)\nNow, for all \u03c9 \u2208 At, E [Xt | X1, . . . , Xt\u22121] (\u03c9) = Q\u2217, and thus [Q\u2217 \u2212Xt | X1, . . . , Xt\u22121] (\u03c9) is a random variable with expectation 0, and support [Q\u2217 \u2212 (h+ 1), Q\u2217]. Hence, for any \u03bb \u2265 0,\nE [ e\u03bb(EXt\u2212Xt) \u2223\u2223\u2223 X1, . . . , Xt\u22121] = e\u03bb(EXt\u2212Q \u2217)E [ e\u03bb(Q \u2217\u2212Xt) \u2223\u2223\u2223 X1, . . . , Xt\u22121]\n\u2264 e\u2212\u03bb\u03b4t,h+1E [ e\u03bb(Q \u2217\u2212Xt) \u2223\u2223\u2223 X1, . . . , Xt\u22121]\n\u2264 e\u2212\u03bb\u03b4t,h+1+ \u03bb2(h+1)2 8 (20)\nusing Fact 1 below.\n4It is easy to see that At \u2208 \u03c3 (X1, . . . , Xt\u22121)\nFact 1 Let Z be a random variable satisfying a \u2264 Z \u2264 b and E [Z] = 0. Then, E [exp(\u03bbZ)] \u2264 exp( (b\u2212a) 2\u03bb2\n8 ) for any \u03bb \u2208 R.\nProceeding now with developing Eq. 16, let \u03b1 = \u2206h+1[s,a] 2 \u2212 1 t \u2211t \u03c4=1 \u03b4\u03c4,h.\nIf \u03b1 > 0, then for all \u03bb \u2265 0, from Markov inequality it holds that (16) = P { Q\u0302\u2217 \u2264 EQ\u0302\u2217 \u2212 \u03b1 \u2223\u2223\u2223 n(a\u2217) = t} = P { tQ\u0302\u2217 \u2264 tEQ\u0302\u2217 \u2212 t\u03b1\n\u2223\u2223\u2223 n(a\u2217) = t} \u2264 e\u2212\u03bbt\u03b1E [ e\u03bb \u2211t i=1(EXi\u2212Xi) ] . (21)\nFocusing on the second multiplicand in Eq. 21, E [ e\u03bb \u2211t i=1(EXi\u2212Xi) ] = EAt [ e\u03bb \u2211t i=1(EXi\u2212Xi) ] + E\u00acAt [ e\u03bb \u2211t i=1(EXi\u2212Xi)\n] \u2264 e\u2212\u03bb\u03b4t,h+1+ \u03bb2(h+1)2 8 E [ e\u03bb \u2211t\u22121 i=1(EXi\u2212Xi) ] + P {\u00acAt} e\u03bbtQ \u2217\n\u2264 e \u03bb2(h+1)2 8 E [ e\u03bb \u2211t\u22121 i=1(EXi\u2212Xi) ] + cp,he t(\u03bb(h+1)\u2212c\u2032h) by Eq. 18, and Q\u2217 \u2264 h+ 1 \u2264 e \u03bb2(h+1)2 8 (t\u2212c \u2032\u2032 h)e\u03bbc \u2032\u2032 h(h+1) +\nt\u2211 \u03c4=c\u2032\u2032h+1 e \u03bb2(h+1)2 8 (t\u2212\u03c4)cp,he \u03c4(\u03bb(h+1)\u2212c\u2032h) (22)\nby iterating until c\u2032\u2032h along Eq. 24 as described below.\nConsidering the recursion\nf (t) = \u03b8f (t\u2212 1) + g (t) , (23)\nit can be shown that, by iterating until t = c, Eq. 23 is equivalent to\nf (t) = \u03b8t\u2212cf (c) + t\u2211 \u03c4=c+1 \u03b8t\u2212\u03c4g (\u03c4) . (24)\nGiven that, the last bound in Eq. 22 is obtained by setting f (t) = E [ e\u03bb \u2211t i=1(EXi\u2212Xi) ] ,\n\u03b8 = e \u03bb2(h+1)2 8 , and g (t) = cp,he t(\u03bb(h+1)\u2212c\u2032h).\nDenoting now \u03be = \u03b1c\u2032h\n(h+1) , and choosing \u03bb = \u03be 2 (h+1) ,\n(22) = e\u03be 2 (t\u2212c\n\u2032\u2032 h)\n2 e2\u03bec \u2032\u2032 h + t\u2211 \u03c4=c\u2032\u2032h+1 e\u03be 2 (t\u2212\u03c4) 2 cp,he \u03c4(2\u03be\u2212c\u2032h)\n\u2264 e\u03be2 (t\u2212c\u2032\u2032h) 2 e2\u03bec \u2032\u2032 h + t\u2211 \u03c4=c\u2032\u2032h+1 e\u03be 2 (t\u2212\u03c4) 2 cp,h\nsince, by definition of \u03b1, it holds that \u03be \u2264 c\u2032h 2\n= e\u03be 2 (t\u2212c\n\u2032\u2032 h)\n2 e2\u03bec \u2032\u2032 h + cp,he \u03be2 t 2 t\u2211 \u03c4=c\u2032\u2032h+1 e\u2212\u03be 2 \u03c4 2\n\u2264 e\u03be2 (t\u2212c\u2032\u2032h) 2 e2\u03bec \u2032\u2032 h + cp,he \u03be2 t 2\n( 2\n\u03be2 e\u2212\n\u03be2\n2 c\u2032\u2032h ) since\n\u221e\u2211 t=c+1 e\u2212kt \u2264 1 k e\u2212ck\n\u2264 e\u03be2 t 2 e2\u03bec \u2032\u2032 h + 2cp,h \u03be2 e\u03be 2 t 2 . (25)\nUsing the bound provided by Eq. 25 for \u03bb = \u03be 2(h+1) into Eq. 21, we obtain\nP { Q\u0302\u2217 \u2264 EQ\u0302\u2217 \u2212 \u03b1 \u2223\u2223\u2223 n(a\u2217) = t} \u2264 e\u2212 3\u03b12c\u2032h 2(h+1)2 t ( e 2\u03b1c\u2032hc \u2032\u2032 h h+1 + 2cp,h (h+ 1) 2\n\u03b12c\u20322h\n) .\n(26)\nSince this bound is true only for \u03b1 > 0, it holds only starting from n such\nthat \u2206h+1[s,a] 2 > 1 t \u2211t \u03c4=1 \u03b4\u03c4,h+1 for all t \u2265 n0 = nph+1 2 . Recalling Eq. 19, by induction hypothesis, for t \u2265 c\u2032\u2032h it holds that\n\u221e\u2211 \u03c4=1 \u03b4\u03c4,h+1 = c\u2032\u2032h\u2211 \u03c4=1 \u03b4\u03c4,h+1 + \u221e\u2211 \u03c4=c\u2032\u2032h+1 \u03b4\u03c4,h+1\n\u2264 c\u2032\u2032h(h+ 1) + \u221e\u2211\n\u03c4=c\u2032\u2032h+1\nc\u03b4,he \u2212c\u2032h\u03c4\n\u2264 c\u2032\u2032h(h+ 1) + c\u03b4,h c\u2032h e\u2212c \u2032 hc \u2032\u2032 h , \u03b2.\n(27)\nTherefore, for t \u2265 2d\u03b2, we have that \u03b1 > 0. Furthermore, to simplify dealing with \u03b1 in the denominator in Eq. 26, for t \u2265 4d\u03b2, we have \u03b1 \u2265 \u2206h+1[s,a] 4 . Hence, for all n \u2265 c\u2032\u2032h+1 = 8 dph+1 \u03b2, we have that \u03b1 \u2265 \u2206h+1[s,a]4 for all t > n0. Consequently, from Eq. 26, for all n \u2265 c\u2032\u2032h+1,\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2 \u2223\u2223\u2223\u2223 n(a\u2217) = t} \u2264 e\u2212 3c\u2032hd 2 8(h+1)2 t e 3c\u2032h 2(h+1) \u03b2 ( e 2\u03b1c\u2032hc \u2032\u2032 h h+1 + 2cp,h (h+ 1) 2\n\u03b12c\u20322h\n)\nsince \u2212t\u03b12 \u2264 \u2212t ( \u2206h+1[s, a] 2\n4 \u2212\u2206h+1[s, a]\n\u03b2\nt\n) \u2264 \u2212 td 2\n4 + (h+ 1)\u03b2\n\u2264 e\u2212 3c\u2032hd 2 8(h+1)2 t e 3c\u2032h 2(h+1) \u03b2 ( ec \u2032 hc \u2032\u2032 h + cp,h 32 (h+ 1)2\nd2c\u20322h\n) , \u03b3(t).\nsince d 4 \u2264 \u03b1 \u2264 h+ 1 2\nThis now provides us with the desired bound on the second summand in Eq. 11 as\nP { Q\u0302\u2217 \u2264 Q\u2217 \u2212 \u2206h+1[s, a]\n2 , n(a\u2217) > n0 } \u2264\nn\u2211 t=n0 \u03b3(t) \u00b7 P {n(a\u2217) = t} \u2264 \u03b3(n0) n\u2211 t=n0 P {n(a\u2217) = t}\n\u2264 \u03b3(n0) = e \u2212n\n3c\u2032hd 2ph+1 16(h+1)2 e 3c\u2032h 2(h+1) \u03b2 ( ec \u2032 hc \u2032\u2032 h + cp,h 32 (h+ 1)2\nd2c\u20322h\n)\nThis finalizes the proof of induction hypothesis with\nch+1 = 2\n( 1 + e 3c\u2032h 2(h+1) \u03b2 ( ec \u2032 hc \u2032\u2032 h + cp,h 32 (h+ 1)2\nd2c\u20322h\n)) ,\nc\u2032h+1 = 3c\u2032hd 2p2h+1\n16 (h+ 1)2 ,\nc\u2032\u2032h+1 = 8\ndph+1 \u03b2."}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].\nIn sailing domain, a sailboat navigates to a destination on an 8-connected grid representing a marine environment, under fluctuating wind conditions. The goal is to reach the destination in as short time as possible, by choosing at each grid location a neighbor location to move to. The duration of each such move depends on the direction of the move (ceteris paribus, diagonal moves take \u221a 2 more time than straight moves), the direction of the wind relative to the sailing direction (the sailboat cannot sail against the wind\nand moves fastest under tail wind), and the tack. The direction of the wind changes over time, but its strength is assumed to be fixed. This sailing problem can be formulated as a goal-driven MDP over finite state space and finite set of actions, with each state capturing the position of the sailboat, wind direction, and tack. In a goal-driven MDP, the lengths of the paths to a terminal state are not necessarily bounded, and thus it is not entirely clear to what depth BRUE shall construct its tree. In the sailing domain, we chose H to be 4 \u00d7 n, where n is the grid-size of the problem instance, as it is unlikely that the optimal path between any two locations on the grid will be larger than a complete encircling of the considered area. We note, however, that the recommendation-oriented samples \u03c1\u0304 always end at a terminal state, similar to the rollouts issued by UCT and GCT.\nSnapshots of the results for different grid sizes are shown in Figure 3. The comparison was made with two MCT-based algorithms: the UCT algorithm, and a recent modification of UCT, GCT, obtained from the former by replacing the UCB1 policy at the root node with the -greedy policy [19]. The motivation behind the design of GCT was to improve the empirical simple regret of UCT, and the results for GCT reported by [19] (and confirmed by our experiments here) are very impressive. All three algorithms were implemented within a single software infrastructure, with the parameters for UCT and GCT being set as in the previously reported evaluations on the sailing domain. Each algorithm was run on 1000 randomly chosen initial states s0, and the performance of the algorithm was assessed in terms of the average error Q(s0, a) \u2212 V (s0), that is, the difference between the true values of\nthe action a chosen by the algorithm and this of the optimal action \u03c0\u2217(s0). Consistently with the results reported by Tolpin and Shimony ([19]), GCT outperformed UCT by a very large margin, with the latter exhibiting very poor performance improvement over time even on the smallest, 5\u00d7 5, grids. In turn, BRUE substantially outperformed GCT, with the improvement being consistent except for relatively short planning deadlines.\nThe above allows us to conclude that BRUE is not only attractive in formal terms of performance guarantees, but can also very effective in practice of online planning. Under the same parameter setting of UCT and GCT, we have also evaluated the three algorithms in a domain of random game trees that aims at a simple modeling of two-person zero-sum games such as Go, Amazons and Globber. In such games, the winner is decided by a global evaluation of the end board, with the evaluation employing this or another feature counting procedure; the rewards thus are associated only with the terminal states. The rewards are calculated by first assigning values to moves, and then summing up these values along the paths to the terminal states. Note that the move values are used for the tree construction only and are not made available to the players. The values are chosen uniformly from [0, 127] for the moves of MAX, and from [\u2212127, 0] for the moves of MIN. The players act so to (depending on the role) maximize/minimize their individual payoff: the aim of MAX is to reach terminal s with as high R(s) as possible, and the objective of MIN is similar, mutatis mutandis. This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players. We have ran some preliminary experiments with two different settings of the branching factor (B) and tree depths (D). As in the sailing domain, we compared the convergence rate obtained by BRUE, UCT and GCT. Figure 4 plots the average error rate for two configurations, B = 6, D = 6 and B = 2, D = 16, with the average in each setting obtained over 20 trees and 100 runs for each tree. The results here appear encouraging as well, with BRUE taking over the other two algorithms faster on the deeper trees."}, {"heading": "5 SUMMARY AND DISCUSSION", "text": "We have introduced BRUE, a simple Monte-Carlo algorithm for online planning in MDPs that guarantees exponential-over-time reduction of the performance measures of interest, namely the simple regret and the probability\nof erroneous action choice. This improves over previous algorithms such as UCT that guarantee only polynomial-over-time reduction of these measures. The algorithm has been formalized for finite horizon MDPs, and it was analyzed as such. However, our empirical evaluation shows that it performs well also on goal-driven MDPs and two-person games.\nOur work leaves a few questions for future work. Considering \u03b3-discounted MDPs with infinite horizon, a straightforward way to employ BRUE in that setting is to fix a horizon H, use the algorithm as it is, and derive guarantees on the aforementioned measures of interest by simply accounting for the additive gap of \u03b3HRmax/(1\u2212\u03b3) between the state/action values under horizon H and these under infinite horizon. However, this is not necessarily the best way to plan online for infinite-horizon MDPs, and thus this setting requires further introspection. Second, it is not unlikely that the state-space independent factors ch, c \u2032 h, and c \u2032\u2032 h in the guarantees of BRUE can be substantially improved by employing more sophisticated combinations of exploration and recommendation probes, and currently we examine this issue. Finally, the core tree sampling scheme employed by BRUE differ from the more standard scheme employed in previous work. While this difference plays a critical role in establishing the formal guarantees of BRUE, it is still unclear whether that difference is necessary for establishing exponential-over-time reduction of the performance measures."}, {"heading": "Acknowledgements", "text": "Work supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "UCT for tactical assault planning in real-time strategy games", "author": ["R. Balla", "A. Fern"], "venue": "In IJCAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Lower bounding Klondike Solitaire with Monte-Carlo planning", "author": ["R. Bjarnason", "A. Fern", "P. Tadepalli"], "venue": "In ICAPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Pure exploration in finitely-armed and continuous-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Nested Monte-Carlo search", "author": ["T. Cazenave"], "venue": "In IJCAI, pages 456\u2013461,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Bandit algorithms for tree search", "author": ["P-A. Coquelin", "R. Munos"], "venue": "In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "High-quality policies for the Canadian Traveler\u2019s problem", "author": ["P. Eyerich", "T. Keller", "M. Helmert"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Monte-Carlo tree search and rapid action value estimation in computer Go", "author": ["S. Gelly", "D. Silver"], "venue": "AIJ, 175(11):1856\u20131875,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Metareasoning for Monte Carlo tree search", "author": ["N. Hay", "S.J. Russell"], "venue": "Technical Report UCB/EECS-2011-119, EECS Department,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M.J. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Probabilistic planning based on UCT", "author": ["T. Keller", "P. Eyerich"], "venue": "In ICAPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In ECML, pages 282\u2013293,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "On-line search for solving Markov decision processes via heuristic sampling", "author": ["L. P\u00e9ret", "F. Garcia"], "venue": "In ECAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1952}, {"title": "Nested rollout policy adaptation for Monte Carlo tree search", "author": ["C.D. Rosin"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "An analysis of forward pruning", "author": ["S.J. Smith", "D.S. Nau"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "An analysis of UCT in multi-player games", "author": ["N. Sturtevant"], "venue": "In CCG, page 3749,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Doing better than UCT: Rational Monte Carlo sampling in trees", "author": ["D. Tolpin", "S.E. Shimony"], "venue": "CoRR, arXiv:1108.3711v1 [cs.AI],", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "An MDP \u3008S,A, Tr,R\u3009 is defined by a set of states S, a set of actions A, a stochastic transition function Tr : S \u00d7 A \u00d7 S \u2192 [0, 1], and a reward function", "startOffset": 122, "endOffset": 128}, {"referenceID": 9, "context": "The sparse sampling algorithm by Kearns, Mansour, and Ng [10] offered near-optimal action selection in time exponential in H but independent of the state space size.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 12, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 11, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 5, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 4, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 14, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 18, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 11, "context": "Numerous concrete instances of MCT have been proposed, with UCT [12]", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 16, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 2, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 1, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 6, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 10, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 0, "context": "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a \u2208 A(si), then", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a \u2208 A(si), then", "startOffset": 206, "endOffset": 210}, {"referenceID": 11, "context": "In that respect, according to Theorem 6 of Kocsis and Szepesv\u00e1ri [12], UCT achieves the optimal logarithmic cumulative regret.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "In contrast, the same Theorem 6 of Kocsis and Szepesv\u00e1ri [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "In contrast, the same Theorem 6 of Kocsis and Szepesv\u00e1ri [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.", "startOffset": 207, "endOffset": 210}, {"referenceID": 18, "context": "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 3, "context": "[4] was probably the first systematic attempt to analyze pure exploration in MABs, showing that the minimal simple regret in MAB can increase as the bound on the cumulative regret is getting smaller.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] for MABs, the BRUE setting of MCTer is as follows: \u2022 probe-S: The probes \u03c1 = \u3008s0, a1, .", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Theorem 1 Let BRUE be called on a state s0 of an MDP \u3008S,A, Tr,R\u3009 with rewards in [0, 1] and finite horizon H.", "startOffset": 81, "endOffset": 87}, {"referenceID": 12, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 11, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 18, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 11, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "The comparison was made with two MCT-based algorithms: the UCT algorithm, and a recent modification of UCT, GCT, obtained from the former by replacing the UCB1 policy at the root node with the -greedy policy [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "The motivation behind the design of GCT was to improve the empirical simple regret of UCT, and the results for GCT reported by [19] (and confirmed by our experiments here) are very impressive.", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "Consistently with the results reported by Tolpin and Shimony ([19]), GCT outperformed UCT by a very large margin, with the latter exhibiting very poor performance improvement over time even on the smallest, 5\u00d7 5, grids.", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.", "startOffset": 102, "endOffset": 110}], "year": 2017, "abstractText": "We consider online planning in Markov decision processes. An algorithm for this problem should explore the set of possible policies from the current state, and, when interrupted, recommend an action to follow based on the outcome of the exploration. The performance of such an algorithm is assessed in terms of its simple regret, that is the loss in performance resulting from choosing the recommended action instead of an optimal one, and/or in terms of probability that the recommended action is not an optimal one. The best guarantees provided by the state-of-the-art algorithms for reduction of these measures over time are only polynomial. We introduce a new algorithm, BRUE, that achieves over time exponential reduction of these two measures. The algorithm is based on a simple yet non-standard state-space sampling scheme in which different samples are dedicated to different objectives. Our preliminary empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art.", "creator": "LaTeX with hyperref package"}}}